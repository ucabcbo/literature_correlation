"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Kong Y.; Hong F.; Leung H.; Peng X.","Kong, Yingying (35186206400); Hong, Fang (57315801600); Leung, Henry (7202811506); Peng, Xiangyang (57214935616)","35186206400; 57315801600; 7202811506; 57214935616","A fusion method of optical image and sar image based on dense-ugan and gram–schmidt transformation","2021","Remote Sensing","13","21","4274","","","","10.3390/rs13214274","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118155325&doi=10.3390%2frs13214274&partnerID=40&md5=3858ddecb0bc7adabfac80b0b9b1b717","To solve the problems such as obvious speckle noise and serious spectral distortion when existing fusion methods are applied to the fusion of optical and SAR images, this paper proposes a fusion method for optical and SAR images based on Dense-UGAN and Gram–Schmidt transformation. Firstly, dense connection with U-shaped network (Dense-UGAN) are used in GAN generator to deepen the network structure and obtain deeper source image information. Secondly, according to the particularity of SAR imaging mechanism, SGLCM loss for preserving SAR texture features and PSNR loss for reducing SAR speckle noise are introduced into the generator loss function. Meanwhile in order to keep more SAR image structure, SSIM loss is introduced to discriminator loss function to make the generated image retain more spatial features. In this way, the generated high-resolution image has both optical contour characteristics and SAR texture characteristics. Finally, the GS transformation of optical and generated image retains the necessary spectral properties. Experimental results show that the proposed method can well preserve the spectral information of optical images and texture information of SAR images, and also reduce the generation of speckle noise at the same time. The metrics are superior to other algorithms that currently perform well. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","Geometrical optics; Image fusion; Image texture; Radar imaging; Remote sensing; Speckle; Synthetic aperture radar; Textures; Fusion methods; Gram-schmidt; Image information; Image-based; Loss functions; Optical image; Optical-; Remote sensing images; SAR Images; Speckle noise; Generative adversarial networks","Generative adversarial network; Gram–Schmidt; Image fusion; Loss function; Remote sensing image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85118155325"
"Mao Q.; Yang X.; Zhang R.; Jeon G.; Hussain F.; Liu K.","Mao, Qingyu (57207574148); Yang, Xiaomin (9237988500); Zhang, Rongzhu (7404865474); Jeon, Gwanggil (15022497800); Hussain, Farhan (57200408659); Liu, Kai (57223776901)","57207574148; 9237988500; 7404865474; 15022497800; 57200408659; 57223776901","Multi-focus images fusion via residual generative adversarial network","2022","Multimedia Tools and Applications","81","9","","12305","12323","18","10.1007/s11042-021-11278-0","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113582031&doi=10.1007%2fs11042-021-11278-0&partnerID=40&md5=b846452b0b007e65e840de7c99651053","Recently, most existing learning-based fusion methods are not fully end-to-end, which still predict the decision map and recover the fused image by the refined decision map. However, in practice, these methods are hard to predict the decision map precisely. Inaccurate prediction further degrades the performance of fusing, resulting in edge blurring and artefacts. This paper proposes an end-to-end multi-focus image fusion model based on conditional generative adversarial network (MFFGAN). In MFFGAN, we introduce a pioneering use of the conditional generative adversarial network to the field of image fusion. Moreover, we introduce the simple and efficient relativistic discriminator to our network, so the network converges faster. More importantly, MFFGAN is fully trained in this adversarial relationship to produce visually perceptive images that contain rich texture information and avoid the post-processing phase. Considering the detailed information of source images, we introduce the widely used perceptual loss to improve fused image performance. Thanks to the element-wise fusion criterion, our model can conveniently and efficiently fuse multiple images. Additionally, extensive experimental results show that the proposed model achieves excellent performance in subjective and objective evaluations. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Forecasting; Image enhancement; Textures; Adversarial networks; Fusion methods; Multifocus image fusion; Multifocus images; Multiple image; Post processing; Subjective and objective evaluations; Texture information; Image fusion","End-to-end model; Generative adversarial network; Multi-focus image fusion; Multiple input images","Article","Final","","Scopus","2-s2.0-85113582031"
"Mehmood R.; Bashir R.; Giri K.J.","Mehmood, Rayeesa (57220032731); Bashir, Rumaan (56032341200); Giri, Kaiser J. (56031665200)","57220032731; 56032341200; 56031665200","Comparative Analysis of AttnGAN, DF-GAN and SSA-GAN","2021","Proceedings - 2021 3rd International Conference on Advances in Computing, Communication Control and Networking, ICAC3N 2021","","","","370","375","5","10.1109/ICAC3N53548.2021.9725424","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126946639&doi=10.1109%2fICAC3N53548.2021.9725424&partnerID=40&md5=2f9d91b3c0a82d44b1c833e3239bd6ef","In computer vision, generating visuals based on the associated text is important, demanding, and interesting task. GANs (Generative Adversarial Networks) are the most powerful generative models for computer vision and natural language processing. GANs ensure that the synthesized image matches the input text semantically. The purpose of this paper is to compare three algorithms for producing images from text. These include Attentional Generative Adversarial Networks (AttnGAN), Deep-Fusion Generative Adversarial Networks (DF-GAN) and Semantic-Spatial Aware Generative Adversarial Networks (SSA-GAN). AttnGAN extends StackGAN++ by including attention in a multi-stage refining pipeline. DF-GAN, and SSA-GAN on the other hand are based on one stage architecture. DF-GAN incorporates Deep Fusion Block (DFBlock) that effectively fuses textual and visual information whereas SSA-GAN uses novel Semantic-Spatial Aware Convolutional Network (SSACN) block for efficient and deep text and image features fusion.  © 2021 IEEE.","Computer vision; Image fusion; Natural language processing systems; Semantics; Attentional generative adversarial network; Comparative analyzes; Deep-fusion generative adversarial network; Diversity; Generative model; Multi-stages; Semantic-spatial aware generative adversarial network; Synthesized images; Textual information; Visual information; Generative adversarial networks","AttnGAN; DF-GAN; diversity; GAN; generative model; mode collapse; SSA-GAN","Conference paper","Final","","Scopus","2-s2.0-85126946639"
"Qiu D.; Hu X.; Liang P.; Liu X.; Jiang J.","Qiu, Defen (58085676600); Hu, Xingyu (57279534600); Liang, Pengwei (57201500677); Liu, Xianming (57204313011); Jiang, Junjun (54902306100)","58085676600; 57279534600; 57201500677; 57204313011; 54902306100","A deep progressive infrared and visible image fusion network; [红外与可见光图像渐进融合深度网络]","2023","Journal of Image and Graphics","28","1","","156","165","9","10.11834/jig.220319","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147183420&doi=10.11834%2fjig.220319&partnerID=40&md5=76da56962094a406fab714557438b5cf","Objective Multi-modal images have been developed based on multiple imaging techniques. The infrared image collects the radiation information of the target in the infrared band. The visible image is more suitable to human visual perception in terms of higher spatial resolution, richer effective information and lower noise. Infrared and visible image fusion (IVIF) can integrate the configurable information of multi-sensors to alleviate the limitations of hardware equipment and obtain more low-cost information for high-quality images. The IVIF can be used for a wide range of applications like surveillance, remote sensing and agriculture. However, there are several challenges to be solved in multi-modal image fusion. For instance, effective information extraction issue from different modalities and the problem-solving for fusion rule of the complementary information of different modalities. Current researches can be roughly divided into two categories: 1) traditional methods and 2) deep learning based methods. The traditional methods decompose the infrared image and the visible image into the transform domain to make the decomposed representation have special properties that are benefit to fusion, then perform fusion in the transform domain, which can depress information loss and avoid the artifacts caused by direct pixel manipulation, and finally reconstruct the fused image. Traditional methods are based on the assumptions on the source image pair and manual-based image decomposition methods to extract features. However, these hand-crafted features are not comprehensive and may cause the sensitivity to high-frequency or primary components and generate image distortion and artifacts. In recent years, data-driven deep learning-based image fusion methods have been developing. Most of the deep learning based fusion methods have been oriented for the infrared and visible image fusion in the deep feature space. Deep learning-based fusion methods can be divided into two categories: 1) convolutional neural network (CNN) for fusion, and 2) generative adversarial network (GAN) to generate fusion images. CNN-based information extraction is not fully utilized by the intermediate layers. The GAN-based methods are challenged to preserving image details in adequately. Method We develop a novel progressive infrared and visible image fusion framework (ProFuse), which extracts multi-scale features with U-Net as our backbone, merges the multi-scale features and reconstructs the fused image layer by layer. Our network has composed of three parts: 1) encoder; 2) fusion module; and 3) decoder. First, a series of multi-scale feature maps are generated from the infrared image and the visible image via the encoder. Next, the multi-scale features of the infrared and visible image pair are fused in the fusion layer to obtain fused features. At last, the fused features pass through the decoder to construct the fused image. The network architecture of the encoder and decoder is designed based on U-Net. The encoder consists of the replicable applications of recurrent residual convolutional unit (RRCU) and the max pooling operation. Each down-sampling step can be doubled the number of feature channels, so that more features can be extracted. The decoder aims to reconstruct the final fused image. Every step in the decoder consists of an up-sampling of the feature map followed by a 3 × 3 convolution that halves the number of feature channels, a concatenation with the corresponding feature maps from the encoder, and a RRCU. At the fusion layer, our spatial attention-based fusion method is used to deal with image fusion tasks. This method has the following two advantages. First, it can perform fusion on global information-contained high-level features (at bottleneck semantic layer), and details-related low-level features (at shallow layers). Second, our method not only perform fusion on the original scale (maintaining more details), but also perform fusion on other smaller scales (maintaining semantic information). Therefore, the design of progressive fusion is mainly specified in the following two aspects: 1) we conduct image fusion progressively from high-level to low-level and 2) from small-scale to large-scale progressively. Result In order to evaluate the fusion performance of our method, we conduct experiments on publicly available Toegepast Natuurwetenschappelijk Onderzoek (TNO) dataset and compare it with some state-of-the-art (SOTA) fusion methods including DenseFuse, discrete wavelet transform (DWT), Fusion-GAN, ratio of low-pass pyramid (RP), generative adversarial network with multiclassification constraints for infrared and visible image fusion (GANMcC), curvelet transform (CVT). All these competitors are implemented according to public code, and the parameters are set by referring to their original papers. Our method is evaluated with other methods in subjective evaluation, and some quality metrics are used to evaluate the fusion performance objectively. Generally speaking, the fusion results of our method obviously have higher contrast, more details and clearer targets. Compared with other methods, our method preserves the detailed information of visible and infrared radiation in maximization. At the same time, very little noise and artifacts are introduced in the results. We evaluate the performances of different fusion methods quantitatively via using six metrics, i. e., entropy (EN), structure similarity (SSIM), edge-based similarity measure (Qabf), mutual information (MI), standard deviation (STD), sum of the correlations of differences (SCD). Our method has achieved a larger value on EN, Qabf, MI and STD. The maximum EN value indicates that our method retains richer information than other competitors. The Qabf is a novel objective quality evaluation metric for fused images. The higher the value of Qabf is, the better the quality of the fusion images are. STD is an objective evaluation index that measures the richness of image information. The larger the value, the more scattered the gray-level distribution of the image, the more information the image carries, and the better the quality of the fused image. The larger the value of MI, the more information obtained from the source images, and the better the fusion effect. Our method has an improvement of 115. 64% in the MI index compared with the generative adversarial network for infrared and visible image fusion (FusionGAN) method, 19. 93% in the STD index compared with the GANMcC method, 1. 91% in the edge preservation (Qabf) index compared with the DWT method and 1. 30% in the EN index compared with the GANMcC method. This indicates that our method is effective for IVIF task. Conclusion Extensive experiments demonstrate the effectiveness and generalization of our method. It shows better results on the evaluations in qualitative and quantitative both. © 2023 Editorial and Publishing Board of JIG. All rights reserved.","","deep learning; image fusion; infrared image; unsupervised learning; visible image","Article","Final","","Scopus","2-s2.0-85147183420"
"Yang Z.; Ma A.; Zhang Z.; Zhang Z.; Li Z.; Gao K.","Yang, Zhijia (57216124646); Ma, Aoqi (57573558500); Zhang, Zefeng (57573934500); Zhang, Zhaocen (57573370900); Li, Zhengjun (57572787900); Gao, Kun (57204363389)","57216124646; 57573558500; 57573934500; 57573370900; 57572787900; 57204363389","Infrared and Passive Millimeter Wave image fusion based on multi-resolution deep learning method","2022","Proceedings of SPIE - The International Society for Optical Engineering","12169","","12169C8","","","","10.1117/12.2627192","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128025908&doi=10.1117%2f12.2627192&partnerID=40&md5=b6c801144306f7376934651546f761a2","Infrared & Passive Millimeter Wave (IR/PMMW) composite guidance is the development hotspot of multimode composite guidance technology. Considering the low penetrability of IR imaging system under nonideal visibility conditions, while the PMMW imaging technology has high atmospheric transmittance but low resolution, a fusion method of infrared/millimeter wave images based on multi-resolution deep learning is proposed. In this method, the infrared and millimeter wave images are first decomposed by NSCT transformation to separate the low-frequency and high-frequency components of the input image. For low-frequency components fusion, we design a specific generative adversarial convolution neural network for activity-level measurement and fusion rules to preserve information in the both scenes as much as possible. The high-frequency components are fused by the Pulse Couple Neural Network algorithm because of its similar processing mechanism with human visual nervous system; the fusion results of the low and high frequency components are subjected to inverse NSCT transformation, the final fused image is obtained. Data augment technology, such as image style transferring, is applied to extend IR/PMMW training set. Extensive results demonstrate that the proposed method can generate image with higher qualities with salient targets inside, deliver better performance than the state-of-the-art methods in both subjective and objective evaluation. © 2022 SPIE","Convolutional neural networks; Deep learning; Image fusion; Image resolution; Infrared imaging; Inverse problems; Millimeter waves; High frequency components; Higher-frequency components; Infrared and passive millimeter wave image; Lower frequency components; Millimeter-wave images; Millimetre-wave images; Multi-modality image fusion; NSCT transform; Passive millimeter wave; PCNN; Generative adversarial networks","generative adversarial network; Infrared and Passive Millimeter Wave image; multi-modality image fusion; NSCT transform; PCNN","Conference paper","Final","","Scopus","2-s2.0-85128025908"
"","","","2021 IEEE International Conference on Medical Imaging Physics and Engineering, ICMIPE 2021 - Proceedings","2021","2021 IEEE International Conference on Medical Imaging Physics and Engineering, ICMIPE 2021 - Proceedings","","","","","","260","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126748806&partnerID=40&md5=c57d1b7c51d4f8d44398733412d4d243","The proceedings contain 45 papers. The topics discussed include: a cascaded 3d neural network for liver tumor segmentation; a geometry information enhanced Unet for tumor segmentation; quantification of pectinate muscles inside left atrial appendage from CT images using fractal analysis; CT/cone-beam CT image characteristics of ameloblastoma, odontogenic keratocyst and dentigerous cyst associated with the impacted mandibular third molar; CT metal artifact correction assisted by the deep learning-based metal segmentation on the projection domain; green fluorescent protein and phase contrast image fusion via dual attention residual network; automatic cone-beam computed tomography segmentation with small samples based on generative adversarial networks and semantic segmentation; a novel landmark detection method for cephalometric measurement; and odontogenic keratocyst involving zygoma: cone-beam and multislice spiral computed tomography aided the precise definition and appropriate treatment for the rare location.","","","Conference review","Final","","Scopus","2-s2.0-85126748806"
"Zhu W.-Q.; Tang X.-Y.; Zhang R.; Chen X.; Miao Z.","Zhu, Wen-Qing (57211781083); Tang, Xin-Yi (55725150400); Zhang, Rui (57309186500); Chen, Xiao (57826822400); Miao, Zhuang (55644039800)","57211781083; 55725150400; 57309186500; 57826822400; 55644039800","Infrared and visible image fusion based on edge-preserving and attention generative adversarial network; [基于边缘保持和注意力生成对抗网络的红外与可见光图像融合]","2021","Hongwai Yu Haomibo Xuebao/Journal of Infrared and Millimeter Waves","40","5","","696","708","12","10.11972/j.issn.1001-9014.2021.05.017","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118466074&doi=10.11972%2fj.issn.1001-9014.2021.05.017&partnerID=40&md5=2f8929c2463e76cf99488eb2cef283f1","Infrared and visible image features are quite different, and there are no ideal fused images supervise neural networks to learn the mapping relationship between the source images and the fused images. Thus, the application of deep learning is limited to the field of image fusion. To solve this problem, a generative adversarial network framework based on attention mechanism and edge loss is proposed, which is applied to the infrared and visible image fusion. Derived from the thoughts of attention mechanism and adversarial training, the fusion problem is regarded as an adversarial game between the source images and the fused images, and combining channel attention and spatial attention mechanism can learn nonlinear relationship between channel domain features and spatial domain features, which enhances the expression of salient target features. At the same time, an edge-based loss function is proposed, which converts the mapping relationship between the source image pixels and the fused image pixels into the mapping relationship between the edges. Experimental results on multiple datasets demonstrate that the proposed method can effectively fuse infrared target and visible texture information, sharpen image edges, and significantly improve image clarity and contrast. © 2021, Science Press. All right reserved.","Deep learning; Image enhancement; Image fusion; Mapping; Pixels; Textures; Attention mechanisms; Domain feature; Edge-based; Edge-based loss function; Fused images; Infrared and visible image; Learn+; Loss functions; Mapping relationships; Source images; Generative adversarial networks","Attention mechanism; Edge-based loss function; Generative adversarial network; Image fusion","Article","Final","","Scopus","2-s2.0-85118466074"
"Chen L.; Han J.","Chen, Lei (57273656000); Han, Jun (55753463900)","57273656000; 55753463900","Infrared and visible image fusion using salient decomposition based on a generative adversarial network","2021","Applied Optics","60","23","","7017","7026","9","10.1364/AO.427245","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112701501&doi=10.1364%2fAO.427245&partnerID=40&md5=4cae304ac3195343ae07f4076faedce0","In order to address the fusion problem of infrared (IR) and visible images, this paper proposes a method using a local non-subsampled shearlet transform (LNSST) based on a generative adversarial network (GAN). We first decompose the source images into basic images and salient images by LNSST, then use two GANs fuse basic images and salient images. Lastly, we compose the fused basic images and salient images by inverse LNSST. We adopt public data sets to verify our method and by comparing with eight objective evaluation parameters obtained by 10 other methods. It is demonstrated that our method is able to achieve better performance than the state of the art on preserving both texture details and thermal information. © 2021 Optical Society of America.","Inverse problems; Textures; Adversarial networks; Infrared and visible image; Objective evaluation; Public data; Shearlet transforms; Source images; State of the art; Visible image; Image fusion","","Article","Final","","Scopus","2-s2.0-85112701501"
"Liu W.; Chen C.; Jiang R.; Lu T.","Liu, Wei (57202069759); Chen, Cheng (57477516100); Jiang, Rui (57478260900); Lu, Tao (56406646300)","57202069759; 57477516100; 57478260900; 56406646300","Four-path unsupervised learning-based image defogging network; [四通道无监督学习图像去雾网络]","2022","Tongxin Xuebao/Journal on Communications","43","10","","210","222","12","10.11959/j.issn.1000-436x.2022201","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141922406&doi=10.11959%2fj.issn.1000-436x.2022201&partnerID=40&md5=200827f094245b2807ccd5f3568ce150","To solve the problems of supervised network and unsupervised network in the field of single image defogging, a four-path unsupervised learning-based image defogging network based on cycle generative adversarial network (CycleGAN) was proposed, which mainly included three sub-networks: defogging network, synthetic fog network and attention feature fusion network. The three sub-networks were sequentially combined to construct four learning paths, which were the defogging path, the color-texture recovery path for defogged result, the synthetic fog path, and the color-texture recovery path for synthetic fog result. Specifically, in the synthetic fog network, to better constrain the defogging network to generate higher quality fogfree images, the atmospheric scattering model (ASM)was introduced to enhance the mapping transformation of the network from the foggy image domain to the fogfree image domain. Furthermore, to further improve the image generation quality of the defogging network and the synthetic fog network, an attention feature fusion network was proposed. The proposed network was based on several fog-derived images, which adopts a multi-channel mapping structure and an attention mechanism to enhance the recovery of color and texture details. Extensive experiments on both synthetic and real-world datasets show that the proposed method can better restore the color and texture details information of foggy images in various scenes. © 2022 Editorial Board of Journal on Communications. All rights reserved.","Color; Computer system recovery; Generative adversarial networks; Image enhancement; Image fusion; Image texture; Mapping; Recovery; Textures; Atmospheric scattering models; Attention feature fusion; Color and textures; Color textures; Cycle generative adversarial network; Features fusions; Image domain; Single image defogging; Single images; Subnetworks; Fog","atmospheric scattering model; attention feature fusion; cycle generative adversarial network; single image defogging; unsupervised learning","Article","Final","","Scopus","2-s2.0-85141922406"
"Shah D.; Wani H.; Das M.; Gupta D.; Radeva P.; Bakde A.","Shah, Dhruvi (57212594647); Wani, Hareshwar (57819112600); Das, Manisha (57222471885); Gupta, Deep (57202197339); Radeva, Petia (56208405900); Bakde, Ashwini (57222463833)","57212594647; 57819112600; 57222471885; 57202197339; 56208405900; 57222463833","STPGANsFusion: Structure and Texture Preserving Generative Adversarial Networks for Multi-modal Medical Image Fusion","2022","2022 National Conference on Communications, NCC 2022","","","","172","177","5","10.1109/NCC55593.2022.9806733","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134895297&doi=10.1109%2fNCC55593.2022.9806733&partnerID=40&md5=27b738aa7b8352e3d746741dd5193774","Medical images from various modalities carry diverse information. The features from these source images are combined into a single image, constituting more information content, beneficial for subsequent medical applications. Recently, deep learning (DL) based networks have demonstrated the ability to produce promising fusion results by integrating the feature extraction and preservation task with less manual interventions. However, using a single network for extracting features from multi-modal source images characterizing distinct information results in the loss of crucial diagnostic information. Addressing this problem, we present structure and texture preserving generative adversarial networks based medical image fusion method (STPGANsFusion). Initially, the textural and structural components of the source images are separated using structure gradient and texture decorrelating regularizer (SGTDR) based image decomposition for more complementary information preservation and higher robustness for the model. Next, the fusion of the structure and the texture components is carried out using two generative adversarial networks (GANs) consisting of a generator and two discriminators to get fused structure and texture components. The loss function for each GAN is framed as per the characteristic of the component being fused to minimize the loss of complementary information. The fused image is reconstructed and undergoes adaptive mask-based structure enhancement to further boost its contrast and visualization. Substantial experimentation is carried out on a wide variety of neurological images. Visual and qualitative results exhibit notable improvement in the fusion performance of the proposed method in comparison to the state-of-the-art fusion methods. © 2022 IEEE.","Deep learning; Diagnosis; Image enhancement; Image fusion; Medical applications; Medical imaging; Particle beams; Positron emission tomography; Textures; Computed tomography; Image fusion methods; Medical image fusion; Multi-modal; Network-based; Source images; Structure-preserving; Structure-texture decomposition; Structure/texture; Texture preserving; Generative adversarial networks","Computed tomography; generative adversarial networks; image fusion; positron emission tomography; single-photon emission computed tomography; structure-texture decomposition","Conference paper","Final","","Scopus","2-s2.0-85134895297"
"Su W.; Huang Y.; Li Q.; Zuo F.; Liu L.","Su, Weijian (57712997800); Huang, Yongdong (57015759900); Li, Qiufu (55429126700); Zuo, Fengyuan (57713415100); Liu, Lijun (55715183600)","57712997800; 57015759900; 55429126700; 57713415100; 55715183600","Infrared and Visible Image Fusion Based on Adversarial Feature Extraction and Stable Image Reconstruction","2022","IEEE Transactions on Instrumentation and Measurement","71","","2510214","","","","10.1109/TIM.2022.3177717","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130850917&doi=10.1109%2fTIM.2022.3177717&partnerID=40&md5=2a14c6ba54942f869a5e35bd068e168a","Due to the discrepancy between training and test data, autoencoder-based image fusion methods trained by natural images easily lose vital information of infrared and visible images. Generative adversarial network (GAN)-based methods usually conduct their adversarial learning in image domain, which are difficult to optimize and ultimately affect the image fusion performance. To address these problems, this article proposes an autoencoder-based network, extraction-and-reconstruction network (ERNet), to fuse infrared and visible images. To weaken the discrepancy problem, ERNet's encoder is trained using infrared and visible images. To stably train the encoder, we conduct adversarial learning in the feature domain of infrared and visible images, which makes the well-trained encoder that could efficiently extract vital features from them. ERNet's decoder is trained using natural images with a supervised mode, which could reconstruct the final fused image from the vital features extracted by the encoder. The encoder and decoder are alternately trained to further weaken the discrepancy problem. The experimental results show that ERNet can effectively extract vital features of infrared and visible images and obtain higher quality fused images. Our fusion results can be found at https://github.com/suweijian1996/ERNet. © 1963-2012 IEEE.","Decoding; Extraction; Feature extraction; Image fusion; Image reconstruction; Infrared imaging; Signal encoding; Auto encoders; Features extraction; Generative adversarial network; Generator; Images reconstruction; Infrared and visible image; Reconstruction networks; Two-stage training; Visible image; Generative adversarial networks","Feature extraction; generative adversarial network (GAN); image fusion; infrared image; two-stage training; visible image","Article","Final","","Scopus","2-s2.0-85130850917"
"Xiao G.; Zhang L.","Xiao, Guangyi (57328144900); Zhang, Long (57222227721)","57328144900; 57222227721","Super-resolved synthetic aperture radar image reconstruction based on multiresolution fusion discrimination","2022","Journal of Electronic Imaging","31","4","043036","","","","10.1117/1.JEI.31.4.043036","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142247391&doi=10.1117%2f1.JEI.31.4.043036&partnerID=40&md5=dfa7f2de212cf7f1070bdf169a0185bd","Generative adversarial networks (GANs) are utilized for synthetic aperture radar (SAR) image super-resolution reconstruction, affording realistic texture details. However, existing GANs only discriminate the final generated high-resolution (HR) image after two consecutive upsampling processes, which ignore some high-frequency information of the reconstructed images. To resolve this issue, a multiresolution fusion discrimination (MRFD) algorithm is proposed to discriminate the reconstructed feature maps after each upsampling. First, a multiresolution discrimination process discriminates the authenticity of each upsampled feature map separately, which reduces the image distortion imposed during two consecutive upsampling processes. Besides, multiresolution feature fusion further preserves the consistent high-frequency texture structures. Finally, a multiscale dense network extracts image features in different scales, with multiscale dense block's dense connections improving parameter utilization. The experimental results on a SAR dataset demonstrate that the proposed MRFD algorithm performs better in reconstructing the texture details of HR images.  © 2022 SPIE and IS&T.","Image enhancement; Image fusion; Image reconstruction; Image texture; Optical resolving power; Radar imaging; Signal sampling; Synthetic aperture radar; Textures; Discrimination algorithms; Feature map; High-resolution images; Image super-resolution reconstruction; Images reconstruction; Multiresolution fusion; Multiresolution fusion discrimination; Superresolution; Synthetic aperture radar images; Upsampling; Generative adversarial networks","generative adversarial network; multiresolution fusion discrimination; super-resolution; synthetic aperture radar","Article","Final","","Scopus","2-s2.0-85142247391"
"Li Z.; Cui G.; Zhao J.; Xiang Q.; He B.","Li, Zihan (57410796300); Cui, Guangmang (55948628100); Zhao, Jufeng (35202542800); Xiang, Qinlei (57849305600); He, Bintao (57484241300)","57410796300; 55948628100; 35202542800; 57849305600; 57484241300","Joint strong edge and multi-stream adaptive fusion network for non-uniform image deblurring","2022","Journal of Visual Communication and Image Representation","89","","103663","","","","10.1016/j.jvcir.2022.103663","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140094520&doi=10.1016%2fj.jvcir.2022.103663&partnerID=40&md5=5b3b33f17795fcd89c7bf24ac0cc2fff","Non-uniform motion deblurring has been a challenging problem in the field of computer vision. Currently, deep learning-based deblurring methods have made promising achievements. In this paper, we propose a new joint strong edge and multi-stream adaptive fusion network to achieve non-uniform motion deblurring. The edge map and the blurred map are jointly used as network inputs and Edge Extraction Network (EEN) guides the Deblurring Network (DN) for image recovery and to complement the important edge information. The Multi-stream Adaptive Fusion Module (MAFM) adaptively fuses the edge information and features from the encoder and decoder to reduce feature redundancy to avoid image artifacts. Furthermore, the Dense Attention Feature Extraction Module (DAFEM) is designed to focus on the severely blurred regions of blurry images to obtain important recovery information. In addition, an edge loss function is added to measure the difference of edge features between the generated and clear images to further recover the edges of the deblurred images. Experiments show that our method outperforms currently public methods in terms of PSNR, SSIM and VIF, and generates images with less blur and sharper edges. © 2022 Elsevier Inc.","Computer system recovery; Deep learning; Extraction; Image enhancement; Image fusion; Adaptive fusion; Attention mechanisms; Deblurring; Edge extraction; Edge extraction algorithm; Extraction algorithms; Motion deblurring; Multi-stream; Non-uniform motion deblurring; Non-uniform motions; Generative adversarial networks","Attention mechanisms; Edge extraction algorithm; Generative adversarial network; Non-uniform motion deblurring","Article","Final","","Scopus","2-s2.0-85140094520"
"Luo X.; Tong X.; Hu Z.","Luo, Xin (56316646000); Tong, Xiaohua (55500134600); Hu, Zhongwen (55630272400)","56316646000; 55500134600; 55630272400","Improving Satellite Image Fusion via Generative Adversarial Training","2021","IEEE Transactions on Geoscience and Remote Sensing","59","8","9212572","6969","6982","13","10.1109/TGRS.2020.3025821","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111155134&doi=10.1109%2fTGRS.2020.3025821&partnerID=40&md5=52efcfa7eaddf11c1f7bf0d58f09c968","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications. © 1980-2012 IEEE.","Geometrical optics; Image fusion; Remote sensing; Satellites; Adversarial networks; Discriminative networks; Multiresolution images; Operational land imager; Remote sensing applications; Remote sensing images; Spatial informations; Spectral information; artificial neural network; optical method; satellite data; Image enhancement","Deep learning; generative adversarial networks (GANs); Landsat 8; remote sensing image fusion; residual dense blocks; Sentinel-2","Article","Final","","Scopus","2-s2.0-85111155134"
"Song J.; Zheng Y.; Xu C.; Zou Z.; Ding G.; Huang W.","Song, Jingqi (57223291489); Zheng, Yuanjie (23471581800); Xu, Chenxi (57223297795); Zou, Zhenxing (57685019500); Ding, Guocheng (57223297405); Huang, Wenhui (56746678300)","57223291489; 23471581800; 57223297795; 57685019500; 57223297405; 56746678300","Improving the classification ability of network utilizing fusion technique in contrast-enhanced spectral mammography","2022","Medical Physics","49","2","","966","977","11","10.1002/mp.15390","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121372308&doi=10.1002%2fmp.15390&partnerID=40&md5=138710c861ac3d12790e0a03d17030d8","Purpose: Contrast-enhanced spectral mammography (CESM) is an effective tool for diagnosing breast cancer with the benefit of its multiple types of images. However, few models simultaneously utilize this feature in deep learning-based breast cancer classification methods. To combine multiple features of CESM and thus aid physicians in making accurate diagnoses, we propose a hybrid approach by taking advantages of both fusion and classification models. Methods: We evaluated the proposed method on a CESM dataset obtained from 95 patients between ages ranging from 21 to 74 years, with a total of 760 images. The framework consists of two main parts: a generative adversarial network based image fusion module and a Res2Net-based classification module. The aim of the fusion module is to generate a fused image that combines the characteristics of dual-energy subtracted (DES) and low-energy (LE) images, and the classification module is developed to classify the fused image into benign or malignant. Results: Based on the experimental results, the fused images contained complementary information of the images of both types (DES and LE), whereas the model for classification achieved accurate classification results. In terms of qualitative indicators, the entropy of the fused images was 2.63, and the classification model achieved an accuracy of 94.784%, precision of 95.016%, recall of 95.912%, specificity of 0.945, F1_score of 0.955, and area under curve of 0.947 on the test dataset, respectively. Conclusions: We conducted extensive comparative experiments and analyses on our in-house dataset, and demonstrated that our method produces promising results in the fusion of CESM images and is more accurate than the state-of-the-art methods in classification of fused CESM. © 2021 American Association of Physicists in Medicine.","Adult; Aged; Breast; Breast Neoplasms; Contrast Media; Female; Humans; Mammography; Middle Aged; Young Adult; Classification (of information); Deep learning; Diagnosis; Diseases; Generative adversarial networks; Image enhancement; Image fusion; Medical imaging; Statistical tests; contrast medium; Classification models; Contrast-enhanced; Contrast-enhanced spectral mammography image; Deep learning; Dual-energy; Fused images; Fusion modules; Images classification; Lower energies; Mammography images; adult; aged; area under the curve; article; contrast enhancement; controlled study; deep learning; entropy; female; human; major clinical study; male; mammography; recall; breast; breast tumor; diagnostic imaging; mammography; middle aged; young adult; Image classification","contrast-enhanced spectral mammography image; deep learning; generative adversarial network; image classification; image fusion","Article","Final","","Scopus","2-s2.0-85121372308"
"Wang Z.; Shao W.; Yang F.; Chen Y.","Wang, Zhishe (36139853000); Shao, Wenyu (57406972500); Yang, Fengbao (8876353000); Chen, Yanlin (57573884600)","36139853000; 57406972500; 8876353000; 57573884600","Infrared and Visible Image Fusion Method via Interactive Attention-based Generative Adversarial Network; [红外与可见光图像交互注意力生成对抗融合方法]","2022","Guangzi Xuebao/Acta Photonica Sinica","51","4","","310","320","10","10.3788/gzxb20225104.0410002","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129465496&doi=10.3788%2fgzxb20225104.0410002&partnerID=40&md5=004f89b61e59b7973f28ef32fe979c42","Infrared sensors can capture prominent target characteristics by thermal radiation imaging, however the obtained infrared images usually lack structural features and texture details. On the contrary, visible sensors can obtain rich scene information by light reflection imaging, the obtained visible images have high spatial resolution and rich texture details, but cannot effectively perceive target characteristics, especially in low illumination environmental conditions. Infrared and visible image fusion aims to integrate the advantages of the two types of sensors to generate a composite image with better target perception and superior scene representation, which is widely applied for object tracking, object detection and pedestrian re-recognition. The existing generative adversarial network-based fusion methods only make use of convolution operation to extract local features, but do not consider their long-range dependence, which is easy to cause the fusion imbalance, resulting in the fusion image cannot retain typical targets of infrared image and texture details of visible image at the same time. To this end, an end-to-end infrared and visible image fusion method via interactive attention-based generative adversarial network is proposed. Firstly, in the generative network model, we adopt a dual-path encoder architecture with weight parameters sharing to extract the respective multi-scale deep features of source images, where the first normal convolution layer is used to extract low-level features, and two multi-scale aggregation convolution models are adopted to extract high-level features. By aggregating multiple available receptive fields, our multi-scale dual-path encoder network can efficiently extract more meaningful information for fusion tasks without down-sample or up-sample operations. Secondly, in the fusion layer, we design an interactive attention fusion model, which is cascading channel and spatial attention models, to establish the global dependence of their local features from the channel and spatial dimensions. The obtained attention maps can refine multi-scale feature maps to more focus on typical infrared targets and visible texture details, so that the fused results achieve better visual results. Finally, in the adversarial network model, we propose two discriminators, such as Discriminator-IR and Discriminator-VIS, to balance the truth-falsity between fusion image and source images. Besides, we introduce the mutually-compensated loss function to supervise the entire network, which can gradually optimize the generative network model to obtain the best fused result. In the ablation study and verified experiments, the TNO and Roadscene datasets and eight evaluation metrics are proposed to demonstrate the effectiveness and superiority of the proposed method. The ablation experimental results of the interactive attention fusion model indicate that our model can effectively establish the global dependency of local features compared with other four models, and further improve infrared and visible image fusion performance. In addition, compared with other nine the state-of-the-art fusion methods, such as WLS, DenseFuse, IFCNN, SEDRFuse, U2Fusion, PMGI, FusionGAN, GANMcC and RFN-Nest, the proposed method can achieve more balanced fusion results in retaining the typical targets of infrared image and rich texture details of visible image, and has a better visual effect, which is more suitable for the human visual system. Meanwhile, from a multi-index evaluation perspective, the proposed method has better image fusion performance, higher computational efficiency and stronger robustness than other state-of-the-art fusion methods. © 2022, Science Press. All right reserved.","Convolution; Deep learning; Generative adversarial networks; Image fusion; Image texture; Infrared detectors; Infrared imaging; Infrared radiation; Light reflection; Object detection; Deep learning; Fusion methods; Image fusion methods; Infrared and visible image; Interactive attention; Local feature; Multi-scales; Network models; Target characteristic; Visible image; Textures","Deep learning; Generative adversarial network; Image fusion; Infrared image; Interactive attention; Visible image","Article","Final","","Scopus","2-s2.0-85129465496"
"Wang Z.; Zhang Z.; Jiang J.","Wang, Zhen (56290448700); Zhang, Zhen (57545540500); Jiang, Jianhui (55731837900)","56290448700; 57545540500; 55731837900","Multi-Feature Fusion based Image Steganography using GAN","2021","Proceedings - 2021 IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2021","","","","280","281","1","10.1109/ISSREW53611.2021.00079","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126989482&doi=10.1109%2fISSREW53611.2021.00079&partnerID=40&md5=1fee3f49ab8a6f0a9bcd4486f0c17ecb","In order to solve the problem of information loss, some image steganography methods utilize generative adversarial networks (GANs), while the existing methods can not capture both texture information and semantic features. In this paper, a more accurate image steganography method is proposed, where a multi-level feature fusion procedure based on GAN is designed. Firstly, convolution and pooling operations are added to the network for feature extraction. Then, short links are used to fuse multi-level feature information. Finally, the stego image is generated by confrontation learning between discriminator and generator. Experimental results show that the proposed method has higher steganalysis security under the detection of high-dimensional feature steganalysis and neural network steganalysis. Comprehensive experiments show that the performance of the proposed method is better than ASDL-GAN and UT-GAN.  © 2021 IEEE.","Feature extraction; Image fusion; Semantics; Steganography; Textures; Features extraction; Features fusions; Image steganography; Information feature; Information loss; Multi-feature fusion; Multilevels; Semantic features; Steganalysis; Texture information; Generative adversarial networks","generative adversarial network; image steganography; multi-feature fusion","Conference paper","Final","","Scopus","2-s2.0-85126989482"
"Tang L.; Hui Y.; Yang H.; Zhao Y.; Tian C.","Tang, Lu (57788681700); Hui, Yu (57858798600); Yang, Hang (57859520400); Zhao, Yinghong (57858798700); Tian, Chuangeng (57191574413)","57788681700; 57858798600; 57859520400; 57858798700; 57191574413","Medical image fusion quality assessment based on conditional generative adversarial network","2022","Frontiers in Neuroscience","16","","986153","","","","10.3389/fnins.2022.986153","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136544963&doi=10.3389%2ffnins.2022.986153&partnerID=40&md5=013ef8a415fbb3f06c443178d260decd","Multimodal medical image fusion (MMIF) has been proven to effectively improve the efficiency of disease diagnosis and treatment. However, few works have explored dedicated evaluation methods for MMIF. This paper proposes a novel quality assessment method for MMIF based on the conditional generative adversarial networks. First, with the mean opinion scores (MOS) as the guiding condition, the feature information of the two source images is extracted separately through the dual channel encoder-decoder. The features of different levels in the encoder-decoder are hierarchically input into the self-attention feature block, which is a fusion strategy for self-identifying favorable features. Then, the discriminator is used to improve the fusion objective of the generator. Finally, we calculate the structural similarity index between the fake image and the true image, and the MOS corresponding to the maximum result will be used as the final assessment result of the fused image quality. Based on the established MMIF database, the proposed method achieves the state-of-the-art performance among the comparison methods, with excellent agreement with subjective evaluations, indicating that the method is effective in the quality assessment of medical fusion images. Copyright © 2022 Tang, Hui, Yang, Zhao and Tian.","accuracy; algorithm; Article; attention; human; image quality; intermethod comparison; multimodal medical image fusion; nerve cell network; quality control","attention mechanism; conditional; generative adversarial networks; image quality assessment; medical image fusion","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85136544963"
"Li H.; Zhuang P.","Li, Hanyu (57218313141); Zhuang, Peixian (56435653500)","57218313141; 56435653500","DewaterNet: A fusion adversarial real underwater image enhancement network","2021","Signal Processing: Image Communication","95","","116248","","","","10.1016/j.image.2021.116248","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104359719&doi=10.1016%2fj.image.2021.116248&partnerID=40&md5=053fa08bc8cfe26d406232d1d5b8b7b2","Underwater image enhancement algorithms have attracted much attention in underwater vision task. However, these algorithms are mainly evaluated on different datasets and metrics. In this paper, we utilize an effective and public underwater benchmark dataset including diverse underwater degradation scenes to enlarge the test scale and propose a fusion adversarial network for enhancing real underwater images. Meanwhile, the multiple inputs and well-designed multi-term adversarial loss can not only introduce multiple input image features, but also balance the impact of multi-term loss functions. The proposed network tested on the benchmark dataset achieves better or comparable performance than the other state-of-the-art methods in terms of qualitative and quantitative evaluations. Moreover, the ablation study experimentally validates the contributions of each component and hyper-parameter setting of loss functions. © 2021 Elsevier B.V.","Benchmarking; Image fusion; Statistical tests; Adversarial networks; Benchmark datasets; Hyper-parameter; Loss functions; Multiple inputs; Quantitative evaluation; State-of-the-art methods; Underwater vision; Image enhancement","Benchmark dataset; Deep learning; Generative adversarial network; Real underwater image enhancement","Article","Final","","Scopus","2-s2.0-85104359719"
"Liu T.; Liu Y.; Xu W.; Pu Y.; Hao Y.; Zuo W.","Liu, Tong (55747719700); Liu, Yufeng (58070551600); Xu, Wenda (58070708600); Pu, Yuandong (58070708700); Hao, Yuqi (58070646700); Zuo, Wei (57213975099)","55747719700; 58070551600; 58070708600; 58070708700; 58070646700; 57213975099","HGGAN: Visible to Thermal Translation Generative Adversarial Network Guided by Heatmap","2022","Proceedings of 2022 IEEE International Conference on Unmanned Systems, ICUS 2022","","","","171","176","5","10.1109/ICUS55513.2022.9986689","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146491123&doi=10.1109%2fICUS55513.2022.9986689&partnerID=40&md5=888067202925588706aa5c8b44f6c428","The realization of multi-modal image fusion requires sufficient cross domain data. Translating the visible images is an effective method to obtain thermal-visible paired images from the visible image domain to the thermal image domain. The current translation methods have some disadvantages, such as unreasonable distribution of thermal radiation intensity, blurred edges, spatial distortion and feature loss. So they are not friendly to downstream tasks. Based on the generation and reconstruction strategy of CycleGAN, we propose an image to image translation network guided by heatmap which is called HGGAN. We use the heatmap of object that detected by network to encode the heatmap code, and combine the image edge code to improve the image generation performance. We test the image standard of the generated image, and use the object detection network to verify. © 2022 IEEE.","Codes (symbols); Computer vision; Image enhancement; Image fusion; Object detection; Object recognition; 'current; Cross-domain; Heatmaps; Image domain; Image translation; Multimodal images; Thermal; Thermal images; Translation method; Visible image; Generative adversarial networks","cross domain; generative adversarial network; heatmap; image translation; thermal image","Conference paper","Final","","Scopus","2-s2.0-85146491123"
"Huang Y.; Yuan F.; Xiao F.; Cheng E.","Huang, Yifan (57204684403); Yuan, Fei (35729820400); Xiao, Fengqi (57217597007); Cheng, En (57725057400)","57204684403; 35729820400; 57217597007; 57725057400","Underwater image enhancement based on color restoration and dual image wavelet fusion","2022","Signal Processing: Image Communication","107","","116797","","","","10.1016/j.image.2022.116797","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133286202&doi=10.1016%2fj.image.2022.116797&partnerID=40&md5=233798fa2ec63e98e4b28c6c25bd215c","Due to the severe light absorption and scattering, underwater images often exhibit problems such as low contrast, detail blurring, color attenuation, and low illumination. To address these issues, this paper presents a two-step strategy based on color restoration and image fusion by combining deep learning and conventional image enhancement technologies to improve the visual performance of underwater images. First, an adaptive color compensation method is proposed to make up for the loss of severely attenuated channels. Color restoration is further implemented to estimate the illuminant color cast caused by the selective attenuation of light. Since the underwater image after color restoration still suffers from scattering and blurring, an effective method based on dual image wavelet fusion (DIWF) and Generative Adversarial Network (GAN) is designed to further enhance the edge details and improve the contrast of the color restored image. Experiments demonstrate that the proposed method significantly outperforms several state-of-the-arts in both qualitative and quantitative qualities. The approach can achieve better performance of color restoration, blur removal, and low illumination enhancement. © 2022 Elsevier B.V.","Color; Deep learning; Generative adversarial networks; Image fusion; Image reconstruction; Light absorption; Restoration; Absorption and scatterings; Color compensations; Color restoration; Dual image; Image wavelet fusion; Low contrast; Low illuminations; Underwater image; Underwater image enhancements; Wavelet fusion; Image enhancement","Color compensation; Color restoration; Generative adversarial network; Image enhancement; Image wavelet fusion; Underwater image","Article","Final","","Scopus","2-s2.0-85133286202"
"Liu S.; Yang L.","Liu, Shangwang (56923956400); Yang, Lihan (58030040600)","56923956400; 58030040600","BPDGAN: A GAN-Based Unsupervised Back Project Dense Network for Multi-Modal Medical Image Fusion","2022","Entropy","24","12","1823","","","","10.3390/e24121823","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144658021&doi=10.3390%2fe24121823&partnerID=40&md5=06eb2ad3790c48a568c9daf31ef6d173","Single-modality medical images often cannot contain sufficient valid information to meet the information requirements of clinical diagnosis. The diagnostic efficiency is always limited by observing multiple images at the same time. Image fusion is a technique that combines functional modalities such as positron emission computed tomography (PET) and single-photon emission computed tomography (SPECT) with anatomical modalities such as computed tomography (CT) and magnetic resonance imaging (MRI) to supplement the complementary information. Meanwhile, fusing two anatomical images (like CT-MRI) is often required to replace single MRI, and the fused images can improve the efficiency and accuracy of clinical diagnosis. To this end, in order to achieve high-quality, high-resolution and rich-detail fusion without artificial prior, an unsupervised deep learning image fusion framework is proposed in this paper. It is named the back project dense generative adversarial network (BPDGAN) framework. In particular, we construct a novel network based on the back project dense block (BPDB) and convolutional block attention module (CBAM). The BPDB can effectively mitigate the impact of black backgrounds on image content. Conversely, the CBAM improves the performance of BPDGAN on the texture and edge information. To conclude, qualitative and quantitative experiments are tested to demonstrate the superiority of BPDGAN. In terms of quantitative metrics, BPDGAN outperforms the state-of-the-art comparisons by approximately 19.58%, 14.84%, 10.40% and 86.78% on AG, EI, Qabf and Qcv metrics, respectively. © 2022 by the authors.","","convolutional block attention; dense residual network; GAN; multi-modal medical image fusion; unsupervised learning","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85144658021"
"Gao Y.; Ma S.; Liu J.","Gao, Yuan (57220187209); Ma, Shiwei (12762283200); Liu, Jingjing (55806165000)","57220187209; 12762283200; 55806165000","DCDR-GAN: A Densely Connected Disentangled Representation Generative Adversarial Network for Infrared and Visible Image Fusion","2023","IEEE Transactions on Circuits and Systems for Video Technology","33","2","","549","561","12","10.1109/TCSVT.2022.3206807","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139394123&doi=10.1109%2fTCSVT.2022.3206807&partnerID=40&md5=588a3b006ce630be0a69e853b41c116f","This paper proposes a new infrared and visible image fusion method based on the densely connected disentangled representation generative adversarial network (DCDR-GAN), which strips the content and the modal features of infrared and visible images through disentangled representation (DR) and fuses them separately. To deal with the mutually exclusive features in infrared and visible images, inject the modal features into the reconstruction of content features through adaptive instance normalization (AdaIN), reducing the interference. To reduce feature loss and ensure the expression of all-level features in the fused image, DCDR-GAN designs the densely connected content encoders and fusion decoder and constructs the multi-scale fusion structures between the enc-dec through long connections. Meanwhile, the content and the modal reconstruction losses are proposed to preserve the information of the source images. Finally, through the two-phase trained model, generate the fused image. The subjective and objective evaluation results of the TNO and INO datasets show that the proposed method has better visual effects and higher index values than other state-of-the-art methods.  © 1991-2012 IEEE.","Computer vision; Decoding; Image fusion; Image reconstruction; Image representation; Network coding; Cycle reconstruction; Decoding; Dense connection; Disentangled representation; Encodings; Features extraction; Generator; Images reconstruction; Infrared and visible image; Infrared and visible image fusion; Generative adversarial networks","Cycle reconstruction; dense connection; disentangled representation; generative adversarial network; infrared; visible image fusion","Article","Final","","Scopus","2-s2.0-85139394123"
"Mao Y.; Chen C.; Wang Z.; Cheng D.; You P.; Huang X.; Zhang B.; Zhao F.","Mao, Yanyan (55459044100); Chen, Chao (57203146321); Wang, Zhenjie (57984564100); Cheng, Dapeng (7402806243); You, Panlu (57893438100); Huang, Xingdan (57670280300); Zhang, Baosheng (57226775724); Zhao, Feng (57190684473)","55459044100; 57203146321; 57984564100; 7402806243; 57893438100; 57670280300; 57226775724; 57190684473","Generative adversarial networks with adaptive normalization for synthesizing T2-weighted magnetic resonance images from diffusion-weighted images","2022","Frontiers in Neuroscience","16","","1058487","","","","10.3389/fnins.2022.1058487","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142833598&doi=10.3389%2ffnins.2022.1058487&partnerID=40&md5=f7b5972b9cb9c2a028dcf3fd8b730a27","Recently, attention has been drawn toward brain imaging technology in the medical field, among which MRI plays a vital role in clinical diagnosis and lesion analysis of brain diseases. Different sequences of MR images provide more comprehensive information and help doctors to make accurate clinical diagnoses. However, their costs are particularly high. For many image-to-image synthesis methods in the medical field, supervised learning-based methods require labeled datasets, which are often difficult to obtain. Therefore, we propose an unsupervised learning-based generative adversarial network with adaptive normalization (AN-GAN) for synthesizing T2-weighted MR images from rapidly scanned diffusion-weighted imaging (DWI) MR images. In contrast to the existing methods, deep semantic information is extracted from the high-frequency information of original sequence images, which are then added to the feature map in deconvolution layers as a modality mask vector. This image fusion operation results in better feature maps and guides the training of GANs. Furthermore, to better preserve semantic information against common normalization layers, we introduce AN, a conditional normalization layer that modulates the activations using the fused feature map. Experimental results show that our method of synthesizing T2 images has a better perceptual quality and better detail than the other state-of-the-art methods. Copyright © 2022 Mao, Chen, Wang, Cheng, You, Huang, Zhang and Zhao.","Article; clinical effectiveness; diffusion weighted imaging; feasibility study; human; image analysis; image display; image processing; image quality; intermethod comparison; mathematical model; neuroimaging; T2 weighted imaging","adaptive normalization; generative adversarial network (GAN); image fusion; images synthesis; magnetic resonance imaging (MRI)","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85142833598"
"Dong W.; Yang Y.; Qu J.; Xie W.; Li Y.","Dong, Wenqian (57196087026); Yang, Yufei (57224191949); Qu, Jiahui (57196097630); Xie, Weiying (56768656200); Li, Yunsong (55986546100)","57196087026; 57224191949; 57196097630; 56768656200; 55986546100","Fusion of Hyperspectral and Panchromatic Images Using Generative Adversarial Network and Image Segmentation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3078711","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107207029&doi=10.1109%2fTGRS.2021.3078711&partnerID=40&md5=ced5e81bab36ff953807405583b9d563","Hyperspectral (HS) image fusion aims at integrating a panchromatic (PAN) image and an HS image, featuring the fused image with the spatial quality of the former and the spectral diversity of the latter. The classic fusion algorithm generally includes three consecutive procedures that are upsampling, detail extraction, and detail injection. In this article, we propose an HS and PAN image fusion method based on generative adversarial network and local estimation of injection gain. Instead of upsampling the HS image by classical interpolation techniques, a generative adversarial super-resolution network (GASN) is designed to obtain the interpolated HS image in the fusion framework. GASN establishes a spectral-information-based discriminator to conduct adversarial learning with the generator, so as to preserve the spectral information of the low-resolution HS image. An image segmentation-based injection gain estimation (ISGE) algorithm is subsequently proposed for HS and PAN images fusion. The injection gain is estimated over image segments obtained by a binary partition tree approach to improve the fusion performance. The proposed GASN and ISGE are implemented into two credible global estimation pansharpening methods, and experimental results prove the performance improvement of the proposed method. The proposed method is also compared with existing state-of-the-art methods, and experiments on several public databases demonstrate that the proposed method is competitive or superior to the state-of-the-art fusion methods.  © 1980-2012 IEEE.","Binary trees; Image enhancement; Image segmentation; Object recognition; Signal sampling; Adversarial learning; Adversarial networks; Binary partition Tree; Image fusion methods; Interpolation techniques; Panchromatic (Pan) image; Spectral information; State-of-the-art methods; artificial neural network; detection method; image analysis; satellite imagery; segmentation; Image fusion","Details injection; hyperspectral (HS) fusion; image segment; injection gains","Article","Final","","Scopus","2-s2.0-85107207029"
"Zhong Z.; Yang J.","Zhong, Zhen (56424760300); Yang, Jinfeng (57855575200)","56424760300; 57855575200","A novel pig-body multi-feature representation method based on multi-source image fusion","2022","Measurement: Journal of the International Measurement Confederation","204","","111968","","","","10.1016/j.measurement.2022.111968","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140997207&doi=10.1016%2fj.measurement.2022.111968&partnerID=40&md5=fbebb17a82c3fd321a348d13bb890977","The detection of pig-body health has been a hot topic since it is closely related to the safety of agricultural products quality in recent years. In this paper, shape and temperature feature are viewed as health representation, and the multi-source image fusion method should be used to obtain pig-body multi-feature. However, traditional infrared and visible fusion algorithm could not represent more thermal radiation and texture information of fused images. To more usefully represent pig-body mulit-feature, a novel multi-source image fusion method is proposed in view of Generative Adversarial Network (GAN), named as MGANFuse. Firstly, multi-source images are fused by modified GAN fusion framework. Then, based on fused images, pig-body shape feature is represented by automatic threshold of Otsu segmentation and morphology method. Finally, maximum temperature feature is obtained based on pig-body shape representation. Experimental results reveal that representation method in view of proposed fusion model is capable of realizing 1.551–3.876% higher average accuracy rate than more recently published algorithms. Moreover, it lays the foundation for effective representation of pig-body multi-feature. © 2022","Agricultural products; Generative adversarial networks; Image segmentation; Mammals; Textures; Body shape features; Body temperature; Fused images; Image fusion methods; Multi-source image fusion; Multi-source images; Multifeatures; Pig-body shape feature; Pig-body temperature feature; Representation method; Image fusion","Generative Adversarial Network; Multi-source image fusion; Pig-body shape feature; Pig-body temperature feature","Article","Final","","Scopus","2-s2.0-85140997207"
"Xia H.; Wu B.; Tan Y.; Tang X.; Song S.","Xia, Haiying (24538087700); Wu, Bo (57713395400); Tan, Yumei (57222903588); Tang, Xiaohu (57220544467); Song, Shuxiang (55469538200)","24538087700; 57713395400; 57222903588; 57220544467; 55469538200","MFC-Net: Multi-scale fusion coding network for Image Deblurring","2022","Applied Intelligence","52","11","","13232","13249","17","10.1007/s10489-021-02993-0","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125278629&doi=10.1007%2fs10489-021-02993-0&partnerID=40&md5=8168e16b3fa263d7e67fbaa8e26acaa6","The existing image blind deblurring methods mostly adopt the “coarse-to-fine” scheme, which always require a mass of parameters and can not mine the blur information effectively. To tackle the above problems, we design a lightweight multi-scale fusion coding deblurring network (MFC-Net). Specifically, we fuse the multi-resolution features in a single-scale deblurring framework based on Wasserstein generative adversarial network (WGAN). Then we propose a feature fusion module to replace the addition operation in each scale in the skip connection of the encoder-decoder. Besides, we propose a regional attention module to alleviate the inconsistency in non-uniform blurry images and excavate its intrinsic blurry features simultaneously. Plenty of experimental results show that our proposed deblurring model is simple, fast yet robust for image motion deblurring with real-time inference of 10 ms per 720p image, outperforming the state-of-the-art methods in terms of performance-complexity trade-off. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Economic and social effects; Image coding; Image enhancement; Image fusion; Network coding; Blind deblurring; Deblurring; Image deblurring; Image motion; Multi-scale fusion coding; Multiscale fusion; Regional attention module; Single image motion blind deblurring; Single images; Wasserstein generative adversarial network; Generative adversarial networks","Multi-scale fusion coding; Regional attention module; Single image motion blind deblurring; Wasserstein generative adversarial networks","Article","Final","","Scopus","2-s2.0-85125278629"
"Wang T.; Cheng J.; Liu T.; Zhao K.; Cheng B.; Liu Z.","Wang, Tao (57768940000); Cheng, Jianghua (36554022800); Liu, Tong (57202057306); Zhao, Kangcheng (57219339950); Cheng, Bang (57213873449); Liu, Zilong (57302991600)","57768940000; 36554022800; 57202057306; 57219339950; 57213873449; 57302991600","Visible light polarization image fusion based on dense connection generative adversarial network","2021","Journal of Physics: Conference Series","2035","1","012028","","","","10.1088/1742-6596/2035/1/012028","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117467766&doi=10.1088%2f1742-6596%2f2035%2f1%2f012028&partnerID=40&md5=b91f943b58abaefcc3870b6f86c0d81c","Polarized images have rich high-frequency information. The texture, contours and edges of objects in the image are very obvious, while the intensity images contains the main energy of the image and the background field is rich in information. Therefore, the polarization image is combined with Light intensity image fusion is of great significance. This article uses deep learning methods, based on the training method of generating adversarial networks, using densely connected generator networks, using SSIM loss and gradient loss functions, and experiments have shown that the ideal fusion effect can be achieved. © 2021 Institute of Physics Publishing. All rights reserved.","Deep learning; Generative adversarial networks; Light; Polarization; Textures; Adversarial networks; Background field; Energy; High-frequency informations; Intensity images; Learning methods; Light intensity; Polarization images; Training methods; Visible light; Image fusion","","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85117467766"
"Luo J.; Cui W.; Liu J.; Li Y.; Guo Y.; Xu S.; Wang L.","Luo, Jie (57737496600); Cui, Weigang (57194527865); Liu, Jingyu (57195733695); Li, Yang (56075073900); Guo, Yuzhu (55519791700); Xu, Song (57219056202); Wang, Lina (57738071000)","57737496600; 57194527865; 57195733695; 56075073900; 55519791700; 57219056202; 57738071000","Visual Image Decoding of Brain Activities using a Dual Attention Hierarchical Latent Generative Network with Multi-Scale Feature Fusion","2022","IEEE Transactions on Cognitive and Developmental Systems","","","","1","1","0","10.1109/TCDS.2022.3181469","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131797146&doi=10.1109%2fTCDS.2022.3181469&partnerID=40&md5=8615ef84d9719e1d2832ebaa66248330","Reconstructing visual stimulus from human brain activity measured with functional magnetic resonance imaging (fMRI) is a challenging decoding task for revealing the visual system. Recent deep learning approaches commonly neglect the relationship between hierarchical image features and different regions of the visual cortex, and fail to use global and local image features in reconstructing visual stimulus. To address these issues, in this paper, a novel neural decoding framework is proposed by using a dual attention hierarchical latent generative network with multi-scale feature fusion (DA-HLGN-MSFF) method. Specifically, the fMRI data is firstly encoded to hierarchical features of our image encoder network which employs a multi-kernel convolution block to extract the multi-scale spatial information of images. In order to reconstruct the perceived images and further improve the performance of our generator network, a dual attention block based on channel-spatial attention mechanism is then proposed to exploit the inter-channel relationships and spatial long-range dependencies of features. Moreover, a multi-scale feature fusion block is finally adopted to aggregate the global and local information of features at different scales and synthesize the final reconstructed images in the generator network. Competitive experimental results on two public fMRI datasets demonstrate that our method is able to achieve promising reconstructing performance compared with the state-of-the-art methods. The codes of our proposed DA-HLGN-MSFF method will be open access on https://github.com/ljbuaa/HLDAGN. IEEE","Brain; Decoding; Deep neural networks; Generative adversarial networks; Image enhancement; Image fusion; Image reconstruction; Neurophysiology; Brain activity; Features extraction; Functional magnetic resonance imaging; Functional magnetic resonance imaging decoding; Generator; Images reconstruction; Multi-scale features; Visual cortexes; Magnetic resonance imaging","Decoding; Deep neural network; Feature extraction; fMRI decoding; Functional magnetic resonance imaging; generative adversarial network; Generative adversarial networks; Generators; Image reconstruction; image reconstruction; visual cortex; Visualization","Article","Article in press","","Scopus","2-s2.0-85131797146"
"Fan Y.; Zhu C.","Fan, Yawen (57941310600); Zhu, Changming (56468407800)","57941310600; 56468407800","Super-resolution reconstruction of underwater sonar image based on multi-scale feature fusion","2022","ACM International Conference Proceeding Series","","","","898","901","3","10.1145/3548608.3559331","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140583270&doi=10.1145%2f3548608.3559331&partnerID=40&md5=c7a3b7ceb8f5db2978d3a7eca44a1a09","Because the underwater environment is too complex, underwater sonar images have many problems, such as low resolution, blurred details, speckle noise and so on. Therefore, it is urgent to improve the quality of underwater sonar images. To solve this problem, we propose the generative adversarial network based on octave convolution and channel attention mechanism (OCTGAN), which is a multi-scale feature fusion super-resolution reconstruction method suitable for underwater sonar images. OCTGAN consists of two parts, namely a generator and a discriminator. The generator is a network after the fusion of Octave convolution and channel attention mechanism. Octave convolution divides the feature map into a low-frequency part and a high-frequency part. The information of the feature map is updated and exchanged before fusion, and then recalibration is performed through the channel attention mechanism module. OCTGAN can learn more high-frequency details in the image and reconstruct super-resolution images with clearer texture details. Experimental results show that compared with the existing super-resolution methods, this method has better image reconstruction effects.  © 2022 ACM.","Generative adversarial networks; Image enhancement; Image fusion; Image reconstruction; Optical resolving power; Sonar; Textures; Underwater acoustics; Attention mechanisms; Channel attention mechanism; Feature map; Features fusions; Multi-scale features; Octave convolution; Sonar image; Super-resolution reconstruction; Superresolution; Underwater sonars; Convolution","channel attention mechanism; generative adversarial network; octave convolution; Super-resolution","Conference paper","Final","","Scopus","2-s2.0-85140583270"
"Yang Y.; Gao X.; Dang J.; Wang Y.","Yang, Yanchun (55246741300); Gao, Xiaoyu (57301442200); Dang, Jianwu (57193118886); Wang, Yangping (38062340200)","55246741300; 57301442200; 57193118886; 38062340200","Infrared and visible image fusion based on WEMD and generative adversarial network reconstruction; [基于WEMD和生成对抗网络重建的红外与可见光图像融合]","2022","Guangxue Jingmi Gongcheng/Optics and Precision Engineering","30","3","","320","330","10","10.37188/OPE.20223003.0320","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125674740&doi=10.37188%2fOPE.20223003.0320&partnerID=40&md5=1d95d9c5962ba850f81a14361477434f","To overcome the problem of blurred edges and low contrast in the fusion of infrared and visible images, a two-dimensional window empirical mode decomposition (WEMD) and infrared and visible light image fusion algorithm for GAN reconstruction was proposed. The infrared and visible light images were decomposed using WEMD to obtain the intrinsic mode function components (IMF) and residual components. The IMF components were fused through principal component analysis, and the residual components were fused by the weighted average. The preliminary fused image was reconstructed and input into the GAN to play against the visible light image, and some background information was supplemented to obtain the final fusion image. The average gradient (AG), edge strength (EI), entropy (EN), structural similarity (SSIM), and mutual information (MI) are used for objective evaluation, and they increased by 46.13%, 39.40%, 19.91%, 3.72%, and 33.10%, respectively, compared with the other five methods. The experimental results show that the proposed algorithm achieves better retention of the edge and texture details of the sources image while simultaneously highlighting the target of the infrared image, has better visibility, and has obvious advantages in terms of objective evaluation indicators. © 2022, Science Press. All right reserved.","Edge detection; Generative adversarial networks; Image reconstruction; Infrared imaging; Light; Principal component analysis; Textures; Empirical Mode Decomposition; Function components; Infrared and visible image; Infrared light; Intrinsic Mode functions; Network reconstruction; Objective evaluation; Residual components; Visible light images; Window empirical mode decomposition; Image fusion","Generative adversarial network; Image fusion; Infrared and visible image; Window empirical mode decomposition","Article","Final","","Scopus","2-s2.0-85125674740"
"Jin B.; Cruz L.; Goncalves N.","Jin, Bo (57218305846); Cruz, Leandro (40560960600); Goncalves, Nuno (7004440931)","57218305846; 40560960600; 7004440931","Pseudo RGB-D Face Recognition","2022","IEEE Sensors Journal","22","22","","21780","21794","14","10.1109/JSEN.2022.3197235","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136840437&doi=10.1109%2fJSEN.2022.3197235&partnerID=40&md5=f687717ca603a14fb1c2a473383a8880","In the last decade, advances and popularity of low-cost RGB-D sensors have enabled us to acquire depth information of objects. Consequently, researchers began to solve face recognition problems by capturing RGB-D face images using these sensors. Until now, it is not easy to acquire the depth of human faces because of limitations imposed by privacy policies, and RGB face images are still more common. Therefore, obtaining the depth map directly from the corresponding RGB image could be helpful to improve the performance of subsequent face processing tasks, such as face recognition. Intelligent creatures can use a large amount of experience to obtain 3D spatial information only from 2D plane scenes. It is machine learning methodology, which is to solve such problems, that can teach computers to generate correct answers by training. To replace the depth sensors by generated pseudo-depth maps, in this article, we propose a pseudo RGB-D face recognition framework and provide data-driven ways to generate the depth maps from 2D face images. Specially we design and implement a generative adversarial network model named 'D+GAN' to perform the multiconditional image-to-image translation with face attributes. By this means, we validate the pseudo RGB-D face recognition with experiments on various datasets. With the cooperation of image fusion technologies, especially non-subsampled shearlet transform (NSST), the accuracy of face recognition has been significantly improved. © 2001-2012 IEEE.","Generative adversarial networks; Image enhancement; Image fusion; Learning systems; Three dimensional displays; D+GAN; Depth Estimation; Depthmap; Face; Face images; Features extraction; Monocular face depth estimation; Pseudo depth; RGB-D; Three-dimensional display; Face recognition","Depth plus generative adversarial network (D+GAN); face recognition; monocular face depth estimation; pseudo-depth; RGB-D","Article","Final","","Scopus","2-s2.0-85136840437"
"Yu N.; Ma A.; Zhong Y.; Gong X.","Yu, Ning (57457393900); Ma, Ailong (55972916000); Zhong, Yanfei (12039673900); Gong, Xiaodong (57937220400)","57457393900; 55972916000; 12039673900; 57937220400","HFGAN: A Heterogeneous Fusion Generative Adversarial Network for Sar-to-Optical Image Translation","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","2864","2867","3","10.1109/IGARSS46834.2022.9883519","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140359276&doi=10.1109%2fIGARSS46834.2022.9883519&partnerID=40&md5=a349a82425091fd96f9ab0317f8e4fc2","Due to the influence of the imaging mechanism of SAR images, it is difficult to interpret ground information through SAR images without expert knowledge. On the contrary, optical images have rich spatial and color information, so it is necessary to conduct research on the translation of SAR to optical remote sensing images. In this end, we propose a heterogeneous fusion generative adversarial network (HFGAN) for SAR-to-optical image translation. There are two main improvements: (1) Complementary generation of global structure and texture information. A heterogeneous fusion generator and a multi-scale discriminator are proposed to ensure that the global and detailed features of the generated image are more accurate and rich. (2) Color fidelity. Chromatic aberration loss are introduced to reduce the color difference between the generated image and the real optical image. Through qualitative and quantitative experiments, it is proved that the proposed method not only obtains better visual effects, but also has certain progress in the evaluation metrics, which proves that the proposed method is superior to the previous advanced methods in SAR-to-optical image translation. © 2022 IEEE.","Aberrations; Color; Colorimetry; Generative adversarial networks; Geometrical optics; Image fusion; Optical remote sensing; Radar imaging; Textures; Color information; Expert knowledge; Image tranlation; Image translation; Imaging mechanism; Optical image; Optical remote sensing; Remote-sensing; SAR Images; Spatial informations; Synthetic aperture radar","Generative Adversarial Network; Image tranlation; Remote sensing","Conference paper","Final","","Scopus","2-s2.0-85140359276"
"Fan Z.; Guan N.; Wang Z.; Su L.; Wu J.; Sun Q.","Fan, Zunlin (57007548800); Guan, Naiyang (7005374484); Wang, Zhiyuan (57221407831); Su, Longfei (57207775875); Wu, Jiangang (56093322300); Sun, Qianchong (57219549272)","57007548800; 7005374484; 57221407831; 57207775875; 56093322300; 57219549272","Unified framework based on multiscale transform and feature learning for infrared and visible image fusion","2021","Optical Engineering","60","12","123102","","","","10.1117/1.OE.60.12.123102","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122707560&doi=10.1117%2f1.OE.60.12.123102&partnerID=40&md5=88aae1986b71999e7254fac354360ce0","Infrared image and visible image can provide the thermal radiation and optical information of objects respectively. Hence, fusing their complementary information for a scene is essential for both human vision and machine perception. The adaptability of image fusion by multiscale transform (MST) is weak because of the fixed fusion rule. Meanwhile, the image fusion algorithm by supervised learning is hard to realize for lack of large labeled data. Therefore, we propose a unified framework for infrared and visible-image fusion via MST and feature learning. The reasonable fusion rule is designed by learning the attention response via training a novel attention architecture built-in the attention-guided generative adversarial network, which does not need the fused result as the ground truth. The experiment results demonstrate the validity of feature learning and the superiority of the proposed algorithm against other state-of-the-art methods in image fusion task. And the proposed method is one of the effective ways to promote the development of deep learning in the field of insufficient labeled data. © 2021 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Generative adversarial networks; Infrared imaging; Learning algorithms; Feature learning; Fusion rule; Image translation; Infrared and visible image; Labeled data; Multi-scale features; Multiscale transforms; Unified framework; Unsupervised feature learning; Visible image; Image fusion","generative adversarial network; image fusion; image translation; unified framework; unsupervised feature learning","Article","Final","","Scopus","2-s2.0-85122707560"
"Mi J.; Wang L.; Liu Y.; Zhang J.","Mi, Jia (57223151845); Wang, LiFang (57142669800); Liu, Yang (57223167850); Zhang, Jiong (57223177352)","57223151845; 57142669800; 57223167850; 57223177352","KDE-GAN: A multimodal medical image-fusion model based on knowledge distillation and explainable AI modules","2022","Computers in Biology and Medicine","151","","106273","","","","10.1016/j.compbiomed.2022.106273","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141824653&doi=10.1016%2fj.compbiomed.2022.106273&partnerID=40&md5=d2cdc8735c4d6139b3b20aa3bbceba23","Background: As medical images contain sensitive patient information, finding a publicly accessible dataset with patient permission is challenging. Furthermore, few large-scale datasets suitable for training image-fusion models are available. To address this issue, we propose a medical image-fusion model based on knowledge distillation (KD) and an explainable AI module-based generative adversarial network with dual discriminators (KDE-GAN). Method: KD reduces the size of the datasets required for training by refining a complex image-fusion model into a simple model with the same feature-extraction capabilities as the complex model. The images generated by the explainable AI module show whether the discriminator can distinguish true images from false images. When the discriminator precisely judges the image based on the key features, the training can be stopped early, reducing overfitting and the amount of data required for training. Results: By training using only small-scale datasets, the trained KDE-GAN can generate clear fused images. KDE-GAN fusion results were evaluated quantitatively using five metrics: spatial frequency, structural similarity, edge information transfer factor, normalized mutual information, and nonlinear correlation information entropy. Conclusion: Experimental results show that the fused images generated by KDE-GAN are superior to state-of-the-art methods, both subjectively and objectively. © 2022","Artificial Intelligence; Benchmarking; Entropy; Humans; Image Processing, Computer-Assisted; Complex networks; Discriminators; Distillation; Image fusion; Large dataset; Medical imaging; Explainable AI module; Fused images; Generative adversarial network with dual discriminator; Image fusion model; Knowledge distillation; Medical image fusion; Model-based OPC; Multimodal medical images; Multimodal medical-image fusion; Patient information; artificial intelligence; benchmarking; entropy; human; image processing; Generative adversarial networks","Explainable AI module; Generative adversarial network with dual discriminators; Knowledge distillation; Multimodal medical-image fusion","Article","Final","","Scopus","2-s2.0-85141824653"
"Le Z.; Huang J.; Xu H.; Fan F.; Ma Y.; Mei X.; Ma J.","Le, Zhuliang (57215469419); Huang, Jun (56688687500); Xu, Han (57201056465); Fan, Fan (35795122500); Ma, Yong (56438173900); Mei, Xiaoguang (55800813300); Ma, Jiayi (26638975600)","57215469419; 56688687500; 57201056465; 35795122500; 56438173900; 55800813300; 26638975600","UIFGAN: An unsupervised continual-learning generative adversarial network for unified image fusion","2022","Information Fusion","88","","","305","318","13","10.1016/j.inffus.2022.07.013","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136145351&doi=10.1016%2fj.inffus.2022.07.013&partnerID=40&md5=369cf237f1b51ed57f2e11e3dec942fe","In this paper, we propose a novel unsupervised continual-learning generative adversarial network for unified image fusion, termed as UIFGAN. In our model, for multiple image fusion tasks, a generative adversarial network for training a single model with memory in a continual-learning manner is proposed, rather than training an individual model for each fusion task or jointly training multiple tasks. We use elastic weight consolidation to avoid forgetting what has been learned from previous tasks when training multiple tasks sequentially. In each task, the generation of the fused image comes from the adversarial learning between a generator and a discriminator. Meanwhile, a max-gradient loss function is adopted for forcing the fused image to obtain richer texture details of the corresponding regions in two source images, which applies to most typical image fusion tasks. Extensive experiments on multi-exposure, multi-modal and multi-focus image fusion tasks demonstrate the advantages of our method over the state-of-the-art approaches. © 2022 Elsevier B.V.","Image fusion; Learning systems; Textures; Adversarial learning; Continual learning; Fused images; Individual modeling; Loss functions; Max-gradient loss; Multiple image; Multiple tasks; Single models; Unified Modeling; Generative adversarial networks","Continual-learning; Generative adversarial network; Image fusion; Max-gradient loss; Unified model","Article","Final","","Scopus","2-s2.0-85136145351"
"Jiang M.; Shen H.; Li J.","Jiang, Menghui (57210173702); Shen, Huanfeng (8359721100); Li, Jie (57214207213)","57210173702; 8359721100; 57214207213","Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5410915","","","","10.1109/TGRS.2022.3188998","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135232679&doi=10.1109%2fTGRS.2022.3188998&partnerID=40&md5=8337c148bae6361ca03536bbf9d398c0","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal-spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.  © 1980-2012 IEEE.","Deep learning; Feature extraction; Feedback; Generative adversarial networks; Optical remote sensing; Radiometers; Satellite imagery; Cloud cover; Deep residual cycle generative adversarial network; Features extraction; Generator; Heterogeneous integrated framework; Integrated frameworks; Land-cover change; Remote-sensing; Spatial resolution; Thick cloud cover; cloud cover; integrated approach; Landsat; MODIS; network analysis; remote sensing; satellite imagery; Sentinel; spatiotemporal analysis; Image fusion","Deep residual cycle generative adversarial network (GAN); heterogeneous integrated framework; land-cover change; thick cloud cover","Article","Final","","Scopus","2-s2.0-85135232679"
"Yuan H.; Wang Y.; Xu G.; Wang F.","Yuan, Hongwu (12790399800); Wang, Yiqing (57572801200); Xu, Guoming (55546062900); Wang, Feng (56498041500)","12790399800; 57572801200; 55546062900; 56498041500","Research on super-resolution reconstruction of single-frame image of infrared focal plane polarization imaging","2022","Proceedings of SPIE - The International Society for Optical Engineering","12169","","121692H","","","","10.1117/12.2622890","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128065704&doi=10.1117%2f12.2622890&partnerID=40&md5=e245dec9dff1177825fa604fdff4149e","Since the low resolution of infrared focal plane arrays may degrade the performance of polarization imaging significantly, it is necessary to study the super-resolution reconstruction method for superior image resolution and contrast. Four typical single-frame image reconstruction methods are studied in this paper, and the comparison of reconstruction result of these methods is conducted by using subjective and objective evaluation. The experiments show that the reconstruction method based on generative adversarial network performs poorly in the evaluation indexes such as peak signal-to-noise ratio and structural similarity, but its reconstructed image has good visual effects, rich texture and details, and has a strong ability to suppress background noise, and using the reconstructed images for polarization information parsing can significantly improve the accuracy of polarization information parsing and the effect of polarization image fusion. © 2022 SPIE","Focal plane arrays; Focusing; Image enhancement; Image fusion; Image reconstruction; Image resolution; Infrared detectors; Polarization; Signal to noise ratio; Textures; Information parsing; Infrared focal plane polarization imaging; Infrared focal planes; Plane polarizations; Polarisation informations; Polarization image fusion; Polarization images; Polarization imaging; Polarization information parsing;; Super-resolution reconstruction; Generative adversarial networks","generative adversarial network; infrared focal plane polarization imaging; polarization information parsing; polarization image fusion; super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85128065704"
"Zhang R.; Bin J.; Liu Z.; Blasch E.","Zhang, Ran (57220076185); Bin, Junchi (57201112309); Liu, Zheng (57192441116); Blasch, Erik (7003503895)","57220076185; 57201112309; 57192441116; 7003503895","WGGAN: A wavelet-guided generative adversarial network for thermal image translation","2021","Generative Adversarial Networks for Image-to-Image Translation","","","","313","327","14","10.1016/B978-0-12-823519-5.00015-4","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126779200&doi=10.1016%2fB978-0-12-823519-5.00015-4&partnerID=40&md5=f0505e37465f28505d07106488b28a10","In modern industries, context enhancement is critical to capture objects, especially in a low illuminance environment. Deploying thermal or infrared (IR) sensors is straightforward to perceive the objects beyond the visible spectrum. Through IR and visual red-green-blue (RGB) image fusion, the images of thermal objects are enhanced for improved detection. Unfortunately, IR/RGB image fusion is limited to dark night, where the RGB camera is not capable of capturing the visual scene. Generative adversarial networks (GANs) have been implemented to convert IR images into RGB images for enriching semantic information. Nonetheless, contemporary methods are limited due to spatial distortion in translated results. Therefore, in this chapter, a wavelet-guided generative adversarial network (WGGAN) is proposed to address the problem. The proposed WGGAN model adopts the discrete wavelet transform to a variational autoencoder for preserving the structural information during image translation. The qualitative and quantitative analyses show considerable improvements with respect to the state-of-the-art GANs in image translation. © 2021 Elsevier Inc. All rights reserved.","","Context enhancement; Generative adversarial network; Image processing; Image translation; IR-to-RGB translation; Variational autoencoder","Book chapter","Final","","Scopus","2-s2.0-85126779200"
"Yang Y.; Liu S.; Xing B.; Li K.","Yang, You (51666215300); Liu, Sixun (57477807400); Xing, Bin (57221481659); Li, Kesen (57478064300)","51666215300; 57477807400; 57221481659; 57478064300","Face inpainting via Learnable Structure Knowledge of Fusion Network","2022","KSII Transactions on Internet and Information Systems","16","3","","877","893","16","10.3837/tiis.2022.03.007","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127838159&doi=10.3837%2ftiis.2022.03.007&partnerID=40&md5=4326535c8e3c6fc2057f9c2ec50cf7ca","With the development of deep learning, face inpainting has been significantly enhanced in the past few years. Although image inpainting framework integrated with generative adversarial network or attention mechanism enhanced the semantic understanding among facial components, the issues of reconstruction on corrupted regions are still worthy to explore, such as blurred edge structure, excessive smoothness, unreasonable semantic understanding and visual artifacts, etc. To address these issues, we propose a Learnable Structure Knowledge of Fusion Network (LSK-FNet), which learns a prior knowledge by edge generation network for image inpainting. The architecture involves two steps: Firstly, structure information obtained by edge generation network is used as the prior knowledge for face inpainting network. Secondly, both the generated prior knowledge and the incomplete image are fed into the face inpainting network together to get the fusion information. To improve the accuracy of inpainting, both of gated convolution and region normalization are applied in our proposed model. We evaluate our LSK-FNet qualitatively and quantitatively on the CelebA-HQ dataset. The experimental results demonstrate that the edge structure and details of facial images can be improved by using LSK-FNet. Our model surpasses the compared models on L1, PSNR and SSIM metrics. When the masked region is less than 20%, L1 loss reduce by more than 4.3%. Copyright © 2022 KSII","Deep learning; Generative adversarial networks; Image enhancement; Image fusion; Semantics; Edge structures; Face inpainting; Gated convolution; Image edge; Image Inpainting; Inpainting; Normalisation; Prior-knowledge; Region normalization; Semantics understanding; Convolution","Face Inpainting; Gated Convolution; Image Edge; Prior Knowledge; Region Normalization","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85127838159"
"Yi S.; Li J.; Yuan X.","Yi, Shi (57212251087); Li, Junjie (57220598871); Yuan, Xuesong (35070881700)","57212251087; 57220598871; 35070881700","DFPGAN: Dual fusion path generative adversarial network for infrared and visible image fusion","2021","Infrared Physics and Technology","119","","103947","","","","10.1016/j.infrared.2021.103947","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118751116&doi=10.1016%2fj.infrared.2021.103947&partnerID=40&md5=48b0834283b92f9f30ea7bef7f901984","Infrared and visible image fusion is an essential task for multi-sensor image fusion. Generative adversarial networks (GAN) have achieved remarkable performance in the fusion of infrared and visible image. Existing GAN based fusion methods merely using infrared and visible image as input for the fusion, while we found that differential images obtained by subtraction between two image sources could provide contrast information for the fusion. To this end, a novel dual fusion path generative adversarial network (DFPGAN) is proposed in this paper for infrared and visible image fusion. We divided the generator of generative adversarial network into two fusion paths namely infrared–visible path and differential path. The input of infrared–visible path concatenated two image sources to make infrared intensity and texture details keep balance fusion in this path. The input of differential path concatenated differential images obtained by subtraction between two image sources to make contrast information fusion in this path. The features extracted by two fusion paths are concatenated at the end of the generator to generate fused images with contrast effect and balanced information distribution. Meanwhile, we have implemented dual self-attention feature refine module (DSAM) on two fusion paths to refine feature maps in two fusion paths. We adopted switchable normalization layer (SN) substitute for batch normalization layer (BN) in the generator and discriminator to avoid fusion artifact. Furthermore, a mixed content loss is integrated in the generator loss functions to guide the generated image keep balanced information distribution and preserving contrast simultaneously. The adversarial training employed dual adversarial architecture to balance the distribution of infrared intensity and texture details. To verifying the improvement effect of fusion image on target detection, we introduce the Scaled-YOLOv4 target detection framework as evaluation framework, and use the proposed network to fuse RGB images and infrared images for target detection. The results of qualitative and quantitative experiments conducted on public datasets demonstrated the superiority of proposed network over other state-of-the-art methods and could generate fused images with distinctly contrast. © 2021 Elsevier B.V.","Image enhancement; Image fusion; Image texture; Infrared imaging; Network architecture; Textures; Differential image; Dual adversarial architecture; Dual fusion path generative adversarial network; Dual self-attention feature refine module; Fusion paths; Image source; Infrared and visible image; Infrared intensity; Mixed content loss; Targets detection; Generative adversarial networks","Differential image; Dual adversarial architecture; Dual fusion path generative adversarial network; Dual self-attention feature refine module; Image fusion; Mixed content loss","Article","Final","","Scopus","2-s2.0-85118751116"
"Kou T.; Zhang Q.; Zhang C.; He T.; Shen J.","Kou, Tingdong (57233748300); Zhang, Qican (57885223400); Zhang, Chongyang (57234000200); He, Tianyue (57224440014); Shen, Junfei (57224448902)","57233748300; 57885223400; 57234000200; 57224440014; 57224448902","Integrated MPCAM: Multi-PSF learning for large depth-of-field computational imaging","2023","Information Fusion","89","","","452","472","20","10.1016/j.inffus.2022.09.005","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137695656&doi=10.1016%2fj.inffus.2022.09.005&partnerID=40&md5=b4a40d9b4bc9a83429733cc0ece18315","Large DOF (depth-of-field) imaging with high SNR (signal-noise-ratio) is useful for applications such as machine vision and medical imaging. In traditional optical systems, DOF extension is always implemented at the cost of SNR. In this paper, we present a MPCAM (Multi-PSF Camera) system highly integrated with AF (auto-focus) function to realize both large DOF and high SNR imaging. MPCAM based on MPGAN (Multi-PSF Generative Adversarial Network) is first proposed to automatically extract multiple PSFs (point spread functions) and realize high fidelity image reconstruction by features fusion. The proposed end-to-end generative image fusion network is flexible and can be designed with different input dimensions for a given AF application, which is vital to circumvent the trade-off between DOF and SNR. We build a dataset containing 5000 raw images tailored to the proposed network by an off-the-shelf camera. Results show that our MPCAM system can produce images with average higher values than raw images over 4.625, and 0.061 in PNSR (peak signal to noise ratio), and SSIM (structure similarity) metrics, respectively. Moreover, compared to the classic and latest image fusion methods, the results also verify that our method has achieved comparable or even better performance. Due to its advance in high SNR and large DOF imaging, this novel, portable and inexpensive system is suitable for computational applications such as microscopic pathological diagnosis, domain-specific computational imaging and smartphone photography. The implementation code of MPGAN and dataset are available from https://www.kaggle.com/datasets/ktd970903/multi-psf-camera. © 2022","Cameras; Computational Imaging; Diagnosis; Economic and social effects; Generative adversarial networks; Image fusion; Image reconstruction; Medical imaging; Signal to noise ratio; Computational imaging; Depth of field; Depth-of-field extension; Field extensions; Generative image fusion network; High signals; M; Multi-points; Point-Spread function; Signal-noise ratio; Optical transfer function","Computational imaging; Depth-of-field extension; Generative image fusion network; M","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85137695656"
"Li M.-R.; Xie K.; Chen H.-Q.; Wen C.; He J.-B.","Li, Mei-Ran (57789296300); Xie, Kai (8972776100); Chen, Hua-Quan (57222900073); Wen, Chang (56811792800); He, Jian-Biao (14032744200)","57789296300; 8972776100; 57222900073; 56811792800; 14032744200","Multi-layer enhancement of low-dose CT images via adaptive fusion","2022","Signal, Image and Video Processing","","","","","","","10.1007/s11760-022-02336-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137197644&doi=10.1007%2fs11760-022-02336-7&partnerID=40&md5=ca5e605ff99fa83176d43fad6af6dea8","In order to solve the problems of low SNR and low use value of low-dose CT images, this study proposes a multi- layer enhancement of low-dose CT images via adaptive fusion. In our study, the image denoising training based on generative adversarial network is carried out, and the perceptual loss and structural loss optimization generator are used to strengthen the denoising ability and retain the details of the image. To clearly observe the pathological tissue structure, it is necessary to perform a certain degree of image enhancement and image fusion on CT images. Using the real clinical data disclosed in the AAPM competition as the experimental dataset, in the image denoising experiment, the PSNR, SSIM, and RMSE are 33.0155, 0.9185, and 5.99. Compared to traditional methods, the effectiveness of the proposed method was better by 10.76%, 4.08% and 24.54% on average, respectively. The proposed model in this study obviously reduces the noise of CT images, and the obtained CT images are more detailed, its brightness and contrast are significantly enhanced, which proves the feasibility and effectiveness of the algorithm. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Computerized tomography; Generative adversarial networks; Image denoising; Image fusion; Signal to noise ratio; Structural optimization; Adaptive fusion; CT Image; De-noising; Losses optimisation; Low dose; Low SNR; Low-dose CT; Multi-layers; Pathological tissue; Structural loss; Image enhancement","Generative adversarial network; Image denoising; Image enhancement; Image fusion; Low dose","Article","Article in press","","Scopus","2-s2.0-85137197644"
"Lyu Q.; Guo M.; Ma M.","Lyu, Qiongshuai (57210117475); Guo, Min (55732390000); Ma, Miao (56285857400)","57210117475; 55732390000; 56285857400","Boosting attention fusion generative adversarial network for image denoising","2021","Neural Computing and Applications","33","10","","4833","4847","14","10.1007/s00521-020-05284-w","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089388102&doi=10.1007%2fs00521-020-05284-w&partnerID=40&md5=308c0a9243d84e54af8aa525b1e01852","Boosting has received considerable attention to improve the overall performance of model in multiple tasks by cascading many steerable sub-modules. In this paper, a boosting attention fusion generative adversarial network (BAF-GAN) was proposed, which allows boosting idea and attention mechanism modeling for high-quality image denoising. Specifically, several boosting module groups (BMGs) with group skip connection were employed to form denoiser. Each BMG contains some boosting attention fusion blocks (BAFBs). Each BAFB consists of parallel spatial attention unit and channel attention unit interleaved connection. Moreover, the multi-dimensional inner skip connection within BAFB can carry abundant informative features. Besides, spatial and channel attention mechanisms were also embedded in the discriminator to enhance its ability of discriminating various dimensional information. Meanwhile, a new loss function was given to assist the training process of the model. BAF-GAN can be applied to remove image noise, e.g., Gaussian noise and mixed noise. Comprehensive experiment results demonstrate that the BAF-GAN has the state-of-the-art performance. © 2020, Springer-Verlag London Ltd., part of Springer Nature.","Gaussian noise (electronic); Image fusion; Adversarial networks; Attention mechanisms; High quality images; Multi dimensional; Multiple tasks; Spatial attention; State-of-the-art performance; Training process; Image denoising","Attention; Boosting; Generative adversarial network; Image denoising","Article","Final","","Scopus","2-s2.0-85089388102"
"Su J.-S.; Zhang M.-J.; Yu W.-J.","Su, Jin-Sheng (57474031400); Zhang, Ming-Jun (57204639713); Yu, Wen-Jing (57216693919)","57474031400; 57204639713; 57216693919","A Single Image Super-Resolution Reconstruction Based on Fusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12083","","120831J","","","","10.1117/12.2623592","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125562436&doi=10.1117%2f12.2623592&partnerID=40&md5=918c4accfa6aed79e0b638526bc932cd","Image super-resolution is to restore a high-resolution image from a low-resolution image or image sequence. High resolution means that the image has a high pixel density and can provide more details, which often play a key role in the application. Aiming at the application of single-frame low-resolution reconstruction and super-resolution, this paper proposes a method based on image fusion. This method combines two or more methods of super-resolution image reconstruction using generative adversarial neural networks. The reconstructed images are fused. Image fusion uses the integration of two or more images into a new image. Fusion can make use of the temporal and spatial correlation and information complementarity of two or more images, which can make the image obtained after fusion have a more comprehensive and clear description of the scene, which is more conducive to human eye recognition. This paper draws on the idea of ensemble learning, and uses the super-resolution images generated by the three super-resolution reconstruction algorithms of BasicSR, SRGAN and ESRGAN to carry out two-by-two cross fusion for simulation experiments. The experimental results show that this kind of reconstruction using different generation adversarial networks to generate the super-resolution image by fusion is simple and effective. The super-resolution image quality after fusion is generally better than the image quality before fusion in terms of PSNR and SSIM. © 2022 SPIE.","Generative adversarial networks; Image quality; Image reconstruction; Optical resolving power; High resolution; High-resolution images; Image sequence; Image super resolutions; Low resolution images; Resolution images; Single frames; Single-image super-resolution reconstruction; Super-resolution reconstruction; Superresolution; Image fusion","Generative adversarial networks; Image fusion; Super-resolution reconstruction","Conference paper","Final","","Scopus","2-s2.0-85125562436"
"Zhang C.; Fang M.; Yang C.; Yu R.; Li T.","Zhang, Chuang (7405492652); Fang, Meihan (57351057700); Yang, Chunyu (57350712200); Yu, Renhai (55803591000); Li, Tieshan (8446636200)","7405492652; 57351057700; 57350712200; 55803591000; 8446636200","Perceptual fusion of electronic chart and marine radar image","2021","Journal of Marine Science and Engineering","9","11","1245","","","","10.3390/jmse9111245","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119678953&doi=10.3390%2fjmse9111245&partnerID=40&md5=5dfe795adfce17841f4d63cbb09c048c","Electronic charts and marine radars are indispensable equipment in ship navigation systems, and the fusion display of these two parts ensures that the vessel can display dangerous moving targets and various obstacles on the sea. To reduce the noise interference caused by external factors and hardware, a novel radar image denoising algorithm using the concept of Generative Adversarial Network (GAN) using Wasserstein distance is proposed. GAN focuses on transferring the image noise distribution between strong and weak noise, while the perceptual loss approach is to suppress the noise by comparing the perceptual characteristics of the output after denoising. Afterwards, an image registration method based on image transformation is proposed to eliminate the imaging difference between the radar image and chart image, in which the visual attribute transfer approach is used to transform images. Finally, the sparse theory is used to process the high frequency and low frequency subband coefficients of the detection image obtained by the fast Fourier transform in parallel to realizing the image fusion. The results show that the fused contour has a high consistency, fast training speed and short registration time. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Electronic nautical charts; Generative adversarial network; Image denoising; Image fusion; Radar image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85119678953"
"Xiao E.; Lin H.; Jian X.","Xiao, Erliang (55502562200); Lin, Huaxi (57920960500); Jian, Xianzhong (9233142500)","55502562200; 57920960500; 9233142500","Medical Image Fusion Algorithm Adopting Generative Adversarial Network to Explore Latent Space","2021","Information and Control","50","5","","538","549","11","10.13976/j.cnki.xk.2021.0576","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137664824&doi=10.13976%2fj.cnki.xk.2021.0576&partnerID=40&md5=a5d38a25e96b09d840fbfc2a3407af34","To solve the problem of poor fusion effect caused by the loss of latent features of source mages produced by existing medical image fusion methods, an mage fusion algorithm is proposed by adopting a generative adversarial network (GAN) to explore latent space (GAN-ELS), used to mprove the fusion mages quality of computed tomography (CT) and T2-weighted magnetic resonance maging (MR-T2). Firstly, the unsuper-vised learning of the feature distribution of CT and MR-T2 mages is fully realized through the feature disentanglement learning and multi-resolution hierarchical style control of the improved GAN based on StyleGAN in the training process. Then, based on the trained generator, the latent feature space of the target fusion mage is explored according to the registered source mage and the corresponding fusion mage generated by the current mainstream fusion method. Finally, high-quality fusion mages with rich semantic information are obtained. Experimental results on the whole brain atlas data set of Havard Medical School show that, in comparson with the five mainstream fusion methods with good performance, the images fused by GAN-ELS are better in structural similarity (SSIM), qualitative mutual information (QMI), peak signal to noise ratio (PSNR), normalized mean squared error (NMSE), and other indicators, with better fusion quality. © 2021 The Authors. All rights reserved.","","atent space; deep learning; feature disentanglement; generative adversarial network; medical image fusion","Article","Final","","Scopus","2-s2.0-85137664824"
"Gite K.R.; Gupta P.","Gite, K.R. (58041608600); Gupta, Praveen (57199836986)","58041608600; 57199836986","GAN-FuzzyNN: Optimization Based Generative Adversarial Network and Fuzzy Neural Network Classification for Change Detection in Satellite Images","2023","Sensing and Imaging","24","1","1","","","","10.1007/s11220-022-00404-3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145509453&doi=10.1007%2fs11220-022-00404-3&partnerID=40&md5=a54308bc84a02004b70f77d8e42b2329","Nowadays, change detection with satellite images plays an essential role in urban planning, resources survey, and understanding global environmental changes. However, numerous satellite images are persistently acquired each and every second and they possess a significant source of data for the assessment of the spatiotemporal case. Moreover, obtaining reference data associated with satellite images dealing with land cover changes still remains a major challenging issue. Besides, traditional techniques for change detection are not valuable because of complex texture features. To cope up with such limitations, an effective mechanism is proposed for change detection by exploiting Fuzzy Neural Network (FNN) classification, which is an integration of the Fuzzy concept with Neural Network (NN), and also segmentation is done using Taylor Shuffled Shepherd Optimization (TSSO)-based Generative Adversarial Network (GAN). The proposed TSSO is obtained by incorporating the Taylor series and Shuffled shepherd Optimization (SSO) and the proposed approach achieved a maximum overall accuracy of 0.932, minimum overall error of 0.0704, and maximum kappa coefficient of 0.911. The accuracy of the devised TSSO-based GAN + Fuzzy NN is 2.28%, 4.78%, 0.33%, and 13.26% improved than the Kernel Principal Component Analysis Convolutional Mapping Network (KPCA-MNet), Multiclass Support Vector Machine (MSVM), Patchlevel and pixel-level change detection network (PPCNET), and Image Fusion Network (IFN), respectively, for Image-1. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Change detection; Convolutional neural networks; Feedforward neural networks; Fuzzy inference; Generative adversarial networks; Image classification; Image enhancement; Image fusion; Image segmentation; Principal component analysis; Satellites; Support vector machines; Textures; Change detection; Fuzzy-neural-networks; Global environmental change; Neural network classification; Optimisations; Optimization algorithms; Reference data; Satellite images; Shuffled shepherd optimization algorithm; Taylor-series; Fuzzy neural networks","Change detection; Fuzzy neural network; Generative adversarial network; Shuffled shepherd optimization algorithm; Taylor series","Article","Final","","Scopus","2-s2.0-85145509453"
"Liu J.; Wang J.; Huang N.; Zhang Q.; Han J.","Liu, Jianan (57222183059); Wang, Jialiang (57608620700); Huang, Nianchang (57214954569); Zhang, Qiang (55624487042); Han, Jungong (14522692900)","57222183059; 57608620700; 57214954569; 55624487042; 14522692900","Revisiting Modality-Specific Feature Compensation for Visible-Infrared Person Re-Identification","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","10","","7226","7240","14","10.1109/TCSVT.2022.3168999","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128659045&doi=10.1109%2fTCSVT.2022.3168999&partnerID=40&md5=80796297dca97d5738966ae88bd45bde","Although modality-specific feature compensation becomes a prevailing paradigm for Visible-Infrared Person Re-Identification (VI-ReID) to learn features, it, performance-wise, is not promising, especially when compared to modality-shared feature learning. In this paper, by revisiting the modality-specific feature compensation based models, we reveal that the reasons for being under-performed are: (1) generated images of one modality from another modality may be poor in quality; (2) such existing models usually achieve the modality-specific feature compensation just via simple pixel-level fusion strategies; (3) generated images cannot fully replace corresponding missing ones, which brings in extra modality discrepancy. To address these issues, we propose a new Two-Stage Modality Enhancement Network (TSME) for VI-ReID. Concretely, it first considers the modality discrepancy for cross-modality style translation and optimizes the structures of image generators by involving a new Deeper Skip-connection Generative Adversarial Networks (DSGAN) to generate high-quality images. Then, it presents an attention mechanism based feature-level fusion module, i.e., Pair-wise Image Fusion (PwIF) module, and an auxiliary learning module, i.e., Invoking All-Images (IAI) module, to better exploit the generated and original images for reducing modality discrepancy from the perspectives of feature fusion and feature constraints, respectively. Comprehensive experiments are carried out to demonstrate the success of TSME in tackling the modality discrepancy issue exposed in VI-ReID.  © 1991-2012 IEEE.","Computer vision; Image fusion; Infrared devices; Semantics; Feature compensation; Feature fusion and constrain; Features extraction; Features fusions; Generator; Gray scale; High quality; High-quality generated image; Image color analysis; Modality-specific feature compensation; Person re identifications; Representation learning; Visible-infrared person re-identification; Generative adversarial networks","constrains; feature fusion; high-quality generated images; modality-specific feature compensation; Visible-infrared person re-identification","Article","Final","","Scopus","2-s2.0-85128659045"
"Aldausari N.; Sowmya A.; Marcus N.; Mohammadi G.","Aldausari, Nuha (57210797455); Sowmya, Arcot (6603623225); Marcus, Nadine (7005484450); Mohammadi, Gelareh (34882049400)","57210797455; 6603623225; 7005484450; 34882049400","Diverse Audio-to-Video GAN using Multiscale Image Fusion","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13728 LNAI","","","29","42","13","10.1007/978-3-031-22695-3_3","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144828768&doi=10.1007%2f978-3-031-22695-3_3&partnerID=40&md5=1c9d3f7011f68c038205e8cf92bf4601","Generative adversarial networks have attained synthesised results that are not distinguishable from real examples in domains such as image, audio, text and video. While state-of-the-art image models synthesise images with high and diverse quality in many domains, video synthesis is more challenging and suffers from poor generalisation; moreover, the generated videos are not diverse, especially if the network is trained on a limited dataset. In such cases, the model overfits the training examples and performs poorly at inference time. Dataset collection, in general, is a tedious task, and it is even more challenging for video data due to its size and accessibility. Also, creating a video in the first place requires more time and effort. In this paper, we expand a previously collected video dataset with a supporting image dataset. Then, we apply a multiscale fusion method on multiple conditioned images to facilitate diverse video sample generation. We combine the multiscale fusion model with an audio extractor; then, the encoded features are input to a video decoder to generate videos synchronised with the audio signals. We compare our multiscale fusion model with other image fusion models on the Flowers, VGGFace and Animal Faces datasets. We also compare the overall architecture with other audio-to-video models. Both experiments show the effectiveness of our model over others, based on different evaluation metrics such as FID, FVD and LPIPS. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Image fusion; Audio-to-video GAN; Diverse video synthesis model; Fusion model; Multiscale fusion; Multiscale image fusions; Real example; State of the art; Synthesis models; Synthesised; Video synthesis; Generative adversarial networks","Audio-to-Video GAN; Diverse video synthesis model; Image fusion","Conference paper","Final","","Scopus","2-s2.0-85144828768"
"Zhang Q.; Zeng Z.; Liu Y.; Tang K.; Wang J.","Zhang, Qianyi (57830977600); Zeng, Zhixin (57215525218); Liu, Yiming (57216954036); Tang, Kang (57216948922); Wang, Ji (56259417500)","57830977600; 57215525218; 57216954036; 57216948922; 56259417500","Dynamic Scene Deblurring Using Enhanced Feature Fusion and Multi - Distillation Mechanism","2021","Proceedings of the International Joint Conference on Neural Networks","2021-July","","","","","","10.1109/IJCNN52387.2021.9533906","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116445429&doi=10.1109%2fIJCNN52387.2021.9533906&partnerID=40&md5=cd94fb001963a0f3e0a6d9d28f0dc726","Despite the surges of deep learning-based method in dynamic scene deblurring achieves good performance, the challenges still remain a lot: (a) the running speed is far from the requirement of processing; (b) there will be inevitable information loss along with the deepening of network layers, which will further lead to the deterioration of the quality of the restored pictures. To deal with these challenges, we propose a novel learning-based model. In our method, we integrate two mechanisms for the generator based on the Generative Adversarial Nets (GAN). First, we develop the Enhanced Feature Fusion (EFF) mechanism which aims at providing multi-layer feature information to assist the image restoration. We further design Feature Multi-distillation (FMD) mechanism to filter and well fuse the multi-scale feature maps. By integrating the two mechanisms, the high-level feature maps can be progressively refined and the detailed semantic information can be properly utilized. In addition, we use the double-scale discriminator architecture which could enables the network to observe the image from the perspective of local and global respectively and obtain overall information for the whole image restoration. Extensive experimental results on the GOPRO and Kohler datasets show that our method can approach comparably to the state-of-the-arts in terms of accuracy while consuming much less inference time, which demonstrates that our method acquiring a better trade off between image restoration quality and running speed. © 2021 IEEE.","Computer vision; Deep learning; Deterioration; Discriminators; Distillation; Dynamics; Economic and social effects; Image enhancement; Image fusion; Image reconstruction; Network layers; Restoration; Semantics; Deblurring; Double-scale; Double-scale discriminator; Dynamic scenes; Enhanced feature fusion; Feature map; Feature multi-distillation; Features fusions; Image deblurring; Running speed; Generative adversarial networks","double-scale discriminator; enhanced feature fusion; feature multi-distillation; generative adversarial network; image deblurring","Conference paper","Final","","Scopus","2-s2.0-85116445429"
"Ma X.; Wang Z.; Hu S.; Kan S.","Ma, Xiaole (57193220596); Wang, Zhihai (23092515400); Hu, Shaohai (7404286949); Kan, Shichao (57193743002)","57193220596; 23092515400; 7404286949; 57193743002","Multi-Focus Image Fusion Based on Multi-Scale Generative Adversarial Network","2022","Entropy","24","5","582","","","","10.3390/e24050582","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132882049&doi=10.3390%2fe24050582&partnerID=40&md5=a43363eb065686dc7c87763b92bd9a96","The methods based on the convolutional neural network have demonstrated its powerful information integration ability in image fusion. However, most of the existing methods based on neural networks are only applied to a part of the fusion process. In this paper, an end-to-end multi-focus image fusion method based on a multi-scale generative adversarial network (MsGAN) is proposed that makes full use of image features by a combination of multi-scale decomposition with a convolutional neural network. Extensive qualitative and quantitative experiments on the synthetic and Lytro datasets demonstrated the effectiveness and superiority of the proposed MsGAN compared to the state-of-the-art multi-focus image fusion methods. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","Generative adversarial network; Multi-focus image fusion; Multi-scale decomposition","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132882049"
"Wang J.; Yu H.","Wang, Jihong (57759439300); Yu, Haiyan (57759888500)","57759439300; 57759888500","Double-channel cascade-based generative adversarial network for power equipment infrared and visible image fusion","2022","EAI Endorsed Transactions on Scalable Information Systems","9","36","e6","","","","10.4108/eai.22-11-2021.172216","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132582663&doi=10.4108%2feai.22-11-2021.172216&partnerID=40&md5=de2c11bbfd69e5040a676577c949e541","At present, visible light imaging sensor and infrared imaging sensor are two commonly used sensors, which are widely used in aviation, navigation and other military fields of detection, monitoring and tracking. Due to their different working principles, their performance is different. The infrared imaging sensor records the infrared radiation information of the target itself by acquiring the infrared radiation of the ground target. It identifies the target by detecting the thermal radiation difference between the target and the background, so it has special recognition and camouflage ability, such as finding people, vehicles and artillery hidden in the woods and grass. Although the infrared imaging sensor has a good detection performance for thermal targets, it is insensitive to the brightness changes of the scene and has low imaging resolution, which is not conducive to human eyes interpretation. Visible light imaging sensor is sensitive to the reflection of the target scene and has nothing to do with the thermal contrast of the target scene. The obtained image has high clarity and can provide the details of the target scene. Therefore, the fusion of infrared and visible images will be beneficial to the combination of infrared image's better target indication characteristics and visible image's scene clearing information. In this paper, we propose a double-channel cascade-based generative adversarial network for power equipment infrared and visible image fusion. The experimental results show that the fusion image not only retains the target information of the infrared image, but also retains more details of the visible image, and achieves better performance in both subjective and objective evaluation. © 2021. Jihong Wang et al., licensed to EAI. This is an open access article distributed under the terms of the Creative Commons Attribution license, which permits unlimited use, distribution and reproduction in any medium so long as the original work is properly cited.","Image fusion; Infrared radiation; Thermography (imaging); Double-channel; Double-channel cascade; Imaging sensors; Infrared and visible image; Infrared and visible image fusion; Infrared imaging sensors; Light imaging; Power equipment; Target scenes; Visible light; Generative adversarial networks","Double-channel cascade; Generative adversarial network; Infrared and visible image fusion; Power equipment","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85132582663"
"He Y.; Wu J.; Zheng Y.; Zhang Y.; Hong X.","He, Yongxiang (57219020736); Wu, Jun (56693265500); Zheng, Yaojia (57455167100); Zhang, Yuxin (57456109300); Hong, Xiaobo (57209780983)","57219020736; 56693265500; 57455167100; 57456109300; 57209780983","Track Defect Detection for High-Speed Maglev Trains via Deep Learning","2022","IEEE Transactions on Instrumentation and Measurement","71","","","","","","10.1109/TIM.2022.3151165","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124721502&doi=10.1109%2fTIM.2022.3151165&partnerID=40&md5=da4ab9c08b6a3cc6dd64c1386e301590","The high-speed maglev train is a new type of transportation. The long stator track plays a critical role in the levitation guidance and traction system. Therefore, its condition directly affects the operation of maglev trains. It is extremely important to detect the abnormal condition of high-speed maglev tracks to ensure the stable, safe, and reliable operation of the train. In this article, an onboard image detection system is designed for high-speed maglev tracks, which can accurately obtain the image of long stator tracks under the harsh conditions of limited installation space, insufficient illumination, and rapid operation of vehicles. High-speed maglev trains are not yet in widespread use. In China, there is currently only one demonstration operating line located in Shanghai, and the length of the track test line is limited. Therefore, the number of track images that can be acquired is extremely limited. In view of the lack of defective samples of high-speed maglev tracks, this article proposes a data enhancement method based on sample generation and image fusion to augment the dataset of defective samples. To improve the quality of generated high-speed maglev track defect images, a joint attention layer (JEA) combining squeeze-and-exception (SE) block and spatial attention module (SAM) is designed and introduced into the generative adversarial network (GAN). This work provides a data basis for the study of track defect detection of high-speed maglev trains. In addition, this article detects the defects of high-speed maglev tracks via deep learning-based target detection algorithms, which can automatically detect, accurately classify and locate the defects of stator surface and cables, filling the gap in the field of high-speed maglev track defect detection.  © 1963-2012 IEEE.","Deep learning; Image enhancement; Image fusion; Magnetic levitation vehicles; Magnetic resonance imaging; Speed; Stators; Surface defects; Abnormal conditions; Condition; Defect detection; Guidance system; High Speed; High-speed maglev train; Maglev Train; Sample generations; Stable operation; Traction systems; Magnetic levitation","Data fusion; defect detection; faster regions with convolutional neural networks features (R-CNN); high-speed maglev trains; sample generation","Article","Final","","Scopus","2-s2.0-85124721502"
"Nguyen T.; Yoo M.","Nguyen, Tri (57368648100); Yoo, Myungsik (55984448800)","57368648100; 55984448800","PatchGAN-Based Depth Completion in Autonomous Vehicle","2022","International Conference on Information Networking","2022-January","","","498","501","3","10.1109/ICOIN53446.2022.9687223","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125676783&doi=10.1109%2fICOIN53446.2022.9687223&partnerID=40&md5=0f830d6185a10205d3f1c7e824ff0898","Depth completion is a challenging task and it serves as the fundamental function for many vision handling operation in autonomous vehicle. It aims to generate a dense depth map from upsampling the sparse input data collected from LIDAR sensor. This paper proposed a GAN architecture with a two-branch autoencoder in the generator that exploits two different modalities and performs a fusion strategy between two modalities. Furthermore, the architecture is evaluated on the KITTI datasets and KITTI benchmark and proved to have a competitive performance.. © 2022 IEEE.","Autonomous vehicles; Benchmarking; Computer vision; Image fusion; Intelligent vehicle highway systems; Network architecture; Optical radar; Auto encoders; Autonomous Vehicles; Dense depth map; Depth completion; Depth image; Input datas; LIDAR sensors; Sensor fusion; Sparse depth image; Upsampling; Generative adversarial networks","autonomous vehicle; depth completion; generative adversarial network; sensor fusion; sparse depth image","Conference paper","Final","","Scopus","2-s2.0-85125676783"
"Rao Y.; Wu D.; Han M.; Wang T.; Yang Y.; Lei T.; Zhou C.; Bai H.; Xing L.","Rao, Yujing (57222428611); Wu, Dan (57566396200); Han, Mina (57221663489); Wang, Ting (58022455600); Yang, Yang (57193219252); Lei, Tao (57209986653); Zhou, Chengjiang (56927199800); Bai, Haicheng (57217561674); Xing, Lin (55273079400)","57222428611; 57566396200; 57221663489; 58022455600; 57193219252; 57209986653; 56927199800; 57217561674; 55273079400","AT-GAN: A generative adversarial network with attention and transition for infrared and visible image fusion","2023","Information Fusion","92","","","336","349","13","10.1016/j.inffus.2022.12.007","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144302001&doi=10.1016%2fj.inffus.2022.12.007&partnerID=40&md5=473e024417c7933466019ddb53eda2fa","Infrared and visible image fusion methods aim to combine high-intensity instances and detail texture features into fused images. However, the ability to capture compact features under various adverse conditions is limited because the distribution of these multimodal features is generally cluttered. Therefore, targeted designs are necessary to constrain multimodal features to be compact. In addition, many attempts are not robust for low-quality images under various adverse conditions and the high fusion time of most fusion methods leads to less effective subsequent vision tasks. To address these issues, we propose a generative adversarial network with intensity attention modules and semantic transition modules, termed AT-GAN, which are more efficient to extract key information from multimodal images. The intensity attention modules aim to keep infrared instance features clearly and semantic transition modules attempt to filter out noise or other redundant features in visible texture. Moreover, an adaptive fused equilibrium point can be learned by a quality assessment module. Finally, experiments with variety of datasets reveal that the AT-GAN can adaptively learn features fusion and image reconstruction synchronously and further improve the timeliness under premise of fusion superiority of the proposed method over state of the art. © 2022","Generative adversarial networks; Image enhancement; Image reconstruction; Image texture; Semantics; Textures; Adverse condition; Attention mechanisms; Compact Features; Condition; Fused images; High intensity; Image fusion methods; Infrared and visible image; Multimodal features; Texture features; Image fusion","Adverse conditions; Attention mechanism; Generative adversarial networks; Image fusion; Infrared and visible images","Article","Final","","Scopus","2-s2.0-85144302001"
"Li A.; Zhao L.; Zuo Z.; Wang Z.; Xing W.; Lu D.","Li, Ailin (57222051099); Zhao, Lei (56136907600); Zuo, Zhiwen (57219639828); Wang, Zhizhong (57219478880); Xing, Wei (57833610700); Lu, Dongming (58017419600)","57222051099; 56136907600; 57219639828; 57219478880; 57833610700; 58017419600","MIGT: Multi-modal image inpainting guided with text","2023","Neurocomputing","520","","","376","385","9","10.1016/j.neucom.2022.11.074","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144008281&doi=10.1016%2fj.neucom.2022.11.074&partnerID=40&md5=8dbb78cf70b442ff92dfd99dbba0aeda","In this work, we propose MIGT, a novel framework for multi-modal image inpainting that introduces textual description as guidance. We divide MIGT into three components: Coarse-to-Fine Image Inpainting Module (CFIM), Visual-Textual modalities Fusion Module (VTFM), and Multi-modal Semantic Alignment Module (MSAM). CFIM is a Unet-based inpainting model in a coarse-to-fine manner. The coarse inpainting stage produces images of rough shape and color according to the source corrupted images and the corresponding textual descriptions. The fine inpainting stage generates the final high-quality images with fine-grained textures. VTFM aims to reasonably fuse visual-textual modalities. First, we feed visual and textual features into the proposed visual-aware textual filtering mechanism to adaptively focus on desired words related to the missing areas. Then the filtered visual-aware textual features pass through an Attentional Generative Network (AGN) to obtain fusion features that are fed into the middle layers of the coarse stage for subsequent image inpainting. MSAM takes the generated image as input, reconstructing a textual description that semantically aligns the given one to guarantee the semantic consistency between the generated image and the input textual description. Extension experiments conducted on Oxford-102 flower and CUB-200–2011 bird datasets demonstrate the effectiveness of our proposed method. © 2022 Elsevier B.V.","Image fusion; Image processing; Semantics; Textures; Coarse to fine; Fine images; Fusion modules; Image Inpainting; Inpainting; Modality Fusion; Multi-modal learning; Multimodal images; Text-to-image; Textual description; article; bird; filtration; flower; learning; nonhuman; Generative adversarial networks","Generative Adversarial Network; Image inpainting; Multimodal learning; Text-to-image","Article","Final","","Scopus","2-s2.0-85144008281"
"Wu J.; Qiu X.; Zhang J.; Wu F.; Kong Y.; Yang G.; Senhadji L.; Shu H.","Wu, Jiasong (34874119700); Qiu, Xiang (57328540100); Zhang, Jing (57221355517); Wu, Fuzhi (57198943431); Kong, Youyong (36082608100); Yang, Guanyu (16176923900); Senhadji, Lotfi (7004293671); Shu, Huazhong (7203086899)","34874119700; 57328540100; 57221355517; 57198943431; 36082608100; 16176923900; 7004293671; 7203086899","Fractional Wavelet-Based Generative Scattering Networks","2021","Frontiers in Neurorobotics","15","","752752","","","","10.3389/fnbot.2021.752752","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118796037&doi=10.3389%2ffnbot.2021.752752&partnerID=40&md5=8dfeda363b2eb9af38cc8db55d31483a","Generative adversarial networks and variational autoencoders (VAEs) provide impressive image generation from Gaussian white noise, but both are difficult to train, since they need a generator (or encoder) and a discriminator (or decoder) to be trained simultaneously, which can easily lead to unstable training. To solve or alleviate these synchronous training problems of generative adversarial networks (GANs) and VAEs, researchers recently proposed generative scattering networks (GSNs), which use wavelet scattering networks (ScatNets) as the encoder to obtain features (or ScatNet embeddings) and convolutional neural networks (CNNs) as the decoder to generate an image. The advantage of GSNs is that the parameters of ScatNets do not need to be learned, while the disadvantage of GSNs is that their ability to obtain representations of ScatNets is slightly weaker than that of CNNs. In addition, the dimensionality reduction method of principal component analysis (PCA) can easily lead to overfitting in the training of GSNs and, therefore, affect the quality of generated images in the testing process. To further improve the quality of generated images while keeping the advantages of GSNs, this study proposes generative fractional scattering networks (GFRSNs), which use more expressive fractional wavelet scattering networks (FrScatNets), instead of ScatNets as the encoder to obtain features (or FrScatNet embeddings) and use similar CNNs of GSNs as the decoder to generate an image. Additionally, this study develops a new dimensionality reduction method named feature-map fusion (FMF) instead of performing PCA to better retain the information of FrScatNets,; it also discusses the effect of image fusion on the quality of the generated image. The experimental results obtained on the CIFAR-10 and CelebA datasets show that the proposed GFRSNs can lead to better generated images than the original GSNs on testing datasets. The experimental results of the proposed GFRSNs with deep convolutional GAN (DCGAN), progressive GAN (PGAN), and CycleGAN are also given. © Copyright © 2021 Wu, Qiu, Zhang, Wu, Kong, Yang, Senhadji and Shu.","Convolution; Convolutional neural networks; Decoding; Embeddings; Image enhancement; Image fusion; Principal component analysis; Signal encoding; White noise; Auto encoders; Convolutional neural network; Embeddings; Feature map; Feature-map fusion; Fractional wavelet scattering network; Generative model; Image generations; Map fusions; Scattering networks; article; convolutional neural network; embedding; principal component analysis; Generative adversarial networks","feature-map fusion; fractional wavelet scattering network; generative model; image fusion; image generation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85118796037"
"Guo L.; Tang D.","Guo, Li (58073704700); Tang, Dandan (58074223100)","58073704700; 58074223100","Infrared and visible image fusion using a generative adversarial network with a dual-branch generator and matched dense blocks","2023","Signal, Image and Video Processing","","","","","","","10.1007/s11760-022-02392-z","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146691513&doi=10.1007%2fs11760-022-02392-z&partnerID=40&md5=c1d711c01ef4e64bbcb4d189778e1467","To obtain a better fusion effect for infrared and visible images, a generative adversarial network using a dual-branch generator with matched dense blocks is proposed. The dual-branch generator consists of two parallel sub-networks, namely the upper and lower branches, which are asymmetrical in structure. It could be applied to nonlinearly extract the textural and contrast information in multiple degrees of freedom. Based on the dual-branch structure, two dense blocks are optimally designed by selectively arranging reduced concatenation connections to effectively employ the shallow information. As a result, both are non-full connection and symmetrically added on the upper and lower branches, respectively. Additionally, a gradient loss function containing the mean square error function was applied in the generator loss function, which could help extract more textural detail information. With such a generator and under adversarial learning with the discriminator, it could allow the fused images to preserve more visible and infrared information while also produce satisfactory visual perception. Experiments were implemented based on the open datasets, which included contrast and optimization experiments. The results demonstrate that the proposed method has superiority in terms of more detail and salient contrast in faint features which is relative to other state-of-the-art methods, and the applied dual-branch structure with the matched dense blocks is an appropriate for better fusion effect. The proposed method could be applied in certain detection or monitoring fields for infrared and visible image fusion. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Degrees of freedom (mechanics); Generative adversarial networks; Mean square error; Branch structure; Dense block; Dual-branch generator; Generative adversarial network; Infrared and visible image; Infrared and visible image fusion; Loss functions; Means square errors; Multiple degrees of freedom; Subnetworks; Image fusion","Dense blocks; Dual-branch generator; Generative adversarial network (GAN); Infrared and visible image fusion","Article","Article in press","","Scopus","2-s2.0-85146691513"
"Tang L.; Liu G.; Xiao G.","Tang, Lili (57222576603); Liu, Gang (56488678500); Xiao, Gang (57203485735)","57222576603; 56488678500; 57203485735","Infrared and Visible Image Fusion Method Based on Dual-path Cascade Adversarial Mechanism; [基于双路级联对抗机制的红外与可见光图像融合方法]","2021","Guangzi Xuebao/Acta Photonica Sinica","50","9","","313","323","10","10.3788/gzxb20215009.0910004","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115447097&doi=10.3788%2fgzxb20215009.0910004&partnerID=40&md5=83ba28bcb69a81fde2bff58bf6197fc5","The thermal target information of infrared image and some detail information of visible images are usually ignored in image fusion method based on generative adversarial network. To address the problem, an infrared and visible image fusion method based on dual-path cascade adversarial mechanism is proposed. In the stage of the generator model, a dual-path cascade is used to extract features of infrared and visible images, respectively. To improve the quality of fusion, structural similarity is introduced into the loss function. In the stage of the discriminator model, a dual discriminator is used to distinguish the generated image from the true natural visual images. The proposed method is experimented on the public data, and compared with eight state-of-the-art image fusion methods. The experimental results show that the fusion image not only retains more the target information of infrared images, but also retains more detail information of visible images, which is superior to state-of-the-art methods in subjective evaluation and objective assessment. © 2021, Science Press. All right reserved.","Discriminators; Image fusion; Infrared imaging; Dual path; Dual-discriminator; Dual-path cascade; Generator modelling; Image fusion methods; Infrared and visible image; Structural similarity; Target information; Thermal; Visible image; Generative adversarial networks","Dual-discriminator; Dual-path cascade; Generative adversarial network; Image fusion; Infrared; Visible image","Article","Final","","Scopus","2-s2.0-85115447097"
"Huang S.; Jiang Q.; Jin X.; Lee S.; Feng J.; Yao S.","Huang, Shanshan (57214939600); Jiang, Qian (57194699462); Jin, Xin (56991832300); Lee, Shinjye (34877262700); Feng, Jianan (57221777999); Yao, Shaowen (24473851600)","57214939600; 57194699462; 56991832300; 34877262700; 57221777999; 24473851600","Semi-Supervised Remote Sensing Image Fusion Method Combining Siamese Structure with Generative Adversarial Networks; [结合双胞胎结构与生成对抗网络的半监督遥感图像融合]","2021","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","33","1","","92","105","13","10.3724/SP.J.1089.2021.18227","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100159308&doi=10.3724%2fSP.J.1089.2021.18227&partnerID=40&md5=377d862236b18292e2dbd0a2820abab3","To solve the problems of acquisition of label image and spectral distortion in the current remote sensing image fusion, a semi-supervised remote sensing image fusion method using Siamese structure is proposed. This method adopted a generative adversarial network structure composed of generator and discriminator, in which the generator contains two encoders and a decoder. First, the multispectral image is amplified and converted into HSV color space. Then, the V channel of the multispectral image and panchromatic images are respectively input into the Siamese network of the encoder, and the image features are extracted through the convolutional layer and the multi-skip connection layer model. Third, the obtained feature map is input to the decoder for image reconstruction. And the fused V channel image is identified by the discriminator, so as to obtain the optimal fusion result. Finally, the fused V channel is concatenated with the H and S channels of the multispectral image to obtain the final fused image. In addition, a compound loss function is designed. Experiments on QuickBird satellite remote sensing image dataset show that this method can effectively improve spatial details and color information in fused images. Compared with the contrast algorithms, the fusion images have certain advantages in subjective visual quality and objective evaluation index. © 2021, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Channel coding; Color; Decoding; Image enhancement; Image reconstruction; Remote sensing; Signal encoding; Space optics; Adversarial networks; Color information; Multispectral images; Objective evaluation; Panchromatic images; QuickBird satellite; Remote sensing images; Spectral distortions; Image fusion","Conditional generative adversarial networks; Multispectral image; Panchromatic image; Remote sensing image fusion; Siamese networks","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85100159308"
"Zhang H.; Xu H.; Tian X.; Jiang J.; Ma J.","Zhang, Hao (57215014270); Xu, Han (57201056465); Tian, Xin (55458978600); Jiang, Junjun (54902306100); Ma, Jiayi (26638975600)","57215014270; 57201056465; 55458978600; 54902306100; 26638975600","Image fusion meets deep learning: A survey and perspective","2021","Information Fusion","76","","","323","336","13","10.1016/j.inffus.2021.06.008","122","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109457013&doi=10.1016%2fj.inffus.2021.06.008&partnerID=40&md5=a6b3eaa2262c1c3dd239ff43b969ae55","Image fusion, which refers to extracting and then combining the most meaningful information from different source images, aims to generate a single image that is more informative and beneficial for subsequent applications. The development of deep learning has promoted tremendous progress in image fusion, and the powerful feature extraction and reconstruction capabilities of neural networks make the fused results promising. Recently, several latest deep learning technologies have made image fusion explode, e.g., generative adversarial networks, autoencoder, etc. However, a comprehensive review and analysis of latest deep-learning methods in different fusion scenarios is lacking. To this end and in this survey, we first introduce the concept of image fusion, and classify the methods from the perspectives of the deep architectures adopted and fusion scenarios. Then, we review the state-of-the-art on the use of deep learning in various types of image fusion scenarios, including the digital photography image fusion, the multi-modal image fusion and the sharpening fusion. Subsequently, the evaluation for some representative methods in specific fusion tasks are performed qualitatively and quantitatively. Moreover, we briefly introduce several typical applications of image fusion, including photography visualization, RGBT object tracking, medical diagnosis, and remote sensing monitoring. Finally, we provide the conclusion, highlight the challenges in image fusion, and look forward to potential future research directions. © 2021 Elsevier B.V.","Deep learning; Diagnosis; Learning systems; Medical imaging; Object tracking; Photography; Remote sensing; Surveys; Adversarial networks; Deep architectures; Digital photography; Future research directions; Learning technology; Reconstruction capability; Remote sensing monitoring; Typical application; Image fusion","Deep learning; Digital photography; Image fusion; Multi-modal; Sharpening","Short survey","Final","","Scopus","2-s2.0-85109457013"
"Zhang H.; Le Z.; Shao Z.; Xu H.; Ma J.","Zhang, Hao (57215014270); Le, Zhuliang (57215469419); Shao, Zhenfeng (7202244409); Xu, Han (57201056465); Ma, Jiayi (26638975600)","57215014270; 57215469419; 7202244409; 57201056465; 26638975600","MFF-GAN: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion","2021","Information Fusion","66","","","40","53","13","10.1016/j.inffus.2020.08.022","83","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090322939&doi=10.1016%2fj.inffus.2020.08.022&partnerID=40&md5=155bfcaaabed59f424dc7a0f8df94057","Multi-focus image fusion is an enhancement method to generate full-clear images, which can address the depth-of-field limitation in imaging of optical lenses. Most existing methods generate the decision map to realize multi-focus image fusion, which usually lead to detail loss due to misclassification, especially near the boundary line of the focused and defocused regions. To overcome this challenge, this paper presents a new generative adversarial network with adaptive and gradient joint constraints to fuse multi-focus images. In our model, an adaptive decision block is introduced to determine whether source pixels are focused or not based on the difference of repeated blur. Under its guidance, a specifically designed content loss can dynamically guide the optimization trend, that is, force the generator to produce a fused result of the same distribution as the focused source images. To further enhance the texture details, we establish an adversarial game so that the gradient map of the fused result approximates the joint gradient map constructed based on the source images. Our model is unsupervised without requiring ground-truth fused images for training. In addition, we release a new dataset containing 120 high-quality multi-focus image pairs for benchmark evaluation. Experimental results demonstrate the superiority of our method over the state-of-the-art in terms of both subjective visual effect and quantitative metrics. Moreover, our method is about one order of magnitude faster compared with the state-of-the-art. © 2020","Benchmarking; Image enhancement; Lenses; Textures; Adversarial networks; Benchmark evaluation; Joint constraint; Misclassifications; Multifocus image fusion; Multifocus images; Quantitative metrics; State of the art; Image fusion","Generative adversarial network; Image fusion; Multi-focus; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85090322939"
"Lian J.; Chen S.; Ding K.; Li L.-H.","Lian, Jing (57643785600); Chen, Shi (57948800700); Ding, Kun (58019100900); Li, Lin-Hui (36072793400)","57643785600; 57948800700; 58019100900; 36072793400","Generative Adversarial Network Based on Multi-scale Dense Feature Fusion for Image Dehazing; [基于多尺度密集特征融合的生成式对抗除雾网络]","2022","Dongbei Daxue Xuebao/Journal of Northeastern University","43","11","","1591","1598","7","10.12068/j.issn.1005-3026.2022.11.010","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144131343&doi=10.12068%2fj.issn.1005-3026.2022.11.010&partnerID=40&md5=8cafdfeff7994898886eac08c293d69a","In view of the poor dehazed effect of the existing dehazing networks in real hazy image and the obvious noise in the sky area of the image, a generative adversarial network based on multi-scale dense feature fusion for image dehazing is proposed. The dehazing network uses the produced synthetic foggy data set for adversarial training. Firstly, the dehazing network is designed and the network model is constructed; secondly, a realistic foggy data set is directly generated from the synthetic sunny weather image using deep tags to be suitable for the dehazed field; finally, the network is tested on the real foggy day data set and selects six representative deep learning dehazing networks in recent years for comparison, and non-reference image quality evaluation indicators are used for objective analysis. The research results show that the effect of the proposed dehazing network in real scenes is significantly improved compared to the other networks. The subjective visual effect is significantly better, and the comprehensive performance is better than the other networks in non-reference image quality evaluation indicators. © 2022 Northeastern University. All rights reserved.","Deep learning; Demulsification; Image fusion; Image quality; Quality control; Adversarial training; Data set; Dehazing; Features fusions; Image dehazing; Images processing; Multi-scale dense feature fusion; Multi-scales; Network-based; Reference image; Generative adversarial networks","adversarial training; generative adversarial network; image dehazing; image processing; multi-scale dense feature fusion","Article","Final","","Scopus","2-s2.0-85144131343"
"Yang X.; Huo H.; Wang R.; Li C.; Liu X.; Li J.","Yang, Xin (57212206381); Huo, Hongtao (56527849900); Wang, Renhua (57221059573); Li, Chang (56718731300); Liu, Xiaowen (57720287400); Li, Jing (57207844651)","57212206381; 56527849900; 57221059573; 56718731300; 57720287400; 57207844651","DGLT-Fusion: A decoupled global–local infrared and visible image fusion transformer","2023","Infrared Physics and Technology","128","","104522","","","","10.1016/j.infrared.2022.104522","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144563763&doi=10.1016%2fj.infrared.2022.104522&partnerID=40&md5=6f71ef11fe7df6be3ebf60f88a26a297","Convolution Neural Networks (CNN) and generative adversarial networks (GAN) based approaches have achieved substantial performance in image fusion field. However, these methods focus on extracting local features and pay little attention to learning global dependencies. In recent years, given the competitive long-term dependency modeling capability, the Transformer based fusion method has made impressive achievement, but this method simultaneously processes long-term correspondences and short-term features, which might result in deficiently global–local information interaction. Towards this end, we propose a decoupled global–local infrared and visible image fusion Transformer (DGLT-Fusion). The DGLT-Fusion decouples global–local information learning into Transformer module and CNN module. The long-term dependencies are modeled by a series of Transformer blocks (global-decoupled Transformer blocks), while the short-term features are extracted by local-decoupled convolution blocks. In addition, we design Transformer dense connection to reserve more information. These two modules are interweavingly stacked that enables our network retain texture and detailed information more integrally. Furthermore, the comparative experiment results show that DGLT-Fusion achieves better performance than state-of-the-art approaches. © 2022 Elsevier B.V.","Convolution; Deep learning; Generative adversarial networks; Infrared imaging; Textures; Convolution neural network; Global-local; Infrared and visible image; Infrared image; Local information; Long-term dependencies; Network-based approach; Performance; Transformer; Visible image; Image fusion","Convolution neural networks; Image fusion; Infrared image; Transformer; Visible image","Article","Final","","Scopus","2-s2.0-85144563763"
"Ju L.I.U.; Duan J.; Youfei H.A.O.; Chen G.; Zhang A.H.A.O.","Ju, L.I.U. (57970131500); Duan, Jin (36849346700); Youfei, H.A.O. (57969666300); Chen, Guangqiu (55733514700); Zhang, And H.A.O. (57969428800)","57970131500; 36849346700; 57969666300; 55733514700; 57969428800","Semantic-guided polarization image fusion method based on a dual-discriminator GAN","2022","Optics Express","30","24","","43601","43621","20","10.1364/OE.472214","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142186955&doi=10.1364%2fOE.472214&partnerID=40&md5=c0eccf4d5cb6bf1c6e7b361b9a0d972f","Polarization image fusion is the process of fusing an intensity image and a polarization parameter image solved by Stokes vector into a more detailed image. Conventional polarization image fusion strategies lack the targeting and robustness for fusing different targets in the images because they do not account for the differences in the characterization of the polarization properties of different materials, and the fusion rule is manually designed. Therefore, we propose a novel end-to-end network model called a semantic guided dual discriminator generative adversarial network (SGPF-GAN) to solve the polarization image fusion problem. We have specifically created a polarization image information quality discriminator (PIQD) block to guide the fusion process by employing this block in a weighted way. The network establishes an adversarial game relationship between a generator and two discriminators. The goal of the generator is to generate a fused image by weighted fusion of each semantic object of the image, the dual discriminator’s objective is to identify specific modalities (polarization/intensity) of various semantic targets. The results of qualitative and quantitative evaluations demonstrate the superiority of our SGPF-GAN in terms of visual effects and quantitative measures. Additionally, using this fusion approach to transparent, camouflaged hidden target detection and image segmentation can significantly boost the performance. © 2022 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement.","Generative adversarial networks; Image fusion; Image segmentation; Polarization; Semantics; Detailed images; End-to-end network; Fusion rule; Fusion strategies; Image fusion methods; Intensity images; Polarization images; Polarization parameters; Polarization properties; Stokes vector; article; data quality; image segmentation; polarization; quantitative analysis; Discriminators","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142186955"
"Arnous F.I.; Narayanan R.M.; Li B.C.","Arnous, Ferris I. (57217305796); Narayanan, Ram M. (7202724032); Li, Bing C. (26643107500)","57217305796; 7202724032; 26643107500","Application of multidomain sensor image fusion and training data augmentation for enhanced CNN image classification","2022","Journal of Electronic Imaging","31","1","013014","","","","10.1117/1.JEI.31.1.013014","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125697473&doi=10.1117%2f1.JEI.31.1.013014&partnerID=40&md5=936ae611883d640721fefd546215f3ee","Convolutional neural networks (CNNs) provide the sensing and detection community with a discriminative approach for classifying images. However, one of the largest limitations of deep CNN image classifiers is the need for extensive training datasets containing a variety of image representations. While current methods, such as generative adversarial network data augmentation, additions of noise, rotations, and translations, can allow CNNs to better associate new images and their feature representations to ones of a learned image class, many fail to provide new contexts of ground truth feature information. To expand the association of critical class features within CNN image training datasets, an image pairing and training dataset augmentation paradigm via a multi-sensor domain image data fusion algorithm is proposed. This algorithm uses a mutual information (MI) and merit-based feature selection subroutine to pair highly correlated cross-domain images from multiple sensor domain image datasets. It then re-augments the corresponding cross-domain image pairs into the opposite sensor domain's feature set via a highest MI, cross sensor domain, and image concatenation function. This augmented image set then acts to retrain the CNN to recognize greater generalizations of image class features via cross domain, mixed representations. Experimental results indicated an increased ability of CNNs to generalize and discriminate between image classes during testing of class images from synthetic aperture radar vehicle, solar cell device reliability screening, and lung cancer detection image datasets.  © 2022 SPIE and IS&T.","Classification (of information); Convolution; Convolutional neural networks; Deep neural networks; Generative adversarial networks; Image classification; Image enhancement; Information fusion; Radar imaging; Sensor data fusion; Synthetic aperture radar; Convolutional neural network; Cross-domain; Data augmentation; Image datasets; Image training; Mutual informations; Sensor domains; Training data; Training data augmentation; Training dataset; Image fusion","convolutional neural network; image fusion; machine learning; mutual information; training data augmentation","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85125697473"
"Zhou T.; Li Q.; Lu H.; Zhang X.; Cheng Q.","Zhou, Tao (57020593700); Li, Qi (57936733500); Lu, Huiling (55729821400); Zhang, Xiangxiang (57936871000); Cheng, Qianru (57936465400)","57020593700; 57936733500; 55729821400; 57936871000; 57936465400","Hybrid Multimodal Medical Image Fusion Method Based on LatLRR and ED-D2GAN","2022","Applied Sciences (Switzerland)","12","24","12758","","","","10.3390/app122412758","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144896246&doi=10.3390%2fapp122412758&partnerID=40&md5=310ade7b7f5dee83c6b3d66370bfd7f9","In order to better preserve the anatomical structure information of Computed Tomography (CT) source images and highlight the metabolic information of lesion regions in Positron Emission Tomography (PET) source images, a hybrid multimodal medical image fusion method (LatLRR-GAN) based on Latent low-rank representation (LatLRR) and the dual discriminators Generative Adversarial Network (ED-D2GAN) is proposed. Firstly, considering the denoising capability of LatLRR, source images were decomposed by LatLRR. Secondly, the ED-D2GAN model was put forward as the low-rank region fusion method, which can fully extract the information contained by the low-rank region images. Among them, encoder and decoder networks were used in the generator; convolutional neural networks were also used in dual discriminators. Thirdly, a threshold adaptive weighting algorithm based on the region energy ratio is proposed as the salient region fusion rule, which can improve the overall sharpness of the fused image. The experimental results show that compared with the best methods of the other six methods, this paper is effective in multiple objective evaluation metrics, including the average gradient, edge intensity, information entropy, spatial frequency and standard deviation. The results of the two experiments are improved by 35.03%, 42.42%, 4.66%, 8.59% and 11.49% on average. © 2022 by the authors.","","deep learning; GAN; LatLRR; medical image fusion; multimodal","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85144896246"
"Luo X.; Lu Z.","Luo, Xiaojie (57963760200); Lu, Zhenkun (35208621500)","57963760200; 35208621500","ECA-based Generative Adversarial Network for Multi-focus Colour Image Fusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12342","","123423J","","","","10.1117/12.2643626","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141860433&doi=10.1117%2f12.2643626&partnerID=40&md5=2efe82372e631e1e2bf0bb451aeac95a","In this paper, the idea of regression model is adopted to complete the fusion of multi-focus images through an end-to-end generative adversarial network (GAN). In the generator part, image features are extracted through multi-branch connection and dense connection technology. In the process of extracting high-dimensional image features, the ECA module is embedded to improve the capability of network. In the discriminator part, the idea of relative GAN is used to predict the relative authenticity between images. Due to the idea and reasonable network construction, the method proposed in this paper can obtain good results of image fusion. And the experimental results demonstrate that the one can also obtain fine results in objective evaluation, which is better than the compared algorithms. © 2022 SPIE.","Image enhancement; Image fusion; Regression analysis; Attention mechanisms; Branch connections; Colour image fusions; End to end; High-dimensional images; Image features; Multi-focus; Multifocus images; Network construction; Regression modelling; Generative adversarial networks","Attention mechanism; GAN; Image fusion; Multi-focus image","Conference paper","Final","","Scopus","2-s2.0-85141860433"
"Wang Z.; Ma Y.; Zhang Y.","Wang, Zhaobin (23669866600); Ma, Yikun (57556806300); Zhang, Yaonan (36633454400)","23669866600; 57556806300; 36633454400","Review of pixel-level remote sensing image fusion based on deep learning","2023","Information Fusion","90","","","36","58","22","10.1016/j.inffus.2022.09.008","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138457315&doi=10.1016%2fj.inffus.2022.09.008&partnerID=40&md5=d67c4dd95a0516903495153d998da707","The booming development of remote sensing images in many visual tasks has led to an increasing demand for obtaining images with more precise details. However, it is impractical to directly supply images that are simultaneously rich in spatial, spectral, and temporal information. One feasible solution is to fuse the information from multiple images. Since deep learning has achieved impressive achievements in image processing recently, this paper aims to provide a comprehensive review of deep learning-based methods for fusing remote sensing images at pixel-level. Specifically, we first introduce some traditional methods with their main limitations. Meanwhile, a brief presentation is made on four basic deep learning models commonly used in the field. On this basis, the research progress of these models in spatial information fusion and spatio-temporal fusion are reviewed. The current status on these models is further discussed with some coarse quantitative comparisons using several image quality metrics. After that, we find that deep learning models have not achieved overwhelming superiority over traditional methods but show great potential, especially the generative adversarial networks with its great capabilities in image generation and unsupervised learning should become a hot topic for future research. The joint use of different models should also be considered to fully extract multi-modal information. In addition, there is a lack of valuable research on pixel-level fusion of radar and optical images, requiring more attention in future work. © 2022 Elsevier B.V.","Deep learning; Geometrical optics; Information fusion; Learning systems; Optical data processing; Optical remote sensing; Pixels; Vision; Deep learning; Learning models; Pixel level; Remote sensing images; Remote-sensing; Spatial informations; Spatial temporals; Spectral information; Temporal information; Visual tasks; Image fusion","Deep learning; Image fusion; Remote sensing","Short survey","Final","","Scopus","2-s2.0-85138457315"
"Guo K.; Hu X.; Li X.","Guo, Kai (57214691999); Hu, Xiaohan (57221326506); Li, Xiongfei (23393374900)","57214691999; 57221326506; 23393374900","MMFGAN: A novel multimodal brain medical image fusion based on the improvement of generative adversarial network","2022","Multimedia Tools and Applications","81","4","","5889","5927","38","10.1007/s11042-021-11822-y","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122068277&doi=10.1007%2fs11042-021-11822-y&partnerID=40&md5=5b0edee71a12707cb7716133690e6031","In recent years, the multimodal medical imaging assisted diagnosis and treatment technology has developed rapidly. In brain disease diagnosis, CT-SPECT, MRI-PET and MRI-SPECT fusion images are more favored by brain doctors because they contain both soft tissue structure information and organ metabolism information. Most of the previous medical image fusion algorithms are the migration of other types of image fusion methods and such operations often lose the features of the medical image itself. This paper proposes a multimodal medical image fusion model based on the residual attention mechanism of the generative adversarial network. In the design of the generator, we construct the residual attention mechanism block and the concat detail texture block. After source images are concatenated to a matrix , the matrix is put into two blocks at the same time to extract information such as size, shape, spatial location and texture details. The obtained features are put into the merge block to reconstruct the image. The obtained reconstructed image and source images are respectively put into two discriminators for correction to obtain the final fused image. The model has been experimented on the images of three databases and achieved good fusion results. Qualitative and quantitative evaluations prove that the model is superior to other comparison algorithms in terms of image fusion quality and detail information retention. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Computerized tomography; Deep learning; Diagnosis; Discriminators; Image enhancement; Image fusion; Image reconstruction; Image texture; Magnetic resonance imaging; Medical imaging; Textures; Attention mechanisms; Concat detail texture block; Deep learning; Dual discriminator; matrix; Medical image fusion; Multi-modal; Residual attention mechanism block; Source images; Texture blocks; Generative adversarial networks","Concat detail texture block; Deep learning; Dual discriminator; Medical image fusion; Residual attention mechanism block","Article","Final","","Scopus","2-s2.0-85122068277"
"Shamsolmoali P.; Zareapoor M.; Granger E.; Zhou H.; Wang R.; Celebi M.E.; Yang J.","Shamsolmoali, Pourya (56350053200); Zareapoor, Masoumeh (56349635100); Granger, Eric (7004286338); Zhou, Huiyu (23062556900); Wang, Ruili (55825442900); Celebi, M. Emre (55667346700); Yang, Jie (15039078800)","56350053200; 56349635100; 7004286338; 23062556900; 55825442900; 55667346700; 15039078800","Image synthesis with adversarial networks: A comprehensive survey and case studies","2021","Information Fusion","72","","","126","146","20","10.1016/j.inffus.2021.02.014","51","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101926388&doi=10.1016%2fj.inffus.2021.02.014&partnerID=40&md5=56bb72e09612e58483b608ab6e94140f","Generative Adversarial Networks (GANs) have been extremely successful in various application domains such as computer vision, medicine, and natural language processing. Moreover, transforming an object or person to a desired shape become a well-studied research in the GANs. GANs are powerful models for learning complex distributions to synthesize semantically meaningful samples. However, there is a lack of comprehensive review in this field, especially lack of a collection of GANs loss-variant, evaluation metrics, remedies for diverse image generation, and stable training. Given the current fast GANs development, in this survey, we provide a comprehensive review of adversarial models for image synthesis. We summarize the synthetic image generation methods, and discuss the categories including image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We organize the literature based on their base models, developed ideas related to architectures, constraints, loss functions, evaluation metrics, and training datasets. We present milestones of adversarial models, review an extensive selection of previous works in various categories, and present insights on the development route from the model-based to data-driven methods. Further, we highlight a range of potential future research directions. One of the unique features of this review is that all software implementations of these GAN methods and datasets have been collected and made available in one place at https://github.com/pshams55/GAN-Case-Study. © 2021 Elsevier B.V.","Medicine; Natural language processing systems; Surveys; Adversarial networks; Data-driven methods; Evaluation metrics; Future research directions; NAtural language processing; Software implementation; Synthetic image generation; Training data sets; Image processing","Classification; GANs; Image fusion; Image synthesis; Image-to-image translation","Short survey","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101926388"
"Wang M.; Meng X.; Shao F.; Fu R.","Wang, Mengyao (57210968797); Meng, Xiangchao (56158755000); Shao, Feng (7006717672); Fu, Randi (14821950200)","57210968797; 56158755000; 7006717672; 14821950200","SAR-Assisted Optical Remote Sensing Image Cloud Removal Method Based on Deep Learning; [基于深度学习的SAR辅助下光学遥感图像去云方法]","2021","Guangxue Xuebao/Acta Optica Sinica","41","12","1228002","","","","10.3788/AOS202141.1228002","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113278742&doi=10.3788%2fAOS202141.1228002&partnerID=40&md5=9b89574ba859279c7a4f3fd7c4491b52","The existing deep learning based SAR-assisted cloud removal methods do not take full into account the texture and spectral information of the optical images, which results in blurring and spectral loss. In this paper, we constructed a data set for SAR-assisted cloud removal based on the Sentinel-1 and Sentinel-2 satellite images in Yuhang District of Hangzhou. In addition, we established a conditional generative adversarial network (cGAN) based model by fully considering the details, texture, and color information of optical remote sensing images, achieving information recovery and reconstruction in the case of optical images covered by thin clouds, fog, and thick clouds. The results show that the proposed method outperforms other methods in SAR-assisted cloud removal. © 2021, Chinese Lasers Press. All right reserved.","Geometrical optics; Image texture; Learning systems; Radar imaging; Remote sensing; Textures; Adversarial networks; Cloud removal; Color information; Information recovery; Optical image; Optical remote sensing; Satellite images; Spectral information; Deep learning","Cloud removal; Conditional generative adversarial network (cGAN); Image fusion; Optical image; Remote sensing; Synthetic aperture radar (SAR) image","Article","Final","","Scopus","2-s2.0-85113278742"
"Liu Q.; Meng X.; Shao F.; Li S.","Liu, Qiang (57208497463); Meng, Xiangchao (56158755000); Shao, Feng (57551562000); Li, Shutao (7409240361)","57208497463; 56158755000; 57551562000; 7409240361","PSTAF-GAN: Progressive Spatio-Temporal Attention Fusion Method Based on Generative Adversarial Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5408513","","","","10.1109/TGRS.2022.3161563","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127042760&doi=10.1109%2fTGRS.2022.3161563&partnerID=40&md5=986ff628c27874e02078a0dee456c263","Spatio-temporal fusion aims to integrate multisource remote sensing images with complementary high spatial and temporal resolutions, so as to obtain time-series high spatial resolution fused images. Currently, deep learning (DL)-based spatio-temporal fusion methods have received broad attention. However, on one hand, most of the existing DL-based methods train the model in a band-by-band manner, ignoring the correlations among bands. On the other hand, the general coarse spatio-temporal changes in low spatial resolution images (e.g., MODIS) calculated at the pixel domain cannot completely cover the fine spatio-temporal changes in high spatial resolution images (e.g., Landsat), due to complex surface features and the general large spatial resolution ratio between fine and coarse images. Besides, the existing DL-based spatio-temporal fusion methods are insufficient in exploring multiscale information by only stacking convolutional kernels with different sizes. To alleviate the above challenges, we propose a progressive spatio-temporal attention fusion model in a multiband training manner based on generative adversarial network (PSTAF-GAN). Specifically, we design a flexible multiscale feature extraction architecture to extract multiscale feature hierarchies. Then, spatio-temporal changes are calculated on the feature domain in different feature hierarchies. Besides, a spatio-temporal attention fusion architecture is proposed to fuse the spatio-temporal changes and ground details in a coarse-to-fine manner, which can explore multiscale information more sufficient and gradually recover the target image. The results of quantitative and qualitative experiments on two publicly available benchmark datasets show that the proposed PSTAF-GAN can achieve the best performance compared with the state-of-the-art methods.  © 1980-2012 IEEE.","Benchmarking; Deep learning; Image fusion; Image resolution; Remote sensing; Fusion methods; Generative adversarial network; High spatial resolution; Mul-ti-level feature; Remote-sensing; Spatio-temporal; Spatio-temporal attention fusion; Spatio-temporal changes; Spatio-temporal fusions; Weight sharing; remote sensing; spatiotemporal analysis; Generative adversarial networks","Generative adversarial network (GAN); multilevel feature; remote sensing; spatio-temporal attention fusion; weight sharing","Article","Final","","Scopus","2-s2.0-85127042760"
"Fu Y.; Wu X.-J.; Durrani T.","Fu, Yu (57222104064); Wu, Xiao-Jun (56191888600); Durrani, Tariq (56897622400)","57222104064; 56191888600; 56897622400","Image fusion based on generative adversarial network consistent with perception","2021","Information Fusion","72","","","110","125","15","10.1016/j.inffus.2021.02.019","28","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101935759&doi=10.1016%2fj.inffus.2021.02.019&partnerID=40&md5=3a7aa53c269ae7b65bcdda4117ebc2d6","Deep learning is a rapidly developing approach in the field of infrared and visible image fusion. In this context, the use of dense blocks in deep networks significantly improves the utilization of shallow information, and the combination of the Generative Adversarial Network (GAN) also improves the fusion performance of two source images. We propose a new method based on dense blocks and GANs, and we directly insert the input image-visible light image in each layer of the entire network. We use structural similarity and gradient loss functions that are more consistent with perception instead of mean square error loss. After the adversarial training between the generator and the discriminator, we show that a trained end-to-end fusion network – the generator network – is finally obtained. Our experiments show that the fused images obtained by our approach achieve good score based on multiple evaluation indicators. Further, our fused images have better visual effects in multiple sets of contrasts, which are more satisfying to human visual perception. © 2021 Elsevier B.V.","Deep learning; Image enhancement; Mean square error; Adversarial networks; Evaluation indicators; Fusion performance; Human visual perception; Infrared and visible image; Structural similarity; Visible light images; Visual effects; Image fusion","Dense block; Generative adversarial networks; Image fusion; Infrared image; Visible image","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85101935759"
"Min L.; Cao S.; Zhao H.; Liu P.","Min, Li (15048479800); Cao, Sijian (57668739700); Zhao, Huaici (35319203100); Liu, Pengfei (57201678471)","15048479800; 57668739700; 35319203100; 57201678471","Infrared and visible image fusion using improved generative adversarial networks; [改进生成对抗网络实现红外与可见光图像融合]","2022","Hongwai yu Jiguang Gongcheng/Infrared and Laser Engineering","51","4","20210291","","","","10.3788/IRLA20210291","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129491968&doi=10.3788%2fIRLA20210291&partnerID=40&md5=30b7f767008e36654f4a29ab54e95231","The infrared and visible image fusion technology can provide both the thermal radiation information of infrared images and the texture detail information of visible images. It has a wide range of applications in the fields of intelligent monitoring, target detection and tracking. The two type of images are based on different imaging principles. How to integrate the advantages of each type of image and ensure that the image will not distorted is the key to the fusion technology. Traditional fusion methods only superimpose images information and ignore the semantic information of images. To solve this problem, an improved generative adversarial network was proposed. The generator was designed with two branches of part detail feature and global semantic feature to capture the detail and semantic information of source images; the spectral normalization module was introduced into the discriminator, which would solve the problem that traditional generation adversarial networks were not easy to train and accelerates the network convergence; the perceptual loss was introduced to maintain the structural similarity between the fused image and source images, and further improve the fusion accuracy. The experimental results show that the proposed method is superior to other representative methods in subjective evaluation and objective indicators. Compared with the method based on the total variation model, the average gradient and spatial frequency are increased by 55.84% and 49.95%, respectively. Copyright ©2022 Infrared and Laser Engineering. All rights reserved.","Generative adversarial networks; Image enhancement; Infrared imaging; Semantic Web; Semantics; Textures; Image fusion technology; Imaging principle; Infrared and visible image; Intelligent monitoring; Radiation information; Semantics Information; Source images; Spectral normalization; Target detection and tracking; Visible image; Image fusion","Generative adversarial network; Image fusion; Semantic information; Spectral normalization","Article","Final","","Scopus","2-s2.0-85129491968"
"Luo Y.; Nie D.; Zhan B.; Li Z.; Wu X.; Zhou J.; Wang Y.; Shen D.","Luo, Yanmei (57223424973); Nie, Dong (57188806186); Zhan, Bo (57221803799); Li, Zhiang (57223440128); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Wang, Yan (56039981100); Shen, Dinggang (7401738392)","57223424973; 57188806186; 57221803799; 57223440128; 57221065403; 21234416400; 56039981100; 7401738392","Edge-preserving MRI image synthesis via adversarial network with iterative multi-scale fusion","2021","Neurocomputing","452","","","63","77","14","10.1016/j.neucom.2021.04.060","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105886086&doi=10.1016%2fj.neucom.2021.04.060&partnerID=40&md5=3bf0dc3ddd0c791de62a6b067f575e13","Magnetic resonance imaging (MRI) is a major imaging technique for studying neuroanatomy. By applying different pulse sequences and parameters, different modalities can be generated regarding the same anatomical structure, which can provide complementary information for diagnosis. However, limited by the scanning time and related cost, multiple different modalities are often not available for the same patient in clinic. Recently, many methods have been proposed for cross-modality MRI synthesis, but most of them only consider pixel-level differences between the synthetic and ground-truth images, ignoring the edge information, which is critical to provide clinical information. In this paper, we propose a novel edge-preserving MRI image synthesis method with iterative multi-scale feature fusion based generative adversarial network (EP_IMF-GAN). Particularly, the generator consists of a shared encoder and two specific decoders to carry out different tasks: 1) a primary task aiming to generate the target modality and 2) an auxiliary task aiming to generate the corresponding edge image of target modality. We assume that infusing the auxiliary edge image generation task can help preserve edge information and learn better latent representation features through the shared encoder. Meanwhile, an iterative multi-scale fusion module is embedded in the primary decoder to fuse supplementary information of feature maps at different scales, thereby further improving quality of the synthesized target modality. Experiments on the BRATS dataset indicate that our proposed method is superior to the state-of-the-art image synthesis approaches in both qualitative and quantitative measures. Ablation study further validates the effectiveness of the proposed components. © 2021 Elsevier B.V.","Decoding; Image fusion; Iterative methods; Signal encoding; Adversarial networks; Edge image; Edge information; Edge preserving; Generative adversarial network; Images synthesis; Iterative multi-scale fusion; Magnetic resonance imaging; Multiscale fusion; Pulse sequence; article; controlled study; human; nuclear magnetic resonance imaging; quantitative analysis; synthesis; Magnetic resonance imaging","Edge-preserving; Generative Adversarial Networks (GAN); Image synthesis; Iterative multi-scale fusion (IMF); Magnetic Resonance Imaging (MRI)","Article","Final","","Scopus","2-s2.0-85105886086"
"Liu X.; Wang R.; Huo H.; Yang X.; Li J.","Liu, Xiaowen (57720287400); Wang, Renhua (57221059573); Huo, Hongtao (56527849900); Yang, Xin (8659596100); Li, Jing (57207844651)","57720287400; 57221059573; 56527849900; 8659596100; 57207844651","An attention-guided and wavelet-constrained generative adversarial network for infrared and visible image fusion","2023","Infrared Physics and Technology","129","","104570","","","","10.1016/j.infrared.2023.104570","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147252794&doi=10.1016%2fj.infrared.2023.104570&partnerID=40&md5=6648153d09568db6ab583a13e0be714f","The GAN-based infrared and visible image fusion methods have gained ever-increasing attention due to its effectiveness and superiority. However, the existing methods adopt the global pixel distribution of source images as the basis for discrimination, which fails to focus on the key modality information. Moreover, the dual-discriminator based methods suffer from the confrontation between the discriminators. To this end, we propose an attention-guided and wavelet-constrained GAN for infrared and visible image fusion (AWFGAN). In this method, two unique discrimination strategies are designed to improve the fusion performance. Specifically, we introduce the spatial attention modules (SAM) into the generator to obtain the spatial attention maps, and then the attention maps are utilized to force the discrimination of infrared images to focus on the target regions. In addition, we extend the discrimination range of visible information to the wavelet subspace, which can force the generator to restore the high-frequency details of visible images. Ablation experiments demonstrate the effectiveness of our method in eliminating the confrontation between discriminators. And the comparison experiments on public datasets demonstrate the effectiveness and superiority of the proposed method. © 2023 Elsevier B.V.","Generative adversarial networks; Image compression; Image fusion; Infrared imaging; Fusion performance; Image fusion methods; Infrared and visible image; Infrared image; Pixel distribution; Source images; Spatial attention; Target regions; Visible image; Wavelets transform; Wavelet transforms","Image fusion; Infrared image; Spatial attention; Visible image; Wavelet transform","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85147252794"
"Wang J.; Yu L.; Tian S.; Wu W.; Zhang D.","Wang, Jing (57881254200); Yu, Long (55272883600); Tian, Shengwei (35119846500); Wu, Weidong (57226092151); Zhang, Dezhi (57274431700)","57881254200; 55272883600; 35119846500; 57226092151; 57274431700","AMFNet: An attention-guided generative adversarial network for multi-model image fusion","2022","Biomedical Signal Processing and Control","78","","103990","","","","10.1016/j.bspc.2022.103990","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135717696&doi=10.1016%2fj.bspc.2022.103990&partnerID=40&md5=29ea0e17723b6d3111b2aede88316f6e","Most of the existing image fusion methods fail to retain sufficient salient information, lack focuses on the most discriminative regions of the image, and often neglect the subjective perception of the human visual system. To address these problems, we propose an attention-guided generative adversarial network (AMFNet) for multi-model image fusion. The generator network of AMFNet consists of three parts: an attention network for capturing long-range dependencies in the internal representations of images, an information refinement network for obtaining image feature maps, and a fusion network for merging the attention network and the information refinement network. In addition, the convolutional block attention module is introduced to force the discriminator to focus on the most discriminative regions of the multi-modal source images. The results of qualitative and quantitative experiments conducted on numerous public datasets demonstrate that the proposed method outperforms other methods on visual effects and retains more detail information about the images. © 2022 Elsevier Ltd","Convolution; Image fusion; Convolutional block attention module; Human Visual System; Image features; Image fusion methods; Internal representation; Long-range dependencies; Model images; Multi-modelling; Self-attention; Subjective perceptions; article; attention; attention network; human; human experiment; quantitative analysis; Generative adversarial networks","Convolutional block attention module; Generative adversarial network; Image fusion; Self-attention","Article","Final","","Scopus","2-s2.0-85135717696"
"Xiong Z.; Xu H.; Li W.; Cai Z.","Xiong, Zuobin (57214689413); Xu, Honghui (57222165564); Li, Wei (57248804000); Cai, Zhipeng (8941786700)","57214689413; 57222165564; 57248804000; 8941786700","Multi-Source Adversarial Sample Attack on Autonomous Vehicles","2021","IEEE Transactions on Vehicular Technology","70","3","9360457","2822","2835","13","10.1109/TVT.2021.3061065","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101755068&doi=10.1109%2fTVT.2021.3061065&partnerID=40&md5=8bf1daf9feee4f5cefc0540ec0357770","Deep learning has an impressive performance of object detection and classification for autonomous vehicles. Nevertheless, the essential vulnerability of deep learning models to adversarial samples makes the autonomous vehicles suffer severe security and safety issues. Although a number of works have been proposed to study adversarial samples, only a few of them are designated for the scenario of autonomous vehicles. Moreover, the state-of-the-art attack models only focus on a single data source without considering the correlation among multiple data sources. To fill this blank, we propose two multi-source adversarial sample attack models, including the parallel attack model and the fusion attack model to simultaneously attack the image and LiDAR perception systems in the autonomous vehicles. In the parallel attack model, adversarial samples are generated from the original image and LiDAR data separately. In the fusion attack model, the adversarial samples of image and LiDAR can be generated from a low-dimension vector at the same time by fully exploring data correlation for data fusion and adversarial sample generation. Through comprehensive real-data experiments, we validate that our proposed models are more powerful and efficient to break down the perception systems of autonomous vehicles compared with the state-of-the-art. Furthermore, we simulate possible attack scenarios in Vehicular Ad hoc Networks (VANETs) to evaluate the attack performance of our proposed methods. © 1967-2012 IEEE.","Deep learning; Image fusion; Object detection; Optical radar; Vehicular ad hoc networks; Attack scenarios; Data correlations; Dimension vectors; Multiple data sources; Perception systems; Sample generations; State of the art; Vehicular Adhoc Networks (VANETs); Autonomous vehicles","Adversarial examples; generative adversarial networks; multi-source data; vehicular ad hoc networks","Article","Final","","Scopus","2-s2.0-85101755068"
"Zhou H.; Hou J.; Wu W.; Zhang Y.; Wu Y.; Ma J.","Zhou, Huabing (55447554500); Hou, Jilei (57222092435); Wu, Wei (57222093753); Zhang, Yanduo (55993581700); Wu, Yuntao (55993578900); Ma, Jiayi (26638975600)","55447554500; 57222092435; 57222093753; 55993581700; 55993578900; 26638975600","Infrared and Visible Image Fusion Based on Semantic Segmentation; [基于语义分割的红外和可见光图像融合]","2021","Jisuanji Yanjiu yu Fazhan/Computer Research and Development","58","2","","436","443","7","10.7544/issn1000-1239.2021.20200244","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101302743&doi=10.7544%2fissn1000-1239.2021.20200244&partnerID=40&md5=c5a8a15eb72bdff37870bc9cd8e01845","Infrared images can distinguish targets from their backgrounds due to the difference in thermal radiation even in poor lighting conditions. By contrast, visible images can represent texture details with high spatial resolution. Meanwhile, both of infrared and visible images preserve corresponding semantic information. Therefore, infrared and visible image fusion should keep both radiation information of the infrared image and texture details of the visible image; additionally, it needs to reserve the semantic information of both. Semantic segmentation can transform the source images into the masks with semantic information. In this paper, an infrared and visible image fusion method is proposed based on semantic segmentation. It can overcome the shortcomings that the existing fusion methods are not specific to different regions. Considering the specific information for different regions of infrared and visible images, we design two loss functions for different regions to improve the quality of fused image under the framework of generative adversarial network. Firstly, we gain the masks of the infrared images with semantic information by semantic segmentation; then we use the masks to divide the infrared and visible images into infrared target area, infrared background area, visible target area, and visible background area. Secondly, we employ different methods to fuse the target and background area, respectively. Finally, we combine the two regions to obtain the final fused image. The experiment shows that the proposed method outperforms state-of-the-art, where our results have higher contrast in the target area and richer texture details in the background area. © 2021, Science Press. All right reserved.","Image enhancement; Image fusion; Image segmentation; Infrared imaging; Semantics; Textures; Adversarial networks; High spatial resolution; Infrared and visible image; Radiation information; Semantic information; Semantic segmentation; Specific information; Target and background; Image texture","Image fusion; Infrared image; Mask; Semantic segmentation; Visible image","Article","Final","","Scopus","2-s2.0-85101302743"
"Tian Y.; Chen Y.; Diming W.; Shaoguang Y.; Wandeng M.; Chao W.; Xu C.; Long Y.","Tian, Yangyang (57192386579); Chen, Yuanhui (57413513300); Diming, Wan (57477455200); Shaoguang, Yuan (57478215800); Wandeng, Mao (57477582100); Chao, Wang (57477962200); Xu, Chunmei (57416006700); Long, Yifan (57220119520)","57192386579; 57413513300; 57477455200; 57478215800; 57477582100; 57477962200; 57416006700; 57220119520","Augmentation Method for anti-vibration hammer on power transimission line based on CycleGAN","2022","International Journal of Image and Data Fusion","13","4","","362","381","19","10.1080/19479832.2022.2033855","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125767255&doi=10.1080%2f19479832.2022.2033855&partnerID=40&md5=30b8942f0897f20a24c3cf46f4dfcbdb","Checking the status of the power grid is very important. However, the low occurrence of defects in an actual power grid makes it difficult to collect training samples, which affects the training of defect-detection models. In this study, we proposed a method for enhancing the defective image of a power grid based on cycle-consistent adversarial networks (CycleGAN). The defective image sample dataset was expanded by fusing artificial defective samples, converted from defect-free components of samples with the trained CycleGAN model and updating its corresponding label file. Comparing the accuracy of the object detection model trained by the augmented dataset, we found a 2%–3% Average Precision (AP) improvement over baseline, and the fusing method of histogram specification reaches the best performance. In conclusion, the generative adversarial network (GAN) and its variants have considerable potential for dataset augmentation as well as scope for further improvement. © 2022 Informa UK Limited, trading as Taylor & Francis Group.","Defects; Electric power transmission networks; Generative adversarial networks; Image enhancement; Object detection; Object recognition; Antivibration; Augmentation methods; Cyclegan; Data augmentation; Detection models; Objects detection; Power; Power grids; Power inspection; Training sample; artificial neural network; data processing; image processing; model; power generation; Image fusion","CycleGAN; Data augmentation; image fusion; object detection; power inspection","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85125767255"
"Li Q.; Lu L.; Li Z.; Wu W.; Liu Z.; Jeon G.; Yang X.","Li, Qilei (57202858223); Lu, Lu (56734964600); Li, Zhen (57202992643); Wu, Wei (56386371900); Liu, Zheng (57192441116); Jeon, Gwanggil (15022497800); Yang, Xiaomin (9237988500)","57202858223; 56734964600; 57202992643; 56386371900; 57192441116; 15022497800; 9237988500","Coupled GAN with Relativistic Discriminators for Infrared and Visible Images Fusion","2021","IEEE Sensors Journal","21","6","8733843","7458","7467","9","10.1109/JSEN.2019.2921803","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101752907&doi=10.1109%2fJSEN.2019.2921803&partnerID=40&md5=fa2ae111658d9078e3db1d5901010be2","Infrared and visible images are a pair of multi-source multi-sensors images. However, the infrared images lack structural details and visible images are impressionable to the imaging environment. To fully utilize the meaningful information of the infrared and visible images, a practical fusion method, termed as RCGAN, is proposed in this paper. In RCGAN, we introduce a pioneering use of the coupled generative adversarial network to the field of image fusion. Moreover, the simple yet efficient relativistic discriminator is applied to our network. By doing so, the network converges faster. More importantly, different from the previous works in which the label for generator is either infrared image or visible image, we innovatively put forward a strategy to use a pre-fused image as the label. This is a technical innovation, which makes the process of generating fused images no longer out of thin air, but from 'existence' to 'excellent.' The extensive experiments demonstrate the proposed RCGAN can produce a faithful fused image, which can efficiently persevere the rich texture from visible images and thermal radiation information from infrared images. Compared with traditional methods, it successfully avoids the complex manual designed fusion rules, and also shows a clear advantages over other deep learning-based fusion methods.  © 2001-2012 IEEE.","Deep learning; Discriminators; Infrared imaging; Textures; Adversarial networks; Fusion methods; Infrared and visible image; Multi-Sources; Radiation information; Structural details; Technical innovation; Visible image; Image fusion","coupled generative adversarial network; deep learning; Image fusion; infrared image; relativistic discriminator; visible image","Article","Final","","Scopus","2-s2.0-85101752907"
"Sun X.; Hu S.; Ma X.","Sun, Xiuyi (57765756700); Hu, Shaohai (7404286949); Ma, Xiaole (57193220596)","57765756700; 7404286949; 57193220596","Infrared and visible image fusion based on unsupervised deep learning; [基 于 无 监 督 深 度 学 习 的 红 外 与 可 见 光 图 像 融 合]","2022","Hangkong Xuebao/Acta Aeronautica et Astronautica Sinica","43","","726938","","","","10.7527/S1000-6893.2022.26938","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142899584&doi=10.7527%2fS1000-6893.2022.26938&partnerID=40&md5=e49442b5c08cb2fe3b910ec8ddddea1c","Most of the known infrared and visible image fusion models based on convolutional neural networks make little use of the hierarchical features from visible images，thus resulting in insufficient texture details of the fused image. Inspired by the residual network and dense network，an image fusion algorithm is proposed based on unsupervised deep learning to solve the problem of insufficient texture information of fused images. The residual dense block has a continuous storage mechanism to retain the feature information of each layer to the maximum extent. The design of local residual fusion and global residual fusion is conducive to learning the structural texture in the image. In addition，to better preserve the detailed texture in visible images，the generative adversarial network is introduced to perform unsupervised learning on the dataset. Subjective and objective experiments show that the proposed algorithm achieves not only a good visual fusion effect，but also more edge texture information of the fused image. Compared with that of the existing state-of-the-art algorithms，the objective evaluation index of the method proposed is also greatly improved. © 2022 AAAS Press of Chinese Society of Aeronautics and Astronautics. All rights reserved.","Convolutional neural networks; Deep learning; Image fusion; Image texture; Infrared imaging; Textures; Unsupervised learning; Convolutional neural network; Dense network; Fused images; Image fusion model; Infrared and visible image; Infrared image; Model-based OPC; Residual dense network; Texture information; Visible image; Generative adversarial networks","generative adversarial network; image fusion; infrared image; residual dense network; unsupervised learning; visible image","Article","Final","","Scopus","2-s2.0-85142899584"
"Gao M.; Zhou Y.; Zhai W.; Zeng S.; Li Q.","Gao, Mingliang (26634962800); Zhou, Yi’nan (58078506800); Zhai, Wenzhe (57252998400); Zeng, Shuai (58079513100); Li, Qilei (57202858223)","26634962800; 58078506800; 57252998400; 58079513100; 57202858223","SaReGAN: a salient regional generative adversarial network for visible and infrared image fusion","2023","Multimedia Tools and Applications","","","","","","","10.1007/s11042-023-14393-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146862705&doi=10.1007%2fs11042-023-14393-2&partnerID=40&md5=2051842f336d33d8f066a8e91d48f344","Multispectral image fusion plays a crucial role in smart city environment safety. In the domain of visible and infrared image fusion, object vanishment after fusion is a key problem which restricts the fusion performance. To address this problem, a novel Salient Regional Generative Adversarial Network GAN (SaReGAN) is presented for infrared and VIS image fusion. The SaReGAN consists of three parts. In the first part, the salient regions of infrared image are extracted by visual saliency map and the information of these regions is preserved. In the second part, the VIS image, infrared image and salient information are merged thoroughly in the generator to gain a pre-fused image. In the third part, the discriminator attempts to differentiate the pre-fused image and VIS image, in order to learn details from VIS image based on the adversarial mechanism. Experimental results verify that the SaReGAN outperforms other state-of-the-art methods in quantitative and qualitative evaluations. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image fusion; Infrared imaging; Smart city; Fused images; Fusion performance; Image-based; Infrared image fusions; Learn+; Multi-spectral image fusions; Saliency map; Salient regions; Visible and infrared image; Visual saliency; Generative adversarial networks","Generative adversarial network; Image fusion; Salient region; Smart city; Visible and infrared image","Article","Article in press","","Scopus","2-s2.0-85146862705"
"Wang L.; Mi J.; Qin P.; Lin S.; Gao Y.; Liu Y.","Wang, Lifang (57142669800); Mi, Jia (57223151845); Qin, Pinle (23393704900); Lin, Suzhen (7407607523); Gao, Yuan (57221270944); Liu, Yang (57211087412)","57142669800; 57223151845; 23393704900; 7407607523; 57221270944; 57211087412","Medical image fusion using improved U-Net3 + and cross-modal attention blocks; [改进 U-Net3 + 与跨模态注意力块的医学图像融合]","2022","Journal of Image and Graphics","27","12","","3622","3636","14","10.11834/jig.211066","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141843211&doi=10.11834%2fjig.211066&partnerID=40&md5=c3559bd6efb4b3930e1729a92083bcf9","Objective Multi-modal medical image fusion tends to get more detailed features beyond single modal defection. The deep features of lesions are essential for clinical diagnosis. However, current multi-modal medical image fusion methods are challenged to capture the deep features. The integrity of fusion image is affected when extracting features from a single modal only. In recent years, deep learning technique is developed in image processing, and generative adversarial network (GAN), as an important branch of deep learning, has been widely used in image fusion. GAN not only reduces information loss but also highlights key features through information confrontation between different original images. The deep feature extraction ability of current multi-modal medical image fusion methods is insufficient and some modal features are ignored. We develop a medical image fusion method based on the improved U-Net3 + and cross-modal attention blocks in combination with dual discriminator generation adversative network (UC-DDGAN). Method The UC-DDGAN image fusion modal is mainly composed of full scale connected U-Net3 + network structure and two modal features integrated cross-modal attention blocks. The U-Net3 + network can extract deep features, and the cross-modal attention blocks can extract different modal features in terms of the correlation between images. Computed tomography (CT) and magnetic resonance (MR) can be fused through the trained UC-DDGAN, which has a generator and two discriminators. The generator is used to extract the deep features of image and generate fusion image. The generator includes two parts of feature extraction and feature fusion. In the feature extraction part, the encoding and decoding of coordinated U-Net3 + network complete feature extraction. In the coding stage, the input image is down-sampled four times to extract features, and cross-modal attention blocks are added after each down-sampling to obtain two modal composite feature maps. Cross-modal attention block not only calculates self-attention in a single image, but also extends the calculation of attention to two modes. By calculating the relationship between local features and global features of the two modes, the fusion image preserves the overall of image information. In the decoding stage, the decoder receives the feature maps in the context of the same scale encoder and the maximum pooling based smaller scale encoder and the dual up-sampling based large scale encoder. Then, 64 filters with a size of 3 × 3 are linked to the feature image channels. The synthesized feature maps of each layer are combined and up-sampled. After 1 × 1 convolution for channel dimension reduction, the feature maps are fused into the image which contains depth features on the full scale of the two modes. In the feature fusion part, to obtain the fusion image with deep details and the key features of the two modes, the two feature maps are synthesized and concatenated via the concat layer, and five convolution modules for channel dimension reduction layer by layer. The discriminator is focused on leveraging original image from fusion image via the distribution of different samples. To identify the credibility of the input images, the characteristics of different modal images are integrated with different distribution. In addition, gradient loss is melted into the loss function calculation, and the weighted sum of gradient loss and pixel loss are as the loss function to optimize the generator. Result To validate the quality of fusion image, UC-DDGAN is compared to five popular multi-modal image fusion methods, including Laplasian pyramid (LAP), pulse-coupled neural network (PCNN), convolutional neural network (CNN), fusion generative adversarial network (FusionGAN) and dual discriminator generative adversarial network (DDcGAN). The edges of fusion results obtained by LAP are fuzzy in qualitative, which are challenged to observe the contour of the lesion. The brightness of fusion results obtained by PCNN is too low. The CNN-based fusion results are lack of deep details, and the internal details cannot be observed. The fusion results obtained by using FusionGAN pay too much attention to MR modal images and lose the bone information of CT images. The edges of fusion results obtained by DDcGAN are not smooth enough. 1) The fusion results of cerebral infarction disease obtained by UC-DDGAN can show clear brain gullies, 2) the fusion results of cerebral apoplexy disease can clarify color features, 3) the fusion results of cerebral tumor disease show brain medulla and bone information are fully reserved, and 4) the fusion results of cerebrovascular disease contain deep-based information of brain lobes. To evaluate the performance of UC-DDGAN, quantitative results are based on the selected thirty typical image pairs and five classical methods. The fusion image generated by UC-DDGAN is improved on spatial frequency (SF), structural similarity (SSIM), edge information transfer factor (QAB/F), correlation coefficient (CC), and the sum of the correlations of differences (SCD). 1) SF is improved by 5. 87% in contrastive to DDcGAN, 2) SSIM is improved by 8% compared to FusionGAN, 3) QAB/F is improved by 12. 66%, CC is improved by 14. 47% and 4) SCD is improved by 14. 48% in comparison with DDcGAN, respectively. Conclusion A dual discriminator generation adversative network based (UC-DDGAN-based) medical image fusion method is developed based on the improved U-Net3 + and cross-modal attention blocks. The fusion image generated by UC-DDGAN is linked to richer deep features and key features of two modes. © 2022 Editorial and Publishing Board of JIG. All rights reserved.","","cross-modal attention block; dual discriminator generation adversative network; gradient loss; multimodal medical image fusion; U-Net3 +","Article","Final","","Scopus","2-s2.0-85141843211"
"Wei J.; Zou H.; Sun L.; Cao X.; Li M.; He S.; Liu S.","Wei, Juan (57310676500); Zou, Huanxin (8366222500); Sun, Li (57311427800); Cao, Xu (57937375000); Li, Meilin (57209947350); He, Shitian (57222956907); Liu, Shuo (57219450775)","57310676500; 8366222500; 57311427800; 57937375000; 57209947350; 57222956907; 57219450775","Generative Adversarial Network for SAR-to-Optical Image Translation with Feature Cross-Fusion Inference","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","6025","6028","3","10.1109/IGARSS46834.2022.9884166","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140400952&doi=10.1109%2fIGARSS46834.2022.9884166&partnerID=40&md5=383264dae2890fd2f04bbd087fad3418","The translation of synthetic aperture radar (SAR) to optical images provides a new solution for the interpretation of SAR images. Most of the existing translation networks are based on generative adversarial networks and use 9-residual blocks or U-Net structures in the feature inference phase. Both structures cause a large amount of information lost during the conversion of SAR image features to optical features, making the outline of the translated image blurred or semantic information lost. Aiming at this problem, this paper proposes a cross-fusion inference network structure, which preserves both high-resolution features and low-resolution features in the whole process of feature inference. Our proposed method broadens the network horizontally while deepening it vertically and improving the image translation performance. The experiments conducted on the public dataset sen1-2 show that the proposed method is superior to other networks. © 2022 IEEE.","Generative adversarial networks; Geometrical optics; Image enhancement; Image fusion; Radar imaging; Semantics; Cross-fusion inference structure; Generative adversarial network; Image translation; Information lost; Large amounts; Net structures; New solutions; Optical image; Synthetic aperture radar images; Synthetic aperture radar-to-optical image translation; Synthetic aperture radar","Cross-fusion inference structure; Generative adversarial network (GAN); SAR-to-optical image translation","Conference paper","Final","","Scopus","2-s2.0-85140400952"
"Narute B.; Bartakke P.","Narute, Bharati (57194393730); Bartakke, Prashant (6505731279)","57194393730; 6505731279","Brain MRI and CT Image Fusion Using Generative Adversarial Network","2022","Communications in Computer and Information Science","1568 CCIS","","","97","109","12","10.1007/978-3-031-11349-9_9","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135027822&doi=10.1007%2f978-3-031-11349-9_9&partnerID=40&md5=3fb15af5e9496e31569036cf8e30c513","The generative adversarial networks (GAN), complete model, is used to fuse computed tomography (CT) and magnetic resonance imaging (MRI) brain images in this research paper. To create a resultant fused image with bone structures from CT images and soft tissues from MRI images, our method develops an adversarial game between a generator and a discriminator. To make a stable training process, we use GAN instead of conventional fusion methods, and our architecture can handle different resolutions of multi-source medical images. The efficacy of the proposed procedure is demonstrated using several evaluation metrics. The proposed algorithms provide the best fused images without distortion and false artefacts. Comparison of proposed methods is done with the conventional techniques. The images obtained by fusing both sources’ content with the help of the above algorithm gives the best with respect to visualization and diagnosis of the condition. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Brain mapping; Computerized tomography; Deep learning; Diagnosis; Generative adversarial networks; Image fusion; A-stable; Bone structure; Brain images; Computed tomography; Computed tomography images; Deep learning; Fused images; Research papers; Soft tissue; Training process; Magnetic resonance imaging","CT; Deep learning; GAN; Image fusion; MRI","Conference paper","Final","","Scopus","2-s2.0-85135027822"
"Li C.; Kong W.; Xue J.; Wang Z.; Chang L.","Li, Chi (57666160200); Kong, Weiwei (36458476300); Xue, Jiawei (57666160300); Wang, Ze (57214954724); Chang, Liang (57223228077)","57666160200; 36458476300; 57666160300; 57214954724; 57223228077","Image blind deblurring networks with back-projection feature fusion","2023","Signal, Image and Video Processing","","","","","","","10.1007/s11760-022-02420-y","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146613959&doi=10.1007%2fs11760-022-02420-y&partnerID=40&md5=58c07fe413332cc2b2bfa2d886b4dc8d","Aiming at the problem of image motion blur caused by handheld camera jitter and object motion in the process of collecting photos, a generative adversarial network (GAN) based on feature fusion of back projection is proposed for blind image deblurring. Firstly, the generator network is established by using U-Net structure, and a feature fusion residual block based on back projection is designed according to the error feedback principle, which solves the problem of saving spatial information in U-Net structure. Secondly, the self-attention module is introduced into the generator network to extract the feature map that pays more attention to detail. Finally, the combination of perceptual loss, mean square error loss and relative generative adversarial loss effectively alleviates the mode collapse problem of traditional GAN and improves the stability of model training. The experimental results show that the peak signal-to-noise ratio and structural similarity of this method on GoPro data set are 30.183 dB and 0.941, respectively, and 26.962 and 0.837 on the Kohler dataset, with the shortest running time, which are better than the existing state-of-the-art methods. The restored image is clearer good visual results and richer in texture details, which can effectively improve the image deblurring effect. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.","Discriminators; Image enhancement; Image fusion; Mean square error; Signal to noise ratio; Textures; Attention mechanisms; Backprojections; Blind deblurring; Features fusions; Image deblurring; Net networks; Net structures; Relative discriminator; Self-attention mechanism; U-net network; Generative adversarial networks","Back projection; Generative adversarial networks; Image deblurring; Relative discriminator; Self-attention mechanism; u-net network","Article","Article in press","","Scopus","2-s2.0-85146613959"
"Huang Y.; Li W.","Huang, Yuping (57220072367); Li, Weisheng (36067507500)","57220072367; 36067507500","A review of medical image fusion methods; [医学图像融合方法综述]","2023","Journal of Image and Graphics","28","1","","118","143","25","10.11834/jig.220603","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147180254&doi=10.11834%2fjig.220603&partnerID=40&md5=476f61a4e053d5a07bad1273eb81c21b","Multimodal medical-fused images are essential to more comprehensive and accurate medical image descriptions for various clinical applications like medical diagnosis, treatment planning, and surgical navigation. However, single-modal medical images is challenged to deal with diagnose disease types and localize lesions due to its variety and complexity of disease types. As a result, multimodal medical image fusion methods are focused on obtaining medical images with rich information in clinical applications. Medical-based imaging techniques are mainly segmented into electromagnetic energy-based and acoustic energy-based. To achieve the effect of real-time imaging and provide dynamic images, the latter one uses the multiple propagation speed of ultrasound in different media. Current medical image fusion techniques are mainly concerned of static images in terms of electromagnetic energy imaging techniques. For example, it is related to some key issues like X-ray computed tomography imaging, single photon emission computed tomography, positron emission tomography and magnetic resonance imaging. We review recent literature-relevant based on the current status of medical image fusion methods. Our critical analysis can divide current medical image fusion techniques into two categories: 1) traditional methods and 2) deep learning methods. Nowadays, spatial domain and frequency domain-based algorithms are very proactive for traditional medical image fusion methods. The spatial domain techniques are implemented for the evaluation of image element values via prior pixel-level strategies, and the images-fused can realize less spatial distortion and a lower signal-to-noise ratio. The spatial domain-based methods are included some key aspects like 1) simple min/max, 2) independent component analysis, 3) principal component analysis, 4) weighted average, 5) simple average, 6) fuzzy logic, and 7) cloud model. The fusion process of spatial domain-based methods is quite simple, and its algorithm complexity can lower the computation cost. It also has a relatively good performance in alleviating the spectral distortion of fused images. However, the challenging issue is called for their fusion results better in terms of clarity, contrast and continuous lower spatial resolution. In the frequency domain, the input image is first converted from the null domain to the frequency domain via Fourier transform computation, and the fusion algorithm is then applied to the image-converted to obtain the final fused image, followed by the inversed Fourier transform. The commonly-used fusion algorithms in the frequency domain are composed of 1) pyramid transform, 2) wavelet transform and 3) multi-scale geometric transform fusion algorithms. This multi-level decomposition based methods can enhance the detail retention of the fused image. The output fusion results contain high spatial resolution and high quality spectral components. However, this type of algorithm is derived from a fine-grained fusion rule design as well. The deep learning-based methods are mainly related to convolutional neural networks (CNN) and generative adversarial networks (GAN), which can avoid fine-grained fusion rule design, reduce the manual involvement in the process, and their stronger feature extraction capability enables their fusion results to retain more source image information. The CNN can be used to process the spatial and structural information effectively in the neighborhood of the input image. It consists of a series of convolutional layers, pooling layers and fully connected layers. The convolution layer and pooling layer can extract the features in the source image, and the fully connected layer can complete the mapping from the features to the final output. In CNN, image fusion is regarded as a classification problem, corresponding to the process of feature extraction, feature option and output prediction. The fusion task is targeted on image transformation, activity level measurement and fusion rule design as well. Different from CNN, GAN network can be used to model saliency information in medical images through adversarial learning mechanism. GAN is a generative model with two multilayer networks, the first network mentioned is a generator-used to generate pseudo data, and the second following network is a discriminator-used to classify images into real data and pseudo data. The back-propagation-based training mode can improve the ability of GAN to distinguish between real data and generated data. Although GAN is not as widely used in multi-model medical image fusion (MMIF) as CNN, it has the potential for in-depth research. A completed overview of existing multimodal medical image databases and fusion quality evaluation metrics is developed further. Four open-source freely accessible medical image databases are involved in, such as the open access series of imaging studies (OASIS) dataset, the cancer immunome atlas (TCIA) dataset, the whole brain atlas (AANLIB) dataset, and the Alzheimer’ s disease neuroimaging initiative (ANDI) dataset. And, a gene database for green fluorescent protein and phase contrast images are included as well, called the John Innes centre (JIC) dataset. Our critical review is based on the summary of 25 commonly-used medical image fusion result evaluation indicators in four types of metrics: 1) information theory-based; 2) image feature-based; 3) image structural similarity-based and 4) human visual perception-based, as well as 22 fusion algorithms for medical image datasets in recent years. The pros and cons of the algorithms are analyzed in terms of the technical-based comparison, fusion modes and evaluation indexes of each algorithm. In addition, our review is carried out on a large number of experiments to compare the performance of deep learning-based and traditional medical image fusion methods. Source images of three modal pairs are tested qualitatively and quantitatively via 22 multimodal medical image fusion algorithms. For qualitative analysis, the brightness, contrast and distortion of the fused image are observed based on the human vision system. For quantitative-based analysis, 15 objective evaluation indexes are used. By analyzing the qualitative and quantitative results, some critical analyses are discussed based on the current situation, challenging issues and future direction of medical image fusion techniques. Both of the traditional and deep learning methods have promoted fusion performance to a certain extent. More medical image fusion methods with good fusion effect and high model robustness are illustrated in the context of the algorithm optimization and the enrichment of medical image data sets. And, the two technical fields will continue to be developed towards the common research trends of expanding the multi-facet and multi-case medical images, proposing effective indicators suitable for medical image fusion, and deepening the research scope of image fusion. © 2023 Editorial and Publishing Board of JIG. All rights reserved.","","deep learning; medical image database; medical image fusion; multimodel medical image; quality evaluation metrics","Review","Final","","Scopus","2-s2.0-85147180254"
"Fang S.; Guo Q.; Cao Y.; Zhang J.","Fang, Shuai (7402422537); Guo, Qing (57938173300); Cao, Yang (57022583200); Zhang, Jing (57211055913)","7402422537; 57938173300; 57022583200; 57211055913","A Two-Layers Super-Resolution Based Generation Adversarial Spatiotemporal Fusion Model","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","891","894","3","10.1109/IGARSS46834.2022.9883547","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140364937&doi=10.1109%2fIGARSS46834.2022.9883547&partnerID=40&md5=ca6a8cca3ffe4be9f310ce7f1b26e2b0","Remote sensing image spatiotemporal fusion (STF) algorism plays an important role by supplementing the lack of original high-resolution remote sensing satellite images in the study scenarios of dense time-series data. In recent years, the deep-learning-based STF algorithm has become a research hotspot with comparatively higher accuracy and robustness. However, due to the lack of sufficient high-quality images for training and the huge resolution gap between low-resolution images and high-resolution images, it is difficult to recover detailed information, especially for areas of land-cover change. In this paper, we propose a two-layers super-resolution based generation adversarial spatiotemporal fusion model(TLSRSTF) using smaller inputs to reduce pressure on data requirements and a mutual affine convolution to reduce model parameters. Specifically, we only use a pair of high-resolution and low-resolution images and a high-resolution image at any time. A spatial degradation consistency is constructed to adaptively determine the ratio of two layers of the super-resolution STF model. The quantitative and qualitative experimental results on public spatiotemporal fusion datasets demonstrate our superiority over the state-of-the-art methods. © 2022 IEEE.","Convolution; Deep learning; Image fusion; Optical resolving power; Remote sensing; Fusion model; High resolution remote sensing; High-resolution images; Low resolution images; Mutual affine convolution; Remote sensing images; Remote sensing satellites; Spatio-temporal fusions; Superresolution; Two-layer; Generative adversarial networks","Generative Adversarial Networks(GAN); Mutual Affine Convolution; Spatiotemporal Fusion","Conference paper","Final","","Scopus","2-s2.0-85140364937"
"Pan Y.; Pi D.; Chen J.; Meng H.","Pan, Yue (57204029531); Pi, Dechang (14038259000); Chen, Junfu (57214761375); Meng, Han (57204031360)","57204029531; 14038259000; 57214761375; 57204031360","FDPPGAN: remote sensing image fusion based on deep perceptual patchGAN","2021","Neural Computing and Applications","33","15","","9589","9605","16","10.1007/s00521-021-05724-1","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099603089&doi=10.1007%2fs00521-021-05724-1&partnerID=40&md5=5a05b6e4aedf435e2681fcbe252cb412","Remote sensing satellites can simultaneously capture high spatial resolution panchromatic (PAN) images and low spatial resolution multispectral (MS) images. Pan-sharpening in the fusion of remote sensing images aims to generate high-resolution MS images by integrating the spatial information of PAN images and the spectral characteristics of MS images. In this study, a novel deep perceptual patch generative adversarial network (FDPPGAN) was proposed to solve the pan-sharpening problem. First, a perception generator was constructed, it included, a matching module, which can process as input images of different resolutions, a fusion module, a reconstruction module based on the residual structure, and a module for the extracting perceptual features. Second, patch discriminator was utilized to convert the dichotomy of the sample into that multiple partial images of the same size to ensure that the generated results can retain more detailed features. Finally, the loss function of FDPPGAN comprised perceptual feature loss, content loss, generator loss, and discriminator loss. Experiments on the QuickBird and WorldView datasets demonstrated that the proposed algorithm is superior to state-of-the-art algorithms in subjective and objective indexes. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.","Image resolution; Remote sensing; Different resolutions; High spatial resolution; Multispectral images; Panchromatic (Pan) image; Remote sensing images; Remote sensing satellites; Spectral characteristics; State-of-the-art algorithms; Image fusion","Image fusion; MS image; PAN image; patchGAN; Perceptual features","Article","Final","","Scopus","2-s2.0-85099603089"
"Wu J.; Liu X.; Lu Q.; Lin Z.; Qin N.; Shi Q.","Wu, Junjun (55657611700); Liu, Xilin (57223272189); Lu, Qinghua (36158484100); Lin, Zeqin (57220867925); Qin, Ningwei (57724421500); Shi, Qingwu (57487490300)","55657611700; 57223272189; 36158484100; 57220867925; 57724421500; 57487490300","FW-GAN: Underwater image enhancement using generative adversarial network with multi-scale fusion","2022","Signal Processing: Image Communication","109","","116855","","","","10.1016/j.image.2022.116855","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137151642&doi=10.1016%2fj.image.2022.116855&partnerID=40&md5=524e8946e56792fec72579c338e74cfa","Underwater robots have broad applications in many fields such as ocean exploration, ocean pasture and environmental monitoring. However, due to the inference of light scattering and absorption, selective color attenuation, suspended particles and other complex factors in the underwater environment, it is difficult for robot vision sensors to obtain high-quality underwater image signal, which is the bottleneck problem that restricts the visual perception of underwater robots. In this paper, we propose a multi-scale fusion generative adversarial network named Fusion Water-GAN (FW-GAN) to enhance the underwater image quality. The proposed model has four convolution branches, these branches refine the features of the three prior inputs and encode the original input, then fuse prior features using the proposed multi-scale fusion connections, and finally use the channel attention decoder to generate satisfactory enhanced results. We conduct qualitative and quantitative comparison experiments on real-world and synthetic distorted underwater image datasets under various degradation conditions. The results show that compared with the recent state-of-the-art underwater image enhancement methods, our proposed method achieves higher quantitative metrics scores and better generalization capability. In addition, the ablation study demonstrated the contribution of each component. © 2022 Elsevier B.V.","Deep learning; Image enhancement; Image fusion; Light scattering; Robots; Underwater acoustics; Broad application; Deep learning; Environmental Monitoring; Generalization capability; Light scattering and absorptions; Multiscale fusion; Ocean exploration; Suspended particles; Underwater image enhancements; Underwater robots; Generative adversarial networks","Deep learning; Generalization capability; Generative adversarial network; Image enhancement; Underwater robot","Article","Final","","Scopus","2-s2.0-85137151642"
"Chen T.; Wang S.; Gao T.; Liu M.; Chen Y.","Chen, Ting (55687638000); Wang, Songtao (57287308700); Gao, Tao (57202469695); Liu, Mengni (57216646395); Chen, Youjing (57560759200)","55687638000; 57287308700; 57202469695; 57216646395; 57560759200","A Relativistic Average Generative Adversarial Network for Pan-Sharpening; [用于全色锐化的相对平均生成对抗网络]","2022","Hsi-An Chiao Tung Ta Hsueh/Journal of Xi'an Jiaotong University","56","3","","54","64","10","10.7652/xjtuxb202203006","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127499207&doi=10.7652%2fxjtuxb202203006&partnerID=40&md5=e4234d8eab146196a5a3fd0d2ecdc6dc","A relativistic average generative adversarial network for pan-sharpening (Pan-RaGAN) based on a deep learning algorithm is proposed to solve the problems that details of fusion result are easy to be lost due to insufficient feature extraction from original image in the pan-sharpening process and information redundancy is caused by ignoring spatial feature difference of different regions in image fusion process. Firstly, the improved dense block structure is utilized to extract the features of the original image in the generator, so as to make full use of the features from different layers of the original image and obtain better fusion results with more details. Secondly, a feature refinement module based on spatial attention mechanism is proposed for feature selection, which can make a better trade-off between retaining effective high-frequency information and eliminating redundant information. Furthermore, the image reconstruction module is utilized to fuse the refined features with the up-sampled low resolution multispectral images to preserve the spectral information. Finally, the relativistic average discriminator is utilized to improve the loss function of the network, and further optimize the fusion effect. Experimental results on Gao Fen-2 dataset and Quick Bird dataset and a comparison with the existing generative adversarial network for remote sensing image pan-sharpening show that the spectral angle mapper index of the proposed Pan-RaGAN network is reduced by 0.075 on average, which verifies the effectiveness of the proposed Pan-RaGAN network. © 2022, Editorial Office of Journal of Xi'an Jiaotong University. All right reserved.","Deep learning; Economic and social effects; Feature extraction; Image enhancement; Image fusion; Image reconstruction; Learning algorithms; Remote sensing; Attention mechanisms; Deep learning; Features extraction; Original images; Pan-sharpening; Process redundancy; Relativistic average generative adversarial network; Relativistics; Spatial attention; Spatial attention mechanism; Generative adversarial networks","Deep learning; Image fusion; Pan-sharpening; Relativistic average generative adversarial network; Spatial attention mechanism","Article","Final","","Scopus","2-s2.0-85127499207"
"Diao W.; Zhang F.; Sun J.; Xing Y.; Zhang K.; Bruzzone L.","Diao, Wenxiu (57406352000); Zhang, Feng (57207771718); Sun, Jiande (12645161300); Xing, Yinghui (57195357426); Zhang, Kai (56451954400); Bruzzone, Lorenzo (7006892410)","57406352000; 57207771718; 12645161300; 57195357426; 56451954400; 7006892410","ZeRGAN: Zero-Reference GAN for Fusion of Multispectral and Panchromatic Images","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","","","","10.1109/TNNLS.2021.3137373","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122586976&doi=10.1109%2fTNNLS.2021.3137373&partnerID=40&md5=3812d8c2e0d267127499fd308b68d534","In this article, we present a new pansharpening method, a zero-reference generative adversarial network (ZeRGAN), which fuses low spatial resolution multispectral (LR MS) and high spatial resolution panchromatic (PAN) images. In the proposed method, zero-reference indicates that it does not require paired reduced-scale images or unpaired full-scale images for training. To obtain accurate fusion results, we establish an adversarial game between a set of multiscale generators and their corresponding discriminators. Through multiscale generators, the fused high spatial resolution MS (HR MS) images are progressively produced from LR MS and PAN images, while the discriminators aim to distinguish the differences of spatial information between the HR MS images and the PAN images. In other words, the HR MS images are generated from LR MS and PAN images after the optimization of ZeRGAN. Furthermore, we construct a nonreference loss function, including an adversarial loss, spatial and spectral reconstruction losses, a spatial enhancement loss, and an average constancy loss. Through the minimization of the total loss, the spatial details in the HR MS images can be enhanced efficiently. Extensive experiments are implemented on datasets acquired by different satellites. The results demonstrate that the effectiveness of the proposed method compared with the state-of-the-art methods. The source code is publicly available at https://github.com/RSMagneto/ZeRGAN. IEEE","Generative adversarial networks; Image enhancement; Image resolution; Generative adversarial network; Generator; High spatial resolution; Multi-spectral; Multispectral images; Pan-sharpening; Panchromatic  image; Spatial resolution; Training data; Zero-reference training.; Image fusion","Generative adversarial network (GAN); Generative adversarial networks; Generators; image fusion; multispectral image; panchromatic (PAN) image; Pansharpening; Satellites; Spatial resolution; Training; Training data; zero-reference training.","Article","Article in press","","Scopus","2-s2.0-85122586976"
"Nguyen T.M.; Yoo M.","Nguyen, Tri Minh (57368648100); Yoo, Myungsik (55984448800)","57368648100; 55984448800","Wasserstein Generative Adversarial Network for Depth Completion with Anisotropic Diffusion Depth Enhancement","2022","IEEE Access","10","","","6867","6877","10","10.1109/ACCESS.2022.3142916","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123309849&doi=10.1109%2fACCESS.2022.3142916&partnerID=40&md5=75c5355091916763a431da8f2b4c8d1a","The objective of depth completion is to generate a dense depth map by upsampling a sparse one. However, irregular sparse patterns or the lack of groundtruth data caused by unstructured data make depth completion extremely challenging. Sensor fusion using both RGB and LIDAR sensors can help produce a more reliable context with higher accuracy. Compared with previous approaches, this method takes semantic segmentation images as additional input and develops an unsupervised loss function. Thus, when combined with supervised depth loss, the depth completion problem is considered as semi-supervised learning. We used an adapted Wasserstein Generative Adversarial Network architecture instead of applying the traditional autoencoder approach and post-processing process to preserve valid depth measurements received from the input and further enhance the depth value precision of the results. Our proposed method was evaluated on the KITTI depth completion benchmark, and its performance in depth completion was proven to be competitive. © 2013 IEEE.","Benchmarking; Computer architecture; Image fusion; Network architecture; Optical radar; Semantic Segmentation; Semantics; Supervised learning; Wind power; Anisotropic Diffusion; Dense depth map; Depth completion; Depth enhancement; Diffusion depth; Features extraction; Generator; LIDAR sparse depth; Upsampling; Wasserstein GAN; Generative adversarial networks","Anisotropic diffusion; Depth completion; Generative adversarial network; LIDAR sparse depth; Wasserstein GAN","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123309849"
"Tang L.; Zhang H.; Xu H.; Ma J.","Tang, Linfeng (57223158028); Zhang, Hao (57215014270); Xu, Han (57201056465); Ma, Jiayi (26638975600)","57223158028; 57215014270; 57201056465; 26638975600","Deep learning-based image fusion: a survey; [基于深度学习的图像融合方法综述]","2023","Journal of Image and Graphics","28","1","","3","36","33","10.11834/jig.220422","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147204739&doi=10.11834%2fjig.220422&partnerID=40&md5=ef8fb9eb40bb95e22d8f40f5b432bba4","Image fusion aims to integrate complementary information from multi-source images into a single fused image to characterize the imaging scene and facilitate the subsequent vision tasks further. In recent years, it has been a concern in the field of image processing, especially in artificial intelligence-related industries like intelligent medical service, autonomous driving, smart photography, security surveillance, and military monitoring. Moreover, the growth of deep learning has been promoting deep learning-based image fusion algorithms. In particular, the emergence of advanced techniques, such as the auto-encoder, generative adversarial network, and Transformer, has led to a qualitative leap in image fusion performance. However, a comprehensive review and analysis of state-of-the-art deep learning-based image fusion algorithms for different fusion scenarios are required to be realized. Thus, we develop a systematic and critical review to explore the developments of image fusion in recent years. First, a comprehensive and systematic introduction of the image fusion field is presented from the following three aspects: 1) the development of image fusion technology, 2) the prevailing datasets, and 3) the common evaluation metrics. Then, more extensive qualitative experiments, quantitative experiments, and running efficiency evaluations of representative image fusion methods are conducted on the public datasets to compare their performance. Finally, the summary and challenges in the image fusion community are highlighted. In particular, some prospects are recommended further in the field of image fusion. First of all, from the perspective of fusion scenarios, the existing image fusion methods can be divided into three categories, i. e., multi-modal image fusion, digital photography image fusion, and remote sensing image fusion. Specifically, multi-modal image fusion is composed of infrared and visible image fusion as well as medical image fusion, and digital photography image fusion consists of multi-exposure image fusion as well as multi-focus image fusion. Multi-spectral and panchromatic image fusion can be as one of the key aspects for remote sensing image fusion. In addition, the domain of deep learning-based image fusion algorithms can be classified into the auto-encoder based (AE-based) fusion framework, convolutional neural network based (CNN-based) fusion framework, and generative adversarial network based (GAN-based) fusion framework from the aspect of network architectures. The AE-based fusion framework achieves the feature extraction and image reconstruction by a pre-trained auto-encoder, and accomplishes deep feature fusion via manual fusion strategies. To clarify feature extraction, fusion, and image reconstruction, the CNN-based fusion framework is originated from detailed network structures and loss functions. The GAN-based framework defines the image fusion problem as an adversarial game between the generators and discriminators. From the perspective of the supervision paradigm, the deep learning fusion methods can also be categorized into three classes, i. e., unsupervised, self-supervised, and supervised. The supervised methods leverage ground truth value to guide the training processes, and the unsupervised approaches construct loss function via constraining the similarity between the fusion result and source images. The self-supervised algorithms are associated with the AE-based framework in common. Our critical review is focused on the main concepts and discussions of the characteristics of each method for different fusion scenarios from the perspectives of the network architecture and supervision paradigm. Especially, we summarize the limitations of different fusion algorithms and provide some recommendations for further research. Secondly, we briefly introduce the popular public datasets and provide the interfaces-related to download them for each specific image fusion scenario. Then, we present the common evaluation metrics in the image fusion field from two aspects: regular-based evaluation metrics and specific-based metrics designed for pan-sharpening. The generic metrics can be utilized to evaluate multi-modal and digital photography image fusion algorithms of those are entropy-based, correlation-based, image feature-based, image structure-based, and human visual perception-based metrics in total. Some of the generic metrics, such as peak signal-to-noise ratio (PSNR), correlation coefficient (CC), structural similarity index measure (SSIM), and visual information fidelity (VIF), are also used for the quantitative assessment of pan-sharpening. The specific metrics designed for pan-sharpening consist of no-reference metrics and full-reference metrics that employ the full-resolution image as the reference image, i. e., ground truth. Thirdly, we present the qualitative/quantitative results, and average running times of representative alternatives for various fusion missions. Finally, this review has critically analyzed the conclusion, highlights the challenges in the image fusion community, and carried out forecasting analysis, such as non-registered image fusion, high-level vision task-driven image fusion, cross-resolution image fusion, real-time image fusion, color image fusion, image fusion based on physical imaging principles, image fusion under extreme conditions, and comprehensive evaluation metrics, etc. The methods, datasets, and evaluation metrics mentioned are linked at: https://github.com/Linfeng-Tang/Image-Fusion. © 2023 SAE-China. All rights reserved.","","deep learning; digital photography; image fusion; multi-modal; remote sensing imagery","Review","Final","","Scopus","2-s2.0-85147204739"
"Wang J.; Yu L.; Tian S.","Wang, Jing (57224989198); Yu, Long (55272883600); Tian, Shengwei (35119846500)","57224989198; 55272883600; 35119846500","MsRAN: a multi-scale residual attention network for multi-model image fusion","2022","Medical and Biological Engineering and Computing","60","12","","3615","3634","19","10.1007/s11517-022-02690-1","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140118523&doi=10.1007%2fs11517-022-02690-1&partnerID=40&md5=f9461256211b5a4b92d9916eddf4d71e","Fusion is a critical step in image processing tasks. Recently, deep learning networks have been considerably applied in information fusion. But the significant limitation of existing image fusion methods is the inability to highlight typical regions of the source image and retain sufficient useful information. To address the problem, the paper proposes a multi-scale residual attention network (MsRAN) to fully exploit the image feature. Its generator network contains two information refinement networks and one information integration network. The information refinement network extracts feature at different scales using convolution kernels of different sizes. The information integration network, with a merging block and an attention block added, prevents the underutilization of information in the intermediate layers and forces the generator to focus on salient regions in multi-modal source images. Furthermore, in the phase of model training, we add an information loss function and adopt a dual adversarial structure, enabling the model to capture more details. Qualitative and quantitative experiments on publicly available datasets validate that the proposed method provides better visual results than other methods and retains more detail information. Graphical abstract: [Figure not available: see fulltext.] © 2022, International Federation for Medical and Biological Engineering.","Deep learning; Generative adversarial networks; Image processing; Information fusion; Information retrieval; Attention mechanisms; Critical steps; Images processing; Information integration; Integration networks; Learning network; Model images; Multi-modelling; Multi-scales; Source images; article; attention; attention network; deep learning; image processing; loss of function mutation; quantitative analysis; Image fusion","Attention mechanism; Generative adversarial network; Image fusion; Multi-scale","Article","Final","","Scopus","2-s2.0-85140118523"
"Man Q.; Cho Y.-I.; Jang S.-G.; Lee H.-J.","Man, Qiaoyue (57226699313); Cho, Young-Im (15764374600); Jang, Seong-Geun (57739637800); Lee, Hae-Jeung (24436161800)","57226699313; 15764374600; 57739637800; 24436161800","Transformer-Based GAN for New Hairstyle Generative Networks","2022","Electronics (Switzerland)","11","13","2106","","","","10.3390/electronics11132106","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133346184&doi=10.3390%2felectronics11132106&partnerID=40&md5=ed03e0d3b60e0e1de4239f770320ef26","Traditional GAN-based image generation networks cannot accurately and naturally fuse surrounding features in local image generation tasks, especially in hairstyle generation tasks. To this end, we propose a novel transformer-based GAN for new hairstyle generation networks. The network framework comprises two modules: Face segmentation (F) and Transformer Generative Hairstyle (TGH) modules. The F module is used for the detection of facial and hairstyle features and the extraction of global feature masks and facial feature maps. In the TGH module, we design a transformer-based GAN to generate hairstyles and fix the details of the fusion part of faces and hairstyles in the new hairstyle generation process. To verify the effectiveness of our model, CelebA-HQ (Large-scale CelebFaces Attribute) and FFHQ (Flickr-Faces-HQ) are adopted to train and test our proposed model. In the image evaluation test used, FID, PSNR, and SSIM image evaluation methods are used to test our model and compare it with other excellent image generation networks. Our proposed model is more robust in terms of test scores and real image generation. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","convolutional neural network; face detection; generative adversarial networks; image fusion; transformer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133346184"
"Liao W.; Hu K.; Yang M.Y.; Rosenhahn B.","Liao, Wentong (56768729400); Hu, Kai (57226860721); Yang, Michael Ying (36015861500); Rosenhahn, Bodo (57203083760)","56768729400; 57226860721; 36015861500; 57203083760","Text to Image Generation with Semantic-Spatial Aware GAN","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","18166","18175","9","10.1109/CVPR52688.2022.01765","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139192930&doi=10.1109%2fCVPR52688.2022.01765&partnerID=40&md5=0e32927236c1958e2b174bbc0413a75a","Text-to-image synthesis (T2I) aims to generate photorealistic images which are semantically consistent with the text descriptions. Existing methods are usually built upon conditional generative adversarial networks (GANs) and initialize an image from noise with sentence embedding, and then refine the features with fine-grained word embedding iteratively. A close inspection of their generated images reveals a major limitation: even though the generated image holistically matches the description, individual image regions or parts of somethings are often not recognizable or consistent with words in the sentence, e.g. 'a white crown'. To address this problem, we propose a novel framework Semantic-Spatial Aware GAN for synthesizing images from input text. Concretely, we introduce a simple and effective Semantic-Spatial Aware block, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a semantic mask in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code available at https://github.com/wtliao/text2image. © 2022 IEEE.","Computer vision; Embeddings; Generative adversarial networks; Image fusion; Iterative methods; Embeddings; Image and video synthesis and generation; Image generations; Images synthesis; Learn+; Photorealistic images; Text images; Video generation; Video synthesis; Vision + language; Semantics","Image and video synthesis and generation; Vision + language","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85139192930"
"Shang C.; Li X.; Yin Z.; Li X.; Wang L.; Zhang Y.; Du Y.; Ling F.","Shang, Cheng (57209801137); Li, Xinyan (36523257300); Yin, Zhixiang (57205742028); Li, Xiaodong (55878368700); Wang, Lihui (57141007100); Zhang, Yihang (55658053900); Du, Yun (56420121700); Ling, Feng (56278268300)","57209801137; 36523257300; 57205742028; 55878368700; 57141007100; 55658053900; 56420121700; 56278268300","Spatiotemporal Reflectance Fusion Using a Generative Adversarial Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3065418","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103284108&doi=10.1109%2fTGRS.2021.3065418&partnerID=40&md5=29addda27e77aa23f573ebe809fcefef","The spatiotemporal reflectance fusion method is used to blend high-temporal and low-spatial resolution images with their low-temporal and high-spatial resolution counterparts that were previously acquired by various satellite sensors. Recently, a wide variety of learning-based solutions have been developed, but challenges remain. These solutions usually require two sets of data acquired before and after the prediction time, making them unsuitable for near-real-time predicting. The solutions are always trained band by band and thus do not consider the spectral correlation. High-resolution temporal changes are difficult to reconstruct accurately with the network structure used, which lowers the accuracy of the fusion result. To address these problems, this study proposes a novel spatiotemporal adaptive reflectance fusion model using a generative adversarial network (GASTFN). In GASTFN, an end-to-end network, including a generative and discriminative network, is simultaneously trained for all spectral bands. The proposed model can be applied to the one-pair case, consider the spectral correlation of each band, and improve the process of producing super-resolution imagery by adopting the discriminative network for image reflectance values rather than temporal changes in reflectance. The proposed model has been verified with two actual satellite data sets acquired in heterogeneous landscapes and areas with abrupt changes, with a comparison of the state-of-art methods. The results show that GASTFN can generate the most accurate fusion images with more detailed textures, more realistic spatial shapes, and higher accuracy, demonstrating that the GASTFN is effective for predicting near-real-time changes in image reflectance and preserves the most valuable spatial information. © 1980-2012 IEEE.","Forecasting; Image enhancement; Image resolution; Reflection; Textures; Adversarial networks; Discriminative networks; Heterogeneous landscapes; High spatial resolution; Spatial informations; Spatial resolution images; Spectral correlation; State-of-art methods; correlation; image resolution; network analysis; reflectance; satellite data; satellite imagery; spatiotemporal analysis; Image fusion","Generative adversarial network (GAN); spatiotemporal fusion; super-resolution; temporal changes","Article","Final","","Scopus","2-s2.0-85103284108"
"Fu J.; Li W.; Du J.; Xu L.","Fu, Jun (57219370906); Li, Weisheng (36067507500); Du, Jiao (55416429400); Xu, Liming (57210788847)","57219370906; 36067507500; 55416429400; 57210788847","DSAGAN: A generative adversarial network based on dual-stream attention mechanism for anatomical and functional image fusion","2021","Information Sciences","576","","","484","506","22","10.1016/j.ins.2021.06.083","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109466051&doi=10.1016%2fj.ins.2021.06.083&partnerID=40&md5=1b4e82208bea3a8d9be41e83090a1f05","In recent years, extensive multimodal medical image fusion algorithms have been proposed. However, existing methods are primarily based on specific transformation theories. There are many problems with existing algorithms, such as poor adaptability, low efficiency and blurry details. To address these problems, this paper proposes a generative adversarial network based on dual-stream attention mechanism (DSAGAN) for anatomical and functional image fusion. The dual-stream architecture and multiscale convolutions are utilized to extract deep features. In addition, the attention mechanism is utilized to further enhance the fused features. Then, the fusion images and multimodal input images are put into the discriminator. In the update stage of the discriminator, we expect to judge the multimodal images as real, and to judge the fusion images as fake. Furthermore, the fusion images are expected to be judged as real in the update stage of the generator, forcing the generator to improve the fusion quality. The training process continues until the generator and discriminator reach a Nash equilibrium. After training, the fusion images can be obtained directly after inputting anatomical and functional images. Compared with the reference algorithms, DSAGAN consumes less fusion time and achieves better objective metrics in terms of QAG, QEN and QNIQE. © 2021 Elsevier Inc.","Image enhancement; Medical imaging; Adversarial networks; Anatomical images; Attention mechanisms; Dual-stream attention; Functional images; Fusion image; Generative adversarial network; Multi-modal; Multimodal medical images; Network-based; Image fusion","Dual-stream attention; Generative adversarial network; Image fusion; Multimodal","Article","Final","","Scopus","2-s2.0-85109466051"
"Nandhini Abirami R.; Durai Raj Vincent P.M.; Srinivasan K.; Manic K.S.; Chang C.-Y.","Nandhini Abirami, R. (57212486754); Durai Raj Vincent, P.M. (55808710700); Srinivasan, Kathiravan (57192191217); Manic, K. Suresh (56049210600); Chang, Chuan-Yu (8076196000)","57212486754; 55808710700; 57192191217; 56049210600; 8076196000","Multimodal Medical Image Fusion of Positron Emission Tomography and Magnetic Resonance Imaging Using Generative Adversarial Networks","2022","Behavioural Neurology","2022","","6878783","","","","10.1155/2022/6878783","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128801534&doi=10.1155%2f2022%2f6878783&partnerID=40&md5=782b2986165120dde19116bfc1aebf56","Multimodal medical image fusion is a current technique applied in the applications related to medical field to combine images from the same modality or different modalities to improve the visual content of the image to perform further operations like image segmentation. Biomedical research and medical image analysis highly demand medical image fusion to perform higher level of medical analysis. Multimodal medical fusion assists medical practitioners to visualize the internal organs and tissues. Multimodal medical fusion of brain image helps to medical practitioners to simultaneously visualize hard portion like skull and soft portion like tissue. Brain tumor segmentation can be accurately performed by utilizing the image obtained after multimodal medical image fusion. The area of the tumor can be accurately located with the information obtained from both Positron Emission Tomography and Magnetic Resonance Image in a single fused image. This approach increases the accuracy in diagnosing the tumor and reduces the time consumed in diagnosing and locating the tumor. The functional information of the brain is available in the Positron Emission Tomography while the anatomy of the brain tissue is available in the Magnetic Resonance Image. Thus, the spatial characteristics and functional information can be obtained from a single image using a robust multimodal medical image fusion model. The proposed approach uses a generative adversarial network to fuse Positron Emission Tomography and Magnetic Resonance Image into a single image. The results obtained from the proposed approach can be used for further medical analysis to locate the tumor and plan for further surgical procedures. The performance of the GAN based model is evaluated using two metrics, namely, structural similarity index and mutual information. The proposed approach achieved a structural similarity index of 0.8551 and a mutual information of 2.8059.  © 2022 R. Nandhini Abirami et al.","Brain; Brain Neoplasms; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Positron-Emission Tomography; Article; comparative study; complex wavelet transform; feature extraction; Hilbert transform; human; image analysis; image segmentation; medical research; multimodal imaging; multimodal medical image fusion; nuclear magnetic resonance imaging; physician; positron emission tomography; wavelet transform; brain; brain tumor; diagnostic imaging; image processing; nuclear magnetic resonance imaging; positron emission tomography; procedures","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85128801534"
"Hoque M.R.U.; Wu J.; Kwan C.; Koperski K.; Li J.","Hoque, Md Reshad Ul (57215344852); Wu, Jian (57193141747); Kwan, Chiman (7201421216); Koperski, Krzysztof (6603540174); Li, Jiang (56226550100)","57215344852; 57193141747; 7201421216; 6603540174; 56226550100","ArithFusion: An Arithmetic Deep Model for Temporal Remote Sensing Image Fusion","2022","Remote Sensing","14","23","6160","","","","10.3390/rs14236160","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143761796&doi=10.3390%2frs14236160&partnerID=40&md5=6d162c4fc285294f57b7c7e89e83f5f4","Different satellite images may consist of variable numbers of channels which have different resolutions, and each satellite has a unique revisit period. For example, the Landsat-8 satellite images have 30 m resolution in their multispectral channels, the Sentinel-2 satellite images have 10 m resolution in the pan-sharp channel, and the National Agriculture Imagery Program (NAIP) aerial images have 1 m resolution. In this study, we propose a simple yet effective arithmetic deep model for multimodal temporal remote sensing image fusion. The proposed model takes both low- and high-resolution remote sensing images at (Formula presented.) together with low-resolution images at a future time (Formula presented.) from the same location as inputs and fuses them to generate high-resolution images for the same location at (Formula presented.). We propose an arithmetic operation applied to the low-resolution images at the two time points in feature space to take care of temporal changes. We evaluated the proposed model on three modality pairs for multimodal temporal image fusion, including downsampled WorldView-2/original WorldView-2, Landsat-8/Sentinel-2, and Sentinel-2/NAIP. Experimental results show that our model outperforms traditional algorithms and recent deep learning-based models by large margins in most scenarios, achieving sharp fused images while appropriately addressing temporal changes. © 2022 by the authors.","Antennas; Deep learning; Generative adversarial networks; Image fusion; Satellite imagery; Space optics; Deep learning; Generative adversarial network; HRNet; LANDSAT; Neural-networks; Remote sensing images; Remote-sensing; Satellite images; Superresolution; U-net; Remote sensing","deep learning; generative adversarial network (GAN); HRNet; image fusion; neural networks; remote sensing; super-resolution; U-Net","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85143761796"
"","","","Proceedings of 2022 12th Iranian/2nd International Conference on Machine Vision and Image Processing, MVIP 2022","2022","Iranian Conference on Machine Vision and Image Processing, MVIP","2022-February","","","","","282","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127394174&partnerID=40&md5=e5e180be2407e51c81a2e40666933f76","The proceedings contain 48 papers. The topics discussed include: an empirical study of the performance of different optimizers in the deep neural networks; underwater image enhancement using a light convolutional neural network and 2D histogram equalization; an augmented reality framework for eye muscle education; feature line based feature reduction of polarimetric-contextual feature cube for polarimetric sar classification; fast multi focus image fusion using determinant; computer-aided brain age estimation via ensemble learning of 3D convolutional neural networks; real-time facial expression recognition using facial landmarks and neural networks; a hybrid of inference and stacked classifiers to indoor scenes classification of RGB-D images; novel Gaussian mixture-based video coding for fixed background video streaming; compressed sensing MRI reconstruction using improved u-net based on deep generative adversarial networks; and towards fine-grained image classification with generative adversarial networks and facial landmark detection.","","","Conference review","Final","","Scopus","2-s2.0-85127394174"
"Yang J.; Zhou Y.; Zhao Y.; Wen J.","Yang, Jiachen (25959803600); Zhou, Yanshuang (57701474800); Zhao, Yang (57206607437); Wen, Jiabao (57754390800)","25959803600; 57701474800; 57206607437; 57754390800","Blind quality assessment of tone-mapped images using multi-exposure sequences","2022","Journal of Visual Communication and Image Representation","87","","103553","","","","10.1016/j.jvcir.2022.103553","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132425262&doi=10.1016%2fj.jvcir.2022.103553&partnerID=40&md5=611a84fd1b0fb18d05e9c2d84493b2c9","The tone mapping operator (TMO) enables high dynamic range (HDR) images to be presented on low dynamic range (LDR) consumer electronic devices. However, the results obtained by this method are not always ideal due to the reduced number of bits. In comparison, the multi-exposure image fusion (MEF) bypasses the intermediate HDR image composition and directly produces an image presented on standard devices. Inspired by this, this paper proposes a quality assessment method for tone-mapped image (TMI) based on generating multi-exposure sequences. Specifically, the method uses a generative adversarial network (GAN) to generate a set of sequences with different exposure levels based on the TMIs. Then a two-branch convolutional neural network (CNN) is used to extract features from the tone-mapped images and the multi-exposure reference sequences, respectively. Finally, the transformer is used to mine the intrinsic connections between TMIs and multi-exposure sequences and learn the mapping relationships from feature space to quality space. We conducted extensive experiments on the ESPL-LIVE HDR database. The applicability and effectiveness of the proposed method are verified by comparing and analyzing relevant features and model configurations with existing mainstream evaluation algorithms. © 2022 Elsevier Inc.","Convolutional neural networks; Image fusion; Image quality; Mapping; Blind quality assessments; High dynamic range; High dynamic range images; Image quality assessment; Low dynamic range; Mapped image; Multi exposure; Tone mapping operators; Tone-mapped image; Generative adversarial networks","High dynamic range; Image quality assessment (IQA); Tone-mapped image","Article","Final","","Scopus","2-s2.0-85132425262"
"Luo W.; Yang S.; Zhang W.","Luo, Wuyang (57210430769); Yang, Su (35319285900); Zhang, Weishan (14038263700)","57210430769; 35319285900; 14038263700","Photo-realistic image synthesis from lines and appearance with modular modulation","2022","Neurocomputing","503","","","81","91","10","10.1016/j.neucom.2022.06.007","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133948309&doi=10.1016%2fj.neucom.2022.06.007&partnerID=40&md5=780cfe4709585996fd0ba5e410273286","The image-to-image translation task has made significant progress by relying on conditional generative adversarial networks. However, for many tasks, multiple condition images are required. This paper considers a very classic application scenario, using lines and appearance to synthesize photo-realistic images, describing structure and appearance information, respectively, for example, generating realistic face images from portrait drawings and color scribbles, and generating photos from sketches and texture patches. The key to this type of task is how to fuse the two conditional information. We propose an image translation system driven by line and appearance images, introducing a modular architecture for condition fusion. Unlike the previous condition fusion schemes, its main body of the generator is composed of stacked modulation units (MUs). Here, structural features and appearance features are progressively incorporated via cascaded MUs, each of which pays attention to the local regions. The visualization experiment shows that such a scheme lets the network automatically learn to decompose the fusion process as multiple sub-steps in latent spaces. Our model produces higher quality results quantitatively and qualitatively compared to the state-of-the-art method on different tasks and datasets. The ablation study demonstrates the effectiveness of the MUs and intuitively explains the process of feature fusion through visualization. © 2022 Elsevier B.V.","Generative adversarial networks; Image fusion; Image processing; Textures; Visualization; Application scenario; Condition; Face images; Features fusions; Image translation; Image-to-image translation; Images synthesis; Modulars; Photo realistic image synthesis; Photorealistic images; article; attention; decomposition; drawing; quantitative analysis; synthesis; Modulation","Feature Fusion; Generative Adversarial Networks; Image Synthesis; Image-to-Image Translation","Article","Final","","Scopus","2-s2.0-85133948309"
"Dayarathna T.; Muthukumarana T.; Rathnayaka Y.; Denman S.; de Silva C.; Pemasiri A.; Ahmedt-Aristizabal D.","Dayarathna, Thisun (57479260800); Muthukumarana, Thamidu (57478566300); Rathnayaka, Yasiru (57479128100); Denman, Simon (56238954000); de Silva, Chathura (23033613700); Pemasiri, Akila (56875413900); Ahmedt-Aristizabal, David (57195974557)","57479260800; 57478566300; 57479128100; 56238954000; 23033613700; 56875413900; 57195974557","Privacy-Preserving in-bed pose monitoring: A fusion and reconstruction study","2023","Expert Systems with Applications","213","","119139","","","","10.1016/j.eswa.2022.119139","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141926062&doi=10.1016%2fj.eswa.2022.119139&partnerID=40&md5=9530b51f5048689dda7f8084c3b80a7c","Background and objectives: Recently, in-bed human pose estimation has attracted the interest of researchers due to its relevance to a wide range of healthcare applications. Compared to the general problem of human pose estimation, in-bed pose estimation has several inherent challenges, the most prominent being frequent and severe occlusions caused by bedding. In this paper we explore the effective use of images from multiple non-visual and privacy-preserving modalities such as depth, long-wave infrared (LWIR) and pressure maps for the task of in-bed pose estimation in two settings. First, we explore the effective fusion of information from different imaging modalities for better pose estimation. Secondly, we propose a framework that can estimate in-bed pose estimation when visible images are unavailable, and demonstrate the applicability of fusion methods to scenarios where only LWIR images are available. Method: We analyze and demonstrate the effect of fusing features from multiple modalities. For this purpose, we consider four different techniques: (1) Addition, (2) Concatenation, (3) Fusion via learned modal weights, and 4) End-to-end fully trainable approach; with a state-of-the-art pose estimation model. We also evaluate the effect of reconstructing a data-rich modality (i.e., visible modality) from a privacy-preserving modality with data scarcity (i.e., long-wavelength infrared) for in-bed human pose estimation. For reconstruction, we use a conditional generative adversarial network. Results: We conduct experiments on a publicly available dataset for feature fusion and visible image reconstruction. We conduct ablative studies across different design decisions of our framework. This includes selecting features with different levels of granularity, using different fusion techniques, and varying model parameters. Through extensive evaluations, we demonstrate that our method produces on par or better results compared to the state-of-the-art. Conclusion: The insights from this research offer stepping stones towards robust automated privacy-preserving systems that utilize multimodal feature fusion to support the assessment and diagnosis of medical conditions. © 2022 Elsevier Ltd","Generative adversarial networks; Image fusion; Image reconstruction; Infrared devices; Infrared radiation; Privacy-preserving techniques; Features fusions; Generative network; Human pose; Human pose estimations; Longwave infrared; Multi-modal; Multimodal human pose analyse; Pose-estimation; Privacy preserving; Visible image; Diagnosis","Feature fusion; Generative networks; Multimodal human pose analysis","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85141926062"
"Wang Z.; Shao W.; Chen Y.; Xu J.; Zhang X.","Wang, Zhishe (36139853000); Shao, Wenyu (57406972500); Chen, Yanlin (57573884600); Xu, Jiawei (55801927200); Zhang, Xiaoqin (35232030000)","36139853000; 57406972500; 57573884600; 55801927200; 35232030000","Infrared and Visible Image Fusion via Interactive Compensatory Attention Adversarial Learning","2022","IEEE Transactions on Multimedia","","","","1","13","12","10.1109/TMM.2022.3228685","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144747145&doi=10.1109%2fTMM.2022.3228685&partnerID=40&md5=2cca56d9553ba29555e35e2437e8efa3","The existing generative adversarial fusion methods generally concatenate source images or deep features, and extract local features through convolutional operations without considering their global characteristics, which tends to produce a limited fusion performance. Toward this end, we propose a novel interactive compensatory attention fusion network, termed ICAFusion. In particular, in the generator, we construct a multi-level encoder-decoder network with a triple path, and design infrared and visible paths to provide additional intensity and gradient information for the concatenating path. Moreover, we develop the interactive and compensatory attention modules to communicate their pathwise information, and model their long-range dependencies through a cascading channel-spatial model. The generated attention maps can more focus on infrared target perception and visible detail characterization, and are used to reconstruct the fusion image. Therefore, the generator takes full advantage of local and global features to further increase the representation ability of feature extraction and feature reconstruction. Extensive experiments illustrate that our ICAFusion obtains superior fusion performance and better generalization ability, which precedes other advanced methods in the subjective visual description and objective metric evaluation. Our codes will be public at <italic><uri>https://github.com/Zhishe-Wang/ICAFusion</uri></italic>. IEEE","Decoding; Discriminators; Extraction; Feature extraction; Image reconstruction; Adversarial learning; Attention compensation; Attention interaction; Decoding; Dual discriminator; Features extraction; Generator; Images reconstruction; Local feature; Task analysis; Generative adversarial networks","adversarial learning; attention compensation; attention interaction; Decoding; dual discriminators; Feature extraction; Generative adversarial networks; Generators; image fusion; Image reconstruction; Task analysis; Training","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85144747145"
"Yin H.; Xiao J.","Yin, Haitao (54421600200); Xiao, Jinghu (57918507300)","54421600200; 57918507300","Laplacian Pyramid Generative Adversarial Network for Infrared and Visible Image Fusion","2022","IEEE Signal Processing Letters","29","","","1988","1992","4","10.1109/LSP.2022.3207621","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139435701&doi=10.1109%2fLSP.2022.3207621&partnerID=40&md5=164900ed2f266a5ee6d64ff097558406","Generative adversarial network (GAN) has recently demonstrated a powerful tool for infrared and visible image fusion. However, existing methods extract the features incompletely, miss some textures, and lack the stability of training. To cope with these issues, this article proposes a novel image fusion Laplacian pyramid GAN (IF-LapGAN). Firstly, a generator is constructed which consists of shallow features extraction module, Laplacian pyramid module, and reconstruction module. Specifically, the Laplacian pyramid module is a pyramid-style encoder-decoder architecture, which progressively extracts the multi-scale features. Moreover, the attention module is equipped in the decoder to effectively decode the salient features. Then, two discriminators are adopted to discriminate the fused image and two different modalities respectively. To improve the stability of adversarial learning, we propose to develop another side supervised loss based on the side pre-trained fusion network. Extensive experiments show that IF-LapGAN achieves 3.27%, 27.28%, 6.32%, 1.39%, 3.14%, 1.15% and 1.07% improvement gains in terms of Q_{NMI}, Q_{M}, Q_{Yang}, Q^{AB/F}, MI, VIF, and FMI, respectively, compared with the second best values.  © 1994-2012 IEEE.","Decoding; Extraction; Image fusion; Image processing; Laplace transforms; Textures; Attention module; Decoding; Encoder-decoder architecture; Features extraction; Fused images; Generator; Infrared and visible image; Laplacian Pyramid; Multi-scale features; Salient features; Generative adversarial networks","attention module; generative adversarial network; Image fusion; Laplacian pyramid","Article","Final","","Scopus","2-s2.0-85139435701"
"Zhuang W.-H.; Tang X.-G.; Zhang B.; Yuan G.-M.","Zhuang, Wen-Hua (57463195100); Tang, Xiao-Gang (56493570400); Zhang, Binquan (57201014211); Yuan, Guang-Ming (57641773400)","57463195100; 56493570400; 57201014211; 57641773400","Infrared and visible image fusion algorithms based on generative adversarial networks","2022","Proceedings - 2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering, AEMCSE 2022","","","","549","554","5","10.1109/AEMCSE55572.2022.00113","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143156514&doi=10.1109%2fAEMCSE55572.2022.00113&partnerID=40&md5=045e5a43366c23c1470061f9268f2027","Visible images have good contour and texture information, while infrared images have the advantage of working in all weather. Therefore, for the detection and analysis of targets in low illumination at night, the information from visible and infrared images can be fused to improve the detection accuracy and anti-interference capability of detection systems for nighttime targets. In this paper, we propose a generative adversarial network-based fusion algorithm for IR and visible images, which can effectively extract the feature information of IR and visible images by adversarial training of two discriminators and generators, improve the feature extraction ability and the quality of fused images by introducing attention mechanism and structural similarity loss function, and enhance the stability of network training by TTUR. The experimental results show that the algorithm in this paper outperforms other typical algorithms in both subjective and objective evaluations. © 2022 IEEE.","Image enhancement; Image fusion; Infrared imaging; Textures; Attention mechanisms; Contour information; Detection accuracy; Image fusion algorithms; Infrared and visible image; Infrared image; IR images; Low illuminations; Texture information; Visible image; Generative adversarial networks","attention mechanism; generative adversarial networks; image fusion; infrared image; visible image","Conference paper","Final","","Scopus","2-s2.0-85143156514"
"Yang Y.; Liu J.; Huang S.; Wan W.; Wen W.; Guan J.","Yang, Yong (55185409700); Liu, Jiaxiang (57221502490); Huang, Shuying (55866636700); Wan, Weiguo (57191506215); Wen, Wenying (55750686300); Guan, Juwei (57221874422)","55185409700; 57221502490; 55866636700; 57191506215; 55750686300; 57221874422","Infrared and Visible Image Fusion via Texture Conditional Generative Adversarial Network","2021","IEEE Transactions on Circuits and Systems for Video Technology","31","12","","4771","4783","12","10.1109/TCSVT.2021.3054584","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100473556&doi=10.1109%2fTCSVT.2021.3054584&partnerID=40&md5=f3e1be301f83a125506240374c9d2608","This paper proposes an effective infrared and visible image fusion method based on a texture conditional generative adversarial network (TC-GAN). The constructed TC-GAN generates a combined texture map for capturing gradient changes in image fusion. The generator in the TC-GAN is designed as a codec structure for extracting more details, and a squeeze-and-excitation module is applied to this codec structure to increase the weight of significant texture information in the combined texture map. The generator loss function is designed by combing the gradient loss and adversarial loss to retain the texture information of the source images. The discriminator brings the texture of the generated image closer to the visible image. To obtain significant texture information from the source images, a multiple decision map-based fusion strategy is proposed using a combined texture map and an adaptive guided filter. Extensive experiments on the public TNO and RoadScene datasets demonstrate that the proposed method is superior to other state-of-the-art algorithms in terms of a subjective evaluation and quantitative indicators.  © 1991-2012 IEEE.","Image fusion; Adversarial networks; Codec structures; Fusion strategies; Infrared and visible image; Quantitative indicators; State-of-the-art algorithms; Subjective evaluations; Texture information; Image texture","combined texture map; Infrared and visible image fusion; multiple decision maps; TC-GAN","Article","Final","","Scopus","2-s2.0-85100473556"
"Wang J.; Ren J.; Li H.; Sun Z.; Luan Z.; Yu Z.; Liang C.; Monfared Y.E.; Xu H.; Hua Q.","Wang, Jingjing (57214140268); Ren, Jinwen (57221815068); Li, Hongzhen (57382184100); Sun, Zengzhao (57370709600); Luan, Zhenye (57224862293); Yu, Zishu (57224865860); Liang, Chunhao (55241842200); Monfared, Yashar E. (55582299400); Xu, Huaqiang (56025859100); Hua, Qing (55659556900)","57214140268; 57221815068; 57382184100; 57370709600; 57224862293; 57224865860; 55241842200; 55582299400; 56025859100; 55659556900","DDGANSE: Dual-Discriminator GAN with a Squeeze-and-Excitation Module for Infrared and Visible Image Fusion","2022","Photonics","9","3","150","","","","10.3390/photonics9030150","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126010616&doi=10.3390%2fphotonics9030150&partnerID=40&md5=2caa714f61ac047792c6d4b3e7af4585","Infrared images can provide clear contrast information to distinguish between the target and the background under any lighting conditions. In contrast, visible images can provide rich texture details and are compatible with the human visual system. The fusion of a visible image and infrared image will thus contain both comprehensive contrast information and texture details. In this study, a novel approach for the fusion of infrared and visible images is proposed based on a dual-discriminator generative adversarial network with a squeeze-and-excitation module (DDGANSE). Our approach establishes confrontation training between one generator and two discriminators. The goal of the generator is to generate images that are similar to the source images, and contain the information from both infrared and visible source images. The purpose of the two discriminators is to increase the similarity between the image generated by the generator and the infrared and visible images. We experimentally demonstrated that using continuous adversarial training, DDGANSE outputs images retain the advantages of both infrared and visible images with significant contrast information and rich texture details. Finally, we compared the performance of our proposed method with previously reported techniques for fusing infrared and visible images using both quantitative and qualitative assessments. Our experiments on the TNO dataset demonstrate that our proposed method shows superior performance compared to other similar reported methods in the literature using various performance metrics. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","","Fusion image; Generative adversarial networks; Infrared image; Squeeze-and-excitation net-works; Visible image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85126010616"
"Yang J.; Zhang W.; Liu J.; Wu J.; Yang J.","Yang, Jingjing (56898178900); Zhang, Weijia (57450071000); Liu, Jiaxing (57217197238); Wu, Jinzhao (55713797700); Yang, Jie (56285344000)","56898178900; 57450071000; 57217197238; 55713797700; 56285344000","Generating De-identification facial images based on the attention models and adversarial examples: Generating De-identification facial images based on the attention models","2022","Alexandria Engineering Journal","61","11","","8417","8429","12","10.1016/j.aej.2022.02.007","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124553910&doi=10.1016%2fj.aej.2022.02.007&partnerID=40&md5=b4ae7fd7dffb0f39bea116885dd5084c","In response to the problem that facial features are likely to cause privacy leakage and identity theft of the user, this paper presents a method for generating De-identification facial images based on attention models and adversarial examples. The method has two training stages. In Stage 1, target classification networks are used as an attention module to extract Refined Features of a facial image to perform feature fusion and obtain the face feature fusion matrix. In Stage 2, the face feature fusion matrix is used as an initial data distribution of the generator in the adversarial generation network and adds pixel-level constraint loss functions and loss functions for cutting adversarial example perturbations. The purpose of this is for ensuring the generated private image of the face is usable and add attention modules to discriminator of generative adversarial network. Next, the method extracts the feature matrix of the de-identity face image and trains it to mimic the face feature fusion matrix to increase the migration of the adversarial response in the de-identity face image. Experimental results have shown that the performance of the proposed method is superior to or approach state-of-the-art methods in terms of image quality and robustness. © 2022 THE AUTHORS","Image fusion; Image processing; Matrix algebra; Adversarial example; Attention model; De-identification; De-identification facial image; Face features; Facial images; Features fusions; Image-based; matrix; Privacy protection; Generative adversarial networks","Adversarial examples; Attention model; De-identification facial images; Generative adversarial network; Privacy protection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124553910"
"Li J.; Li B.; Jiang Y.; Cai W.","Li, Junwu (57300134800); Li, Binhua (56129818400); Jiang, Yaoxi (57432769900); Cai, Weiwei (57215898790)","57300134800; 56129818400; 57432769900; 57215898790","MSAt-GAN: a generative adversarial network based on multi-scale and deep attention mechanism for infrared and visible light image fusion","2022","Complex and Intelligent Systems","8","6","","4753","4781","28","10.1007/s40747-022-00722-9","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133962068&doi=10.1007%2fs40747-022-00722-9&partnerID=40&md5=8ce193b0f659297153a43f8ad9df458c","For the past few years, image fusion technology has made great progress, especially in infrared and visible light image infusion. However, the fusion methods, based on traditional or deep learning technology, have some disadvantages such as unobvious structure or texture detail loss. In this regard, a novel generative adversarial network named MSAt-GAN is proposed in this paper. It is based on multi-scale feature transfer and deep attention mechanism feature fusion, and used for infrared and visible image fusion. First, this paper employs three different receptive fields to extract the multi-scale and multi-level deep features of multi-modality images in three channels rather than artificially setting a single receptive field. In this way, the important features of the source image can be better obtained from different receptive fields and angles, and the extracted feature representation is also more flexible and diverse. Second, a multi-scale deep attention fusion mechanism is designed in this essay. It describes the important representation of multi-level receptive field extraction features through both spatial and channel attention and merges them according to the level of attention. Doing so can lay more emphasis on the attention feature map and extract significant features of multi-modality images, which eliminates noise to some extent. Third, the concatenate operation of the multi-level deep features in the encoder and the deep features in the decoder are cascaded to enhance the feature transmission while making better use of the previous features. Finally, this paper adopts a dual-discriminator generative adversarial network on the network structure, which can force the generated image to retain the intensity of the infrared image and the texture detail information of the visible image at the same time. Substantial qualitative and quantitative experimental analysis of infrared and visible image pairs on three public datasets show that compared with state-of-the-art fusion methods, the proposed MSAt-GAN network has comparable outstanding fusion performance in subjective perception and objective quantitative measurement. © 2022, The Author(s).","","Deep attention mechanism; Generative adversarial network; Infrared and visible light image fusion; MSAt-GAN; Multi-level receptive field; Multi-scale feature transfer","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85133962068"
"Li J.; Feng H.; Deng Z.; Cui X.; Deng H.; Li H.","Li, Jiao (57984260200); Feng, Hao (57661030600); Deng, Zhennan (57985156800); Cui, Xintong (57984441600); Deng, Hongxia (52163361700); Li, Haifang (55707609200)","57984260200; 57661030600; 57985156800; 57984441600; 52163361700; 55707609200","Image Derain Method for Generative Adversarial Network Based on Wavelet High Frequency Feature Fusion","2022","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13537 LNCS","","","165","178","13","10.1007/978-3-031-18916-6_14","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142802687&doi=10.1007%2f978-3-031-18916-6_14&partnerID=40&md5=f2562787fb6e48e84acc43db27e3927c","Photos taken on rainy days can be affected by rain streaks that reduce the sharpness of the image. Due to insufficient attention on feature extraction of rain streaks area, the removal effect of noise area needs to be improved. Wavelet transform is used to separate the high frequency information of the image, convolution is used to extract the high frequency features of the image, and the features of the original image with rain streaks extracted by the network are superimposed and fused. High frequency information graph represents the location of image noise. By incorporating high frequency features, the network can further learn the features of the rain stripes region. Feature fusion is introduced into the generative network, and the generative network guided by the attention distribution map considers global information more on the precondition of paying attention to the rain stripes region, so as to improve the clarity of the image after removing the rain fringes. The comparison experiment of wavelet high frequency feature fusion generative adversarial network and other methods is completed. The evaluation metrics are peak signal-to-noise ratio (PSNR) and structure similarity (SSIM), which verify the superiority of the proposed method compared with other methods. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2022.","Generative adversarial networks; Image compression; Image enhancement; Image fusion; Rain; Signal to noise ratio; Derain; Features extraction; Features fusions; Frequency features; High frequency HF; High-frequency informations; Network-based; Rainy days; Self-attention; Wavelets transform; Wavelet transforms","Derain; Feature fusion; Generative adversarial network; Self-attention; Wavelet transform","Conference paper","Final","","Scopus","2-s2.0-85142802687"
"Song Y.; Zhang H.; Huang H.; Zhang L.","Song, Yiyao (57221595735); Zhang, Hongyan (54954032600); Huang, He (57670565200); Zhang, Liangpei (8359720900)","57221595735; 54954032600; 57670565200; 8359720900","Remote Sensing Image Spatiotemporal Fusion via a Generative Adversarial Network With One Prior Image Pair","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5528117","","","","10.1109/TGRS.2022.3171331","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129574414&doi=10.1109%2fTGRS.2022.3171331&partnerID=40&md5=9e727c21ea45cae7ba89360b31f69013","Spatiotemporal fusion (STF) is an effective solution to promote the application of remote sensing images, given that the tradeoff between the temporal resolution and the spatial resolution is ubiquitous in the production of remote sensing images. However, cloud coverage makes it difficult to obtain dense cloud-free Landsat-Moderate Resolution Imaging Spectroradiometer (MODIS) image pairs on the timeline, which limits the application of existing STF methods. Considering the lack of prior image pairs and the huge spatial resolution gap between Landsat and MODIS images, this article presents a novel remote sensing image STF method based on a generative adversarial network to handle one Landsat-MODIS prior image pair case (OPGAN), which contains a generator and a discriminator simultaneously trained in a min-max game. OPGAN is built based on the STF observation model that learns the base information from the prior Landsat image and then captures temporal change (TC) information from a difference image constructed from MODIS images collected at times 1 and 2 and sensor difference information from the difference image between Landsat and MODIS images at time 1. They are combined together to reconstruct the Landsat image at time 2 at both high spatial and high temporal resolution. Moreover, a change loss is proposed to further improve the accuracy of TC prediction. Extensive experiments on the STF dataset illustrate that the proposed OPGAN method can obtain more accurate prediction of spatial information and TCs in the case of insufficient prior information.  © 1980-2012 IEEE.","Generative adversarial networks; Image fusion; Radiometers; Remote sensing; Satellite imagery; Change loss; Fusion methods; Image pairs; LANDSAT; One prior image pair; Remote sensing images; Remote-sensing; Spatial resolution; Spatio-temporal fusions; Temporal change; accuracy assessment; artificial neural network; Landsat; MODIS; remote sensing; satellite imagery; spatiotemporal analysis; Image resolution","Change loss; generative adversarial network (GAN); one prior image pair; spatiotemporal fusion (STF)","Article","Final","","Scopus","2-s2.0-85129574414"
"Zhou H.; Liu Q.; Weng D.; Wang Y.","Zhou, Huanyu (57221308389); Liu, Qingjie (55534263100); Weng, Dawei (26026709600); Wang, Yunhong (34870959400)","57221308389; 55534263100; 26026709600; 34870959400","Unsupervised Cycle-Consistent Generative Adversarial Networks for Pan Sharpening","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5408814","","","","10.1109/TGRS.2022.3166528","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128256933&doi=10.1109%2fTGRS.2022.3166528&partnerID=40&md5=008acac24b30b052779f054251a1db6f","Deep learning-based pan sharpening has received significant research interest in recent years. Most of the existing methods fall into the supervised learning framework in which they downsample the multispectral (MS) and panchromatic (PAN) images and regard the original MS images as ground truths to form training samples based on Wald's protocol. Although impressive performance could be achieved, they have difficulties when generalizing to the original full-scale images due to the scale gap, which makes them lack of practicability. In this article, we propose an unsupervised generative adversarial framework that learns from the full-scale images without the ground truths to alleviate this problem. We first extract the modality-specific features from the PAN and MS images with a two-stream generator, perform fusion in the feature domain, and then reconstruct the pan-sharpened images. Furthermore, we introduce a novel hybrid loss based on the cycle-consistency and adversarial scheme to improve the performance. Comparison experiments with the state-of-the-art methods are conducted on GaoFen-2 (GF-2) and WorldView-3 satellites. Results demonstrate that the proposed method can greatly improve the pan-sharpening performance on the full-scale images, which clearly shows its practical value. Codes are available at http://github.com/zhysora/UCGAN.  © 2022 IEEE.","Deep learning; Generative adversarial networks; Image enhancement; Cycle-consistency; Deep learning; Features extraction; Generator; Ground truth; Multispectral images; Pan-sharpening; Performance; Spatial resolution; Task analysis; algorithm; artificial neural network; detection method; image analysis; satellite imagery; WorldView; Image fusion","Cycle consistency; generative adversarial network (GAN); image fusion; pan sharpening; unsupervised learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85128256933"
"Han T.; Wu J.; Luo W.; Wang H.; Jin Z.; Qu L.","Han, Tingting (57993175400); Wu, Jun (57266847900); Luo, Wenting (57292230300); Wang, Huiming (57993498100); Jin, Zhe (57535952100); Qu, Lei (36175185400)","57993175400; 57266847900; 57292230300; 57993498100; 57535952100; 36175185400","Review of Generative Adversarial Networks in mono- and cross-modal biomedical image registration","2022","Frontiers in Neuroinformatics","16","","933230","","","","10.3389/fninf.2022.933230","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143329028&doi=10.3389%2ffninf.2022.933230&partnerID=40&md5=181aae90d9c041660b410808c99dcb87","Biomedical image registration refers to aligning corresponding anatomical structures among different images, which is critical to many tasks, such as brain atlas building, tumor growth monitoring, and image fusion-based medical diagnosis. However, high-throughput biomedical image registration remains challenging due to inherent variations in the intensity, texture, and anatomy resulting from different imaging modalities, different sample preparation methods, or different developmental stages of the imaged subject. Recently, Generative Adversarial Networks (GAN) have attracted increasing interest in both mono- and cross-modal biomedical image registrations due to their special ability to eliminate the modal variance and their adversarial training strategy. This paper provides a comprehensive survey of the GAN-based mono- and cross-modal biomedical image registration methods. According to the different implementation strategies, we organize the GAN-based mono- and cross-modal biomedical image registration methods into four categories: modality translation, symmetric learning, adversarial strategies, and joint training. The key concepts, the main contributions, and the advantages and disadvantages of the different strategies are summarized and discussed. Finally, we analyze the statistics of all the cited works from different points of view and reveal future trends for GAN-based biomedical image registration studies. Copyright © 2022 Han, Wu, Luo, Wang, Jin and Qu.","image registration; learning; review","adversarial training; biomedical image registration; cross-modal; Generative Adversarial Networks; image translation","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85143329028"
"Apostolopoulos I.D.; Papathanasiou N.D.; Apostolopoulos D.J.; Panayiotakis G.S.","Apostolopoulos, Ioannis D. (57195641603); Papathanasiou, Nikolaos D. (23995562200); Apostolopoulos, Dimitris J. (24068454400); Panayiotakis, George S. (7006755481)","57195641603; 23995562200; 24068454400; 7006755481","Applications of Generative Adversarial Networks (GANs) in Positron Emission Tomography (PET) imaging: A review","2022","European Journal of Nuclear Medicine and Molecular Imaging","49","11","","3717","3739","22","10.1007/s00259-022-05805-w","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128743709&doi=10.1007%2fs00259-022-05805-w&partnerID=40&md5=0f68fe139fab472500baa33b207c781f","Purpose: This paper reviews recent applications of Generative Adversarial Networks (GANs) in Positron Emission Tomography (PET) imaging. Recent advances in Deep Learning (DL) and GANs catalysed the research of their applications in medical imaging modalities. As a result, several unique GAN topologies have emerged and been assessed in an experimental environment over the last two years. Methods: The present work extensively describes GAN architectures and their applications in PET imaging. The identification of relevant publications was performed via approved publication indexing websites and repositories. Web of Science, Scopus, and Google Scholar were the major sources of information. Results: The research identified a hundred articles that address PET imaging applications such as attenuation correction, de-noising, scatter correction, removal of artefacts, image fusion, high-dose image estimation, super-resolution, segmentation, and cross-modality synthesis. These applications are presented and accompanied by the corresponding research works. Conclusion: GANs are rapidly employed in PET imaging tasks. However, specific limitations must be eliminated to reach their full potential and gain the medical community's trust in everyday clinical practice. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Artifacts; Humans; Image Processing, Computer-Assisted; Positron-Emission Tomography; artifact; catalysis; clinical practice; deep learning; diagnostic imaging; drug megadose; human; nuclear medicine; positron emission tomography; review; Scopus; search engine; synthesis; trust; Web of Science; image processing; procedures","Deep Learning; Generative Adversarial Networks; Nuclear Medicine; Positron Emission Tomography","Review","Final","","Scopus","2-s2.0-85128743709"
"Zhong J.; Huyan J.; Zhang W.; Cheng H.; Zhang J.; Tong Z.; Jiang X.; Huang B.","Zhong, Jingtao (57215037557); Huyan, Ju (56575068700); Zhang, Weiguang (57016053900); Cheng, Hanglin (57267946300); Zhang, Jing (56470564300); Tong, Zheng (57192940516); Jiang, Xi (57190069301); Huang, Baoshan (56036550500)","57215037557; 56575068700; 57016053900; 57267946300; 56470564300; 57192940516; 57190069301; 56036550500","A deeper generative adversarial network for grooved cement concrete pavement crack detection","2023","Engineering Applications of Artificial Intelligence","119","","105808","","","","10.1016/j.engappai.2022.105808","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145663551&doi=10.1016%2fj.engappai.2022.105808&partnerID=40&md5=12101084e5d73afa13a5738c64c21e7f","Periodic grooved cement concrete pavement crack detection is of great importance for pavement condition monitoring and maintenance. The current state-of-the-art (SOTA) detection solutions highly depend on datasets. However, due to the limited access to crack images, more efficient methods are urgently needed to advance the detection of cracking on grooved cement concrete pavement. This study proposes an improved deeper Wasserstein generative adversarial network with gradient penalty (WGAN-GP) to generate datasets of pavement images with a size of 512 × 512 pixels 2. Poisson bleeding is adopted to create the synthesized grooved cement concrete pavement crack images based on the generated crack images and groove images. The robustness of the proposed improved deeper WGAN-GP model is validated by Faster R-CNN, YOLOv3, and YOLOv4 models trained on original crack images and generated crack images for region-level detection. U-Net and W-segnet are used to achieve pixel-level crack detection to evaluate the effectiveness of proposed model. Results show that the improved deeper WGAN-GP could generate more realistic transverse, longitudinal and oblique crack images. In addition, the Poisson bleeding algorithm contributes to synthesizing grooved cement concrete pavement crack images. Moreover, it is observed that YOLOv3 trained by the augmented dataset could achieve a mean average precision (MAP) of 81.98%, 6% MAP higher than the non-augmented dataset. U-Net and W-segnet benefit from augmented dataset with a better pixel-level segmentation result. Based on the results, it can be concluded that the improved deeper WAGN-GP image generation method can provide a straightforward way to fill the data shortage gap of grooved cement concrete pavement cracks, thus increasing the problem-solving capability of the SOTA crack detection models. © 2023 Elsevier Ltd","Cements; Concrete pavements; Condition monitoring; Deep learning; Generative adversarial networks; Image enhancement; Image fusion; Bleedings; Cement concrete pavements; Crack image; Deep learning; Generative adversarial network; Pavement condition; Pavement crack detection; Pavement cracks; Pixel level; State of the art; Crack detection","Crack detection; Deep learning; Generative adversarial network (GAN); Image fusion; Pavement crack","Article","Final","","Scopus","2-s2.0-85145663551"
"Zhao C.; Wang T.; Lei B.","Zhao, Cheng (57220883689); Wang, Tianfu (55602702200); Lei, Baiying (26422280400)","57220883689; 55602702200; 26422280400","Medical image fusion method based on dense block and deep convolutional generative adversarial network","2021","Neural Computing and Applications","33","12","","6595","6610","15","10.1007/s00521-020-05421-5","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092495531&doi=10.1007%2fs00521-020-05421-5&partnerID=40&md5=27b8e03852666dbf0f3a9c8c83398139","Medical image fusion techniques can further improve the accuracy and time efficiency of clinical diagnosis by obtaining comprehensive salient features and detail information from medical images of different modalities. We propose a novel medical image fusion algorithm based on deep convolutional generative adversarial network and dense block models, which is used to generate fusion images with rich information. Specifically, this network architecture integrates two modules: an image generator module based on dense block and encoder–decoder and a discriminator module. In this paper, we use the encoder network to extract the image features, process the features using fusion rule based on the Lmax norm, and use it as the input of the decoder network to obtain the final fusion image. This method can overcome the weaknesses of the active layer measurement by manual design in the traditional methods and can process the information of the intermediate layer according to the dense blocks to avoid the loss of information. Besides, this paper uses detail loss and structural similarity loss to construct the loss function, which is used to improve the extraction ability of target information and edge detail information related to images. Experiments on the public clinical diagnostic medical image dataset show that the proposed algorithm not only has excellent detail preserve characteristics but also can suppress the artificial effects. The experiment results are better than other comparison methods in different types of evaluation. © 2020, Springer-Verlag London Ltd., part of Springer Nature.","Convolution; Convolutional neural networks; Decoding; Diagnosis; Image enhancement; Medical imaging; Network architecture; Signal encoding; Adversarial networks; Clinical diagnosis; Clinical diagnostics; Comparison methods; Intermediate layers; Structural similarity; Target information; Time efficiencies; Image fusion","Deep convolutional GAN; Dense block; Encoder–decoder; Loss function; Medical image fusion","Article","Final","","Scopus","2-s2.0-85092495531"
"Zhang Q.; Jia R.-S.; Li Z.-H.; Li Y.-C.; Sun H.-M.","Zhang, Qi (57367179500); Jia, Rui-Sheng (25927894300); Li, Zeng-Hu (57563441600); Li, Yong-Chao (57411683300); Sun, Hong-Mei (55729286100)","57367179500; 25927894300; 57563441600; 57411683300; 55729286100","Superresolution reconstruction of optical remote sensing images based on a multiscale attention adversarial network","2022","Applied Intelligence","52","15","","17896","17911","15","10.1007/s10489-022-03548-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127706645&doi=10.1007%2fs10489-022-03548-7&partnerID=40&md5=82bde35960d7e8ee7f087ef8569b915c","Due to the influence of imaging equipment and environmental conditions on optical remote sensing image acquisition, image resolution is generally low. Superresolution reconstruction technology is an important way to improve image quality. However, the existing optical remote sensing image superresolution reconstruction methods have some problems, such as insufficient feature extraction, blurred texture details of reconstructed images, and excessive network accumulation. To solve the above problems, a superresolution reconstruction method for optical remote sensing images based on a multiscale attention adversarial network is proposed in this paper. The method takes a generative adversarial network (GAN) as the basic framework. The generator uses four multiscale attention residual blocks (MSARBs) to extract image multiscale feature information and carries out feature fusion through a binary feature fusion structure (BFFS) to generate more realistic images. The discriminator uses a depth convolution network to distinguish the differences between real images and superresolution images. In the aspect of loss function construction, the perceptual loss and adversarial loss are combined to improve the perceptual quality of the images. Experimental results show that this method is superior to the compared algorithm in regard to the objective evaluation metrics of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM), and its reconstructed images have better visual effect. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Image acquisition; Image enhancement; Image fusion; Image quality; Image reconstruction; Image resolution; Optical remote sensing; Signal to noise ratio; Textures; Adversarial networks; Features fusions; Image-based; Multiscale attention residual network; Optical remote sensing; Optical remote sensing image; Reconstructed image; Reconstruction method; Remote sensing images; Super-resolution reconstruction; Generative adversarial networks","Generative adversarial network; Multiscale attention residual network; Optical remote sensing images; Superresolution reconstruction","Article","Final","","Scopus","2-s2.0-85127706645"
"","","","2022 International Conference on Machine Vision, Automatic Identification and Detection, MVAID 2022","2022","Journal of Physics: Conference Series","2284","1","","","","203","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132834305&partnerID=40&md5=c09c0ebdbc57407b29921c32ccbc6ac0","The proceedings contain 27 papers. The topics discussed include: a fault diagnosis method for power grid based on image feature extraction; a cross-attention based image fusion network for prediction of mild cognitive impairment; a metal character enhancement method based on conditional generative adversarial networks; image principal distance calibration method by binocular camera based on improved differential evolution algorithm; generative rendering network based on U-shape discriminator; LIDAR–camera deep fusion for end-to-end trajectory planning of autonomous vehicle; high-precision velocity measurement method based on high frame rate image processing; soybean image segmentation based on multi-scale Retinex with color restoration; and reformatted contrastive learning for image classification via attention mechanism and self-distillation.","","","Conference review","Final","","Scopus","2-s2.0-85132834305"
"Song A.; Duan H.; Pei H.; Ding L.","Song, Anyang (57449519900); Duan, Huixian (35182860000); Pei, Haodong (36070023300); Ding, Lei (57188828694)","57449519900; 35182860000; 36070023300; 57188828694","Triple-discriminator generative adversarial network for infrared and visible image fusion","2022","Neurocomputing","483","","","183","194","11","10.1016/j.neucom.2022.02.025","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124489274&doi=10.1016%2fj.neucom.2022.02.025&partnerID=40&md5=9acf16104e35bf7b5f9b28a988f0bd45","We aim to address the challenging task of infrared and visible image fusion. The existed fusion methods cannot achieve the balance of clear boundaries and rich details. In this paper, we propose a novel fusion model using a triple-discriminator generative adversarial network, which can achieve the balance. The difference image obtained by image subtraction can highlight the difference information, extract image details, and obtain the target outlines in some scenes. Therefore, besides the visible discriminator and infrared discriminator, a new difference image discriminator is added to retain the difference between infrared and visible images, thereby improving the contrast of infrared targets and keeping the texture details in visible images. Multi-level features extracted by the discriminators are used for information measurement, and as a result, deriving perceptual fusion weights for adaptive fusion. SSIM loss function and target edge-enhancement loss are also introduced to improve the quality of the fused image. Compared with existing state-of-the-art fusion methods on public datasets, it is demonstrated that our model has a better performance on quantitative metrics and qualitative effects. © 2022 The Authors","Discriminators; Image enhancement; Image fusion; Infrared imaging; Textures; Difference images; Fusion methods; Fusion model; Image details; Image subtraction; Information extract; Infrared and visible image; Infrared target; Multilevels; Visible image; article; image subtraction; infrared radiation; loss of function mutation; quantitative analysis; Generative adversarial networks","Generative adversarial network; Image fusion; Infrared image","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85124489274"
"Zhou H.; Hou J.; Zhang Y.; Ma J.; Ling H.","Zhou, Huabing (55447554500); Hou, Jilei (57222092435); Zhang, Yanduo (55993581700); Ma, Jiayi (26638975600); Ling, Haibin (57191091290)","55447554500; 57222092435; 55993581700; 26638975600; 57191091290","Unified gradient- and intensity-discriminator generative adversarial network for image fusion","2022","Information Fusion","88","","","184","201","17","10.1016/j.inffus.2022.07.016","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135955602&doi=10.1016%2fj.inffus.2022.07.016&partnerID=40&md5=22fea08a0a1cee2afc30f0bca3bc0afb","This study proposes a unified gradient- and intensity-discriminator generative adversarial network for various image fusion tasks, including infrared and visible image fusion, medical image fusion, multi-focus image fusion, and multi-exposure image fusion. On the one hand, we unify all fusion tasks into discriminating a fused image's gradient and intensity distributions based on a generative adversarial network. The generator adopts a dual-encoder–single-decoder framework to extract source image features by using different encoder paths. A dual-discriminator is employed to distinguish the gradient and intensity, ensuring that the generated image contains the desired geometric structure and conspicuous information. The dual adversarial game can tackle the generative adversarial network's mode collapse problem. On the other hand, we define a loss function based on the gradient and intensity that can be adapted to various fusion tasks by using varying relevant parameters with the source images. Qualitative and quantitative experiments on publicly available datasets demonstrate our method's superiority over state-of-the-art methods. © 2022 Elsevier B.V.","Discriminators; Image fusion; Medical imaging; Signal encoding; Fused images; Gradient; Image gradients; Image intensities; Infrared and visible image; Intensity; Medical image fusion; Multi-exposure images; Multifocus image fusion; Source images; Generative adversarial networks","Generative adversarial network; Gradient; Image fusion; Intensity","Article","Final","","Scopus","2-s2.0-85135955602"
"Ma Y.; Hua Y.; Zuo Z.","Ma, Yangyang (57440066700); Hua, Yanling (57215547325); Zuo, Zhengrong (7103368253)","57440066700; 57215547325; 7103368253","Infrared Image Generation by Pix2pix Based on Multi-receptive Field Feature Fusion","2021","10th International Conference on Control, Automation and Information Sciences, ICCAIS 2021 - Proceedings","","","","1029","1036","7","10.1109/ICCAIS52680.2021.9624500","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123985233&doi=10.1109%2fICCAIS52680.2021.9624500&partnerID=40&md5=af9c5b321955f07f3692b605fa33e428","Infrared imaging has the advantages of strong anti-interference capability, long-range imaging, and night imaging, and has important applications in both civilian and military fields. In the development of infrared-related equipment, a large number of infrared images under a variety of conditions are required as verification test data. The field test of infrared images requires huge manpower and material resources, and it is difficult to obtain full-time infrared images. To address the problem of insufficient infrared image samples, the paper introduces generative adversarial networks into the infrared image generation task and investigates the infrared image generation method based on visible images by applying Pix2pix networks to paired visible infrared image datasets. To address the problem of missing detailed information of infrared images generated by the Pix2pix network, the paper proposes a Pix2pix network based on multi-receptive field feature fusion and constructs a multi-receptive field feature extractor based on Unet++ structure; the multi-receptive field feature fusion mechanism of nested pixel level by level is proposed. Experiments show that the Pix2pix network based on multi-receptive field feature fusion achieves finer infrared texture generation. © 2021 IEEE.","Image fusion; Military applications; Military photography; Textures; Thermography (imaging); Anti-interference; Features fusions; Field features; Image generations; Infrared image generation; Military fields; Multi-receptive-field feature fusion; Network-based; Range imaging; Receptive fields; Generative adversarial networks","Generative Adversarial Networks; Infrared image generation; Multi-receptive-field feature fusion","Conference paper","Final","","Scopus","2-s2.0-85123985233"
"Yang X.; Huo H.; Li J.; Li C.; Liu Z.; Chen X.","Yang, Xin (57212206381); Huo, Hongtao (56527849900); Li, Jing (57207844651); Li, Chang (56718731300); Liu, Zhao (57213430933); Chen, Xun (36456894700)","57212206381; 56527849900; 57207844651; 56718731300; 57213430933; 36456894700","DSG-Fusion: Infrared and visible image fusion via generative adversarial networks and guided filter","2022","Expert Systems with Applications","200","","116905","","","","10.1016/j.eswa.2022.116905","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127323431&doi=10.1016%2fj.eswa.2022.116905&partnerID=40&md5=4f3c5876524fb58897b7a812f54c4ff5","The goal of infrared and visible image fusion is to generate an informative image which preserves texture details and infrared targets. Most generative adversarial network (GAN) based methods take a concatenation of two source images as the input, hence the extracted feature is insufficient for preserving background and detail information. To this end, we propose a novel GAN based fusion framework, termed as double-stream guided filter network (DSG-Fusion). Given the diverse modalities of infrared and visible images, the generator of DSG network extracts features of two images through two independent data flows. To address the problem of extracting representative background information and force the DSG network focus on details, we integrate guided filter into the generator to obtain base and detail layers of source images. The base layers are concatenated with the corresponding source images to extract deeper features, while detail layers participate in the fusion procedure directly. Thus, DSG-Fusion can extract texture and intensity information sufficiently, and more background and detail information are preserved. Furthermore, a DSG loss consisting of intensity and structural similarity (SSIM) is designed to impel the network to generate a better fused image. Extensive experimental results show that DSG-Fusion achieves better performance comparing with five representative methods. © 2022 Elsevier Ltd","Generative adversarial networks; Image processing; Textures; Dataflow; Filter networks; Guided filters; Infrared and visible image; Infrared and visible image fusion; Infrared target; Network filters; Network-based; Source images; Two sources; Image fusion","Generative adversarial networks; Guided filter; Infrared and visible image fusion","Article","Final","","Scopus","2-s2.0-85127323431"
"Wang Y.; Xu S.; Liu J.; Zhao Z.; Zhang C.; Zhang J.","Wang, Yicheng (57768106100); Xu, Shuang (56367405500); Liu, Junmin (42761838200); Zhao, Zixiang (57218542866); Zhang, Chunxia (55703936800); Zhang, Jiangshe (9737712100)","57768106100; 56367405500; 42761838200; 57218542866; 55703936800; 9737712100","MFIF-GAN: A new generative adversarial network for multi-focus image fusion","2021","Signal Processing: Image Communication","96","","116295","","","","10.1016/j.image.2021.116295","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104667913&doi=10.1016%2fj.image.2021.116295&partnerID=40&md5=f56cedcafe70e764bffd4114218c454e","Multi-Focus Image Fusion (MFIF) is a promising image enhancement technique to generate all-in-focus images meeting visual needs, and it is a precondition for other computer vision tasks. One emergent research trend in MFIF involves approaches to avoiding a defocus spread effect (DSE) around a focus/defocus boundary (FDB). This study proposes a generative adversarial network for MFIF tasks called MFIF-GAN, to attenuate the DSE by generating focus maps in which the foreground region is correctly larger than corresponding objects. A Squeeze and Excitation residual module is employed in the proposed network. By combining prior knowledge of a training condition, the network is trained on a synthetic dataset based on an α-matte model. In addition, reconstruction and gradient regularization terms are combined in the loss functions to enhance boundary details and improve the quality of fused images. Extensive experiments demonstrate that the MFIF-GAN outperforms eight state-of-the-art (SOTA) methods in visual perception and quantitative analysis, as well as efficiency. Moreover, an edge diffusion and contraction module is proposed to verify that focus maps generated by our method are accurate at the pixel level. © 2021 Elsevier B.V.","Image enhancement; Adversarial networks; All-in-focus image; Foreground regions; Multifocus image fusion; Regularization terms; State of the art; Training conditions; Visual perception; Image fusion","Deep learning; Defocus spread effect; Generative adversarial network; Multi-focus image fusion","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85104667913"
"Zhao Z.; Zhang J.; Xu S.; Sun K.; Huang L.; Liu J.; Zhang C.","Zhao, Zixiang (57218542866); Zhang, Jiangshe (9737712100); Xu, Shuang (56367405500); Sun, Kai (57193093191); Huang, Lu (57222016088); Liu, Junmin (42761838200); Zhang, Chunxia (55703936800)","57218542866; 9737712100; 56367405500; 57193093191; 57222016088; 42761838200; 55703936800","FGF-GAN: A LIGHTWEIGHT GENERATIVE ADVERSARIAL NETWORK FOR PANSHARPENING VIA FAST GUIDED FILTER","2021","Proceedings - IEEE International Conference on Multimedia and Expo","","","","","","","10.1109/ICME51207.2021.9428272","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126434284&doi=10.1109%2fICME51207.2021.9428272&partnerID=40&md5=8d20df2cce1d516406276ae77807356d","Pansharpening is a widely used image enhancement technique for remote sensing. Its principle is to fuse the input high-resolution single-channel panchromatic (PAN) image and low-resolution multi-spectral image and to obtain a high-resolution multi-spectral (HRMS) image. The existing deep learning pansharpening method has two shortcomings. First, features of two input images need to be concatenated along the channel dimension to reconstruct the HRMS image, which makes the importance of PAN images not prominent, and also leads to high computational cost. Second, the implicit information of features is difficult to extract through the manually designed loss function. To this end, we propose a generative adversarial network via the fast guided filter (FGF) for pansharpening. In generator, traditional channel concatenation is replaced by FGF to better retain the spatial information while reducing the number of parameters. Meanwhile, the fusion objects can be highlighted by the spatial attention module. In addition, the latent information of features can be preserved effectively through adversarial training. Numerous experiments illustrate that our network generates high-quality HRMS images that can surpass existing methods, and with fewer parameters. © 2021 IEEE Computer Society. All rights reserved.","Deep learning; Image enhancement; Image fusion; Remote sensing; Spectroscopy; Channel dimension; Fast guided filter; Guided filters; High resolution; Input image; Lower resolution; Multispectral images; Pan-sharpening; Remote-sensing; Single channels; Generative adversarial networks","Fast guided filter; Generative adversarial network; Image fusion; Pansharpening","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85126434284"
"Ning Y.; You Y.; Cao J.; Liu F.; Yan Q.; Zhang Y.","Ning, Yuanyong (57222241669); You, Yanan (54409614600); Cao, Jingyi (57210948716); Liu, Fang (57091791100); Yan, Qing (55307568700); Zhang, Yue (57839925000)","57222241669; 54409614600; 57210948716; 57091791100; 55307568700; 57839925000","FUSION DETECTION OF CLOSED WATER IN MEDIUM-LOW RESOLUTION REMOTE SENSING IMAGERY","2021","International Geoscience and Remote Sensing Symposium (IGARSS)","2021-July","","","4027","4030","3","10.1109/IGARSS47720.2021.9553554","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126054343&doi=10.1109%2fIGARSS47720.2021.9553554&partnerID=40&md5=b537674d1f996c16fb1e1fa780654ec3","Aiming at the closed water detection in remote sensing imagery at medium-low resolution, this paper proposes a novel method for closed water detection based on fusion detection which conducts detection via informative fused images blended by Synthetic Aperture Radar (SAR) and optical images. Firstly, it utilizes SAR and optical image pairs containing the same closed water object to generate aligned image pairs according to latitude and longitude information. Next, generative adversarial network (GAN) is adopted to fuse two categories of images. At last, a target detection network driven by optical image samples is used to detect the closed water on the fused image. The experiment result on Sentinel-1&2 shows that the proposed method can effectively make up for the shortage of SAR image in closed water detection and improve the detection performance. © 2021 IEEE.","","Closed water detection; GAN; Image fusion","Conference paper","Final","","Scopus","2-s2.0-85126054343"
"Bhattacharya J.; Modi S.; Gregorat L.; Ramponi G.","Bhattacharya, Jhilik (37015389000); Modi, Shatrughan (57189439801); Gregorat, Leonardo (57194661567); Ramponi, Giovanni (57208182558)","37015389000; 57189439801; 57194661567; 57208182558","D2BGAN: A Dark to Bright Image Conversion Model for Quality Enhancement and Analysis Tasks Without Paired Supervision","2022","IEEE Access","10","","","57942","57961","19","10.1109/ACCESS.2022.3178698","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131761467&doi=10.1109%2fACCESS.2022.3178698&partnerID=40&md5=452661b5a568882c47e777454fbfdcce","This paper presents an image enhancement model, D2BGAN (Dark to Bright Generative Adversarial Network), to translate low light images to bright images without a paired supervision. We introduce the use of geometric and lighting consistency along with a contextual loss criterion. These when combined with multiscale color, texture and edge discriminators prove to provide competitive results. We performed extensive experiments using benchmark datasets to visually and objectively compare our results. We observed the performance of D2BGAN on real-time driving datasets that are subject to motion blur, noise, and other artifacts. We further demonstrated that our enhanced images can be profitably used in image-understanding tasks. Images processed using our technique obtain the best or second best average scores for three different image quality evaluation methods on the Naturalness Preserved Enhancement (NPE), Low Light Image Enhancement (LIME), Multi-Exposure Image Fusion (MEF) benchmark datasets. Best scores are also obtained on the LOw-Light (LOL) test set and on Berkeley Driving Dataset (BDD) images processed with D2BGAN. Face detection tasks on the DarkFace benchmark dataset show an mAP (mean Average Precision) improvement from 0.209 to 0.301 when images are processed using D2BGAN. mAP further improves to 0.525 when finetuning techniques are adopted.  © 2013 IEEE.","Benchmarking; Edge detection; Face recognition; Generative adversarial networks; Image analysis; Image fusion; Lighting; Lime; Quality control; Statistical tests; Benchmark datasets; Bright images; Conversion model; Image color analysis; Image conversion; Image edge detection; Low-light images; Quality enhancement; Task analysis; Unpaired supervision; Image enhancement","Generative adversarial network; Image enhancement; Unpaired supervision","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85131761467"
"Chen J.; Wang L.; Feng R.; Liu P.; Han W.; Chen X.","Chen, Jia (57216636841); Wang, Lizhe (23029267900); Feng, Ruyi (55853730300); Liu, Peng (57075315400); Han, Wei (57191570975); Chen, Xiaodao (35247612500)","57216636841; 23029267900; 55853730300; 57075315400; 57191570975; 35247612500","CycleGAN-STF: Spatiotemporal Fusion via CycleGAN-Based Image Generation","2021","IEEE Transactions on Geoscience and Remote Sensing","59","7","9206067","5851","5865","14","10.1109/TGRS.2020.3023432","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106728266&doi=10.1109%2fTGRS.2020.3023432&partnerID=40&md5=d1466492958b214b4630f25756363f0c","Due to the trade-off of temporal resolution and spatial resolution, spatiotemporal image-fusion uses existing high-spatial-low-temporal (HSLT) and high-temporal-low-spatial (HTLS) images as prior knowledge to reconstruct high-temporal-high-spatial (HTHS) images. However, some existing spatiotemporal image-fusion algorithms ignore the issue that the spatial information of HTLS images is insufficient to support the acquisition of spatial information, which leads to the unsatisfactory accuracy of the fusion result. To introduce more spatial information, the algorithm in this article uses Cycle-generative adversarial networks (GANs) to simulate the change process of two HSLT images at $k-1$ and $k+1$ , and to generate some simulated images between $k-1$ and $k+1$. Then, the generated images are selected under the help of HTLS images, and the selected ones are then enhanced with wavelet transform. Finally, the image with spatial information is introduced into the Flexible Spatiotemporal DAta Fusion (FSDAF) framework to improve the performance of spatiotemporal image-fusion. Extensive experiments on two real data sets demonstrate that our proposed method outperforms current state-of-the-art spatiotemporal image-fusion methods.  © 1980-2012 IEEE.","Economic and social effects; Image compression; Image enhancement; Wavelet transforms; Adversarial networks; Image generations; Spatial informations; Spatial resolution; Spatio-temporal data; Spatio-temporal fusions; Spatiotemporal images; Temporal resolution; algorithm; data processing; image analysis; remote sensing; satellite data; spatiotemporal analysis; Image fusion","Generative adversarial network (GAN); Image-generation; Remote sensing; Spatiotemporal image-fusion; Wavelet transform","Article","Final","","Scopus","2-s2.0-85106728266"
"Yanli L.; Zimu L.; Junce F.; Gu Y.","Yanli, Liu (55742182800); Zimu, Li (58087445200); Junce, Feng (58087742100); Gu, Yuanjie (57224571534)","55742182800; 58087445200; 58087742100; 57224571534","An Unsupervised GAN-based Quality-enhanced Medical Image Fusion Network","2022","2022 IEEE Conference on Telecommunications, Optics and Computer Science, TOCS 2022","","","","429","432","3","10.1109/TOCS56154.2022.10016141","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147252825&doi=10.1109%2fTOCS56154.2022.10016141&partnerID=40&md5=020a4c5b6c110ae819e8e70b2ed32b9b","Medical image fusion technology can improve the precision of clinical diagnosis by fusing medical information from different modalities. However, the quality of fusion is restricted due to the particular imaging mechanism. This paper proposes a quality-enhanced medical image fusion algorithm based on a generative adversarial network for the lossless fusion of MRI and PET images. It consists of a lightweight image enhancement depth network to make the quality of the fused image suit human vision perceptual system better and a generative adversarial network to enhance texture details and edge information further. Our model is unsupervised and does not require paired fused images for training. The test results show that our algorithm performs better in both subjective visual effects and objective evaluation metrics.  © 2022 IEEE.","Diagnosis; Image enhancement; Image fusion; Magnetic resonance imaging; Medical imaging; Textures; Clinical diagnosis; Fused images; Image fusion algorithms; Image fusion technology; Imaging mechanism; Lossless; Medical image fusion; Medical information; Multi-modality; Quality-enhanced GAN; Generative adversarial networks","Medical image fusion; Multi-modality; Quality-enhanced GAN; Unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85147252825"
"Qiao P.; Gao X.; Man W.","Qiao, Pingan (57401745900); Gao, Xiwang (57402605900); Man, Wen (57401890800)","57401745900; 57402605900; 57401890800","AttnGAN++: Enhencing the Edge of Images on AttnGAN","2022","Lecture Notes on Data Engineering and Communications Technologies","89","","","792","802","10","10.1007/978-3-030-89698-0_81","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122384565&doi=10.1007%2f978-3-030-89698-0_81&partnerID=40&md5=0b272de8da0a1c792bf15caf8be88508","The text-to-image synthesis has made great progress at present, but the extraction of key semantic information and the restoration of the edge details of the generated image are still not perfect. In this paper, we proposed Attentional Generative Adversarial Network++ (AttnGAN++) model based on AttnGAN that allows to effectively solve the problem of missing the edge information of the generated image and extracting insufficient text features. First, we introduced Bi-directional Gated Recurrent Unit model (BiGRU), which can still ensure sufficient extraction of contextual information in processing long texts. This model combined with the attention mechanism, to achieve higher weights for the important words of the text. Then, we proposed the edge enhancement network consists four modules: Edge extraction, Residual Dense Block (RDB), Edge enhancement fusion, and Up-sampling model. Edge extraction obtains the edge of the final generated image. RDB to form a continuous memory mechanism, adaptively learn from previously more effective features to enhance feature propagation. Edge enhancement fusion and up-sampling modules to fuse the edge information and global information to generate high-resolution images with clearer edges. Thorough experiments on CUB dataset demonstrate that Attentional Generative Adversarial Network++ model significantly outperforms Attentional Generative Adversarial Network, boosting the best reported inception score by 3.78% and R-precision by 9.71% on CUB dataset, which can generate clearer image edges and improve the quality of the image. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.","Extraction; Image enhancement; Image fusion; Semantics; Signal sampling; Attentional generative adversarial network; Bi-directional; Bi-directional gated recurrent unit; Edge enhancement network; Edge enhancements; Edge extraction; Edge information; Network models; Residual dense block; Text-to-image; Generative adversarial networks","Attentional generative adversarial network; Bi-directional gated recurrent unit; Edge enhancement network; Residual dense block; Text-to-image","Book chapter","Final","","Scopus","2-s2.0-85122384565"
"Liu P.; Li J.; Wang L.; He G.","Liu, Peng (57075315400); Li, Jun (24481713500); Wang, Lizhe (23029267900); He, Guojin (14028364400)","57075315400; 24481713500; 23029267900; 14028364400","Remote Sensing Data Fusion with Generative Adversarial Networks: State-of-the-art methods and future research directions","2022","IEEE Geoscience and Remote Sensing Magazine","10","2","","295","328","33","10.1109/MGRS.2022.3165967","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130701827&doi=10.1109%2fMGRS.2022.3165967&partnerID=40&md5=ccc91ff1d29589805aa9e756e5bf1185","In the past decades, remote sensing (RS) data fusion has always been an active research community. A large number of algorithms and models have been developed. Generative adversarial networks (GANs), as an important branch of deep learning, show promising performances in a variety of RS image fusions. This review provides an introduction to GANs for RS data fusion. We briefly review the frequently used architecture and characteristics of GANs in data fusion and comprehensively discuss how to use GANs to realize fusion for homogeneous RS, heterogeneous RS, and RS and ground observation (GO) data. We also analyze some typical applications with GAN-based RS image fusion. This review provides insight into how to make GANs adapt to different types of fusion tasks and summarizes the advantages and disadvantages of GAN-based RS data fusion. Finally, we discuss promising future research directions and make a prediction on their trends.  © 2013 IEEE.","Deep learning; Image fusion; Remote sensing; Future research directions; Network state; Network-based; Performance; Remote grounds; Remote sensing data fusion; Remote sensing images; Remote-sensing; Research communities; State-of-the-art methods; algorithm; artificial neural network; data; remote sensing; research; Generative adversarial networks","","Article","Final","","Scopus","2-s2.0-85130701827"
"Wang S.; Li X.; Huo W.; You J.","Wang, Shengchen (57907048400); Li, Xisheng (14056258400); Huo, Wenyu (57907048500); You, Jia (55017610200)","57907048400; 14056258400; 57907048500; 55017610200","Fusion of Infrared and Visible Images Based on Improved Generative Adversarial Networks","2022","2022 3rd International Conference on Information Science, Parallel and Distributed Systems, ISPDS 2022","","","","247","251","4","10.1109/ISPDS56360.2022.9874034","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138813166&doi=10.1109%2fISPDS56360.2022.9874034&partnerID=40&md5=14a0baea302f278b2ab6b123ec63b958","In order to improve the fusion quality of infrared and visible light images, enhance the visual effect of fused images, and solve the problems that traditional fusion methods need to manually set fusion rules and the background details of fused images are poorly preserved, this paper proposes an improved generative adversarial network that combines multi-scale information. The generator used in this method is a typical encoder and decoder structure, and the discriminator uses a dual discriminator to establish the confrontation relationship between the infrared source image, the visible light source image and the fusion image respectively. Before the source image is input to the encoder, multi-scale information is introduced through the Inception network, which effectively extracts the multi-scale features of the image, which ensures the subsequent improvement of the quality of the fusion image. In addition, the loss function is improved to retain more background details and highlight infrared feature information. The control experiment results show that the method in this paper obtains better fusion effect in subjective and objective evaluation.  © 2022 IEEE.","Image enhancement; Image fusion; Infrared imaging; Light; Light sources; Signal encoding; Fused images; Fusion image; Fusion quality; Image-based; Infrared and visible image; Infrared image; Multi-scales; Multiscale information; Source images; Visible light images; Generative adversarial networks","generative adversarial network; image fusion; infrared image; multi-scale; visible light image","Conference paper","Final","","Scopus","2-s2.0-85138813166"
"Ma C.; Zhu J.; Li Y.; Li J.; Jiang Y.; Li X.","Ma, Chunyan (57209848050); Zhu, Junwu (55704615100); Li, Yujie (36761032400); Li, Jianru (56201183700); Jiang, Yi (55613237374); Li, Xin (55718311000)","57209848050; 55704615100; 36761032400; 56201183700; 55613237374; 55718311000","Single image super resolution via wavelet transform fusion and SRFeat network","2022","Journal of Ambient Intelligence and Humanized Computing","13","11","","5023","5031","8","10.1007/s12652-020-02065-0","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084968696&doi=10.1007%2fs12652-020-02065-0&partnerID=40&md5=40605a9bc0c43b43d7751e3f44e05862","Image super resolution is a vital research topic in the field of computer vision. It aims to reconstruct high resolution images from low resolution images. Although the conventional image super resolution methods have achieved good performance and effect, there are still have some issues, e.g., the high-frequency details information is insufficient, and the reconstruction process will bring additional noise, and most basic interpolation techniques produce blurry results. To settle the problems mentioned above, we consider combining the deep learning method with the frequency domain fusion method. In this paper, a novel single image super resolution method based on SRFeat network and wavelet fusion is proposed. First, the training image is taken as the input of the backbone SRFeat network, then the generative adversarial network training is carried out. Then, the up-sampling is utilized to obtain the coarse super resolved image. Finally, the output image after the network training is combined with the up-sampling image of the low-resolution image by Wavelet fusion to obtain the final result. Without increasing the depth of the network and the redundant parameters, the proposed method can achieve better reconstruct result. The experimental results show that the proposed method can not only reduce the probability of image distortion, but recover the global information of the reconstructed image and remove the noise brought by the reconstruction process. The PSNR value of the proposed method is improved 0.3 dB, and the SSIM is improved 0.02. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Frequency domain analysis; Generative adversarial networks; Image compression; Image fusion; Image reconstruction; Learning systems; Optical resolving power; Signal sampling; De-noising; Image super resolutions; Low resolution images; Reconstruction process; Single images; Superresolution; Superresolution methods; Wavelet fusion; Wavelet transform fusion; Wavelets transform; Wavelet transforms","Denoising; Generative adversarial network; Super resolution; Wavelet transform fusion","Article","Final","","Scopus","2-s2.0-85084968696"
"Li L.; Chen M.; Shi H.; Duan Z.; Xiong X.","Li, Lan (57212674121); Chen, Mingju (56039055500); Shi, Haode (57838592900); Duan, Zhengxu (57838137100); Xiong, Xingzhong (57737410700)","57212674121; 56039055500; 57838592900; 57838137100; 57737410700","Multiscale Structure and Texture Feature Fusion for Image Inpainting","2022","IEEE Access","10","","","82668","82679","11","10.1109/ACCESS.2022.3196021","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135753764&doi=10.1109%2fACCESS.2022.3196021&partnerID=40&md5=8bf865cac577f55b17fc4d2d4eb87519","In order to achieve interaction between structure and texture information in generative adversarial image inpainting networks and improve the semantic veracity of the restored images, unlike the original two-stage inpainting ideas where texture and structure are restored separately, this paper constructs a multi-scale fusion approach to image generation, which embeds images into two collaborative subtasks, that is, structure generation and texture synthesis under structural constraints. We also introduce a self-attention mechanism into the partial convolution of the encoder to enhance the long range contextual information acquisition of the model in image inpainting, and design a multi-scale fusion network to fuse the generated structure and texture feature, so that the structure and texture information can be reused for reconstruction, perception and style loss compensation, thus enabling the fused images to achieve global consistency. In the training phase, feature matching loss are introduced to enhance the image in terms of structural generation plausibility. Finally, through comparison experiments with other inpainting networks on the CelebA, Paris StreetView and Places2 datasets, it is demonstrated that our method constructed in this paper has better objective evaluation metrics, more effective inpainting of structural and texture information of corrupted images and better image inpainting performance. © 2013 IEEE.","Deep learning; Edge detection; Generative adversarial networks; Image enhancement; Image fusion; Image reconstruction; Image texture; Restoration; Semantics; Textures; Deep learning; Generative model; Generator; Image edge detection; Image Inpainting; Images reconstruction; Inpainting; Kernel; Texture information; Convolution","deep learning; generative adversarial network; generative model; Image inpainting","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135753764"
"Wang J.; Ke C.; Wu M.; Liu M.; Zeng C.","Wang, Juan (57216678183); Ke, Cong (57212622449); Wu, Minghu (55616993200); Liu, Min (56399667900); Zeng, Chunyan (57200673422)","57216678183; 57212622449; 55616993200; 56399667900; 57200673422","Infrared and visible image fusion based on laplacian pyramid and generative adversarial network","2021","KSII Transactions on Internet and Information Systems","15","5","","1761","1777","16","10.3837/tiis.2021.05.010","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107396813&doi=10.3837%2ftiis.2021.05.010&partnerID=40&md5=c4f3122d1620368926ba50eccc3ed45f","An image with infrared features and visible details is obtained by processing infrared and visible images. In this paper, a fusion method based on Laplacian pyramid and generative adversarial network is proposed to obtain high quality fusion images, termed as Laplacian-GAN. Firstly, the base and detail layers are obtained by decomposing the source images. Secondly, we utilize the Laplacian pyramid-based method to fuse these base layers to obtain more information of the base layer. Thirdly, the detail part is fused by a generative adversarial network. In addition, generative adversarial network avoids the manual design complicated fusion rules. Finally, the fused base layer and fused detail layer are reconstructed to obtain the fused image. Experimental results demonstrate that the proposed method can obtain state-of-the-art fusion performance in both visual quality and objective assessment. In terms of visual observation, the fusion image obtained by Laplacian-GAN algorithm in this paper is clearer in detail. At the same time, in the six metrics of MI, AG, EI, MS_SSIM, Qabf and SCD, the algorithm presented in this paper has improved by 0.62%, 7.10%, 14.53%, 12.18%, 34.33% and 12.23%, respectively, compared with the best of the other three algorithms.  © 2021 KSII.","Image coding; Laplace transforms; Adversarial networks; Fusion performance; Infrared and visible image; Infrared features; Laplacian Pyramid; Objective assessment; Visual observations; Visual qualities; Image fusion","Base layer; Detail layer; Fusion; Generative adversarial network; Laplacian pyramid","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85107396813"
"Zuo Y.; Fang Y.; Ma K.","Zuo, Yifan (36996817300); Fang, Yuming (8435698900); Ma, Kede (55624220200)","36996817300; 8435698900; 55624220200","The critical review of the growth of deep learning-based image fusion techniques; [深度学习时代图像融合技术进展]","2023","Journal of Image and Graphics","28","1","","102","117","15","10.11834/jig.220556","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147181781&doi=10.11834%2fjig.220556&partnerID=40&md5=020952af9a43f2d7392f384888b582b3","To capture more effective visual information of the natural scenes, multi-sensor imaging systems have been challenging in multiple configurations or modalities due to the hardware design constraints. It is required to fuse multiple source images into a single high-quality image in terms of rich and feasible perceptual information and few artifacts. To facilitate various image processing and computer vision tasks, image fusion technique can be used to generate a single and clarified image features. Traditional image fusion models are often constructed in accordance with label-manual features or unidentified feature-learned representations. The generalization ability of the models needs to be developed further. Deep learning technique is focused on progressive multi-layer features extraction via end-to-end model training. Most of demonstration-relevant can be learned for specific task automatically. Compared with the traditional methods, deep learning-based models can improve the fusion performance intensively in terms of image fusion. Current image fusion-related deep learning models are often beneficial based on convolutional neural networks (CNNs) and generative adversarial networks (GANs). In recent years, the newly network structures and training techniques have been incorporated for the growth of image fusion like vision transformers and meta-learning techniques. Most of image fusion-relevant literatures are analyzed from specific multi-fusion issues like exposure, focus, spectrum image, and modality issues. However, more deep learning-related model designs and training techniques is required to be incorporated between multi-fusion tasks. To draw a clear picture of deep learning-based image fusion techniques, we try to review the latest image fusion researches in terms of 1) dataset generation, 2) neural network construction, 3) loss function design, 4) model optimization, and 5) performance evaluation. For dataset generation, we emphasize two categories: a) supervised learning and b) unsupervised (or self-supervised) learning. For neural network construction, we distinguish the early or late stages of this construction process, and the issue of information fusion is implemented between multi-scale, coarse-to-fine and the adversarial networks-incorporated (i. e., discriminative networks) as well. For loss function design, the perceptual loss functions-specific method is essential for image fusion-related perceptual applications like multi-exposure and multi-focus image fusion. For model optimization, the generic first-order optimization techniques are covered (e. g., stochastic gradient descent (SGD), SGD + momentum, Adam, and AdamW) and the advanced alternation and bi-level optimization methods are both taken into consideration. For performance evaluation, a commonly-used quantitative metrics are reviewed for the manifested measurement of fusion performance. The relationship between the loss functions (also as a form of evaluation metrics) are used to drive the learning of CNN-based image fusion methods and the evaluation metrics. In addition, to illustrate the transfer feasibility of image fusion-consensus to a tailored image fusion application, a selection of image fusion methods is discussed (e. g., a high-quality texture image-fused depth map enhancement). Some popular computer vision tasks are involved in (such as image denoising, blind image deblurring, and image super-resolution), which can be resolved by image fusion innovatively. Finally, we review some potential challenging issues, including: 1) reliable and efficient ground-truth training data-constructed (i. e., the input image sequence and the predictable image-fused), 2) lightweight, interpretable, and generalizable CNN-based image fusion methods, 3) human or machine-related vision-perceptual calibrated loss functions, 4) convergence-accelerated image fusion models in related to adversarial training setting-specific and the bias-related of the test-time training, and 5) human-related ethical issues in relevant to fairness and unbiased performance evaluation. © 2023 Editorial and Publishing Board of JIG. All rights reserved.","","deep learning; deep neural networks; image fusion; image quality assessment; stochastic gradient descent (SGD)","Article","Final","","Scopus","2-s2.0-85147181781"
"Qin P.; Huang H.; Tang H.; Wang J.; Liu C.","Qin, Peng (57215550489); Huang, Huabing (57216494536); Tang, Hailong (57210148473); Wang, Jie (56050004800); Liu, Chong (57970016500)","57215550489; 57216494536; 57210148473; 56050004800; 57970016500","MUSTFN: A spatiotemporal fusion method for multi-scale and multi-sensor remote sensing images based on a convolutional neural network","2022","International Journal of Applied Earth Observation and Geoinformation","115","","103113","","","","10.1016/j.jag.2022.103113","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142150477&doi=10.1016%2fj.jag.2022.103113&partnerID=40&md5=397559d93ab6afae2c76f93f62f85af1","Spatiotemporal data fusion is a commonly-used and well-proven technique to enhance the application potential of multi-source remote sensing images. However, most existing methods have trouble in generating quality fusion results when areas covered by the images undergoes rapid land cover changes or images have substantial registration errors. While deep learning algorithms have demonstrated their capabilities for imagery fusion, it is challenging to apply deep-learning-based fusion methods in regions that experiences persistent cloud covers and have limited cloud-free imagery observations. To address these challenges, we developed a Multi-scene Spatiotemporal Fusion Network (MUSTFN) algorithm based on a Convolutional Neural Network (CNN). Our approach uses multi-level features to fuse images at different resolutions acquired by multiple sensors. Furthermore, MUSTFN uses the multi-scale features to overcome the effects of geometric registration errors between different images. Additionally, a multi-constrained loss function is proposed to improve the accuracy of imagery fusion over large areas and solve fusion and gap-filling problems simultaneously by utilizing cloud-contaminated images with the fine-tuning method. Compared with several commonly-used methods, our proposed MUSTFN performs better in fusing the 30-m Landsat-7 images and 500-m MODIS images over a small area that has undergone large changes (the average relative Mean Absolute Errors (rMAE) of the first four bands are 6.8% by MUSTFN as compared to 14.1% by the Enhanced Spatial and Temporal Adaptive Reflectance Fusion Model (ESTARFM), 12.8% by the Flexible Spatiotemporal Data Fusion (FSDAF), 8.4% by the Extended Super-Resolution Convolutional Neural Network (ESRCNN), 8.1% by the Spatiotemporal Fusion Using a Generative Adversarial Network (STFGAN)). In particularly for images at different resolutions with different registration accuracies (e.g., 16-m Chinese GaoFen-1 and 500-m MODIS), MUSTFN achieved fusion results of good quality with an average rMAE of 9.3% in spectral reflectance at the first four bands. Finally, we demonstrated the applicability of MUSTFN (average rMAE of 9.18%) when fusing long-term Landsat-8 composite images and MODIS images over a large region (830 km × 600 km). Overall, our results suggest the effectiveness of MUSTFN to address the challenges in imagery fusion, including rapid land cover changes between image acquisition dates, geometric misregistration between images and limited availabilities of cloud-free images. The program of MUSTFN is freely available at: https://github.com/qpyeah/MUSTFN. © 2022 The Authors","algorithm; artificial neural network; image processing; remote sensing; satellite data; spatiotemporal analysis","CNN; Large-area image fusion; Multi-scale fusion scenarios; Multi-sensor satellite data; Spatiotemporal fusion","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85142150477"
"Zhang Y.; Zhang Z.; Huo L.; Xie B.; Wang X.","Zhang, Yihan (57222750409); Zhang, Zhaohui (57104099000); Huo, Lina (56779564400); Xie, Bin (57203740709); Wang, Xiuqing (35216334100)","57222750409; 57104099000; 56779564400; 57203740709; 35216334100","Image Saliency Detection via Two-Stream Feature Fusion and Adversarial Learning; [结合双流特征融合及对抗学习的图像显著性检测]","2021","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","33","3","","376","384","8","10.3724/SP.J.1089.2021.18438","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103964641&doi=10.3724%2fSP.J.1089.2021.18438&partnerID=40&md5=71a5accbca0439c53b28b45f05d69776","To achieve meaningful combination of low-level features and semantic information of salient regions or targets, and to obtain saliency detection results with more complete structure and clearer boundary, an algorithm of color image saliency detection via two-stream feature fusion and adversarial learning (SaTSAL) is proposed. Firstly, different levels of image features are extracted from bottom to top by means of a two-stream heterogeneous backbone network based on VGG-16 and Res2Net-50. Secondly, in each stream, different feature maps from the same level are fetched into one convolution tower module to enrich intra-level multi-scale information. Thirdly, a predicted saliency map is generated by top-down laterally fusing of cross-stream feature maps level by level, so as to effectively make full use of high-level semantic features and low-level image features. Finally, under the mainframe of conditional generative adversarial networks (CGAN), a higher structural similarity between detected results and salient objects can be strengt¬h-e¬ne¬d by adversarial learning. By taking P-R curve, F-measure, mean absolute error and S-measure as evaluation indexes, comparative experiments performed on four public datasets including ECSSD, PASCAL- S, DUT-OMRON and DUTS-test show that SaTSAL algorithm is superior to most of other ten saliency detection methods based on deep learning. © 2021, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Deep learning; Design for testability; Image fusion; Learning algorithms; Object detection; Semantics; Adversarial learning; Adversarial networks; Comparative experiments; High-level semantic features; Low-level image features; Multi-scale informations; Semantic information; Structural similarity; Feature extraction","Adversarial learning; Conditional generative adversarial networks; Convolutional tower; Saliency detection; Two-stream feature fusion","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85103964641"
"Sun X.; Hu S.; Ma X.; Hu Q.; Xu S.","Sun, Xiuyi (57765756700); Hu, Shaohai (7404286949); Ma, Xiaole (57193220596); Hu, Qiu (57214857646); Xu, Shuwen (57207942043)","57765756700; 7404286949; 57193220596; 57214857646; 57207942043","IMGAN: Infrared and visible image fusion using a novel intensity masking generative adversarial network","2022","Infrared Physics and Technology","125","","104221","","","","10.1016/j.infrared.2022.104221","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132898421&doi=10.1016%2fj.infrared.2022.104221&partnerID=40&md5=70661e8c06dc0435ed97d2946b52e585","Fusion methods based on a deep convolutional neural network enable to catch visible details and infrared intensity. However, it is insufficient for most fusion models to extract the hierarchical features of visible images, leading to the loss of detailed information such as texture features in the final fused images. This paper proposes an unsupervised infrared and visible fusion model based on a novel intensity masking adversarial learning network (IMGAN). A designed deep generator with a residual dense attention module in the network can not only make full use of all characteristics of convolution layers but also achieve the more accurate extraction of visible detailed texture. Two discriminators are employed to balance the fused information, making it possible for the fused image to reserve much more information from source images. An infrared intensity masking loss is also proposed to reserve more detailed texture features in the weaker infrared area of the visible image. Furthermore, a large number of compared experiments with the infrared and visible benchmark datasets demonstrate the superiority of our IMGAN than the state-of-the-art methods in terms of both human eye visual perception and quantitative evaluation metrics. Our IMGAN can retain the infrared target information well and secure more detailed textures from the visible image. © 2022","Convolution; Convolutional neural networks; Deep neural networks; Discriminators; Image fusion; Image texture; Large dataset; Learning systems; Textures; Adversarial learning; Dual-discriminator; Fusion model; Infrared and visible image; Infrared and visible image fusion; Infrared intensity; Intensity masking loss; Learning network; Residual dense attention module; Visible image; Generative adversarial networks","Dual-discriminator; Generative adversarial network; Infrared and visible image fusion; Intensity masking loss; Residual dense attention module","Article","Final","","Scopus","2-s2.0-85132898421"
"Tan Z.; Gao M.; Yuan J.; Jiang L.; Duan H.","Tan, Zhenyu (56421169400); Gao, Meiling (57191226894); Yuan, Jun (57432147700); Jiang, Liangcun (55946431600); Duan, Hongtao (22233326100)","56421169400; 57191226894; 57432147700; 55946431600; 22233326100","A Robust Model for MODIS and Landsat Image Fusion Considering Input Noise","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5407217","","","","10.1109/TGRS.2022.3145086","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123713287&doi=10.1109%2fTGRS.2022.3145086&partnerID=40&md5=2065674a5485a003856a97624774f004","Significant progress has been made in spatiotemporal fusion for remote sensing images; however, most models require inputs to be free of clouds and without missing data, considerably confining their applications in practice. Due to recent advances in deep learning technologies, powerful modeling capabilities could be leveraged to bring potential solutions to this problem. This article proposes a novel architecture named the robust spatiotemporal fusion network (RSFN) based on the generative adversarial network and attention mechanism with dual temporal references to automatically handle input noise. The RSFN only needs one coarse-resolution image on the prediction date and two referential fine-resolution images before and after the prediction date as model inputs. Most notably, there is no special restriction attached on the data quality of referential images. The comparison with other models demonstrates the effectiveness of the RSFN model quantitatively and visually in four study areas using MODIS and Landsat images. Two main conclusions can draw from the experiments. First, the input data noise hardly affects the prediction results of the RSFN, and the RSFN can gain a comparable or even higher accuracy; conversely, the other methods only show limited resistance to input noise. Second, the RSFN with cloud-contaminated references outperforms the other models with cloud-free references after data filtering in the same study area during the same period. The satellite data quality usually varies significantly; the model robustness and fault tolerance are considered critical for actual applications. The RSFN is a simple end-to-end deep model with high accuracy and fault tolerance designed for spatiotemporal fusion with imperfect data inputs, showing promising prospects in practical applications. © 1980-2012 IEEE.","Biological systems; Deep learning; Fault tolerance; Forecasting; Generative adversarial networks; Image fusion; Radiometers; Biological system modeling; Deep learning; Noisy data; Remote-sensing; Robust spatiotemporal fusion network; Robustness; Spatial resolution; Spatio-temporal fusions; Spatiotemporal; Spatiotemporal phenomenon; data acquisition; data quality; Landsat; MODIS; remote sensing; satellite data; satellite imagery; Remote sensing","Data fusion; deep learning; noisy data; robust spatiotemporal fusion network (RSFN); robustness; spatiotemporal","Article","Final","","Scopus","2-s2.0-85123713287"
"Gupta N.; Srivastava H.S.; Sivasankar T.; Patel P.","Gupta, Neeharika (57772595200); Srivastava, Hari Shanker (7102601428); Sivasankar, Thota (57202915146); Patel, Parul (57208689231)","57772595200; 7102601428; 57202915146; 57208689231","A Deep Learning Framework for Fusion of Sar and Optical Satellite Imagery","2021","2021 IEEE India Geoscience and Remote Sensing Symposium, InGARSS 2021 - Proceedings","","","","488","491","3","10.1109/InGARSS51564.2021.9792062","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133166743&doi=10.1109%2fInGARSS51564.2021.9792062&partnerID=40&md5=f88fc43ad6c46be0b50dbe7a29230cc1","In remote sensing, image fusion is the process of converting information from various source images to a single image such that the features of the source are preserved and relevant information is being highlighted. Through this research work, we propose an unsupervised deep learning Generative Adversarial Network (GAN) for the fusion process of SAR and optical Images. For SAR image, we chose VV, VH, VV-VH bands and for optical image we did Principal Component Analysis (PCA) on its image bands to extract the top three principal components and compose an image out of it. Images were then converted into HSV space. The GAN is primarily trained to capture the maximum gradient features from both the images and secondarily to capture other noticeable features. Experimental results on both training and test samples indicate that the proposed method is able to preserve gradient features and other details of the images with respect to input images.  © 2021 IEEE.","Deep learning; Generative adversarial networks; Geometrical optics; Optical remote sensing; Principal component analysis; Radar imaging; Satellite imagery; Space optics; Space-based radar; Synthetic aperture radar; Deep learning; Gradient feature; Learning frameworks; Optical image; Optical satellite imagery; Optical-; Remote sensing images; SAR Images; Single images; Source images; Image fusion","deep learning; GAN; image fusion; optical; SAR","Conference paper","Final","","Scopus","2-s2.0-85133166743"
"Cheng L.; Tong Z.; Xie S.; Kersemans M.","Cheng, Liangliang (57191042910); Tong, Zongfei (57196084990); Xie, Shejuan (36683776800); Kersemans, Mathias (54958029400)","57191042910; 57196084990; 36683776800; 54958029400","IRT-GAN: A generative adversarial network with a multi-headed fusion strategy for automated defect detection in composites using infrared thermography","2022","Composite Structures","290","","115543","","","","10.1016/j.compstruct.2022.115543","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128445024&doi=10.1016%2fj.compstruct.2022.115543&partnerID=40&md5=2ee3a20aa424462f30d78501da6d3ee6","InfraRed Thermography (IRT) is a valuable diagnostic tool to non-destructively detect defects in fiber reinforced polymers. Often, a range of processing techniques are applied, e.g. principal component analysis, Fourier transformation, and thermographic signal reconstruction, in an attempt to enhance the defect detectability. Still, for the actual defect detection and evaluation, the interpretation by an expert operator is required which thus limits the (industrial) application potential of infrared thermography. This study proposes a Generative Adversarial Network (GAN) framework, termed IRT-GAN, to create a single unique thermal-image-to-segmentation translation of defects in composite materials. A large augmented numerical dataset has been simulated for a range of composite materials with different defects in order to train the IRT-GAN model. Integrated with the Spatial Group-wise Enhance layer, the IRT-GAN takes six pre-processed thermal images, thermographic signal reconstruction images in our case, as input and progressively fuses them via a multi-headed fusion strategy in the Generator. As such, this proposed IRT-GAN framework leads to the automated generation of a unique defect segmentation image. The high performance of the IRT-GAN, trained on the virtual dataset, is demonstrated on experimental data of both glass and carbon fiber reinforced polymers with various defect types, sizes, and depths. In addition, it is investigated how early, middle, and late-stage feature fusion in the GAN influences the segmentation performance. © 2022 Elsevier Ltd","Carbon fiber reinforced plastics; Deep learning; Defects; Fourier transforms; Generative adversarial networks; Image enhancement; Image fusion; Image segmentation; Infrared devices; Large dataset; Nondestructive examination; Principal component analysis; Signal to noise ratio; Automated defect detection; Composites material; Deep learning; Defect detection; Diagnostics tools; Fusion strategies; In-fiber; Network frameworks; Thermal images; Thermographic signal reconstruction; Thermography (imaging)","Composite; Deep learning; Defect detection; GAN; Image fusion; Infrared thermography; Non-destructive testing","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85128445024"
"He M.; Yu S.; Nie R.; Wang C.","He, Min (57188562814); Yu, Shishuang (57468512800); Nie, Rencan (23668359400); Wang, Chengchao (57468512700)","57188562814; 57468512800; 23668359400; 57468512700","Preference Learning to Multifocus Image Fusion via Generative Adversarial Network","2022","IEEE Transactions on Cognitive and Developmental Systems","14","4","","1604","1614","10","10.1109/TCDS.2021.3126330","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145346497&doi=10.1109%2fTCDS.2021.3126330&partnerID=40&md5=53fa7996508914951d1a8110abaa253b","Multifocus image fusion (MFIF) is to produce an all-in-focus fused image by integrating a pair of multifocus images with the same scene. In this article, an effective focus detection method is proposed for MFIF, by a generative adversarial network (GAN) with preference learning (PL). Benefitting from more obvious focus characteristics from the luminance channel in HSV, we take this luminance as the input of our GAN to carry out the easier focus detection, instead of the grayscale images in existing methods. On the other hand, to train our GAN more effectively, we utilize the ℓ2,1 norm to construct a focus fidelity loss with structural group sparseness, to regularize the generator loss, pledging a more accurate focus confidence map. More importantly, a novel learning strategy, termed PL, is further developed to enhance model training. Functionally, it assigns a larger learning weight to a sample more difficult to be learned. Extensive experiments demonstrate that our proposed method is superior to other state-of-the-art methods.  © 2016 IEEE.","Image fusion; Luminance; Focus detection; Fused images; Generative adversarial network; HSV space; Multifocus image fusion; Multifocus images; Preference learning; Structural loss; Generative adversarial networks","Generative adversarial network (GAN); HSV space; multifocus image fusion (MFIF); preference learning (PL); structural loss","Article","Final","","Scopus","2-s2.0-85145346497"
"Jiang L.; Zhang D.; Pan B.; Zheng P.; Che L.","Jiang, Liubing (37018302600); Zhang, Dian (57220764965); Pan, Bo (57649926400); Zheng, Peng (57645496400); Che, Li (55781404800)","37018302600; 57220764965; 57649926400; 57645496400; 55781404800","Multi-Focus Image Fusion Based on Generative Adversarial Network; [基于生成对抗网络的多聚焦图像融合]","2021","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","33","11","","1715","1725","10","10.3724/SP.J.1089.2021.18770","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119602447&doi=10.3724%2fSP.J.1089.2021.18770&partnerID=40&md5=069d4bcb7fe5f6352d7aee5146ea06ad","Multi-focus image fusion can fuse a series of images that have a different focus in the same scene. To overcome the disadvantage extraction of the blurring character in multi-focus images, a generative adversarial network model based on U-Net is proposed. Firstly, the generator uses U-Net and SSE to extract the feature of the multi-focus image, and fuses images. Then, the discriminator uses convolutional layers to distinguish the fused result between the existed and the generative. Furthermore, a loss function has the loss of adversarial in the generator, loss of mapping, loss of gradient, loss mean square error and the loss of adversarial in the discriminator. The train data of generative adversarial network uses the dataset of Pascal VOC2012 to generate and includes near-focus image, far-focus image, mapping image and all-in-focus image. The experimental result shows that the proposed generative adversarial network model can effectively extract the blurring feature in multi-focus images, and the fused image have good performances on mutual information, phase congruency and structural similarity. © 2021, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Image fusion; Image processing; Mapping; Mean square error; All-in-focus image; Fused images; Image mapping; Loss functions; Means square errors; Model-based OPC; Multifocus image fusion; Multifocus images; Network models; U-net; Generative adversarial networks","Generative adversarial network; Loss function; Multi-focus image fusion; U-Net","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85119602447"
"Wang C.; Luo D.; Liu Y.; Xu B.; Zhou Y.","Wang, Congqing (55735865200); Luo, Di (57477558700); Liu, Yang (57211609738); Xu, Bin (57712075100); Zhou, Yongjun (57408671400)","55735865200; 57477558700; 57211609738; 57712075100; 57408671400","Near-surface pedestrian detection method based on deep learning for UAVs in low illumination environments","2022","Optical Engineering","61","2","023103","","","","10.1117/1.OE.61.2.023103","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125789228&doi=10.1117%2f1.OE.61.2.023103&partnerID=40&md5=bc9971bc959c7a3295378edbbbeaa753","With the development of unmanned aerial vehicles (UAVs) and computer vision, target detection methods based on UAVs have been increasingly applied in military and civilian fields. Considering the adaptability requirements of low illumination environments such as rain, fog, and night, visible and infrared (IR) sensors are often installed on UAVs to perform in all-weather and all-day conditions. To improve the near-surface detection performance of UAVs in low illumination environments, a pedestrian detection method using image fusion and deep learning is proposed. Visible and IR pedestrian images are collected by the UAV. The corresponding aerial images are registered and annotated. These two different types of aerial images are aligned at the time sequence and matched using the scale invariant feature transform. A U-type generative adversarial network (GAN) is first developed to fuse visible and IR images. A convolutional block attention module is introduced to strengthen the pedestrian target information in the GAN. The spatial domain and channel domain attention mechanisms are proposed to generate color fusion images with rich details and solve the problems of feature extraction as well as fusion rules designed manually in the existing image fusion methods. Then, You Only Look Once Version 3 (YOLOv3)-spatial pyramid pooling combined with transfer learning is adopted using the fused images to train the model on our aerial dataset to verify the pedestrian detection performance. In addition, comparison experiments are carried out. The experimental results demonstrate that the YOLOv3 model is successfully transferred to the target dataset. The performance of the proposed detection model using the fused images for transfer training is the best among the different methods. Finally, the accuracy P, recall R, mean average precision, and F1 score reach 0.804, 0.923, 0.928, and 0.859, respectively.  © 2022 Society of Photo-Optical Instrumentation Engineers (SPIE).","Aircraft detection; Antennas; Deep learning; Generative adversarial networks; Image enhancement; Image fusion; Infrared imaging; Military vehicles; Aerial images; Attention mechanisms; Detection methods; Detection performance; Fused images; Low illuminations; Near surfaces; Pedestrian detection; Targets detection; Transfer learning; Unmanned aerial vehicles (UAV)","attention mechanism; generative adversarial network; images fusion; target detection; transfer learning; unmanned aerial vehicle","Article","Final","","Scopus","2-s2.0-85125789228"
"Hou J.; Zhang D.; Wu W.; Ma J.; Zhou H.","Hou, Jilei (57222092435); Zhang, Dazhi (57226173525); Wu, Wei (57222093753); Ma, Jiayi (26638975600); Zhou, Huabing (55447554500)","57222092435; 57226173525; 57222093753; 26638975600; 55447554500","A generative adversarial network for infrared and visible image fusion based on semantic segmentation","2021","Entropy","23","3","376","","","","10.3390/e23030376","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103580575&doi=10.3390%2fe23030376&partnerID=40&md5=bbf5ceeeb1357387229f69655e5ad51d","This paper proposes a new generative adversarial network for infrared and visible image fusion based on semantic segmentation (SSGAN), which can consider not only the low-level features of infrared and visible images, but also the high-level semantic information. Source images can be divided into foregrounds and backgrounds by semantic masks. The generator with a dual-encoder-single-decoder framework is used to extract the feature of foregrounds and backgrounds by different encoder paths. Moreover, the discriminator’s input image is designed based on semantic segmentation, which is obtained by combining the foregrounds of the infrared images with the backgrounds of the visible images. Consequently, the prominence of thermal targets in the infrared images and texture details in the visible images can be preserved in the fused images simultaneously. Qualitative and quantitative experiments on publicly available datasets demonstrate that the proposed approach can significantly outperform the state-of-the-art methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Generative adversarial network; Image fusion; Infrared image; Semantic segmentation; Visible image","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103580575"
"Xu Q.; Li Y.; Nie J.; Liu Q.; Guo M.","Xu, Qizhi (50562407300); Li, Yuan (57219397235); Nie, Jinyan (57220955756); Liu, Qingjie (55534263100); Guo, Mengyao (57393351400)","50562407300; 57219397235; 57220955756; 55534263100; 57393351400","UPanGAN: Unsupervised pansharpening based on the spectral and spatial loss constrained Generative Adversarial Network","2023","Information Fusion","91","","","31","46","15","10.1016/j.inffus.2022.10.001","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140142315&doi=10.1016%2fj.inffus.2022.10.001&partnerID=40&md5=d0fbc8eb596c9d58af6f89650683a1ed","It is observed that, in most of the CNN-based pansharpening methods, the multispectral (MS) images are taken as the ground truth, and the downsampled panchromatic (Pan) and MS images are taken as the training data. However, the trained models from the downsampled images are not suitable for the pansharpening of the MS images with rich spatial and spectral information at their original spatial resolution. To tackle this problem, a novel iterative network based on spectral and textural loss constrained Generative Adversarial Network (GAN) is proposed for pansharpening. First, instead of directly outputting the fused imagery, the GAN focuses on generating the mean difference image. The input of the GAN is a good initial difference image, which will make the network work better. Second, the coarse-to-fine fusion framework is designed to generate the fused imagery. It uses two optimized discriminators to distinguish the generated images, and performs multi-level fusion processing on PAN and MS images to generate the best pansharpening image in full resolution. Finally, the well-designed loss functions are embedded into both the generator and the discriminators to accurately preserve the fidelity of the fused imagery. We validated our method by the images from QuickBird, GaoFen-2 and WorldView-2 satellites. The experimental results demonstrated that the proposed method obtained a better fusion performance than the state-of-the-art methods in both visual comparison and quantitative evaluation. © 2022 Elsevier B.V.","Convolutional neural networks; Image fusion; Iterative methods; Convolutional neural network; Difference images; Ground truth; Iterative networks; Multispectral images; Pan-sharpening; Spatial informations; Spatial resolution; Spectral information; Training data; Generative adversarial networks","Convolutional neural network; Generative Adversarial Network; Image fusion; Pansharpening","Article","Final","","Scopus","2-s2.0-85140142315"
"Gao X.; Liu S.","Gao, Xueyan (57971953600); Liu, Shiguang (36621189700)","57971953600; 36621189700","DAFuse: A fusion for infrared and visible images based on generative adversarial network","2022","Journal of Electronic Imaging","31","4","043023","","","","10.1117/1.JEI.31.4.043023","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142262474&doi=10.1117%2f1.JEI.31.4.043023&partnerID=40&md5=dd0d3b2dc30f8dfca23b0d4c58c6bce8","Infrared and visible image fusion is a popular research hotspot in the field of image processing. However, the existing fusion methods still have some limitations, such as insufficient use of intermediate information and inability to focus on features that are meaningful for fusion. To solve these problems, we propose an infrared and visible image fusion method based on generative adversarial networks with dense connection and attention mechanism (DAFuse). Since infrared and visible image are different modalities, we design two branches to extract the features in infrared and visible image, respectively. To make full use of the features extracted from the middle layer and make the model focus on useful information, we introduce the dense block, channel attention mechanism, and spatial attention mechanism into the generator. The self-attention model is incorporated into the discriminator. The proposed method not only retains rich texture detail features and sufficient contrast information but also conforms to human visual perception. Extensive qualitative and quantitative experimental results show that the proposed method has better performance in visual perception and quantitative evaluation than the existing state-of-the-art methods.  © 2022 SPIE and IS&T.","Generative adversarial networks; Information use; Infrared imaging; Infrared radiation; Textures; Vision; Attention mechanisms; Dense block; Fusion methods; Hotspots; Image fusion methods; Image-based; Images processing; Infrared and visible image; Infrared image; Visible image; Image fusion","attention mechanism; dense block; generative adversarial network; image fusion; infrared image; visible image","Article","Final","","Scopus","2-s2.0-85142262474"
"Jiang M.; Zhi M.; Wei L.; Yang X.; Zhang J.; Li Y.; Wang P.; Huang J.; Yang G.","Jiang, Mingfeng (16175216000); Zhi, Minghao (57219910908); Wei, Liying (57224524874); Yang, Xiaocheng (55971485400); Zhang, Jucheng (55581543800); Li, Yongming (56457768700); Wang, Pin (56456764000); Huang, Jiahao (57223238396); Yang, Guang (57216243504)","16175216000; 57219910908; 57224524874; 55971485400; 55581543800; 56457768700; 56456764000; 57223238396; 57216243504","FA-GAN: Fused attentive generative adversarial networks for MRI image super-resolution","2021","Computerized Medical Imaging and Graphics","92","","101969","","","","10.1016/j.compmedimag.2021.101969","18","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112746586&doi=10.1016%2fj.compmedimag.2021.101969&partnerID=40&md5=674f2c377dd390e8e183aa9d379eafcf","High-resolution magnetic resonance images can provide fine-grained anatomical information, but acquiring such data requires a long scanning time. In this paper, a framework called the Fused Attentive Generative Adversarial Networks(FA-GAN) is proposed to generate the super- resolution MR image from low-resolution magnetic resonance images, which can reduce the scanning time effectively but with high resolution MR images. In the framework of the FA-GAN, the local fusion feature block, consisting of different three-pass networks by using different convolution kernels, is proposed to extract image features at different scales. And the global feature fusion module, including the channel attention module, the self-attention module, and the fusion operation, is designed to enhance the important features of the MR image. Moreover, the spectral normalization process is introduced to make the discriminator network stable. 40 sets of 3D magnetic resonance images (each set of images contains 256 slices) are used to train the network, and 10 sets of images are used to test the proposed method. The experimental results show that the PSNR and SSIM values of the super-resolution magnetic resonance image generated by the proposed FA-GAN method are higher than the state-of-the-art reconstruction methods. © 2021 The Author(s)","Algorithms; Attention; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Image enhancement; Image fusion; Magnetism; Optical resolving power; Resonance; Adversarial networks; Anatomical information; Convolution kernel; Important features; Local fusion features; Reconstruction method; Spectral normalization; Super resolution; article; attention; human; nuclear magnetic resonance imaging; algorithm; attention; image processing; Magnetic resonance imaging","Attention; Generative adversarial networks; Mechanism; MRI; Super-resolution","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85112746586"
"Yuan C.; Wang H.; He P.; Luo J.; Li B.","Yuan, Chao (57417570600); Wang, Hongxia (35216738700); He, Peisong (57075586900); Luo, Jie (57418898600); Li, Bin (57102112000)","57417570600; 35216738700; 57075586900; 57418898600; 57102112000","GAN-based image steganography for enhancing security via adversarial attack and pixel-wise deep fusion","2022","Multimedia Tools and Applications","81","5","","6681","6701","20","10.1007/s11042-021-11778-z","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123060014&doi=10.1007%2fs11042-021-11778-z&partnerID=40&md5=be2e0e0e817fe9303d5eaaf813cb04df","In recent years, the development of steganalysis based on convolutional neural networks (CNN) has brought new challenges to the security of image steganography. However, the current steganographic methods are difficult to resist the detection of CNN-based steganalyzers. To solve this problem, we propose an end-to-end image steganographic scheme based on generative adversarial networks (GAN) with adversarial attack and pixel-wise deep fusion. There are mainly four modules in the proposed scheme: the universal adversarial network is utilized in Attack module to fool CNN-based steganalyzers for enhancing security; Encoder module is seen as the generator to implement the pixel-wise deep fusion for imperceptible information embedding with high payload; Decoder module is responsible for the process of recovering embedded information; Critic module is designed for the discriminator to provide objective scores and conduct adversarial training. Besides, multiple loss functions together with Wasserstein GAN strategy are applied to enhance the stability and availability of the proposed scheme. Experiments on different datasets have verified the advantages of adding universal adversarial perturbations for higher security against CNN-based steganalyzers without compromising imperceptibility. Compared with state-of-the-art methods, the proposed scheme has achieved better performance in security. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Convolutional neural networks; Image enhancement; Image fusion; Network security; Pixels; Steganography; 'current; Adversarial attack; Adversarial networks; Convolutional neural network; End to end; Image steganographic scheme; Image steganography; Network-based; Pixel-wise deep fusion; Steganalysis; Generative adversarial networks","Adversarial attack; Generative adversarial networks; Image steganography; Pixel-wise deep fusion","Article","Final","","Scopus","2-s2.0-85123060014"
"Wang Y.; Gu L.; Li X.; Gao F.; Jiang T.; Ren R.","Wang, Yuhan (57205026672); Gu, Lingjia (15834718400); Li, Xiaofeng (57192498694); Gao, Fang (58018538300); Jiang, Tao (57226766245); Ren, Ruizhi (15835523400)","57205026672; 15834718400; 57192498694; 58018538300; 57226766245; 15835523400","An Improved Spatiotemporal Fusion Algorithm for Monitoring Daily Snow Cover Changes With High Spatial Resolution","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5413617","","","","10.1109/TGRS.2022.3224126","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144077745&doi=10.1109%2fTGRS.2022.3224126&partnerID=40&md5=c4145953ec063badeee08f9ff18e6948","Considering the tradeoff between spatial resolution and temporal resolution, spatiotemporal fusion has become a promising technique to monitor snow cover dynamics with both high spatial and temporal resolutions. The representative spatiotemporal fusion methods, e.g., spatial-temporal data fusion approach (STDFA), usually exist obvious phenomenon of spectral distortion when the surface reflectance changes nonlinearly, which affects the quality of the spatiotemporal fusion image. To address this issue, an effective STDFA-matching-Pix2pix-generative adversarial network (SMPG) algorithm combining the unmixing-based method, deep learning method, and prematching and postmatching module is proposed to reduce the spectral distortion of STDFA fusion image. The high-temporal-low-spatial (HTLS) resolution MOD09GA data and high-spatial-low-temporal resolution (HSLT) Landsat 8 data are selected in this study. An SMPG algorithm is first employed to obtain daily high-spatial-high-temporal (HSHT) images, and then, daily snow cover results with a spatial resolution of 30 m are obtained by calculating the normalized difference snow index (NDSI). The SMPG algorithm is further compared with STDFA, spatial and temporal adaptive reflectance fusion model (STARFM), Flexible Spatiotemporal DAta Fusion (FSDAF), Swin spatiotemporal fusion model (SwinSTFM), and generative adversarial network-based spatiotemporal fusion model (GAN-STFM). The experimental results indicate that the proposed algorithm yields better overall performance in daily spatiotemporal fusion image and snow cover result with a spatial resolution of 30 m. The mean correlation coefficient (CC) of SMPG can achieve 0.962, which is 0.06-0.36 higher than that of other spatiotemporal fusion methods. The error between the percentage of snow cover area obtained through SMPG and validation data is within 0.84%. © 2022 IEEE.","Deep learning; Image fusion; Image resolution; Reflection; Fusion image; Fusion model; Matchings; Network algorithms; Pix2pixgan; Snow covers; Spatial resolution; Spatial temporal data fusion approach; Spatial-temporal data; Spatio-temporal fusions; algorithm; correlation; environmental monitoring; Landsat; machine learning; pixel; snow cover; spatial resolution; spatiotemporal analysis; spectral reflectance; surface reflectance; Generative adversarial networks","Pix2pixGAN; snow cover; spatial-temporal data fusion approach (STDFA); spatiotemporal fusion","Article","Final","","Scopus","2-s2.0-85144077745"
"Guo S.; Wang Y.; Yang W.","Guo, Shaoyi (58000740200); Wang, Yaxian (58000951200); Yang, Wei (57215197021)","58000740200; 58000951200; 57215197021","A Study on the Collision of Artificial Intelligence and Art Based on Generative Adversarial Networks (GAN)","2022","Proceedings - 2022 International Conference on 3D Immersion, Interaction and Multi-Sensory Experiences, ICDIIME 2022","","","","27","31","4","10.1109/ICDIIME56946.2022.00014","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143778029&doi=10.1109%2fICDIIME56946.2022.00014&partnerID=40&md5=46eb8fcec8ecdfdba1b95973abd30944","The cross-collision of artificial intelligence and art has attracted significant attention in related fields, such as the migration and integration of painting styles. Due to the inherent differences of different painting techniques, direct application of existing methods does not bring satisfactory results. This paper proposes a multi-layer Pyramid Generative Adversarial Network (MPGAN), an end-to-end Generative Adversarial Network(GAN) based architecture, whose advantages are in image quality and better-scaled fusion of painting styles compared with state-of-the-art algorithms demonstrate the effectiveness of the method. The method's effectiveness is demonstrated by its advantages in image quality compared to the state-of-the-art algorithms and its better scaling fusion of painting styles.  © 2022 IEEE.","Image fusion; Image quality; Network layers; Cross collisions; End to end; Multi-layer pyramid; Multi-layers; Network-based architectures; Painting style migration; Painting techniques; Scale fusion; Scalings; State-of-the-art algorithms; Generative adversarial networks","artificial intelligence; generative adversarial networks; multi-layer pyramids; painting style migration; scale fusion","Conference paper","Final","","Scopus","2-s2.0-85143778029"
"Dai H.; Liu X.; Qiao Y.; Zheng K.; Xiao X.; Cai Z.","Dai, Haoran (57573734600); Liu, Xiaobo (36647829600); Qiao, Yulin (57215855806); Zheng, Kexin (57573734500); Xiao, Xiao (57573926200); Cai, Zhihua (56424630500)","57573734600; 36647829600; 57215855806; 57573734500; 57573926200; 56424630500","UFN-GAN: An unsupervised generative adversarial network for remote sensing image fusion","2021","Proceeding - 2021 China Automation Congress, CAC 2021","","","","1803","1808","5","10.1109/CAC53003.2021.9727490","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128081131&doi=10.1109%2fCAC53003.2021.9727490&partnerID=40&md5=d58e630e4449514acdb2991dd153cc28","Different sensors acquire different images in the same area, such as multi-spectral (MS) images and panchromatic (PAN) images. Normally, the MS images possess high spectral resolution but low spatial resolution, while PAN images are opposite in the distribution of spectral and spatial information. Image fusion is a common method to obtain the information of PAN and MS images simultaneously. To generate clearer fusion image with abundant information, we design an unsupervised fusion net based on generative adversarial network (GAN), named UFN-GAN for remote sensing image fusion. In our proposed UFN-GAN, an adversarial net is designed between our generator and two discriminators to adequately retain the spectral and spatial information of original images without supervision. MS images and PAN images are fused by our generator, which consists of an encoder and a decoder. Our encoder is used to extract deeper feature maps of the original images, and the decoder is applied to rebuild images. Furthermore, the Spatial-Information-Enhancement (SIE) model is utilized to obtain spatial information of MS images for enhancing PAN image, and the Edge-Detection-Registration (EDR) method is applied to register the original images to avoid fused images distortion. At last, experiments are performed on QuickBird and GaoFen-2 datasets. © 2021 IEEE","Decoding; Deep learning; Edge detection; Image enhancement; Image fusion; Remote sensing; Signal encoding; Spectral resolution; Unsupervised learning; Deep learning; Feature map; Fusion image; High spectral resolution; Multispectral images; Original images; Remote sensing images; Spatial informations; Spatial resolution; Spectral information; Generative adversarial networks","Deep learning; Generative adversarial network; Image fusion; Remote sensing images; Unsupervised learning; Unsupervised learning","Conference paper","Final","","Scopus","2-s2.0-85128081131"
"Tan Z.; Gao M.; Li X.; Jiang L.","Tan, Zhenyu (56421169400); Gao, Meiling (57191226894); Li, Xinghua (55626987300); Jiang, Liangcun (55946431600)","56421169400; 57191226894; 55626987300; 55946431600","A Flexible Reference-Insensitive Spatiotemporal Fusion Model for Remote Sensing Images Using Conditional Generative Adversarial Network","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","","","","","10.1109/TGRS.2021.3050551","35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100463952&doi=10.1109%2fTGRS.2021.3050551&partnerID=40&md5=917ee417718b3afaec23296f8a1b2296","Due to the tradeoff between spatial and temporal resolutions of remote sensing images, spatiotemporal fusion models were proposed to synthesize the high spatiotemporal image series. Currently, spatiotemporal fusion models usually employ one coarse-resolution image acquired on a prediction date and at least another pair of coarse-fine resolution images close to the prediction time as references to derive the fine-resolution image on the prediction date. After years of development, the model accuracy has gained a certain improvement, but nearly, all the models require at least three image inputs and rigid time constraints must be applied to the references to guarantee the fusion accuracy. However, it is not always that easy to collect adequate data pairs for fine-resolution image series simulation in practice because of the bad weather condition or the time inconsistency between the coarse-fine resolution data sources, which causes some difficulties in the actual application. This article introduces the conditional generative adversarial network (CGAN) and switchable normalization technique into the spatiotemporal fusion problem and proposes a flexible deep network named the GAN-based SpatioTemporal Fusion Model (GAN-STFM) to reduce the number of model inputs and broke the time restriction on reference image selection. The GAN-STFM just needs a coarse-resolution image on the prediction date and another fine-resolution reference image at an arbitrary time in the same area for model inputs. As far as we know, this is the first spatiotemporal fusion model that requires only two images as model inputs and puts no restriction on the acquisition time of references. Even so, the GAN-STFM performs on par or better than other classical fusion models in the experiments. With this improvement, the data preparation for spatiotemporal fusion tends to be much easier than before, showing a promising perspective for practical applications.  © 1980-2012 IEEE.","Forecasting; Image enhancement; Remote sensing; Adversarial networks; Fine-resolution images; Remote sensing images; Resolution images; Spatial and temporal resolutions; Spatio-temporal fusions; Spatiotemporal images; Time inconsistency; image; network analysis; remote sensing; spatiotemporal analysis; Image fusion","Conditional generative adversarial network (CGAN); convolutional neural network (CNN); data fusion; deep learning; generative adversarial network (GAN)-based SpatioTemporal Fusion Model (GAN-STFM); remote sensing; spatiotemporal","Article","Final","","Scopus","2-s2.0-85100463952"
"Xue W.; Huan-Xin C.; Sheng-Yi S.; Ze-Qin J.; Kai C.; Li C.","Xue, Wang (57580649800); Huan-Xin, Cheng (57581039700); Sheng-Yi, Sun (57581139700); Ze-Qin, Jiang (57580649900); Kai, Cheng (57580650000); Li, Cheng (57580746800)","57580649800; 57581039700; 57581139700; 57580649900; 57580650000; 57580746800","MSFSA-GAN: Multi-Scale Fusion Self Attention Generative Adversarial Network for Single Image Deraining","2022","IEEE Access","10","","","34442","34448","6","10.1109/ACCESS.2022.3162224","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128474460&doi=10.1109%2fACCESS.2022.3162224&partnerID=40&md5=dd90aaa3692116526c86f9b59b688298","Bad weather such as rainy days will seriously affect the image quality and the accuracy of visual processing algorithm. In order to improve the image deraining quality, a multi-scale fusion self attention generation adversarial network (MSFSA-GAN) is proposed. This network uses different scales to extract input characteristics of rain lines. First, Gaussian pyramid rain maps with different scales are generated by Gaussian algorithm. Then, in order to extract the features of rain lines with different scales, the coarse fusion module and fine fusion module are designed respectively. Next, the extracted features are fused at different scales. In this process, the self attention mechanism is introduced to make the network focus on the extracted features of different scales. And before the fusion, the rain pattern reconstruction operation is also carried out, so that the network can reproduce the input image more perfectly. Finally, it is input into the discriminator network with dense blocks to obtain the image that removes the rain lines. We used R100H and R100L datasets to train and test our network. The results show that our method as high as 27.79 in PSNR and UQI is 0.94, which is superior to the existing methods in performance. Meanwhile, we also compared the cost of time, the result of our network is only 0.02s.  © 2013 IEEE.","Generative adversarial networks; Image enhancement; Image fusion; Adversarial networks; Dense block; Fusion modules; Multi-scale fusion self attention generation adversarial network; Multiscale fusion; Rain removals; Rainy days; Self attention; Single images; Visual-processing; Rain","dense block; MSFSA-GAN; Rain removal; self attention","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85128474460"
"Jing B.; Ding H.; Yang Z.; Li B.; Bao L.","Jing, Beibei (57225923498); Ding, Hongwei (35995295900); Yang, Zhijun (57188805913); Li, Bo (57195397849); Bao, Liyong (36141085800)","57225923498; 35995295900; 57188805913; 57195397849; 36141085800","Video prediction: a step-by-step improvement of a video synthesis network","2022","Applied Intelligence","52","4","","3640","3652","12","10.1007/s10489-021-02500-5","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109980636&doi=10.1007%2fs10489-021-02500-5&partnerID=40&md5=efc2e59bd2204a677192b198cf35ed79","Although focusing on the field of video generation has made some progress in network performance and computational efficiency, there is still much room for improvement in terms of the predicted frame number and clarity. In this paper, a depth learning model is proposed to predict future video frames. The model can predict video streams with complex pixel distributions of up to 32 frames. Our framework is mainly composed of two modules: a fusion image prediction generator and an image-video translator. The fusion picture prediction generator is realized by a U-Net neural network built by a 3D convolution, and the image-video translator is composed of a conditional generative adversarial network built by a 2D convolution network. In the proposed framework, given a set of fusion images and labels, the image picture prediction generator can learn the pixel distribution of the fitted label pictures from the fusion images. The image-video translator then translates the output of the fused image prediction generator into future video frames. In addition, this paper proposes an accompanying convolution model and corresponding algorithm for improving image sharpness. Our experimental results prove the effectiveness of this framework. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Computational efficiency; Convolution; Forecasting; Image fusion; Pixels; Video cameras; Adversarial networks; Convolution model; Image sharpness; Learning models; Pixel distribution; Video generation; Video prediction; Video synthesis; Image enhancement","2D convolution network; 3D convolution; Conditional generative adversarial nets; Fusion image prediction generator; Image-video translator; Video generation","Article","Final","","Scopus","2-s2.0-85109980636"
"Tao M.; Tang H.; Wu F.; Jing X.; Bao B.-K.; Xu C.","Tao, Ming (57219565980); Tang, Hao (57208238003); Wu, Fei (57211431144); Jing, Xiaoyuan (7202420489); Bao, Bing-Kun (24922856800); Xu, Changsheng (7404181140)","57219565980; 57208238003; 57211431144; 7202420489; 24922856800; 7404181140","DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","16494","16504","10","10.1109/CVPR52688.2022.01602","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136334410&doi=10.1109%2fCVPR52688.2022.01602&partnerID=40&md5=08d9998ce94eaf94b80e3037222d97c8","Synthesizing high-quality realistic images from text descriptions is a challenging task. Existing text-to-image Generative Adversarial Networks generally employ a stacked architecture as the backbone yet still remain three flaws. First, the stacked architecture introduces the entanglements between generators of different image scales. Second, existing studies prefer to apply and fix extra networks in adversarial learning for text-image semantic consistency, which limits the supervision capability of these networks. Third, the cross-modal attention-based text-image fusion that widely adopted by previous works is limited on several special image scales because of the computational cost. To these ends, we propose a simpler but more effective Deep Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose: (i) a novel one-stage text-to-image backbone that directly synthesizes high-resolution images without entanglements between different generators, (ii) a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty and One-Way Output, which enhances the text-image semantic consistency without introducing extra networks, (iii) a novel deep text-image fusion block, which deepens the fusion process to make a full fusion between text and visual features. Compared with current state-of-the-art methods, our proposed DF-GAN is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance on widely used datasets. Code is available at https://github.com/tobran/DF-GAN. © 2022 IEEE.","Computer vision; Image enhancement; Image fusion; Network architecture; Semantics; Image and video synthesis and generation; Image scale; Image semantics; Images synthesis; Semantic consistency; Simple++; Text images; Video generation; Video synthesis; Vision + language; Generative adversarial networks","Image and video synthesis and generation; Vision + language","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85136334410"
"Liu M.; Mukerji T.","Liu, Mingliang (57201483621); Mukerji, Tapan (7003413039)","57201483621; 7003413039","Multiscale Fusion of Digital Rock Images Based on Deep Generative Adversarial Networks","2022","Geophysical Research Letters","49","9","e2022GL098342","","","","10.1029/2022GL098342","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133780065&doi=10.1029%2f2022GL098342&partnerID=40&md5=184477e2812594947a58c26c59166b7e","Computation of petrophysical properties on digital rock images is becoming important in geoscience. However, it is usually complicated for natural heterogeneous porous media due to the presence of multiscale pore structures. To capture the heterogeneity of rocks, we develop a method based on deep generative adversarial networks to assimilate multiscale imaging data for the generation of synthetic high-resolution digital rocks having a large field of view. The reconstructed images not only honor the geometric structures of 3-D micro-CT images but also recover fine details existing at the scale of 2-D scanning electron microscopy images. Furthermore, the consistency between the real and synthetically generated images in terms of porosity, specific perimeter, two-point correlation and effective permeability reveals the validity of our proposed method. It provides an effective way to fuse multiscale digital rock images for better characterization of heterogeneous porous media and better prediction of pore-scale flow and petrophysical properties. © 2022. American Geophysical Union. All Rights Reserved.","Computerized tomography; Deep learning; E-learning; Generative adversarial networks; Image fusion; Petrophysics; Rocks; Scanning electron microscopy; Deep learning; Digital rock physic; Geosciences; Heterogeneous porous media; Image-based; Multiscale; Multiscale fusion; Petrophysical properties; Pores structure; Rock physics; heterogeneity; image analysis; permeability; petrology; porosity; rock property; Pore structure","data fusion; deep learning; digital rock physics; generative adversarial networks; multiscale","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85133780065"
"Arun P.V.; Sadeh R.; Avneri A.; Tubul Y.; Camino C.; Buddhiraju K.M.; Porwal A.; Lati R.N.; Zarco-Tejada P.J.; Peleg Z.; Herrmann I.","Arun, P.V. (57202034266); Sadeh, R. (57209321842); Avneri, A. (57207958385); Tubul, Y. (57266839800); Camino, C. (55387868700); Buddhiraju, K.M. (36815713200); Porwal, A. (9738526000); Lati, R.N. (37088792100); Zarco-Tejada, P.J. (6701731228); Peleg, Z. (15136927700); Herrmann, I. (36070624100)","57202034266; 57209321842; 57207958385; 57266839800; 55387868700; 36815713200; 9738526000; 37088792100; 6701731228; 15136927700; 36070624100","Multimodal Earth observation data fusion: Graph-based approach in shared latent space","2022","Information Fusion","78","","","20","39","19","10.1016/j.inffus.2021.09.004","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115401406&doi=10.1016%2fj.inffus.2021.09.004&partnerID=40&md5=180247e9a56560637a79ea9eed3cfb89","Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few. © 2021 Elsevier B.V.","Antennas; Convolutional neural networks; Generative adversarial networks; Graphic methods; Image enhancement; Image fusion; Learning systems; Metadata; Observatories; Satellite imagery; Signal encoding; Unmanned aerial vehicles (UAV); Convolutional neural network; Cross-domain; Cross-modal; Earth observation data; Ground measured spectrum; HyperSpectral; Multi-modal; Multi-spectral; Multispectral unmanned aerial vehicle; Spectra's; Convolution","Convolutional neural networks; Fusion; Ground measured spectra; Hyperspectral; Multispectral UAV","Article","Final","","Scopus","2-s2.0-85115401406"
"Yin H.; Yue Y.","Yin, Haitao (54421600200); Yue, Yongying (57930607700)","54421600200; 57930607700","Medical Image Fusion Based on Semisupervised Learning and Generative Adversarial Network; [基于半监督学习和生成对抗网络的医学图像融合算法]","2022","Laser and Optoelectronics Progress","59","22","2215005","","","","10.3788/LOP202259.2215005","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139985653&doi=10.3788%2fLOP202259.2215005&partnerID=40&md5=8ad496011306d38f5e6b71fb1845a7e3","To efficiently employ a small amount of labeled data, a medical image fusion network based on semisupervised learning and a generative adversarial network is developed. The developed fusion network comprises one generator and two discriminators. A semisupervised learning scheme is developed to train the network, including the supervised-training, unsupervised training, and parameters fine-tuning phases. Furthermore, the generator is constructed using a fusion inspired U-Net, squeeze and excitation attention modules. The discriminator contains three convolution layers, one fully connected layer, and a sigmoid activation function. The experimental findings on different multimodal medical images exhibit the proposed approach is competitive with six existing deep-learning based approaches in terms of visual effects and objective indexes. Moreover, the ablation investigations show the effectiveness of a semisupervised learning scheme that can enhance the quality of fused images. © 2022 Universitat zu Koln. All rights reserved.","","attention mechanism; generate adversarial network; machine vision; medical image fusion; semisupervised learning","Article","Final","","Scopus","2-s2.0-85139985653"
"Chen S.; Shu T.; Zhao H.; Wan Q.; Huang J.; Li C.","Chen, Shengchao (57838581100); Shu, Ting (57838218000); Zhao, Huan (57207407321); Wan, Qilin (57837942700); Huang, Jincan (57838125000); Li, Cailing (57837942800)","57838581100; 57838218000; 57207407321; 57837942700; 57838125000; 57837942800","Dynamic Multiscale Fusion Generative Adversarial Network for Radar Image Extrapolation","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","5115811","","","","10.1109/TGRS.2022.3193458","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135750849&doi=10.1109%2fTGRS.2022.3193458&partnerID=40&md5=fd98178f629baaa29499e0090174c2d7","Typhoons, a kind of devastating natural disaster, have caused incalculable damages worldwide. The meteorological radar image is essential for weather forecasting, especially typhoons. The weather nowcasting (future 0-6 h) can be implemented via extrapolating radar images without using the primary weather forecasting method - the numerical weather prediction model. However, the existing related techniques based on statistics or artificial intelligence were not efficient enough. In this article, a novel radar image extrapolation algorithm named dynamic multiscale fusion-generative adversarial network (DMSF-GAN) was proposed. DMSF-GAN captures the future radar image distribution based on current radar images through modifying the GAN. In the generative module of GAN, an auto-encoder consisting of dynamic inception-3-D and feature connection blocks extracts significant features from current radar images. The feasibility of the proposed model was verified on a real radar image dataset, and the experimental results proved that the proposed algorithm could effectively capture the location and pattern of the future radar echo, especially for typhoon weather systems. Compared with the mainstream methods of radar image extrapolation such as optical-flow and recurrent neural network (RNN)-based models, DMSF-GAN has a more superior and robust performance, which is also suitable for running on low-configuration machines.  © 1980-2012 IEEE.","Disasters; Extrapolation; Hurricanes; Image fusion; Meteorological radar; Numerical methods; Radar imaging; Rain; Recurrent neural networks; Storms; Weather forecasting; Convolutional neural network; Deep learning; Features extraction; Multiscale fusion; Nowcasting; Precipitation nowcasting; Radar echo extrapolation; Radar echoes; Tropical cyclone; Typhoon prediction; natural disaster; precipitation (climatology); radar imagery; typhoon; weather forecasting; Generative adversarial networks","Convolutional neural network; deep learning; generative adversarial network; precipitation nowcasting; radar echo extrapolation; typhoon prediction","Article","Final","","Scopus","2-s2.0-85135750849"
"Wu Z.; Wu X.; Zhu Y.; Zhai J.; Yang H.; Yang Z.; Wang C.; Sun J.","Wu, Zhaoli (57382530000); Wu, Xuehan (57447785900); Zhu, Yuancai (57195398960); Zhai, Jingxuan (57447665900); Yang, Haibo (57447174100); Yang, Zhiwei (57447914800); Wang, Chao (57447666000); Sun, Jilong (57447546800)","57382530000; 57447785900; 57195398960; 57447665900; 57447174100; 57447914800; 57447666000; 57447546800","Research on Multimodal Image Fusion Target Detection Algorithm Based on Generative Adversarial Network","2022","Wireless Communications and Mobile Computing","2022","","1740909","","","","10.1155/2022/1740909","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124425895&doi=10.1155%2f2022%2f1740909&partnerID=40&md5=a9cfd1e05acddbd071b400f25f8cf986","In this paper, we propose a target detection algorithm based on adversarial discriminative domain adaptation for infrared and visible image fusion using unsupervised learning methods to reduce the differences between multimodal image information. Firstly, this paper improves the fusion model based on generative adversarial network and uses the fusion algorithm based on the dual discriminator generative adversarial network to generate high-quality IR-visible fused images and then blends the IR and visible images into a ternary dataset and combines the triple angular loss function to do migration learning. Finally, the fused images are used as the input images of faster RCNN object detection algorithm for detection, and a new nonmaximum suppression algorithm is used to improve the faster RCNN target detection algorithm, which further improves the target detection accuracy. Experiments prove that the method can achieve mutual complementation of multimodal feature information and make up for the lack of information in single-modal scenes, and the algorithm achieves good detection results for information from both modalities (infrared and visible light).  © 2022 Zhaoli Wu et al.","Image enhancement; Image fusion; Object detection; Object recognition; Signal detection; Unsupervised learning; Domain adaptation; Fused images; Fusion model; Fusion targets; Image information; Infrared and visible image; Multimodal image fusions; Multimodal images; Target detection algorithm; Unsupervised learning method; Generative adversarial networks","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85124425895"
"Zhang Y.; Han S.; Zhang Z.; Wang J.; Bi H.","Zhang, Yubo (39763341600); Han, Shuang (57443527900); Zhang, Zhongxin (57443915200); Wang, Jianyang (57221705454); Bi, Hongbo (25026660900)","39763341600; 57443527900; 57443915200; 57221705454; 25026660900","CF-GAN: cross-domain feature fusion generative adversarial network for text-to-image synthesis","2022","Visual Computer","","","","","","","10.1007/s00371-022-02404-6","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124230130&doi=10.1007%2fs00371-022-02404-6&partnerID=40&md5=7f6d272bd170ba27d865afc106034d23","In recent years, generative adversarial networks have successfully synthesized images through text descriptions. However, there are still problems that the generated image cannot be deeply embedded in the text description semantics, the target object of the generated image is incomplete, and the texture structure of the target object is not rich enough. Consequently, we propose a network framework, cross-domain feature fusion generative adversarial network (CF-GAN), which includes two modules, feature fusion-enhanced response module (FFERM) and multi-branch residual module (MBRM), to fine-grain the generated images with the way of deep fusion. FFERM can integrate both the word-level vector features and image features deeply. MBRM is a relatively simple and innovative residual network structure instead of the traditional residual module to extract features fully. We conducted experiments on the CUB and COCO datasets, and the results reveal that the Inception Score has improved from 4.36 to 4.83 (increased by 10.78%) on the CUB dataset, compared with AttnGAN. Compared with DM-GAN, the Inception Score has increased from 30.49 to 31.13 (increased by 2.06%) on the COCO dataset. Extensive experiments and ablation studies demonstrate the proposed CF-GAN’s superiority compared to other methods. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Image enhancement; Image fusion; Semantics; Textures; Cross-domain; Deep learning; Domain feature; Features fusions; Images synthesis; Residual structure; Synthesized images; Target object; Text-to-image; Texture structure; Generative adversarial networks","Deep learning; Generative adversarial networks; Residual structure; Text-to-image","Article","Article in press","","Scopus","2-s2.0-85124230130"
"Zhang Y.; Zhong L.; Shu H.; Dai Z.; Zheng K.; Chen Z.; Feng Q.; Wang X.; Yang W.","Zhang, Yiwen (57223002396); Zhong, Liming (56966065200); Shu, Hai (56666767000); Dai, Zhenhui (55276557300); Zheng, Kaiyi (57792718000); Chen, Zefeiyun (57571455700); Feng, Qianjin (57791057600); Wang, Xuetao (55631198800); Yang, Wei (56982069100)","57223002396; 56966065200; 56666767000; 55276557300; 57792718000; 57571455700; 57791057600; 55631198800; 56982069100","Cross-Task Feedback Fusion GAN for Joint MR-CT Synthesis and Segmentation of Target and Organs-At-Risk","2022","IEEE Transactions on Artificial Intelligence","","","","1","12","11","10.1109/TAI.2022.3187388","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133733196&doi=10.1109%2fTAI.2022.3187388&partnerID=40&md5=cd3fd3765d75902c9eb9c761a4f3bac1","The synthesis of computed tomography (CT) images from magnetic resonance imaging (MR) images and segmentation of target and organs-at-risk (OARs) are two important tasks in MR-only radiotherapy treatment planning (RTP). Some methods have been proposed to utilize the paired MR and CT images for MR-CT synthesis or target and OARs segmentation. However, these methods usually handle synthesis and segmentation as two separate tasks, and ignore the inevitable registration errors in paired images after standard registration. In this paper, we propose a cross-task feedback fusion generative adversarial network (CTFF-GAN) for joint MR-CT synthesis and segmentation of target and OARs to enhance each task&#x2019;s performance. Specifically, we propose a cross-task feedback fusion (CTFF) module to feedback the semantic information from the segmentation task to the synthesis task for the anatomical structure correction in synthetic CT images. Besides, we use CT images synthesized from MR images for multi-modal segmentation to eliminate the registration errors. Moreover, we develop a multi-task discriminator to urge the generator to devote more attention to the organ boundaries. Experiments on our nasopharyngeal carcinoma dataset show that CTFF-GAN achieves impressive performance with MAE of 70.69 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 10.50 HU, SSIM of 0.755 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 0.03, and PSNR of 27.44 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 1.20 dB in synthetic CT, and the mean Dice of 0.783 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 0.075 in target and OARs segmentation. Our CTFF-GAN outperforms state-of-the-art methods in both the synthesis and segmentation tasks. <italic>Impact Statement</italic>&#x2014;Radiation therapy is a crucial part of cancer treatment, with nearly half of all cancer patients receiving it at some point during their illness. It usually takes a radiation oncologist several hours to delineate the targets and organs-at-risk (OARs) for a radiotherapy treatment planning (RTP). Worse, the inevitable registration errors between computed tomography (CT) images and magnetic resonance imaging (MR) images increase the uncertainty of delineation. Although some deep-learning based segmentation and synthesis methods have been proposed to solve the above-mentioned difficulties respectively, they ignore the potential relationship between the two tasks. The technology proposed in this paper takes the synergy of synthesis and segmentation into account and achieves superior performance in both tasks. Our method can automatically realize MR-CT synthesis and segmentation of targets and OARs only based on MR images in half a minute, which will simplify the workflow of RTP and improve the efficiency of radiation oncologist. IEEE","Computerized tomography; Edge detection; Generative adversarial networks; Image fusion; Image segmentation; Radiotherapy; Risk assessment; Semantics; Computed tomography; Feedback fusion; Feedback fusion mechanism; Fusion mechanism; Image edge detection; Images segmentations; Joint synthesis and segmentation; Magnetic resonance imaging-only radiotherapy treatment planning; Radiotherapy treatment planning; Task analysis; Magnetic resonance imaging","Computed tomography; Feedback fusion mechanism; generative adversarial network; Generative adversarial networks; Image edge detection; Image segmentation; joint synthesis and segmentation; MR-only radiotherapy treatment planning; Standards; Task analysis; Training","Article","Article in press","","Scopus","2-s2.0-85133733196"
"Chen L.; Zhao C.; Huang X.; Wang Y.; Deng J.","Chen, Lingjun (57224216829); Zhao, Caidan (23669845300); Huang, Xiangyu (57607026100); Wang, Yilin (57222662012); Deng, Junjie (57216971556)","57224216829; 23669845300; 57607026100; 57222662012; 57216971556","Dehazing Algorithm Based on Multi-Scale Feature Extraction","2022","Proceedings of SPIE - The International Society for Optical Engineering","12247","","122471R","","","","10.1117/12.2636944","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131921073&doi=10.1117%2f12.2636944&partnerID=40&md5=b8231791983587f36c380cfd6d559e68","Fog seriously affects the visual perception of human eyes and reduces the quality of captured images. This paper proposes a dehazing Generative Adversarial Network based on multi-scale feature extraction. The method is an end-to-end dehazing network that avoids the dependence on physical models. By adding the edge feature extraction module to the generator network to obtain the high-frequency information of the foggy image, the attention to the edge information of the image is effectively improved. In addition, the multi-scale features of the image are extracted, and then the foggy image is enhanced by a unique feature fusion mechanism. The discriminator network uses the global discriminator and the local discriminator to make a joint judgement, which further improves the dehazing performance. Compared with state-of-the-art approaches available in the literature, the algorithm proposed in this paper obtains better subjective and objective image quality evaluation on the cityscape foggy image synthesis dataset. © 2022 SPIE.","Demulsification; Discriminators; Extraction; Generative adversarial networks; Image enhancement; Image fusion; Image quality; Dehazing; End to end; Features extraction; Features fusions; Human eye; Image dehazing; Multi-scale features; Network-based; Physical modelling; Visual perception; Feature extraction","feature extraction; feature fusion; generative adversarial network; image dehazing; Image enhancement","Conference paper","Final","","Scopus","2-s2.0-85131921073"
"Qiao S.; Pan S.; Luo G.; Pang S.; Chen T.; Singh A.K.; Lv Z.","Qiao, Sibo (57209331290); Pan, Silin (14627564000); Luo, Gang (57214385230); Pang, Shanchen (56257339200); Chen, Taotao (57193424451); Singh, Amit Kumar (55726466900); Lv, Zhihan (55925162500)","57209331290; 14627564000; 57214385230; 56257339200; 57193424451; 55726466900; 55925162500","A Pseudo-Siamese Feature Fusion Generative Adversarial Network for Synthesizing High-quality Fetal Four-chamber Views","2022","IEEE Journal of Biomedical and Health Informatics","","","","","","","10.1109/JBHI.2022.3143319","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123374390&doi=10.1109%2fJBHI.2022.3143319&partnerID=40&md5=f103dbf986032e778b9be8d85a676438","Four-chamber (FC) views are the primary ultrasound (US) images that cardiologists diagnose whether the fetus has congenital heart disease (CHD) in prenatal diagnosis and screening. FC views intuitively depict the developmental morphology of the fetal heart. Early diagnosis of fetal CHD has always been the focus and difficulty of prenatal screening. Furthermore, deep learning technology has achieved great success in medical image analysis. Hence, applying deep learning technology in the early screening of fetal CHD helps improve diagnostic accuracy. However, the lack of large-scale and high-quality fetal FC views brings incredible difficulties to deep learning models or cardiologists. Hence, we propose a Pseudo-Siamese Feature Fusion Generative Adversarial Network (PSFFGAN), synthesizing high-quality fetal FC views using FC sketch images. In addition, we propose a novel Triplet Generative Adversarial Loss Function (TGALF), which optimizes PSFFGAN to fully extract the cardiac anatomical structure information provided by FC sketch images to synthesize the corresponding fetal FC views with speckle noises, artifacts, and other ultrasonic characteristics. The experimental results show that the fetal FC views synthesized by our proposed PSFFGAN have the best objective evaluation values: SSIM of 0.4627, MS-SSIM of 0.6224, and FID of 83.92, respectively. More importantly, two professional cardiologists evaluate healthy FC views and CHD FC views synthesized by our PSFFGAN, giving a subjective score that the average qualified rate is 82% and 79%, respectively, which further proves the effectiveness of the PSFFGAN. IEEE","Cardiology; Deep learning; Diagnosis; Diseases; Heart; Image fusion; Medical imaging; Speckle; Congenital heart disease; Deep learning; Deep) learning; Features extraction; Fetal four-chamber view; Fetal fourchamber sketch image; GAN; Generator; Images synthesis; Generative adversarial networks","Congenital heart disease; Deep learning; Deep) learning; Feature extraction; Fetal four-chamber views; Fetal fourchamber sketch images; GAN; Generative adversarial networks; Generators; Image synthesis; Images synthesis; Speckle; Training","Article","Article in press","","Scopus","2-s2.0-85123374390"
"Zhu W.; Zhang L.","Zhu, Wanning (57221264507); Zhang, Libao (35325855000)","57221264507; 35325855000","Pan-Sharpening Based on Joint Visual Saliency Analysis and Parallel Bidirectional Network","2022","IEEE Geoscience and Remote Sensing Letters","19","","6516805","","","","10.1109/LGRS.2022.3209787","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139530693&doi=10.1109%2fLGRS.2022.3209787&partnerID=40&md5=185037328fafed47da660821ccc35a07","In remote sensing (RS) images, the demands for spectral and spatial quality of different regions are different, which means that the unified fusion strategy on the whole image is not suitable for pan-sharpening task. Saliency, derived from visual attention mechanism, provides an effective way to satisfy these demands. Inspired by this, we propose a novel pan-sharpening method based on joint visual saliency analysis and parallel bidirectional network (JSPBN). First, considering the complex scenes and uneven distribution of targets in RS images, we develop a Bayesian optimization-based joint visual saliency analysis (B-JVSA) method that integrates prior saliency based on global color contrast with likelihood saliency based on joint co-occurrence histogram, which can highlight common salient regions while suppressing individual ones and irrelevant background by exploring the correlation among multiple RS images. Second, we construct a parallel bidirectional feature pyramid (PBFP) network to obtain coarse fusion features, fully considering individual characteristics of panchromatic (PAN) images and multispectral (MS) images. Finally, we design a saliency-aware layer (SAL) according to B-JVSA to further refine the fusion effect in salient regions and nonsalient regions. With the help of SAL, diverse strategies for certain regions are learned through two independent residual dense networks (RDNs) and thereby generating accurate fusion results. Experimental results show that our proposal performs better than the competing methods in both spatial quality enhancement and spectral fidelity preservation.  © 2004-2012 IEEE.","Behavioral research; Generative adversarial networks; Image analysis; Image fusion; Quality control; Visualization; Bayes method; Bidirectional networks; Features extraction; Histogram; Image color analysis; Joint visual saliency analyse; Pan-sharpening; Parallel bidirectional network; Remote-sensing; Saliency analysis; Visual saliency; Bayesian analysis; complexity; demand analysis; histogram; maximum likelihood analysis; remote sensing; satellite imagery; spatial analysis; strategic approach; visual analysis; Remote sensing","Joint visual saliency analysis; pan-sharpening; parallel bidirectional network; remote sensing (RS)","Article","Final","","Scopus","2-s2.0-85139530693"
"Li L.; Wang H.; Li C.","Li, Lin (56965923100); Wang, Hongmei (57196428011); Li, Chenkai (57455607700)","56965923100; 57196428011; 57455607700","A review of deep learning fusion methods for infrared and visible images; [红外与可见光图像深度学习融合方法综述]","2022","Hongwai yu Jiguang Gongcheng/Infrared and Laser Engineering","51","12","20220125","","","","10.3788/IRLA20220125","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145774531&doi=10.3788%2fIRLA20220125&partnerID=40&md5=0c42591d27cf1b15a29855d10007df0d","Infrared and visible image fusion technology makes full use of the advantages of different sensors, retains the complementary information and redundant information of the original image in the fused image, and improves the image quality. In recent years, with the development of deep learning methods, many researchers have begun to introduce this method into the field of image fusion, and have achieved fruitful results. According to different fusion frameworks, the infrared and visible image fusion methods based on deep learning are classified, analyzed and summarized, the commonly used evaluation indicators and data sets are reviewed. In addition, some representative algorithm models of different categories are selected to fuse different scene images, the advantages and disadvantages of each algorithm are compared and analyzed by evaluation indicators. Finally, the research direction of infrared and visible image fusion technology based on deep learning is prospected, infrared and visible fusion technology is summarized, which is the basis for future research work. © 2022 Chinese Society of Astronautics. All rights reserved.","Convolutional neural networks; Deep learning; Engineering education; Generative adversarial networks; Image analysis; Image enhancement; Infrared imaging; Learning systems; Auto encoders; Autoencoder network; Convolutional neural network; Evaluation indicators; Fusion methods; Image fusion technology; Infrared and visible image; Infrared image; Original images; Visible image; Image fusion","autoencoder network; convolutional neural network; generative adversarial network; image fusion; infrared image; visible image","Article","Final","","Scopus","2-s2.0-85145774531"
"Zhang T.; Sun X.; Li X.; Yi Z.","Zhang, Tao (57211544697); Sun, Xing (56706675700); Li, Xuan (57203730951); Yi, Zhengming (57218282711)","57211544697; 56706675700; 57203730951; 57218282711","Image generation and constrained two-stage feature fusion for person re-identification","2021","Applied Intelligence","51","11","","7679","7689","10","10.1007/s10489-021-02271-z","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102788371&doi=10.1007%2fs10489-021-02271-z&partnerID=40&md5=67c444b48ae5ff9983be3ada64a70f9c","Generative adversarial network is widely used in person re-identification to expand data by generating auxiliary data. However, researchers all believe that using too much generated data in the training phase will reduce the accuracy of re-identification models. In this study, an improved generator and a constrained two-stage fusion network are proposed. A novel gesture discriminator embedded into the generator is used to calculate the completeness of skeleton pose images. The improved generator can make generated images more realistic, which would be conducive to feature extraction. The role of the constrained two-stage fusion network is to extract and utilize the real information of the generated images for person re-identification. Unlike previous studies, the fusion of shallow features is considered in this work. In detail, the proposed network has two branches based on the structure of ResNet50. One branch is for the fusion of images that are generated by the generated adversarial network, the other is applied to fuse the result of the first fusion and the original image. Experimental results show that our method outperforms most existing similar methods on Market-1501 and DukeMTMC-reID. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.","Image enhancement; Adversarial networks; Auxiliary data; Feature fusion; Image generations; Original images; Person re identifications; Re identifications; Training phase; Image fusion","Generative adversarial network; Person re-identification; Shallow features fusion","Article","Final","","Scopus","2-s2.0-85102788371"
"Yi D.; Jia Z.; Wei H.; Shi P.; Feng C.","Yi, Dong (57212531644); Jia, Zhai (57208879262); Wei, He (57555292400); Shi, Peng (57226602603); Feng, Chen (57212474470)","57212531644; 57208879262; 57555292400; 57226602603; 57212474470","An Infrared and Visible Image Fusion Method Based on Adaptive Weight Learning","2021","Proceedings - 2021 International Conference on Intelligent Computing, Automation and Systems, ICICAS 2021","","","","55","59","4","10.1109/ICICAS53977.2021.00018","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127255786&doi=10.1109%2fICICAS53977.2021.00018&partnerID=40&md5=78418e536b3c77bd16aee1cbef7e2c3f","Aiming at the low utilization efficiency of depth features and insufficient restoration of fusion image texture information in existing fusion algorithm based on adversarial neural network, this paper proposed an infrared and visible image fusion technology based on adaptive weight Learning. Under the framework of the adversarial neural network, a deep feature adaptive extraction module based on pixel-level attention mechanism and fusion weight adaptive generation was designed to efficiently extract image features, and then effectively combine multiple depth features adaptive extraction modules through hierarchical cascade to further improve the ability of deep-level feature extraction and build a generator network. Then, a discriminator group based on a two-way neural network was constructed. Finally, the fusion image was completed through the game confrontation between the generator and the discriminator. The experimental results show that the fusion image generated by the proposed method can well restore the infrared characteristic information of the infrared band data and the detailed scene information in the visible light data, and has a strong ability to enhance the fusion of infrared and visible images.  © 2021 IEEE.","Deep learning; Extraction; Image enhancement; Image fusion; Image reconstruction; Image texture; Restoration; Textures; Adaptive weight learning; Adaptive weights; Deep learning; Depth features; Fusion image; Image fusion methods; Infrared and visible image; Neural-networks; Texture information; Utilization efficiency; Generative adversarial networks","adaptive weight learning; deep learning; generative adversarial networks; image fusion","Conference paper","Final","","Scopus","2-s2.0-85127255786"
"You S.; Lei B.; Wang S.; Chui C.K.; Cheung A.C.; Liu Y.; Gan M.; Wu G.; Shen Y.","You, Senrong (57220890828); Lei, Baiying (26422280400); Wang, Shuqiang (53872228000); Chui, Charles K. (24297847200); Cheung, Albert C. (57215432085); Liu, Yong (55954392300); Gan, Min (57208454607); Wu, Guocheng (23390775700); Shen, Yanyan (25121829200)","57220890828; 26422280400; 53872228000; 24297847200; 57215432085; 55954392300; 57208454607; 23390775700; 25121829200","Fine Perceptive GANs for Brain MR Image Super-Resolution in Wavelet Domain","2022","IEEE Transactions on Neural Networks and Learning Systems","","","","","","","10.1109/TNNLS.2022.3153088","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126291613&doi=10.1109%2fTNNLS.2022.3153088&partnerID=40&md5=be92c9518844c8d0cec134e0efdb67a9","Magnetic resonance (MR) imaging plays an important role in clinical and brain exploration. However, limited by factors such as imaging hardware, scanning time, and cost, it is challenging to acquire high-resolution MR images clinically. In this article, fine perceptive generative adversarial networks (FP-GANs) are proposed to produce super-resolution (SR) MR images from the low-resolution counterparts. By adopting the divide-and-conquer scheme, FP-GANs are designed to deal with the low-frequency (LF) and high-frequency (HF) components of MR images separately and parallelly. Specifically, FP-GANs first decompose an MR image into LF global approximation and HF anatomical texture subbands in the wavelet domain. Then, each subband generative adversarial network (GAN) simultaneously concentrates on super-resolving the corresponding subband image. In generator, multiple residual-in-residual dense blocks are introduced for better feature extraction. In addition, the texture-enhancing module is designed to trade off the weight between global topology and detailed textures. Finally, the reconstruction of the whole image is considered by integrating inverse discrete wavelet transformation in FP-GANs. Comprehensive experiments on the MultiRes_7T and ADNI datasets demonstrate that the proposed model achieves finer structure recovery and outperforms the competing methods quantitatively and qualitatively. Moreover, FP-GANs further show the value by applying the SR results in classification tasks. IEEE","Discrete wavelet transforms; Economic and social effects; Generative adversarial networks; Image enhancement; Image fusion; Image reconstruction; Inverse problems; Magnetism; Optical resolving power; Resonance; Discrete wavelets transformations; Generative adversarial network; Images reconstruction; Lower frequencies; Subbands; Super-resolution; Superresolution; Task analysis; Texture enhance.; Wavelet domain; Magnetic resonance imaging","Discrete wavelet transformation; Discrete wavelet transforms; generative adversarial network (GAN); Generative adversarial networks; Hafnium; Image reconstruction; magnetic resonance (MR) imaging; Magnetic resonance imaging; super-resolution (SR); Task analysis; textures enhance.; Wavelet domain","Article","Article in press","All Open Access; Green Open Access","Scopus","2-s2.0-85126291613"
"Zhang F.; Li K.; Li S.; Zhao L.; Dong H.","Zhang, Fuzheng (57348036800); Li, Kun (56321961200); Li, Shilin (57348499900); Zhao, Liqiang (57348191000); Dong, Houqi (57213935705)","57348036800; 56321961200; 57348499900; 57348191000; 57213935705","Research on transmission line image generation based on ensemble learning PCA multiple fusion; [基于集成学习PCA多元融合的输电线路图像生成研究]","2021","Guangdianzi Jiguang/Journal of Optoelectronics Laser","32","8","","841","851","10","10.16136/j.joel.2021.08.0058","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119525310&doi=10.16136%2fj.joel.2021.08.0058&partnerID=40&md5=8073ec9a1e1582f0a30a89705d0334ed","There are few defective sample images among patrol images in the power system, leading to an imbalance between normal samples and defective samples, and It is impossible to use algorithms such as deep learning to study the fault detection of transmission lines further. At present, various of image generation methods based on deep machine learning have problems such as low resolution and unconspicuous defect features, which makes it difficult for the generated sample images to meet the needs of researchers. This paper proposes a PCA weighted average (diverse integration, DI ) generation method based on (ensemble learning, EL).Experiments were carried out with normal and defective transmission line insulator images. The experimental results show that the quality of the generated images is obvious, which can be effectively used in the construction of a professional sample library for power systems, providing big data support for subsequent related research, and also proposed a new and feasible research idea for this field. © 2021, Science Press in China. All right reserved.","","Deep convolution generative adversarial network; Ensemble learning; Image fusion; Principal component analysis algorithm; Transmission lines; Variational autoencoder","Article","Final","","Scopus","2-s2.0-85119525310"
"Song B.; Liu P.; Li J.; Wang L.; Zhang L.; He G.; Chen L.; Liu J.","Song, Bingze (57274635700); Liu, Peng (57075315400); Li, Jun (24481713500); Wang, Lizhe (23029267900); Zhang, Luo (57712604600); He, Guojin (14028364400); Chen, Lajiao (55647804800); Liu, Jianbo (56055157000)","57274635700; 57075315400; 24481713500; 23029267900; 57712604600; 14028364400; 55647804800; 56055157000","MLFF-GAN: A Multilevel Feature Fusion With GAN for Spatiotemporal Remote Sensing Images","2022","IEEE Transactions on Geoscience and Remote Sensing","60","","4410816","","","","10.1109/TGRS.2022.3169916","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130860119&doi=10.1109%2fTGRS.2022.3169916&partnerID=40&md5=54886bdbfe431356842d4961a6fbee4e","Due to the limitation of technology and budget, it is often difficult for sensors of a single remote sensing satellite to have both high-temporal and high-spatial (HTHS) resolution at the same time. In this article, we proposed a new multilevel feature fusion with generative adversarial network (MLFF-GAN) for generating fusion HTHS images. The MLFF-GAN mainly uses U-net-like architecture, and its generator is composed of three stages: feature extraction, feature fusion, and image reconstruction. In the feature extraction and reconstruction stage, the generator employs the encoding and decoding structure to extract three groups of multilevel features (MLFs), which can cope with the huge difference of resolution between high-resolution images and low-resolution images. In the feature fusion stage, adaptive instance normalization (AdaIN) block is designed to learn the global distribution relationship between multitemporal images, and an attention module (AM) is used to learn the local information weights for the change of small areas. The proposed MLFF-GAN was tested on two Landsat and Moderate-Resolution Imaging Spectroradiometer (MODIS) datasets. Some state-of-the-art algorithms are comprehensively compared with MLFF-GAN. We also carried on the ablation experiment to test the effectiveness of different submodules in the MLFF-GAN. The experiment results and ablation analysis show the better performances of the proposed method when compared with other methods. The code is available at https://github.com/songbingze/MLFF-GAN. © 1980-2012 IEEE.","Ablation; Budget control; Extraction; Generative adversarial networks; Image fusion; Image reconstruction; Image resolution; Remote sensing; Adaptive instance normalization; Attention mechanisms; Features extraction; Generative adversarial network; Generator; Images reconstruction; Normalisation; Spatial attention; Spatial attention mechanism; Spatial resolution; Spatiotemporal phenomenon; U-net; artificial neural network; Landsat; MODIS; reconstruction; remote sensing; satellite sensor; spatial resolution; Feature extraction","Adaptive instance normalization (AdaIN); generative adversarial network (GAN); spatial attention mechanism; U-net","Article","Final","","Scopus","2-s2.0-85130860119"
"Zhou T.; Li Q.; Lu H.; Cheng Q.; Zhang X.","Zhou, Tao (57020593700); Li, Qi (57936733500); Lu, Huiling (55729821400); Cheng, Qianru (57936465400); Zhang, Xiangxiang (57936871000)","57020593700; 57936733500; 55729821400; 57936465400; 57936871000","GAN review: Models and medical image fusion applications","2023","Information Fusion","91","","","134","148","14","10.1016/j.inffus.2022.10.017","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140323593&doi=10.1016%2fj.inffus.2022.10.017&partnerID=40&md5=05999685aaeefe5307ececb7057ae9bf","Generative Adversarial Network (GAN) is a research hotspot in deep generative models, which has been widely used in the field of medical image fusion. This paper summarizes GAN models from the following four aspects: firstly, the basic principles of GAN are expounded from two aspects: basic model and training process; secondly, variant GAN models are summarized into three directions (Probability Distribution Distance, Overall Network Architecture, Neural Network Structure), from the methods based on f-divergence, the methods based on IPM, Single-Generator and Dual-Discriminators GAN, Multi-Generators and Single-Discriminator GAN, Multi-Generators and Multi-Discriminators GAN, Conditional Constraint GAN, Convolutional Neural Network structure GAN and Auto-Encoder Neural Network structure GAN are eight dimensions to summarize the typical models in recent years; thirdly, the advantages and application of GAN models in the field of medical image fusion are explored from three aspects; fourthly, the main challenges faced by GAN and the challenges faced by GAN models in medical image fusion field are discussed and the future prospects are given. This paper systematically summarizes various models of GAN, advantages and challenges of GAN models in medical image fusion field, which is very important for the future research of GAN. © 2022","Convolutional neural networks; Discriminators; Image fusion; Medical imaging; Network architecture; Probability distributions; Generative adversarial network; Generative model; Hotspots; Image fusion applications; Medical image fusion; Multi-generator and multi-discriminator generative adversarial network; Network models; Neural networks structure; Probability distribution distance; Probability: distributions; Generative adversarial networks","Generative adversarial network (GAN); Generative models; Medical image fusion; Multi-generators and multi-discriminators GAN; Probability distribution distance","Article","Final","","Scopus","2-s2.0-85140323593"
"Zhang L.; Dai Y.; Fan F.; He C.","Zhang, Lin (57851985700); Dai, Yang (57738160000); Fan, Fuyou (55598815500); He, Chunlin (57696906000)","57851985700; 57738160000; 55598815500; 57696906000","Anomaly Detection of GAN Industrial Image Based on Attention Feature Fusion","2023","Sensors","23","1","355","","","","10.3390/s23010355","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145972789&doi=10.3390%2fs23010355&partnerID=40&md5=eddd2719a1b7cccac9d1ec4089a5125d","As life becomes richer day by day, the requirement for quality industrial products is becoming greater and greater. Therefore, image anomaly detection on industrial products is of significant importance and has become a research hotspot. Industrial manufacturers are also gradually intellectualizing how product parts may have flaws and defects, and that industrial product image anomalies have characteristics such as category diversity, sample scarcity, and the uncertainty of change; thus, a higher requirement for image anomaly detection has arisen. For this reason, we proposed a method of industrial image anomaly detection that applies a generative adversarial network based on attention feature fusion. For the purpose of capturing richer image channel features, we added attention feature fusion based on an encoder and decoder, and through skip-connection, this performs the feature fusion for the encode and decode vectors in the same dimension. During training, we used random cut-paste image augmentation, which improved the diversity of the datasets. We displayed the results of a wide experiment, which was based on the public industrial detection MVTec dataset. The experiment illustrated that the method we proposed has a higher level AUC and the overall result was increased by 4.1%. Finally, we realized the pixel level anomaly localization of the industrial dataset, which illustrates the feasibility and effectiveness of this method © 2022 by the authors.","Image Processing, Computer-Assisted; Industry; Uncertainty; Decoding; Feature extraction; Generative adversarial networks; Image enhancement; Image fusion; Industrial research; Anomaly detection; Attention feature fusion; Features fusions; Hotspots; Image augmentation; Image-based; Industrial images; Industrial manufacturers; Industrial product; Product images; image processing; industry; uncertainty; Anomaly detection","anomaly detection; attention feature fusion; generative adversarial network; image augmentation","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85145972789"
"Wang M.; Lang C.; Liang L.; Feng S.; Wang T.; Gao Y.","Wang, Min (57221235394); Lang, Congyan (7402002472); Liang, Liqian (57218477192); Feng, Songhe (7402531247); Wang, Tao (56135273700); Gao, Yutong (57218478378)","57221235394; 7402002472; 57218477192; 7402531247; 56135273700; 57218478378","Fine-Grained Semantic Image Synthesis with Object-Attention Generative Adversarial Network","2021","ACM Transactions on Intelligent Systems and Technology","12","5","3470008","","","","10.1145/3470008","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122065997&doi=10.1145%2f3470008&partnerID=40&md5=3e520b1f65ff5fe3740c4ab135e35306","Semantic image synthesis is a new rising and challenging vision problem accompanied by the recent promising advances in generative adversarial networks. The existing semantic image synthesis methods only consider the global information provided by the semantic segmentation mask, such as class label, global layout, and location, so the generative models cannot capture the rich local fine-grained information of the images (e.g., object structure, contour, and texture). To address this issue, we adopt a multi-scale feature fusion algorithm to refine the generated images by learning the fine-grained information of the local objects. We propose OA-GAN, a novel object-attention generative adversarial network that allows attention-driven, multi-fusion refinement for fine-grained semantic image synthesis. Specifically, the proposed model first generates multi-scale global image features and local object features, respectively, then the local object features are fused into the global image features to improve the correlation between the local and the global. In the process of feature fusion, the global image features and the local object features are fused through the channel-spatial-wise fusion block to learn gwhat' and gwhere' to attend in the channel and spatial axes, respectively. The fused features are used to construct correlation filters to obtain feature response maps to determine the locations, contours, and textures of the objects. Extensive quantitative and qualitative experiments on COCO-Stuff, ADE20K and Cityscapes datasets demonstrate that our OA-GAN significantly outperforms the state-of-the-art methods.  © 2021 Association for Computing Machinery.","Image enhancement; Image fusion; Semantic Segmentation; Semantic Web; Semantics; Textures; Attention-driven; Channel-spatial-wise fusion; Features fusions; Fine grained; GAN; Image features; Images synthesis; Local object; Semantic image synthesis; Semantic images; Generative adversarial networks","attention-driven; channel-spatial-wise fusion; GAN; Semantic image synthesis","Article","Final","","Scopus","2-s2.0-85122065997"
"Wang L.; Qi Z.; Liu Y.","Wang, Lei (57070575800); Qi, Zhengzheng (58085807000); Liu, Yu (57203675380)","57070575800; 58085807000; 57203675380","The review of multi-focus image fusion methods based on deep learning; [深度学习多聚焦图像融合方法综述]","2023","Journal of Image and Graphics","28","1","","80","101","21","10.11834/jig.220593","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147175985&doi=10.11834%2fjig.220593&partnerID=40&md5=045d87482555a552ac3d1842a99e8231","Multi-focus image fusion technique can extend the depth-of-field (DOF) of optical lenses effectively via a software-based manner. It aims to fuse a set of partially focused source images of the same scene by generating an all-in-focus fused image, which will be more suitable for human or machine perception. As a result, multi-focus image fusion is of high practical significance in many areas including digital photography, microscopy imaging, integral imaging, thermal imaging, etc. Traditional multi-focus image fusion methods, which generally include transform domain methods (e. g., multi-scale transform-based methods and sparse representation-based methods) and spatial domain methods (e. g., block-based methods and pixel-based methods), are often based on manually designed transform models, activity level measures and fusion rules. To achieve high fusion performance, these key factors tend to become much more complicated in the fusion algorithm, which are usually at the cost of computational efficiency. In addition, these key factors are often independently designed with relatively weak association, which limits the fusion performance to a great extent. In the past few years, deep learning has been introduced into the study of multi-focus image fusion and has rapidly emerged as the current mainstream of this field, with a variety of deep learning-based fusion methods being proposed in the literature. Deep learning models like convolutional neural networks (CNNs) and generative adversarial networks (GANs) have been facilitating in the study of multi-focus image fusion. It is of high significance to conduct a comprehensive survey to review the recent advances achieved in deep learning-based multi-focus image fusion and put forward some future prospects for further improvement. Some survey papers related to image fusion including multi-focus image fusion have been recently published in the international journals around 2020. However, the survey works on multi-focus image fusion are rarely reported in Chinese journals. Moreover, considering that this field grows very rapidly with dozens of papers being published each year, a more timely survey is also highly expected. Based on the above considerations, we demonstrate a systematic review for the deep learning-based multi-focus image fusion methods. In this paper, the existing deep learning-based methods are classified into two main categories: 1) deep classification model-based methods and 2) deep regression model-based methods. Additionally, these two categories of methods are further divided into sub-categories. Specifically, the classification model-based methods are further divided into image block-based methods and image segmentation-based methods in terms of the pixel processing manner adopted. The regression model-based methods are further divided into supervised learning-based methods and unsupervised learning-based methods, according to the learning manner of network models. For each category, the representative fusion methods are introduced as well. In addition, we conduct a comparative study on the performance of 25 representative multi-focus image fusion methods, including 5 traditional transform domain methods, 5 traditional spatial domain methods and 15 deep learning-based methods. To this end, we use three commonly-used multi-focus image fusion datasets in the experiments including “Lytro”, “MFFW” and “Classic”. Additionally, eight objective evaluation metrics that are widely used in multi-focus image fusion are adopted for performance assessment, which are composed of include two information theory-based metrics, two image feature-based metrics, two structural similarity-based metrics and two human visual perception-based metrics. The experimental results verify that deep learning-based methods can achieve very promising fusion results. However, it is worth noting that the performance of most deep learning-based methods is not significantly better than that of the traditional fusion methods. One main reason for this phenomenon is the lack of large-scale and realistic datasets for training in multi-focus image fusion, and the way to create synthetic datasets for training is inevitability different from the real situation, leading to that the potential of deep learning-based methods cannot be fully tapped. Finally, we summarize some challenging problems in the study of deep learning-based multi-focus image fusion and put forward some future prospects accordingly, which mainly include the four aspects as following: 1) the fusion of focus boundary regions; 2) the fusion of mis-registered source images; 3) the construction of large-scale datasets with real labels for network training; and 4) the improvement of network architecture and model training approach. © 2023 Editorial and Publishing Board of JIG. All rights reserved.","","convolutional neural network (CNN); deep learning; generative adversarial network (GAN); image fusion; multi-focus image fusion (MFIF)","Review","Final","","Scopus","2-s2.0-85147175985"
"Fan X.; Yang X.; Shi P.; Han S.; Xin Y.","Fan, Xinnan (8633441200); Yang, Xin (57778409300); Shi, Pengfei (57213385715); Han, Song (57212414004); Xin, Yuanxue (57188986953)","8633441200; 57778409300; 57213385715; 57212414004; 57188986953","Underwater Image Enhancement Based on Feature Fusion Generative Adversaral Networks; [特征融合生成对抗网络的水下图像增强]","2022","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","34","2","","264","272","8","10.3724/SP.J.1089.2022.18843","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124901179&doi=10.3724%2fSP.J.1089.2022.18843&partnerID=40&md5=0a59a88d87118cac838ca2a7c4e802bf","Aiming at the problems of low contrast and color distortion of underwater images, an underwater image en-hancement algorithm based on feature fusion generative adversarial networks is proposed. Firstly, color correction algorithm is applied to the underwater degraded image, and then the feature of color corrected image is extracted by convolution neural network. Secondly, the feature of underwater degraded image can be extracted by the feature extraction network which is based on U-Net, and then fuse it with the feature of color correction image. Finally, the convolution neural network is used to reconstruct the fusion feature to the enhanced image. The experimental results on Underwater-ImageNet dataset show that the algorithm can effectively improve the contrast and clarity of underwater degraded image, and the enhanced image color is more realistic. Compared with other algorithms on the Underwater-ImageNet dataset, the underwater image quality measure (UIQE) is increased by 0.308, the natural image quality evaluator (NIQE) is reduced by 0.638. The contrast and sharpness of the enhanced underwater image are improved and the colors are more realistic. © 2022, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Color; Convolution; Image enhancement; Image fusion; Image quality; Underwater imaging; Color distortions; Colour corrections; Convolution neural network; Corrected image; Correction algorithms; Degraded images; Features fusions; Low contrast; U-net; Underwater image enhancements; Generative adversarial networks","Feature fusion; Generative adversarial networks; U-Net; Underwater image enhancement","Article","Final","","Scopus","2-s2.0-85124901179"
"Tang L.; Yan L.; Chen J.","Tang, Linruize (57980117300); Yan, Longbin (57155570800); Chen, Jie (55963076200)","57980117300; 57155570800; 55963076200","DB-GAN: A Low Contrast Image Enhancer Based on NIR-RGB Fusion","2022","IEEE International Workshop on Machine Learning for Signal Processing, MLSP","2022-August","","","","","","10.1109/MLSP55214.2022.9943490","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142678198&doi=10.1109%2fMLSP55214.2022.9943490&partnerID=40&md5=21002c9cceead3a4173c8abd4a17802d","RGB images captured under haze or over-/under-exposure conditions frequently have low contrast and lack of detail. Due to the limited information in the original image, the majority of enhancement techniques that rely solely on visible information fail to restore the original image satisfactorily. This emphasizes the need for information beyond the visible spectrum. In this paper, we formulate the low contrast image enhancement problem based on near-infrared (NIR)-RGB fusion. A Dual-Branch Generative Adversarial Network (DB-GAN) is designed based on the specific characteristics of NIR-RGB fusion problem. To be specific, with the guidance of the two discriminators that respectively extract information from RGB and NIR images, a U-net based generator generates informative, high-quality fused images. In addition, we create an NIR-RGB dataset with over 1300 aligned image pairs for training the network. Quantitative and qualitative experimental results show the superior performance of our proposed framework.  © 2022 IEEE.","Computer vision; Image enhancement; Image fusion; Infrared devices; Contrast Enhancement; Exposure conditions; Low contrast; Low contrast enhancement; Low contrast image; Near Infrared; Near-infrared; Original images; RGB; RGB images; Generative adversarial networks","Adversarial Generative Network; Image fusion; low contrast enhancement; near-infrared(NIR); RGB","Conference paper","Final","","Scopus","2-s2.0-85142678198"
"Zhou J.","Zhou, Jiayi (57956382900)","57956382900","Research on Generative Adversarial Networks and Their Applications in Image Generation","2022","2022 IEEE International Conference on Advances in Electrical Engineering and Computer Applications, AEECA 2022","","","","1144","1147","3","10.1109/AEECA55500.2022.9918833","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141374315&doi=10.1109%2fAEECA55500.2022.9918833&partnerID=40&md5=cdede79ca90cd093c7aba31083a4b6a3","This paper first introduces the basic principles, model structures, and advantages and disadvantages of generative adversarial networks. Then we give a detailed introduction to three application areas of generative adversarial networks in image generation: medical imaging, 3D reconstruction, and image fusion. Finally, the development trend of generative adversarial networks and their applications in the field of image generation prospects.  © 2022 IEEE.","Image fusion; Medical imaging; 3-D image; 3D reconstruction; 3D-images; Application area; Application research; Basic principles; Development trends; Image generations; Principle modeling; Generative adversarial networks","Application Research; Generative Adversarial Networks; Image Generation","Conference paper","Final","","Scopus","2-s2.0-85141374315"
"Guo X.; Liu X.; Królczyk G.; Sulowicz M.; Glowacz A.; Gardoni P.; Li Z.","Guo, Xiaoqiang (57226067958); Liu, Xinhua (55717246900); Królczyk, Grzegorz (56925609000); Sulowicz, Maciej (15124876700); Glowacz, Adam (24824306700); Gardoni, Paolo (12644936300); Li, Zhixiong (57199421651)","57226067958; 55717246900; 56925609000; 15124876700; 24824306700; 12644936300; 57199421651","Damage Detection for Conveyor Belt Surface Based on Conditional Cycle Generative Adversarial Network","2022","Sensors","22","9","3485","","","","10.3390/s22093485","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129401670&doi=10.3390%2fs22093485&partnerID=40&md5=c21de27ae6c9aceb2e86730bb5b0cdd5","The belt conveyor is an essential piece of equipment in coal mining for coal transportation, and its stable operation is key to efficient production. Belt surface of the conveyor is vulnerable to foreign bodies which can be extremely destructive. In the past decades, much research and numerous approaches to inspect belt status have been proposed, and machine learning-based non-destructive testing (NDT) methods are becoming more and more popular. Deep learning (DL), as a branch of machine learning (ML), has been widely applied in data mining, natural language processing, pattern recognition, image processing, etc. Generative adversarial networks (GAN) are one of the deep learning methods based on generative models and have been proved to be of great potential. In this paper, a novel multi-classification conditional CycleGAN (MCC-CycleGAN) method is proposed to generate and discriminate surface images of damages of conveyor belt. A novel architecture of improved CycleGAN is designed to enhance the classification performance using a limited capacity images dataset. Experimental results show that the proposed deep learning network can generate realistic belt surface images with defects and efficiently classify different damaged images of the conveyor belt surface. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Belt conveyors; Damage detection; Data mining; Deep learning; Generative adversarial networks; Image enhancement; Image fusion; Natural language processing systems; Nondestructive examination; Coal-mining; Conditional cyclegan; Conveyor belts; Foreign bodies; Images processing; Incremental image fusion; Stable operation; Surface image; Surface-based; Transfer learning; Learning algorithms","conditional CycleGAN; damage detection; incremental image fusion; transfer learning","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85129401670"
"Zhang H.; Song Y.; Han C.; Zhang L.","Zhang, Hongyan (54954032600); Song, Yiyao (57221595735); Han, Chang (57037564100); Zhang, Liangpei (8359720900)","54954032600; 57221595735; 57037564100; 8359720900","Remote Sensing Image Spatiotemporal Fusion Using a Generative Adversarial Network","2021","IEEE Transactions on Geoscience and Remote Sensing","59","5","9159647","4273","4286","13","10.1109/TGRS.2020.3010530","46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099555332&doi=10.1109%2fTGRS.2020.3010530&partnerID=40&md5=d16c320c94837f71c0d3161be454faee","Due to technological limitations and budget constraints, spatiotemporal fusion is considered a promising way to deal with the tradeoff between the temporal and spatial resolutions of remote sensing images. Furthermore, the generative adversarial network (GAN) has shown its capability in a variety of applications. This article presents a remote sensing image spatiotemporal fusion method using a GAN (STFGAN), which adopts a two-stage framework with an end-to-end image fusion GAN (IFGAN) for each stage. The IFGAN contains a generator and a discriminator in competition with each other under the guidance of the optimization function. Considering the huge spatial resolution gap between the high-spatial, low-temporal (HSLT) resolution Landsat imagery and the corresponding low-spatial, high-temporal (LSHT) resolution MODIS imagery, a feature-level fusion strategy is adopted. Specifically, for the generator, we first super-resolve the MODIS images while also extracting the high-frequency features of the Landsat images. Finally, we integrate the features from the MODIS and Landsat images. STFGAN is able to learn an end-to-end mapping between the Landsat-MODIS image pairs and predicts the Landsat-like image for a prediction date by considering all the bands. STFGAN significantly improves the accuracy of phenological change and land-cover-type change prediction with the help of residual blocks and two prior Landsat-MODIS image pairs. To examine the performance of the proposed STFGAN method, experiments were conducted on three representative Landsat-MODIS data sets. The results clearly illustrate the effectiveness of the proposed method.  © 1980-2012 IEEE.","Budget control; Image fusion; Radiometers; Remote sensing; Adversarial networks; Feature level fusion; Optimization function; Phenological changes; Remote sensing images; Spatio-temporal fusions; Technological limitations; Temporal and spatial; artificial neural network; remote sensing; satellite imagery; spatiotemporal analysis; Image enhancement","Generative adversarial network (GAN); multisource satellite data; remote sensing; spatiotemporal fusion","Article","Final","","Scopus","2-s2.0-85099555332"
"Yang Z.; Chen Y.; Le Z.; Ma Y.","Yang, Zhiguang (57212527694); Chen, Youping (9638564800); Le, Zhuliang (57215469419); Ma, Yong (56438173900)","57212527694; 9638564800; 57215469419; 56438173900","GANFuse: a novel multi-exposure image fusion method based on generative adversarial networks","2021","Neural Computing and Applications","33","11","","6133","6145","12","10.1007/s00521-020-05387-4","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095418330&doi=10.1007%2fs00521-020-05387-4&partnerID=40&md5=d5682a0840710e9ff68687145dd01fd2","In this paper, a novel multi-exposure image fusion method based on generative adversarial networks (termed as GANFuse) is presented. Conventional multi-exposure image fusion methods improve their fusion performance by designing sophisticated activity-level measurement and fusion rules. However, these methods have a limited success in complex fusion tasks. Inspired by the recent FusionGAN which firstly utilizes generative adversarial networks (GAN) to fuse infrared and visible images and achieves promising performance, we improve its architecture and customize it in the task of extreme exposure image fusion. To be specific, in order to keep content of extreme exposure image pairs in the fused image, we increase the number of discriminators differentiating between fused image and extreme exposure image pairs. While, a generator network is trained to generate fused images. Through the adversarial relationship between generator and discriminators, the fused image will contain more information from extreme exposure image pairs. Thus, this relationship can realize better performance of fusion. In addition, the method we proposed is an end-to-end and unsupervised learning model, which can avoid designing hand-crafted features and does not require a number of ground truth images for training. We conduct qualitative and quantitative experiments on a public dataset, and the experimental result shows that the proposed model demonstrates better fusion ability than existing multi-exposure image fusion methods in both visual effect and evaluation metrics. © 2020, The Author(s).","Image enhancement; Activity levels; Adversarial networks; Evaluation metrics; Fusion performance; Infrared and visible image; ITS architecture; Multi-exposure images; Quantitative experiments; Image fusion","Generative adversarial network; Image fusion; Multi-exposure image","Article","Final","All Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85095418330"
"Gao P.; Tian T.; Zhao T.; Li L.; Zhang N.; Tian J.","Gao, Peng (57216120423); Tian, Tian (57206471430); Zhao, Tianming (57215549257); Li, Linfeng (57216234297); Zhang, Nan (57610201300); Tian, Jinwen (7401635999)","57216120423; 57206471430; 57215549257; 57216234297; 57610201300; 7401635999","GF-Detection: Fusion with GAN of Infrared and Visible Images for Vehicle Detection at Nighttime","2022","Remote Sensing","14","12","2771","","","","10.3390/rs14122771","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135251593&doi=10.3390%2frs14122771&partnerID=40&md5=0234a0439d61ae462c5e0b44b945b749","Vehicles are important targets in the remote sensing applications and nighttime vehicle detection has been a hot study topic in recent years. Vehicles in the visible images at nighttime have inadequate features for object detection. Infrared images retain the contours of vehicles while they lose the color information. Thus, it is valuable to fuse infrared and visible images to improve the vehicle detection performance at nighttime. However, it is still a challenge to design effective fusion models due to the complexity of visible and infrared images. In order to improve vehicle detection performance at nighttime, this paper proposes a fusion model of infrared and visible images with Generative Adversarial Networks (GAN) for vehicle detection named GF-detection. GAN is utilized in the image reconstruction and introduced in the image fusion recently. To be specific, to exploit more features for the fusion, GAN is utilized to fuse the infrared and visible images via the image reconstruction. The generator fuses the image features and detection features, and then generates the reconstructed images for the discriminator to classify. Two branches, visible and infrared branches, are designed in the GF-detection model. Different feature extraction strategies are conducted according to the variance of the visible and infrared images. Detection features and self-attention mechanism are added to the fusion model aiming to build a detection task-driven fusion model of infrared and visible images. Extensive experiments based on nighttime images are conducted to demonstrate the effectiveness of the proposed fusion model in night vehicle detection. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.","Feature extraction; Image enhancement; Image fusion; Image reconstruction; Infrared imaging; Object detection; Object recognition; Remote sensing; Vehicle performance; Detection features; Detection fusion; Detection performance; Fusion model; Fusion of visible and infrared image; Images reconstruction; Infrared and visible image; Remote sensing applications; Vehicles detection; Visible image; Generative adversarial networks","Fusion of visible and infrared images; Generative Adversarial Networks; Vehicle detection","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85135251593"
"Ji J.; Zhang Y.; Lin Z.; Li Y.; Wang C.; Hu Y.; Huang F.; Yao J.","Ji, Jingyu (57654191600); Zhang, Yuhua (57210432712); Lin, Zhilong (57222477591); Li, Yongke (57207835665); Wang, Changlong (56005391100); Hu, Yongjiang (7407115704); Huang, Fuyu (26026997800); Yao, Jiangyi (57652530000)","57654191600; 57210432712; 57222477591; 57207835665; 56005391100; 7407115704; 26026997800; 57652530000","End to End Infrared and Visible Image Fusion With Texture Details and Contrast Information","2022","IEEE Access","10","","","92410","92425","15","10.1109/ACCESS.2022.3202974","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137549439&doi=10.1109%2fACCESS.2022.3202974&partnerID=40&md5=5a538c78a4895d0810155531402a0b48","Infrared and visible image fusion combine data information from different sensors to achieve a richer description of the same scene. In order to highlight the salient features of the infrared image and the visible image in the fusion image and obtain a fusion image with good performance, an end-to-end infrared and visible image fusion algorithm is proposed in this paper. The contrast attention module and visible image cascade part are introduced in the generator, so that the fusion image can focus on the detail information in the visible image and the contrast information in the infrared image. And in order to retain more structural contour information in the original image, the contour loss is added to the content loss function. In addition, the contrast and detail information in infrared and visible images are balanced by two discriminators. And a goal-guided reward function is introduced into the discriminator, which further facilitates the generator to produce effective fused images. Finally, extensive fusion experiments on public datasets verify the advantages of the proposed algorithm compared with other classical algorithms, and ablation experiments demonstrate the effectiveness of the improved part of the algorithm.  © 2013 IEEE.","Data mining; Deep learning; Image fusion; Infrared imaging; Contour loss; Contrast attention module; Deep learning; End to end; End-to-end infrared and visible image fusion; Features extraction; Generator; Infrared and visible image; Reward function; Target-guided reward function; Training data; Generative adversarial networks","contour loss; contrast attention module; End-to-end infrared and visible image fusion; target-guided reward function","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85137549439"
"Liu J.","Liu, Jinping (57823796700)","57823796700","Packaging Design Based on Deep Learning and Image Enhancement","2022","Computational Intelligence and Neuroscience","2022","","9125234","","","","10.1155/2022/9125234","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136000968&doi=10.1155%2f2022%2f9125234&partnerID=40&md5=25b0c0b646eb607185d82c3b6ebc3eb2","Packaging design is an important part of product design. How to improve the efficiency of packaging design is a problem that must be considered in product design. Existing packaging design methods require a lot of human and material resources. In view of this situation, this paper proposes a packaging design method based on deep learning. This paper innovatively proposes a packaging design model based on deep convolution generative adversarial networks (DCGAN). This paper constructs a dataset of packaging design schemes and trains the proposed DCGAN model. The results show that the packaging design generated by the model proposed in this paper can get a score similar to that of the expert design scheme, which proves the effectiveness and rationality of the proposed model. In addition, in order to further improve the imaging quality of packaging design images, this paper proposes a packaging design image enhancement method based on visual communication technology. The packaging design image enhancement processing is carried out through the guided filtering method, and the visual communication optimization and edge pixel fusion methods are used to decompose the multidimensional scale features of the packaging design image under the visual communication technology to realize the packaging design image enhancement processing. The simulation results show that the method used for packaging design image enhancement processing has better visual communication ability, higher degree of image information fusion, and improved packaging design effect.  © 2022 Jinping Liu.","Deep Learning; Humans; Image Enhancement; Image Processing, Computer-Assisted; Deep learning; Generative adversarial networks; Image fusion; Product design; Visual communication; Communicationtechnology; Design method; Design models; Design scheme; Guided filtering; Imaging quality; Material resources; Model-based OPC; Network models; Packaging designs; human; image enhancement; image processing; procedures; Image enhancement","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85136000968"
"Minahil S.; Kim J.-H.; Hwang Y.","Minahil, Syeda (57287450700); Kim, Jun-Hyung (57287626800); Hwang, Youngbae (7402311392)","57287450700; 57287626800; 7402311392","Patch-wise infrared and visible image fusion using spatial adaptive weights","2021","Applied Sciences (Switzerland)","11","19","9255","","","","10.3390/app11199255","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116539735&doi=10.3390%2fapp11199255&partnerID=40&md5=919fb8863326a92bccafe933fc8cf7e2","In infrared (IR) and visible image fusion, the significant information is extracted from each source image and integrated into a single image with comprehensive data. We observe that the salient regions in the infrared image contain targets of interests. Therefore, we enforce spatial adaptive weights derived from the infrared images. In this paper, a Generative Adversarial Network (GAN)-based fusion method is proposed for infrared and visible image fusion. Based on the end-to-end network structure with dual discriminators, a patch-wise discrimination is applied to reduce blurry artifact from the previous image-level approaches. A new loss function is also proposed to use constructed weight maps which direct the adversarial training of GAN in a manner such that the informative regions of the infrared images are preserved. Experiments are performed on the two datasets and ablation studies are also conducted. The qualitative and quantitative analysis shows that we achieve competitive results compared to the existing fusion methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Dual-discriminator; Generative Adversarial Network; Infrared and visible image fusion; PatchGAN; Spatial adaptive weights","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85116539735"
"Chen Y.; Zheng W.; Shin H.","Chen, Yunfan (57193164348); Zheng, Wenqi (57222385681); Shin, Hyunchul (7404011785)","57193164348; 57222385681; 7404011785","Infrared and visible image fusion using a feature attention guided perceptual generative adversarial network","2022","Journal of Ambient Intelligence and Humanized Computing","","","","","","","10.1007/s12652-022-04414-7","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138807542&doi=10.1007%2fs12652-022-04414-7&partnerID=40&md5=38d621fb6aba757d92589615caa1b17e","In recent years, the performance of infrared and visible image fusion has been dramatically improved by using deep learning techniques. However, the fusion results are still not satisfactory as the fused images frequently suffer from blurred details, unenhanced vital regions, and artifacts. To resolve these problems, we have developed a novel feature attention-guided perceptual generative adversarial network (FAPGAN) for fusing infrared and visible images. In FAPGAN, a feature attention module is proposed to incorporate with the generator aiming to produce a fused image that maintains the detailed information while highlighting the vital regions in the source images. Our feature attention module consists of spatial attention and pixel attention parts. The spatial attention aims to enhance the vital regions while the pixel attention aims to make the network focus on high frequency information to retain the detailed information. Furthermore, we introduce a perceptual loss combined with adversarial loss and content loss to optimize the generator. The perceptual loss is to make the fused image more similar to the source infrared image at the semantic level, which can not only make the fused image maintain the vital target and detailed information from the infrared image, but also remove the halo artifacts by reducing the discrepancy. Experimental results on public datasets demonstrate that our FAPGAN is superior to those of state-of-the-art approaches in both subjective visual effect and objective assessment. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Deep learning; Generative adversarial networks; Image enhancement; Infrared imaging; Learning systems; Pixels; Semantics; Deep learning; Features extraction; Fused images; High-frequency informations; Images processing; Infrared and visible image; Learning techniques; Performance; Source images; Spatial attention; Image fusion","Deep learning; Feature extraction; Image fusion; Image processing","Article","Article in press","","Scopus","2-s2.0-85138807542"
"Wang R.; Zhao R.; Fu W.; Zhang X.; Zhang Y.; Feng R.","Wang, Runhan (58073855300); Zhao, Ruiwei (37021956300); Fu, Weijia (58074371000); Zhang, Xiaobo (37092301200); Zhang, Yuejie (9734634900); Feng, Rui (56611353400)","58073855300; 37021956300; 58074371000; 37092301200; 9734634900; 56611353400","Multi-contrast High Quality MR Image Super-Resolution with Dual Domain Knowledge Fusion","2022","Proceedings - 2022 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2022","","","","2127","2134","7","10.1109/BIBM55620.2022.9995219","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146704027&doi=10.1109%2fBIBM55620.2022.9995219&partnerID=40&md5=bb6484d9a0fae1ff287ecfde3f8a4e20","Multi-contrast high quality high-resolution (HR) Magnetic Resonance (MR) images enrich available information for diagnosis and analysis. Deep convolutional neural network methods have shown promising ability for MR image super-resolution (SR) given low-resolution (LR) MR images. Methods taking HR images as references (Ref) have made progress to enhance the effect of MR images SR. However, existing multi-contrast MR image SR approaches are based on contrasting-expanding backbones, which lose high frequency information of Ref image during downsampling. They also failed to transfer textures of Ref image into target domain. In this paper, we propose Edge Mask Transformer UNet (EMFU) for accelerating MR images SR. We propose Edge Mask Transformer (EMF) to generate global details and texture representation of target domain. Dual domain fusion module in UNet aggregates semantic information of the representation and LR image of target domain. Specifically, we extract and encode edge masks to guide the attention in EMF by re-distributing the embedding tensors, so that the network allocates more attention to image edge area. We also design a dual domain fusion module with self-attention and cross-attention to deeply fuse semantic information of multiple protocols for MRI. Extensive experiments show the effectiveness of our proposed EMFU, which surpasses state-of-the-art methods on benchmarks quantitatively and visually. Codes will be released to the community.  © 2022 IEEE.","Convolutional neural networks; Deep neural networks; Domain Knowledge; Generative adversarial networks; Image enhancement; Image fusion; Magnetic resonance imaging; Medical imaging; Optical resolving power; Quality control; Semantics; Textures; Adversarial learning; Domain knowledge; Dual domain; Dual domain knowledge fusion; Edge mask transformer; Generative adversarial learning; Image super resolutions; Knowledge fusion; MRI super-resolution; Superresolution; Magnetic resonance","Dual Domain Knowledge Fusion; Edge Mask Transformer; Generative Adversarial Learning; MRI Super-Resolution","Conference paper","Final","","Scopus","2-s2.0-85146704027"
"Ma C.; Gao H.","Ma, Conghui (56562155900); Gao, Hongchao (57471481400)","56562155900; 57471481400","A GAN based method for SAR and optical images fusion","2022","Proceedings of SPIE - The International Society for Optical Engineering","12166","","121664F","","","","10.1117/12.2617316","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125474916&doi=10.1117%2f12.2617316&partnerID=40&md5=04c5d0df6dd2dfd1a20109005ebc35dd","With the development of the remote sensing technology, the availability of satellite images has been dramatically increased with high quantity and quality. Diverse information can be obtained from these multiple imaging sources. For example, synthetic aperture radar (SAR) imagery measures physical properties of the observed scene in all-weather and full-time situation and follows a range-based imaging geometry, while optical imagery measures chemical characteristics of the scene and follows a perspective imaging geometry and needs both daylight and a cloudless sky. These multisource remote sensing images, once fused together, provide a more comprehensive interpretation of remote sensing scenes. Recent advances in Generative adversarial networks (GANs) have shown great promise in translating imagery between modalities, as well in the generation of high resolution and realistic imagery. In this paper, a GAN architecture is used to solve the task of fusing SAR and optical remote sensing imagery. The network learns the mapping between input and output image, and learns a loss function to train this mapping. Specifically, the generated network is divided into two parts, encoding and decoding. The fused image including SAR intensity and texture information is generated by the generator. Other details of the optical image are added to the fusion image gradually by the discriminator. The structural similarity loss function of GAN is to make the training of GAN model more accurate on the whole structure. Experiments on Sentinel-1and Sentinel-2 imagery confirm the effectiveness and efficiency of the proposed method.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Generative adversarial networks; Geometrical optics; Image fusion; Mapping; Radar imaging; Remote sensing; Textures; Imaging geometry; Learn+; Loss functions; Multiple imaging; Network-based; Optical image; Optical imagery; Remote sensing technology; Satellite images; Synthetic aperture radar images; Synthetic aperture radar","Generative adversarial network; Image fusion; Optical imagery; SAR","Conference paper","Final","","Scopus","2-s2.0-85125474916"
"Gao Y.; Ma S.; Liu J.; Xiu X.","Gao, Yuan (57220187209); Ma, Shiwei (12762283200); Liu, Jingjing (55806165000); Xiu, Xianchao (56673806500)","57220187209; 12762283200; 55806165000; 56673806500","Fusion-UDCGAN: Multifocus Image Fusion via a U-Type Densely Connected Generation Adversarial Network","2022","IEEE Transactions on Instrumentation and Measurement","71","","5008013","","","","10.1109/TIM.2022.3159978","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126522111&doi=10.1109%2fTIM.2022.3159978&partnerID=40&md5=93e57a5b3deefb458373fba93dc345c0","Multifocus image fusion has attracted considerable attention because it can overcome the physical limitations of optical imaging equipment and fuse multiple images with different depths of the field into one full-clear image. However, most existing deep learning-based fusion methods concentrate on the segmentation of focus-defocus regions, resulting in the loss of the details near the boundaries. To address the issue, this article proposes a novel generation adversarial network with dense connections (Fusion-UDCGAN) to fuse multifocus images. More specifically, the encoder and the decoder are first composed of dense modules with dense long connections to ensure the generated image's quality. The content and clarity loss based on the L1 norm and the novel sum-modified-Laplacian (NSML) is further embedded to provide the fused images retaining more texture features. Considering that the previous dataset-making approaches may lose the relation between the overall structure and the information near the boundaries, a new dataset, which is uniformly distributed and can simulate natural focusing boundary conditions, is constructed for model training. Subjective and objective experimental results indicate that the proposed method significantly improves the sharpness, contrast, and detail richness compared to several state-of-the-art methods.  © 1963-2012 IEEE.","Decoding; Deep learning; Generative adversarial networks; Image fusion; Laplace transforms; Textures; Adversarial networks; Dense connection; Features extraction; Generation adversarial network; Generator; Images segmentations; Multifocus image fusion; Novel sum-modified-laplacian; Physical limitations; Sum-Modified-Laplacian; Image segmentation","Dense connections; Generation adversarial network (GAN); Multifocus image fusion; Novel sum-modified-Laplacian (NSML)","Article","Final","","Scopus","2-s2.0-85126522111"
"Ozcelik F.; Alganci U.; Sertel E.; Unal G.","Ozcelik, Furkan (57208564493); Alganci, Ugur (24281315400); Sertel, Elif (21934838300); Unal, Gozde (57220534209)","57208564493; 24281315400; 21934838300; 57220534209","Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic Images via GANs","2021","IEEE Transactions on Geoscience and Remote Sensing","59","4","9153037","3486","3501","15","10.1109/TGRS.2020.3010441","31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103340930&doi=10.1109%2fTGRS.2020.3010441&partnerID=40&md5=4547984f760413e2dede1e82f04b7766","Convolutional neural network (CNN)-based approaches have shown promising results in the pansharpening of the satellite images in recent years. However, they still exhibit limitations in producing high-quality pansharpening outputs. To that end, we propose a new self-supervised learning framework, where we treat pansharpening as a colorization problem, which brings an entirely novel perspective and solution to the problem compared with the existing methods that base their solution solely on producing a super-resolution version of the multispectral image. Whereas the CNN-based methods provide a reduced-resolution panchromatic image as the input to their model along with the reduced-resolution multispectral images and, hence, learn to increase their resolution together, we instead provide the grayscale transformed multispectral image as the input and train our model to learn the colorization of the grayscale input. We further address the fixed downscale ratio assumption during training, which does not generalize well to the full-resolution scenario. We introduce a noise injection into the training by randomly varying the downsampling ratios. Those two critical changes, along with the addition of adversarial training in the proposed PanColorization generative adversarial network (PanColorGAN) framework, help overcome the spatial-detail loss and blur problems that are observed in CNN-based pansharpening. The proposed approach outperforms the previous CNN-based and traditional methods, as demonstrated in our experiments. © 1980-2012 IEEE.","Electrical engineering; Geology; Adversarial networks; Full resolutions; Multispectral images; Noise injection; Panchromatic images; Reduced resolution; Satellite images; Super resolution; data set; image analysis; image resolution; multispectral image; panchromatic image; satellite imagery; Convolutional neural networks","AI; colorization; convolutional neural networks (CNNs); deep learning; generative adversarial networks (GANs); image fusion; PanColorization generative adversarial network (PanColorGAN); pansharpening; self-supervised learning; super-resolution (SR)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85103340930"
"Sumi P.; Sindhuja S.; Sureshkumar S.","Sumi, Philo (57226834477); Sindhuja, S. (57226833318); Sureshkumar, S. (57226830923)","57226834477; 57226833318; 57226830923","A comparison between AttnGAN and DF GAN: Text to image synthesis","2021","2021 3rd International Conference on Signal Processing and Communication, ICPSC 2021","","","9451789","615","619","4","10.1109/ICSPC51351.2021.9451789","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112847788&doi=10.1109%2fICSPC51351.2021.9451789&partnerID=40&md5=6706b0c7f0a8e801d0441a5f7708f963","Nowadays conversion from text to high resolution image is a challenging task due to its wide variety of application area. For text to image conversion almost all systems use Generative Adversarial Networks as the basic part of the system and GAN guarantees semantic consistency between the text input and the generated image output. In this paper we are comparing two algorithms that is used for generating image from text. The first algorithm is the AttnGAN and the second one is the DF-GAN. AttnGAN builds on top of StackGAN by using attention network which allows it to capture word level information along with the broader sentence level information. The second algorithm is the DF-GAN, which uses single generator and discriminator model to synthesize high resolution images and also uses Matching-Aware Gradient Penalty (MA-GP) to get real images with real description. The model contains a Deep text-image Fusion Block (DFBlock) to generate image features from text. Both algorithms work efficiently for image generation from text but DF-GAN generates the perfect output than AttnGAN. The AttnGAN always focus on the textual part to generate output image but DF-GAN also focuses on background of image. © 2021 IEEE.","Image fusion; Semantics; Adversarial networks; Application area; High resolution image; Image conversion; Image generations; Image synthesis; Semantic consistency; Sentence level; Image processing","AttnGAN; BLSTM; CNN; DF GAN","Conference paper","Final","","Scopus","2-s2.0-85112847788"
"Li H.; Qian W.; Nie R.; Cao J.; Xu D.","Li, Huaguang (57204395860); Qian, Wenhua (55570494500); Nie, Rencan (23668359400); Cao, Jinde (7403354075); Xu, Dan (35622853100)","57204395860; 55570494500; 23668359400; 7403354075; 35622853100","Siamese conditional generative adversarial network for multi-focus image fusion","2023","Applied Intelligence","","","","","","","10.1007/s10489-022-04406-2","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145720056&doi=10.1007%2fs10489-022-04406-2&partnerID=40&md5=a8a5872e8a748d8404beeb12e5999c22","Multi-focus image fusion (MFIF) combines information by utilizing various image sequences of the same scenes at different of focus depths. The available MFIF method based on generative adversarial networks (GAN) lacks the feature complementarity of multi-focus images, resulting in the loss of details and noises in the generated decision maps. To resolve this problem, the learning framework of a joint distribution was developed via Siamese conditional generative adversarial network (SCGAN). This framework utilizes Siamese conditional generator that produces two-probabilistic feature maps from multi-focus images with complementary information. Additionally, the proposed framework also considers both the diversity of datasets and network convergence. The structural sparse objective function is designed to penalize the prediction of low confidence by sparse calculation of the rows and columns of the matrix. So, it endows a better Dice coefficient with higher values and improves the generalization capability of the GAN. Also, Wasserstein Divergence (DIV) is utilized to optimize the discrimination performance with stable training. In both quantitative and qualitative experiments, SCGAN has better scores on MFIF than other methods. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.","Deep learning; Image fusion; Decision maps; Deep learning; Image fusion methods; Image sequence; Joint distributions; Learning frameworks; Multifocus image fusion; Multifocus images; Siamese conditional generative adversarial network; Structural sparse loss; Generative adversarial networks","Deep learning; Multi-focus image fusion; Siamese conditional generative adversarial network; Structural sparse loss","Article","Article in press","","Scopus","2-s2.0-85145720056"
"Xing Y.; Yang S.; Zhang Y.; Zhang Y.","Xing, Yinghui (57195357426); Yang, Shuyuan (8159166000); Zhang, Yan (57218471311); Zhang, Yanning (56075029000)","57195357426; 8159166000; 57218471311; 56075029000","Learning Spectral Cues for Multispectral and Panchromatic Image Fusion","2022","IEEE Transactions on Image Processing","31","","","6964","6975","11","10.1109/TIP.2022.3215906","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141570373&doi=10.1109%2fTIP.2022.3215906&partnerID=40&md5=65dddd0edccfef56fc1ba324ca59f176","Recently, deep learning based multispectral (MS) and panchromatic (PAN) image fusion methods have been proposed, which extracted features automatically and hierarchically by a series of non-linear transformations to model the complicated imaging discrepancy. But they always pay more attention to the extraction and compensation of spatial details and use the mean squared error or mean absolute error as a loss function, regardless of the preservation of spectral information contained in multispectral images. For the sake of the improvements in both spatial and spectral resolution, this paper presents a novel fusion model that takes the spectral preservation into consideration, and learns the spectral cues from the process of generating a spectrally refined multispectral image, which is constrained by a spectral loss between the generated image and the reference image. Then these spectral cues are used to modulate the PAN features to obtain final fusion result. Experimental results on reduced-resolution and full-resolution datasets demonstrate that the proposed method can obtain a better fusion result in terms of visual inspection and evaluation indices when compared with current state-of-the-art methods.  © 1992-2012 IEEE.","Deep learning; Error compensation; Extraction; Generative adversarial networks; Image fusion; Linear transformations; Mean square error; Remote sensing; Features extraction; Multi-spectral; Multispectral images; Pan-sharpening; Remote-sensing; Spatial resolution; Spectral cue; Spectral loss; Task analysis; article; clinical assessment; learning; Image enhancement","generative adversarial networks; Image fusion; pansharpening; spectral cues; spectral loss","Article","Final","","Scopus","2-s2.0-85141570373"
"Gao J.; Yuan Q.; Li J.; Zhang H.; Su X.","Gao, Jianhao (57211516266); Yuan, Qiangqiang (36635300800); Li, Jie (57214207213); Zhang, Hai (57192694132); Su, Xin (57200950410)","57211516266; 36635300800; 57214207213; 57192694132; 57200950410","Cloud removal with fusion of high resolution optical and SAR images using generative adversarial networks","2020","Remote Sensing","12","1","191","","","","10.3390/RS12010191","56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083956677&doi=10.3390%2fRS12010191&partnerID=40&md5=90bf07a924911d7fca557a2c635a2b74","The existence of clouds is one of the main factors that contributes to missing information in optical remote sensing images, restricting their further applications for Earth observation, so how to reconstruct the missing information caused by clouds is of great concern. Inspired by the image-to-image translation work based on convolutional neural network model and the heterogeneous information fusion thought, we propose a novel cloud removal method in this paper. The approach can be roughly divided into two steps: in the first step, a specially designed convolutional neural network (CNN) translates the synthetic aperture radar (SAR) images into simulated optical images in an object-to-object manner; in the second step, the simulated optical image, together with the SAR image and the optical image corrupted by clouds, is fused to reconstruct the corrupted area by a generative adversarial network (GAN) with a particular loss function. Between the first step and the second step, the contrast and luminance of the simulated optical image are randomly altered to make the model more robust. Two simulation experiments and one real-data experiment are conducted to confirm the effectiveness of the proposed method on Sentinel 1/2, GF 2/3 and airborne SAR/optical data. The results demonstrate that the proposed method outperforms state-of-the-art algorithms that also employ SAR images as auxiliary data. © 2020 by the authors.","Convolution; Convolutional neural networks; Geometrical optics; Image fusion; Image reconstruction; Remote sensing; Synthetic aperture radar; Adversarial networks; Earth observations; Heterogeneous information; Image translation; Missing information; Optical remote sensing; State-of-the-art algorithms; Synthetic aperture radar (SAR) images; Radar imaging","Cloud removal; Deep learning; GAN; Information fusion; SAR","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85083956677"
"Sun C.; Zhang C.; Xiong N.","Sun, Changqi (57219872251); Zhang, Cong (55630034600); Xiong, Naixue (35231569200)","57219872251; 55630034600; 35231569200","Infrared and visible image fusion techniques based on deep learning: A review","2020","Electronics (Switzerland)","9","12","2162","1","24","23","10.3390/electronics9122162","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097914962&doi=10.3390%2felectronics9122162&partnerID=40&md5=b7178854975666042d505db4ac65b290","Infrared and visible image fusion technologies make full use of different image features obtained by different sensors, retain complementary information of the source images during the fusion process, and use redundant information to improve the credibility of the fusion image. In recent years, many researchers have used deep learning methods (DL) to explore the field of image fusion and found that applying DL has improved the time-consuming efficiency of the model and the fusion effect. However, DL includes many branches, and there is currently no detailed investigation of deep learning methods in image fusion. In this work, this survey reports on the development of image fusion algorithms based on deep learning in recent years. Specifically, this paper first conducts a detailed investigation on the fusion method of infrared and visible images based on deep learning, compares the existing fusion algorithms qualitatively and quantitatively with the existing fusion quality indicators, and discusses various fusions. The main contribution, advantages, and disadvantages of the algorithm. Finally, the research status of infrared and visible image fusion is summarized, and future work has prospected. This research can help us realize many image fusion methods in recent years and lay the foundation for future research work. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.","","Evaluation metric; Generative adversarial network; Image fusion; Infrared image; Visible image","Review","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85097914962"
"Xu H.; Ma J.; Zhang X.-P.","Xu, Han (57201056465); Ma, Jiayi (26638975600); Zhang, Xiao-Ping (35214025100)","57201056465; 26638975600; 35214025100","MEF-GAN: Multi-Exposure Image Fusion via Generative Adversarial Networks","2020","IEEE Transactions on Image Processing","29","","9112609","7203","7216","13","10.1109/TIP.2020.2999855","69","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088151985&doi=10.1109%2fTIP.2020.2999855&partnerID=40&md5=e3de7b573e6eea66a0a8a88b619821d9","In this paper, we present an end-to-end architecture for multi-exposure image fusion based on generative adversarial networks, termed as MEF-GAN. In our architecture, a generator network and a discriminator network are trained simultaneously to form an adversarial relationship. The generator is trained to generate a real-like fused image based on the given source images which is expected to fool the discriminator. Correspondingly, the discriminator is trained to distinguish the generated fused images from the ground truth. The adversarial relationship makes the fused image not limited to the restriction of the content loss. Therefore, the fused images are closer to the ground truth in terms of probability distribution, which can compensate for the insufficiency of single content loss. Moreover, aiming at the problem that the luminance of multi-exposure images varies greatly with spatial location, the self-attention mechanism is employed in our architecture to allow for attention-driven and long-range dependency. Thus, local distortion, confusing results, or inappropriate representation can be corrected in the fused image. Qualitative and quantitative experiments are performed on publicly available datasets, where the results demonstrate that MEF-GAN outperforms the state-of-the-art, in terms of both visual effect and objective evaluation metrics. Our code is publicly available at https://github.com/jiayi-ma/MEF-GAN. © 1992-2012 IEEE.","Network architecture; Probability distributions; Adversarial networks; Attention mechanisms; Long-range dependencies; Multi-exposure images; Objective evaluation; Quantitative experiments; Spatial location; State of the art; Image fusion","generative adversarial network; Image fusion; multi-exposure; self-attention","Article","Final","","Scopus","2-s2.0-85088151985"
"Murugan A.; Arumugam G.; Gobinath D.","Murugan, A. (56656227700); Arumugam, G. (57220410853); Gobinath, D. (56905380400)","56656227700; 57220410853; 56905380400","Multi-Focus Image Fusion Using Conditional Generative Adversarial Networks","2021","Advances in Intelligent Systems and Computing","1172","","","559","566","7","10.1007/978-981-15-5566-4_50","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092712995&doi=10.1007%2f978-981-15-5566-4_50&partnerID=40&md5=6b6df75c22f93f7c1b5895b9e34d81b3","Multi-focus image fusion is a task of generating a composite image from the numerous limited depth of focus images of the identical clips. The aim of multi-image fusion is to achieve a better quality image for better human visual perception or better machine interpretation. Calculating the focus map to distinguish the focused and unfocused pixels is the challenging task in image fusion algorithms. Numerous ways of calculating focus map have been proposed in the last two decades. Very recently, researchers show more interests on artificial intelligence and deep learning based algorithms to identify the focused regions from the limited depth of field image. Image fusion involves the following three subproblems: (1) Focus region detection, (2) Selecting the focus region based on confidence level, and (3) Final fusion. Motivated by Conditional Generative Adversarial Network (cGAN), a novel way of detecting the focused regions same as image-to-image translation has been proposed here. GAN requires more number of images to train the network. Due to lacking of limited depth of field images dataset, we synthesize a dataset based on PASCAL VOC 2012. We compare the results of the proposed deep learning based fusion methods with the traditional fusion methods. Results prove that the proposed unsupervised neural network based approach outperforms the other traditional methods. © 2021, Springer Nature Singapore Pte Ltd.","Deep learning; Intelligent computing; Learning algorithms; Adversarial networks; Human visual perception; Image fusion algorithms; Image translation; Learning-based algorithms; Machine interpretation; Multifocus image fusion; Unsupervised neural networks; Image fusion","Deep learning and generative adversarial network; Image fusion","Conference paper","Final","","Scopus","2-s2.0-85092712995"
"Yang Y.; Zhou H.; Zhang W.; Yang C.; Yu Z.","Yang, Yicheng (57215543768); Zhou, Huabing (55447554500); Zhang, Wei (57215936511); Yang, Changcai (24451597600); Yu, Zhenghong (55654324300)","57215543768; 55447554500; 57215936511; 24451597600; 55654324300","Image fusion via domain and feature transfer","2019","Proceedings - 2019 IEEE Intl Conf on Parallel and Distributed Processing with Applications, Big Data and  Cloud Computing, Sustainable Computing and Communications, Social Computing and Networking, ISPA/BDCloud/SustainCom/SocialCom 2019","","","9047282","1168","1172","4","10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00166","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085510312&doi=10.1109%2fISPA-BDCloud-SustainCom-SocialCom48970.2019.00166&partnerID=40&md5=00b7a855bf3dcb125031d37647eec1bd","The infrared and visible images fusion technique is designed to simultaneously preserve the thermal radiation information of the infrared image and the detailed texture information of the visible image. Therefore, the fused image with rich detail information and a clear target area in order to find it more efficiently. Inspired by the powerful GANs technology in recent years, we proposed a novel method for fusing infrared and visible images that based on domain and feature transfer. First, it is designated as an optimization problem with a latent encoding which can be mapped into a pixel intensity consistent image on the latent image. We employ generative adversarial network, which can capture content characteristics of one image dataset and figure out how these characteristics could be translated into the domain of another image dataset, to transfer the appearance of an image from visible image to infrared one. And then we use the feature consistent constrains to enhance the features of the fused image. Moreover, by adding gradient constraints to preserve the details of visible image, therefore we combine the fused image with a similar gradient to the visible image. Qualitative and quantitative comparisons on public datasets demonstrate our proposed method is superior to the state-of-the-art technology. The fused image we have generated is more like a high-resolution enhanced infrared image, which is more efficient for discovering the target and tracking. © 2019 IEEE.","Big data; Cloud computing; Image enhancement; Infrared imaging; Social networking (online); Textures; Adversarial networks; Fusion techniques; Infrared and visible image; Optimization problems; Quantitative comparison; Radiation information; State-of-the-art technology; Texture information; Image fusion","Feature/gradient consistent constrains; Infrared and visible images fusion; Pixel intensity consistent","Conference paper","Final","","Scopus","2-s2.0-85085510312"
"Cornett D., III; Yen A.; Nayola G.; Montez D.; Johnson C.R.; Baird S.T.; Santos-Villalobos H.; Bolme D.S.","Cornett, David (57215205996); Yen, Alec (57206660172); Nayola, Grace (57215196761); Montez, Diane (57215205174); Johnson, Christi R. (57007514800); Baird, Seth T. (57215202648); Santos-Villalobos, Hector (16043812000); Bolme, David S. (8410729500)","57215205996; 57206660172; 57215196761; 57215205174; 57007514800; 57215202648; 16043812000; 8410729500","Through the windshield driver recognition","2019","IS and T International Symposium on Electronic Imaging Science and Technology","2019","13","COIMG-140","","","","10.2352/ISSN.2470-1173.2019.13.COIMG-140","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080032647&doi=10.2352%2fISSN.2470-1173.2019.13.COIMG-140&partnerID=40&md5=e4503f3648e466f35b7b66926ab6d532","Biometric recognition of vehicle occupants in unconstrained environments is rife with a host of challenges. In particular, the complications arising from imaging through vehicle windshields provide a significant hurdle. Distance to target, glare, poor lighting, head pose of occupants, and speed of vehicle are some of the challenges. We explore the construction of a multi-unit computational camera system to mitigate these challenges in order to obtain accurate and consistent face recognition results. This paper documents the hardware components and software design of the computational imaging system. Also, we document the use of Region-based Convolutional Neural Network (RCNN) for face detection and Generative Adversarial Network (GAN) for machine learning-inspired High Dynamic Range Imaging, artifact removal, and image fusion. © 2019, Society for Imaging Science and Technology","Convolutional neural networks; Image fusion; Software design; Windshields; Adversarial networks; Biometric recognition; Computational cameras; Computational imaging system; Distance to targets; Hardware components; High dynamic range imaging; Unconstrained environments; Face recognition","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85080032647"
"Li W.; Xiong W.; Liao H.; Huo J.; Gao Y.; Luo J.","Li, Wenbin (57089688500); Xiong, Wei (57207868248); Liao, Haofu (57193613750); Huo, Jing (55336296400); Gao, Yang (56403534500); Luo, Jiebo (7404182441)","57089688500; 57207868248; 57193613750; 55336296400; 56403534500; 7404182441","CariGAN: Caricature generation through weakly paired adversarial learning","2020","Neural Networks","132","","","66","74","8","10.1016/j.neunet.2020.08.011","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089849166&doi=10.1016%2fj.neunet.2020.08.011&partnerID=40&md5=5b9e41b2f9d03ab605df8ac31c00e470","Caricature generation is an interesting yet challenging task. The primary goal is to generate a plausible caricature with reasonable exaggerations given a face image. Conventional caricature generation approaches mainly use low-level geometric transformations such as image warping to generate exaggerated images, which lack richness and diversity in terms of content and style. The recent progress in generative adversarial networks (GANs) makes it possible to learn an image-to-image transformation from data so as to generate diverse images. However, directly applying GAN-based models to this task leads to unsatisfactory results due to the large variance in the caricature distribution. Moreover, conventional models typically require pixel-wisely paired training data which largely limits their usage scenarios. In this paper, we model caricature generation as a weakly paired image-to-image translation task, and propose CariGAN to address these issues. Specifically, to enforce reasonable exaggeration and facial deformation, manually annotated caricature facial landmarks are used as an additional condition to constrain the generated image. Furthermore, an image fusion mechanism is designed to encourage our model to focus on the key facial parts so that more vivid details in these regions can be generated. Finally, a diversity loss is proposed to encourage the model to produce diverse results. Extensive experiments on a large-scale “WebCaricature” dataset show that the proposed CariGAN can generate more visually plausible caricatures with larger diversity compared with the state-of-the-art models. © 2020 Elsevier Ltd","Automated Facial Recognition; Caricatures as Topic; Face; Family Characteristics; Female; Humans; Image Processing, Computer-Assisted; Male; Image fusion; Large dataset; Mathematical transformations; Adversarial learning; Adversarial networks; Caricature generation; Conventional models; Facial deformations; Geometric transformations; Image fusion mechanisms; Image transformations; art; article; face; learning; art; family size; female; human; image processing; male; procedures; Metadata","Caricature generation; GANs; Image fusion mechanism","Article","Final","All Open Access; Bronze Open Access; Green Open Access","Scopus","2-s2.0-85089849166"
"Guo X.; Nie R.; Cao J.; Zhou D.; Mei L.; He K.","Guo, Xiaopeng (57195421420); Nie, Rencan (23668359400); Cao, Jinde (7403354075); Zhou, Dongming (57195231085); Mei, Liye (57204005684); He, Kangjian (57093043000)","57195421420; 23668359400; 7403354075; 57195231085; 57204005684; 57093043000","FuseGAN: Learning to Fuse Multi-Focus Image via Conditional Generative Adversarial Network","2019","IEEE Transactions on Multimedia","21","8","8625482","1982","1996","14","10.1109/TMM.2019.2895292","99","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065898406&doi=10.1109%2fTMM.2019.2895292&partnerID=40&md5=d5718dcfecf7da5645c0ca04610300d6","We study the problem of multi-focus image fusion, where the key challenge is detecting the focused regions accurately among multiple partially focused source images. Inspired by the conditional generative adversarial network (cGAN) to image-to-image task, we propose a novel FuseGAN to fulfill the images-to-image for multi-focus image fusion. To satisfy the requirement of dual input-to-one output, the encoder of the generator in FuseGAN is designed as a Siamese network. The least square GAN objective is employed to enhance the training stability of FuseGAN, resulting in an accurate confidence map for focus region detection. Also, we exploit the convolutional conditional random fields technique on the confidence map to reach a refined final decision map for better focus region detection. Moreover, due to the lack of a large-scale standard dataset, we synthesize a large enough multi-focus image dataset based on a public natural image dataset PASCAL VOC 2012, where we utilize a normalized disk point spread function to simulate the defocus and separate the background and foreground in the synthesis for each image. We conduct extensive experiments on two public datasets to verify the effectiveness of the proposed method. Results demonstrate that the proposed method presents accurate decision maps for focus regions in multi-focus images, such that the fused images are superior to 11 recent state-of-the-art algorithms, not only in visual perception, but also in quantitative analysis in terms of five metrics. © 1999-2012 IEEE.","Convolution; Image processing; Large dataset; Optical transfer function; Random processes; Adversarial networks; Conditional random field; images-to-image; Multifocus image fusion; synthesize dataset; Image fusion","Conditional generative adversarial network; convolutional conditional random fields; images-to-image; multi-focus image fusion; synthesize dataset","Article","Final","","Scopus","2-s2.0-85065898406"
"Marcos L.; Alirezaie J.; Babyn P.","Marcos, Luella (57404726500); Alirezaie, Javad (6701846696); Babyn, Paul (7006367819)","57404726500; 6701846696; 7006367819","Low Dose CT Image Denoising Using Boosting Attention Fusion GAN with Perceptual Loss","2021","Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS","","","","3407","3410","3","10.1109/EMBC46164.2021.9630790","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122544861&doi=10.1109%2fEMBC46164.2021.9630790&partnerID=40&md5=968ac488c53174070b728cdac369b9be","Image denoising of Low-dose computed tomography (LDCT) images has continues to receive attention in the research community due to ongoing concerns about high-dose radiation exposure of patients for diagnosis. The use of low radiation CT image, however, could lead to inaccurate diagnosis due to the presence of noise. Deep learning techniques are being integrated into denoising methods to address this problem. In this paper, a General Adversarial Network (GAN) composed of boosting fusion of spatial and channel attention modules is proposed. These modules are embedded in the denoiser to address the limitations of other GAN-based denoising models that tend to only focus on the local processing and neglect the dependencies of creating feature maps with spatial- and channel- wise image characteristics. This study aims to preserve structural details of LDCT images by applying boosting attention modules, prevents edge over-smoothing by integrating perceptual loss via VGG16 pre-trained network, and finally, improves the computational efficiency by taking advantage of deep learning techniques and GPU parallel computation. © 2021 IEEE.","Algorithms; Attention; Humans; Image Processing, Computer-Assisted; Signal-To-Noise Ratio; Tomography, X-Ray Computed; Computational efficiency; Computerized tomography; Deep learning; Diagnosis; Generative adversarial networks; Image enhancement; Image fusion; Learning algorithms; Medical imaging; Adversarial networks; Attention fusion; Computed tomography images; CT Image; Dose computed tomographies; High dose; Learning techniques; Low dose; Low-dose CT; Research communities; algorithm; attention; human; image processing; signal noise ratio; x-ray computed tomography; Image denoising","Attention fusion; Computed tomography; Generative adversarial network; Image denoising; Medical imaging","Conference paper","Final","","Scopus","2-s2.0-85122544861"
"Zhao K.; Cheng J.; Liu T.; Deng H.","Zhao, Kangcheng (57219339950); Cheng, Jianghua (36554022800); Liu, Tong (57202057306); Deng, Huafu (57214779522)","57219339950; 36554022800; 57202057306; 57214779522","A generative adversarial network for fusion of infrared and visible images based on UNet++","2020","Proceedings of SPIE - The International Society for Optical Engineering","11584","","1158405","","","","10.1117/12.2577947","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097130283&doi=10.1117%2f12.2577947&partnerID=40&md5=ddb13d1bf9bc75578ad5c7f9ce9c5c78","This paper proposes a novel generation adversarial network (GAN) based on UNet++ architecture for infrared and visible image fusion. The idea of this method is to establish an adversarial game between the generator that generates the fusion image and the discriminator that determines whether the fusion image meets the standard. The generator uses the structure of UNet++, which does not have a deep network structure but establishes a dense connection at the shallow layer, so it has strong ability to obtain shallow features. As for the discriminator, it adopts a network structure similar to the Visual Geometry Group (VGG) network. The loss function uses the method of comparing the high-frequency part of the fused image with the two source images to make the fused image have more high-frequency information. In terms of high-frequency detail extraction of source images, two gradient extraction methods based on different combinations of directional extraction operators and high-frequency extraction are proposed in this paper. This paper compares the two methods above with other fusion networks, and proves that the fusion image generated by our network is highly similar to the infrared image, and has a lot of gradient information of the visible image.  © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Artificial intelligence; Extraction; Infrared imaging; Video signal processing; Adversarial networks; Extraction method; Gradient informations; High frequency HF; High-frequency extraction; High-frequency informations; Infrared and visible image; Network structures; Image fusion","GAN; UNet++; VGG-net","Conference paper","Final","","Scopus","2-s2.0-85097130283"
"Wang M.; He L.; Chang X.; Cheng Y.","Wang, Mi (57205635094); He, Luxiao (57201260628); Chang, Xueli (56421263200); Cheng, Yufeng (56861501800)","57205635094; 57201260628; 56421263200; 56861501800","Corrections to: Superresolution of single gaofen-4 visible-light and near-infrared (VNIR) image based on texture image extraction (IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing DOI: 10.1109/JSTARS.2019.2926490)","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","8","8823074","3149","","","10.1109/JSTARS.2019.2937427","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072749190&doi=10.1109%2fJSTARS.2019.2937427&partnerID=40&md5=f37eaf1a7aff60b63f26d90894a244bc","In the above article [1], production errors are corrected as follows. Funding information should be added as follows: This work was supported in part by the China National Funds for Distinguished Young Scientists under Grant 61825103 and in part by the National Natural Science Foundation of China under Grant 91838303. On page 3, Ref. [36] should be cited in the sentence: ""For panchromatic and multispectral images of the same scene, the two have strong correlation [34]-[36]."" On page 5, the sentence ""For referenced frame construction, it is more important to choose which LR texture image."" should be ""For referenced frame construction, it is important to choose LR texture image."" On page 5, the sentence ""Local or global motion can be ignored since the data a single GF-4 VNIR image."" should be ""Local or global motion can be ignored since the input data is a single image."" On page 7, a sentence should be added as follows. In Tables III-V, the evaluation parameters of SR results in urban, lake, and mountain area, respectively, have been presented. The bold values in Tables III-V are the best parameters of the five methods."" An acknowledgement should be added as follows: ""The authors would like to thank CRESDA for providing the experimental data. Corrected references are as follows. [15] M. Irani and S. Peleg, ""Improving resolution by image registration,"" CVGIP, Graphical Models Image Process., vol. 53, no. 3, pp. 231-239, 1991.[19] R. R. Schultz and R. L. Stevenson, ""Extraction of HR frames from video sequences,"" IEEE Trans. Image Process., vol. 5, no. 6, pp. 996-1011, Jun. 1996. [28] C. Ledig et al., ""Photo-realistic single image super-resolution using a generative adversarial network,"" in Proc. IEEE Conf. Comput. Vision Pattern Recognit., Jul. 2017, pp. 105-114. [30] C. Dong, C. C. Loy, K. He, and X. Tang, ""Learning a deep convolutional network for image super-resolution,"" in Proc. Eur. Conf. Comput. Vision, 2014, pp. 184-199. [33] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, ""Residual dense network for image super-resolution,"" in Proc. IEEE Conf. Comput. Vision Pattern Recognit., 2018, pp. 2472-2481. [34] X. U. Qi-Zhi and G. Feng, ""High fidelity panchromatic and multispectral image fusion based on ratio transform,"" Comput. Sci., vol. 41, no. 10, pp. 19-22, 2014. [37] R. C. Gonzalez and R. E. Woods, Digital Image Processing. Beijing, China: Publishing House Electron. Ind., 2007. © 2008-2012 IEEE.","","","Erratum","Final","","Scopus","2-s2.0-85072749190"
"Kang J.; Lu W.; Zhang W.","Kang, Jiayin (14625118900); Lu, Wu (57214103892); Zhang, Wenjuan (37010575100)","14625118900; 57214103892; 37010575100","Fusion of Brain PET and MRI Images Using Tissue-Aware Conditional Generative Adversarial Network with Joint Loss","2020","IEEE Access","8","","8949485","6368","6378","10","10.1109/ACCESS.2019.2963741","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078270543&doi=10.1109%2fACCESS.2019.2963741&partnerID=40&md5=77f3143017b85521f84ed5966694d414","Positron emission tomography (PET) has rich pseudo color information that reflects the functional characteristics of tissue, but lacks structural information and its spatial resolution is low. Magnetic resonance imaging (MRI) has high spatial resolution as well as strong structural information of soft tissue, but lacks color information that shows the functional characteristics of tissue. For the purpose of integrating the color information of PET with the anatomical structures of MRI to help doctors diagnose diseases better, a method for fusing brain PET and MRI images using tissue-aware conditional generative adversarial network (TA-cGAN) is proposed. Specifically, the process of fusing brain PET and MRI images is treated as an adversarial machine between retaining the color information of PET and preserving the anatomical information of MRI. More specifically, the fusion of PET and MRI images can be regarded as a min-max optimization problem with respect to the generator and the discriminator, where the generator attempts to minimize the objective function via generating a fused image mainly contains the color information of PET, whereas the discriminator tries to maximize the objective function through urging the fused image to include more structural information of MRI. Both the generator and the discriminator in TA-cGAN are conditioned on the tissue label map generated from MRI image, and are trained alternatively with joint loss. Extensive experiments demonstrate that the proposed method enhances the anatomical details of the fused image while effectively preserving the color information from the PET. In addition, compared with other state-of-the-art methods, the proposed method achieves better fusion effects both in subjectively visual perception and in objectively quantitative assessment. © 2013 IEEE.","Color; Image enhancement; Image fusion; Image resolution; Positron emission tomography; Positrons; Tissue; Tissue engineering; Adversarial networks; Anatomical information; Functional characteristics; High spatial resolution; Loss functions; Positron emission tomography (PET); Quantitative assessments; State-of-the-art methods; Magnetic resonance imaging","generative adversarial network; image fusion; loss function; magnetic resonance imaging; Positron emission tomography","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85078270543"
"Chen L.; Han J.; Tian F.","Chen, Lei (57273656000); Han, Jun (55753463900); Tian, Feng (57669528000)","57273656000; 55753463900; 57669528000","Infrared and visible image fusion using two-layer generative adversarial network","2021","Journal of Intelligent and Fuzzy Systems","40","6","","11897","11913","16","10.3233/JIFS-210041","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111165045&doi=10.3233%2fJIFS-210041&partnerID=40&md5=937637b6027ecd590486debf4defa74c","Infrared (IR) images can distinguish targets from their backgrounds based on difference in thermal radiation, whereas visible images can provide texture details with high spatial resolution. The fusion of the IR and visible images has many advantages and can be applied to applications such as target detection and recognition. This paper proposes a two-layer generative adversarial network (GAN) to fuse these two types of images. In the first layer, the network generate fused images using two GANs: one uses the IR image as input and the visible image as ground truth, and the other with the visible as input and the IR as ground truth. In the second layer, the network transfer one of the two fused images generated in the first layer as input and the other as ground truth to GAN to generate the final fused image. We adopt TNO and INO data sets to verify our method, and by comparing with eight objective evaluation parameters obtained by other ten methods. It is demonstrated that our method is able to achieve better performance than state-of-arts on preserving both texture details and thermal information. © 2021 - IOS Press. All rights reserved.","Infrared imaging; Network layers; Textures; Adversarial networks; Fused images; High spatial resolution; Infrared and visible image; Network transfers; Objective evaluation; Target detection and recognition; Visible image; Image fusion","deep learning; generative adversarial network; image fusion; IR and visible images","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85111165045"
"Wang A.; Zhang S.; Chen C.; Zheng Z.","Wang, Anni (57688500700); Zhang, Shengsen (57250616500); Chen, Chunxu (57251077100); Zheng, Zengqiang (57221207111)","57688500700; 57250616500; 57251077100; 57221207111","Simulation algorithm of industrial defects based on generative adversarial network","2021","Digest of Technical Papers - SID International Symposium","52","S1","","510","513","3","10.1002/sdtp.14538","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114459615&doi=10.1002%2fsdtp.14538&partnerID=40&md5=f20493afdcdec54e671bd06a6753413a","Due to the application of deep learning method, defect detection in the industrial field is no longer limited to the traditional machine vision algorithm. However, the dependency of deep learning on samples and the lack of defect samples during industrial detection process are urgent problems that need to be solved. Based on Generative Adversarial Network (GAN) network, this paper proposes a defect sample simulation generation algorithm for industrial defect. Based on the characteristics of industrial defects, this paper first extracts the samples, intercepts the specific defects according to the location, and then uses the StyleGAN network training to generate the model. Then, based on the corresponding model of location information, new samples are generated. Finally, according to the generated samples and location information, we provide a typical image fusion method to generate the final samples. Through the simulation experiment of panel defects, the algorithm proposed in this paper can achieve the generation and fusion of defects, and form a new sample data set for use. © 2021, John Wiley and Sons Inc. All rights reserved.","Deep learning; Image fusion; Learning systems; Location; Adversarial networks; Generation algorithm; Image fusion methods; Industrial detection; Location information; Machine vision algorithm; Sample simulations; Simulation algorithms; Defects","Defects Simulation; Generative Adversarial Network; Sample expansion","Conference paper","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85114459615"
"Tao M.; Wu S.; Zhang X.; Wang C.","Tao, Ming (57219565980); Wu, Songsong (24485676900); Zhang, Xiaofeng (57217109516); Wang, Cailing (35367319800)","57219565980; 24485676900; 57217109516; 35367319800","DCFGAN: Dynamic convolutional fusion generative adversarial network for text-to-image synthesis","2020","Proceedings of 2020 IEEE International Conference on Information Technology, Big Data and Artificial Intelligence, ICIBA 2020","","","9277299","1250","1254","4","10.1109/ICIBA50161.2020.9277299","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099320789&doi=10.1109%2fICIBA50161.2020.9277299&partnerID=40&md5=85ba94bb23f30c1ec3c60cea1b4f6821","Text-to-image synthesis is the task of synthesizing realistic and text-matching images according to given text descriptions. Most text-to-image generative networks consist of two modules: a pre-trained text-image encoder and a text-to-image generative adversarial network. In this paper, we propose a stronger text encoder which employs a text Transformer to extract semantically meaningful parts from text descriptions. With the stronger text encoder, the generator can obtain more meaningful text information to synthesize realistic and text-matching images. In addition, we propose a Dynamic Convolutional text-image Fusion Generative Adversarial Network (DCFGAN) which employs the Dynamic Convolutional Fusion Block to fuse text and image features efficiently. The Dynamic Convolutional Fusion block adjusts the parameters in the convolution layer according to different text descriptions to synthesize text-matching images. It improves the efficiency of fusing text features and image features in generator network. We evaluate the proposed DCF-GAN on two benchmark datasets, the CUB and the Oxford-102. The extensive experiments demonstrate that our proposed stronger text encoder and Dynamic Convolutional Fusion Layer can greatly promote the performance of text-to-image synthesis. © 2020 IEEE.","Artificial intelligence; Big data; Convolution; Image enhancement; Signal encoding; Adversarial networks; Benchmark datasets; Fusion layers; Image features; Image synthesis; Text feature; Text information; Text-matching; Image fusion","Cross-modal; Deep representation learning; Generative adversarial network; Image-text matching; Text-to-Image synthesis","Conference paper","Final","","Scopus","2-s2.0-85099320789"
"Tang W.; Liu Y.; Zhang C.; Cheng J.; Peng H.; Chen X.","Tang, Wei (57712357100); Liu, Yu (57192560727); Zhang, Chao (57206684155); Cheng, Juan (56849663000); Peng, Hu (55512248000); Chen, Xun (36456894700)","57712357100; 57192560727; 57206684155; 56849663000; 55512248000; 36456894700","Green Fluorescent Protein and Phase-Contrast Image Fusion via Generative Adversarial Networks","2019","Computational and Mathematical Methods in Medicine","2019","","5450373","","","","10.1155/2019/5450373","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076885535&doi=10.1155%2f2019%2f5450373&partnerID=40&md5=d13dcbcecf6116fd1dfe38019cde62f7","In the field of cell and molecular biology, green fluorescent protein (GFP) images provide functional information embodying the molecular distribution of biological cells while phase-contrast images maintain structural information with high resolution. Fusion of GFP and phase-contrast images is of high significance to the study of subcellular localization, protein functional analysis, and genetic expression. This paper proposes a novel algorithm to fuse these two types of biological images via generative adversarial networks (GANs) by carefully taking their own characteristics into account. The fusion problem is modelled as an adversarial game between a generator and a discriminator. The generator aims to create a fused image that well extracts the functional information from the GFP image and the structural information from the phase-contrast image at the same time. The target of the discriminator is to further improve the overall similarity between the fused image and the phase-contrast image. Experimental results demonstrate that the proposed method can outperform several representative and state-of-the-art image fusion methods in terms of both visual quality and objective evaluation. © 2019 Wei Tang et al.","Algorithms; Cell Biology; Computational Biology; Deep Learning; Green Fluorescent Proteins; Image Processing, Computer-Assisted; Microscopy, Phase-Contrast; Models, Biological; Bioinformatics; Fluorescence; Image enhancement; Molecular biology; Proteins; Quality control; green fluorescent protein; green fluorescent protein; Adversarial networks; Functional information; Green fluorescent protein; Molecular distribution; Phase-contrast image; Protein functional analysis; Structural information; Subcellular localizations; Article; contrast enhancement; evaluation study; image analysis; imaging algorithm; information processing; mathematical computing; molecular biology; phase contrast image fusion; algorithm; biological model; biology; cytology; image processing; metabolism; phase contrast microscopy; procedures; Image fusion","","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85076885535"
"Luo X.; Wang A.; Zhang Z.; Xiang X.; Wu X.-J.","Luo, Xiaoqing (23985472800); Wang, Anqi (57248306500); Zhang, Zhancheng (57219233537); Xiang, Xinguang (56453889000); Wu, Xiao-Jun (56191888600)","23985472800; 57248306500; 57219233537; 56453889000; 56191888600","LatRAIVF: An Infrared and Visible Image Fusion Method Based on Latent Regression and Adversarial Training","2021","IEEE Transactions on Instrumentation and Measurement","70","","9520770","","","","10.1109/TIM.2021.3105250","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114362734&doi=10.1109%2fTIM.2021.3105250&partnerID=40&md5=711ec5bd3699c971340805be7a6f52be","In this article, we propose a novel method for infrared and visible image fusion based on latent regression and adversarial training, which is named as LatRAIVF. Compared to existing deep learning (DL)-based image fusion method that only focuses on the spatial information, we consider to utilize the information provided by high-level feature maps from latent space, which can guide the network to learn about semantically important feature information. The proposed method is based on the framework of conditional generative adversarial network (GAN), and two encoders are adopted to learn the respective semantic latent representations for the infrared and visible images, which are then combined by max-selection strategy and input into the decoder, with skip connections between the corresponding layers of the encoder and the decoder, to achieve the fused image. Apart from the adversarial process that enables the fused image to obtain more realistic details, we design two branches to constrain the generation of the image: a content loss to make the fused image close to the label image, and a latent regression loss to ensure the fused image with salient features from the infrared and visible images. Due to the lack of physical ground-truth fused images in public infrared and visible image datasets and the difficulties in defining desired fused image, we make use of existing RGB-D dataset to synthesize an infrared and visible image dataset with ground truths based on the widely used optical model for better network training. Comparison experiments show that the fused results of the proposed method can transfer meaningful features from the source image and provide good fusion quality. © 1963-2012 IEEE.","Decoding; Deep learning; Image processing; Semantics; Signal encoding; Adversarial networks; High-level features; Image fusion methods; Important features; Infrared and visible image; Network training; Optical modeling; Spatial informations; Image fusion","Deep learning (DL); generative adversarial networks (GANs); image fusion; infrared and visible image; latent space regression","Article","Final","","Scopus","2-s2.0-85114362734"
"Li J.; Huo H.; Liu K.; Li C.","Li, Jing (57207844651); Huo, Hongtao (56527849900); Liu, Kejian (57208860186); Li, Chang (56718731300)","57207844651; 56527849900; 57208860186; 56718731300","Infrared and visible image fusion using dual discriminators generative adversarial networks with Wasserstein distance","2020","Information Sciences","529","","","28","41","13","10.1016/j.ins.2020.04.035","41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083735057&doi=10.1016%2fj.ins.2020.04.035&partnerID=40&md5=64c7fe245eb2041fc6debe254e28378f","Generative adversarial network (GAN) has shown great potential in infrared and visible image fusion. The existing GAN-based methods establish an adversarial game between generative image and source images to train the generator until the generative image contains enough meaningful information from source images. However, they only design one discriminator to force the fused result to complement gradient information from visible image, which may lose some detail information that existing in infrared image and omit some texture information that existing in visible image. To this end, we propose an end-to-end dual discriminators Wasserstein generative adversarial network, termed as D2WGAN, a framework that extends GAN to dual discriminators. In D2WGAN, the fused image can keep pixel intensity and details of infrared image by the first discriminator, and capture rich texture information of visible image by the second discriminator. In addition, to improve the performance of D2WGAN, we employ the GAN with Wasserstein distance. Moreover, in order to make the fused image keep more details from visible image in texture feature domain, we define a novel LBP (local binary pattern) loss. The extensive qualitative and quantitative experiments on public datasets demonstrate that D2WGAN can generate better results compared with the other state-of-the-art methods. © 2020 Elsevier Inc.","Discriminators; Image fusion; Infrared imaging; Textures; Adversarial networks; Gradient informations; Infrared and visible image; Local binary patterns; Quantitative experiments; State-of-the-art methods; Texture information; Wasserstein distance; Image texture","Image fusion; Infrared image; Local binary pattern; Visible image; Wasserstein generative adversarial network","Article","Final","","Scopus","2-s2.0-85083735057"
"Zhou Y.; Li K.","Zhou, Yunfei (57216540088); Li, Kang (57222052374)","57216540088; 57222052374","A cross-spectral image fusion based visual odometry","2020","Proceedings - 2020 35th Youth Academic Annual Conference of Chinese Association of Automation, YAC 2020","","","9337596","428","432","4","10.1109/YAC51587.2020.9337596","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101084090&doi=10.1109%2fYAC51587.2020.9337596&partnerID=40&md5=22b892c3068027c003382c74f441df41","This paper proposes a cross-spectral image fusion based visual odometry with GAN (Generative Adversarial Networks) and Residual Dense Blocks. It can well preserve the intensity information in infrared images and the gradient information in visible images. Based on fused images, the pose can be obtained. Experimental verification was performed on All-Day Visual Place Recognition Benchmark Dataset. And results in experiment show that compared to merely infrared and visible images, the positioning accuracy of the fusion image has been greatly improved. © 2020 IEEE.","Computer vision; Image enhancement; Infrared imaging; Spectroscopy; Vision; Adversarial networks; Benchmark datasets; Experimental verification; Gradient informations; Infrared and visible image; Intensity information; Positioning accuracy; Spectral image fusions; Image fusion","GAN; Image Fusion; Residual Dense Blocks; Visual Odometry","Conference paper","Final","","Scopus","2-s2.0-85101084090"
"Wang L.; Liu Z.; Huang J.; Liu C.; Zhang L.; Liu C.","Wang, Lei (57213182331); Liu, ZhouQi (57223269471); Huang, Jin (57221063142); Liu, Cong (56654610800); Zhang, LongBo (14520051300); Liu, ChunXiang (56416711200)","57213182331; 57223269471; 57221063142; 56654610800; 14520051300; 56416711200","The Fusion of Multi-Focus Images Based on the Complex Shearlet Features-Motivated Generative Adversarial Network","2021","Journal of Advanced Transportation","2021","","5439935","","","","10.1155/2021/5439935","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112419797&doi=10.1155%2f2021%2f5439935&partnerID=40&md5=d0a97e42553faf57b39ae74b487dbc79","The traditional methods for multi-focus image fusion, such as the typical multi-scale geometric analysis theory-based methods, are usually restricted by sparse representation ability and the transferring efficiency of the fusion rules for the captured features. Aiming to integrate the partially focused images into the fully focused image with high quality, the complex shearlet features-motivated generative adversarial network is constructed for multi-focus image fusion in this paper. Different from the popularly used wavelet, contourlet, and shearlet, the complex shearlet provides more flexible multiple scales, anisotropy, and directional sub-bands with the approximate shift invariance. Therefore, the features in complex shearlet domain are more effective. With of help of the generative adversarial network, the whole procedure of multi-focus fusion is modeled to be the process of adversarial learning. Finally, several experiments are implemented and the results prove that the proposed method outperforms the popularly used fusion algorithms in terms of four typical objective metrics and the comparison of visual appearance.  © 2021 Lei Wang et al.","Complex networks; Adversarial learning; Adversarial networks; Fusion algorithms; Multi-scale geometric analysis; Multifocus image fusion; Multifocus images; Sparse representation; Visual appearance; Image fusion","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85112419797"
"Jin X.; Huang S.; Jiang Q.; Lee S.-J.; Wu L.; Yao S.","Jin, Xin (56991832300); Huang, Shanshan (57214939600); Jiang, Qian (57194699462); Lee, Shin-Jye (34877262700); Wu, Liwen (57200984308); Yao, Shaowen (24473851600)","56991832300; 57214939600; 57194699462; 34877262700; 57200984308; 24473851600","Semisupervised Remote Sensing Image Fusion Using Multiscale Conditional Generative Adversarial Network with Siamese Structure","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9461404","7066","7084","18","10.1109/JSTARS.2021.3090958","9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111651246&doi=10.1109%2fJSTARS.2021.3090958&partnerID=40&md5=3031ab016fdaacc95522cebf92003b10","Remote sensing image fusion (RSIF) can generate an integrated image with high spatial and spectral resolution. The fused remote sensing image is conducive to applications including disaster monitoring, ecological environment investigation, and dynamic monitoring. However, most existing deep learning based RSIF methods require ground truths (or reference images) to train a model, and the acquisition of ground truths is a difficult problem. To address this, we propose a semisupervised RSIF method based on the multiscale conditional generative adversarial networks by combining the multiskip connection and pseudo-Siamese structure. This new method can simultaneously extract the features of panchromatic and multispectral images to fuse them without a ground truth; the adopted multiskip connection contributes to presenting image details. In addition, we propose a composite loss function, which combines the least squares loss, L1 loss, and peak signal-to-noise ratio loss to train the model; the composite loss function can help to retain the spatial details and spectral information of the source images. Moreover, we verify the proposed method by extensive experiments, and the results show that the new method can achieve outstanding performance without relying on the ground truth. © 2008-2012 IEEE.","Deep learning; Remote sensing; Signal to noise ratio; Adversarial networks; Disaster monitoring; Dynamic monitoring; Ecological environments; Multispectral images; Peak signal to noise ratio; Remote sensing images; Spectral information; computer simulation; data acquisition; experimental study; numerical model; performance assessment; remote sensing; spatial resolution; spectral resolution; supervised classification; Image fusion","Conditional generative adversarial network (cGAN); deep learning (DL); image fusion; loss function; remote sensing image fusion (RSIF)","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85111651246"
"Wang J.; Li X.; Liu H.","Wang, Jinhua (56019276900); Li, Xuewei (57202971176); Liu, Hongzhe (26660676100)","56019276900; 57202971176; 26660676100","Exposure fusion using a relative generative adversarial network","2021","IEICE Transactions on Information and Systems","E104D","7","","1017","1027","10","10.1587/transinf.2021EDP7028","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109609437&doi=10.1587%2ftransinf.2021EDP7028&partnerID=40&md5=2aa6fdbf358f2ce35f2f1d103dbb8f18","At present, the generative adversarial network (GAN) plays an important role in learning tasks. The basic idea of a GAN is to train the discriminator and generator simultaneously. A GAN-based inverse tone mapping method can generate high dynamic range (HDR) images corresponding to a scene according to multiple image sequences of a scene with different exposures. However, subsequent tone mapping algorithm processing is needed to display it on a general device. This paper proposes an end-to-end multi-exposure image fusion algorithm based on a relative GAN (called RaGAN-EF), which can fuse multiple image sequences with different exposures directly to generate a high-quality image that can be displayed on a general device without further processing. The RaGAN is used to design the loss function, which can retain more details in the source images. In addition, the number of input image sequences of multi-exposure image fusion algorithms is often uncertain, which limits the application of many existing GANs. This paper proposes a convolutional layer with weights shared between channels, which can solve the problem of variable input length. Experimental results demonstrate that the proposed method performs better in terms of both objective evaluation and visual quality. Copyright © 2021 The Institute of Electronics, Information and Communication Engineers.","Conformal mapping; Display devices; Inverse problems; Adversarial networks; Exposure fusions; High dynamic range images; High quality images; Inverse tone mappings; Multi-exposure images; Objective evaluation; Variable input lengths; Image fusion","Exposure fusion; High dynamic range image; Relative generative adversarial network","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85109609437"
"Wang G.; Gong E.; Banerjee S.; Martin D.; Tong E.; Choi J.; Chen H.; Wintermark M.; Pauly J.M.; Zaharchuk G.","Wang, Guanhua (57212214103); Gong, Enhao (56046651600); Banerjee, Suchandrima (8442706400); Martin, Dann (56603517700); Tong, Elizabeth (57222527858); Choi, Jay (57211131780); Chen, Huijun (57212853092); Wintermark, Max (7003404861); Pauly, John M. (7101724924); Zaharchuk, Greg (6602464023)","57212214103; 56046651600; 8442706400; 56603517700; 57222527858; 57211131780; 57212853092; 7003404861; 7101724924; 6602464023","Synthesize High-Quality Multi-Contrast Magnetic Resonance Imaging from Multi-Echo Acquisition Using Multi-Task Deep Generative Model","2020","IEEE Transactions on Medical Imaging","39","10","9063444","3089","3099","10","10.1109/TMI.2020.2987026","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092680711&doi=10.1109%2fTMI.2020.2987026&partnerID=40&md5=d4334ba1877979ea8afa0d1f1fea9f68","Multi-echo saturation recovery sequence can provide redundant information to synthesizemulti-contrast magnetic resonance imaging. Traditional synthesis methods, such as GE's MAGiC platform, employ a model-fitting approach to generate parameter-weighted contrasts. However, models' over-simplification, as well as imperfections in the acquisition, can lead to undesirable reconstruction artifacts, especially in T2-FLAIR contrast. To improve the image quality, in this study, a multi-task deep learning model is developed to synthesize multi-contrast neuroimaging jointly using both signal relaxation relationships and spatial information. Compared with previous deep learning-based synthesis, the correlation between different destination contrast is utilized to enhance reconstruction quality. To improvemodel generalizabilityand evaluate clinical significance, the proposedmodelwas trained and tested on a large multi-center dataset, including healthy subjects and patients with pathology. Results from both quantitative comparison and clinical reader study demonstrate that the multi-task formulation leads to more efficient and accurate contrast synthesis than previous methods. © 2020 IEEE.","Artifacts; Brain; Humans; Image Processing, Computer-Assisted; Magnetic Resonance Imaging; Neuroimaging; Image enhancement; Image quality; Large dataset; Magnetic resonance imaging; Multi-task learning; Neuroimaging; Generative model; Healthy subjects; Model-fitting approach; Quantitative comparison; Reconstruction artifacts; Reconstruction quality; Spatial informations; Synthesis method; adult; article; clinical evaluation; controlled study; deep learning; human; image quality; leisure; neuroimaging; nuclear magnetic resonance imaging; quantitative analysis; synthesis; artifact; brain; diagnostic imaging; image processing; Deep learning","Deep learning (dl); Generative adversarial network (gan); Image fusion; Image synthesis; Magnetic resonance imaging (mri)","Article","Final","","Scopus","2-s2.0-85092680711"
"Zhang H.; Yuan J.; Tian X.; Ma J.","Zhang, Hao (57215014270); Yuan, Jiteng (57310387700); Tian, Xin (55458978600); Ma, Jiayi (26638975600)","57215014270; 57310387700; 55458978600; 26638975600","GAN-FM: Infrared and Visible Image Fusion Using GAN with Full-Scale Skip Connection and Dual Markovian Discriminators","2021","IEEE Transactions on Computational Imaging","7","","","1134","1147","13","10.1109/TCI.2021.3119954","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117809283&doi=10.1109%2fTCI.2021.3119954&partnerID=40&md5=1a62aa5d50c9b8e1cae753e318570c67","A good result of infrared and visible image fusion should not only maintain significant contrast for distinguishing targets from the backgrounds, but also contain rich scene textures to cater for human visual perception. However, previous fusion methods usually do not fully utilize the information, and hence their fused results sacrifice either the salience of thermal targets or the sharpness of textures. To address this challenge, we propose a novel Generative Adversarial Network with Full-scale skip connection and dual Markovian discriminators (GAN-FM) to fully preserve effective information in infrared and visible images. First, a full-scale skip connected generator is designed to extract and fuse deep features of different scales, which can promote the direct transmission of shallow high-contrast features to the deep level, preserving the thermal radiation targets from the semantic level. As a result, the fused image can maintain significant contrast. Second, we propose two Markovian discriminators to establish adversarial games with the generator, so as to estimate probability distributions of infrared and visible modalities at the same time. Unlike conventional global discriminator, the Markovian discriminators try to distinguish each patch of input images, thus the attention of network is restricted to local regions and the fused results are forced to contain more details. In addition, we propose an effective joint gradient loss to ensure the harmonious coexistence of contrast and texture, which prevents the background texture pollution caused by the edge diffusion of the high-contrast target regions. Extensive qualitative and quantitative experiments demonstrate that our GAN-FM has advantages over the state-of-the-art methods in preserving significant contrast and rich textures. Moreover, we apply the fused image generated by our method to object detection and image segmentation, which can effectively improve the performance.  © 2015 IEEE.","Discriminators; Frequency modulation; Image enhancement; Image fusion; Image segmentation; Object detection; Object recognition; Probability distributions; Semantics; Features extraction; Full-scale skip connection; Fused images; Game; Generator; High contrast; Human visual perception; Infrared and visible image; Marko-vian discriminator; Markovian; Generative adversarial networks","full-scale skip connection; generative adversarial network; Image fusion; infrared; Markovian discriminator","Article","Final","","Scopus","2-s2.0-85117809283"
"Chen Z.; Fang M.; Chai X.; Fu F.; Yuan L.","Chen, Zhuo (57219109305); Fang, Ming (55682195100); Chai, Xu (57219110272); Fu, Feiran (57191573174); Yuan, Lihong (57219112835)","57219109305; 55682195100; 57219110272; 57191573174; 57219112835","U-GAN Model for Infrared and Visible Images Fusion; [红外与可见光图像融合的U-GAN模型]","2020","Xibei Gongye Daxue Xuebao/Journal of Northwestern Polytechnical University","38","4","","904","912","8","10.1051/jnwpu/20203840904","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091312571&doi=10.1051%2fjnwpu%2f20203840904&partnerID=40&md5=58daa04d67cbdfcc60b653849d4e2126","Infrared and visible image fusion is an effective method to solve the lack of single sensor imaging. The purpose is that the fusion images are suitable for human eyes and conducive to the next application and processing. In order to solve the problems of incomplete feature extraction, loss of details, and less samples of common data sets, it is not conducive to training, an end-to-end network architecture for image fusion is proposed. U-net is introduced into image fusion, and the final fusion result is obtained by using the generative adversarial network. Through its special convolution structure, the important feature information is extracted to the maximum extent, and the sample does not need to be cut to avoid the problem of reducing the fusion accuracy, but also to improve the training speed. Then the U-net extracted feature is confronted with the discriminator containing infrared image, and the generator model is obtained. The experimental results show that the present algorithm can obtain the fusion image with clear outline, prominent texture and obvious target. SD, SF, SSIM, AG and other indicators are obviously improved. © 2020 Journal of Northwestern Polytechnical University.","Extraction; Infrared imaging; Network architecture; Textures; Adversarial networks; Convolution structure; Fusion image; Generator modeling; Important features; Infrared and visible image; Single-sensor imaging; Training speed; Image fusion","Generative adversarial network; Image fusion; Infrared image; U-net feature extraction","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85091312571"
"Li J.; Huo H.; Li C.; Wang R.; Sui C.; Liu Z.","Li, Jing (57207844651); Huo, Hongtao (56527849900); Li, Chang (56718731300); Wang, Renhua (57221059573); Sui, Chenhong (56262190500); Liu, Zhao (57213430933)","57207844651; 56527849900; 56718731300; 57221059573; 56262190500; 57213430933","Multigrained Attention Network for Infrared and Visible Image Fusion","2021","IEEE Transactions on Instrumentation and Measurement","70","","9216075","","","","10.1109/TIM.2020.3029360","29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097396835&doi=10.1109%2fTIM.2020.3029360&partnerID=40&md5=774a1bfcde05940693b23a2a7f2019ba","Methods based on generative adversarial network (GAN) have been widely used in infrared and visible images fusion. However, these methods cannot perceive the discriminative parts of an image. Therefore, we introduce a multigrained attention module into encoder-decoder network to fuse infrared and visible images (MgAN-Fuse). The infrared and visible images are encoded by two independent encoder networks due to their diverse modalities. Then, the results of the two encoders are concatenated to calculate the fused result by the decoder. To exploit the features of multiscale layers fully and force the model focus on the discriminative regions, we integrate attention modules into multiscale layers of the encoder to obtain multigrained attention maps, and then, the multigrained attention maps are concatenated with the corresponding multiscale features of the decoder network. Thus, the proposed method can preserve the foreground target information of the infrared image and capture the context information of the visible image. Furthermore, we design an additional feature loss in the training process to preserve the important features of the visible image, and a dual adversarial architecture is employed to help the model capture enough infrared intensity information and visible details simultaneously. The ablation studies illustrate the validity of the multigrained attention network and feature loss function. Extensive experiments on two infrared and visible image data sets demonstrate that the proposed MgAN-Fuse has a better performance than state-of-the-art methods.  © 1963-2012 IEEE.","Decoding; Infrared imaging; Network coding; Adversarial networks; Context information; Important features; Infrared and visible image; Infrared intensity; Multi-scale features; State-of-the-art methods; Target information; Image fusion","Feature loss; generative adversarial network (GAN); image fusion; multigrained attention mechanism","Article","Final","","Scopus","2-s2.0-85097396835"
"Huang J.; Le Z.; Ma Y.; Fan F.; Zhang H.; Yang L.","Huang, Jun (56688687500); Le, Zhuliang (57215469419); Ma, Yong (56438173900); Fan, Fan (35795122500); Zhang, Hao (57215014270); Yang, Lei (57155175900)","56688687500; 57215469419; 56438173900; 35795122500; 57215014270; 57155175900","MGMDcGAN: Medical image fusion using multi-generator multi-discriminator conditional generative adversarial network","2020","IEEE Access","8","","9042283","55145","55157","12","10.1109/ACCESS.2020.2982016","25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082655336&doi=10.1109%2fACCESS.2020.2982016&partnerID=40&md5=f1b51ae47dadfe5cf353272325736905","In this paper, we propose a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS , and images characterizing functional information, i.e., IF, of different resolutions, by using a multi-generator multi-discriminator conditional generative adversarial network (MGMDcGAN). In the first cGAN, the generator aims to generate a real-like fused image based on a specifically designed content loss to fool two discriminators, while the discriminators aim to distinguish the structure differences between the fused image and source images. On this basis, we employ the second cGAN with a mask to enhance the information of dense structure in the final fused image, while preventing the functional information from being weakened. Consequently, the final fused image is forced to concurrently keep the structural information in IS and the functional information in IF. In addition, as a unified method, MGMDcGAN can be applied to different kinds of medical image fusion, i.e., MRI-PET, MRI-SPECT, and CT-SPECT, where MRI and CT are two kinds of IS of high resolution, PET and SPECT are typical kinds of IF of low resolution. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our MGMDcGAN over the state-of-the-art. © 2013 IEEE.","Computerized tomography; Discriminators; Image enhancement; Magnetic resonance imaging; Medical imaging; Adversarial networks; Different resolutions; End to end; Functional information; Quantitative experiments; Structural information; Structure difference; Unified method; Image fusion","different resolutions; end-to-end; generative adversarial network; Medical image fusion; unified method","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85082655336"
"","","","2nd Chinese Conference on Computer Vision, CCCV 2017","2017","Communications in Computer and Information Science","771","","","","","1377","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037854791&partnerID=40&md5=18e5108e37cb9ea3346e936251542663","The proceedings contain 113 papers. The special focus in this conference is on Computer Vision. The topics include: Visual saliency fusion based multi-feature for semantic image retrieval; unsupervised multi-view subspace learning via maximizing dependence; structured multi-view supervised feature selection algorithm research; an automatic shoeprint retrieval method using neural codes for commercial shoeprint scanners; uncovering the effect of visual saliency on image retrieval; shape-color differential moment invariants under affine transforms; a novel layer based image fusion approach via transfer learning and coupled dictionary; local saliency extraction for fusion of visible and infrared images; distributed compressive sensing for light field reconstruction using structured random matrix; improved face verification with simple weighted feature combination; stereoscopic image quality assessment based on binocular adding and subtracting; quality assessment of palm vein image using natural scene statistics; an error-activation-guided blind metric for stitched panoramic image quality assessment; image aesthetic quality evaluation using convolution neural network embedded fine-tune; high capacity reversible data hiding with contrast enhancement; rank learning for dehazed image quality assessment; a low-rank total-variation regularized tensor completion algorithm; nighttime haze removal with fusion atmospheric light and improved entropy; light field super-resolution using cross-resolution input based on patchmatch and learning method; a new image sparse reconstruction method for mixed Gaussian-poisson noise with multiple constraints; pore-scale facial features matching under 3D morphable model constraint; face video super-resolution with identity guided generative adversarial networks; exemplar-based pixel by pixel inpainting based on patch shift; GAN based sample simulation for SEM-image super resolution; PSO-based single image defogging.","","","Conference review","Final","","Scopus","2-s2.0-85037854791"
"Le Z.; Huang J.; Fan F.; Tian X.; Ma J.","Le, Zhuliang (57215469419); Huang, Jun (56688687500); Fan, Fan (35795122500); Tian, Xin (55458978600); Ma, Jiayi (26638975600)","57215469419; 56688687500; 35795122500; 55458978600; 26638975600","A Generative Adversarial Network for Medical Image Fusion","2020","Proceedings - International Conference on Image Processing, ICIP","2020-October","","9191089","370","374","4","10.1109/ICIP40778.2020.9191089","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098644627&doi=10.1109%2fICIP40778.2020.9191089&partnerID=40&md5=0cddf92c47846fa59b64eabfcd57770c","In this paper, a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS, and images characterizing functional information, i.e., IF, of different resolutions is proposed, which is achieved by using a conditional generative adversarial network with multiple generators and multiple discriminators (MGMDcGAN). In the first cGAN, a real-like fused image is generated by a generator, simultaneously fooling two discriminators. While the discriminators are to distinguish the fused image from source images. Besides, to prevent the functional information from being weakened in the final fused image when enhancing the dense structure information, we employ the second cGAN with a mask calculated. Meanwhile, the structural information in IS and the functional information in IF the final fused image can be concurrently kept. Furthermore, our MGMDcGAN is a unified method, which is applicable to different kinds of medical image fusion, including MRI-PET, MRISPECT, and CT-SPECT. Extensive experiments on publicly available datasets substantiate the superiority of our MGMDcGAN over the current state-of-the-art. © 2020 IEEE.","Computerized tomography; Image enhancement; Magnetic resonance imaging; Medical imaging; Adversarial networks; Dense structures; Different resolutions; End-to-end models; Functional information; State of the art; Structural information; Unified method; Image fusion","different resolutions; end-to-end; GAN; Medical image fusion; unified method","Conference paper","Final","","Scopus","2-s2.0-85098644627"
"Hong X.; Kintak U.","Hong, Xinxin (57214153360); Kintak, U. (54973732900)","57214153360; 54973732900","Multi-Focus Image Fusion Algorithm Based on Non-Uniform Rectangular Partition and Generative Adversarial Network","2019","International Conference on Wavelet Analysis and Pattern Recognition","2019-July","","8946467","","","","10.1109/ICWAPR48189.2019.8946467","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078361426&doi=10.1109%2fICWAPR48189.2019.8946467&partnerID=40&md5=72bd0e47cb23cd36bc1303f87ee7b094","Based on Non-uniform Rectangular Partition (NURP) and Generative Adversarial Network (GAN), this paper proposes an effective multi-focus image fusion method to generate a full-focus image by combining multi-focus images. Firstly, NURP is applied to left-focus and right-focus images, the size of partitioning grids obtained can be used to judge the fusion pixel to form a rough Fusion Guiding Map (FGM) which will be further optimized by morphological operation and manual adjustment to form an optimized FGM. Then the rough FGM and optimized FGM become the training dataset for the pix2pix GAN. After finishing the training, the trained pix2pix model can be used to optimize any rough FGM from NURP. Finally, the fused pixels are determined according to the FGM to construct the final fused image. The experimental results show that the algorithm improves the visual clarity of the fused image by enhancing the spatial detail of the image and obtains better objective evaluation indicators. © 2019 IEEE.","Image enhancement; Mathematical morphology; Pattern recognition; Pixels; Wavelet analysis; Adversarial networks; Morphological operations; Multifocus image fusion; Multifocus images; Objective evaluation; Rectangular partition; Training dataset; Visual clarities; Image fusion","Generative adversarial network; Multi-Focus Image fusion; Non-uniform rectangular partition","Conference paper","Final","","Scopus","2-s2.0-85078361426"
"Zhang D.; Zhou Y.; Zhao J.; Zhou Z.; Yao R.","Zhang, Di (57219908378); Zhou, Yong (35480110700); Zhao, Jiaqi (57138970300); Zhou, Ziyuan (57219904145); Yao, Rui (23567304500)","57219908378; 35480110700; 57138970300; 57219904145; 23567304500","Structural similarity preserving GAN for infrared and visible image fusion","2021","International Journal of Wavelets, Multiresolution and Information Processing","19","1","2050063","","","","10.1142/S0219691320500630","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095988911&doi=10.1142%2fS0219691320500630&partnerID=40&md5=06fa53c486ac16725df7fb1bfa02932b","Compared with a single image, in a complex environment, image fusion can utilize the complementary information provided by multiple sensors to significantly improve the image clarity and the information, more accurate, reliable, comprehensive access to target and scene information. It is widely used in military and civil fields, such as remote sensing, medicine, security and other fields. In this paper, we propose an end-to-end fusion framework based on structural similarity preserving GAN (SSP-GAN) to learn a mapping of the fusion tasks for visible and infrared images. Specifically, on the one hand, for making the fusion image natural and conforming to visual habits, structure similarity is introduced to guide the generator network produce abundant texture structure information. On the other hand, to fully take advantage of shallow detail information and deep semantic information for achieving feature reuse, we redesign the network architecture of multi-modal image fusion meticulously. Finally, a wide range of experiments on real infrared and visible TNO dataset and RoadScene dataset prove the superior performance of the proposed approach in terms of accuracy and visual. In particular, compared with the best results of other seven algorithms, our model has improved entropy, edge information transfer factor, multi-scale structural similarity and other evaluation metrics, respectively, by 3.05%, 2.4% and 0.7% on TNO dataset. And our model has also improved by 0.7%, 2.82% and 1.1% on RoadScene dataset.  © 2021 World Scientific Publishing Company.","Image enhancement; Infrared imaging; Network architecture; Remote sensing; Semantics; Textures; Complex environments; Evaluation metrics; Improved entropies; Infrared and visible image; Semantic information; Structural similarity; Structure similarity; Texture structure; Image fusion","generative adversarial network; Image fusion; infrared images; structural similarity; visible images","Article","Final","","Scopus","2-s2.0-85095988911"
"Yang Z.; Chen Y.; Le Z.; Fan F.; Pan E.","Yang, Zhiguang (57212527694); Chen, Youping (9638564800); Le, Zhuliang (57215469419); Fan, Fan (35795122500); Pan, Erting (57192236085)","57212527694; 9638564800; 57215469419; 35795122500; 57192236085","Multi-Source Medical Image Fusion Based on Wasserstein Generative Adversarial Networks","2019","IEEE Access","7","","8911320","175947","175958","11","10.1109/ACCESS.2019.2955382","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076969122&doi=10.1109%2fACCESS.2019.2955382&partnerID=40&md5=98982494bfe22d3eb9d0075f69e7305d","In this paper, we propose the medical Wasserstein generative adversarial networks (MWGAN), an end-to-end model, for fusing magnetic resonance imaging (MRI) and positron emission tomography (PET) medical images. Our method establishes two adversarial games between a generator and two discriminators to generate a fused image with the details of soft tissue structures in organs from MRI images and the functional and metabolic information from PET images. Different information from source images can be effectively adjusted with a specifically designed loss function. In addition, we use WGAN instead of the traditional generative adversarial networks to make the training process more stable and allow our architecture to deal with source images of different resolutions. Qualitative and quantitative comparisons on publicly available datasets demonstrate the superiority of MWGAN over the state-of-the-art networks. Furthermore, our MWGAN is applied to the fusion of MRI and computed tomography images of different resolutions, achieving a satisfactory performance. © 2013 IEEE.","Computerized tomography; Magnetic resonance imaging; Medical imaging; Positron emission tomography; Adversarial networks; Computed tomography images; Different resolutions; End to end; End-to-end models; Metabolic information; Positron emission tomography (PET); Quantitative comparison; Image fusion","different resolutions; end-to-end; Medical image fusion; Wasserstein generative adversarial networks","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85076969122"
"","","","18th International Conference on Artificial Intelligence and Soft Computing, ICAISC 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11508 LNAI","","","","","1391","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066741016&partnerID=40&md5=66070253a2eec9a3bb6dd09d845ac7c2","The proceedings contain 122 papers. The special focus in this conference is on Artificial Intelligence and Soft Computing. The topics include: Robust Training of Radial Basis Function Neural Networks; Sequential Data Mining of Network Traffic in URL Logs; On Learning and Convergence of RBF Networks in Regression Estimation and Classification; Application of Deep Neural Networks to Music Composition Based on MIDI Datasets and Graphical Representation; dense Multi-focus Fusion Net: A Deep Unsupervised Convolutional Network for Multi-focus Image Fusion; microscopic Sample Segmentation by Fully Convolutional Network for Parasite Detection; application of Spiking Neural Networks to Fashion Classification; text Language Identification Using Attention-Based Recurrent Neural Networks; filter Pruning for Efficient Transfer Learning in Deep Convolutional Neural Networks; modifications of the Givens Training Algorithm for Artificial Neural Networks; Regularized Learning of Neural Network with Application to Sparse PCA; trimmed Robust Loss Function for Training Deep Neural Networks with Label Noise; on Proper Designing of Deep Structures for Image Classification; constructive Cascade Learning Algorithm for Fully Connected Networks; generative Adversarial Networks: Recent Developments; fuzzy Modeling in the Task of Control Cartographic Visualization; method for Generalization of Fuzzy Sets; the 2-Additive Choquet Integral of Bi-capacities; a New Class of Uninorm Aggregation Operations for Fuzzy Theory; different Forms of Generalized Hypothetical Syllogism with Regard to R-Implications; Deep Neural Networks Applied to the Dynamic Helper System in a GPGPU; a Fuzzy-Dynamic Bayesian Network Approach for Inference Filtering; the Estimation of Uncertain Gates: An Application to Educational Indicators; application of Type-2 Fuzzy Sets for Analyzing Production Processes.","","","Conference review","Final","","Scopus","2-s2.0-85066741016"
"Zhang H.; Riggan B.S.; Hu S.; Short N.J.; Patel V.M.","Zhang, He (57191984770); Riggan, Benjamin S. (56406510300); Hu, Shuowen (34881794700); Short, Nathaniel J. (56940280300); Patel, Vishal M. (56660008900)","57191984770; 56406510300; 34881794700; 56940280300; 56660008900","Synthesis of High-Quality Visible Faces from Polarimetric Thermal Faces using Generative Adversarial Networks","2019","International Journal of Computer Vision","127","6-7","","845","862","17","10.1007/s11263-019-01175-3","56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071729955&doi=10.1007%2fs11263-019-01175-3&partnerID=40&md5=37ab500a47f4810945743e18d3317f3d","The large domain discrepancy between faces captured in polarimetric (or conventional) thermal and visible domains makes cross-domain face verification a highly challenging problem for human examiners as well as computer vision algorithms. Previous approaches utilize either a two-step procedure (visible feature estimation and visible image reconstruction) or an input-level fusion technique, where different Stokes images are concatenated and used as a multi-channel input to synthesize the visible image given the corresponding polarimetric signatures. Although these methods have yielded improvements, we argue that input-level fusion alone may not be sufficient to realize the full potential of the available Stokes images. We propose a generative adversarial networks based multi-stream feature-level fusion technique to synthesize high-quality visible images from polarimetric thermal images. The proposed network consists of a generator sub-network, constructed using an encoder–decoder network based on dense residual blocks, and a multi-scale discriminator sub-network. The generator network is trained by optimizing an adversarial loss in addition to a perceptual loss and an identity preserving loss to enable photo realistic generation of visible images while preserving discriminative characteristics. An extended dataset consisting of polarimetric thermal facial signatures of 111 subjects is also introduced. Multiple experiments evaluated on different experimental protocols demonstrate that the proposed method achieves state-of-the-art performance. Code will be made available at https://github.com/hezhangsprinter. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.","Deep learning; Image enhancement; Image fusion; Image reconstruction; Polarimeters; Adversarial networks; Computer vision algorithms; Experimental protocols; Face synthesis; Feature estimation; Feature level fusion; Polarimetric data; State-of-the-art performance; Face recognition","Deep learning; Face synthesis; Generative adversarial networks; Heterogeneous face recognition; Polarimetric data; Thermal face recognition","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85071729955"
"","","","15th Conference on Image and Graphics Technology and Applications, IGTA 2020","2021","Communications in Computer and Information Science","1314 CCIS","","","","","324","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107336660&partnerID=40&md5=732561f30fcff4a8e043e84759d04152","The proceedings contain 24 papers. The special focus in this conference is on Image and Graphics Technology and Applications. The topics include: Infrared Small Target Recognition with Improved Particle Filtering Based on Feature Fusion; single Image Super-Resolution Based on Generative Adversarial Networks; Simplifying Sketches with Conditional GAN; Improved Method of Target Tracking Based on SiamRPN; An Improved Target Tracking Method Based on DIMP; target Recognition Framework and Learning Mode Based on Parallel Images; view Consistent 3D Face Reconstruction Using Siamese Encoder-Decoders; an Angle-Based Smoothing Method for Triangular and Tetrahedral Meshes; AUIF: An Adaptive User Interface Framework for Multiple Devices; A Striping Removal Method Based on Spectral Correlation in MODIS Data; deep Attention Network for Remote Sensing Scene Classification; thin Cloud Removal Using Cirrus Spectral Property for Remote Sensing Images; accurate Estimation of Motion Blur Kernel Based on Genetic Algorithms; graph Embedding Discriminant Analysis and Semi-Supervised Extension for Face Recognition; 3D Human Body Reconstruction from a Single Image; abnormal Crowd Behavior Detection Based on Movement Trajectory; image Recognition Method of Defective Button Battery Base on Improved MobileNetV1; preface; control and on-Board Calibration Method for in-Situ Detection Using the Visible and Near-Infrared Imaging Spectrometer on the Yutu-2 Rover; full Convolutional Color Constancy with Attention; fast and Accurate Face Alignment Algorithm Based on Deep Knowledge Distillation; multi-modal 3-D Medical Image Fusion Based on Tensor Robust Principal Component Analysis.","","","Conference review","Final","","Scopus","2-s2.0-85107336660"
"","","","1st International Workshop on Smart Ultrasound Imaging, SUSI 2019, and the 4th International Workshop on Preterm, Perinatal and Paediatric Image Analysis, PIPPI 2019, held in conjunction with the 22nd International Conference on Medical Imaging and Computer-Assisted Intervention, MICCAI 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11798 LNCS","","","","","188","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075723269&partnerID=40&md5=13c4df1b05891d6e07c3c51ef063deb8","The proceedings contain 20 papers. The special focus in this conference is on International Workshop on Smart Ultrasound Imaging, and the International Workshop on Preterm, Perinatal and Paediatric Image Analysis, held in conjunction with the International Conference on Medical Imaging and Computer-Assisted Intervention. The topics include: Deep learning based minimum variance beamforming for ultrasound imaging; estimation of preterm birth markers with u-net segmentation network; investigating image registration impact on preterm birth classification: An interpretable deep learning approach; dual network generative adversarial networks for pediatric echocardiography segmentation; reproducibility of functional connectivity estimates in motion corrected fetal fmri; plug-and-play priors for reconstruction-based placental image registration; a longitudinal study of the evolution of the central sulcus’ shape in preterm infants using manifold learning; prediction of failure of induction of labor from ultrasound images using radiomic features; longitudinal analysis of fetal mri in patients with prenatal spina bifida repair; quantifying residual motion artifacts in fetal fmri data; registration of untracked 2d laparoscopic ultrasound liver images to ct using content-based retrieval and kinematic priors; topology-preserving augmentation for cnn-based segmentation of congenital heart defects from 3d paediatric cmr; direct detection and measurement of nuchal translucency with neural networks from ultrasound images; automated left ventricle dimension measurement in 2d cardiac ultrasound via an anatomically meaningful cnn approach; SPRNet: Automatic fetal standard plane recognition network for ultrasound images; representation disentanglement for multi-task learning with application to fetal ultrasound; adversarial learning for deformable image registration: Application to 3d ultrasound image fusion; deep learning-based pneumothorax detection in ultrasound videos.","","","Conference review","Final","","Scopus","2-s2.0-85075723269"
"Zhang Y.; Li X.; Zhou J.","Zhang, Yutian (57210126022); Li, Xiaohua (55718203400); Zhou, Jiliu (21234416400)","57210126022; 55718203400; 21234416400","SFTGAN: A generative adversarial network for pan-sharpening equipped with spatial feature transform layers","2019","Journal of Applied Remote Sensing","13","2","026507","","","","10.1117/1.JRS.13.026507","10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069457255&doi=10.1117%2f1.JRS.13.026507&partnerID=40&md5=81b55330e192044dfa7b6f388c1ef808","Pan-sharpening is an indispensable technology for remote sensing that aims to combine low-resolution multispectral images and high-resolution panchromatic images to create a multispectral image with high resolution. However, pan-sharpening approaches often encounter spectral distortion and detail distortion issues. In order to overcome the drawbacks of pan-sharpening methodologies, we propose an end-to-end pan-sharpening model consisting of an effective generative adversarial network architecture equipped with spatial feature transform layers that generate spatial detail features under spectral feature constraints. Through a large number of quantitative and visual assessments, we demonstrate that the proposed method achieves superior performance to other state-of-the-art methods. © 2019 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Image fusion; Network architecture; Adversarial networks; Low resolution multispectral images; Multispectral images; Pan-sharpening; Panchromatic images; Spatial features; Spectral distortions; State-of-the-art methods; Remote sensing","deep learning; multispectral image; pan-sharpening; remote image fusion; spatial feature transform","Article","Final","","Scopus","2-s2.0-85069457255"
"Ma J.; Yu W.; Chen C.; Liang P.; Guo X.; Jiang J.","Ma, Jiayi (26638975600); Yu, Wei (56479633000); Chen, Chen (57192217138); Liang, Pengwei (57201500677); Guo, Xiaojie (36607970100); Jiang, Junjun (54902306100)","26638975600; 56479633000; 57192217138; 57201500677; 36607970100; 54902306100","Pan-GAN: An unsupervised pan-sharpening method for remote sensing image fusion","2020","Information Fusion","62","","","110","120","10","10.1016/j.inffus.2020.04.006","183","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084634225&doi=10.1016%2fj.inffus.2020.04.006&partnerID=40&md5=dedd4ce965c307d547a20af2658b9785","Pan-sharpening in remote sensing image fusion refers to obtaining multi-spectral images of high-resolution by fusing panchromatic images and multi-spectral images of low-resolution. Recently, convolution neural network (CNN)-based pan-sharpening methods have achieved the state-of-the-art performance. Even though, two problems still remain. On the one hand, the existing CNN-based strategies require supervision, where the low-resolution multi-spectral image is obtained by simply blurring and down-sampling the high-resolution one. On the other hand, they typically ignore rich spatial information of panchromatic images. To address these issues, we propose a novel unsupervised framework for pan-sharpening based on a generative adversarial network, termed as Pan-GAN, which does not rely on the so-called ground-truth during network training. In our method, the generator separately establishes the adversarial games with the spectral discriminator and the spatial discriminator, so as to preserve the rich spectral information of multi-spectral images and the spatial information of panchromatic images. Extensive experiments are conducted to demonstrate the effectiveness of the proposed Pan-GAN compared with other state-of-the-art pan-sharpening approaches. Our Pan-GAN has shown promising performance in terms of qualitative visual effects and quantitative evaluation metrics. © 2020 Elsevier B.V.","Remote sensing; Spectroscopy; Adversarial networks; Convolution neural network; Multispectral images; Quantitative evaluation; Remote sensing images; Spatial informations; Spectral information; State-of-the-art performance; Image fusion","Deep learning; Generative adversarial network; Image fusion; Pan-sharpening; Unsupervised learning","Article","Final","","Scopus","2-s2.0-85084634225"
"Park J.; Han D.K.; Ko H.","Park, Jaihyun (57189090323); Han, David K. (7403219841); Ko, Hanseok (35069749800)","57189090323; 7403219841; 35069749800","Fusion of Heterogeneous Adversarial Networks for Single Image Dehazing","2020","IEEE Transactions on Image Processing","29","","9018375","4721","4732","11","10.1109/TIP.2020.2975986","47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081957368&doi=10.1109%2fTIP.2020.2975986&partnerID=40&md5=8a81a2c8c41df6e9951d054096213e4a","In this paper, we propose a novel image dehazing method. Typical deep learning models for dehazing are trained on paired synthetic indoor dataset. Therefore, these models may be effective for indoor image dehazing but less so for outdoor images. We propose a heterogeneous Generative Adversarial Networks (GAN) based method composed of a cycle-consistent Generative Adversarial Networks (CycleGAN) for producing haze-clear images and a conditional Generative Adversarial Networks (cGAN) for preserving textural details. We introduce a novel loss function in the training of the fused network to minimize GAN generated artifacts, to recover fine details, and to preserve color components. These networks are fused via a convolutional neural network (CNN) to generate dehazed image. Extensive experiments demonstrate that the proposed method significantly outperforms the state-of-the-art methods on both synthetic and real-world hazy images. © 1992-2012 IEEE.","Convolutional neural networks; Deep learning; Image fusion; Adversarial networks; Color component; Dehazing; Fusion methods; Learning models; Outdoor images; Single image dehazing; State-of-the-art methods; Demulsification","fusion method; generative adversarial networks; Image dehazing","Article","Final","","Scopus","2-s2.0-85081957368"
"Chen L.; Han J.; Tian F.","Chen, Lei (57273656000); Han, Jun (55753463900); Tian, Feng (57669528000)","57273656000; 55753463900; 57669528000","Colorization of fusion image of infrared and visible images based on parallel generative adversarial network approach","2021","Journal of Intelligent and Fuzzy Systems","41","1","","2255","2264","9","10.3233/JIFS-210987","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113201264&doi=10.3233%2fJIFS-210987&partnerID=40&md5=ebc1116333ef8be9515140c0df123c21","Fusing the infrared (IR) and visible images has many advantages and can be applied to applications such as target detection and recognition. Colors can give more accurate and distinct features, but the low resolution and low contrast of fused images make this a challenge task. In this paper, we proposed a method based on parallel generative adversarial networks (GANs) to address the challenge. We used IR image, visible image and fusion image as ground truth of 'L', 'a' and 'b' of the Lab model. Through the parallel GANs, we can gain the Lab data which can be converted to RGB image. We adopt TNO and RoadScene data sets to verify our method, and compare with five objective evaluation parameters obtained by other three methods based on deep learning (DL). It is demonstrated that the proposed approach is able to achieve better performance against state-of-arts methods. © 2021 - IOS Press. All rights reserved.","Deep learning; Infrared imaging; Adversarial networks; Fused images; Infrared and visible image; Low contrast; Low resolution; Objective evaluation; Target detection and recognition; Visible image; Image fusion","generative adversarial network; image fusion; IR and visible images; lab","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85113201264"
"Song Y.; Zhang H.; Zhang L.","Song, Yiyao (57221595735); Zhang, Hongyan (54954032600); Zhang, Liangpei (8359720900)","57221595735; 54954032600; 8359720900","Remote Sensing Image Spatio-Temporal Fusion via a Generative Adversarial Network Through One Prior Image Pair","2020","International Geoscience and Remote Sensing Symposium (IGARSS)","","","9324101","7009","7012","3","10.1109/IGARSS39084.2020.9324101","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101970280&doi=10.1109%2fIGARSS39084.2020.9324101&partnerID=40&md5=b466b9de0709752c257e83ee82bc3b54","Spatio-temporal fusion is a promising way to deal with the tradeoff between the temporal resolution and spatial resolution of the remote sensing images. This paper presents a novel remote sensing image spatio-temporal fusion model to expand the application of spatio-temporal fusion with insufficient data, based on a generative adversarial network to handle one prior image pair cases (OPGAN). Considering the huge spatial resolution gap between the high-spatial, low-temporal (HSLT) resolution Landsat imagery and the corresponding low-spatial, high-temporal (LSHT) resolution MODIS imagery, the proposed OPGAN simultaneously trains a generator and a discriminator in a min-max game to reconstruct the high-spatial-high-temporal (HSHT) resolution Landsat images, significantly improving the accuracy of change prediction with the help of the temporal changes and sensor differences. Experimental results on three representative Landsat-MODIS datasets illustrate the effectiveness of the proposed OPGAN method. © 2020 IEEE.","Geology; Image enhancement; Image fusion; Image resolution; Radiometers; Adversarial networks; Change prediction; Landsat imagery; Remote sensing images; Spatial resolution; Spatio-temporal fusions; Temporal change; Temporal resolution; Remote sensing","generative adversarial network; one prior image pair; Remote sensing; spatio-temporal fusion","Conference paper","Final","","Scopus","2-s2.0-85101970280"
"Shao H.; Wang Y.","Shao, Hang (57215205168); Wang, Yongxiong (56048944100)","57215205168; 56048944100","Generative High-Resolution Image Inpainting with Parallel Adversarial Network and Multi-condition Fusion; [基于并行对抗与多条件融合的生成式高分辨率图像修复]","2020","Moshi Shibie yu Rengong Zhineng/Pattern Recognition and Artificial Intelligence","33","4","","363","374","11","10.16451/j.cnki.issn1003-6059.202004009","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085626319&doi=10.16451%2fj.cnki.issn1003-6059.202004009&partnerID=40&md5=988c683d916eb266a5dff3800d25f909","Regions with artifacts and semantic inaccuracy are often caused by existing image inpainting algorithms. Moreover, the inpainting effect is limited for images with large missing regions and high-resolution. Therefore, a two-stage image inpainting approach based on parallel adversarial network and multi-condition fusion is proposed in this paper. Firstly, an improved deep residual network is utilized to fill the corrupted image. The first-stage adversarial network is employed to complete the image edge map. Next, the color feature of the filled image is extracted and fused with the completed edge image. Then, the fused image is applied as the condition label of the second-stage adversarial network. Finally, the inpainting result is obtained by the second-stage network with a contextual attention module. Experiments on multiple public datasets demonstrate that realistic inpainting results can be obtained by the proposed approach. © 2020, Science Press. All right reserved.","Image enhancement; Semantics; Adversarial networks; Color features; Corrupted images; Fused images; High resolution; High resolution image; Image edge; Image Inpainting; Image fusion","Contextual Attention Mechanism; Deep Learning; Generative Adversarial Network; Image Inpainting; Multi-condition Fusion","Article","Final","","Scopus","2-s2.0-85085626319"
"Zhang G.; Hu Q.; Gong L.","Zhang, Guimei (55738871900); Hu, Qiang (57215611157); Gong, Lei (57219566185)","55738871900; 57215611157; 57219566185","Non-rigid medical image registration based on residual-in-residual dense block and GAN; [融合密集残差块和GAN变体的医学图像非刚性配准]","2020","Journal of Image and Graphics","25","10","","2182","2194","12","10.11834/jig.200130","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093958868&doi=10.11834%2fjig.200130&partnerID=40&md5=fec1077e6094551c8a4021eba3603ac9","Objective: Medical image registration is an important research task in the field of medical image analysis, which is the basis of medical image fusion and medical image reconstruction. Conventional registration methods that build loss function based on normalized mutual information by using iterative gradient descent to achieve registration tend to be time consuming. The existing deep learning-based medical image registration methods have limitation in registering medical images with large non-rigid deformation, which cannot achieve high registration accuracy and have poor generalization ability. Thus, this paper proposes a method to register multi-modal medical images by combining residual-in-residual dense block (RRDB) with generative adversarial network (GAN). Method: First, RRDB are introduced to the standard generator network to extract high-level feature information from unpaired image pairs; thus, registration accuracy is improved. Then, a least-squares loss is used to substitute cross-entropy loss constructed by the logistic regression objective. The convergence condition of least-squares loss is strict and promotes the model to reach convergence at the optimal parameters, which can alleviate gradient disappearance and overfitting; thus, the robustness of model training is enhanced. In addition, relative average GAN (RaGAN) is embedded into the standard discriminator network, namely, adding a gradient penalty in the discriminator network, which reduce the error of discriminator to estimate the relative true and false probability between the fixed image and moving image. The enhanced discriminator can help the generator to learn clearer and texture information; therefore, the registration error can be decreased, and the registration accuracy can be stabilized. Result: This registration model is trained and validated on DRIVE(digital retinal images for vessel extraction) dataset. Generalization performance tests are performed on Sunybrook Cardiac Dataset and Brain MRI Dataset. Compared with state-of-the-art registration methods, extensive experiments demonstrate that the proposed model achieves good registration results; both registration accuracy and generalization ability have been improved. Compared with the basic literature, the registration Dice values of retinal images, cardiac images, and brain images are improved by 3.3%, 3.0%, and 1.5%, respectively. According to the stability verification experiment of the registration model, as the number of iterations augments, the Dice value of this paper gradually increases, and the change is more stable than that of baseline literature. The number of iterations in this paper is 80 000, whereas that of baseline literature is 10 000. This verification experiment shows that this registration model has been effectively improved in the training phase; not only the convergence speed is accelerated but also stability is higher compared with that of baseline literature. Conclusion: The proposed registration method can obtain high-level feature information; thus, the registration accuracy is improved. Simultaneously, the loss function is built on the basis of the least-squares method, and the discriminator is also strengthened, which can enable the registration model to quickly converge during the training phase and then improve model stability and generalization ability. This method is suitable for medical image registration with large non-rigid deformation. © 2020, Editorial and Publishing Board of Journal of Image and Graphics. All right reserved.","","Generative adversarial network(GAN); Least square loss; Medical image; Non-rigid registration; Relative average GAN(RaGAN); Residual-in-residual dense block(RRDB)","Article","Final","","Scopus","2-s2.0-85093958868"
"Ren J.; Wang J.; Li H.; Sun Z.; Luan Z.; Yu Z.; Xu H.; Hua Q.","Ren, Jinwen (57221815068); Wang, Jingjing (57214140268); Li, Hongzhen (57382184100); Sun, Zengzhao (57370709600); Luan, Zhenye (57224862293); Yu, Zishu (57224865860); Xu, Huaqiang (56025859100); Hua, Qing (55659556900)","57221815068; 57214140268; 57382184100; 57370709600; 57224862293; 57224865860; 56025859100; 55659556900","DDGANA: Dual-discriminator GAN with attention module for infrared and visible image fusion","2021","Proceedings of SPIE - The International Society for Optical Engineering","12076","","120760E","","","","10.1117/12.2611730","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121483347&doi=10.1117%2f12.2611730&partnerID=40&md5=1660b7ecc2c7592edab4d3feb083b285","Infrared images can be differentiated between target and background according to the distinction in thermal radiation, which generally works under any lighting condition. In comparison, visible images available in abundant detail, consistent as human visual system. In this study, a novel approach for the fusion of infrared and visible images is proposed built on a dual-discriminator GAN with attention module (DDGANA). Our approach establishes confrontation training between one generator and two discriminators. The goal of the generator is to output result with contrast information and details. The two discriminants aim to increase the similarity between the images generated by the generator and the infrared and visible images. After sequential adversarial training, DDGANA outputs images that have preserved the high contrast and the abundant texture detail. Experiments on the TNO dataset prove that our approach obtains an improved performance over the other approaches.  © 2021 SPIE.","Discriminators; Image fusion; Infrared imaging; Textures; Attention module; Fusion image; High contrast; Human Visual System; Infrared and visible image; Lighting conditions; Performance; Target and background; Visible image; Generative adversarial networks","Attention module; Fusion image; Generative adversarial networks; Infrared image; Visible image","Conference paper","Final","","Scopus","2-s2.0-85121483347"
"Zhang L.; Li W.; Zhang C.; Lei D.","Zhang, Liping (57200044819); Li, Weisheng (36067507500); Zhang, Ce (57216935063); Lei, Dajiang (36627356400)","57200044819; 36067507500; 57216935063; 36627356400","A generative adversarial network with structural enhancement and spectral supplement for pan-sharpening","2020","Neural Computing and Applications","32","24","","18347","18359","12","10.1007/s00521-020-04973-w","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085396047&doi=10.1007%2fs00521-020-04973-w&partnerID=40&md5=ac288a7b77ea458da25f625788cbb545","Pan-sharpening aims to obtain high-resolution multi-spectral images by fusing panchromatic images and low-resolution multi-spectral images though reasonable rules. This paper proposed a novel generative adversarial network for pan-sharpening, which utilizes the supplemented spectral information from low-resolution multi-spectral images and the enhanced structural information from panchromatic images to generate high-resolution multi-spectral images. Firstly, the forward differential operator is used to extract the spatial structural information of the panchromatic image both in the horizontal and vertical directions. Secondly, an architecture of generative adversarial network is designed. The enhanced structural information generated by the accumulation of the structural information of the two directions is added to the image fusion process in generator and the discriminating process in discriminator, and a new optimization objective is designed accordingly. What is more, the low-resolution multi-spectral image is added to the convolution process in the generator as a supplement to the spectral information. Finally, in order to obtain better image generation effect, a special objective function of the generator is designed, which adds a unique relationship to reduce the loss of spatial structural information and spectral information of fused images. Experiments on QuickBird and WorldView-3 satellites datasets show that the proposed method can generate high quality fused images and is better than most advanced methods in both objective indicators and intuitive observations. © 2020, Springer-Verlag London Ltd., part of Springer Nature.","Image fusion; Mathematical operators; Spectroscopy; Adversarial networks; Differential operators; Multispectral images; Objective functions; Spatial structural information; Spectral information; Structural enhancements; Structural information; Image enhancement","Enhanced structural information; Generative adversarial network; Pan-sharpening; Spectral supplement","Article","Final","","Scopus","2-s2.0-85085396047"
"Zhang D.; Hou J.; Wu W.; Lu T.; Zhou H.","Zhang, Dazhi (57226173525); Hou, Jilei (57222092435); Wu, Wei (57222093753); Lu, Tao (56406646300); Zhou, Huabing (55447554500)","57226173525; 57222092435; 57222093753; 56406646300; 55447554500","A Generative Adversarial Network with Dual Discriminators for Infrared and Visible Image Fusion Based on Saliency Detection","2021","Mathematical Problems in Engineering","2021","","4209963","","","","10.1155/2021/4209963","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120885520&doi=10.1155%2f2021%2f4209963&partnerID=40&md5=0f24d7ca968be9f4db991ee0ed27a665","Infrared and visible image fusion needs to preserve both the salient target of the infrared image and the texture details of the visible image. Therefore, an infrared and visible image fusion method based on saliency detection is proposed. Firstly, the saliency map of the infrared image is obtained by saliency detection. Then, the specific loss function and network architecture are designed based on the saliency map to improve the performance of the fusion algorithm. Specifically, the saliency map is normalized to [0, 1], used as a weight map to constrain the loss function. At the same time, the saliency map is binarized to extract salient regions and nonsalient regions. And, a generative adversarial network with dual discriminators is obtained. The two discriminators are used to distinguish the salient regions and the nonsalient regions, respectively, to promote the generator to generate better fusion results. The experimental results show that the fusion results of our method are better than those of the existing methods in both subjective and objective aspects.  © 2021 Dazhi Zhang et al.","Generative adversarial networks; Image fusion; Image segmentation; Infrared imaging; Network architecture; Textures; Fusion algorithms; Image fusion methods; Infrared and visible image; Loss networks; Performance; Saliency detection; Saliency map; Salient regions; Specific loss function; Visible image; Discriminators","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85120885520"
"Wang J.; Li Y.; Miao Z.","Wang, Jixiao (57210435093); Li, Yang (57213272280); Miao, Zhuang (35729019500)","57210435093; 57213272280; 35729019500","A New Infrared and Visible Image Fusion Method Based on Generative Adversarial Networks and Attention Mechanism","2021","ACM International Conference Proceeding Series","","","","109","119","10","10.1145/3447587.3447603","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107787477&doi=10.1145%2f3447587.3447603&partnerID=40&md5=1188020fae7f584a35506d12e53a75c1","With various attention mechanisms were proposed, the application of attention mechanisms in the different deep learning tasks has occurred. Meanwhile, most practices and experiments demonstrate that the attention mechanism can help convolutional networks to capture the critical features and raise the performance of the CNNs. In image fusion field, few methods including attention mechanisms are used to integrate the images containing different modalities. In this paper, so, we utilize three types of attention mechanisms which are self-attention, dual attention and multi-scale attention to add into the basic network of image fusion for developing the performance of the fused images. Since the traditional convolutional neural networks and generative adversarial networks (GAN) have some shortcomings, the modified GAN is treated as the basic network. Besides, the dilated convolution is used into the basic network because it can enlarge the convolutional map and receptive field of the kernels under the same kernel size condition. By the experimental comparison, the multi-scale attention is the best practice for infrared and visible image fusion. And the extensive experimental results show that our method can enhance the contrast of the fused image and preserve more thermal and detail information.  © 2021 ACM.","Convolution; Convolutional neural networks; Deep learning; Image enhancement; Adversarial networks; Attention mechanisms; Best practices; Convolutional networks; Critical features; Experimental comparison; Infrared and visible image; Receptive fields; Image fusion","attention mechanisms; dilated convolution; GAN; image fusion","Conference paper","Final","","Scopus","2-s2.0-85107787477"
"Tian S.; Lin S.; Lei H.; Li D.; Wang L.","Tian, Songwang (57219971327); Lin, Suzhen (7407607523); Lei, Haiwei (57171469000); Li, Dawei (37013419900); Wang, Lifang (57142669800)","57219971327; 7407607523; 57171469000; 37013419900; 57142669800","Multi-Band Image Synchronous Super-Resolution and Fusion Method Based on Improved WGAN-GP; [基于改进WGAN-GP的多波段图像同步超分与融合方法]","2020","Guangxue Xuebao/Acta Optica Sinica","40","20","2010001","","","","10.3788/AOS202040.2010001","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096351228&doi=10.3788%2fAOS202040.2010001&partnerID=40&md5=fa419a873859f65b09baaca6aeb66f33","Aiming at the problem that the fused results of low resolution source images are not good for the subsequent target extraction, a multi-band image synchronous super-resolution and fusion method based on Wasserstein generative adversarial network with gradient penalty (WGAN-GP) is proposed. Firstly, the multi-band low-resolution source images are enlarged to the target size respectively based on the bicubic interpolation method. Secondly, the enlarged results are input to a feature extraction (encoding) network to extract features respectively and combine them in a high-level feature space. Then, the initial fused images are reconstructed by decoding network. Finally, a high-resolution fused image is obtained through a dynamic game between the generator and the discriminator. The experimental results show that the proposed method can not only achieve multi-band images super-resolution and fusion simultaneously, but also the information amount, clarity, and visual quality of the fused images are significantly higher than other representative methods. © 2020, Chinese Lasers Press. All right reserved.","Extraction; Image fusion; Optical resolving power; Adversarial networks; Bicubic interpolation; High-level features; High-resolution fused images; Information amount; Multi-band images; Super resolution; Target extraction; Image enhancement","Generative adversarial network; Image fusion; Image processing; Image super-resolution; Multi-band image","Article","Final","","Scopus","2-s2.0-85096351228"
"Liu X.; Gao Z.; Chen B.M.","Liu, Xiaodong (56420642200); Gao, Zhi (55256514200); Chen, Ben M. (7408611448)","56420642200; 55256514200; 7408611448","MLFcGAN: Multilevel Feature Fusion-Based Conditional GAN for Underwater Image Color Correction","2020","IEEE Geoscience and Remote Sensing Letters","17","9","8894129","1488","1492","4","10.1109/LGRS.2019.2950056","36","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090421130&doi=10.1109%2fLGRS.2019.2950056&partnerID=40&md5=b1e5f0eac4469df44bd182968b2d24ef","Color correction for underwater images has received increasing interest, due to its critical role in facilitating available mature vision algorithms for underwater scenarios. Inspired by the stunning success of deep convolutional neural network (DCNN) techniques in many vision tasks, especially the strength in extracting features in multiple scales, we propose a deep multiscale feature fusion net based on the conditional generative adversarial network (GAN) for underwater image color correction. In our network, multiscale features are extracted first, followed by augmenting local features in each scale with global features. This design was verified to facilitate more effective and faster network learning, resulting in better performance in both color correction and detail preservation. We conducted extensive experiments and compared the results with state-of-the-art approaches quantitatively and qualitatively, showing that our method achieves significant improvements. © 2004-2012 IEEE.","Color; Convolutional neural networks; Deep neural networks; Image fusion; Adversarial networks; Color correction; Detail preservation; Extracting features; Multi-scale features; Network learning; State-of-the-art approach; Vision algorithms; algorithm; artificial neural network; color; image analysis; scale effect; underwater environment; Color image processing","Conditional generative adversarial network (cGAN); feature extraction and fusion; image enhancement; underwater image color correction","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85090421130"
"Huang B.; Li Z.; Yang C.; Sun F.; Song Y.","Huang, Binghui (57216946822); Li, Zhi (57208551292); Yang, Chao (57195032153); Sun, Fuchun (57204699218); Song, Yixu (15124457200)","57216946822; 57208551292; 57195032153; 57204699218; 15124457200","Single satellite optical imagery dehazing using SAR image prior based on conditional generative adversarial networks","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093471","1795","1802","7","10.1109/WACV45572.2020.9093471","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085521974&doi=10.1109%2fWACV45572.2020.9093471&partnerID=40&md5=4ebab6919c32b079b180d91c1193b85f","Satellite image dehazing aims at precisely retrieving the real situations of the obscured parts from the hazy remote sensing (RS) images, which is a challenging task since the hazy regions contain both ground features and haze components. Many approaches of removing haze focus on processing multi-spectral or RGB images, whereas few of them utilize multi-sensor data. The multi-sensor data fusion is significant to provide auxiliary information since RGB images are sensitive to atmospheric conditions. In this paper, a dataset called SateHaze1k is established and composed of 1200 pairs clear Synthetic Aperture Radar (SAR), hazy RGB, and corresponding ground truth images, which are divided into three degrees of the haze, i.e. thin, moderate, and thick fog. Moreover, we propose a novel fusion dehazing method to directly restore the haze-free RS images by using an end-to-end conditional generative adversarial network(cGAN). The proposed network combines the information of both RGB and SAR images to eliminate the image blurring. Besides, the dilated residual blocks of the generator can also sufficiently improve the dehazing effects. Our experiments demonstrate that the proposed method, which fuses the information of different sensors applied to the cloudy conditions, can achieve more precise results than other baseline models. © 2020 IEEE.","Computer vision; Demulsification; Image fusion; Remote sensing; Satellite imagery; Sensor data fusion; Space-based radar; Synthetic aperture radar; Adversarial networks; Atmospheric conditions; Auxiliary information; Cloudy conditions; Multi-sensor data; Multisensor data fusion; Remote sensing images; Satellite optical imagery; Radar imaging","","Conference paper","Final","","Scopus","2-s2.0-85085521974"
"Xu D.; Wang Y.; Xu S.; Zhu K.; Zhang N.; Zhang X.","Xu, Dongdong (56299205100); Wang, Yongcheng (56437944700); Xu, Shuyan (55704101100); Zhu, Kaiguang (13004992800); Zhang, Ning (57188816117); Zhang, Xin (57774426900)","56299205100; 56437944700; 55704101100; 13004992800; 57188816117; 57774426900","Infrared and visible image fusion with a generative adversarial network and a residual network","2020","Applied Sciences (Switzerland)","10","2","554","","","","10.3390/app10020554","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079740278&doi=10.3390%2fapp10020554&partnerID=40&md5=ff08c2c0a06fe5933c56fc2e1568b8ed","Infrared and visible image fusion can obtain combined images with salient hidden objectives and abundant visible details simultaneously. In this paper, we propose a novel method for infrared and visible image fusion with a deep learning framework based on a generative adversarial network (GAN) and a residual network (ResNet). The fusion is accomplished with an adversarial game and directed by the unique loss functions. The generator with residual blocks and skip connections can extract deep features of source image pairs and generate an elementary fused image with infrared thermal radiation information and visible texture information, and more details in visible images are added to the final images through the discriminator. It is unnecessary to design the activity level measurements and fusion rules manually, which are now implemented automatically. Also, there are no complicated multi-scale transforms in this method, so the computational cost and complexity can be reduced. Experiment results demonstrate that the proposed method eventually gets desirable images, achieving better performance in objective assessment and visual quality compared with nine representative infrared and visible image fusion methods. © 2020 by the authors.","","Deep learning; Generative adversarial network; Infrared and visible image fusion; Residual network; Structural similarity loss","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85079740278"
"Jintao S.; Chaoyue G.; Hui S.; Jiangang S.; Zhe L.","Jintao, Shi (57214884972); Chaoyue, Gu (57214891562); Hui, Sun (57215533350); Jiangang, Shen (57189940618); Zhe, Li (57191699656)","57214884972; 57214891562; 57215533350; 57189940618; 57191699656","Data Expansion for Foreign Object Detection in Power Grid","2019","Asia-Pacific Power and Energy Engineering Conference, APPEEC","2019-December","","8994654","","","","10.1109/APPEEC45492.2019.8994654","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081089583&doi=10.1109%2fAPPEEC45492.2019.8994654&partnerID=40&md5=8279c544cd519835e9fde6a0ca1f7009","There have been some researches on the use of deep learning algorithm in the work of power patrol inspection. With some current object detection algorithms as the core, a power grid image recognition system can be built to detect abnormal operation or foreign invasion in the power grid, which could save manpower and material resources and improve the security of the power grid. Deep learning requires a number of effective samples to work. There are few real images of hidden dangers, which cannot meet the demand. This paper aims to explore a feasible data expansion scheme. One possible way is to merge the target with the context background image according to some rules. Another method is to generate samples via GAN (Generative adversarial network). Experiments results show that the performance of the power grid image recognition system is improved by using the extended training set. © 2019 IEEE.","Deep learning; Image enhancement; Image fusion; Image recognition; Learning algorithms; Object detection; Object recognition; Abnormal operation; Adversarial networks; Data expansion; DCGAN; Image recognition system; Material resources; Object detection algorithms; Poisson image editing; Electric power transmission networks","Data expansion; DCGAN; Image fusion; Poisson image editing; Power patrol inspection","Conference paper","Final","","Scopus","2-s2.0-85081089583"
"Dong Y.; Liu Y.; Zhang H.; Chen S.; Qiao Y.","Dong, Yu (57220878589); Liu, Yihao (57206486742); Zhang, He (57191984770); Chen, Shifeng (56098326400); Qiao, Yu (36086392600)","57220878589; 57206486742; 57191984770; 56098326400; 36086392600","FD-GAN: Generative adversarial networks with fusion-discriminator for single image dehazing","2020","AAAI 2020 - 34th AAAI Conference on Artificial Intelligence","","","","10729","10736","7","","76","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106398794&partnerID=40&md5=9ac0aeb18772ad09aac5734dcc4f9b73","Recently, convolutional neural networks (CNNs) have achieved great improvements in single image dehazing and attained much attention in research. Most existing learningbased dehazing methods are not fully end-to-end, which still follow the traditional dehazing procedure: first estimate the medium transmission and the atmospheric light, then recover the haze-free image based on the atmospheric scattering model. However, in practice, due to lack of priors and constraints, it is hard to precisely estimate these intermediate parameters. Inaccurate estimation further degrades the performance of dehazing, resulting in artifacts, color distortion and insufficient haze removal. To address this, we propose a fully end-to-end Generative Adversarial Networks with Fusiondiscriminator (FD-GAN) for image dehazing. With the proposed Fusion-discriminator which takes frequency information as additional priors, our model can generator more natural and realistic dehazed images with less color distortion and fewer artifacts. Moreover, we synthesize a large-scale training dataset including various indoor and outdoor hazy images to boost the performance and we reveal that for learning-based dehazing methods, the performance is strictly influenced by the training data. Experiments have shown that our method reaches state-of-the-art performance on both public synthetic datasets and real-world images with more visually pleasing dehazed results. © AAAI 2020 - 34th AAAI Conference on Artificial Intelligence. All Rights Reserved.","Convolutional neural networks; Discriminators; Finite difference method; Image enhancement; Image fusion; Large dataset; Light transmission; Adversarial networks; Atmospheric scattering models; Color distortions; Frequency information; Real-world image; Single image dehazing; State-of-the-art performance; Synthetic datasets; Demulsification","","Conference paper","Final","","Scopus","2-s2.0-85106398794"
"Li X.; Wen J.-M.; Chen A.-L.; Chen B.","Li, Xiang (57221400267); Wen, Jin-Mei (57207778449); Chen, An-Long (57207762137); Chen, Bo (56948850500)","57221400267; 57207778449; 57207762137; 56948850500","A Method for Face Fusion Based on Variational Auto-Encoder","2019","2018 15th International Computer Conference on Wavelet Active Media Technology and Information Processing, ICCWAMTIP 2018","","","8632589","77","80","3","10.1109/ICCWAMTIP.2018.8632589","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062874356&doi=10.1109%2fICCWAMTIP.2018.8632589&partnerID=40&md5=73be7b355b8edfea0dfe76852df169a6","Face fusion refers to fuse two different facial images into a new face image that retains the facial features of the original image. Our network model is combine Variational Auto-Encoder(V Ae)and Generative Adversarial Networks(GAN), which achieved the end-to-end fusion task. Not only does it guarantee the quality of the fusion image(the image generated by GAN is sharp and photorealistic), but also doesn't lose specific details of the face(This is guaranteed by VAE). In the end, the experiment achieved a promising result on the CelebA dataset. © 2018 IEEE.","Learning systems; Signal encoding; Adversarial networks; Auto encoders; Face fusion; Facial feature; Facial images; Network modeling; Original images; Photo-realistic; Image fusion","Face fusion; GAN; VAE","Conference paper","Final","","Scopus","2-s2.0-85062874356"
"Zhou H.; Wu W.; Zhang Y.; Ma J.; Ling H.","Zhou, Huabing (55447554500); Wu, Wei (57222093753); Zhang, Yanduo (55993581700); Ma, Jiayi (26638975600); Ling, Haibin (57191091290)","55447554500; 57222093753; 55993581700; 26638975600; 57191091290","Semantic-Supervised Infrared and Visible Image Fusion Via a Dual-Discriminator Generative Adversarial Network","2023","IEEE Transactions on Multimedia","25","","","635","648","13","10.1109/TMM.2021.3129609","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120078911&doi=10.1109%2fTMM.2021.3129609&partnerID=40&md5=939c67ffa45a7975b0d1a81ddda0d6fa","Image fusion synthesizes a new image from multiple images of the same scene. The synthesized image should be suitable for human visual perception and follow-up high-level image-processing tasks. However, existing methods focus on fusing low-level features, ignoring high-level semantic perception information. We propose a new end-to-end model to obtain a more semantically consistent image in infrared and visible image fusion, termed semantic-supervised dual-discriminator generative adversarial network (SDDGAN). In particular, we design an information quantity discrimination (IQD) block to guide fusion progress. For each source image, the block determines the weight for preserving each semantic object's feature. By this way, the generator learns to fuse various semantic objects via different weights to preserve their characteristics. Moreover, the dual discriminator is employed to identify the distribution of infrared and visible information in the fused image. Each discriminator acts on a certain modality (infrared/visible) of different semantic objects in the fused image to preserve and enhance their modality features. Thus, our fused image is more informative. Both the thermal radiation in the infrared image and the visible image texture details can be well preserved. Qualitative and quantitative experiments demonstrate the superiority of our SDDGAN over state-of-the-art methods in terms of visual effects, efficiency, and quantitative metrics.  © 1999-2012 IEEE.","Computer vision; Discriminators; Image enhancement; Image fusion; Image texture; Infrared imaging; Semantics; Textures; Dual-discriminator; Features extraction; Fused images; Game; Generator; Infrared and visible image; Infrared image; Semantic objects; Semantic supervised; Visible image; Generative adversarial networks","dual-discriminator; Image fusion; infrared image; semantic supervised; visible image","Article","Final","","Scopus","2-s2.0-85120078911"
"Yuan C.; Sun C.Q.; Tang X.Y.; Liu R.F.","Yuan, C. (9041176000); Sun, C.Q. (57219872251); Tang, X.Y. (36239463000); Liu, R.F. (56313409500)","9041176000; 57219872251; 36239463000; 56313409500","FLGC-Fusion GAN: An Enhanced Fusion GAN Model by Importing Fully Learnable Group Convolution","2020","Mathematical Problems in Engineering","2020","","6384831","","","","10.1155/2020/6384831","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095833740&doi=10.1155%2f2020%2f6384831&partnerID=40&md5=ff0a817d2850b22384deb4b8daa1776a","The purpose of image fusion is to combine the source images of the same scene into a single composite image with more useful information and better visual effects. Fusion GAN has made a breakthrough in this field by proposing to use the generative adversarial network to fuse images. In some cases, considering retain infrared radiation information and gradient information at the same time, the existing fusion methods ignore the image contrast and other elements. To this end, we propose a new end-to-end network structure based on generative adversarial networks (GANs), termed as FLGC-Fusion GAN. In the generator, using the learnable grouping convolution can improve the efficiency of the model and save computing resources. Therefore, we can have a better trade-off between the accuracy and speed of the model. Besides, we take the residual dense block as the basic network building unit and use the perception characteristics of the inactive as content loss characteristics of input, achieving the effect of deep network supervision. Experimental results on two public datasets show that the proposed method performs well in subjective visual performance and objective criteria and has obvious advantages over other current typical methods.  © 2020 C. Yuan et al.","Convolution; Economic and social effects; Infrared radiation; Adversarial networks; Computing resource; End-to-end network; Gradient informations; Loss characteristics; Objective criteria; Radiation information; Visual performance; Image fusion","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85095833740"
"Liu Z.; Zhang W.; Zhao P.","Liu, Zhengyi (55714527800); Zhang, Wei (57276085800); Zhao, Peng (57189503904)","55714527800; 57276085800; 57189503904","A cross-modal adaptive gated fusion generative adversarial network for RGB-D salient object detection","2020","Neurocomputing","387","","","210","220","10","10.1016/j.neucom.2020.01.045","37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078587034&doi=10.1016%2fj.neucom.2020.01.045&partnerID=40&md5=f572f857a51fcd172ce5747ea8ffe718","Salient object detection in RGB-D images aims to identify the most attractive objects in a pair of color and depth images for the observer. As an important branch of salient object detection, it focuses on solving the following two major challenges: how to achieve cross-modal fusion that is efficient and beneficial for salient object detection; how to effectively extract the information of depth image with relatively poor quality. This paper proposes a cross-modal adaptive gated fusion generative adversarial network for RGB-D salient object detection by using color and depth images. Specifically, the generator network adopts double-stream encoder-decoder network and receives RGB and depth images at the same time. The proposed depthwise separable residual convolution module is used to deal with deep semantic information, and the processed feature is combined with side-output features of the encoder network progressively. In order to compensate for the shortcoming of poor quality of the depth image, the proposed method adds the cross-modal guidance from the side-output features of the RGB stream to the decoder network of depth stream. The discriminator network adaptively fuses the features of double streams using a gated fusion module, then sends the gated fusion saliency map to the discriminator to distinguish the similarity from ground-truth map. Adversarial learning forms the better generator network and discriminator network, and the gated fusion saliency map generated by the best generator network is served as final result. Experiments on five publicly RGB-D datasets demonstrate the effect of cross-modal fusion, depthwise separable residual convolution and adaptive gated fusion. Compared with the state-of-the-art methods, our method achieves the better performance. © 2020 Elsevier B.V.","Convolution; Decoding; Discriminators; Image fusion; Network coding; Object recognition; Semantics; Adversarial learning; Adversarial networks; Color and depth images; Cross-modal; Encoder-decoder; Salient object detection; Semantic information; State-of-the-art methods; article; human; learning; Object detection","Adaptive gated fusion; Cross-modal guidance; Generative adversarial network; RGB-D salient object detection","Article","Final","","Scopus","2-s2.0-85078587034"
"Fang J.; Ma X.; Wang J.; Qin K.; Hu S.; Zhao Y.","Fang, Jing (57191247277); Ma, Xiaole (57193220596); Wang, Jingjing (57214140268); Qin, Kai (35220430700); Hu, Shaohai (7404286949); Zhao, Yuefeng (14033580400)","57191247277; 57193220596; 57214140268; 35220430700; 7404286949; 14033580400","A noisy sar image fusion method based on nlm and gan","2021","Entropy","23","4","410","","","","10.3390/e23040410","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103906348&doi=10.3390%2fe23040410&partnerID=40&md5=0585a877bf873a8416e1291f2193192f","The unavoidable noise often present in synthetic aperture radar (SAR) images, such as speckle noise, negatively impacts the subsequent processing of SAR images. Further, it is not easy to find an appropriate application for SAR images, given that the human visual system is sensitive to color and SAR images are gray. As a result, a noisy SAR image fusion method based on nonlocal matching and generative adversarial networks is presented in this paper. A nonlocal matching method is applied to processing source images into similar block groups in the pre‐processing step. Then, adversarial networks are employed to generate a final noise‐free fused SAR image block, where the generator aims to generate a noise‐free SAR image block with color information, and the discriminator tries to increase the spatial resolution of the generated image block. This step ensures that the fused image block contains high resolution and color information at the same time. Finally, a fused image can be obtained by aggregating all the image blocks. By extensive comparative experiments on the SEN1–2 datasets and source images, it can be found that the proposed method not only has better fusion results but is also robust to image noise, indicating the superiority of the proposed noisy SAR image fusion method over the state‐of‐the‐art methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.","","Generative adversarial networks; Image fusion; Nonlocal matching","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85103906348"
"Joo D.; Kim D.; Kim J.","Joo, Donggyu (57201372854); Kim, Doyeon (57202400782); Kim, Junmo (36015494900)","57201372854; 57202400782; 36015494900","Generating a Fusion Image: One's Identity and Another's Shape","2018","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","8578274","1635","1643","8","10.1109/CVPR.2018.00176","26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062862372&doi=10.1109%2fCVPR.2018.00176&partnerID=40&md5=af8e5833c8dda5aa84f841fe0a59fa66","Generating a novel image by manipulating two input images is an interesting research problem in the study of generative adversarial networks (GANs). We propose a new GAN-based network that generates a fusion image with the identity of input image x and the shape of input image y. Our network can simultaneously train on more than two image datasets in an unsupervised manner. We define an identity loss LI to catch the identity of image x and a shape loss LS to get the shape of y. In addition, we propose a novel training method called Min-Patch training to focus the generator on crucial parts of an image, rather than its entirety. We show qualitative results on the VGG Youtube Pose dataset, Eye dataset (MPIIGaze and UnityEyes), and the Photo-Sketch-Cartoon dataset. © 2018 IEEE.","Computer vision; Adversarial networks; Fusion image; GaN based; Image datasets; Input image; Research problems; Training methods; YouTube; Image fusion","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062862372"
"Ye G.; Zhang Z.; Ding L.; Li Y.; Zhu Y.","Ye, Guoyao (57200414468); Zhang, Zixin (57219018733); Ding, Li (57211896597); Li, Yinwei (55342581200); Zhu, Yiming (24400323100)","57200414468; 57219018733; 57211896597; 55342581200; 24400323100","GAN-Based Focusing-Enhancement Method for Monochromatic Synthetic Aperture Imaging","2020","IEEE Sensors Journal","20","19","9098931","11484","11489","5","10.1109/JSEN.2020.2996656","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091001032&doi=10.1109%2fJSEN.2020.2996656&partnerID=40&md5=1fde52fb2cb2319904d85cd02e17f21a","Two-dimensional (2-D) synthetic aperture imaging with a single frequency suffers from limited depth-of-focus (DOF), and leads to the difficulty of focusing volume targets. In this paper,as opposed to using a wide band for 3-D imaging, this out-of-focus problem is examined as a multi-focal imaging issue. To solve the limited DOF problem, we propose a generative adversarial network (GAN) based focusing-enhancement method (GAN-FEM) to fit an unknown out-of-focus kernel for MMW monochromatic synthetic aperture imaging. To determine which type of MMW-images dataset of input can be better suitable for GAN, the grayscale and pseudo-color images dataset are tested respectively to train the neural network. Proof-of-principle experiments are performed at 94 GHz and the results prove that our proposed GAN-FEM can greatly improve the focusing performance for volume targets. The effectiveness of our proposed method confirms the focusing-enhancement capacity of 2-D monochromatic imaging system for 3-D targets, and provides a possible solution to reduce the system complexity for practical 3-D imaging missions. © 2001-2012 IEEE.","Synthetic apertures; Adversarial networks; Focusing performance; Monochromatic imaging system; Proof-of-principle experiments; Single frequency; Synthetic aperture imaging; System complexity; Two Dimensional (2 D); Focusing","GAN-FEM; image fusion; MMW near field imaging; monochromatic full-focus; SAR","Article","Final","","Scopus","2-s2.0-85091001032"
"Ma J.; Liang P.; Yu W.; Chen C.; Guo X.; Wu J.; Jiang J.","Ma, J. (26638975600); Liang, Pengwei (57201500677); Yu, Wei (56479633000); Chen, Chen (57192217138); Guo, Xiaojie (36607970100); Wu, Jia (23971568900); Jiang, Junjun (54902306100)","26638975600; 57201500677; 56479633000; 57192217138; 36607970100; 23971568900; 54902306100","Infrared and visible image fusion via detail preserving adversarial learning","2020","Information Fusion","54","","","85","98","13","10.1016/j.inffus.2019.07.005","197","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069882474&doi=10.1016%2fj.inffus.2019.07.005&partnerID=40&md5=f2498d10bf50cfba306a92c1158de4e8","TargefTablets can be detected easily from the background of infrared images due to their significantly discriminative thermal radiations, while visible images contain textural details with high spatial resolution which are beneficial to the enhancement of target recognition. Therefore, fused images with abundant detail information and effective target areas are desirable. In this paper, we propose an end-to-end model for infrared and visible image fusion based on detail preserving adversarial learning. It is able to overcome the limitations of the manual and complicated design of activity-level measurement and fusion rules in traditional fusion methods. Considering the specific information of infrared and visible images, we design two loss functions including the detail loss and target edge-enhancement loss to improve the quality of detail information and sharpen the edge of infrared targets under the framework of generative adversarial network. Our approach enables the fused image to simultaneously retain the thermal radiation with sharpening infrared target boundaries in the infrared image and the abundant textural details in the visible image. Experiments conducted on publicly available datasets demonstrate the superiority of our strategy over the state-of-the-art methods in both objective metrics and visual impressions. In particular, our results look like enhanced infrared images with clearly highlighted and edge-sharpened targets as well as abundant detail information. © 2019 Elsevier B.V.","Heat radiation; Image enhancement; Infrared imaging; Infrared radiation; Adversarial learning; Adversarial networks; Convolution neural network; Detail preserving; High spatial resolution; Infrared and visible image; Specific information; State-of-the-art methods; Image fusion","Convolution neural network; Detail preserving; Generative adversarial network; Image fusion; Infrared","Article","Final","","Scopus","2-s2.0-85069882474"
"Zhang X.; Cheng L.; Bai S.; Zhang F.; Sun N.; Wang Z.","Zhang, Xuefei (57216288337); Cheng, Lechao (57210252202); Bai, Shengli (57216287248); Zhang, Fan (57208893083); Sun, Nongliang (7202557280); Wang, Zhangye (8060929000)","57216288337; 57210252202; 57216287248; 57208893083; 7202557280; 8060929000","Face Image Inpainting via Variational Autoencoder; [基于变分自编码器的人脸图像修复]","2020","Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics","32","3","","401","409","8","10.3724/SP.J.1089.2020.17938","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083029031&doi=10.3724%2fSP.J.1089.2020.17938&partnerID=40&md5=ab305cfcd8a4b16c49221210d22bd7e8","Face image inpainting based on the convolutional neural network has enabled a variety of applications, ranging from criminal investigation to cultural relics protection. However, the results of existing methods are often limited to insufficient diversity and still far from realistic. In this work, we generate more reasonable missing facial content with a variant of variational auto-encoder, as well as generative adversarial network. Furthermore, we impose constraints on latent variables to encourage the distribution of representations to be factorial that making them independent across dimensions. Latter, the optimal boundary was obtained through dynamic programming, and finally we get the seamless results by Poisson image editing. Experiments on CelebA dataset demonstrated that the proposed method achieved better inpainting results and disentanglement. © 2020, Beijing China Science Journal Publishing Co. Ltd. All right reserved.","Convolutional neural networks; Image processing; Learning systems; Adversarial networks; Auto encoders; Criminal investigation; Cultural relics protections; Face images; Latent variable; Optimal boundary; Poisson image editing; Dynamic programming","Discriminative network; Disentanglement; Image fusion; Image inpainting; Variational autoencoder","Article","Final","","Scopus","2-s2.0-85083029031"
"Zhou W.; Wu W.; Zhou H.","Zhou, Wenhao (57292496000); Wu, Wei (57222093753); Zhou, Huabing (55447554500)","57292496000; 57222093753; 55447554500","Semantic-Aware Infrared and Visible Image Fusion","2021","2021 4th International Conference on Robotics, Control and Automation Engineering, RCAE 2021","","","","82","85","3","10.1109/RCAE53607.2021.9638835","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123753923&doi=10.1109%2fRCAE53607.2021.9638835&partnerID=40&md5=0f524d369fd9596ff1b8469fe363e1f3","Aiming at the problems of poor visual effect and lack of background details in infrared and visible image fusion, we proposed an image fusion algorithm based on semantic segmentation. This method obtains the position and shape of each target in the source image through semantic segmentation, and we will set the weight value for each target, so that the information of the image can be preserved to a large extent. In addition, we also design a generative adversarial network, which uses different loss functions to adjust the generator and discriminator to ensure that the fused image is clearer and has richer texture features. Experimental results show that our method is superior to the new method in both visual effect and qualitative index.  © 2021 IEEE.","Generative adversarial networks; Image fusion; Semantic Web; Semantics; Textures; Fused images; Image fusion algorithms; Infrared and visible image; Loss functions; Semantic segmentation; Semantic-aware; Source images; Texture features; Visual effects; Weight values; Semantic Segmentation","generative adversarial network; image fusion; semantic segmentation","Conference paper","Final","","Scopus","2-s2.0-85123753923"
"Yu B.; Zhou L.; Wang L.; Fripp J.; Bourgeat P.","Yu, Biting (57201496052); Zhou, Luping (23398846800); Wang, Lei (54958774700); Fripp, Jurgen (13605436500); Bourgeat, Pierrick (8576247900)","57201496052; 23398846800; 54958774700; 13605436500; 8576247900","3D cGAN based cross-modality MR image synthesis for brain tumor segmentation","2018","Proceedings - International Symposium on Biomedical Imaging","2018-April","","","626","630","4","10.1109/ISBI.2018.8363653","54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048105885&doi=10.1109%2fISBI.2018.8363653&partnerID=40&md5=a1d870167cabc988a548341d770448e9","Different modalities of magnetic resonance imaging (MRI) can indicate tumor-induced tissue changes from different perspectives, thus benefit brain tumor segmentation when they are considered together. Meanwhile, it is always interesting to examine the diagnosis potential from single modality, considering the cost of acquiring multi-modality images. Clinically, T1-weighted MRI is the most commonly used MR imaging modality, although it may not be the best option for contouring brain tumor. In this paper, we investigate whether synthesizing FLAIR images from T1 could help improve brain tumor segmentation from the single modality of T1. This is achieved by designing a 3D conditional Generative Adversarial Network (cGAN) for FLAIR image synthesis and a local adaptive fusion method to better depict the details of the synthesized FLAIR images. The proposed method can effectively handle the segmentation task of brain tumors that vary in appearance, size and location across samples. © 2018 IEEE.","Brain; Diagnosis; Image enhancement; Image fusion; Image segmentation; Medical imaging; Tumors; Adversarial networks; Brain tumor segmentation; Cross modality; Image synthesis; Local-adaptive; Magnetic Resonance Imaging (MRI); Multi modality image; Tissue changes; Magnetic resonance imaging","Brain tumor segmentation; Generative Adversarial Network; Image synthesis; Local adaptive fusion","Conference paper","Final","","Scopus","2-s2.0-85048105885"
"Zhang H.; Shen Y.; Ou Y.; Ji B.; He J.","Zhang, Hongzhi (57404913400); Shen, Yifan (57404747600); Ou, Yangyan (57404913500); Ji, Bo (57405399400); He, Jia (57405071200)","57404913400; 57404747600; 57404913500; 57405399400; 57405071200","A GAN-based visible and infrared image fusion algorithm","2021","Proceedings of SPIE - The International Society for Optical Engineering","12061","","120610Z","","","","10.1117/12.2605587","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122536648&doi=10.1117%2f12.2605587&partnerID=40&md5=f4b7d531b0dc3a373c218c25a1ad863a","In this paper, we propose a new GAN-based end-to-end image fusion network (VIFGAN) for fusion of visible and infrared images. VIFGAN contains a generator and a discriminator. We added the DenseNet module to the generator, and this module can extract deeper features and details. We also propose a Two-way regulation loss function(TWR-Loss). The loss function considers both the radiation information and texture information in the image, which can make the network suitable for image fusion tasks of different spectrum combinations. The experimental results show that in the fusion task of visible light and infrared images, the proposed network has better fusion performance than the existing fusion algorithm, the visual effect of the fused image is better, and the extracted details are more abundant.  Copyright © 2021 SPIE.","Generative adversarial networks; Infrared imaging; Textures; Densenet; End to end; Image fusion algorithms; Information information; Infrared image fusions; Loss functions; Radiation information; Spectra's; Texture information; Two ways; Image fusion","DenseNet; Generative adversarial network; Image fusion; Infrared","Conference paper","Final","","Scopus","2-s2.0-85122536648"
"Li S.; Qian P.; Zhang X.; Chen A.","Li, Size (57415839400); Qian, Pengjiang (36598989000); Zhang, Xin (57774448700); Chen, Aiguo (55447313100)","57415839400; 36598989000; 57774448700; 55447313100","Research on Image Denoising and Super-Resolution Reconstruction Technology of Multiscale-Fusion Images","2021","Mobile Information Systems","2021","","5184688","","","","10.1155/2021/5184688","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123000907&doi=10.1155%2f2021%2f5184688&partnerID=40&md5=85e9f9226314e878894303c4ec9c48ee","Image denoising and image super-resolution reconstruction are two important techniques for image processing. Deep learning is used to solve the problem of image denoising and super-resolution reconstruction in recent years, and it usually has better results than traditional methods. However, image denoising and super-resolution reconstruction are studied separately by state-of-the-art work. To optimally improve the image resolution, it is necessary to investigate how to integrate these two techniques. In this paper, based on Generative Adversarial Network (GAN), we propose a novel image denoising and super-resolution reconstruction method, i.e., multiscale-fusion GAN (MFGAN), to restore the images interfered by noises. Our contributions reflect in the following three aspects: (1) the combination of image denoising and image super-resolution reconstruction simplifies the process of upsampling and downsampling images during the model learning, avoiding repeated input and output images operations, and improves the efficiency of image processing. (2) Motivated by the Inception structure and introducing a multiscale-fusion strategy, our method is capable of using the multiple convolution kernels with different sizes to expand the receptive field in parallel. (3) The ablation experiments verify the effectiveness of each employed loss measurement in our devised loss function. And our experimental studies demonstrate that the proposed model can effectively expand the receptive field and thus reconstruct images with high resolution and accuracy and that the proposed MFGAN method performs better than a few state-of-the-art methods. Copyright © 2021 Size Li et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.","Deep learning; Generative adversarial networks; Image denoising; Image enhancement; Image fusion; Image resolution; Signal sampling; Art work; Fusion image; Image super-resolution reconstruction; Images processing; Multiscale fusion; Receptive fields; Reconstruction method; State of the art; Super-resolution reconstruction; Upsampling; Image reconstruction","","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85123000907"
"Lin S.; Liu S.; Tang Y.","Lin, Sen (36613212800); Liu, Shiben (57217079079); Tang, Yandong (12791654900)","36613212800; 57217079079; 12791654900","Multi-input fusion adversarial network for underwater image enhancement; [多输入融合对抗网络的水下图像增强]","2020","Hongwai yu Jiguang Gongcheng/Infrared and Laser Engineering","49","5","20200015","","","","10.3788/IRLA20200015","8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086072734&doi=10.3788%2fIRLA20200015&partnerID=40&md5=da6981b659db6401b22b32c1474f6a88","For underwater image of low contrast, color deviation and blurred details and other issues, the multi-input fusion adversarial networks was proposed to enhance underwater images. The main feature of this method was that the generative network used encoding and decoding structure, filtering noise through convolution layer, recovering lost details through deconvolution layer and refining the image pixel by pixel. Firstly, the original image was preprocessed to obtain two types of images: color correction and contrast enhancement. Secondly, the confidence graph of the difference between the two enhanced images and the original image was learned by using the generated network. Then, in order to reduce artifacts and details blur introduced by the two enhancement algorithms in the process of generating network learning, the texture extraction unit was added to extract texture features from the two enhanced images, and the extracted texture features were fused with the corresponding confidence map. Finally, the enhanced underwater image was obtained by constructing multiple loss functions and training the adversarial network repeatedly. The experimental results show that the enhanced underwater image has bright color and improved contrast, the average value of UCIQE and NIQE is 0.639 9 and 3.727 3 respectively. Compared with other algorithms, the algorithm has significant advantages and proves its good effect. © 2020, Editorial Board of Journal of Infrared and Laser Engineering. All right reserved.","Color; Deconvolution; Image fusion; Pixels; Textures; Adversarial networks; Color correction; Color deviations; Contrast Enhancement; Encoding and decoding; Enhancement algorithms; Improved contrasts; Texture extraction; Image enhancement","Deep learning; Encoding and decoding structure; Generative adversarial network; Multi-input fusion; Underwater image enhancement","Article","Final","","Scopus","2-s2.0-85086072734"
"Zhang J.; Shamsolmoali P.; Zhang P.; Feng D.; Yang J.","Zhang, Junhao (56368684700); Shamsolmoali, Pourya (56350053200); Zhang, Pengpeng (56104492700); Feng, Deying (35219755900); Yang, Jie (15039078800)","56368684700; 56350053200; 56104492700; 35219755900; 15039078800","Multispectral image fusion using super-resolution conditional generative adversarial networks","2019","Journal of Applied Remote Sensing","13","2","022002","","","","10.1117/1.JRS.13.022002","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055792733&doi=10.1117%2f1.JRS.13.022002&partnerID=40&md5=88db0212e3ac0f09a92f60ac8917e082","In multispectral image fusion scenarios, deep learning has been widely applied. However, the fusion performance and image quality are still restricted by inflexible architecture and supervised learning mode. We proposed multispectral image fusion using super-resolution conditional generative adversarial networks (MS-cGANs) based on conditional cGANs, which produces the fused image through the flexible encode-and-decode procedure. In the proposed network, a least square model is extended to solve the gradients vanishing problem in cGANs. Then, to improve the fusion quality, the multiscale features are used to preserve the details. Furthermore, the image resolution is promoted by adding the perceptual loss in object function and injecting the super-resolution structure into a deconvolution procedure. In experimental results, MS-cGANs demonstrates a significant performance in fusing multispectral images and top-ranking image quality compared with the state-of-the-art methods. © 2018 Society of Photo-Optical Instrumentation Engineers (SPIE).","Deep learning; Fusion reactions; Image quality; Image resolution; Least squares approximations; Optical resolving power; Remote sensing; Adversarial networks; Fusion performance; Least square model; Multi-scale features; Multi-spectral image fusions; Multispectral images; State-of-the-art methods; Super resolution; Image fusion","fusion; multispectral image; multispectral-conditional generative adversarial network; remote sensing","Article","Final","","Scopus","2-s2.0-85055792733"
"Liu X.; Wang Y.; Liu Q.","Liu, Xiangyu (57192693924); Wang, Yunhong (34870959400); Liu, Qingjie (55534263100)","57192693924; 34870959400; 55534263100","Psgan: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","2018","Proceedings - International Conference on Image Processing, ICIP","","","8451049","873","877","4","10.1109/ICIP.2018.8451049","84","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062900060&doi=10.1109%2fICIP.2018.8451049&partnerID=40&md5=1849950b74eeded4a0073d53039a6838","Remote sensing image fusion (also known as pan-sharpening) aims to generate a high resolution multi -spectral image from inputs of a high spatial resolution single band panchromatic (PAN) image and a low spatial resolution multi-spectral (MS) image. In this paper, we propose PSGAN, a generative adversarial network (GAN) for remote sensing image pansharpening. To the best of our knowledge, this is the first attempt at producing high quality pan-sharpened images with GANs. The PSGAN consists of two parts. Firstly, a two-stream fusion architecture is designed to generate the desired high resolution multi -spectral images, then a fully convolutional network serving as a discriminator is applied to distinct 'real' or 'pan-sharpened' MS images. Experiments on images acquired by Quickbird and GaoFen-1 satellites demonstrate that the proposed PSGAN can fuse PAN and MS images effectively and significantly improve the results over the state of the art traditional and CNN based pan-sharpening methods. © 2018 IEEE.","Deep learning; Image fusion; Image resolution; Remote sensing; Spectroscopy; Adversarial networks; Convolutional networks; Fusion architecture; High spatial resolution; Multispectral images; Pan-sharpening; Panchromatic (Pan) image; Remote sensing images; Image enhancement","Deep learning; GAN; Image fusion; Pan-sharpening; Remote sensing","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85062900060"
"Gai D.; Shen X.; Chen H.; Su P.","Gai, Di (57209858124); Shen, Xuanjing (7402721669); Chen, Haipeng (35753222600); Su, Pengxiang (57217144717)","57209858124; 7402721669; 35753222600; 57217144717","Multi-focus image fusion method based on two stage of convolutional neural network","2020","Signal Processing","176","","107681","","","","10.1016/j.sigpro.2020.107681","21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086446351&doi=10.1016%2fj.sigpro.2020.107681&partnerID=40&md5=186140c0a74bd0c69c5638ef4b4da816","This paper proposes a two-stage convolutional neural network (CNN) fusion method that can obtain accurate decision map and deal with the problem of unclear fusion boundaries. In the first stage, an improved densenet is trained to classify whether the image patch is in focus or defocus, and then the corresponding fusion rule is utilized to acquire a perfect decision map. In addition, a multi-version blurred dataset is designed to improve the generalization ability of the network. In the second stage, edge-deblurring generative adversarial networks (EDGAN) is introduced to process the boundary. Furthermore, five different loss functions are applied to generate approving boundary deblurred images. At the same time, natural images are selected from the COCO dataset for special processing to simulate the boundary blurring situation to create the second-stage dataset. After two stages of processing, an image with rich details and decent fusion boundaries are attained. Experimental results demonstrate that the proposed algorithm is superior to other fusion algorithms in subjective vision and objective assessment. © 2020","Convolution; Convolutional neural networks; Image enhancement; Adversarial networks; Fusion algorithms; Fusion boundary; Fusion methods; Generalization ability; Multifocus image fusion; Objective assessment; Special processing; Image fusion","Convolutional neural network; Deep learning; Fusion rule; Multi-focus image fusion","Article","Final","","Scopus","2-s2.0-85086446351"
"Hughes L.H.; Merkle N.; Burgmann T.; Auer S.; Schmitt M.","Hughes, Lloyd Haydn (57201113391); Merkle, Nina (57194604557); Burgmann, Tatjana (57211533315); Auer, Stefan (57216043589); Schmitt, Michael (7401931279)","57201113391; 57194604557; 57211533315; 57216043589; 7401931279","Deep Learning for SAR-Optical Image Matching","2019","International Geoscience and Remote Sensing Symposium (IGARSS)","","","8898635","4877","4880","3","10.1109/IGARSS.2019.8898635","24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077692111&doi=10.1109%2fIGARSS.2019.8898635&partnerID=40&md5=c382fe0d785719e2bb15b46747bf7656","The automatic matching of corresponding regions in remote sensing imagery acquired by synthetic aperture radar (SAR) and optical sensors is a crucial pre-requesite for many data fusion endeavours such as target recognition, image registration, or 3D-reconstruction by stereogrammetry. Driven by the success of deep learning in conventional optical image matching, we have carried out extensive research with regard to deep matching for SAR-optical multi-sensor image pairs in the recent past. In this paper, we summarize the achieved findings, including different concepts based on (pseudo-)siamese convolutional neural network architectures, hard negative mining, alternative formulations of the underlying loss function, and creation of artificial images by generative adversarial networks. Based on data from state-of-the-art remote sensing missions such as TerraSAR-X, Prism, Worldview-2, and Sentinel-1/2, we show what is already possible today, while highlighting challenges to be tackled by future research endeavors. © 2019 IEEE.","Convolutional neural networks; Data fusion; Deep learning; Geology; Geometrical optics; Image fusion; Image matching; Network architecture; Radar target recognition; Remote sensing; Synthetic aperture radar; Adversarial networks; Automatic matching; Multi sensor images; Optical image; Remote sensing imagery; Remote sensing missions; SAR Images; Target recognition; Radar imaging","Data Fusion; Deep Learning; Image Matching; Optical Images; SAR Images","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077692111"
"Lu S.; Gao H.","Lu, Shaowen (26657424200); Gao, Hongru (57212034341)","26657424200; 57212034341","Deep Learning Based Fusion of RGB and Infrared Images for the Detection of Abnormal Condition of Fused Magnesium Furnace","2019","IEEE International Conference on Control and Automation, ICCA","2019-July","","8899693","987","993","6","10.1109/ICCA.2019.8899693","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075799892&doi=10.1109%2fICCA.2019.8899693&partnerID=40&md5=31885bad353dbf5c7b055bd0d2ab9f2b","In fused magnesium furnace (FMF) process, semi-molten is one of the most harmful abnormal working conditions, under which the furnace wall is thinned by the overheated fused magnesium because of the uneven impurities in raw material. If the condition is not detected on its early stage, the furnace can be burnt through. At present, semi-molten is detected by experienced operators by directly 'observing the fire' at the production site of FMF. There is a high risk and the labor intensity is high. Such practice of detection relied on human may cause safety issues and can lead to missed or false detection. This work introduces a detection technology for the semi-molten working condition of FMF based on the fusion of RGB images and infrared thermal images. The classifier is established using deep Convolutional Neural Network (CNN) model trained using historical data. Also, to tackle the problem of insufficient training data, the Deep Convolutional Generative Adversarial Networks (DCGAN) is employed to generate extra samples. Finally, industrial experiments carried out in a magnesium oxide plant to show the effectiveness of the technology. © 2019 IEEE.","Chemical detection; Convolution; Deep neural networks; Furnaces; Infrared imaging; Magnesia; Neural networks; Abnormal conditions; Adversarial networks; Convolutional neural network; Detection technology; False detections; Fused magnesium furnaces; Industrial experiments; Infrared thermal image; Image fusion","","Conference paper","Final","","Scopus","2-s2.0-85075799892"
"Li J.; Huo H.; Li C.; Wang R.; Feng Q.","Li, Jing (57207844651); Huo, Hongtao (56527849900); Li, Chang (56718731300); Wang, Renhua (57221059573); Feng, Qi (57217586786)","57207844651; 56527849900; 56718731300; 57221059573; 57217586786","AttentionFGAN: Infrared and Visible Image Fusion Using Attention-Based Generative Adversarial Networks","2021","IEEE Transactions on Multimedia","23","","9103116","1383","1396","13","10.1109/TMM.2020.2997127","63","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104971178&doi=10.1109%2fTMM.2020.2997127&partnerID=40&md5=14901c10939c29b54c490c210ef8ab9e","Infrared and visible image fusion aims to describe the same scene from different aspects by combining complementary information of multi-modality images. The existing Generative adversarial networks (GAN) based infrared and visible image fusion methods cannot perceive the most discriminative regions, and hence fail to highlight the typical parts existing in infrared and visible images. To this end, we integrate multi-scale attention mechanism into both generator and discriminator of GAN to fuse infrared and visible images (AttentionFGAN). The multi-scale attention mechanism aims to not only capture comprehensive spatial information to help generator focus on the foreground target information of infrared image and background detail information of visible image, but also constrain the discriminators focus more on the attention regions rather than the whole input image. The generator of AttentionFGAN consists of two multi-scale attention networks and an image fusion network. Two multi-scale attention networks capture the attention maps of infrared and visible images respectively, so that the fusion network can reconstruct the fused image by paying more attention to the typical regions of source images. Besides, two discriminators are adopted to force the fused result keep more intensity and texture information from infrared and visible image respectively. Moreover, to keep more information of attention region from source images, an attention loss function is designed. Finally, the ablation experiments illustrate the effectiveness of the key parts of our method, and extensive qualitative and quantitative experiments on three public datasets demonstrate the advantages and effectiveness of AttentionFGAN compared with the other state-of-the-art methods. © 1999-2012 IEEE.","Infrared imaging; Textures; Ablation experiments; Adversarial networks; Attention mechanisms; Infrared and visible image; Multi modality image; Quantitative experiments; Spatial informations; State-of-the-art methods; Image fusion","Attention mechanism; generative adversarial networks; infrared and visible image fusion","Article","Final","","Scopus","2-s2.0-85104971178"
"Li G.; Li J.; Fan H.","Li, Guihui (57201908517); Li, Jinjiang (25638923500); Fan, Hui (36647626600)","57201908517; 25638923500; 36647626600","Edge-guided multispectral image fusion algorithm","2020","Journal of Applied Remote Sensing","14","4","046515","","","","10.1117/1.JRS.14.046515","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098642871&doi=10.1117%2f1.JRS.14.046515&partnerID=40&md5=1cfd168ef028c7b969d79143cb468b45","Most existing multispectral fusion algorithms often suffer from spectral or spatial information distortion. Driven by this motivation, we propose an edge-guided multispectral (MS) image fusion algorithm. In particular, it combines the advantages of generative adversarial networks and improved fusion frameworks, so the merged image can better preserve the spectral information of the original multispectral image while injecting spatial detail information. Specifically, first, an MS image with more image detail is generated using the generated confrontation network for preliminary reconstruction. The panchromatic image edge information and the antagonistic learning strategy are introduced for the robust multispectral image reconstruction. Then, using the reconstructed MS image and the general component substitution image fusion framework, the whole fusion system of this paper is constructed. An enhancement operator is introduced to inject spatial details. Our extensive dataset evaluations show that our approach performs better in terms of high objective quality and human visual perception than several of the most advanced fusion methods.  © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE).","Edge detection; Image enhancement; Image reconstruction; Adversarial networks; Component substitution; Human visual perception; Multi-spectral image fusions; Multispectral fusion; Multispectral images; Spatial informations; Spectral information; Image fusion","edge enhancement; generative adversarial network; multispectral image fusion; super-resolution reconstruction","Article","Final","","Scopus","2-s2.0-85098642871"
"Vizil’ter Y.V.; Vygolov O.V.; Komarov D.V.; Lebedev M.A.","Vizil’ter, Yu. V. (6506127474); Vygolov, O.V. (10439430400); Komarov, D.V. (56539656000); Lebedev, M.A. (56539470900)","6506127474; 10439430400; 56539656000; 56539470900","Fusion of Images of Different Spectra Based on Generative Adversarial Networks","2019","Journal of Computer and Systems Sciences International","58","3","","441","453","12","10.1134/S1064230719030201","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067951151&doi=10.1134%2fS1064230719030201&partnerID=40&md5=db086ef5940ff415f6c1eefd9fd1c51e","Abstract: A method for fusing images of different spectra by using generative adversarial networks is proposed. An original architecture of a FusionNet neural network is developed based on pix2pix. It enables the synthesis of a complex (integrated) image that comprises the most informative fragments of different-spectra images, thus being more informative than any of these individual images. A technique for generating training and test sets, as well as the process of data augmentation, is described. The operation of the proposed image fusion method is demonstrated on some real-world infrared and visible images. © 2019, Pleiades Publishing, Ltd.","Computer science; Software engineering; Adversarial networks; Data augmentation; Image fusion methods; Infrared and visible image; Real-world; Test sets; Image fusion","","Article","Final","","Scopus","2-s2.0-85067951151"
"Yao Z.; Guo H.; Ren L.","Yao, Zhiqiang (57225178032); Guo, Huinan (54906415000); Ren, Long (57225748488)","57225178032; 54906415000; 57225748488","An improved fusion method of infrared and visible images based on fusionGAN","2021","Proceedings of SPIE - The International Society for Optical Engineering","11878","","118781H","","","","10.1117/12.2599559","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109396295&doi=10.1117%2f12.2599559&partnerID=40&md5=76eaa36a2c82c6381e8b1aa600dce813","Convolutional neural network is widely used in image fusion. However, the deep learning framework is only applied in some part of the fusion process in most existing methods. To generate a full end-to-end image fusion pipeline, a Y-shaped Generator model based on Generative Adversarial Network for infrared and visible image fusion is proposed. The idea of this method is to establish an adversarial game between the generator and the discriminator. The generator consisting of two Pyramid networks and three convolutional layers works as an autoencoder to improve the characteristic information of the fused images. As for the discriminator, it adopts a network structure similar to the Visual Geometry Group (VGG) network. The loss function uses the ratio loss to control the trade-off among generation loss and reconstruction loss. Results on publicly available datasets demonstrate that our method can improve the quality of detail information and sharpen the edge of infrared targets. © 2021 SPIE","Convolution; Convolutional neural networks; Deep learning; Economic and social effects; Image fusion; Learning systems; Adversarial networks; Fusion methods; Generator modeling; Infrared and visible image; Infrared target; Learning frameworks; Network structures; Pyramid network; Image enhancement","FusionGAN; Image fusion; Infrared image; Pyramid network; Residual network; Visible image","Conference paper","Final","","Scopus","2-s2.0-85109396295"
"Xu J.; Wan C.; Yang W.; Zheng B.; Yan Z.; Shen J.","Xu, Jianguo (57232340300); Wan, Cheng (23398892800); Yang, Weihua (56070103200); Zheng, Bo (57202691724); Yan, Zhipeng (57224773943); Shen, Jianxin (56727028700)","57232340300; 23398892800; 56070103200; 57202691724; 57224773943; 56727028700","A novel multi-modal fundus image fusion method for guiding the laser surgery of central serous chorioretinopathy","2021","Mathematical Biosciences and Engineering","18","4","","4797","4816","19","10.3934/mbe.2021244","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108298949&doi=10.3934%2fmbe.2021244&partnerID=40&md5=051da0d8962f60834193cbb8554b6f43","The angiography and color fundus images are of great assistance for the localization of central serous chorioretinopathy (CSCR) lesions. However, it brings much inconvenience to ophthalmologists because of these two modalities working independently in guiding laser surgery. Hence, a novel fundus image fusion method in non-subsampled contourlet transform (NSCT) domain, aiming to integrate the multi-modal CSCR information, is proposed. Specifically, the source images are initially decomposed into high-frequency and low-frequency components based on NSCT. Then, an improved deep learning-based method is employed for the fusion of low-frequency components, which helps to alleviate the tedious process of manually designing fusion rules and enhance the smoothness of the fused images. The fusion of high-frequency components based on pulse-coupled neural network (PCNN) is closely followed to facilitate the integration of detailed information. Finally, the fused images can be obtained by applying an inverse transform on the above fusion components. Qualitative and quantitative experiments demonstrate the proposed scheme is superior to the baseline methods of multi-scale transform (MST) in most cases, which not only implies its potential in multi-modal fundus image fusion, but also expands the research direction of MST-based fusion methods. © 2021 American Institute of Mathematical Sciences. All rights reserved.","Algorithms; Central Serous Chorioretinopathy; Humans; Laser Therapy; Neural Networks, Computer; Deep learning; Electric arcs; Image enhancement; Inverse problems; Inverse transforms; Laser surgery; Surgery; Fusion components; High frequency components; Learning-based methods; Low-frequency components; Multi-scale transforms; Non subsampled contourlet transform (NSCT); Pulse coupled neural network; Quantitative experiments; algorithm; central serous retinopathy; diagnostic imaging; human; low level laser therapy; Image fusion","Central serous chorioretinopathy; Generative adversarial network; Image fusion; Non-subsampled contourlet transform; Pulse-coupled neural network","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85108298949"
"Yu X.; Liu H.; Han X.; Li Z.; Xiong Z.; Cui S.","Yu, Xianggang (57221157190); Liu, Haolin (57222865266); Han, Xiaoguang (55451013500); Li, Zhen (57191478250); Xiong, Zixiang (7202954235); Cui, Shuguang (57210290902)","57221157190; 57222865266; 55451013500; 57191478250; 7202954235; 57210290902","JAFPro: Joint Appearance Fusion and Propagation for Human Video Motion Transfer from Multiple Reference Images","2020","MM 2020 - Proceedings of the 28th ACM International Conference on Multimedia","","","","2544","2552","8","10.1145/3394171.3414001","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106925073&doi=10.1145%2f3394171.3414001&partnerID=40&md5=804b2e28c2ba0d8f01c0054e14ca53a7","We present a novel framework for human video motion transfer. Deviating from recent studies that use only single source image, we propose to allow users to supply multiple source images by simply imitating some poses in the desired target video. To aggregate the appearance from multiple input images, we propose a JAFPro framework that incorporates two modules: an appearance fusion module that adaptively fuses the information in the supplied images and an appearance propagation module that propagates textures through flow-based warping to further improve the result. An attractive feature of JAFPro is that the quality of its results progressively improves as more imitating images are supplied. Furthermore, we build a new dataset containing a large variety of dancing videos in the wild. Extensive experiments conducted on this dataset demonstrate JAFPro outperforms state-of-the-art methods both qualitatively and quantitatively. We will release our code and dataset upon publication of this work.  © 2020 ACM.","Image fusion; Large dataset; Textures; Fusion modules; Multiple inputs; Multiple reference images; Multiple source; Propagation modules; Single source; State-of-the-art methods; Video motion; Image enhancement","generative adversarial network; human video generation; motion transfer; multiple reference images","Conference paper","Final","","Scopus","2-s2.0-85106925073"
"","","","2nd Chinese Conference on Computer Vision, CCCV 2017","2017","Communications in Computer and Information Science","773","","","","","1377","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038086371&partnerID=40&md5=b6622d05247c58921045dc68f6d2e997","The proceedings contain 113 papers. The special focus in this conference is on Computer Vision. The topics include: Visual saliency fusion based multi-feature for semantic image retrieval; unsupervised multi-view subspace learning via maximizing dependence; structured multi-view supervised feature selection algorithm research; an automatic shoeprint retrieval method using neural codes for commercial shoeprint scanners; uncovering the effect of visual saliency on image retrieval; shape-color differential moment invariants under affine transforms; a novel layer based image fusion approach via transfer learning and coupled dictionary; local saliency extraction for fusion of visible and infrared images; distributed compressive sensing for light field reconstruction using structured random matrix; improved face verification with simple weighted feature combination; stereoscopic image quality assessment based on binocular adding and subtracting; quality assessment of palm vein image using natural scene statistics; an error-activation-guided blind metric for stitched panoramic image quality assessment; image aesthetic quality evaluation using convolution neural network embedded fine-tune; high capacity reversible data hiding with contrast enhancement; rank learning for dehazed image quality assessment; a low-rank total-variation regularized tensor completion algorithm; nighttime haze removal with fusion atmospheric light and improved entropy; light field super-resolution using cross-resolution input based on patchmatch and learning method; a new image sparse reconstruction method for mixed Gaussian-poisson noise with multiple constraints; pore-scale facial features matching under 3D morphable model constraint; face video super-resolution with identity guided generative adversarial networks; exemplar-based pixel by pixel inpainting based on patch shift; GAN based sample simulation for SEM-image super resolution; PSO-based single image defogging.","","","Conference review","Final","","Scopus","2-s2.0-85038086371"
"Yang N.; Zheng Z.; Zhou M.; Guo X.; Qi L.; Wang T.","Yang, Nan (57206512912); Zheng, Zeyu (35308540900); Zhou, Mengchu (7403506743); Guo, Xiwang (55267162600); Qi, Liang (55270493600); Wang, Tianran (7405563735)","57206512912; 35308540900; 7403506743; 55267162600; 55270493600; 7405563735","A Domain-Guided Noise-Optimization-Based Inversion Method for Facial Image Manipulation","2021","IEEE Transactions on Image Processing","30","","9462525","6198","6211","13","10.1109/TIP.2021.3089905","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112101743&doi=10.1109%2fTIP.2021.3089905&partnerID=40&md5=5a621d5bb113aff11cf5e57b9959eb76","A style-based architecture (StyleGAN2) yields outstanding results in data-driven unconditional generative image modeling. This work proposes a Domain-guided Noise-optimization-based Inversion (DNI) method to perform facial image manipulation. It works based on an inverse code that includes: 1) a novel domain-guided encoder called Image2latent to project the image to StyleGAN2 latent space, which can reconstruct an input image with high-quality and maintain its semantic meaning well; 2) a noise optimization mechanism in which a set of noise vectors are used to capture the high-frequency details such as image edges, further improving image reconstruction quality; and 3) a mask for seamless image fusion and local style migration. We further propose a novel semantic alignment evaluation pipeline. It evaluates the semantic alignment with an inverse code by using different attribute boundaries. Extensive qualitative and quantitative comparisons show that DNI can capture rich semantic information and achieve a satisfactory image reconstruction. It can realize a variety of facial image manipulation tasks and outperform state of the art.  © 1992-2012 IEEE.","Alignment; Codes (symbols); Image enhancement; Image fusion; Semantics; Vector spaces; High frequency HF; Inversion methods; Noise optimization; Quantitative comparison; Reconstruction quality; Semantic alignments; Semantic information; State of the art; Image reconstruction","Deep learning; domain-guided encoder; generative adversarial networks; noise optimization","Article","Final","","Scopus","2-s2.0-85112101743"
"Lu X.; Yang L.; Li M.; Zhang X.","Lu, Xin (57212537770); Yang, Lin (57221069722); Li, Min (57201044203); Zhang, Xuewu (25925595900)","57212537770; 57221069722; 57201044203; 25925595900","Infrared and Visible Image Fusion Method Based on Tikhonov Regularization and Detail Reconstruction; [基于Tikhonov正则化和细节重建的红外与可见光图像融合方法]","2020","Guangxue Xuebao/Acta Optica Sinica","40","2","0210001","","","","10.3788/AOS202040.0210001","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081001507&doi=10.3788%2fAOS202040.0210001&partnerID=40&md5=a529c229f7a2a783b03c1e0e832a15bb","Traditional infrared and visible image fusion method decomposes images into several frequency components, fuses them separately, and then adds them together, resulting in problems of edge fuzziness, low contrast, and so on. The paper proposes a fusion method based on Tikhonov regularization and detail reconstruction. Firstly, images are decomposed into base layers and detail layers by Tikhonov regularization. A generative adversarial network is trained aiming at detail information reconstruction for base layers. Secondly, features of base layers to be fused are extracted, and the principal component analysis method is used for feature fusion. Finally, the fused results of base layers are input into generative network to reconstruct a fusion image with abundant high frequency information. Experimental results show that the method proposed in this paper preserves detail information and highlight areas of the source images well, with a good robustness to the images with different resolutions. © 2020, Chinese Lasers Press. All right reserved.","Image fusion; Image processing; Principal component analysis; Adversarial networks; Different resolutions; Frequency components; High-frequency informations; Information reconstruction; Infrared and visible image; Principal component analysis method; Tikhonov regularization; Image reconstruction","Detail reconstruction; Generative adversarial network; Image processing; Principal component analysis; Tikhonov regularization","Article","Final","","Scopus","2-s2.0-85081001507"
"Palsson F.; Sveinsson J.R.; Ulfarsson M.O.","Palsson, Frosti (55052918200); Sveinsson, Johannes R. (7003642214); Ulfarsson, Magnus O. (6507677875)","55052918200; 7003642214; 6507677875","Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","2018","Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","2018-September","","8747268","","","","10.1109/WHISPERS.2018.8747268","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072027457&doi=10.1109%2fWHISPERS.2018.8747268&partnerID=40&md5=5e989f4449d87f7a978a0f1c7090f7ec","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel2 datasets. © 2018 IEEE.","Convolution; Image processing; Image resolution; Infrared devices; Infrared radiation; Remote sensing; Spectroscopy; Adversarial networks; Convolutional networks; Electromagnetic spectra; Multispectral images; Multispectral sensors; Sentinel-2; Short wave infrared; Spatial resolution; Image fusion","convolutional network; generative adversarial network; Image fusion; Sentinel-2","Conference paper","Final","","Scopus","2-s2.0-85072027457"
"Huang J.; Le Z.; Ma Y.; Mei X.; Fan F.","Huang, Jun (56688687500); Le, Zhuliang (57215469419); Ma, Yong (56438173900); Mei, Xiaoguang (55800813300); Fan, Fan (35795122500)","56688687500; 57215469419; 56438173900; 55800813300; 35795122500","A generative adversarial network with adaptive constraints for multi-focus image fusion","2020","Neural Computing and Applications","32","18","","15119","15129","10","10.1007/s00521-020-04863-1","15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082942131&doi=10.1007%2fs00521-020-04863-1&partnerID=40&md5=9198ea0226cd0d05d07e95e060d7cfb8","In this paper, we propose a novel end-to-end model for multi-focus image fusion based on generative adversarial networks, termed as ACGAN. In our model, due to the different gradient distribution between the corresponding pixels of two source images, an adaptive weight block is proposed in our model to determine whether source pixels are focused or not based on the gradient. Under this guidance, we design a special loss function for forcing the fused image to have the same distribution as the focused regions in source images. In addition, a generator and a discriminator are trained to form a stable adversarial relationship. The generator is trained to generate a real-like fused image, which is expected to fool the discriminator. Correspondingly, the discriminator is trained to distinguish the generated fused image from the ground truth. Finally, the fused image is very close to ground truth in probability distribution. Qualitative and quantitative experiments are conducted on publicly available datasets, and the results demonstrate the superiority of our ACGAN over the state-of-the-art, in terms of both visual effect and objective evaluation metrics. © 2020, Springer-Verlag London Ltd., part of Springer Nature.","Pixels; Probability distributions; Adaptive constraints; Adaptive weights; Adversarial networks; End to end; Gradient distributions; Multifocus image fusion; Objective evaluation; Quantitative experiments; Image fusion","Adaptive weight block; End-to-end; Generative adversarial networks; Multi-focus image fusion","Article","Final","","Scopus","2-s2.0-85082942131"
"Wang T.; Lei Y.; Tian Z.; Tang X.; Curran W.J.; Liu T.; Yang X.","Wang, Tonghe (57189639465); Lei, Yang (57202715941); Tian, Zhen (35320361000); Tang, Xiangyang (7404101165); Curran, Walter J. (57203070877); Liu, Tian (26643332700); Yang, Xiaofeng (36712893800)","57189639465; 57202715941; 35320361000; 7404101165; 57203070877; 26643332700; 36712893800","Synthesizing high-resolution ct from low-resolution ct using self-learning","2021","Progress in Biomedical Optics and Imaging - Proceedings of SPIE","11595","","115952N","","","","10.1117/12.2581080","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103690019&doi=10.1117%2f12.2581080&partnerID=40&md5=36c7ad2a7ae1eca849a9615bf5a7b944","We propose a learning-based method to synthesize high-resolution (HR) CT images from low-resolution (LR) CT images. A self-super-resolution framework with cycle consistent generative adversarial network (CycleGAN) is proposed. As an ill-posed problem, recent super-resolution methods rely on the presence of external/training atlases to learn the transform LR images to HR images, which is often not available for CT imaging to have high resolution for slice thickness. To circumvent the lack of HR training data in z-axis, the network learns the mapping from LR 2D transverse plane slices to HR 2D transverse plane slices via CycleGAN and inference HR 2D sagittal and coronal plane slices by feeding these sagittal and coronal slices into the trained CycleGAN. The 3D HR CT image is then reconstructed by collecting these HR 2D sagittal and coronal slices and image fusion. In addition, in order to force the ill-posed LR to HR mapping to be close to a one-to-one mapping, CycleGAN is used to model the mapping. To force the network focusing on learning the difference between LR and HR image, residual network is integrated into the CycleGAN. To evaluate the proposed method, we retrospectively investigate 20 brain datasets. For each dataset, the original CT image volume was served as ground truth and training target. Low-resolution CT volumes were simulated by downsampling the original CT images at slice thickness direction. The MAE is 17.9±2.9 HU and 25.4±3.7 HU for our results at downsampling factor of 3 and 5, respectively. The proposed method has great potential in improving the image resolution for low pitch scan without hardware modification. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.","Image enhancement; Image fusion; Image resolution; Mapping; Medical imaging; Optical resolving power; Signal sampling; Adversarial networks; Hardware modifications; High resolution CT; Ill posed problem; Learning-based methods; One-to-one mappings; Superresolution methods; Transverse planes; Computerized tomography","Ct; High-resolution.; Self-learning","Conference paper","Final","","Scopus","2-s2.0-85103690019"
"Li Z.; Ogino M.","Li, Zisheng (35242982000); Ogino, Masahiro (57212020008)","35242982000; 57212020008","Adversarial learning for deformable image registration: Application to 3d ultrasound image fusion","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11798 LNCS","","","56","64","8","10.1007/978-3-030-32875-7_7","3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075712663&doi=10.1007%2f978-3-030-32875-7_7&partnerID=40&md5=8bf4cab09672353a9ba94cc335a21e1a","We present an adversarial learning algorithm for deep-learning-based deformable image registration (DIR) and apply to 3D liver ultrasound image fusion. We consider DIR as a parametric optimization model that aims to find displacement field of deformation. We propose an adversarial learning framework inspired by generative adversarial network (GAN) to predict the displacement field without ground-truth spatial transformation. We use convolutional neural network (CNN) and a spatial transform layer as registration network to generate the registered image. Similarity metrics of image intensity and vessel masks are used as loss function for the training. We also optimize a discrimination network to measure the divergence between the registered image and the fixed image. Feedback from the discrimination network can guide the registration network for more accurate and realistic deformation. Moreover, we incorporate an autoencoder network to extract anatomical features from vessel masks as shape regularization. Our approach is end-to-end, only requires image pair as input in registration tasks. Experiments show that the proposed method outperforms state-of-the-art deep-learning-based methods. © 2019, Springer Nature Switzerland AG.","Computer aided analysis; Deep learning; Deformation; Discriminators; Image analysis; Image registration; Learning algorithms; Medical imaging; Multilayer neural networks; Network layers; Pediatrics; Ultrasonic imaging; Adversarial networks; Convolutional neural network; Deformable image registration; Learning-based methods; Parametric optimization; Realistic deformations; Shape regularization; Spatial transformation; Image fusion","Deep learning; Deformable image registration; GAN","Conference paper","Final","","Scopus","2-s2.0-85075712663"
"Nair R.R.; Singh T.; Sankar R.; Gunndu K.","Nair, Rekha R. (57202982056); Singh, Tripty (36007656400); Sankar, Rashmi (57351418500); Gunndu, Klement (57351418600)","57202982056; 36007656400; 57351418500; 57351418600","Multi-modal medical image fusion using LMF-GAN - A maximum parameter infusion technique","2021","Journal of Intelligent and Fuzzy Systems","41","5","","5375","5386","11","10.3233/JIFS-189860","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119841145&doi=10.3233%2fJIFS-189860&partnerID=40&md5=dcb6004d7ae3bf6c6fcc546dccd0fe6c","The multi-sensor, multi-modal, composite design of medical images merged into a single image, contributes to identifying features that are relevant to medical diagnoses and treatments. Although, current image fusion technologies, including conventional and deep learning algorithms, can produce superior fused images, however, they will require huge volumes of images of various modalities. This solution may not be viable for some situations, where time efficiency is expected or the equipment is inadequate. This paper addressed a modified end-to-end Generative Adversarial Network(GAN), termed Loss Minimized Fusion Generative Adversarial Network (LMF-GAN), a triple ConvNet deep learning architecture for the fusion of medical images with a limited sampling rate. The encoding network is combined with a convolutional neural network layer and a dense block called GAN, in contrast to conventional convolutional networks. The loss is minimized by training GAN's discriminator with all the source images by learning more parameters to generate more features in the fused image. The LMF-GAN can produce fused images with clear textures through adversarial training of the generator and discriminator. The proposed fusion method has the ability to achieve state-of-the-art quality in objective and subjective evaluation, in comparison with current fusion methods. The model has experimented with standard data sets.  © 2021 - IOS Press. All rights reserved.","Convolution; Convolutional neural networks; Deep learning; Diagnosis; Discriminators; Image fusion; Learning algorithms; Medical imaging; Multilayer neural networks; Network layers; Textures; ADAM optimizer; Composite designs; Fused images; Fusion methods; Generator; Medical image fusion; Multi sensor; Multi-modal; Optimizers; Single images; Generative adversarial networks","ADAM optimizer; discriminator; generative adversarial network; generator; Medical image fusion","Article","Final","","Scopus","2-s2.0-85119841145"
"Zhang Z.; Zhang R.; Wang Q.-F.; Huang K.","Zhang, Zhejian (57215413204); Zhang, Rui (56024777800); Wang, Qiu-Feng (56097694800); Huang, Kaizhu (13403476100)","57215413204; 56024777800; 56097694800; 13403476100","Improving disentanglement-based image-to-image translation with feature joint block fusion","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11691 LNAI","","","540","549","9","10.1007/978-3-030-39431-8_52","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080957604&doi=10.1007%2f978-3-030-39431-8_52&partnerID=40&md5=44c9afb9f7edba84b1d50c41ebc29eab","Image-to-image translation aims to change attributes or domains of images, where the feature disentanglement based method is widely used recently due to its feasibility and effectiveness. In this method, a feature extractor is usually integrated in the encoder-decoder architecture generative adversarial network (GAN), which extracts features from domains and images, respectively. However, the two types of features are not properly combined, resulting in blurry generated images and indistinguishable translated domains. To alleviate this issue, we propose a new feature fusion approach to leverage the ability of the feature disentanglement. Instead of adding the two extracted features directly, we design a joint block fusion that contains integration, concatenation, and squeeze operations, thus allowing the generator to take full advantage of the two features and generate more photo-realistic images. We evaluate both the classification accuracy and Fréchet Inception Distance (FID) of the proposed method on two benchmark datasets of Alps Seasons and CelebA. Extensive experimental results demonstrate that the proposed joint block fusion can improve both the discriminability of domains and the quality of translated image. Specially, the classification accuracies are improved by 1.04% (FID reduced by 1.22) and 1.87% (FID reduced by 4.96) on Alps Seasons and CelebA, respectively. © Springer Nature Switzerland AG 2020.","Brain; Classification (of information); Cognitive systems; Image fusion; Adversarial networks; Benchmark datasets; Classification accuracy; Encoder-decoder architecture; Feature disentanglement; Feature fusion; Image translation; Photorealistic images; Image enhancement","Feature disentanglement; Feature fusion; Generative adversarial networks; Image-to-image translation","Conference paper","Final","","Scopus","2-s2.0-85080957604"
"Xu J.; Shi X.; Qin S.; Lu K.; Wang H.; Ma J.","Xu, Jiangtao (56137165600); Shi, Xingping (57212405237); Qin, Shuzhen (57209638909); Lu, Kaige (57212410627); Wang, Han (56969829000); Ma, Jianguo (55647071600)","56137165600; 57212405237; 57209638909; 57212410627; 56969829000; 55647071600","LBP-BEGAN: A generative adversarial network architecture for infrared and visible image fusion","2020","Infrared Physics and Technology","104","","103144","","","","10.1016/j.infrared.2019.103144","16","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076605038&doi=10.1016%2fj.infrared.2019.103144&partnerID=40&md5=34e7c2b8877e5c348aff10a896dff27e","This paper proposes a novel generative adversarial network (GAN) architecture to fuse infrared (IR) and visible images (VIS), named as LBP-BEGAN. The fused images generated by this network have rich boundary information through a loss function based on local binary patterns (LBP). At the same time, a distribution-based discriminator is applied to distinguish the fused images and the original IR and VIS images to guarantee the quality of the fusion results. This structure is able to establish adversarial loss without an idealfused image as the label. Qualitative and quantitative comparisons against eight classical and state-of-the-art fusion methods demonstrate the effectiveness of our strategy. Our approach can generate fused images with clear edges and textures and successfully preserves a large amount of information in the original images. © 2019 Elsevier B.V.","Infrared imaging; Network architecture; Textures; Adversarial networks; Boundary information; Fusion methods; Infrared and visible image; Local binary patterns; Original images; Quantitative comparison; State of the art; Image fusion","Generative adversarial network (GAN); Image fusion; Infrared image processing; Local binary patterns (LBP)","Article","Final","","Scopus","2-s2.0-85076605038"
"Fujii R.; Hachiuma R.; Saito H.","Fujii, Ryo (57215272562); Hachiuma, Ryo (57193735508); Saito, Hideo (55619315207)","57215272562; 57193735508; 55619315207","Rgb-d image inpainting using generative adversarial network with a late fusion approach","2020","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12242 LNCS","","","440","451","11","10.1007/978-3-030-58465-8_32","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091157706&doi=10.1007%2f978-3-030-58465-8_32&partnerID=40&md5=59415c403f5d62d00c419ef5f6bc9f0d","Diminished reality is a technology that aims to remove objects from video images and fills in the missing region with plausible pixels. Most conventional methods utilize the different cameras that capture the same scene from different viewpoints to allow regions to be removed and restored. In this paper, we propose an RGB-D image inpainting method using generative adversarial network, which does not require multiple cameras. Recently, an RGB image inpainting method has achieved outstanding results by employing a generative adversarial network. However, RGB inpainting methods aim to restore only the texture of the missing region and, therefore, does not recover geometric information (i.e, 3D structure of the scene). We expand conventional image inpainting method to RGB-D image inpainting to jointly restore the texture and geometry of missing regions from a pair of RGB and depth images. Inspired by other tasks that use RGB and depth images (e.g., semantic segmentation and object detection), we propose late fusion approach that exploits the advantage of RGB and depth information each other. The experimental results verify the effectiveness of our proposed method. © Springer Nature Switzerland AG 2020.","Augmented reality; Cameras; Image fusion; Image segmentation; Object detection; Restoration; Semantics; Textures; Virtual reality; Adversarial networks; Conventional methods; Depth information; Geometric information; Image Inpainting; Inpainting method; Multiple cameras; Semantic segmentation; Image reconstruction","Generative adversarial network; Image inpainting; Mixed reality","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85091157706"
"Ma J.; Zhang H.; Shao Z.; Liang P.; Xu H.","Ma, Jiayi (26638975600); Zhang, Hao (57215014270); Shao, Zhenfeng (7202244409); Liang, Pengwei (57201500677); Xu, Han (57201056465)","26638975600; 57215014270; 7202244409; 57201500677; 57201056465","GANMcC: A Generative Adversarial Network with Multiclassification Constraints for Infrared and Visible Image Fusion","2021","IEEE Transactions on Instrumentation and Measurement","70","","9274337","","","","10.1109/TIM.2020.3038013","101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097389945&doi=10.1109%2fTIM.2020.3038013&partnerID=40&md5=8266e000d81708d9732f9c5787fe9e7b","Visible images contain rich texture information, whereas infrared images have significant contrast. It is advantageous to combine these two kinds of information into a single image so that it not only has good contrast but also contains rich texture details. In general, previous fusion methods cannot achieve this goal well, where the fused results are inclined to either a visible or an infrared image. To address this challenge, a new fusion framework called generative adversarial network with multiclassification constraints (GANMcC) is proposed, which transforms image fusion into a multidistribution simultaneous estimation problem to fuse infrared and visible images in a more reasonable way. We adopt a generative adversarial network with multiclassification to estimate the distributions of visible light and infrared domains at the same time, in which the game of multiclassification discrimination will make the fused result to have these two distributions in a more balanced manner, so as to have significant contrast and rich texture details. In addition, we design a specific content loss to constrain the generator, which introduces the idea of main and auxiliary into the extraction of gradient and intensity information, which will enable the generator to extract more sufficient information from source images in a complementary manner. Extensive experiments demonstrate the advantages of our GANMcC over the state-of-the-art methods in terms of both qualitative effect and quantitative metric. Moreover, our method can achieve good fused results even the visible image is overexposed. Our code is publicly available at https://github.com/jiayi-ma/GANMcC. © 1963-2012 IEEE.","Image classification; Image fusion; Infrared imaging; Textures; Adversarial networks; Infrared and visible image; Intensity information; Multi-classification; Quantitative metric; Simultaneous estimation; State-of-the-art methods; Texture information; Image texture","Deep learning; generative adversarial network (GAN); image fusion; infrared; multiclassification","Article","Final","","Scopus","2-s2.0-85097389945"
"Wang Y.; Zhou L.; Yu B.; Wang L.; Zu C.; Lalush D.S.; Lin W.; Wu X.; Zhou J.; Shen D.","Wang, Yan (56039981100); Zhou, Luping (23398846800); Yu, Biting (57201496052); Wang, Lei (54958774700); Zu, Chen (55377165500); Lalush, David S. (7004144493); Lin, Weili (56999175100); Wu, Xi (57221065403); Zhou, Jiliu (21234416400); Shen, Dinggang (7401738392)","56039981100; 23398846800; 57201496052; 54958774700; 55377165500; 7004144493; 56999175100; 57221065403; 21234416400; 7401738392","3D Auto-Context-Based Locality Adaptive Multi-Modality GANs for PET Synthesis","2019","IEEE Transactions on Medical Imaging","38","6","8552676","1328","1339","11","10.1109/TMI.2018.2884053","85","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057882665&doi=10.1109%2fTMI.2018.2884053&partnerID=40&md5=e271530cf68d8523ac59224cde9ecf0b","Positron emission tomography (PET) has been substantially used recently. To minimize the potential health risk caused by the tracer radiation inherent to PET scans, it is of great interest to synthesize the high-quality PET image from the low-dose one to reduce the radiation exposure. In this paper, we propose a 3D auto-context-based locality adaptive multi-modality generative adversarial networks model (LA-GANs) to synthesize the high-quality FDG PET image from the low-dose one with the accompanying MRI images that provide anatomical information. Our work has four contributions. First, different from the traditional methods that treat each image modality as an input channel and apply the same kernel to convolve the whole image, we argue that the contributions of different modalities could vary at different image locations, and therefore a unified kernel for a whole image is not optimal. To address this issue, we propose a locality adaptive strategy for multi-modality fusion. Second, we utilize 1 × 1 × 1 kernel to learn this locality adaptive fusion so that the number of additional parameters incurred by our method is kept minimum. Third, the proposed locality adaptive fusion mechanism is learned jointly with the PET image synthesis in a 3D conditional GANs model, which generates high-quality PET images by employing large-sized image patches and hierarchical features. Fourth, we apply the auto-context strategy to our scheme and propose an auto-context LA-GANs model to further refine the quality of synthesized images. Experimental results show that our method outperforms the traditional multi-modality fusion methods used in deep networks, as well as the state-of-the-art PET estimation approaches. © 1982-2012 IEEE.","Brain; Databases, Factual; Deep Learning; Humans; Imaging, Three-Dimensional; Magnetic Resonance Imaging; Phantoms, Imaging; Positron-Emission Tomography; Radiation Dosage; Health risks; Image fusion; Magnetic resonance imaging; Positron emission tomography; Positrons; Adaptive fusion; Adversarial networks; Image synthesis; Multi modality; Positron emission; article; low drug dose; nuclear magnetic resonance imaging; positron emission tomography; synthesis; topography; brain; diagnostic imaging; factual database; human; imaging phantom; positron emission tomography; procedures; radiation dose; three-dimensional imaging; Image processing","generative adversarial networks (GANs); Image synthesis; locality adaptive fusion; multi-modality; positron emission topography (PET)","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85057882665"
"Lebedev M.A.; Komarov D.V.; Vygolov O.V.; Vizilter Y.V.","Lebedev, M.A. (56539470900); Komarov, D.V. (56539656000); Vygolov, O.V. (10439430400); Vizilter, Yu. V. (6506127474)","56539470900; 56539656000; 10439430400; 6506127474","Multisensor image fusion based on generative adversarial networks","2019","Proceedings of SPIE - The International Society for Optical Engineering","11155","","111551T","","","","10.1117/12.2533098","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078141442&doi=10.1117%2f12.2533098&partnerID=40&md5=f38c3486368ebb972b244f944ea32e65","The paper addresses the further advance in our complex research in the field of multisensory image fusion based on generative adversarial models [1-2] and their application to such practical tasks as visual representation of fused images, acquired in different spectral ranges (e.g. TV and IR), and changes detection on images, acquired in different conditions (e.g. season-varying images). A developed architecture of a neural network based on pix2pix model is presented, which can solve the both tasks mentioned above. A technique for generating training and test datasets including data augmentation process is described. The results are demonstrated on real-world images. © 2019 SPIE.","Deep learning; Deep neural networks; Image acquisition; Image processing; Neural networks; Remote sensing; Adversarial networks; Changes detections; Data augmentation; Multisensor image fusion; Multisensory image fusions; Multispectral images; Real-world image; Visual representations; Image fusion","Deep learning; Generative adversarial networks; Image fusion; Image processing; Multispectral images; Neural networks","Conference paper","Final","","Scopus","2-s2.0-85078141442"
"Li B.; Yuan X.; Shi M.","Li, Bo (57199786743); Yuan, Xue (7402202691); Shi, Minghan (57215542710)","57199786743; 7402202691; 57215542710","Synthetic data generation based on local-foreground generative adversarial networks for surface defect detection","2020","Journal of Electronic Imaging","29","1","013016","","","","10.1117/1.JEI.29.1.013016","7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081137937&doi=10.1117%2f1.JEI.29.1.013016&partnerID=40&md5=9f830c97642b935acaa5522ed140fbe8","We propose a synthetic image method named local-foreground generative adversarial networks for surface defect detection in the industry. The method comprises three contributions: First, in order to provide more training data, an algorithm that generates full synthetic images of defects is proposed. The method may blend the defect samples stored in the different mobile terminal into existing background images of the production environment in a natural way, accounting for both geometry and appearance. Second, the diversity of defects appearance is increased due to the use of deep convolutional generative adversarial networks only for local-foreground and the defect edge details that are preserved by introducing a guided filter in the process of image synthesis. Third, the image fusion method can adapt to the various production environment of different brightness and camera angles, which has strong adaptability and expansion ability. We also discuss the experimental results of synthetic data on an end-to-end detection system based on deep learning. As we know, YOLO detector trained on synthetic data achieving average precision of 95.2% on test dataset, which is 12.5% higher than heuristic data augmentation individually. Furthermore, it can process 23 images/s on 1080Ti GPU, which meets the requirements for real-time industrial inspection. © 2020 SPIE and IS&T.","Deep learning; Defects; Image processing; Statistical tests; Surface defects; Adversarial networks; Image fusion methods; Image synthesis; Industrial inspections; Production environments; Surface defect detections; Synthetic data generations; Visual inspection; Image fusion","full image synthesis; generative adversarial network; image fusion; surface defect detection; visual inspection","Article","Final","","Scopus","2-s2.0-85081137937"
"Li J.; Huo H.; Liu K.; Li C.; Li S.; Yang X.","Li, Jing (57207844651); Huo, Hongtao (56527849900); Liu, Kejian (57208860186); Li, Chang (56718731300); Li, Shuo (57248288100); Yang, Xin (57212206381)","57207844651; 56527849900; 57208860186; 56718731300; 57248288100; 57212206381","Infrared and visible image fusion via multi-discriminators wasserstein generative adversarial network","2019","Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019","","","8999112","2014","2019","5","10.1109/ICMLA.2019.00322","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080956021&doi=10.1109%2fICMLA.2019.00322&partnerID=40&md5=b8b7350813148ff6a2d0b95c7f934d76","Generative adversarial network (GAN) has been widely applied to infrared and visible image fusion. However, the existing GAN-based image fusion methods only establish one discriminator in the network to make the fused image capture gradient information from the visible image, which may result in the loss of some infrared intensity information and texture information on the fused images. To solve this problem and improve the performance of GAN, we extend GAN to multiple discriminators and propose an end-to-end multi-discriminators Wasserstein generative adversarial network (MD-WGAN). In this framework, the fused image can preserve major infrared intensity and detail information from the first discriminator, and keep more texture information that existing in visible image from the second discriminator. We also design a texture loss function via local binary patterns to preserve more texture from visible image. The extensive qualitative and quantitative experiments show the advantages of our method compared with other state-of-the-art fusion methods. © 2019 IEEE.","Discriminators; Image fusion; Infrared imaging; Machine learning; Textures; Adversarial networks; Gradient informations; Image fusion methods; Infrared and visible image; Local binary patterns; Quantitative experiments; Texture information; Visible image; Image texture","Image fusion; Infrared image; Visible image; Wasserstein generative adversarial network","Conference paper","Final","","Scopus","2-s2.0-85080956021"
"Wang W.-L.; Yang S.-L.; Zhao Y.-W.; Li Z.-R.","Wang, Wan-Liang (35790696300); Yang, Sheng-Lan (57212278001); Zhao, Yan-Wei (7406636894); Li, Zhuo-Rong (57202927569)","35790696300; 57212278001; 7406636894; 57202927569","Estimation of river surface flow velocity based on conditional boundary equilibrium generative adversarial network; [基于条件边界平衡生成对抗网络的河流表面流速估测]","2019","Zhejiang Daxue Xuebao (Gongxue Ban)/Journal of Zhejiang University (Engineering Science)","53","11","","2118","2128","10","10.3785/j.issn.1008-973X.2019.11.009","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076375401&doi=10.3785%2fj.issn.1008-973X.2019.11.009&partnerID=40&md5=e7a1a61639265240b2798be96825cb92","Aiming at the difficulty in image classification due to the high similarity between different flow speeds, a conditional boundary equilibrium generative adversarial network and a convolutional classification network based on the multi-feature fusion were proposed to realize the generation and the classification of flow velocity images, respectively. A labeling mechanism and a verification module were introduced to realize the fitting and generation of corresponding category images, in order to achieve data enhancement. To enhance the impact of different texture features on velocity estimation, a multi-feature fusion layer was introduced to realize the feature extraction and the flow velocity recognition so as to realize the classification for images with small differences. The proposed method was applied to the actual river surface velocity estimation. Results demonstrate that the added tag information and the verification module can guide the data generation of corresponding class to a certain extent in the image generation module. Compared with other methods, the multi-feature fusion mechanism makes the proposed classifier more robust in identifying flow velocity images with small differences. © 2019, Zhejiang University Press. All right reserved.","Flow velocity; Image classification; Image enhancement; Image fusion; Textures; Velocity; Adversarial networks; Boundary equilibrium; Classification networks; Feature fusion; Flow rate estimations; Image verification; Multi-feature fusion; Velocity recognition; Classification (of information)","Feature fusion; Flow rate estimation; Generative adversarial network; Image verification; Velocity category","Article","Final","","Scopus","2-s2.0-85076375401"
"","","","10th International Conference on Image and Graphics, ICIG 2019","2019","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11901 LNCS","","","","","2248","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076903085&partnerID=40&md5=db3b181d08cc97d09383a34f1f6dc4a3","The proceedings contain 183 papers. The special focus in this conference is on Image and Graphics. The topics include: Image Dehazing Framework Using Brightness-Area Suppression Mechanism; Parallel-Structure-based Transfer Learning for Deep NIR-to-VIS Face Recognition; MF-SORT: Simple Online and Realtime Tracking with Motion Features; semantic Segmentation of Street Scenes Using Disparity Information; Pulmonary DR Image Anomaly Detection Based on Deep Learning; A Stackable Attention-Guided Multi-scale CNN for Number Plate Detection; residual Joint Attention Network with Graph Structure Inference for Object Detection; saliency Detection Based on Foreground and Background Propagation; online Handwritten Diagram Recognition with Graph Attention Networks; Object Detection for Chinese Traditional Costume Images Based GRP-DSOD++ Network; CNN-Based Erratic Cigarette Code Recognition; modified Capsule Network for Object Classification; insulator Segmentation Based on Community Detection and Hybrid Feature; saliency Detection Based on Manifold Ranking and Refined Seed Labels; hierarchical Convolution Feature for Target Tracking with Kernel-Correlation Filtering; non-negative Representation Based Discriminative Dictionary Learning for Face Recognition; iterative Face Detection from the Global to Local; challenges Driven Network for Visual Tracking; towards Photo-Realistic Visible Watermark Removal with Conditional Generative Adversarial Networks; Online Detection of Welding Quality Based on ZYNQ and Data Mining; combining Cross Entropy Loss with Manually Defined Hard Example for Semantic Image Segmentation; visual Tracking with Attentional Convolutional Siamese Networks; enhanced Video Segmentation with Object Tracking; Infrared and Visible Image Fusion Using NSCT and Convolutional Sparse Representation; a Weakly Supervised Text Detection Based on Attention Mechanism; proposal-Refined Weakly Supervised Object Detection in Underwater Images; blurred Template Matching Based on Cascaded Network.","","","Conference review","Final","","Scopus","2-s2.0-85076903085"
"Lei D.; Zhang C.; Li Z.; Wu Y.","Lei, Dajiang (36627356400); Zhang, Ce (57216935063); Li, Zhixing (56181457200); Wu, Yu (56468297100)","36627356400; 57216935063; 56181457200; 56468297100","Remote Sensing Image Fusion Based on Generative Adversarial Network with Multi-stream Fusion Architecture; [基于多流融合生成对抗网络的遥感图像融合方法]","2020","Dianzi Yu Xinxi Xuebao/Journal of Electronics and Information Technology","42","8","","1942","1949","7","10.11999/JEIT17_190273","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089700151&doi=10.11999%2fJEIT17_190273&partnerID=40&md5=45ddf43c40faef4e31b10888e507a93d","The generative adversarial network receives extensive attention in the study of computer vision such as image fusion and image super-resolution, due to its strong ability of generating high quality images. At present, the remote sensing image fusion method based on generative adversarial network only learns the mapping between the images, and lacks the unique Pan-sharpening domain knowledge. This paper proposes a remote sensing image fusion method based on optimized generative adversarial network with the integration of the spatial structure information of panchromatic image. The proposed algorithm extracts the spatial structure information of the panchromatic image by the gradient operator. The extracted feature would be added to both the discriminator and the generator which uses a multi-stream fusion architecture. The corresponding optimization objective and fusion rules are then designed to improve the quality of the fused image. Experiments on images acquired by WorldView-3 satellites demonstrate that the proposed method can generate high quality fused images, which is better than the most of advanced remote sensing image fusion methods in both subjective visual and objective evaluation indicators.","Image enhancement; Network architecture; Remote sensing; Adversarial networks; Fusion architecture; High quality images; Image super resolutions; Objective evaluation; Panchromatic images; Remote sensing images; Spatial structure information; Image fusion","Computer vision; Generative adversarial network; Multi-stream fusion architecture; Remote sensing image fusion","Article","Final","","Scopus","2-s2.0-85089700151"
"Yamaguchi S.; Kanai S.; Eda T.","Yamaguchi, Shin’ya (57219583496); Kanai, Sekitoshi (57202061107); Eda, Takeharu (57207935951)","57219583496; 57202061107; 57207935951","Effective data augmentation with multi-domain learning GANs","2020","AAAI 2020 - 34th AAAI Conference on Artificial Intelligence","","","","6566","6574","8","","6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098393497&partnerID=40&md5=e7a11dfac9fcba37fd431951c7990a9b","For deep learning applications, the massive data development (e.g., collecting, labeling), which is an essential process in building practical applications, still incurs seriously high costs. In this work, we propose an effective data augmentation method based on generative adversarial networks (GANs), called Domain Fusion. Our key idea is to import the knowledge contained in an outer dataset to a target model by using a multi-domain learning GAN. The multi-domain learning GAN simultaneously learns the outer and target dataset and generates new samples for the target tasks. The simultaneous learning process makes GANs generate the target samples with high fidelity and variety. As a result, we can obtain accurate models for the target tasks by using these generated samples even if we only have an extremely low volume target dataset. We experimentally evaluate the advantages of Domain Fusion in image classification tasks on 3 target datasets: CIFAR-100, FGVC-Aircraft, and Indoor Scene Recognition. When trained on each target dataset reduced the samples to 5,000 images, Domain Fusion achieves better classification accuracy than the data augmentation using fine-tuned GANs. Furthermore, we show that Domain Fusion improves the quality of generated samples, and the improvements can contribute to higher accuracy. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Classification (of information); Deep learning; Image fusion; Adversarial networks; Classification accuracy; Data augmentation; High-fidelity; Learning process; Massive data; Multi domains; Scene recognition; Learning systems","","Conference paper","Final","","Scopus","2-s2.0-85098393497"
"Zhou H.; Liu Q.; Wang Y.","Zhou, Huanyu (57221308389); Liu, Qingjie (55534263100); Wang, Yunhong (34870959400)","57221308389; 55534263100; 34870959400","PGMAN: An Unsupervised Generative Multiadversarial Network for Pansharpening","2021","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14","","9463717","6316","6327","11","10.1109/JSTARS.2021.3090252","20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112092413&doi=10.1109%2fJSTARS.2021.3090252&partnerID=40&md5=07a4a9c593ce5490817550cafe27a818","Pansharpening aims at fusing a low-resolution multispectral (MS) image and a high-resolution (HR) panchromatic (PAN) image acquired by a satellite to generate an HR MS image. Many deep learning based methods have been developed in the past few years. However, since there are no intended HR MS images as references for learning, almost all of the existing methods downsample the MS and PAN images and regard the original MS images as targets to form a supervised setting for training. These methods may perform well on the down-scaled images; however, they generalize poorly to the full-resolution images. To conquer this problem, we design an unsupervised framework that is able to learn directly from the full-resolution images without any preprocessing. The model is built based on a novel generative multiadversarial network. We use a two-stream generator to extract the modality-specific features from the PAN and MS images, respectively, and develop a dual discriminator to preserve the spectral and spatial information of the inputs when performing fusion. Furthermore, a novel loss function is introduced to facilitate training under the unsupervised setting. Experiments and comparisons with other state-of-the-art methods on GaoFen-2, QuickBird, and WorldView-3 images demonstrate that the proposed method can obtain much better fusion results on the full-resolution images.  © 2012 IEEE.","Deep learning; Image fusion; Learning systems; Full resolution images; High resolution; Learning-based methods; Low resolution; Multispectral images; Panchromatic (Pan) image; Spatial informations; State-of-the-art methods; image processing; image resolution; panchromatic image; QuickBird; satellite imagery; spectral resolution; unsupervised classification; WorldView; Image processing","Generative adversarial network (GAN); image fusion; pansharpening; unsupervised learning","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85112092413"
"Kim J.-W.; Ryu J.-H.; Kim J.-O.","Kim, Jae-Woo (37106991100); Ryu, Je-Ho (57191056429); Kim, Jong-Ok (36118022200)","37106991100; 57191056429; 36118022200","Deep gradual flash fusion for low-light enhancement","2020","Journal of Visual Communication and Image Representation","72","","102903","","","","10.1016/j.jvcir.2020.102903","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091476698&doi=10.1016%2fj.jvcir.2020.102903&partnerID=40&md5=a2b59e0da348292e69274eda1d98cb41","In this paper, we propose gradual flash fusion, a new imaging concept that enables acquisition of pseudo multi-exposure images in a passive manner. This means that our gradual flash capture does not require any user-side manipulation (taking multiple shots or varying camera settings). Continuous high-speed capture naturally contains different intensities of flash in a single shooting. The captured gradual flash images, containing different information of the same scene, are fused to generate higher-quality images, especially in a low light scenario. For gradual flash fusion, we use a Generative Adversarial Network (GAN) based approach, where the generator is a tailored convolutional Auto-Encoder for image fusion. For the training, we build a custom dataset comprising gradual flash images and corresponding ground truths. This enables supervised learning, unlike most conventional image fusion studies. Experimental results demonstrate that gradual flash fusion achieves artifact-free and noise-free results resembling ground truth, owing to supervised adversarial fusion. © 2020 Elsevier Inc.","Image processing; Visual communication; Adversarial networks; Auto encoders; Camera settings; Ground truth; High Speed; Low light; Multi-exposure images; Quality image; Image fusion","Auto-encoder; Flash fusion; GAN; Image fusion; Low light enhancement; Pseudo multi-exposure","Article","Final","","Scopus","2-s2.0-85091476698"
"Liao B.; Du Y.; Yin X.","Liao, Bin (23491650600); Du, You (57216220613); Yin, Xiangyun (57216220280)","23491650600; 57216220613; 57216220280","Fusion of Infrared-Visible Images in UE-IoT for Fault Point Detection Based on GAN","2020","IEEE Access","8","","9078757","79754","79763","9","10.1109/ACCESS.2020.2990539","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084933876&doi=10.1109%2fACCESS.2020.2990539&partnerID=40&md5=70eb88a514577f41d10718755815e20c","In the development of modern intelligent Ubiquitous Electric Internet of Things (UE-IoT), infrared thermal imaging always plays an important role in automated early-warning detection of developing failures of critical assets such as transformers, disconnects and capacity banks in electrical power substation non-intrusively. However, the low resolution and contrast of infrared images hinder the subsequent analysis and recognition of fault points. In contrast, visible images present abundant texture details of the equipment without thermal information. In order to assist the detection of fault points, this paper proposes a Generative adversarial networks (GAN) based infrared and visible image fusion method to produce a composite image with enhanced edges and better quality. The edge loss function is added to represent the perceptual edges. In the discriminator, the proposed method improves the texture similarity between fusion image and visible image by minimizing the Wasserstein distance in VGG (Visual Geometry Group network) feature space. The experimental results show that the fault regions become more salient and the details are enhanced. In this way, it can facilitate the detection of fault points both reliably and accurately. © 2013 IEEE.","Electric substations; Fault detection; Image fusion; Image texture; Infrared imaging; Internet of things; Textures; Adversarial networks; Composite images; Electrical power substations; Infrared and visible image; Infrared thermal imaging; Perceptual edges; Texture similarity; Wasserstein distance; Image enhancement","generative adversarial networks; Image fusion; ubiquitous electric Internet of Things; VGG","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85084933876"
"","","","2nd Chinese Conference on Computer Vision, CCCV 2017","2017","Communications in Computer and Information Science","772","","","","","1377","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038019215&partnerID=40&md5=63cbdb532ddc908d7392462faa549b91","The proceedings contain 113 papers. The special focus in this conference is on Computer Vision. The topics include: Visual saliency fusion based multi-feature for semantic image retrieval; unsupervised multi-view subspace learning via maximizing dependence; structured multi-view supervised feature selection algorithm research; an automatic shoeprint retrieval method using neural codes for commercial shoeprint scanners; uncovering the effect of visual saliency on image retrieval; shape-color differential moment invariants under affine transforms; a novel layer based image fusion approach via transfer learning and coupled dictionary; local saliency extraction for fusion of visible and infrared images; distributed compressive sensing for light field reconstruction using structured random matrix; improved face verification with simple weighted feature combination; stereoscopic image quality assessment based on binocular adding and subtracting; quality assessment of palm vein image using natural scene statistics; an error-activation-guided blind metric for stitched panoramic image quality assessment; image aesthetic quality evaluation using convolution neural network embedded fine-tune; high capacity reversible data hiding with contrast enhancement; rank learning for dehazed image quality assessment; a low-rank total-variation regularized tensor completion algorithm; nighttime haze removal with fusion atmospheric light and improved entropy; light field super-resolution using cross-resolution input based on patchmatch and learning method; a new image sparse reconstruction method for mixed Gaussian-poisson noise with multiple constraints; pore-scale facial features matching under 3D morphable model constraint; face video super-resolution with identity guided generative adversarial networks; exemplar-based pixel by pixel inpainting based on patch shift; GAN based sample simulation for SEM-image super resolution; PSO-based single image defogging.","","","Conference review","Final","","Scopus","2-s2.0-85038019215"
"Ma J.; Xu H.; Jiang J.; Mei X.; Zhang X.-P.","Ma, Jiayi (26638975600); Xu, Han (57201056465); Jiang, Junjun (54902306100); Mei, Xiaoguang (55800813300); Zhang, Xiao-Ping (35214025100)","26638975600; 57201056465; 54902306100; 55800813300; 35214025100","DDcGAN: A Dual-Discriminator Conditional Generative Adversarial Network for Multi-Resolution Image Fusion","2020","IEEE Transactions on Image Processing","29","","9031751","4980","4995","15","10.1109/TIP.2020.2977573","292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082061404&doi=10.1109%2fTIP.2020.2977573&partnerID=40&md5=82b3819d0f140707faea4f0fce666325","In this paper, we proposed a new end-to-end model, termed as dual-discriminator conditional generative adversarial network (DDcGAN), for fusing infrared and visible images of different resolutions. Our method establishes an adversarial game between a generator and two discriminators. The generator aims to generate a real-like fused image based on a specifically designed content loss to fool the two discriminators, while the two discriminators aim to distinguish the structure differences between the fused image and two source images, respectively, in addition to the content loss. Consequently, the fused image is forced to simultaneously keep the thermal radiation in the infrared image and the texture details in the visible image. Moreover, to fuse source images of different resolutions, e.g., a low-resolution infrared image and a high-resolution visible image, our DDcGAN constrains the downsampled fused image to have similar property with the infrared image. This can avoid causing thermal radiation information blurring or visible texture detail loss, which typically happens in traditional methods. In addition, we also apply our DDcGAN to fusing multi-modality medical images of different resolutions, e.g., a low-resolution positron emission tomography image and a high-resolution magnetic resonance image. The qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our DDcGAN over the state-of-the-art, in terms of both visual effect and quantitative metrics. Our code is publicly available at https://github.com/jiayi-ma/DDcGAN. © 1992-2012 IEEE.","Discriminators; Heat radiation; Infrared imaging; Magnetic resonance imaging; Medical imaging; Positron emission tomography; Textures; Adversarial networks; Different resolutions; High resolution visible; Infrared and visible image; Multiresolution images; Quantitative experiments; Quantitative metrics; Radiation information; Image fusion","different resolutions; generative adversarial network; Image fusion; infrared image; medical image","Article","Final","","Scopus","2-s2.0-85082061404"
"Yang C.; Eschweiler D.; Stegmaier J.","Yang, Canyu (57232724000); Eschweiler, Dennis (57194396472); Stegmaier, Johannes (55584699900)","57232724000; 57194396472; 55584699900","Semi- and Self-supervised Multi-view Fusion of 3D Microscopy Images Using Generative Adversarial Networks","2021","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12964 LNCS","","","130","139","9","10.1007/978-3-030-88552-6_13","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116937914&doi=10.1007%2f978-3-030-88552-6_13&partnerID=40&md5=ea265b6f990759e10bc9b0af62845ccc","Recent developments in fluorescence microscopy allow capturing high-resolution 3D images over time for living model organisms. To be able to image even large specimens, techniques like multi-view light-sheet imaging record different orientations at each time point that can then be fused into a single high-quality volume. Based on measured point spread functions (PSF), deconvolution and content fusion are able to largely revert the inevitable degradation occurring during the imaging process. Classical multi-view deconvolution and fusion methods mainly use iterative procedures and content-based averaging. Lately, Convolutional Neural Networks (CNNs) have been deployed to approach 3D single-view deconvolution microscopy, but the multi-view case waits to be studied. We investigated the efficacy of CNN-based multi-view deconvolution and fusion with two synthetic data sets that mimic developing embryos and involve either two or four complementary 3D views. Compared with classical state-of-the-art methods, the proposed semi- and self-supervised models achieve competitive and superior deconvolution and fusion quality in the two-view and quad-view cases, respectively. © 2021, Springer Nature Switzerland AG.","Fluorescence microscopy; Image fusion; Iterative methods; Medical imaging; Optical transfer function; 3-D microscopy; Convolutional neural network; Deconvolutions; Image deconvolution; Light-sheet microscopies; Microscopy images; Multi-view fusion; Multi-view light-sheet microscopy; Multi-views; Convolution","Convolutional Neural Networks; Image deconvolution; Multi-view fusion; Multi-view light-sheet microscopy","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85116937914"
"Yang X.; Lin S.","Yang, Xiaoli (57210203737); Lin, Suzhen (7407607523)","57210203737; 7407607523","Method for multi-band image feature-level fusion based on the attention mechanism; [一种注意力机制的多波段图像特征级融合方法]","2020","Xi'an Dianzi Keji Daxue Xuebao/Journal of Xidian University","47","1","","120","127","7","10.19665/j.issn1001-2400.2020.01.017","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082679338&doi=10.19665%2fj.issn1001-2400.2020.01.017&partnerID=40&md5=93a2611e3bfd3634a8e9f681b4bc4bb6","Aiming at the low definition and poor details of synchronous multi-band image fusion, a novel method based on attention generative adversarial networks is proposed. First, the attention weight map is constructed using the difference between the multi-band feature map and its mean, then the feature enhancement map is obtained by the point multiplication and addition of the feature map and the attention weight map to construct the feature enhancement module. Second, the feature-level fusion module is designed, which connects the multi-band feature enhancement map and reconstructs the fused image through normalization, upsampling, convolution, etc. Finally, the feature enhancement module and the feature-level fusion module are cascaded to build the generator, and the VGG-16 is used as a discriminator to establish a Generative Adversarial Network, thereby implementing multi-band image end-to-end fusion. Experimental results show that the proposed fusion method can lead to the most prominent average gradient compared with classical fusion methods, and that the effectiveness of the proposed method is verified. © 2020, The Editorial Board of Journal of Xidian University. All right reserved.","Image enhancement; Adversarial networks; Attention mechanisms; Average gradient; Feature enhancement; Feature level fusion; Fusion methods; Multi-band images; Point multiplication; Image fusion","Attention; Deep learning; Feature-level fusion; Generative adversarial networks; Image fusion; Multi-band image","Article","Final","","Scopus","2-s2.0-85082679338"
"Wang L.; Chang C.; Hao B.; Liu C.","Wang, Lei (57213182331); Chang, Chunhong (57216896523); Hao, Benli (57221831950); Liu, Chunxiang (56416711200)","57213182331; 57216896523; 57221831950; 56416711200","Multi-modal Medical Image Fusion Based on GAN and the Shift-Invariant Shearlet Transform","2020","Proceedings - 2020 IEEE International Conference on Bioinformatics and Biomedicine, BIBM 2020","","","9313288","2538","2543","5","10.1109/BIBM49941.2020.9313288","5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100348730&doi=10.1109%2fBIBM49941.2020.9313288&partnerID=40&md5=664d9b25cd0c013b73f3393c6ccde754","The typical medical image fusion methods are of great practicability in computer-aided medical diagnosis, however, they are facing with some confusing disadvantages, for example, the loss of the important features, the inefficiency of the used fusion rules. In order to obtain the better fusion results to facilitate the doctors to read the comprehensive medical information, a novel medical image fusion method based on the generative adversarial network (GAN) and the shift-invariant shearlet transform (SIST) is developed. The source images are firstly decomposed into the high-pass and low-pass sub-bands by the SIST. Then, the combination of the high-pass sub-bands in different levels is implemented by the trained GAN model, which mainly consists of two parts: the construction of the generator and the discriminator; for the low-frequency coefficients, the local energy weighted summation and bilateral filter are used to calculate the low-pass details by the preserving of the local energy. Finally, the fusion results can be shown by inversion of the SIST. Under the subjective comparison and four typical objective measurements, i.e. the standard deviation, entropy, \mathrm{Q}^{\mathrm{A}\mathrm{B}/\mathrm{F}}, and the mutual information, the experimental results indicate that the artifacts and distortions can be effectively suppressed and the detail information can be well preserved by the proposed method, as well as the better quantitative performance. © 2020 IEEE.","Bioinformatics; Computer aided diagnosis; Convolutional neural networks; Medical imaging; Adversarial networks; Important features; Medical information; Mutual informations; Objective measurement; Shearlet transforms; Standard deviation; Weighted summations; Image fusion","Bilateral filter; Generative adversarial network; Medical image fusion; shift-invariant shearlet transform","Conference paper","Final","","Scopus","2-s2.0-85100348730"
"Zhao L.; Wang Y.; Duan Z.; Chen D.; Liu S.","Zhao, Liang (57190893962); Wang, Ying (57226199809); Duan, Zhongxing (12141355000); Chen, Dengfeng (24921289200); Liu, Shipeng (57226182535)","57190893962; 57226199809; 12141355000; 24921289200; 57226182535","Multi-Source Fusion Image Semantic Segmentation Model of Generative Adversarial Networks Based on FCN","2021","IEEE Access","9","","9483944","101985","101993","8","10.1109/ACCESS.2021.3097054","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110807870&doi=10.1109%2fACCESS.2021.3097054&partnerID=40&md5=ae63b2d607340a51b5f08c2beb106447","At present, most of the methods used in the research of image semantic segmentation ignore the low-level feature information of image, such as space, edge, etc., which leads to the problems that the segmentation of edge and small part is not precise enough and the accuracy of segmentation result is not high. To solve this problem, this paper proposes a multi-source fusion image semantic segmentation model of generative adversarial networks based on FCN: SCAGAN. In VGG19 network, add super-pixel and edge detection algorithm, and introduce the efficient spatial pyramid module to reduce the number of parameters while adding the spatial and edge information of image; Adjust the skipping structure to better integrate the low-level features and high-level features; build a generation model DeepLab-SCFCN combining with the atrous spatial pyramid pooling to better capture the feature information of different scales of the target for segmentation; The FCN with five modules is designed as the discrimination model for GAN. It is verified on the data set PASCAL VOC 2012 that the model achieves IoU of 70.1% with a small number of network layers, and the segmentation effect of edge and small part is better at the same time. This technology can be used in image semantic segmentation.  © 2013 IEEE.","Edge detection; Image fusion; Network layers; Semantic Web; Semantics; Adversarial networks; Discrimination model; Edge detection algorithms; Feature information; High-level features; Low-level features; Multi-source fusion; Segmentation results; Image segmentation","atrous spatial pyramid pooling; efficient spatial pyramid; fully convolutional networks; generative adversarial networks; Image semantic segmentation","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85110807870"
"Ma J.; Yu W.; Liang P.; Li C.; Jiang J.","Ma, Jiayi (26638975600); Yu, Wei (56479633000); Liang, Pengwei (57201500677); Li, Chang (56718731300); Jiang, Junjun (54902306100)","26638975600; 56479633000; 57201500677; 56718731300; 54902306100","FusionGAN: A generative adversarial network for infrared and visible image fusion","2019","Information Fusion","48","","","11","26","15","10.1016/j.inffus.2018.09.004","654","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053147465&doi=10.1016%2fj.inffus.2018.09.004&partnerID=40&md5=e0713ed6e69bd8979031fe5db6da471a","Infrared images can distinguish targets from their backgrounds on the basis of difference in thermal radiation, which works well at all day/night time and under all weather conditions. By contrast, visible images can provide texture details with high spatial resolution and definition in a manner consistent with the human visual system. This paper proposes a novel method to fuse these two types of information using a generative adversarial network, termed as FusionGAN. Our method establishes an adversarial game between a generator and a discriminator, where the generator aims to generate a fused image with major infrared intensities together with additional visible gradients, and the discriminator aims to force the fused image to have more details existing in visible images. This enables that the final fused image simultaneously keeps the thermal radiation in an infrared image and the textures in a visible image. In addition, our FusionGAN is an end-to-end model, avoiding manually designing complicated activity level measurements and fusion rules as in traditional methods. Experiments on public datasets demonstrate the superiority of our strategy over state-of-the-arts, where our results look like sharpened infrared images with clear highlighted targets and abundant details. Moreover, we also generalize our FusionGAN to fuse images with different resolutions, say a low-resolution infrared image and a high-resolution visible image. Extensive results demonstrate that our strategy can generate clear and clean fused images which do not suffer from noise caused by upsampling of infrared information. © 2018 Elsevier B.V.","Deep learning; Heat radiation; Image fusion; Infrared imaging; Adversarial networks; Different resolutions; High resolution visible; High spatial resolution; Human Visual System; Infrared and visible image; Infrared intensity; Visible image; Image texture","Deep learning; Generative adversarial network; Image fusion; Infrared image; Visible image","Article","Final","","Scopus","2-s2.0-85053147465"
