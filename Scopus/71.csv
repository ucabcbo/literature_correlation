"Authors","Author full names","Author(s) ID","Titles","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","DOI","Cited by","Link","Abstract","Indexed Keywords","Author Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Zhu J.; Kelly T.","Zhu, Jialin (57345928000); Kelly, Tom (57196629280)","57345928000; 57196629280","Seamless Satellite-image Synthesis","2021","Computer Graphics Forum","40","7","","193","204","11","10.1111/cgf.14413","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119963462&doi=10.1111%2fcgf.14413&partnerID=40&md5=94f642ee172e0aca5b75dee237ffea26","We introduce Seamless Satellite-image Synthesis (SSS), a novel neural architecture to create scale-and-space continuous satellite textures from cartographic data. While 2D map data is cheap and easily synthesized, accurate satellite imagery is expensive and often unavailable or out of date. Our approach generates seamless textures over arbitrarily large spatial extents which are consistent through scale-space. To overcome tile size limitations in image-to-image translation approaches, SSS learns to remove seams between tiled images in a semantically meaningful manner. Scale-space continuity is achieved by a hierarchy of networks conditioned on style and cartographic data. Our qualitative and quantitative evaluations show that our system improves over the state-of-the-art in several key areas. We show applications to texturing procedurally generation maps and interactive satellite image manipulation. © 2021 The Author(s) Computer Graphics Forum © 2021 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.","Image processing; Textures; Cartographic data; CCS concept; Computing methodologies; Images processing; Images synthesis; Map data; Neural architectures; Satellite images; Scale spaces; • computing methodology → texturing; Satellite imagery","CCS Concepts; Image processing; • Computing methodologies → Texturing","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85119963462"
"","","","CEUR Workshop Proceedings","2017","CEUR Workshop Proceedings","1814","","","","","106","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017109142&partnerID=40&md5=2191650923ebd46528638b258b7bb1b3","The proceedings contain 12 papers. The topics discussed include: 2D and 3D density block models creation based on isostasy usage; filtration and restoration of satellite images using doubly stochastic random fields; analysis of cranes control processes for converter production based on simulation; irregular objects. shape detection and characteristic sizes; 3D radio holographic images synthesis and filtration on multiprocessor computing systems; dielectric permittivity and permeability measurement system; visual metaphor of mathematical abstractions and their visualization through newly uprised PDF-document facilities; estimation of spring stiffness under conditions of uncertainty. interval approach; inkjet printers linearization using 3D gradation curves; optimal scanning of Gaussian and fractal Brownian images with an estimation of correlation dimension; the usage of optical flow algorithm to the problem of recovery contour of the left ventricle of the human heart on the ultrasound image data; and InSAR data coherence estimation using 2D fast Fourier transform.","","","Conference review","Final","","Scopus","2-s2.0-85017109142"
"Maniraj Kumar P.; Thirumurugan P.; Karthikeyan P.; Saravanan S.; Kumarnath J.","Maniraj Kumar, P. (56006722000); Thirumurugan, P. (56087871100); Karthikeyan, P. (57211774744); Saravanan, S. (57968968500); Kumarnath, J. (57211803990)","56006722000; 56087871100; 57211774744; 57968968500; 57211803990","Image synthesis M/2D/HWT in VLSI technology","2019","International Journal of Innovative Technology and Exploring Engineering","9","1","","2976","2982","6","10.35940/ijitee.A9119.119119","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075072229&doi=10.35940%2fijitee.A9119.119119&partnerID=40&md5=923c3d3348862a80d52d06535a71a0dd","Image synthesis is grouping of valid information from a group of images in to unique image. The ensuing image is an improved quality than any other images. The spectral deformation major con in standard method. The different multiscale transforms are proposed the overcome the issue. The image is affected by impulse noise because of satellite images, are filtered the impulse noise in the image syntehesis. In this paper we studied the architecture of Edge preserve algorithm which good in PSNR and MSE. The proposed technique using wavelet decomposition is implemented in Matlab for low resolution images are multispectral image and high resolution image are panchromatic image and then we combined the synthesized image in Altera Cyclone. The result shows the significant area and power. The synthesis image has a colour combination which is effective than any other in set of images. © BEIESP.","","Edge preserving Filter; Haar Wavelet; Image Fusion; Impulse Noise","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85075072229"
"","","","IS and T International Symposium on Electronic Imaging Science and Technology","2017","IS and T International Symposium on Electronic Imaging Science and Technology","","","","","","162","","0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040607260&partnerID=40&md5=2746976b1db69af7f19bc510fd88a9ce","The proceedings contain 22 papers. The topics discussed include: texture representations in different basis functions for image synthesis using system criteria analysis; 2-D octonion discrete Fourier transform: fast algorithms; full-reference metrics multi-distortional analysis; a robust line segmentation for Arabic printed text with diacritics; refining raw pixel values using a value error model to drive texture synthesis; color interpolation based on colorization for RGB-white color filter array; video frame synthesizing method for HDR video capturing system with four image sensors; robust defect pixel detection and correction for Bayer imaging systems; cloud and shadow detection using sequential characteristics on multispectral satellite images; thermal facial signatures for state assessment during deception; and alpha-rooting method of gray-scale image enhancement in the quaternion frequency domain.","","","Conference review","Final","","Scopus","2-s2.0-85040607260"
"Bejiga M.B.; Melgani F.; Vascotto A.","Bejiga, Mesay Belete (57192697078); Melgani, Farid (35613488300); Vascotto, Antonio (57208124510)","57192697078; 35613488300; 57208124510","Retro-Remote Sensing: Generating Images from Ancient Texts","2019","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12","3","8660422","950","960","10","10.1109/JSTARS.2019.2895693","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063894914&doi=10.1109%2fJSTARS.2019.2895693&partnerID=40&md5=300d238b0b3891057325948999c8e038","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area. © 2008-2012 IEEE.","Deep learning; Deep neural networks; Image processing; Neural networks; Adversarial networks; Convolutional neural network; Geographical area; Image synthesis; Multi-modal learning; Multimodal sources; Remote sensing images; Statistical properties; artificial neural network; civilization; geographical variation; image analysis; learning; pixel; remote sensing; Remote sensing","Convolutional neural networks (CNN); deep learning; generative adversarial networks (GAN); multimodal learning; remote sensing; text-to-image synthesis","Article","Final","","Scopus","2-s2.0-85063894914"
"Zexing Z.; Qizhi X.; Haibo W.; Wenyong Y.","Zexing, Zhao (57202311139); Qizhi, Xu (50562407300); Haibo, Wang (57681846600); Wenyong, Yu (57202309456)","57202311139; 50562407300; 57681846600; 57202309456","High-reflectivity objects distributed optical satellite image fusion based on NDVI classification","2017","Proceedings of 2017 2nd International Conference on Frontiers of Sensors Technologies, ICFST 2017","2017-January","","","231","235","4","10.1109/ICFST.2017.8210509","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047793500&doi=10.1109%2fICFST.2017.8210509&partnerID=40&md5=eccca03a328a30ceba7bc11aea7a449a","Ratioing method is one type of the most famous fusion methods in remote sensing image fusion domain. Generally, the ratioing method synthesizes a low-resolution panchromatic (Pan) image by adaptive weighted summation of a multispectral (MS) image. Consequently, the accuracy of the weights for low-resolution Pan image synthesis is of great importance. However, in most cases, the optical satellite images contain lots of high-reflectivity objects, such as clouds covered regions, and high-reflectivity buildings. These objects are saturate due to their strong reflectance. The distortion of saturated objects results in the failure of weights calculation, so that causes the color distortion of fused images. To solve the problem, this paper proposes a high-reflectivity objects distributed optical satellite image fusion method based on NDVI classification. First, the NDVI index is employed to classify the pixels of a MS image into high-reflectivity group and normal group, then the pixels in normal group is used to calculate the weighted coefficients, finally the fused image is obtained by ratioing transform. Experimental results on a large number of test images show that the proposed method has good performance on reducing color distortion. © 2017 IEEE.","Adaptive optics; Image classification; Pixels; Reflection; Remote sensing; Satellites; Multispectral images; NDVI index; Optical satellite images; Pan-sharpening; Panchromatic (Pan) image; Remote sensing images; Weighted coefficients; Weights calculation; Image fusion","Image fusion; NDVI index; Pan-sharpening; Remote sensing image","Conference paper","Final","","Scopus","2-s2.0-85047793500"
"Lee K.-Y.; Sim J.-Y.","Lee, Kyu-Yul (56770317600); Sim, Jae-Young (7202819179)","56770317600; 7202819179","Cloud Removal of Satellite Images Using Convolutional Neural Network with Reliable Cloudy Image Synthesis Model","2019","Proceedings - International Conference on Image Processing, ICIP","2019-September","","8803666","3581","3585","4","10.1109/ICIP.2019.8803666","12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076800275&doi=10.1109%2fICIP.2019.8803666&partnerID=40&md5=4460e3e496c2c3cb146013d4d3472603","Cloudy pixels in satellite images degrade the visibility of captured surface structure. We propose a novel cloudy image synthesis model and develop a cloud removal algorithm using convolutional neural network. We extract the cloud masks from real cloudy satellite images and from real sky images with clouds. Then we investigate the characteristics of real cloudy images and devise a reliable cloudy image synthesis model which considers the background surface color, misalignement of channel images, and blur in clouds. We train a hierarchical cloud removal network using the synthetic cloudy images. Experimental results demonstrate that the proposed algorithm removes the clouds from cloudy satellite images faithfully and outperforms the existing methods. © 2019 IEEE.","","Cloud removal; convolutional neural network; satellite image","Conference paper","Final","","Scopus","2-s2.0-85076800275"
"Tian X.; Shao J.; Ouyang D.; Shen H.T.","Tian, Xiaoyang (57802794400); Shao, Jie (57002035900); Ouyang, Deqiang (57193701886); Shen, Heng Tao (7404523209)","57802794400; 57002035900; 57193701886; 7404523209","UAV-Satellite View Synthesis for Cross-View Geo-Localization","2022","IEEE Transactions on Circuits and Systems for Video Technology","32","7","","4804","4815","11","10.1109/TCSVT.2021.3121987","11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134174015&doi=10.1109%2fTCSVT.2021.3121987&partnerID=40&md5=8be1fbbeeca02c50825f61609dc4a728","The goal of cross-view image matching based on geo-localization is to determine the location of a given ground-view image (front view) by matching it with a group of satellite-view images (vertical view) with geographic tags. Due to the rapid development of unmanned aerial vehicle (UAV) technology in recent years, it has provided a real viewpoint close to 45 degrees (oblique view) to bridge the visual gap between views. However, existing methods ignore the direct geometric space correspondence of UAV-satellite views, and only use brute force for feature matching, leading to inferior performance. In this context, we propose an end-to-end cross-view matching method that integrates cross-view synthesis module and geo-localization module, which fully considers the spatial correspondence of UAV-satellite views and the surrounding area information. To be specific, the cross-view synthesis module includes two parts: the oblique view of UAV is first converted to the vertical view by perspective projection transformation (PPT), which makes the UAV image closer to the satellite image; then we use conditional generative adversarial nets (CGAN) to synthesize the UAV image with vertical view style, which is close to the real satellite image by learning the converted UAV as the input image and the real satellite image as the label. Geo-localization module refers to existing local pattern network (LPN), which explicitly considers the surrounding environment of the target building. These modules are integrated in a single architecture called PCL, which mutually reinforce each other. Our method is superior to the existing UAV-satellite cross-view methods, which improves by about 5%. © 1991-2012 IEEE.","Antennas; Mathematical transformations; Satellites; Unmanned aerial vehicles (UAV); Aerial vehicle; Cross-view image matching; Geo-localisation; Geographics; Images synthesis; Matchings; Oblique view; Satellite images; Vehicle images; View synthesis; Image matching","Cross-view image matching; geo-localization; image synthesis","Article","Final","","Scopus","2-s2.0-85134174015"
"Spick R.; Walker J.A.","Spick, Ryan (57204808077); Walker, James Alfred (57214383954)","57204808077; 57214383954","Realistic and textured terrain generation using GANs","2019","Proceedings - CVMP 2019: 16th ACM SIGGRAPH European Conference on Visual Media Production","","","a3","","","","10.1145/3359998.3369407","2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077331936&doi=10.1145%2f3359998.3369407&partnerID=40&md5=f6352e86cbbf7d72ef6f3a2dd23c54bf","In computer graphics and virtual environment development, a large portion of time is spent creating assets - one of these being the terrain environment, which usually forms the basis of many large graphical worlds. The texturing of height maps is usually performed as a post-processing step - with software requiring access to the height and gradient of the terrain in order to generate a set of conditions for colouring slopes, flats, mountains etc. With further additions such as biomes specifying which predominant texturing the region should exhibit such as grass, snow, dirt etc. much like the real-world. These methods combined with a height map generation algorithm can create impressive terrain renders which look visually stunning - however can appear somewhat repetitive. Previous work has explored the use of variants of Generative Adversarial Networks for the learning of elevation data through real-world data sets of world height data. In this paper, a method is proposed for learning not only the height map values but also the corresponding satellite image of a specific region. This data is trained through a non-spatially dependant generative adversarial network, which can produce an endless amount of variants of a specific region. The textured outputs are measured using existing similarity metrics and compared to the original region, which yields strong results. Additionally, a visual and statistical comparison of other deep learning image synthesis techniques is performed. The network outputs are also rendered in a 3D graphics engine and visualised in the paper. This method produces powerful outputs when compared directly with the training region, creating a tool that can produce many different variants of the target terrain. This is ideally suited for the use of a developer wanting a large number of specific structures of terrain. © 2019 Copyright held by the owner/author(s).","Deep learning; Interactive computer graphics; Rendering (computer graphics); Textures; Virtual reality; Adversarial networks; Image synthesis; Post processing; Procedural content generations; Satellite images; Similarity metrics; Statistical comparisons; Terrain generations; Landforms","Deep Learning; Generative Adversarial Networks; Procedural Content Generation","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85077331936"
"Xu Z.; Wu K.; Ren P.","Xu, Zunxiao (57325014400); Wu, Kang (57324824800); Ren, Peng (57736297300)","57325014400; 57324824800; 57736297300","Recovering Thin Cloud Covered Regions in Gf Satellite Images Based on Cloudy Image Arithmetic +","2022","International Geoscience and Remote Sensing Symposium (IGARSS)","2022-July","","","1800","1803","3","10.1109/IGARSS46834.2022.9884528","1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140413995&doi=10.1109%2fIGARSS46834.2022.9884528&partnerID=40&md5=013ac2b426e70b93a8a3c29c5e9d5e32","We propose the Cloudy Image Arithmetic + (CIA +) for training dataset construction of thin cloud removal, which addresses the deficiency of Cloud Image Arithmetic (CIA) that cloud shadows cannot be simulated. CIA + is able to synthesize cloudy images with cloud shadows, and as in nature, the angle and intensity of the cloud shadows vary depending on the irradiation angle and cloud thickness, which achieves state-of-the-art cloudy image synthesis. The first thin cloud removal dataset on GF satellite (TCR-GF) constructed with CIA + is released to supplement public data for cloud removal. Meanwhile, we propose the Dual-attention MSGAN to remove thin clouds. The network is capable to focus on thin cloud covered regions for the coordinate attention module encodes both channel relationship and long-range dependencies with precise position information. Several qualitative and quantitative experiments validate that the Dual-attention performs excellently on our TCR-GF dataset. © 2022 IEEE.","Computers; Cloud image; Cloud removal; Cloud shadows; Cloudy image arithmetic +; Cloudy shadow synthesis; Coordinate attention; Image-based; Satellite images; Thin cloud removal; Training dataset; Image processing","Cloudy Image Arithmetic + (CIA +); Cloudy Shadows Synthesis; Coordinate Attention; Thin Cloud Removal","Conference paper","Final","","Scopus","2-s2.0-85140413995"
"Lu X.; Li Z.; Cui Z.; Oswald M.R.; Pollefeys M.; Qin R.","Lu, Xiaohu (56828362400); Li, Zuoyue (57215772453); Cui, Zhaopeng (56118660600); Oswald, Martin R. (7004700342); Pollefeys, Marc (7004040532); Qin, Rongjun (55790585000)","56828362400; 57215772453; 56118660600; 7004700342; 7004040532; 55790585000","Geometry-aware satellite-to-ground image synthesis for Urban areas","2020","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","9156602","856","864","8","10.1109/CVPR42600.2020.00094","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089201262&doi=10.1109%2fCVPR42600.2020.00094&partnerID=40&md5=749160f9cb195629f11fee0a22b188a2","We present a novel method for generating panoramic street-view images which are geometrically consistent with a given satellite image. Different from existing approaches that completely rely on a deep learning architecture to generalize cross-view image distributions, our approach explicitly loops in the geometric configuration of the ground objects based on the satellite views, such that the produced ground view synthesis preserves the geometric shape and the semantics of the scene. In particular, we propose a neural network with a geo-transformation layer that turns predicted ground-height values from the satellite view to a ground view while retaining the physical satellite-to-ground relation. Our results show that the synthesized image retains well-articulated and authentic geometric shapes, as well as texture richness of the street-view in various scenarios. Both qualitative and quantitative results demonstrate that our method compares favorably to other state-of-the-art approaches that lack geometric consistency. ©2020 IEEE.","Deep learning; Geometry; Multilayer neural networks; Pattern recognition; Satellites; Semantics; Textures; Geometric configurations; Geometric shape; Image distributions; Learning architectures; Quantitative result; Satellite images; State-of-the-art approach; Synthesized images; Image processing","","Conference paper","Final","","Scopus","2-s2.0-85089201262"
"Günen M.A.","Günen, Mehmet Akif (57190371587)","57190371587","Weighted differential evolution algorithm based pansharpening","2021","International Journal of Remote Sensing","42","22","","8468","8491","23","10.1080/01431161.2021.1976874","4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117145380&doi=10.1080%2f01431161.2021.1976874&partnerID=40&md5=15d9ded9769a2c12d946e13e0d550127","Imaging Satellites acquire multispectral images, MIs, in low resolution, LR, and panchromatic images, PANs, in high resolution, HR, due to some advantages provided in satellite design. The pansharpening, PS, is a super resolution image synthesis method that is used to generate the pansharpened image, PI, by fusion of PAN and MI. The use of PS process is unavoidable in applications such as efficient use of communication-bandwidth of imaging satellites and fusion of images derived from various image sensors. The PS process is a multi-step process consisting of various complex image processing stages, such as registration, resampling, synthesis, and fusion. The Weighted Differential Evolution Algorithm-based PS method, WDEPS, has been proposed in this paper. The WDEPS uses WDE to synthesize the intensity image, which is the blended-image of MI -bands. WDE has been used to compute the relevant image-blending weights, efficiently. In the experiments, several satellite images (QuickBird-2, Ikonos-2, and GeoEye-1) with different spatial resolutions were used. The WDEPS’s experimental results have been compared with 17 well-known PS methods by using 9 full-reference and 3 blind image quality assessment metrics. The experimental results exposed that WDEPS generates high-quality PIs than traditional PS methods used in the experimental aspect of qualitative and quantitative assessment. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","Satellites; Evolutionary algorithms; Image processing; Optimization; Satellite communication systems; Satellites; Differential evolution algorithms; High resolution; Images synthesis; Imaging satellites; Lower resolution; Multispectral images; Pan-sharpening; Resolution images; Satellites design; Superresolution; algorithm; GeoEye; IKONOS; image resolution; multispectral image; panchromatic image; QuickBird; satellite imagery; Image fusion","","Article","Final","","Scopus","2-s2.0-85117145380"
"Sarukkai V.; Jain A.; Uzkent B.; Ermon S.","Sarukkai, Vishnu (57216946562); Jain, Anirudh (57216947092); Uzkent, Burak (36969362600); Ermon, Stefano (35791579200)","57216946562; 57216947092; 36969362600; 35791579200","Cloud removal in satellite images using spatiotemporal generative networks","2020","Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020","","","9093564","1785","1794","9","10.1109/WACV45572.2020.9093564","22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085468190&doi=10.1109%2fWACV45572.2020.9093564&partnerID=40&md5=69b7cadd791f520c3b5a472ac43d4583","Satellite images hold great promise for continuous environmental monitoring and earth observation. Occlusions cast by clouds, however, can severely limit coverage, making ground information extraction more difficult. Existing pipelines typically perform cloud removal with simple temporal composites and hand-crafted filters. In contrast, we cast the problem of cloud removal as a conditional image synthesis challenge, and we propose a trainable spatiotemporal generator network (STGAN) to remove clouds. We train our model on a new large-scale spatiotemporal dataset that we construct, containing 97640 image pairs covering all continents. We demonstrate experimentally that the proposed STGAN model outperforms standard models and can generate realistic cloud-free images with high PSNR and SSIM values across a variety of atmospheric conditions, leading to improved performance in downstream tasks such as land cover classification. © 2020 IEEE.","Computer vision; Large dataset; Atmospheric conditions; Earth observations; Environmental Monitoring; Image synthesis; Land cover classification; Satellite images; Spatio-temporal dataset; Standard model; Image enhancement","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85085468190"
"Puente C.; Olague G.; Trabucchi M.; Arjona-Villicaña P.D.; Soubervielle-Montalvo C.","Puente, Cesar (14034565500); Olague, Gustavo (6602577942); Trabucchi, Mattia (25628782100); Arjona-Villicaña, P. David (36010416600); Soubervielle-Montalvo, Carlos (16048045700)","14034565500; 6602577942; 25628782100; 36010416600; 16048045700","Synthesis of Vegetation Indices using genetic programming for soil erosion estimation","2019","Remote Sensing","11","2","156","","","","10.3390/rs11020156","14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060675312&doi=10.3390%2frs11020156&partnerID=40&md5=f89c588ebb042c01b52f2d6c0f6e3d13","Vegetation Indices (VIs) represent a useful method for extracting vegetation information from satellite images. Erosion models like the Revised Universal Soil Loss Equation (RUSLE), employ VIs as an input to determine the RUSLE soil Cover factor (C). From the standpoint of soil conservation planning, the C factor is one of the most important RUSLE parameters because it measures the combined effect of all interrelated cover and management variables. Despite its importance, the results are generally incomplete because most indices recognize healthy or green vegetation, but not senescent, dry or dead vegetation, which can also be an important contributor to C. The aim of this research is to propose a novel approach for calculating new VIs that are better correlated with C, using field and satellite information. The approach followed by this research is to state the generation of new VIs in terms of a computer optimization problem and then applying a machine learning technique, named Genetic Programming (GP), which builds new indices by iteratively recombining a set of numerical operators and spectral channels until the best composite operator is found. Experimental results illustrate the efficiency and reliability of this approach to estimate the C factor and the erosion rates for two watersheds in Baja California, Mexico, and Zaragoza, Spain. The synthetic indices calculated using this methodology produce better approximation to the C factor from field data, when compared with state-of-the-art indices, like NDVI and EVI. © 2019 by the authors.","Erosion; Evolutionary algorithms; Genetic algorithms; Genetic programming; Image processing; Iterative methods; Learning systems; Sediment transport; Soil conservation; Soils; Vegetation; C factors; Efficiency and reliability; Image synthesis; Machine learning techniques; Revised universal soil loss equations; RUSLE; Satellite information; Vegetation index; C (programming language)","C factor; Evolutionary computation; Genetic programming; Image synthesis; RUSLE; Vegetation indices","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85060675312"
"Scheck L.; Weissmann M.; Mayer B.","Scheck, Leonhard (6507442072); Weissmann, Martin (9940496700); Mayer, Bernhard (57202531041)","6507442072; 9940496700; 57202531041","Efficient methods to account for cloud-top inclination and cloud overlap in synthetic visible Satellite images","2018","Journal of Atmospheric and Oceanic Technology","35","3","","665","685","20","10.1175/JTECH-D-17-0057.1","17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044639479&doi=10.1175%2fJTECH-D-17-0057.1&partnerID=40&md5=57a5d9f4e65c2223a71a4647ba5aeb04","Visible satellite images contain high-resolution information about clouds that would be well suited for convective-scale data assimilation. This application requires a forward operator to generate synthetic images from the output of numerical weather prediction models. Only recently have 1D radiative transfer (RT) solvers become sufficiently fast for this purpose. Here computationally efficient methods are proposed to increase the accuracy and consistency of an operator based on the Method for Fast Satellite Image Synthesis (MFASIS) 1D RT. Two important problems are addressed: the 3D RT effects related to inclined cloud tops and the overlap of subgrid clouds. It is demonstrated that in a rotated frame of reference, an approximate solution for the 3D RT problem can be obtained by solving a computationally much cheaper 1D RT problem. Several deterministic and stochastic schemes that take the overlap of subgrid clouds into account are discussed. The impact of the inclination correction and the overlap schemes is evaluated for synthetic 0.6-μm SEVIRI images computed from operational forecasts of the German-focused COSMO (COSMO-DE) Model for a test period in May-June 2016. The cloud-top inclination correction increases the information content of the synthetic images considerably and reduces systematic errors, in particular for larger solar zenith angles. Taking subgrid cloud overlap into account is essential to avoid large systematic errors. The results obtained using several different 2D cloud overlap schemes are very similar, whereas small but significant differences are found for the most consistent 3D method, which accounts for the fact that the RT problem is solved for columns tilted toward the satellite. © 2018 American Meteorological Society.","Algorithms; Clouds; Radiative transfer; Stochastic systems; Systematic errors; Weather forecasting; Approximate solution; Computationally efficient; Data assimilation; Information contents; Numerical weather prediction models; Operational forecasts; Radiances; Satellite observations; algorithm; data assimilation; imaging method; observational method; radiance; radiative transfer; satellite imagery; weather forecasting; Satellite imagery","Algorithms; Clouds; Data assimilation; Radiances; Radiative transfer; Satellite observations","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85044639479"
"Toker A.; Zhou Q.; Maximov M.; Leal-Taixé L.","Toker, Aysim (57222956742); Zhou, Qunjie (57214450557); Maximov, Maxim (57204783772); Leal-Taixé, Laura (35758586600)","57222956742; 57214450557; 57204783772; 35758586600","Coming Down to Earth: Satellite-to-Street View Synthesis for Geo-Localization","2021","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","6484","6493","9","10.1109/CVPR46437.2021.00642","19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122605271&doi=10.1109%2fCVPR46437.2021.00642&partnerID=40&md5=65140bbd9991ca963bee7af512343ed3","The goal of cross-view image based geo-localization is to determine the location of a given street view image by matching it against a collection of geo-tagged satellite images. This task is notoriously challenging due to the drastic viewpoint and appearance differences between the two domains. We show that we can address this discrepancy explicitly by learning to synthesize realistic street views from satellite inputs. Following this observation, we propose a novel multi-task architecture in which image synthesis and retrieval are considered jointly. The rationale behind this is that we can bias our network to learn latent feature representations that are useful for retrieval if we utilize them to generate images across the two input domains. To the best of our knowledge, ours is the first approach that creates realistic street views from satellite images and localizes the corresponding query street-view simultaneously in an end-to-end manner. In our experiments, we obtain state-of-the-art performance on the CVUSA and CVACT benchmarks. Finally, we show compelling qualitative results for satellite-to-street view synthesis. © 2021 IEEE","Computer vision; Satellites; Earth's satellites; Geo-localisation; Image-based; Images synthesis; Matchings; Multi tasks; Satellite images; Task architectures; Two domains; View synthesis; Benchmarking","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85122605271"
