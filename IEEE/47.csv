"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Desktop-Based Methodology for Collecting Wetland Reference data over Inaccessible Arctic Landscapes","M. Merchant; B. Brisco; M. Mahdianpari; J. Granger; F. Mohammadimanesh; B. DeVries; A. Berg","The Canada Centre for Mapping and Earth Observation, Ottawa, ON, Canada; The Canada Centre for Mapping and Earth Observation, Ottawa, ON, Canada; Department of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John's, NL, Canada; C-CORE, St. John's, NL, Canada; C-CORE, St. John's, NL, Canada; Department of Geography, Environment and Geomatics, University of Guelph, Guelph, ON, Canada; Department of Geography, Environment and Geomatics, University of Guelph, Guelph, ON, Canada","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5893","5896","Arctic environments are remote and inaccessible, making conventional field-based data collection challenging. Thus, this study describes an efficient desktop-based methodology for deriving reference data to support large-scale remote sensing classification focusing on wetland ecosystems. Our study area was Canada's Southern Arctic Ecozone. Various Earth observation (EO) datasets, including optical, multi-spectral, and topographic, were used as a base to support a photointerpretation process for collecting reference data. Ten 30-by-30-kilometer sampling plots were established across the ecozone for this activity based on a suite of minimum criteria. Reference polygons were assigned to one of the five major wetland classes of the Canadian Wetland Classification System (CWCS), along with a detailed wetland type definition. It is anticipated this methodology will be applied later to other northern ecozones to support large-scale wetland classification updates and status and trends reporting.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883985","Arctic;Machine learning;Google Earth Engine;photointerpretation;CWIM","Training;Earth;Data collection;Market research;Arctic;Optical sensors;Remote sensing","ecology;geophysical image processing;image classification;remote sensing;terrain mapping;vegetation;vegetation mapping","collecting Wetland reference data;inaccessible Arctic landscapes;Arctic environments;conventional field-based data collection challenging;efficient desktop-based methodology;large-scale remote sensing classification;wetland ecosystems;Canada's Southern Arctic Ecozone;Earth observation datasets;30-by-30-kilometer sampling plots;reference polygons;wetland classes;Canadian Wetland Classification System;detailed wetland type definition;northern ecozones;large-scale wetland classification updates","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"The DALO-ARCTIC campaign: Multi-spectral SAR Imaging of Ice Features in Greenland","A. Reigber; E. Krogager; M. Keller; M. Jaeger; I. Hajnsek; R. Horn",NA; NA; NA; NA; NA; NA,"Proceedings of EUSAR 2016: 11th European Conference on Synthetic Aperture Radar","5 Sep 2016","2016","","","1","3","In May 2015, the Danish Defence Acquisition and Logistics Organization (DALO) together with the German Aerospace Center (DLR) conducted the joint DALO-ARCTIC airborne SAR campaign with the F-SAR sensor over several testsites in Greenland. Principal goal of this campaign was to demonstrate the capabilities of SAR for security applications in arctic environments, as well as to investigate various advanced methods for extracting ice and snow parameters from SAR data. During this campaign, the ability of F-SAR to simultaneously record fully-polarimetric SAR data in several frequency bands was used for the first time on a large scale. Due to the significantly varying penetration depth of the different bands into ice and snow, it is of particular interest to understand and analyse what can be seen in each band and what are the dominating scattering processes. This paper will discuss this based on examples of polarimetric multi-band imaging of ice and snow layers from data acquired during the DALO-ARCTIC campaign. In this way, the huge potential of multi-spectral SAR imaging for the analysis of ice bodies will be demonstrated.","","978-3-8007-4228-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559298","","","","","","","","","","5 Sep 2016","","","VDE","VDE Conferences"
"Ocean Color Net (OCN) for the Barents Sea","M. Asim; C. Brekke; A. Mahmood; T. Eltoft; M. Reigstad","Department of Physics and Technology, UiT The Arctic University of Norway, Tromsø, Norway; Department of Physics and Technology, UiT The Arctic University of Norway, Tromsø, Norway; Department of Computer Science, Information Technology University, Lahore, Pakistan; Department of Physics and Technology, UiT The Arctic University of Norway, Tromsø, Norway; Department of Arctic and Marine Biology, UiT The Arctic University of Norway, Tromsø, Norway","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","5881","5884","Over recent years, rapid environmental changes in the Arctic and subarctic regions have caused significant alterations in the ecosystem structure and seasonality, including the primary productivity of the Barents Sea. This work aims at improving methodology for studying these features, by estimating chlorophyll-a (chl-a) concentrations in the transitional Barents Sea by remotely sensing its optical properties, in order to better understand the large-scale algal bloom dynamics in the region. The in-situ measurements of chl-a are collected from the year 2016 to 2018 over a wide area of the Barents Sea to cover the spatial and temporal variations in chl-a concentration. Optical images of the Barents Sea are captured by the Multi-Spectral Imager Instrument on Sentinel-2. Using these remotely sensed optical images and the in-situ measurements, we propose a match-up dataset creation method based on the distribution of the remotely sensed reflectance spectra. Different Machine Learning (ML) techniques are assessed to estimate concentration of chl-a using the match-up dataset. Most of these techniques have not been investigated before in the subarctic region such as the Barents Sea. The Ocean Color Net (OCN) regression model proposed in this study has outperformed other ML-based techniques including Support Vector Regression, Gaussian Process Regression, and the globally trained Case-2 Regional/Coast Colour (C2RCC) processing chain model C2RCC-Nets, as well as empirical methods based on spectral band ratios. A wide range of experiments has demonstrated the effectiveness of the proposed OCN for ocean color remote sensing in the subarctic region. The performance of the OCN is also presented spatially by computing chl-a maps in the Barents Sea.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323687","Research Council of Norway(grant numbers:276730); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323687","Barents Sea;Ocean Color;chl-a Monitoring","Arctic;Oceans;Image color analysis;Sea measurements;Optical sensors;Remote sensing;Monitoring","ecology;geophysical image processing;image colour analysis;oceanographic techniques;optical images;remote sensing;underwater optics","subarctic region;Ocean Color Net regression model;OCN;ocean color remote sensing;chl-a;transitional Barents Sea;optical images;chlorophyll-a concentrations;MultiSpectral Imager Instrument;Sentinel-2;Machine Learning techniques;Support Vector Regression","","2","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Convolutional Neural Network Approach for Mapping Arctic Vegetation Using Multi-Sensor Remote Sensing Fusion","Z. L. Langford; J. Kumar; F. M. Hoffman","University of Tennessee, Knoxville, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA","2017 IEEE International Conference on Data Mining Workshops (ICDMW)","18 Dec 2017","2017","","","322","331","Accurate and high-resolution maps of vegetation are critical for projects seeking to understand the terrestrial ecosystem processes and land-atmosphere interactions in Arctic ecosystems, such as U.S. Department of Energy's Next Generation Ecosystem Experiment (NGEE) Arctic. However, most existing Arctic vegetation maps are at a coarse resolution and with a varying degree of detail and accuracy. Remote sensing-based approaches for mapping vegetation, while promising, are challenging in high latitude environments due to frequent cloud cover, polar darkness, and limited availability of high-resolution remote sensing datasets (e.g., ~ 5 m). This study proposes a new remote sensing based multi-sensor data fusion approach for developing high-resolution maps of vegetation in the Seward Peninsula, Alaska. We focus detailed analysis and validation study around the Kougarok river, located in the central Seward Peninsula of Alaska. We seek to evaluate the integration of hyper-spectral, multi-spectral, radar, and terrain datasets using unsupervised and supervised classification techniques over a ~343.72 km2 area for generating vegetation classifications at a variety of resolutions (5 m and 12.5 m). We fist applied a quantitative goodness-of-fit method, called Mapcurves, that shows the degree of spatial concordance between the public coarse resolution maps and k-means clustering values and relabels the k values based on the best overlap. We develop a convolutional neural network (CNN) approach for developing high resolution vegetation maps for our study region in Arctic. We compare two CNN approaches: (1) breaking up the images into small patches (e.g., 6 × 6) and predict the vegetation class for entire patch and (2) semantic segmentation and predict the vegetation class for every pixel. We also perform accuracy assessments of the developed data products and evaluate varying CNN architectures. The fusion of hyperspectral and optical datasets performed the best, with accuracy values increased from 0.64 to 0.96-0.97 when using a training map produced by unsupervised clustering and Mapcurves labeling for both CNN models.","2375-9259","978-1-5386-3800-2","10.1109/ICDMW.2017.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8215680","Multi-Sensor;Fusion;Remote Sensing;Vegetation Classificaiton","Vegetation mapping;Remote sensing;Arctic;Earth;Biological system modeling;Artificial satellites;Meteorology","geophysical image processing;image classification;image fusion;image segmentation;neural nets;pattern clustering;sensor fusion;terrain mapping;vegetation;vegetation mapping","US Department of Energy;Arctic vegetation mapping;Next Generation Ecosystem Experiment;Mapcurves labeling;unsupervised clustering;hyperspectral-optical dataset fusion;k values;training map;developed data products;vegetation class;CNN approaches;high resolution vegetation maps;public coarse resolution maps;vegetation classifications;unsupervised classification techniques;central Seward Peninsula;validation study;Alaska;multisensor data fusion approach;high-resolution remote sensing datasets;frequent cloud cover;high latitude environments;existing Arctic vegetation maps;Arctic ecosystems;land-atmosphere interactions;terrestrial ecosystem processes;multisensor remote sensing fusion;convolutional neural network approach;size 343.72 km","","5","","44","IEEE","18 Dec 2017","","","IEEE","IEEE Conferences"
"Learning Relevant Features of Optical Water Types","K. Blix; A. B. Ruescas; J. E. Johnson; G. Camps-Valls","Department of Physics and Technology, University of Tromsø—The Arctic University of Norway, Tromso, Norway; Image Processing Laboratory (IPL), Universitat de València, Valencia, Spain; Image Processing Laboratory (IPL), Universitat de València, Valencia, Spain; Image Processing Laboratory (IPL), Universitat de València, Valencia, Spain","IEEE Geoscience and Remote Sensing Letters","15 Dec 2021","2022","19","","1","5","This work introduces a novel method that makes use of machine learning (ML) techniques to classify hyper- and multi spectral observations into optical water types (OWTs). Classification was done using  $k$ -means clustering, which was followed by a feature relevance step based on the sensitivity analysis (SA) of the predictive mean and variance function of a Gaussian process (GP) regression model. The method was used both in training and predictive mode. The latter allows applying the approach for new unlabeled observations, so that the OWTs and the associated relevant features can automatically be assessed. The methods were studied on hyperspectral synthesized and in situ Arctic data, and were further evaluated on a test image acquired over Arctic seas. Good empirical results encourage wide adoption of the methodology to be applied in operational processing and assessment of water types.","1558-0571","","10.1109/LGRS.2021.3072049","Centre for Integrated Remote Sensing and Forecasting for Arctic Operations (CIRFA) [The Research Council of Norway (RCN)](grant numbers:237906); The Nansen Legacy(grant numbers:276730); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415736","Chlorophyll;explainability;Gaussian processes (GPs);machine learning (ML);ocean;Ocean and Land Color Instrument (OLCI);onboard the Sentinel-3B (S3B);optical water types (OWTs);XAI","Arctic;Training;Optical sensors;Sensitivity;Sea measurements;Optical imaging;Training data","","","","","","28","IEEE","26 Apr 2021","","","IEEE","IEEE Journals"
"The 1st Agriculture-Vision Challenge: Methods and Results","M. T. Chiu; X. Xu; K. Wang; J. Hobbs; N. Hovakimyan; T. S. Huang; H. Shi; Y. Wei; Z. Huang; A. Schwing; R. Brunner; I. Dozier; W. Dozier; K. Ghandilyan; D. Wilson; H. Park; J. Kim; S. Kim; Q. Liu; M. C. Kampffmeyer; R. Jenssen; A. B. Salberg; A. Barbosa; R. Trevisan; B. Zhao; S. Yu; S. Yang; Y. Wang; H. Sheng; X. Chen; J. Su; R. Rajagopal; A. Ng; V. T. Huynh; S. -H. Kim; I. -S. Na; U. Baid; S. Innani; P. Dutande; B. Baheti; S. Talbar; J. Tang","UIUC; UIUC; University of Oregon; Intelinair; UIUC; UIUC; University of Oregon; UIUC; UIUC; UIUC; UIUC; Intelinair; Intelinair; Intelinair; Intelinair; Agency for Defense Development, South Korea; Agency for Defense Development, South Korea; Agency for Defense Development, South Korea; Norwegian Computing Center; UiT The Arctic University of Norway; UiT The Arctic University of Norway; Norwegian Computing Center; UIUC; UIUC; Tongji University, China; Tongji University, China; Tongji University, China; Tongji University, China; Stanford University; Stanford University; Chegg, Inc.; Stanford University; Stanford University; Chonnam National University, South Korea; Chonnam National University, South Korea; Chosun University, South Korea; SGGS Institute of Engineering and Technology, India; SGGS Institute of Engineering and Technology, India; SGGS Institute of Engineering and Technology, India; SGGS Institute of Engineering and Technology, India; SGGS Institute of Engineering and Technology, India; Tsinghua University, China","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","28 Jul 2020","2020","","","212","218","The first Agriculture-Vision Challenge aims to encourage research in developing novel and effective algorithms for agricultural pattern recognition from aerial images, especially for the semantic segmentation task associated with our challenge dataset. Around 57 participating teams from various countries compete to achieve state-of-the-art in aerial agriculture semantic segmentation. The Agriculture- Vision Challenge Dataset was employed, which comprises of 21,061 aerial and multi-spectral farmland images. This paper provides a summary of notable methods and results in the challenge. Our submission server and leaderboard will continue to open for researchers that are interested in this challenge dataset and task; the link can be found $\color{OrangeRed}{here}$.","2160-7516","978-1-7281-9360-1","10.1109/CVPRW50498.2020.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9150731","","Semantics;Image segmentation;Pattern recognition;Conferences;Agriculture;Computational modeling;Computer vision","agriculture;geophysical image processing;geophysical techniques;image recognition;image segmentation","agricultural pattern recognition;aerial images;semantic segmentation task;aerial agriculture semantic segmentation;Agriculture-Vision Challenge Dataset;multispectral farmland images","","11","","35","IEEE","28 Jul 2020","","","IEEE","IEEE Conferences"
