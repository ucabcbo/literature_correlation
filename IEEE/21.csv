"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Remote Sensing Image Synthesis via Graphical Generative Adversarial Networks","G. Wang; G. Dong; H. Li; L. Han; X. Tao; P. Ren","College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","10027","10030","We explore the use of graphical generative adversarial networks (Graphical-GAN) for synthesizing remote sensing images. The model is probabilistic graphical based generative adversarial networks (GAN). It pairs a generative network G with a recognition network R. Both of them are adversarially trained with a discriminative network D. Particularly, R is employed to infer the underlying causal relationships among both observed and latent variables from real remote sensing images. The advantages of the Graphical-GAN for synthesizing multiple categories of remote sensing images are two fold. Firstly, it considers the underlying causal relationships and captures the true data distribution of remote sensing images. Secondly, the adversarial learning generates synthetic sensing images that are similar to real ones with slight differences. Our remote sensing image synthesis scheme paves a promising way for remote sensing dataset augmentation, which is an effective means of improving the accuracy of learning models. Experimental results with high Inception Scores (IS) validate the effectiveness of the Graphical-GAN for remote sensing image synthesis.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898915","Remote Sensing Image Synthesis;Graphical Generative Adversarial Networks","Remote sensing;Image synthesis;Training;Generative adversarial networks;Bayes methods;Neural networks;Probabilistic logic","inference mechanisms;learning (artificial intelligence);remote sensing","graphical generative adversarial networks;Graphical-GAN;remote sensing images;generative network;recognition network;causal relationships;synthetic sensing images;remote sensing image synthesis scheme paves;remote sensing dataset augmentation;discriminative network","","5","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Remote Sensing Images Dehazing Algorithm based on Cascade Generative Adversarial Networks","X. Sun; J. Xu","School of Computer and Control Engineering, Yantai University, Yantai, China; School of Computer and Control Engineering, Yantai University, Yantai, China","2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","25 Nov 2020","2020","","","316","321","The existing remote sensing image dehazing methods based on deep learning networks usually use pairs of clear images and corresponding haze images to train the model. However, pairs of clear images and their haze counterparts are extremely lacking, and synthetically haze images could not accurately simulate the real haze generation process in real-world scenarios. To address this problem, a cascade method combining two GANs (generative adversarial networks) is proposed. It contains a learning-to-haze GAN (UGAN) and learning-to-dehaze GAN (PAGAN). UGAN learns how to haze remote sensing images with unpaired clear and haze images sets, and then guides the PAGAN to learn how to correctly dehaze such images. To reduce the discrepancy between real haze and synthetic haze images, we added self-attention mechanism to PAGAN. The details can be generated using cues from all feature locations. Moreover, the discriminator could check that highly detailed features in distant portions of the images that are consistent with each other. Compared with other dehazing methods, this algorithm does not require numerous pairs of images to train the network repeatedly. And the results show that the cascaded generative adversarial networks has visual and quantitative effectiveness for the removal of haze, thin clouds.","","978-0-7381-0545-1","10.1109/CISP-BMEI51763.2020.9263540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9263540","generative adversarial networks;dehazing;visual attention;remote sensing images;cloud removal","Remote sensing;Generators;Clouds;Training;Generative adversarial networks;Convolution;Cloud computing","geophysical image processing;image colour analysis;image enhancement;image restoration;learning (artificial intelligence);neural nets;remote sensing","PAGAN;haze remote sensing images;synthetic haze images;cascade generative adversarial networks;deep learning networks;haze generation process;cascade method;learning-to-haze GAN;learning-to-dehaze GAN;remote sensing images dehazing algorithm","","3","","24","IEEE","25 Nov 2020","","","IEEE","IEEE Conferences"
"RTC-GAN: REAL-TIME CLASSIFICATION OF SATELLITE IMAGERY USING DEEP GENERATIVE ADVERSARIAL NETWORKS WITH INFUSED SPECTRAL INFORMATION","R. Gandikota; R. K. K; A. Sharma; M. M; V. M. Bothale","National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India; National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India; National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India; National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India; National Remote Sensing Center (NRSC), Indian Space Research Organisation (ISRO), Hyderabad, India","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6993","6996","This paper implements a deep learning-based Convolutional Neural Network (CNN) with adversarial training and infused pixel information to classify multi-spectral data into 4 LULC classes and cloud. The network is capable of classifying the image on a real-time basis at acquisition time in pixel level by considering the various spectral band values at the pixel and a spatial region around the pixel to collect the spatial features. This way, both spatial information, and spectral information are considered to classify the image. This novel GAN architecture named RTC-GAN is generalized over all the satellites that have their sensors in and around standard NIR, R and G spectral bands while being able to classify the images in realtime. This network is realized and tested on data obtained from satellites Landsat 8 Sentinel2 and Indian Remote Sensing (IRS) satellites like Cartosat-2S, Resourcesat-2/2A. The dataset is not open-sourced and hence very minimal information is provided regarding the IRS data.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323363","Generative Adversarial Networks;Real-Time Analysis;Satellite Imagery;Landsat8;Cartosat;LULC","Satellites;Feature extraction;Remote sensing;Generative adversarial networks;Gallium nitride;Training;Real-time systems","convolutional neural nets;geophysical image processing;image classification;learning (artificial intelligence);remote sensing","RTC-GAN;real-time classification;satellite imagery;deep generative adversarial networks;infused spectral information;deep learning-based Convolutional Neural Network;CNN;adversarial training;pixel information;multispectral data;cloud;real-time basis;acquisition time;pixel level;spectral band values;spatial region;spatial features;spatial information;novel GAN architecture;satellites Landsat 8 Sentinel2;open-sourced information;IRS data;standard NIR;Indian Remote Sensing satellites;Cartosat-2S;Resourcesat-2/2A","","1","","5","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Improved Generative Adversarial Networks for VHR Remote Sensing Image Classification","C. Shi; L. Fang; Z. Lv; H. Shen","School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China; Quanzhou Institute of Equipment Manufacturing, Haixi Institutes, Chinese Academy of Sciences, Quanzhou, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China; Quanzhou Institute of Equipment Manufacturing, Haixi Institutes, Chinese Academy of Sciences, Quanzhou, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","With increasing spatial resolution of remote sensing images, accurate classification of land classes depends more on the number of labeled samples. However, the acquisition of labeled samples is difficult and time-consuming. Hence, generative adversarial networks (GANs) have become a new method for collecting training samples for very-high-resolution (VHR) remote sensing image classification. A traditional GAN generates new samples with the same distribution as the labeled samples. However, the generated samples have features close to their class center, and the network cannot obtain effective discriminative ability for the samples close to the decision boundary. This letter presents an improved GAN (IGAN) for VHR remote sensing image classification. In the proposed framework, the generator aims to generate synthetic samples close to the classification boundary, and the discriminator aims to constrain the labels of the synthetic samples. The obtained synthetic samples can effectively improve the classification accuracy of the classification boundary. Experiments are conducted on two VHR remote sensing images, and the results show that the proposed method performs better than several state-of-the-art methods.","1558-0571","","10.1109/LGRS.2020.3025099","National Natural Science Foundation of China(grant numbers:61701396,61902313); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9209060","Generative adversarial network (GAN);remote sensing image classification;semisupervised learning","Training;Remote sensing;Gallium nitride;Generators;Generative adversarial networks;Sensors;Spatial resolution","image classification;image resolution;neural nets;remote sensing","very-high-resolution remote sensing image classification;improved GAN;VHR remote sensing image classification;generative adversarial networks;IGAN","","1","","22","IEEE","29 Sep 2020","","","IEEE","IEEE Journals"
"Text-to-Remote-Sensing-Image Generation With Structured Generative Adversarial Networks","R. Zhao; Z. Shi","State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Synthesizing high-resolution remote sensing images based on the given text descriptions has great potential in expanding the image data set to release the power of deep learning in the remote sensing image processing field. However, there has been no efficient research carried out on this formidable task yet. Given a remote sensing image, the structural rationality of ground objects is critical to judge it whether real or fake, e.g., real bridges are always straight, while a sinuous one can be easily judged as fake. Inspired by this, we propose a multistage structured generative adversarial network (StrucGAN) to synthesize remote sensing images in a structured way given the text descriptions. StrucGAN utilizes structural information extracted by an unsupervised segmentation module to enable the discriminators to distinguish the image in a structured way. The generators of StrucGAN are, thus, forced to synthesize structural reasonable image contents, which could enhance the image authenticity. The multistage framework enables the StrucGAN to generate remote sensing images with increasing resolution stage by stage. The quantitative and qualitative experiments’ results show that the proposed StrucGAN achieves better performance compared with the baseline, and it could synthesize high resolution, realistic, structural reasonable remote sensing images that are semantically consistent with the given text descriptions.","1558-0571","","10.1109/LGRS.2021.3068391","National Key Research and Development Program of China(grant numbers:2019YFC1510905); National Natural Science Foundation of China(grant numbers:61671037); Beijing Natural Science Foundation(grant numbers:4192034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390223","Generative adversarial networks (GANs);remote sensing image synthesize;structural rationality;text description","Remote sensing;Generators;Task analysis;Bridges;Sensors;Semantics;Image segmentation","geophysical image processing;image resolution;image segmentation;neural nets;remote sensing","text-to-remote-sensing-image generation;structured generative adversarial networks;high-resolution remote sensing images;text descriptions;image data;remote sensing image processing field;structural rationality;generative adversarial network;StrucGAN;structural information;structural reasonable image contents;image authenticity;realistic images;structural reasonable remote sensing images","","1","","18","IEEE","30 Mar 2021","","","IEEE","IEEE Journals"
"Building Extraction from Remote Sensing Images with Conditional Generative Adversarial Networks","H. Chen; W. Sun","School of Information Technology, Beijing Normal University, Zhuhai, Zhuhai, China; School of Information Technology, Beijing Normal University, Zhuhai, Zhuhai, China","2022 7th International Conference on Signal and Image Processing (ICSIP)","19 Sep 2022","2022","","","655","658","Automated building extraction from remote sensing images is one of the most challenging problems. In order to automatically extract buildings from remote sensing images by Conditional Generative Adversarial Networks(CGAN), the pix2pix model was adopted and tested on the aerial imagery dataset of WHU Building Dataset. Results show that the pix2pix model performances well and achieves a high precision.","","978-1-6654-9563-9","10.1109/ICSIP55141.2022.9886096","Department of Education of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9886096","building extraction;generative adversarial networks;pix2pix;remote sensing","Image processing;Buildings;Generative adversarial networks;Sensors;Remote sensing","buildings (structures);feature extraction;geophysical image processing;neural nets;terrain mapping","remote sensing images;automated building extraction;WHU Building Dataset;pix2pix model performances;conditional generative adversarial networks;aerial imagery dataset","","","","13","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Research on Super-resolution Reconstruction Algorithm of Remote Sensing Image Based on Generative Adversarial Networks","J. Wenjie; L. Xiaoshu","School of Electronic Engineering, Guangxi Normal University, Guilin, China; School of Electronic Engineering, Guangxi Normal University, Guilin, China","2019 IEEE 2nd International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)","12 Mar 2020","2019","","","438","441","Due to natural conditions and hardware manufacturing processes, the resolution of remote sensing images is generally low. Obtaining high-definition remote sensing images by simply improving hardware and manufacturing processes is not only costly and technically challenging but also cannot be deployed on a large scale. Aiming at the limitations of the traditional methods, this paper studies the image super-resolution reconstruction method for improving the generated anti-network. Firstly, the generator network is optimized, and an RRDB (Residual-in-Residual Dense without BN (Batch Normalization) is used. Block) module; secondly, the related idea of relativistic GAN (relativistic generative adversarial network) is introduced, the relative value of the discriminator is not the absolute value; finally, the sensation loss is improved, and the feature is used before the function is activated. The test results show that the proposed algorithm is better than SRGAN (Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network), SRCNN (Super-Resolution Convolutional Neural Network) and FSRCNN (Fast Super-Resolution Convolutional Neural Network). The clarity of the reconstructed image is improved, and the reconstructed image quality is significantly improved.","","978-1-7281-5030-7","10.1109/AUTEEE48671.2019.9033352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9033352","Aerial image;Super-resolution;Generative Adversarial Networks","Image reconstruction;Image resolution;Generators;Remote sensing;Gallium nitride;Generative adversarial networks;Reconstruction algorithms","geophysical image processing;image reconstruction;image resolution;neural nets;remote sensing","generator network;generated anti-network;image super-resolution reconstruction method;high-definition remote sensing images;hardware manufacturing processes;natural conditions;Generative Adversarial networks;remote sensing Image;Super-resolution reconstruction algorithm;reconstructed image quality;Fast Super-Resolution Convolutional;Super-Resolution Convolutional Neural Network;Photo-Realistic Single Image Super-Resolution;relativistic generative adversarial network","","","","19","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Building Footprint Generation Using Improved Generative Adversarial Networks","Y. Shi; Q. Li; X. X. Zhu","Institute of Remote Sensing Technology, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany","IEEE Geoscience and Remote Sensing Letters","25 Mar 2019","2019","16","4","603","607","Building footprint information is an essential ingredient for 3-D reconstruction of urban models. The automatic generation of building footprints from satellite images presents a considerable challenge due to the complexity of building shapes. In this letter, we have proposed improved generative adversarial networks (GANs) for the automatic generation of building footprints from satellite images. We used a conditional GAN (CGAN) with a cost function derived from the Wasserstein distance and added a gradient penalty term. The achieved results indicated that the proposed method can significantly improve the quality of building footprint generation compared to CGANs, the U-Net, and other networks. In addition, our method nearly removes all hyperparameters tuning.","1558-0571","","10.1109/LGRS.2018.2878486","H2020 European Research Council(grant numbers:ERC-2016-StG-714087); Helmholtz-Gemeinschaft(grant numbers:VH-NG-1018); Munich Aerospace e.V. Fakultät für Luft- und Raumfahrt; Bavaria California Technology Center through the Large-Scale Problems in Earth Observation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8581486","Building footprint;conditional generative adversarial networks (CGANs);generative adversarial networks (GANs);segmentation;Wasserstein GANs (WGANs)","Gallium nitride;Buildings;Generators;Generative adversarial networks;Satellites;Training;Remote sensing","building;computational geometry;gallium compounds;geophysical image processing;gradient methods;image resolution;optimisation;solid modelling;terrain mapping","automatic generation;building footprints;satellite images;building shapes;improved generative adversarial networks;building footprint generation;building footprint information","","31","","15","OAPA","19 Dec 2018","","","IEEE","IEEE Journals"
"Learning to Generate Radar Image Sequences Using Two-Stage Generative Adversarial Networks","C. Zhang; X. Yang; Y. Tang; W. Zhang","University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","26 Feb 2020","2020","17","3","401","405","While quantitative precipitation estimation (QPE) using weather radar is widely adopted in operation, precipitation data sets are often highly imbalanced. In particular, extreme precipitation usually lacks representation, which may introduce the bottleneck for radar QPE with machine learning models. Discovering the intrinsic characteristic of extreme precipitation with few samples is challenging. In this letter, we focus on the radar reflectivity data and aim to generate synthetic radar image sequences with respect to extreme precipitation. Considering the relatively long interval between continuous radar images due to radar volume scan, traditional methods in video generation are not suitable. In this letter, we propose Two-stage Generative Adversarial Networks (TsGANs) to address the above-mentioned problem. In general, our TsGAN constructs adversarial process between generators and discriminators: the generator produces samples similar to real data, while the discriminator determines whether or not a sample is eligible. In Stage I, we generate an image sequence containing content and motion features. In Stage II, we design an enhanced net structure to enrich the adversarial processes and further improve the motion features. Experimental testing is performed within the radar coverage in Shenzhen, China, on rainfall events in 2014-2016. Results show that our TsGAN is superior to previous works.","1558-0571","","10.1109/LGRS.2019.2922326","National Natural Science Foundation of China(grant numbers:U1636220,61602482,61532006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754756","Deep learning;extreme precipitation;generative adversarial networks (GANs);radar image sequences","Radar imaging;Image sequences;Videos;Generators;Radar remote sensing;Generative adversarial networks","atmospheric precipitation;atmospheric techniques;image sequences;learning (artificial intelligence);meteorological radar;radar imaging;remote sensing by radar","weather radar;radar coverage;adversarial processes;image sequence;TsGAN constructs adversarial process;video generation;radar volume;continuous radar images;synthetic radar image sequences;radar reflectivity data;machine learning models;radar QPE;extreme precipitation;precipitation data sets;quantitative precipitation estimation;two-stage generative adversarial networks","","5","","21","IEEE","3 Jul 2019","","","IEEE","IEEE Journals"
"Cloud-Gan: Cloud Removal for Sentinel-2 Imagery Using a Cyclic Consistent Generative Adversarial Networks","P. Singh; N. Komodakis","Ecole des Ponts ParisTech & Université Paris Est, France; Ecole des Ponts ParisTech & Université Paris Est, France","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1772","1775","Cloud cover is a serious impediment in land surface analysis from Remote Sensing images either causing complete obstruction (thick clouds) with loss of information or blurry effects when being semi-transparent (thin clouds). While thick clouds require complete pixel replacement, thin cloud removal is fairly challenging as the atmospheric and land-cover information is inter-twined. In this paper, we address this problem and propose a Cloud-GAN to learn the mapping between cloudy images and cloud-free images. The adver-sarialloss in the proposed method constrains the distribution of generated images to be close enough to the underlying distribution of the non-cloudy images. An additional cycle consistency loss is used to further restrain the generator to predict cloud-free images only of the same scene as reflected in the cloudy images. Our method not only rejects the necessity of any paired (cloud/cloud-free) training dataset but also avoids the need of any additional (expensive) spectral source of information such as Synthetic Aperture Radar imagery which is cloud penetrable. Lastly, we demonstrate the efficacy of our technique by training on an openly available and fairly new Sentinel-2 Imagery dataset consisting of real clouds. We also show significant improvement in PSNR values after removing clouds on synthetic images thus validating the competency of our methodology.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519033","Cloud Removal;Generative Adversarial Networks;Deep Learning;Sentinel-2 Imagery","Clouds;Training;Generators;Remote sensing;Synthetic aperture radar;Gallium nitride;Generative adversarial networks","clouds;geophysical image processing;image restoration;radar imaging;remote sensing by radar;synthetic aperture radar","cloud removal;cyclic consistent generative adversarial networks;Remote Sensing images;thick clouds;atmospheric land-cover information;Cloud-GAN;cloudy images;cloud-free images;generated images;noncloudy images;cloud cover;synthetic aperture radar imagery;thin cloud;Sentinel-2 imagery;land surface analysis","","73","","13","EU","4 Nov 2018","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks for Cross-Scene Classification in Remote Sensing Images","L. Bashmal; Y. Bazi; H. AlHichri; N. Alajlan","Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia; Computer Engineering Department, King Saud University, Riyadh, Saudi Arabia","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4752","4755","In this paper, we present a novel method for cross-scene classification in remote sensing images based on generative adversarial networks (GANs). To this end, we train in an adversarial manner an encoder-decoder network coupled with a discriminator network on labeled and unlabeled data coming from two different domains. The encoder-decoder network aims to reduce the discrepancy between the distributions of the two domains, while the discriminator tries to discriminate between them. At the end of the optimization process, we train an extra network on the obtained encoded labeled data and then classify the encoded unlabeled data. Experimental results on two datasets acquired over the cities of Potsdam and Vaihingen with spatial resolutions of 5cm and 9cm, respectively, confirm the promising capability of the proposed method.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517487","Cross-scene classification;domain adaptation;generative adversarial networks (GANs)","Gallium nitride;Feature extraction;Generators;Training;Remote sensing;Support vector machines;Urban areas","geophysical image processing;image classification;image coding;learning (artificial intelligence);neural nets;optimisation;remote sensing","Potsdam;Vaihingen;labeled unlabeled data;discriminator network;remote sensing images;cross-scene classification;generative adversarial networks;encoded unlabeled data;encoder-decoder network","","5","","20","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Classifier-Constrained Deep Adversarial Domain Adaptation for Cross-Domain Semisupervised Classification in Remote Sensing Images","W. Teng; N. Wang; H. Shi; Y. Liu; J. Wang","College of Forestry, University of Nanjing Forestry, Nanjing, China; College of Geographic Information and Tourism, University of Chuzhou, Chuzhou, China; College of Geographic Information and Tourism, University of Chuzhou, Chuzhou, China; College of Geographic Information and Tourism, University of Chuzhou, Chuzhou, China; College of Geographic Information and Tourism, University of Chuzhou, Chuzhou, China","IEEE Geoscience and Remote Sensing Letters","22 Apr 2020","2020","17","5","789","793","This letter presents a classifier-constrained deep adversarial domain adaptation (CDADA) method for cross-domain semisupervised classification in remote sensing (RS) images. A deep convolutional neural network (DCNN) is used to build feature representations to describe the semantic content of scenes before the adaptation process. Then, adversarial domain adaptation is used to align the feature distribution of the source and the target. Specifically, two different land-cover classifiers are used as a discriminator to consider land-cover decision boundaries between classes and increase their distance to separate them from the original land-cover class boundaries. The generator then creates robust transferable features far from the original land-cover class boundaries under the classifier constraint. The experimental results of six scenarios built from three benchmark RS scene data sets (AID, Merced, and RSI-CB data sets) are reported and discussed.","1558-0571","","10.1109/LGRS.2019.2931305","National Natural Science Foundation of China(grant numbers:41601455); Key Projects of Anhui Natural Science Research in Universities(grant numbers:KJ2017A416); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794530","Cross-domain classification;deep convolutional neural networks (DCNNs);domain adaptation (DA);generative adversarial networks (GANs);remote sensing (RS)","Generators;Feature extraction;Training;Linear programming;Remote sensing;Data mining;Probabilistic logic","convolutional neural nets;feature extraction;geophysical image processing;image classification;land cover;learning (artificial intelligence);pattern classification;terrain mapping","classifier constraint;cross-domain semisupervised classification;classifier-constrained deep adversarial domain adaptation method;remote sensing images;deep convolutional neural network;feature representations;adaptation process;feature distribution;land-cover decision;original land-cover class boundaries;robust transferable features;land-cover classifiers;RSI-CB data sets","","38","","18","IEEE","12 Aug 2019","","","IEEE","IEEE Journals"
"Self-Attention Generative Adversarial Networks for Times Series VHR Multispectral Image Generation","F. Chaabane; S. Réjichi; F. Tupin","COSIM laboratory, SUP'COM, Carthage University, Tunisia; COSIM laboratory, SUP'COM, Carthage University, Tunisia; Department of Image and Signal Processing, Telecom ParisTech, France","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4644","4647","Recently classical deep learning approaches are commonly used to perform spatial and temporal classification especially for Very High Resolution (VHR) images. They learn from existing low resolution or undersized datasets because of the availability and prices of VHR remote sensing images. Thus, they have witnessed a conspicuous success because it is quite challenging to classify high-dimensional multispectral time series data with few labeled samples. It is also difficult to simulate high quality samples having the same features as the real ones. It goes without saying that the introduction of GANs (Generative Adversarial Network) models as an unsupervised learning method, has allowed the extraction of accurate representations of the data via latent codes and backpropagation techniques. However, it is difficult to acquire high-quality samples with unwanted noises and uncontrolled divergences. To generate high-quality multispectral time series samples, a Self-Attention Generative Adversarial Network (SAGAN) is proposed in this work. SAGAN allows attention-driven, long-range dependency modeling for VHR Multispectral time series image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations which improves training dynamics. The proposed SAGAN performs better than traditional GANs, boosting the best inception score. The main contribution of this work is the use of one of the new generation of learning techniques, SAGAN, for Times Series VHR Multispectral Image Generation. SAGAN has been recently used only for single image generation.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553597","Self-Attention Generative Adversarial Networks;Multispectral VHR time series;deep learning techniques;etc.","Training;Image synthesis;Time series analysis;Generative adversarial networks;Feature extraction;Spatial resolution;Task analysis","backpropagation;feature extraction;geophysical image processing;geophysical signal processing;image classification;image resolution;learning (artificial intelligence);remote sensing;time series;unsupervised learning","GANs models;Generative Adversarial Network;unsupervised learning method;high-quality samples;high-quality multispectral time series samples;SAGAN;VHR Multispectral time series image generation tasks;high-resolution details;lower-resolution feature maps;Times Series VHR Multispectral Image Generation;single image generation;Generative Adversarial networks;classical deep learning approaches;High Resolution images;VHR remote sensing images;high-dimensional multispectral time series data;high quality samples","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"State-of-the-Art and Gaps for Deep Learning on Limited Training Data in Remote Sensing","J. E. Ball; D. T. Anderson; P. Wei","Department of Electrical and Computer Engineering, Mississippi State University; Department of Electrical Engineering and Computer Science, The University of Missouri; Department of Electrical and Computer Engineering, Mississippi State University","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4119","4122","Deep learning usually requires big data, with respect to both volume and variety. However, most remote sensing applications only have limited training data, of which a small subset is labeled. Herein, we review three state-of-the-art approaches in deep learning to combat this challenge. The first topic is transfer learning, in which some aspects of one domain, e.g., features, are transferred to another domain. The next is unsupervised learning, e.g., autoencoders, which operate on unlabeled data. The last is generative adversarial networks, which can generate realistic looking data that can fool the likes of both a deep learning network and human. The aim of this article is to raise awareness of this dilemma, to direct the reader to existing work and to highlight current gaps that need solving.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518681","Deep learning;remote sensing;limited training data;transfer learning;generative adversarial networks","Gallium nitride;Remote sensing;Training data;Training;Feature extraction;Generative adversarial networks","Big Data;remote sensing;unsupervised learning","transfer learning;unlabeled data;deep learning network;big data;remote sensing applications;unsupervised learning;autoencoders;generative adversarial networks","","4","","36","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Hyperspectral Anomaly Detection Using Bilateral-Filtered Generative Adversarial Networks","C. Zhao; C. Li; S. Feng; N. Su","Ministry of Industry and Information Technology, Key Laboratory of Advanced Marine Communication and Information Technology, Harbin Engineering University, Harbin, China; Ministry of Industry and Information Technology, Key Laboratory of Advanced Marine Communication and Information Technology, Harbin Engineering University, Harbin, China; Ministry of Industry and Information Technology, Key Laboratory of Advanced Marine Communication and Information Technology, Harbin Engineering University, Harbin, China; Ministry of Industry and Information Technology, Key Laboratory of Advanced Marine Communication and Information Technology, Harbin Engineering University, Harbin, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4408","4411","Without any prior information of anomalies or background, hyperspectral anomaly detection has received a wide attention. However, such unsupervised style brings difficulties in training and learning effective features of hyperspectral image to perform detection. This paper proposes a novel hyperspectral anomaly detection algorithm using bilateral-filtered generative adversarial networks (BFGAN). Bilateral filter can smooth images and remove anomalous points while preserving edges. With closeness weights and similarity weights, the bilateral-filtered hyperspectral image can be considered as background data, so that hyperspectral background labels are obtained. Only with one class of labels, the structure of generative adversarial networks has an ability to solve two-class problem. By using the filtered background data and their labels, generative adversarial networks are trained to improve discriminator's discriminative capability for background data in a competing style. Finally, the model discriminator can finally output big probabilities for background samples and small probabilities for anomalous samples. Experiments on two real hyperspectral images demonstrate that the proposed method outperforms other state-of-the-art competitors.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553233","National Natural Science Foundation of China(grant numbers:62002083,61971153,62071136,61801142); Fundamental Research Funds for the Central Universities(grant numbers:3072021CF0814,3072021CF0807,3072021CF0808); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553233","Hyperspectral remote sensing;anomaly detection;bilateral filter;deep learning;generative adversarial networks","Training;Adaptation models;Image edge detection;Training data;Filtering algorithms;Generative adversarial networks;Information filters","edge detection;feature extraction;filtering theory;geophysical image processing;hyperspectral imaging;image classification;image denoising;image segmentation;image sensors;learning (artificial intelligence);object detection;probability","generative adversarial networks;filtered background data;background samples;learning effective features;novel hyperspectral anomaly detection algorithm;bilateral filter;bilateral-filtered hyperspectral image;hyperspectral background labels","","2","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Semi-Supervised Object Detection in Remote Sensing Images Using Generative Adversarial Networks","G. Chen; L. Liu; W. Hu; Z. Pan","Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","2503","2506","Object detection is a challenging task in computer vision. Now many detection networks can get a good detection result when applying large training dataset. However, annotating sufficient amount of data for training is often time-consuming. To address this problem, a semi-supervised learning based method is proposed in this paper. Semi-supervised learning trains detection networks with few annotated data and massive amount of unannotated data. In the proposed method, Generative Adversarial Network is applied to extract data distribution from unannotated data. The extracted information is then applied to improve the performance of detection network. Experiment shows that the method in this paper greatly improves the detection performance compared with supervised learning using only few annotated data. The results prove that it is possible to achieve acceptable detection result when only few target object is annotated in the training dataset.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519132","Semi-supervised learning;objet detection;generative adversarial networks (GAN);convolutional neural networks (CNN)","Gallium nitride;Generative adversarial networks;Detectors;Generators;Airplanes;Training;Semisupervised learning","computer vision;feature extraction;geophysical image processing;image annotation;learning (artificial intelligence);object detection;remote sensing","generative adversarial network;target object;supervised learning;detection performance;extract data distribution;semisupervised learning trains detection networks;semisupervised learning based method;detection network;computer vision;remote sensing images;semisupervised object detection","","11","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Semi-Supervised Variational Generative Adversarial Networks for Hyperspectral Image Classification","H. Wang; C. Tao; J. Qi; H. Li; Y. Tang","School of Geosciences and Info-physics, Central South University Changsha, Hunan, China; School of Geosciences and Info-physics, Central South University Changsha, Hunan, China; School of Geosciences and Info-physics, Central South University Changsha, Hunan, China; School of Geosciences and Info-physics, Central South University Changsha, Hunan, China; School of Geosciences and Info-physics, Central South University Changsha, Hunan, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9792","9794","Though Hyperspectral Image (HSI) Classification has been extensively investigated over recent decades, it is still a challenge task especially when the number of labeled samples is extremely limited. In this paper, we overcome this challenge by using synthetic samples, and proposed a semi-supervised variational Generative Adversarial Networks(GANs) for this purpose. Compared to the conditional GAN which is recently used for generating samples for HSI classification, the proposed approach has two novel aspects. First, we extend the classic variational generative adversarial network to the semi-supervised context through an ensemble prediction technique. By this way, our model can be trained using limited labeled samples (only 5 samples per class) with a large number of unlabeled samples. Second, we adopt an encoder-decoder network to explicitly learn the relationship between the latent space and the real image space. This property enables our model producing diverse samples by simply varying some latent parameters, which is desirable for enriching the training dataset. We have shown that the proposed model can achieve better and robust performance for HSI classification compared to conditional GAN, especially when the labeled data is limited.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900073","Semi-supervised Learning;Hyperspectral Images Classification;Variational Auto-Encoder (VAE);Generative Adversarial Networks (GAN);Deep Learning","Generative adversarial networks;Hyperspectral imaging;Training;Task analysis;Data models;Sensors","geophysical image processing;hyperspectral imaging;image classification;neural nets;remote sensing","hyperspectral image classification;conditional GAN;HSI classification;unlabeled samples;encoder-decoder network;image space;semisupervised variational generative adversarial networks;ensemble prediction technique;latent space;real image space","","14","","5","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Modeling Urbanization Patterns with Generative Adversarial Networks","A. Albert; E. Strano; J. Kaur; M. González","Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA; Philips Research U.S.A., Cambridge, MA, 02141; University of California, Berkeley, CA, p94720","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","2095","2098","In this study we propose a new method to simulate hyper-realistic urban patterns using Generative Adversarial Networks trained with a global urban land-use inventory. We generated a synthetic urban “universe” that qualitatively reproduces the complex spatial organization observed in global urban patterns, while being able to quantitatively recover certain key high-level urban spatial metrics.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518032","generative adversarial networks;urban modeling;global urbanization","Urban areas;Generative adversarial networks;Gallium nitride;Training;Remote sensing;Machine learning;Generators","land use;learning (artificial intelligence);neural nets","Generative Adversarial Networks;hyper-realistic urban patterns;global urban land-use inventory;synthetic urban universe;global urban patterns;high-level urban spatial metrics","","13","","17","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Pixels-to-Abundances Translation with Spatial-Spectral Conditional Generative Adversarial Networks for Hyperspectral Unmixing","L. Wang; X. Zhang; S. Zheng; T. Li; J. Wang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3329","3332","As the interaction of light with the Earth surface is very complex, spectral pixels are composed of sophisticated mixtures of distinct substances. The parameters of the estimated models are difficult to set. In this paper, we first propose a spatial-spectral conditional generative adversarial networks (scGANs) method to solve this problem, based on the following assumptions: the unmixing process from pixels to abundance can be viewed as a transformation of two modalities with an intrinsic specific relationship. The method learns the manifold structure of the hyperspectral data using an adversarial strategy, inputting pixels blocks with spatial information to generate the abundance of the central pixels. Then, we proposed superpixel segmentation and random splitting method to synthesize data with spatial structure. Finally, the proposed scGANs method is evaluated using synthetic data and real hyperspectral data, and compared to several state-of-the-art methods. The proposed method outperforms all the comparison methods in the experiments.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553398","National Natural Science Foundation of China(grant numbers:61877066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553398","Hyperspectral unmixing;spatial-spectral information;conditional generative adversarial networks;superpixel segmentation and random splitting","Manifolds;Earth;Geoscience and remote sensing;Generative adversarial networks;Spatial databases;Surface treatment;Hyperspectral imaging","feature extraction;geophysical image processing;image colour analysis;image segmentation;iterative methods;learning (artificial intelligence);neural nets","pixels-to-abundances translation;Earth surface;spectral pixels;sophisticated mixtures;distinct substances;estimated models;spatial-spectral conditional generative adversarial networks method;unmixing process;intrinsic specific relationship;hyperspectral data;adversarial strategy;pixels blocks;spatial information;central pixels;random splitting method;spatial structure;comparison methods;scGAN method","","","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Semi-Supervised Change Detection Based on Graphs with Generative Adversarial Networks","J. Liu; K. Chen; G. Xu; H. Li; M. Yan; W. Diao; X. Sun","Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","74","77","In this paper, we present a semi-supervised remote sensing change detection method based on graph model with Generative Adversarial Networks (GANs). Firstly, the multi-temporal remote sensing change detection problem is converted as a problem of semi-supervised learning on graph where a majority of unlabeled nodes and a few labeled nodes are contained. Then, GANs are adopted to generate samples in a competitive manner and help improve the classification accuracy. Finally, a binary change map is produced by classifying the unlabeled nodes to a certain class with the help of both the labeled nodes and the unlabeled nodes on graph. Experimental results carried on several very high resolution remote sensing image data sets demonstrate the effectiveness of our method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898913","Change Detection;Semi-supervised Learning;Graph Model;Generative Adversarial Network","Gallium nitride;Generative adversarial networks;Task analysis;Remote sensing;Semisupervised learning;Training;Generators","geophysical image processing;geophysical signal processing;image classification;learning (artificial intelligence);remote sensing","unlabeled nodes;high resolution remote;semisupervised change detection;Generative Adversarial Networks;semisupervised remote sensing change detection method;graph model;GANs;multitemporal remote sensing change detection problem;semisupervised learning;labeled nodes;binary change map","","10","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Super Resolution Approach for the Satellite Data Based on the Generative Adversarial Networks","M. Lavreniuk; N. Kussul; A. Shelestov; A. Lavrenyuk; L. Shumilo","Space Research Institute NASU-SSAU, Kyiv, Ukraine; National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, Kyiv, Ukraine; National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, Kyiv, Ukraine; National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, Kyiv, Ukraine; National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”, Kyiv, Ukraine","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1095","1098","In the past few years, medium and high-resolution data became freely available for downloading. It provides great opportunity for researchers not to select between solving the task with high-resolution data on small territory or on global scale, but with low-resolution satellite images. Due to high spectral and spatial resolution of the data, Sentinel-1 and Sentinel-2 are very popular sources of information. Nevertheless, in practice if we would like to receive final product in 10 m resolution we should use bands with 10 m resolution. Sentinel-2 has four such bands, but also has other bands, especially red-edge 20 m resolution bands that are useful for vegetation analysis and often are omitted due to lower resolution. Thus, in this study we propose methodology for enhancing resolution (super-resolution) of the existing low-resolution images to higher resolution images. The main idea is to use advanced methods of deep learning - Generative Adversarial Networks (GAN) and train it to increase the resolution for the satellite images. Experimental results for the Sentinel-2 data showed that this approach is efficient and could be used for creating high resolution products.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884460","deep learning;Generative Adversarial Networks;GAN;super-resolution;Sentinel-2","Deep learning;Image resolution;Satellites;Superresolution;Vegetation mapping;Generative adversarial networks;Spatial resolution","geophysical image processing;image resolution;remote sensing;vegetation","high-resolution data;low-resolution satellite images;high spectral resolution;spatial resolution;Sentinel-1;enhancing resolution;existing low-resolution images;higher resolution images;Generative Adversarial Networks;Sentinel-2 data;high resolution products;super resolution approach;satellite data;size 10.0 m;size 20.0 m","","","","23","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Improving Text Encoding for Retro-Remote Sensing","M. B. Bejiga; G. Hoxha; F. Melgani","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","24 Mar 2021","2021","18","4","622","626","A recent work on retro-remote sensing (converting ancient text descriptions into images) was proposed using a multilabel encoding scheme in which an input text description is represented by a binary vector indicating the presence or absence of specific objects. However, this kind of encoding disregards information such as object attributes and spatial relationship between multiple objects in a description, resulting in images that do not semantically (fully) conform to the input description. In this letter, we propose an improved text-encoding mechanism that takes into account different levels of information available from an input text. The encoded text is then used as conditional information to guide the image synthesis process using generative adversarial networks (GANs). Besides, we present a modified GAN architecture intending to improve the semantic content of the generated images. Both the qualitative and quantitative results obtained indicate that the proposed method is particularly promising.","1558-0571","","10.1109/LGRS.2020.2983851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066830","Generative adversarial networks (GANs);multimodal learning;retro-remote sensing;text-to-image synthesis","Gallium nitride;Training;Generators;Generative adversarial networks;Context modeling;Encoding;Sensors","encoding;image coding;neural net architecture;remote sensing;text analysis","input text description;binary vector;object attributes;spatial relationship;encoded text;conditional information;image synthesis;text encoding;retro-remote sensing;ancient text;text-encoding mechanism;multilabel encoding;generative adversarial networks;GAN architecture","","4","","20","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"Generative Adversarial Networks Under CutMix Transformations for Multimodal Change Detection","A. Radoi","Department of Applied Electronics and Information Engineering, University POLITEHNICA of Bucharest, Bucharest, Romania","IEEE Geoscience and Remote Sensing Letters","5 Sep 2022","2022","19","","1","5","The current technological developments lead to increased heterogeneity and variability in remote sensing imagery. In this context, unsupervised multimodal change detection techniques are mandatory to perform a continuous monitoring and rapid damage assessment by means of heterogeneous remote sensing data. Taking advantage of the latest advances in deep learning, we address multimodal change detection from an intermodality image translation perspective. Intermodality translation is achieved by means of generative adversarial networks (GANs) built over U-Net architectures at both generator and discriminator levels and trained under CutMix transformations. A change prior is used to guide the learning process of the neural network framework and to reduce the impact of changed locations over the learned model. The change prior is derived in an unsupervised manner from comparisons between the postevent locations and  $k$  nearest neighbor ( $k$ NN) locations determined in the preevent image. The experiments were conducted over several pairs of heterogeneous remote sensing images, and the comparisons with current state-of-the-art approaches show the effectiveness of the proposed multimodal change detection framework.","1558-0571","","10.1109/LGRS.2022.3201003","Ministry of Research, Innovation and Digitization, CNCS/CCCDI-UEFISCDI(grant numbers:PN-III-P2-2.1-SOL-2021-0084 (BDAGeoINT)); Romanian Ministry of Education and Research, CNCS-UEFISCDI, within PNCDI III(grant numbers:PN-III-P1-1.1-PD-2019-0843 (MDM-SITS)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864607","CutMix;generative adversarial networks (GANs);image translation;multimodal change detection;U-Net","Sensors;Image sensors;Imaging;Training;Remote sensing;Machine-to-machine communications;Deep learning","deep learning (artificial intelligence);feature extraction;geophysical image processing;nearest neighbour methods;object detection;remote sensing;unsupervised learning","heterogeneous remote sensing images;generative adversarial networks;CutMix transformations;unsupervised multimodal change detection techniques;rapid damage assessment;deep learning;intermodality image translation perspective;intermodality translation;discriminator levels;learning process;neural network framework;postevent locations;k nearest neighbor locations;continuous monitoring;U-Net architectures","","2","","21","IEEE","23 Aug 2022","","","IEEE","IEEE Journals"
"Adversarial Hash-Code Learning for Remote Sensing Image Retrieval","C. Liu; J. Ma; X. Tang; X. Zhang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4324","4327","Hashing, a useful solution for approximate nearest neighbor (ANN) search, is popular for large-scale image retrieval. In this paper, we presents a deep supervised hashing model for remote sensing image retrieval (RSIR) in the framework of generative adversarial networks (GAN), named GAN-assist Hashing (GAAH). First, to learn the compact and useful hash codes from the images, we define a novel loss function for the generator. The loss function mainly consists of classification, similarity, and bits entropy terms. The classification term makes the hash code is discriminative, the similarity term constrains the binary code is similarity preserving, and the bits entropy term assures the learned code is low-error in the quantization. Second, we construct the unique ""true"" matrix with the uniform distribution as the input of discriminator to limit the leaned hash codes are bit balanced. The final hash code is learned by a minimax optimization. The positive experimental results on a ground-truth remote sensing image archive validate the usefulness of our GAAH model. Compare with the popular deep hashing methods, our GAAH achieves improved performance.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900431","Generative adversarial networks;hashing;remote sensing image retrieval","Quantization (signal);Training;Image retrieval;Generative adversarial networks;Remote sensing;Generators;Binary codes","cryptography;image coding;image retrieval;minimax techniques;neural nets","loss function;binary code;leaned hash codes;ground-truth remote sensing image archive;remote sensing image retrieval;deep supervised hashing model;generative adversarial networks;adversarial Hash-code learning;GAN-assist Hashing;bits entropy terms;uniform distribution;minimax optimization","","11","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Dehaze-AGGAN: Unpaired Remote Sensing Image Dehazing Using Enhanced Attention-Guide Generative Adversarial Networks","Y. Zheng; J. Su; S. Zhang; M. Tao; L. Wang","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","28 Sep 2022","2022","60","","1","13","Remote sensing image dehazing is of great scientific interest and application value in both military and civil fields. In this article, we propose an enhanced attention-guide generative adversarial network (GAN) network, Dehaze-AGGAN, to solve the remote sensing images dehazing problem, which does not require paired training data. Since haze images have a great influence on remote sensing object detection, the dehazing of remote sensing images has become significantly important. Typical image dehazing methods require a hazy input image and its ground truth in a paired manner, while paired training data are usually not available in the field of remote sensing. To solve this problem, we propose the Dehaze-AGGAN network and train it by feeding unpaired clean and hazy images into the model. We present a novel total variation loss combined with the cycle consistency loss to eliminate wave noise and improve the target edge quality in the test dataset. Moreover, we present a new dehazing dataset called remote sensing dehazing dataset (RSD), which contains 7000 simulate and real hazy images including 3500 warship images and 3500 civilian ship images, and evaluate our method in the dataset. We conduct experiments on RSD. Extensive experiments demonstrate that the proposed Dehaze-AGGAN is effective and has strong robustness and adaptability in different settings.","1558-0644","","10.1109/TGRS.2022.3204890","National Natural Science Foundation of China (NSFC)(grant numbers:62171379,62271409); Innovation Capability Support Program of Shaanxi(grant numbers:2021KJXX-99); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9881213","Attention guided;dehaze;generative adversarial networks (GANs);total variation loss","Remote sensing;Marine vehicles;Task analysis;Generative adversarial networks;Pollution;Generators;Training data","computer vision;feature extraction;geophysical image processing;image colour analysis;image denoising;image enhancement;image resolution;image restoration;object detection;remote sensing;ships","dehazing dataset;7000 simulate images;real hazy images;3500 warship images;3500 civilian ship images;unpaired remote sensing image dehazing;enhanced attention-guide generative adversarial networks;great scientific interest;application value;military fields;civil fields;enhanced attention-guide generative adversarial network network;remote sensing images dehazing;paired training data;haze images;remote sensing object detection;typical image dehazing methods;hazy input image;Dehaze-AGGAN network;unpaired clean images","","3","","37","IEEE","7 Sep 2022","","","IEEE","IEEE Journals"
"Image registration method based on Generative Adversarial Networks","Y. Sun; H. Qi; C. Wang; L. Tao","College of Software, North Automatic Control Technology Institute, Jinzhong, China; College of Software, North Automatic Control Technology Institute, Jinzhong, China; Institute of Systems Engineering, Walter Reed Army Research Laboratories, Beijing, China; College of Software, North Automatic Control Technology Institute, Jinzhong, China","2020 Eighth International Conference on Advanced Cloud and Big Data (CBD)","21 Apr 2021","2020","","","183","188","Image registration technology has been gradually applied in military and civil fields, such as unmanned aerial vehicle (UAV) target recognition, remote sensing image registration, 3D object reconstruction and so on. The registration accuracy of traditional registration algorithms, such as sift and surf, are not high and even mismatch, when the image content changes or affine transformation occurs. In order to solve this problem, an image registration method (IR-GAN method) based on Generative adversarial networks (GAN) was proposed, in view of the content change and affine transformation of images for training. At the same time, the sample images are expanded by translation, rotation, reflection, scaling, shearing, brightness transformation to improve the precision and accuracy of the registration algorithm. Simulation results show that our method is correct in principle and can improve the registration accuracy of both images with content changing and affine transform images.","","978-1-6654-2313-7","10.1109/CBD51900.2020.00041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406840","Generative adversarial networks (GAN);Image registration;Convolution network;Feature vectors","Training;Image registration;Three-dimensional displays;Target recognition;Simulation;Transforms;Generative adversarial networks","affine transforms;autonomous aerial vehicles;image reconstruction;image registration;remote sensing;remotely operated vehicles","image registration method;Generative adversarial networks;image registration technology;military fields;civil fields;unmanned aerial vehicle target recognition;remote sensing image registration;registration accuracy;traditional registration algorithms;image content changes;affine transformation;IR-GAN method;sample images;registration algorithm;content changing","","","","12","IEEE","21 Apr 2021","","","IEEE","IEEE Conferences"
"Balancing Colors of Nonoverlapping Mosaicking Images With Generative Adversarial Networks","Y. Ma; J. Wei; X. Huang","Institute of Space Science and Technology and the School of Resources, Environmental and Chemical Engineering, Nanchang University, Nanchang, China; Institute of Space Science and Technology, Nanchang University, Nanchang, China; Jiangxi Center for Data and Application of High Resolution Earth Observation System, Nanchang, China","IEEE Geoscience and Remote Sensing Letters","11 Jan 2022","2022","19","","1","5","Remote sensing images of different moments or sensors can be stitched together to produce a new image under uniform geographic coordinate systems, where the overlapping areas were needed for color harmony. In this letter, a reference-based mosaicking method is proposed for images either with or without overlapping areas. The new method introduces a low-resolution image for spectral reference that spans all the mosaicking scope. A generative adversarial network is harnessed for color harmony, which transfers all the mosaicking images to the time of the reference image for further stitch with the graph cut and pyramid gradient methods. The proposed method is compared with three color harmony methods or tools by mosaicking the red, green, and blue bands of Landsat-8 images with MODIS as the reference. The digital evaluations demonstrate that the new method outweighs other methods regarding radiometric and spectral fidelity.","1558-0571","","10.1109/LGRS.2021.3126261","National Natural Science Foundation of China(grant numbers:61860130,41974195); 03 Special and 5G Project of the Jiangxi Province(grant numbers:20204ABC03A40); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606709","Color harmony;deep neural networks;enblend;generative adversarial networks;LandSat;mosaicking;spatiotemporal fusion","Image color analysis;Spatial resolution;Kernel;Generators;Remote sensing;Generative adversarial networks;Training","geophysical image processing;geophysical signal processing;gradient methods;image resolution;image segmentation;remote sensing","balancing colors;mosaicking images;generative adversarial network;sensing images;different moments;sensors;uniform geographic coordinate systems;overlapping areas;reference-based;low-resolution image;spectral reference;spans all the mosaicking scope;reference image;gradient methods;color harmony methods;Landsat-8 images;method outweighs other methods","","","","12","IEEE","8 Nov 2021","","","IEEE","IEEE Journals"
"Semi-Supervised Classification of Hyperspectral Data for Geologic Body Based on Generative Adversarial Networks at Tianshan Area","J. Qin; Y. Zhan; K. Wu; W. Liu; Z. Yang; W. Yao; Y. Medjadba; Y. Zhang; X. Yu","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; China Non-ferrous Metals Resource Geological Survey, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4776","4779","Hyperspectral remote sensing data contains near continuous spectral information of the object, which is very suitable for mineral classification and geologic body mapping. However, the collecting of a lot of labeled hyperspectral data is expensive, time-consuming and labor-intensive. We choose a semi-supervised method to classify hyperspectral data based on a generative adversarial nertwork (GAN), just use a small amount of labeled data, named HSGAN. The GAN is made up of a generator and a discriminator, and the generator generates data similar to the real data so that the discriminator cannot tell if it is real data or generated data. We designed a one-dimensional GAN to extract spectral features from hyperspectral data. Using this method, we test the Tianshan hyperspectral data and use the actual geological map as the ground-truth produced by us. We find that HSGAN still achieves better results than the traditional CNN and SVM.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518946","geological mapping;semi-supervised learning (SSL);generative adversarial networks (GAN);deep learning;Hymap data","Hyperspectral imaging;Generative adversarial networks;Gallium nitride;Training;Geology","geology;minerals;terrain mapping","mineral classification;HSGAN;actual geological map;Tianshan hyperspectral data;generated data;generative adversarial nertwork;semisupervised method;labeled hyperspectral data;continuous spectral information;hyperspectral remote sensing data;generative adversarial networks;geologic body;semisupervised classification","","4","","9","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Generating Multiscale Maps From Satellite Images via Series Generative Adversarial Networks","X. Chen; B. Yin; S. Chen; H. Li; T. Xu","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Computer Science, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","Considering the success of generative adversarial networks (GANs) for image-to-image translation, researchers have attempted to translate satellite images to maps (si2map) through GAN for cartography. However, these studies involved limited scales, which hinders multiscale map creation. By extending their method, high-resolution satellite images can be trivially translated to multiscale maps through scale-wise si2map generators trained for certain scales. However, this strategy has two theoretical limitations. First, inconsistency between high-resolution satellite images and object generalization on multiscale maps (SI-M inconsistency) increasingly complicates the extraction of geographical information from satellite images for generators with decreasing scale. Second, as si2map translation is cross-domain, generators incur high computation costs to transform the pixel distribution on satellite images to that on maps. Thus, we designed a series strategy of generators for multiscale si2map translation to address these limitations. In this strategy, high-resolution satellite images are inputted to an si2map generator to output large-scale maps, which are translated to multiscale maps through series multiscale map generators. The series strategy avoids SI-M inconsistency as high-resolution satellite images are only translated to large-scale maps and transforms cross-domain translation to approximately intradomain translation when generating multiscale maps. Our experimental results showed better quality multiscale map generation with the series strategy, as shown by average increases of 11.69%, 53.78%, 55.42%, and 72.34% in the structural similarity index (SSIM), edge structural similarity index (ESSI), intersection over union (road), and intersection over union (water) for data from Mexico City and Tokyo at zoom levels 17–13.","1558-0571","","10.1109/LGRS.2021.3129285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9620093","Cartography generalization;generative adversarial networks (GANs);multiscale maps generation","Satellites;Generators;Spatial resolution;Roads;Indexes;Generative adversarial networks;Costs","cartography;feature extraction;geographic information systems;image resolution;neural nets","generative adversarial networks;image-to-image translation;multiscale map creation;high-resolution satellite images;scale-wise si2map generators;multiscale si2map translation;cartography;SI-M inconsistency;geographical information extraction","","1","","18","IEEE","18 Nov 2021","","","IEEE","IEEE Journals"
"Large-Factor Super-Resolution of Remote Sensing Images With Spectra-Guided Generative Adversarial Networks","Y. Meng; W. Li; S. Lei; Z. Zou; Z. Shi","Shanghai Artificial Intelligence Laboratory, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; AVIC Chengdu Aircraft Industrial (Group) Company Ltd., Chengdu, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2022","2022","60","","1","11","Large-factor image super-resolution (SR) is a challenging task due to the high uncertainty and incompleteness of the missing details to be recovered. In remote sensing images, the subpixel spectral mixing and semantic ambiguity of ground objects make this task even more challenging. In this article, we propose a novel method for large-factor SR of remote sensing images named spectra-guided generative adversarial networks (SpecGANs). In response to the above problems, we explore whether introducing additional hyperspectral images (HSIs) to GAN as conditional input can be the key to solving the problems. Different from previous approaches that mainly focus on improving the feature representation of a single source input, we propose a dual-branch network architecture to effectively fuse low-resolution (LR) red, green, blue (RGB) images and corresponding HSIs, which fully exploit the rich hyperspectral information as conditional semantic guidance. Due to the spectral specificity of ground objects, the semantic accuracy of the generated images is guaranteed. To further improve the visual fidelity of the generated output, we also introduce the Latent Code Bank with rich visual priors under a generative adversarial training framework so that high-resolution, detailed, and realistic images can be progressively generated. Extensive experiments show the superiority of our method over the state-of-art image SR methods in terms of both quantitative evaluation metrics and visual quality. Ablation experiments also suggest the necessity of adding spectral information and the effectiveness of our designed fusion module. To our best knowledge, we are the first to achieve up to 32x SR of remote sensing images with high visual fidelity under the premise of accurate ground object semantics. Our code can be publicly available at https://github.com/YapengMeng/SpecGAN.","1558-0644","","10.1109/TGRS.2022.3222360","National Key Research and Development Program of China (Titled “Brain-inspired General Vision Models and Applications”); National Natural Science Foundation of China(grant numbers:62125102); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950553","Deep convolutional neural networks (CNNs);generative adversarial networks (GANs);hyperspectral image (HSI);remote sensing image;super-resolution (SR)","Superresolution;Hyperspectral imaging;Semantics;Task analysis;Visualization;Generative adversarial networks;Image reconstruction","geophysical image processing;image classification;image fusion;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","accurate ground object semantics;additional hyperspectral images;factor super-resolution;generative adversarial training framework;ground objects;large-factor image super-resolution;large-factor SR;realistic images;remote sensing images;spectra-guided generative adversarial networks;state-of-art image SR methods","","","","63","IEEE","14 Nov 2022","","","IEEE","IEEE Journals"
"Domain Adaptation of Landsat-8 and Proba-V Data Using Generative Adversarial Networks for Cloud Detection","G. Mateo-García; V. Laparra; L. Gómez-Chova","Image Processing Laboratory (IPL), University of Valencia, Spain; Image Processing Laboratory (IPL), University of Valencia, Spain; Image Processing Laboratory (IPL), University of Valencia, Spain","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","712","715","Training machine learning algorithms for new satellites requires collecting new data. This is a critical drawback for most remote sensing applications and specially for cloud detection. A sensible strategy to mitigate this problem is to exploit available data from a similar sensor, which involves transforming this data to resemble the new sensor data. However, even taking into account the technical characteristics of both sensors to transform the images, statistical differences between data distributions still remain. This results in a poor performance of the methods trained on one sensor and applied to the new one. In this this work, we propose to use the generative adversarial networks (GANs) framework to adapt the data from the new satellite. In particular, we use Landsat-8 images, with the corresponding ground truth, to perform cloud detection in Proba-V. Results show that the GANs adaptation significantly improves the detection accuracy.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899193","Generative Adversarial Networks;Convolutional Neural Networks;Domain Adaptation;Landsat-8;Proba-V;Cloud Detection","Remote sensing;Earth;Artificial satellites;Cloud computing;Generators;Adaptation models;Gallium nitride","atmospheric techniques;clouds;geophysical image processing;learning (artificial intelligence);remote sensing","domain adaptation;cloud detection;training machine learning algorithms;remote sensing applications;sensible strategy;sensor data;data distributions;generative adversarial networks framework;Landsat-8 images;GAN adaptation;GAN framework;Landsat-8 data;Proba-V data","","3","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Perturbation-Seeking Generative Adversarial Networks: A Defense Framework for Remote Sensing Image Scene Classification","G. Cheng; X. Sun; K. Li; L. Guo; J. Han","Research and Development Institute, Northwestern Polytechnical University, Shenzhen, China; Research and Development Institute, Northwestern Polytechnical University, Shenzhen, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","4 Jan 2022","2022","60","","1","11","The methods for remote sensing image (RSI) scene classification based on deep convolutional neural networks (DCNNs) have achieved prominent success. However, confronted with adversarial examples obtained by adding imperceptible perturbations to clean images, the great vulnerability of DCNNs makes it worth exploring effective defense methods. To date, numerous countermeasures for adversarial examples have been proposed, but how to improve the defensive ability for unknown attacks still to be answered. To address this issue, in this article, we propose an effective defense framework specified for RSI scene classification, named perturbation-seeking generative adversarial networks (PSGANs). In brief, a new training framework is designed to train the classifier by introducing the examples generated during the image reconstruction process, in addition to clean examples and adversarial ones. These generated examples can be random kinds of unknown attacks during training and thus are utilized to eliminate the blind spots of a classifier. To assist the proposed training framework, a reconstruction method is developed. First, instead of modeling the distribution of clean examples, we model the distributions of the perturbations added in adversarial examples. Second, to make a tradeoff between the diversity of the reconstructed examples and the optimization of PSGAN, a scale factor named seeking radius is introduced to scale the generated perturbations before they are subtracted by the given adversarial examples. Comprehensive and extensive experimental results on three widely used benchmarks for RSI scene classification demonstrate the great effectiveness of PSGAN when faced with both known and unknown attacks. Our source code is available at https://github.com/xuxiangsun/PSGAN.","1558-0644","","10.1109/TGRS.2021.3081421","National Science Foundation of China(grant numbers:61772425,61773315); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021B1515020072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442932","Adversarial defense;generative adversarial networks (GANs);image reconstruction;remote sensing image (RSI) scene classification","Training;Image reconstruction;Generators;Perturbation methods;Optimization;Remote sensing;Generative adversarial networks","geophysical image processing;image classification;image reconstruction;learning (artificial intelligence);neural nets;remote sensing;security of data","clean examples;reconstructed examples;generated perturbations;given adversarial examples;RSI scene classification;remote sensing image scene classification;deep convolutional neural networks;DCNNs;imperceptible perturbations;clean images;effective defense methods;defensive ability;effective defense framework;named perturbation-seeking generative adversarial networks;training framework;image reconstruction process;adversarial ones;generated examples;reconstruction method","","16","","58","IEEE","27 May 2021","","","IEEE","IEEE Journals"
"The Application of Semisupervised Attentional Generative Adversarial Networks in Desert Seismic Data Denoising","Y. Li; X. Luo; N. Wu; X. Dong","Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","For imaging and interpretation, high-quality seismic data are necessary. However, noise, which is strong in field desert seismic data, inevitably diminishes the quality of the data and reduces the signal-to-noise ratio. Moreover, the effective signals and noise in field desert seismic data are mostly distributed in the low-frequency band, which leads to severe spectral aliasing. Recently, some deep learning methods have improved the quality of desert seismic data in certain aspects. However, due to limitations of their networks and the serious spectral aliasing of desert seismic data, the denoising results usually show some false seismic reflections. To solve the above problems, we introduce Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (U-GAT-IT) to the denoising of desert seismic data in a semisupervised manner. U-GAT-IT is an unsupervised attentional generative adversarial network (GAN) combined with an attention module guided by the class activation map (CAM). The attention module guided by the CAM can guide the model to better distinguish between noise and effective signals. The experiment shows that the U-GAT-IT can effectively suppress desert seismic noise. Also, the denoising result has fewer false seismic reflections.","1558-0571","","10.1109/LGRS.2021.3073419","National Natural Science Foundation of China(grant numbers:41730422); Cross Disciplinary Research Support Project for Doctoral Candidates of Jilin University(grant numbers:101832020DJX062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9418561","Class activation map (CAM);deep learning (DL);desert seismic data;generative adversarial networks (GANs)","Noise reduction;Noise measurement;Training;Generators;Signal to noise ratio;Generative adversarial networks;Mathematical model","geophysical image processing;geophysical techniques;image denoising;neural nets;seismology;supervised learning;unsupervised learning","unsupervised attentional generative adversarial network;attention module;effective signals;desert seismic noise;denoising result;false seismic reflections;desert seismic data denoising;high-quality seismic data;field desert seismic data;signal-to-noise ratio;semisupervised attentional generative adversarial networks;U-GAT-IT;CAM;class activation map;GAN","","2","","15","IEEE","28 Apr 2021","","","IEEE","IEEE Journals"
"SAR2OPT: Image Alignment Between Multi-Modal Images Using Generative Adversarial Networks","H. Toriya; A. Dewan; I. Kitahara","University of Tsukuba, Japan; Curtin University, Australia; University of Tsukuba, Japan","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","923","926","This work proposes an image-alignment method for multi-modal images (e.g., synthetic aperture radar (SAR) and optical satellite images) using an image-feature-based keypoint-matching algorithm. In applying the matching algorithm to multi-modal images, common features need to be obtained at the corresponding positions. However, the appearances of features among images are different. We solve this issue by translating the appearance of one modal image to the other using generative adversarial networks (GANs). In this work, we attempt to generate optical images from SAR images as a way to extract common features. Through an experiment, we confirm that the proposed method can estimate accurate correspondences between SAR and optical images.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898605","SAR;Multi-Modal;Image Alignment;Generative Adversarial Networks (GAN);Image Feature;Image Matching","Optical imaging;Adaptive optics;Synthetic aperture radar;Radar polarimetry;Generative adversarial networks;Training;Optical detectors","feature extraction;image matching;neural nets;optical images;radar imaging;synthetic aperture radar","SAR2OPT;multimodal images;generative adversarial networks;image-alignment method;image-feature-based keypoint-matching algorithm;optical satellite images;SAR images","","6","","14","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"SAR Image Simulation by Generative Adversarial Networks","X. Bao; Z. Pan; L. Liu; B. Lei","Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9995","9998","SAR image simulation plays an important role in the process of SAR target interpretation and recognition, especially when the number of SAR images is limited. Due to the restriction of acquisition process, the numbers of SAR target images are always insufficient. The traditional SAR image simulation, which is based on calculation of electromagnetic theory, is easily to be affected by parameter distortion due to the lack of joint optimization. Consequently, it makes a big effect on the quality of the simulated images. This paper presents a novel approach, end-to-end models, to simulate the desired images from the SAR image database. A series of generative adversarial networks include DCGAN, weight clipping WGAN and WGAN with gradient penalty are optimized and applied to generate typical SAR target images. Three kinds of network structures are used, include structure of DCGAN, newly proposed structure of four residual blocks networks and Resnet. Experimental results show that the proposed novel method is not only efficient for SAR image simulation, but also can generate excellent SAR images. Furthermore, we analysis the results and the characteristics of different networks, which pave a good way for SAR image simulation based on artificial intelligence method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899286","image simulation;generative adversarial networks;synthetic aperture radar (SAR);image processing","Radar polarimetry;Generative adversarial networks;Synthetic aperture radar;Training;Solid modeling;Generators;Convolution","image recognition;neural nets;radar computing;radar imaging;synthetic aperture radar","generative adversarial networks;SAR image simulation;SAR image database;typical SAR target images;SAR target interpretation;SAR target recognition;acquisition process;electromagnetic theory;parameter distortion;DCGAN;artificial intelligence;residual blocks networks;Resnet","","6","","10","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Hyperspectral Image Classification Based on Generative Adversarial Networks with Feature Fusing and Dynamic Neighborhood Voting Mechanism","Y. Zhan; J. Qin; T. Huang; K. Wu; D. Hu; Z. Zhao; Y. Wang; Y. Cao; R. Jiao; Y. Medjadba; G. Wang; X. Yu","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; Beijing Institute of Geology, Beijing, China; Beijing Institute of Geology, Beijing, China; Beijing Institute of Geology, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; Libary, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","811","814","Classifying Hyperspectral images with few training samples is a challenging problem. The generative adversarial networks (GAN) are promising techniques to address the problems. GAN constructs an adversarial game between a discriminator and a generator. The generator generates samples that are not distinguishable by the discriminator, and the discriminator determines whether or not a sample is composed of real data. In this paper, by introducing multilayer features fusion in GAN and a dynamic neighborhood voting mechanism, a novel algorithm for HSIs classification based on 1-D GAN was proposed. Extracting and fusing multiple layers features in discriminator, and using a little labeled samples, we fine-tuned a new sample 1-D CNN spectral classifier for HSIs. In order to improve the accuracy of the classification, we proposed a dynamic neighborhood voting mechanism to classify the HSIs with spatial features. The obtained results show that the proposed models provide competitive results compared to the state-of-the-art methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899291","Hyperspectral images classification;semi-supervised learning (SSL);generative adversarial networks (GAN);deep learning;spectral-spatial classification","Generative adversarial networks;Feature extraction;Hyperspectral imaging;Semisupervised learning;Image classification;Gallium nitride","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence)","hyperspectral image classification;generative adversarial networks;dynamic neighborhood voting mechanism;multilayer features fusion;HSI classification;CNN spectral classifier;feature extraction;Indian Pines dataset","","4","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Remote Sensing Image Inpainting with Generative Adversarial Networks","A. Kuznetsov; M. Gashnikov","Department of GIS and ITsec, Samara National Research University, Samara, Russia; Department of GIS and ITsec, Samara National Research University, Samara, Russia","2020 8th International Symposium on Digital Forensics and Security (ISDFS)","15 Jun 2020","2020","","","1","6","We investigate generative adversarial neural networks (GAN) for remote sensing image inpainting. We are considering a generative neural network with a contour predictor. We use this neural network to inpainting of the natural remote sensing images obtained by “Canopus”, “Meteor”, “AIST”, “Resurs” aircrafts, as well as Google Earth images. As a basis for comparison, we use an exemplar-based algorithm. We experimentally prove the effectiveness of the generative neural network with the contour predictor for remote sensing image inpainting, in particular for generation forgery Earth remote sensing data.","","978-1-7281-6939-2","10.1109/ISDFS49300.2020.9116347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9116347","image inpainting;remote sensing;generative adversarial neural networks;forgery generation;exemplar -based algorithm","Earth;Neural networks;Digital forensics;Prediction algorithms;Generative adversarial networks;Forgery;Internet","aircraft;geophysical image processing;image restoration;neural nets;remote sensing","contour predictor;remote sensing image inpainting;Earth remote sensing data;generative adversarial neural networks;natural remote sensing images;Google Earth images;AIST aircrafts;Resurs aircrafts;Meteor aircrafts;Canopus aircrafts","","2","","26","IEEE","15 Jun 2020","","","IEEE","IEEE Conferences"
"Shadow Detection in High-Resolution Multispectral Satellite Imagery Using Generative Adversarial Networks","G. Morales; D. Arteaga; S. G. Huamán; J. Telles; W. Palomino","National Institute of Research and Training in Telecommunications (INICTEL-UNI), National University of Engineering; National Institute of Research and Training in Telecommunications (INICTEL-UNI), National University of Engineering; National Institute of Research and Training in Telecommunications (INICTEL-UNI), National University of Engineering; National Institute of Research and Training in Telecommunications (INICTEL-UNI), National University of Engineering; National Institute of Research and Training in Telecommunications (INICTEL-UNI), National University of Engineering","2018 IEEE XXV International Conference on Electronics, Electrical Engineering and Computing (INTERCON)","8 Nov 2018","2018","","","1","4","Detecting shadows in high-resolution satellite images is a challenging task due to the fact that shadows can easily be mistaken for low reflectance soil or water and that such images have limited spectral bands. In this work, we propose a semantic level shadow segmentation by using generative adversarial networks and created a dataset of pre-processed images for training, validation and test. In this way, we trained a generator network that produces shadow masks with condition on a satellite image patch and tries to fool a discriminator, which is trained to discern if a given mask comes from the ground truth or from the generator model. The results achieve an accuracy of 95.85% and a Kappa coefficient of 91.76%, which is superior to the compared methods.","","978-1-5386-5491-0","10.1109/INTERCON.2018.8526416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8526416","Shadow detection;Generative Adversarial Networks;end-to-end learning;satellite image","Satellites;Generators;Training;Generative adversarial networks;Decoding;Measurement;Support vector machines","feature extraction;geophysical image processing;image classification;image segmentation;object detection;remote sensing;soil","generator model;satellite image patch;shadow masks;generator network;pre-processed images;semantic level shadow segmentation;spectral bands;low reflectance soil;high-resolution satellite images;generative adversarial networks;high-resolution multispectral satellite imagery;shadow detection","","4","","15","IEEE","8 Nov 2018","","","IEEE","IEEE Conferences"
"Hyperspectral Plant Disease Forecasting Using Generative Adversarial Networks","A. Förster; J. Behley; J. Behmann; R. Roscher","Institute of Geodesy and Geoinformation, University of Bonn; Institute of Geodesy and Geoinformation, University of Bonn; INRES-Plant Diseases and Plant Protection, University of Bonn; Institute of Geodesy and Geoinformation, University of Bonn","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1793","1796","With a limited amount of arable land, increasing demand for food induced by growth in population can only be meet with more effective crop production and more resistant plants. Since crop plants are exposed to many different stress factors, it is relevant to investigate those factors as well as their behavior and reactions. One of the most severe stress factors are diseases, resulting in a high loss of cultivated plants. Our main objective is the forecasting of the spread of disease symptons on barley plants using a Cycle-Consistent Generative Adversarial Network. Our contributions are: (1) we provide a daily forecast for one week to advance research for better planning of plant protection measures, and (2) in contrast to most approaches which use only RGB images, we learn a model with hyperspectral images, providing an information-rich result. In our experiments, we analyze healthy barley leaves and leaves which were inoculated by powdery mildew. Images of the leaves were acquired daily with a hyperspectral microscope, from day 3 to day 14 after inoculation. We provide two methods for evaluating the predicted time series with respect to the reference time series.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898749","hyperspectral phenotyping;plant disease;barley;generative adversarial networks;deep learning","Diseases;Hyperspectral imaging;Time series analysis;Generators;Generative adversarial networks;Forecasting;Stress","agriculture;crops;diseases;geophysical image processing;plant diseases;time series","barley plants;cycle-consistent generative adversarial network;daily forecast;plant protection measures;RGB images;hyperspectral images;information-rich result;healthy barley leaves;hyperspectral microscope;hyperspectral plant disease forecasting;generative adversarial networks;arable land;effective crop production;resistant plants;crop plants;severe stress factors;cultivated plants;disease symptons","","12","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","L. Maggiolo; D. Solarna; G. Moser; S. B. Serpico","University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2089","2092","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type l2 similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an l2 similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323235","European Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323235","Multisensor image registration;conditional generative adversarial network;$\ell^{2}$ similarity;COBYLA","Radar polarimetry;Optical imaging;Measurement;Optical sensors;Feature extraction;Training;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;image registration;radar imaging;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","automatic area-based registration;generative adversarial networks;correlation-type metric;automatic registration;multisensor remote sensing images;textural properties;optical-SAR images;deep learning;area-based registration concepts;conditional generative adversarial network;area-based cross-correlation-type l;image translation capabilities;Sentinel-2 data","","4","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Synthetic aperture radar ship discrimination, generation and latent variable extraction using information maximizing generative adversarial networks","C. P. Schwegmann; W. Kleynhans; B. P. Salmon; L. W. Mdakane; R. G. V. Meyer","Department of Electrical, Electronic and Computer Engineering, University of Pretoria, South Africa; Department of Electrical, Electronic and Computer Engineering, University of Pretoria, South Africa; School of Engineering and ICT, University of Tasmania, Australia; Department of Electrical, Electronic and Computer Engineering, University of Pretoria, South Africa; Department of Electrical, Electronic and Computer Engineering, University of Pretoria, South Africa","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2263","2266","A major task in any discrimination scenario requires the collection and validation of as many examples as possible. Depending on the type of data this can be a time consuming process, especially when dealing with large remote sensing data such as Synthetic Aperture Radar imagery. To aid in the creation of improved machine learning-based ship detection and discrimination methods this paper applies a type of neural network known as an Information Maximizing Generative Adversarial Network. Generative Adversarial Networks pit a generating and discriminating network against each other. A generator tries to create samples that are indistinguishable from real data whereas the discriminator tries to identify whether a sample is real or generated. Information Maximizing Generative Adversarial Network extend this idea by extracting untangled latent variables as part of the discrimination process which help to classify the data in terms of categories/classes and properties such as ship rotation. Despite the limited size and class distribution of the dataset, the paper showed that the trained network was able to generate convincing samples from the three given classes as well as create a discriminator that performs similarly to state-of-the-art ship discrimination methods despite using no labels for training.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127440","Synthetic aperture radar;Machine learning;Marine technology","Marine vehicles;Synthetic aperture radar;Generators;Oceans;Gallium nitride;Training;Remote sensing","geophysical image processing;image classification;learning (artificial intelligence);marine radar;neural nets;radar imaging;remote sensing by radar;ships;synthetic aperture radar","state-of-the-art ship discrimination methods;information maximizing generative adversarial networks;ship detection;discrimination process;synthetic aperture radar imagery;synthetic aperture radar ship discrimination;machine learning-based ship detection","","9","","8","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Retro-Remote Sensing With Doc2Vec Encoding","M. B. Bejiga; G. Hoxha; F. Melgani","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","2 Jun 2020","2020","","","89","92","In this work, we attempt to address the issue of developing a sophisticated text encoder for retro-remote sensing application. The encoder converts ancient landscape descriptions into a fixed-size vector that, adequately, represents the available information. This vector is then used as a conditioning data to a Generative adversarial network (GAN) that synthesizes the equivalent image. We propose using a pre-trained Doc2Vec encoder for text encoding and train a Wasserstein GAN (a variant of GAN) to convert landscape descriptions written by travelers and geographers into the equivalent image. Qualitative and quantitative analysis of the generated images signify usefulness of the proposed method.","","978-1-7281-2190-1","10.1109/M2GARSS47143.2020.9105139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105139","Deep learning;Generative adversarial networks;Retro-remote sensing;Text embedding;Text-to-image synthesis","Knowledge engineering;Image coding;Statistical analysis;Semantics;Geoscience and remote sensing;Machine learning;Generative adversarial networks","geophysical image processing;image coding;neural nets;remote sensing;text analysis;vectors","ancient landscape descriptions;fixed-size vector;text encoding;Wasserstein GAN;text encoder;retro-remote sensing;Doc2Vec encoder;generative adversarial network;Doc2Vec encoding","","2","","11","IEEE","2 Jun 2020","","","IEEE","IEEE Conferences"
"Towards Generating Remote Sensing Images of the Far Past","M. B. Bejiga; F. Melgani","Department of Information Engineering and Computer science, University of Trento, Trento, Italy; Department of Information Engineering and Computer science, University of Trento, Trento, Italy","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9502","9505","Text-to-image synthesis is a research topic that has not yet been addressed by the remote sensing community. It consists in learning a mapping from text description to image pixels. In this paper, we propose to address this topic for the very first time. More specifically, our objective is to convert ancient text descriptions of geographic areas written by past explorers into an equivalent remote sensing image. To this effect, we rely on generative adversarial networks (GANs) to learn the mapping. GANs aim to represent the distribution of a dataset using weights of a deep neural network, which are trained as an adversarial competition between two networks. We collected ancient texts dating back to 7 BC to train our network and obtained interesting results, which form the basis to highlight future research directions to advance this new topic.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899834","GANs;text-to-image synthesis;remote sensing","Remote sensing;Generators;Generative adversarial networks;Training;Gallium nitride;Cost function;Mathematical model","geophysical image processing;learning (artificial intelligence);neural nets;remote sensing;text analysis;text detection","text-to-image synthesis;text description;image pixels;generative adversarial networks;GAN;deep neural network;remote sensing image generation","","","","18","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"A Super Resolution Method for Remote Sensing Images Based on Cascaded Conditional Wasserstein GANs","B. Liu; H. Li; Y. Zhou; Y. Peng; A. Elazab; C. Wang","Northwest China Research Institute of Electronics Equipment, Xi’an, China; Northwest China Research Institute of Electronics Equipment, Xi’an, China; Northwest China Research Institute of Electronics Equipment, Xi’an, China; Shenzhen Institutes of Adavanced Technology, Chinese Academy of Sciences, Shenzhen, China; Computer Science Department, Misr Higher Institute for Commerce and Computers, Mansoura City, Egypt; University of Science and Technology of China, Hefei, P.R.China","2020 IEEE 3rd International Conference on Information Communication and Signal Processing (ICICSP)","20 Oct 2020","2020","","","284","289","High-resolution (HR) remote sensing imagery is quite beneficial for subsequent interpretation. Obtaining HR images can be achieved by upgrading the imaging device. Yet, the cost to perform this task is very huge. Thus, it is necessary to obtain HR images from low-resolution (LR) ones. In the literature, the super-resolution image reconstruction methods based on deep learning have unparalleled advantages in comparison to traditional reconstruction methods. This work is inspired by these current mainstream methods and proposes a novel cascaded conditional Wasserstein generative adversarial network (CCWGAN) architecture with the residual dense block to generate high quality remote sensing images. We validate the proposed method on the NWPU VHR-10 dataset. Experimental results show our CCWGAN method has superior performance compared with the state-of-the-art GAN methods.","","978-1-7281-8823-2","10.1109/ICICSP50920.2020.9232066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9232066","remote sensing images;cascaded conditional generative adversarial networks;wasserstein generative adversarial networks;residual dense block","Gallium nitride;Image edge detection;Remote sensing;Generative adversarial networks;Training;Generators;Deep learning","convolutional neural nets;geophysical image processing;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","super-resolution image reconstruction methods;conditional Wasserstein generative adversarial network architecture;high quality remote sensing images;CCWGAN method;super resolution method;high-resolution remote sensing imagery;HR images;imaging device;cascaded conditional Wasserstein GAN;NWPU VHR-10 dataset","","","","18","IEEE","20 Oct 2020","","","IEEE","IEEE Conferences"
"Comparison on Generative Adversarial Networks –A Study","A. Sharma; N. Jindal; A. Thakur","Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India; Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India; Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India","2018 First International Conference on Secure Cyber Computing and Communication (ICSCCC)","2 May 2019","2018","","","391","396","Various new deep learning models have been invented, among which generative adversarial networks have gained exceptional prominence in last four years due to its property of image synthesis. GANs have been utilized in diverse fields ranging from conventional areas like image processing, biomedical signal processing, remote sensing, video generation to even off beat areas like sound and music generation. In this paper, we provide an overview of GANs along with its comparison with other networks, as well as different versions of Generative Adversarial Networks.","","978-1-5386-6373-8","10.1109/ICSCCC.2018.8703267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703267","Machine learning;Generative Adversarial Networks","Generative adversarial networks;Training;Generators;Gallium nitride;Loss measurement;Computational modeling","image processing;learning (artificial intelligence)","deep learning models;image synthesis;GANs;image processing;biomedical signal processing;video generation;music generation;sound generation;generative adversarial networks","","3","","22","IEEE","2 May 2019","","","IEEE","IEEE Conferences"
"Hard Ship Detection via Generative Adversarial Networks","J. Ma; Z. Zhou; B. Wang; Z. An","School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing; State Key Laboratory of Advanced Power Transmission Technology, Beijing","2019 Chinese Control And Decision Conference (CCDC)","12 Sep 2019","2019","","","3961","3965","In optical remote sensing images, many ships have very similar shapes and textures with backgrounds. In this case, it is very hard to accurately detect these ships. In this paper, we introduce generative adversarial networks (GANs) to perform hard ship detection. GANs consist of one generative network and one discriminator network. We take state-of-the-art object (ship) detection network Faster R-CNN as the generative network, which outputs the detection results as fake samples. The ground-truth ships in the input image are set as the real samples. The discriminator network is responsible for distinguishing between fake samples and real samples. The two networks are simultaneously trained. Through continuous adversarial training, the fake samples generated by the generative network can be very similar to the real samples, and the discriminator network would not correctly distinguish between fake samples and real samples. As a result, the ship detection network (generative network) correctly recognizes hard-detection ships, producing satisfactory detection results. What's more, the discriminator network is only used in training process, and thus the proposed method not only improves detection accuracy, but also does not increase computational cost.","1948-9447","978-1-7281-0106-4","10.1109/CCDC.2019.8833176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8833176","Ship detection;Generative adversarial networks;Convolutional neural networks","Marine vehicles;Optical imaging;Optical sensors;Training;Remote sensing;Proposals;Gallium nitride","convolutional neural nets;geophysical image processing;image classification;learning (artificial intelligence);object detection;remote sensing;ships","hard-detection ships;discriminator network;hard ship detection;generative adversarial networks;generative network;ground-truth ships;ship detection network;Faster R-CNN;object detection network;optical remote sensing images","","1","","17","IEEE","12 Sep 2019","","","IEEE","IEEE Conferences"
"Self-Attentive Generative Adversarial Network for Cloud Detection in High Resolution Remote Sensing Images","Z. Wu; J. Li; Y. Wang; Z. Hu; M. Molinier","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, China; VTT Technical Research Centre of Finland, Ltd., Espoo, Finland","IEEE Geoscience and Remote Sensing Letters","24 Sep 2020","2020","17","10","1792","1796","Cloud detection is an important step in the processing of remote sensing images. Most methods based on convolutional neural networks (CNNs) for cloud detection require pixel-level labels, which are time-consuming and expensive to annotate. To overcome this challenge, this letter proposes a novel semisupervised algorithm for cloud detection by training a self-attentive generative adversarial network (SAGAN) to extract the feature difference between cloud images and cloud-free images. Our main idea is to introduce visual attention into the process of generating “real” cloud-free images. The training of SAGAN is based on three guiding principles: expansion of attention maps of cloud regions which will be replaced with translated cloud-free images, reduction of attention maps to coincide with cloud boundaries, and optimization of a self-attentive network to handle the extreme cases. The inputs for SAGAN training are the images and image-level labels, which are easier, cheaper, and more time-saving than the existing methods based on CNN. To test the performance of SAGAN, experiments are conducted on the Sentinel-2A Level 1C image data. The results show that the proposed method achieves very promising results with only the image-level labels of training samples.","1558-0571","","10.1109/LGRS.2019.2955071","National Key Research and Development Program of China(grant numbers:2017YFC0506200); National Natural Science Foundation of China(grant numbers:41501369,41871227); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924781","Cloud detection;deep learning (DL);generative adversarial network (GAN);remote sensing;self-attention","Image restoration;Remote sensing;Gallium nitride;Generative adversarial networks;Clouds;Training;Optimization","clouds;convolutional neural nets;feature extraction;geophysical image processing;image classification;image segmentation;learning (artificial intelligence);remote sensing","image-level labels;SAGAN;Level 1C image data;self-attentive generative adversarial network;cloud detection;high resolution remote sensing images;convolutional neural networks;visual attention;attention maps;cloud regions;translated cloud-free images;cloud boundaries;Sentinel-2A Level 1C image data;semisupervised algorithm","","17","","19","IEEE","5 Dec 2019","","","IEEE","IEEE Journals"
"Generation of Lidar-Predicted Forest Biomass Maps from Radar Backscatter with Conditional Generative Adversarial Networks","S. Björk; S. N. Anfinsen; E. Næsset; T. Gobakken; E. Zahabu","Department of Physics and Technology, UiT The Arctic University of Norway, Tromsø, Norway; Department of Physics and Technology, UiT The Arctic University of Norway, Tromsø, Norway; Faculty of Environmental Sciences and Natural Resource Management, Norwegian University of Life Sciences, Ås, Norway; Faculty of Environmental Sciences and Natural Resource Management, Norwegian University of Life Sciences, Ås, Norway; Department of Forest Resources Assessment and Management, Sokoine University of Agriculture, Morogoro, Tanzania","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","4327","4330","This paper studies the generation of LiDAR-predicted aboveground biomass (AGB) maps from synthetic aperture radar (SAR) intensity images by use of conditional generative adversarial networks (cGANs). The purpose is to improve on traditional regression models based on SAR intensity, which are trained with a limited amount of AGB in situ measurements. Although they are costly to collect, data from airborne laser scanning (ALS) sensors are highly correlated with AGB and can replace in situ measurements as the regression target. Thus, the amount of training data increases dramatically, and we can learn an expressive two-stage regression model for SAR backscatter intensity. We propose to model the regression function between SAR intensity and ALS-predicted AGB with a Pix2Pix convolutional neural network for image translation that uses a ResNet-5-based cGAN architecture with the Wasserstein GAN gradient penalty (WGAN-GP) objective function. The synthesized ALS-predicted AGB maps are evaluated qualitatively and quantitatively against real ALS-predicted AGB maps. Our results show that the proposed architecture manages to capture characteristics of the real data, which suggests further use of the ResNet-5 for a SAR intensity regression model of AGB.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324296","Tanzania Forest Services (TFS) Agency; Norwegian University of Life Sciences; Swedish University of Agricultural Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324296","","Gallium nitride;Training;Synthetic aperture radar;Linear programming;Generative adversarial networks;Laser radar;Forestry","forestry;neural nets;optical radar;regression analysis;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar;vegetation mapping","synthesized ALS-predicted AGB maps;SAR intensity regression model;lidar-predicted forest biomass maps;radar backscatter;conditional generative adversarial networks;LiDAR-predicted aboveground biomass maps;synthetic aperture radar intensity images;traditional regression models;airborne laser scanning sensors;regression target;two-stage regression model;SAR backscatter intensity;regression function;Pix2Pix convolutional neural network;image translation;ResNet-5-based;Wasserstein GAN gradient penalty objective function","","2","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Refocusing of SAR Ground Moving Target Based on Generative Adversarial Networks","W. Tang; J. Qian; L. Wang; Y. Wang","UESTC, School of Resources and Environment, Chengdu, Sichuan, China; School of Information and Communication Engineering, UESTC, Chengdu, Sichuan, China; Institute for Infocomm Research (I2R), Agency for Science, Technology and Research (A*STAR), Singapore; Dept. of Geography, Planning, and Environment, East Carolina University, Greenville, NC, USA","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","755","758","Due to the range and azimuth velocity of moving targets, severe defocusing occurs in synthetic aperture radar (SAR) images. The traditional ground moving target imaging algorithm generally needs to estimate the parameters of the moving target, and then conduct the refocusing of the moving target according to the estimated parameters. In this paper, a SAR moving target refocusing algorithm based on generative adversarial network (GAN) is proposed without estimating the motion parameters of the targets. To get a sufficiently trained network, we propose to use simulated moving target data to train the model and evaluate its performance using real data. The results of numerical experiments show that the trained network using simulated data can be well transferred to real test data and effectively achieve to refocus multiple moving targets with distinct velocities at the circumstance of heavy noise.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884310","National Natural Science Foundation of China(grant numbers:61401077); China Postdoctoral Science Foundation(grant numbers:2015M580784); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884310","Synthetic aperture radar (SAR);ground moving target;refocusing algorithm;generative adversarial networks (GAN)","Training;Imaging;Geoscience and remote sensing;Radar imaging;Generative adversarial networks;Data models;Radar polarimetry","image denoising;image motion analysis;neural nets;radar computing;radar imaging;synthetic aperture radar;target tracking","generative adversarial network;sufficiently trained network;multiple moving targets;SAR ground moving target;synthetic aperture radar images;traditional ground moving target;azimuth velocity;GAN;real data performance","","","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Automatic Large-Scale 3D Building Shape Refinement Using Conditional Generative Adversarial Networks","K. Bittner; M. Körner","German Aerospace Center - DLR, Munich, Germany; Technical University of Munich, Munich, Germany","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","1968","19682","Three-dimensional realistic representations of buildings in urban environments have been increasingly applied as data sources in a growing number of remote sensing fields such as urban planning and city management, navigation, environmental simulation (i.e. flood, earthquake, air pollution), 3D change detection after events like natural disasters or conflicts, etc. With recent technological developments, it becomes possible to acquire high-quality 3D input data. There are two main ways to obtain elevation information: from active remote sensing systems, such as light detection and ranging (LIDAR), and from passive remote sensing systems, such as optical images, which allow the acquisition of stereo images for automatic digital surface models (DSMs) generation. Although airborne laser scanning provides very accurate DSMs, it is a costly method. On the other hand, the DSMs from stereo satellite imagery show a large coverage and lower costs. However, they are not as accurate as LIDAR DSMs. With respect to automatic 3D information extraction, the availability of accurate and detailed DSMs is a crucial issue for automatic 3D building model reconstruction. We present a novel methodology for generating a better-quality stereo DSM with refined buildings shapes using a deep learning framework. To this end, a conditional generative adversarial network (cGAN) is trained to generate accurate LIDAR DSM-like height images from noisy stereo DSMs.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575415","","Three-dimensional displays;Buildings;Shape;Laser radar;Image segmentation;Remote sensing;Generative adversarial networks","geophysical image processing;image reconstruction;image resolution;optical radar;remote sensing;remote sensing by laser beam;solid modelling;stereo image processing;town and country planning","automatic large-scale 3D building shape refinement;conditional generative adversarial network;urban environments;data sources;growing number;remote sensing fields;urban planning;city management;environmental simulation;air pollution;3D change detection;natural disasters;recent technological developments;high-quality 3D input data;elevation information;active remote sensing systems;light detection;passive remote sensing systems;optical images;stereo images;automatic digital surface models generation;airborne laser scanning;costly method;stereo satellite imagery;automatic 3D information extraction;automatic 3D building model reconstruction;better-quality stereo DSM;refined buildings shapes;accurate LIDAR DSM-like height images;noisy stereo DSMs","","7","","9","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"BFGAN – building footprint extraction from satellite images","Y. Shi; Q. Li; X. X. Zhu","Chair of Remote Sensing Technology (LMF), Technische Universität München (TUM); Signal Processing in Earth Observation (SIPEO), Technische Universität München (TUM); Signal Processing in Earth Observation (SIPEO), Technische Universität München (TUM)","2019 Joint Urban Remote Sensing Event (JURSE)","22 Aug 2019","2019","","","1","4","Building footprint information is an essential ingredient for 3-D reconstruction of urban models. The automatic generation of building footprints from satellite images presents a considerable challenge due to the complexity of building shapes. In this work, we have proposed improved generative adversarial networks (GANs) for the automatic generation of building footprints from satellite images. We used a conditional GAN with a cost function derived from the Wasserstein distance and added a gradient penalty term. The achieved results indicated that the proposed method can significantly improve the quality of building footprint generation compared to conditional generative adversarial networks, the U-Net, and other networks. In addition, our method nearly removes all hyperparameter tuning.","2642-9535","978-1-7281-0009-8","10.1109/JURSE.2019.8809048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809048","building footprint;generative adversarial networks (GANs);conditional generative adversarial networks (CGANs);Wasserstein generative adversarial networks (WGANs)","Buildings;Generators;Training;Gallium nitride;Generative adversarial networks;Satellites;Stability analysis","feature extraction;geophysical image processing;image resolution","3-D reconstruction;urban models;automatic generation;satellite images;building shapes;conditional GAN;building footprint generation;conditional generative adversarial networks;building footprint information;BFGAN","","2","","13","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Visual Prediction of Tropical Cyclones with Deep Convolutional Generative Adversarial Networks","P. Xie; F. Meng; B. Li; Y. Li; Z. Yu; H. Sun; T. Song; D. Xu","College of Computer Science and Technology, China University of Petroleum, Qingdao, China; School of Geosciences, China University of Petroleum, Qingdao, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; College of Computer Science and Technology, China University of Petroleum, Qingdao, China; College of Computer Science and Technology, China University of Petroleum, Qingdao, China; College of Computer Science and Technology, China University of Petroleum, Qingdao, China; College of Computer Science and Technology, China University of Petroleum, Qingdao, China; Southern Marine Science and Engineering Guangdong Laboratory (Zhuhai), Zhuhai, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","8297","8300","The prediction of tropical cyclones (TCs) is a valuable and challenging task. As a research hotspot of deep learning, generative adversarial network (GAN) has obtained promising results in TC prediction recently. However, different kinds of GAN applied in meteorology usually focus on how to generate high-quality images, but ignore how GAN learns the physical characteristics in the training process. This paper visualizes the intermediate results of GAN in the training process, and shows the process of learning the physical characteristics of data distribution similar to TC images for GAN. The core method in this paper is deep convolutional generative adversarial networks (DCGAN). In order to obtain prediction results with interpretable physical characteristics, we propose two training strategies for DCGAN, namely the long short-term training method and training parameters selection according to physical characteristics. Experimental results show that the DCGAN model and two strategies proposed in this paper have good performance in the visual prediction of TCs.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554325","National Key Research and Development Program(grant numbers:2018YFC1406205,2018YFC1406201); Natural Science Foundation of China(grant numbers:U1811464); Natural Science Foundation of Shandong Province(grant numbers:ZR2019MF012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554325","Generative adversarial networks;tropical cyclones;visual prediction;meteorological satellite images","Training;Deep learning;Visualization;Satellites;Tropical cyclones;Geoscience and remote sensing;Predictive models","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;storms","deep convolutional generative adversarial networks;deep learning;TC prediction;high-quality images;training process;TC images;interpretable physical characteristics;short-term training method;training parameters selection;tropical cyclone visual prediction;meteorology;data distribution;DCGAN model","","","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Application of Conditional Generative Adversarial Networks for Generation of Micro-Doppler Signatures of Different Aspect Angles","I. Alnujaim; Y. Kim","California State University, Fresno, USA; California State University, Fresno, USA","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5056","5058","We have applied the generative adversarial networks (GANs) to produce micro-Doppler signatures from different radar aspect angles. Although the micro-Doppler signatures play a critical role in target classification, the signature is a function of a radar aspect angle. Obtaining diverse signatures of a target from different aspect angles requires labor as well as monetary costs. To circumvent this issue, we synthesis the micro-Doppler signatures of diverse aspect angles from a given micro-Doppler signature of a single aspect angle. We trained separate GANs through simulated micro-Doppler signatures for each angle. The trained GANs produce artificial micro-Doppler signatures of the desired angle when micro-Doppler signatures from a certain angle are inputted. The synthesized micro-Doppler signatures from GANs and the true ones obtained by simulations are compared.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553743","NSF(grant numbers:EIA-0196217); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553743","Micro-Doppler signatures;Radar imagery classification;Generative adversarial networks;Deep learning","Costs;Radar;Generative adversarial networks;Spectrogram","Doppler radar;electromagnetic wave scattering;gallium compounds;image classification;radar target recognition;wide band gap semiconductors","microDoppler signature;different aspect angles;different radar aspect angles;simulated microDoppler signatures","","","","6","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"E-DBPN: Enhanced Deep Back-Projection Networks for Remote Sensing Scene Image Superresolution","Y. Yu; X. Li; F. Liu","Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China.; The Chinese University of Hong Kong, Hong Kong; Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China.","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2020","2020","58","8","5503","5515","Generative adversarial networks (GANs) have been widely used to single image superresolution (SR) (SISR), and these GAN-based methods have achieved significant performance on natural images due to their abilities to generate realistic textures. However, previous GAN-based methods are of poor performance when applied to remote sensing scene image SR. In order to further enhance the visual quality, in this article, we present a GAN-based SISR method by proposing a novel generator, which is capable of generating perceptually pleasing remote sensing scene images. First, we design the enhanced deep back-projection network (E-DBPN) generators based on the architecture of the original DBPN and mainly make two modifications. The first one is to add the proposed enhanced residual channel attention module (ERCAM) into the original DBPN, which can keep good properties of the original input features but also has the ability to emphasize more important features and suppressing less useful features. The other is to replace the concatenation operation with the proposed sequential feature fusion module (SFFM) for dealing with the feature maps generated by different up-projection units discriminatorily. As for the training process, the E-DBPN generator is first trained using the mean squared error (MSE) loss. Next, in order to improve the perceptual quality of the recovered images, we employ the content loss and the adversarial loss to train our initialized generator network. Experiments show that our method achieves state-of-the-art performance compared to other SISR methods.","1558-0644","","10.1109/TGRS.2020.2966669","National Natural Science Foundation of China(grant numbers:71771216,71701209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8974409","Attention mechanism;generative adversarial networks (GANs);perceptual quality;remote sensing scene image;single image superresolution (SISR)","Remote sensing;Generators;Task analysis;Generative adversarial networks;Training;Deep learning","feature extraction;image enhancement;image reconstruction;image resolution;image texture;mean square error methods;neural nets;remote sensing","deep back-projection network;enhanced residual channel attention module;DBPN;remote sensing scene images;GAN-based SISR method;remote sensing scene image SR;natural images;generative adversarial networks;remote sensing scene image superresolution;SISR methods;initialized generator network;E-DBPN generator;feature maps;sequential feature fusion module","","8","","88","IEEE","29 Jan 2020","","","IEEE","IEEE Journals"
"Mixture of Spectral Generative Adversarial Networks for Imbalanced Hyperspectral Image Classification","T. Dam; S. G. Anavatti; H. A. Abbass","School of Engineering and Information Technology, University of New South Wales Canberra, Canberra, ACT, Australia; School of Engineering and Information Technology, University of New South Wales Canberra, Canberra, ACT, Australia; School of Engineering and Information Technology, University of New South Wales Canberra, Canberra, ACT, Australia","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","We propose a three-player spectral generative adversarial network (GAN) architecture to afford GAN the ability to manage minority classes under imbalanced conditions. A class-dependent mixture generator spectral GAN (MGSGAN) was developed to force generated samples to remain within the actual distribution of the data. MGSGAN was able to generate minority classes, even when the imbalanced ratio of majority to minority classes was high. A classifier based on lower features was adopted along with a sequential discriminator to develop a three-player GAN game. The generative networks performed data augmentation to improve the classifier ’ s performance. The proposed method was validated using two hyperspectral image data sets and compared with state-of-the-art methods in two class-imbalanced settings corresponding with real data distributions.","1558-0571","","10.1109/LGRS.2020.3041864","University International Postgraduate Award (UIPA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9295333","Class imbalance;mixture generators spectral generative adversarial networks (GANs)","Generative adversarial networks;Generators;Gallium nitride;Hyperspectral imaging;Games;Feature extraction;Testing","game theory;hyperspectral imaging;image classification;neural nets","imbalanced hyperspectral image classification;three-player spectral generative adversarial network architecture;class-dependent mixture generator spectral GAN;MGSGAN;three-player GAN game;data distributions","","1","","18","IEEE","15 Dec 2020","","","IEEE","IEEE Journals"
"Residential area extraction based on conditional generative adversarial networks","F. Jin; F. Wang; J. Rui; Z. Liu; C. Wang; H. Zhang","Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; Chinese Academy of Sciences, Beijing, Beijing, CN; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China","2017 SAR in Big Data Era: Models, Methods and Applications (BIGSARDATA)","30 Nov 2017","2017","","","1","5","Automatic extraction of residential area from SAR image is a difficult task due to its complexity. The traditional methods based on segmentation or classification are effective. In this article, we present a novel method based on conditional generative adversarial networks (CGANs) to extract regular residential area in rural and urban region. CGANs is applied to extend the scope of research from supervised learning to semi supervised learning and adversarial training is used to improve the training effect. The experimental results show that the proposed method can achieve better results than traditional methods.","","978-1-5386-4519-2","10.1109/BIGSARDATA.2017.8124931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8124931","residential area;object extraction;convolutional neural networks;semi-supervised learning;conditional generative adversarial networks","Training;Remote sensing;Gallium nitride;Generators;Feature extraction;Machine learning;Image segmentation","image classification;image segmentation;learning (artificial intelligence);radar computing;radar imaging;synthetic aperture radar","urban region;semisupervised learning;adversarial training;residential area extraction;conditional generative adversarial networks;automatic extraction;regular residential area;rural region;SAR image segmentation;CGAN;supervised learning","","3","","11","IEEE","30 Nov 2017","","","IEEE","IEEE Conferences"
"Rotation Consistency-Preserved Generative Adversarial Networks for Cross-Domain Aerial Image Semantic Segmentation","T. Shi; Y. Li; Y. Zhang","School of Remote Sensing and Information Engineering, Wuhan University, China; School of Remote Sensing and Information Engineering, Wuhan University, China; School of Remote Sensing and Information Engineering, Wuhan University, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","8668","8671","Due to its wide applications, aerial image semantic segmentation attracts increasing research interest in recent years. As well known, deep semantic segmentation network (DSSN) has been widely used to deal with aerial image segmentation and achieves spectacular success. However, when applying the DSSN trained with the labeled aerial images (i.e., the source domain) to predict the aerial images acquired with different acquisition conditions (i.e., the target domain), the performance often dramatically degrades. To alleviate the negative influence of cross-domain data shift, this paper proposes a domain adaptation approach to deal with cross-domain aerial image semantic segmentation. More precisely, this paper proposes a novel rotation consistency-preserved generative adversarial network (RCP-GAN) to carry out domain adaptation for mapping aerial images in the source domain to the target domain. Furthermore, the mapped aerial imageries with labels are used to train DSSN, which is further used to classify aerial imagery in the target domain. To verify the validity of the presented approach, we give two cross-domain experimental settings including: (I) variation of geographic location; (II) variation of both geographic location and imaging mode. Extensive experiments under two typical cross-domain settings show that our proposed method can effectively address the domain shift problem and outperform the state-of-the-art methods with a large margin.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554606","National Natural Science Foundation of China(grant numbers:42030102,41971284); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554606","Rotation consistency-preserved generative adversarial network (RCP-GAN);cross-domain aerial image semantic segmentation;domain adaptation;unsupervised style transfer","Image segmentation;Adaptation models;Costs;Semantics;Imaging;Geoscience and remote sensing;Generative adversarial networks","deep learning (artificial intelligence);geophysical image processing;image classification;image segmentation","aerial imagery;target domain;cross-domain experimental settings;imaging mode;domain shift problem;cross-domain aerial image semantic segmentation;deep semantic segmentation network;DSSN;labeled aerial images;source domain;cross-domain data shift;domain adaptation;aerial image mapping;rotation consistency-preserved generative adversarial network;RCP-GAN;aerial imagery classification;geographic location","","2","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Super-Resolution of Remote Sensing Images Based on Transferred Generative Adversarial Network","W. Ma; Z. Pan; J. Guo; B. Lei","Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1148","1151","Single image super-resolution (SR) has been widely studied in recent years as a crucial technique for remote sensing applications. This paper proposes a SR method for remote sensing images based on a transferred generative adversarial network (TGAN). Different from the previous GAN-based SR approaches, the novelty of our method mainly reflects from two aspects. First, the batch normalization layers are removed to reduce the memory consumption and the computational burden, as well as raising the accuracy. Second, our model is trained in a transfer-learning fashion to cope with the insufficiency of training data, which is the crux of applying deep learning methods to remote sensing applications. The model is firstly trained on an external dataset DIV2K and further fine-tuned with the remote sensing dataset. Our experimental results demonstrate that the proposed method is superior to SRCNN and SRGAN in terms of both the objective evaluation and the subjective perspective.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517442","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517442","Remote sensing images;super-resolution;generative adversarial network;transfer learning","Image resolution;Remote sensing;Signal resolution;Training;Generative adversarial networks;Knowledge engineering;Task analysis","geophysical image processing;image resolution;learning (artificial intelligence);remote sensing","remote sensing images;transferred generative adversarial network;single image super-resolution;remote sensing applications;SR method;previous GAN-based SR approaches;transfer-learning fashion;deep learning methods;remote sensing dataset","","23","","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Motion Deblurring via Using Generative Adversarial Networks for Space-Based Imaging","Y. Chen; F. Wu; J. Zhao","University of Chinese Academy of Sciences, UCAS, Beijing, P.R.China; University of Chinese Academy of Sciences, UCAS, Beijing, P.R.China; University of Chinese Academy of Sciences, UCAS, Beijing, P.R.China","2018 IEEE 16th International Conference on Software Engineering Research, Management and Applications (SERA)","30 Sep 2018","2018","","","37","41","In some missions of NanoSats, we find images captured are disturbed by motion blur which caused under the situation that NanoSats work in low-earth orbit at high speeds. In this paper, we address the problem of deblurring images degraded due to space-based imaging system shaking or movements of observing targets. We propose a motion deblurring strategy via using Generative Adversarial Networks(GAN) to realize an end-to-end image processing without kernel estimation in orbit. We combine Wasserstein GAN(WGAN) and loss function based on adversarial loss and perceptual loss to optimize the result of deblurred image. The experimental results on the two different datasets prove the feasibility and effectiveness of the proposed strategy which outperforms the state-of-the-art blind deblurring algorithms using for remote sensing images both quantitatively and qualitatively.","","978-1-5386-5886-4","10.1109/SERA.2018.8477191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8477191","NanoSats;Space-Based Imaging;Motion De-blurring;Generative Adversarial Networks","Imaging;Image restoration;Kernel;Satellites;Orbits;Generators;Gallium nitride","aerospace computing;artificial satellites;image denoising;image motion analysis;image restoration;remote sensing","motion blur;low-earth orbit;observing targets;motion deblurring strategy;end-to-end image processing;kernel estimation;loss function;adversarial loss;perceptual loss;deblurred image;remote sensing images;generative adversarial networks;Wasserstein GAN;NanoSats;blind deblurring algorithms","","3","","18","IEEE","30 Sep 2018","","","IEEE","IEEE Conferences"
"Translating Multispectral Imagery to Nighttime Imagery via Conditional Generative Adversarial Networks","X. Huang; D. Xu; Z. Li; C. Wang","Department of Geography, University of South Carolina, Columbia, SC, U.S.A; School of Geography Science, East China Normal University, Shanghai, China; Department of Geography, University of South Carolina, Columbia, SC, U.S.A; Department of Geography, University of South Carolina, Columbia, SC, U.S.A","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6758","6761","Nighttime satellite imagery has been applied in a wide range of fields. However, our limited understanding of how observed light intensity is formed and whether it can be simulated greatly hinders its further application. This study explores the potential of conditional Generative Adversarial Networks (cGAN) in translating multispectral imagery to nighttime imagery. A popular cGAN framework, pix2pix, was adopted and modified to facilitate this translation using gridded training image pairs derived from Landsat 8 and Visible Infrared Imaging Radiometer Suite (VIIRS). The results of this study prove the possibility of multispectral-to-nighttime translation and further indicate that, with the additional social media data, the generated nighttime imagery can be very similar to the ground-truth imagery. This study fills the gap in understanding the composition of satellite observed nighttime light and provides new paradigms to solve the emerging problems in nighttime remote sensing fields, including nighttime series construction, light desaturation, and multi-sensor calibration.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323669","generative adversarial network;image translation;nighttime imagery","Remote sensing;Social networking (online);Generators;Training;Measurement;Earth;Reflectivity","calibration;geophysical image processing;image sensors;infrared imaging;remote sensing","cGAN framework;VIIRS;multispectral-to-nighttime translation;Visible Infrared Imaging Radiometer Suite;gridded training image pairs;pix2pix;light intensity;nighttime satellite imagery;conditional Generative Adversarial Networks;multispectral imagery;nighttime series construction;nighttime remote sensing fields;nighttime light;ground-truth imagery;generated nighttime imagery","","","","9","USGov","17 Feb 2021","","","IEEE","IEEE Conferences"
"REL-SAGAN: Relative Generation Adversarial Network Integrated With Attention Mechanism for Scene Data Augmentation of Remote Sensing","Y. Cao; B. Sui; W. Zhang","Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 May 2022","2022","15","","3107","3119","Deep learning shows a strong ability in target detection, scene classification, and change detection of remote sensing. However, the training process requires a large number of samples, and the production of most high-quality training samples requires a lot of time and human resources. With the resolution of remote sensing image gradually improved, and the scene information becoming more and more complicated, the sample augmentation methods at this stage have shown obvious defects, such as the loss of sample key information and the lack of diversity in augmented data sets. In order to solve these problems, this article proposes a new augmentation method for remote sensing scene data named Rel-SAGAN based on generative adversarial networks (GAN). First, combined with self-attention mechanism to improve the large-scale feature learning ability and reduce the calculation parameters of GAN. Second, using relativity adversarial loss function to improve the structural stability of GAN. Furthermore, increasing convolution kernel and deeper structural of GAN to improve the ability of global feature extraction and saving the training time. Finally, taking NWPU remote sensing image dataset as experimental data, the effectiveness of the method proposed in this article is verified through ResNet-18. The experimental results show that it can generate more diverse high-resolution remote sensing natural scene images, the overall classification accuracy of augmented training dataset of remote sensing used high-quality generated images selected based on inception score and Frechet inception distance evaluations is improved by more than 3% and the classification accuracy is generally higher than the traditional data augmentation methods.","2151-1535","","10.1109/JSTARS.2022.3166927","National Natural Science Foundation of China(grant numbers:41771451); Sichuan Science and Technology Program(grant numbers:2020JDTD0003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756307","Generative adversarial networks (GAN);relativity loss;remote sensing;sample augment;self-attention mechanism","Remote sensing;Generative adversarial networks;Convolution;Training;Sensors;Feature extraction;Kernel","convolutional neural nets;data handling;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;natural scenes;object detection;remote sensing","self-attention mechanism;NWPU remote sensing image dataset;high-resolution remote sensing;natural scene images;REL-SAGAN;scene data augmentation;deep learning;target detection;scene classification;change detection;remote sensing scene data;Rel-SAGAN;generative adversarial networks;inception score;Frechet inception distance evaluation;feature extraction;ResNet-18","","","","34","CCBY","12 Apr 2022","","","IEEE","IEEE Journals"
"Unsupervised Adversarial Domain Adaptation Network for Semantic Segmentation","W. Liu; F. Su","Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","26 Oct 2020","2020","17","11","1978","1982","With the rapid development of deep learning technology, semantic segmentation methods have been widely used in remote sensing data. A pretrained semantic segmentation model usually cannot perform well when the testing images (target domain) have an obvious difference from the training data set (source domain), while a large enough labeled data set is almost impossible to be acquired for each scenario. Unsupervised domain adaptation (DA) techniques aim to transfer knowledge learned from the source domain to a totally unlabeled target domain. By reducing the domain shift, DA methods have shown the ability to improve the classification accuracy for the target domain. Hence, in this letter, we propose an unsupervised adversarial DA network that converts deep features into 2-D feature curves and reduces the discrepancy between curves from the source domain and curves from the target domain based on a conditional generative adversarial networks (cGANs) model. Our proposed DA network is able to improve the semantic labeling accuracy when we apply a pretrained semantic segmentation model to the target domain. To test the effectiveness of the proposed method, experiments are conducted on the International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D Semantic Labeling data set. Results show that our proposed network is able to stably improve overall accuracy not only when the source and target domains are from the same city but with different building styles but also when the source and target domains are from different cities and acquired by different sensors. By comparing with a few state-of-the-art DA methods, we demonstrate that our proposed method achieves the best cross-domain semantic segmentation performance.","1558-0571","","10.1109/LGRS.2019.2956490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8932673","Domain adaptation (DA);generative adversarial networks (GANs);remote sensing image;semantic segmentation;transfer learning","Feature extraction;Semantics;Image segmentation;Data models;Remote sensing;Labeling;Training","feature extraction;geophysical image processing;image classification;image segmentation;neural nets;remote sensing;unsupervised learning","source domain;domain shift;conditional generative adversarial networks;remote sensing;cross-domain semantic segmentation;unsupervised adversarial domain adaptation network;remote sensing data;labeled data set;2-D semantic labeling data;semantic labeling;unlabeled target domain;unsupervised domain adaptation;deep learning;classification accuracy;unsupervised adversarial DA network;deep features;2-D feature curves;cGAN","","18","","28","IEEE","13 Dec 2019","","","IEEE","IEEE Journals"
"Semi-Supervised Representation Learning for Remote Sensing Image Classification Based on Generative Adversarial Networks","P. Yan; F. He; Y. Yang; F. Hu","National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, Wuhan, China","IEEE Access","24 Mar 2020","2020","8","","54135","54144","In the existing studies on remote sensing image scene classification, the supervised learning methods which are fine-tuned from pre-trained model require a large amount of labeled training data and parameters, while unsupervised learning methods do not make full use of label information, and the classification performance could be improved. In this paper, we introduced semi-supervised learning into generative adversarial network (GAN), so the discriminator learned more discriminative features from labeled data and unlabeled data. Moreover, the mixup data augmentation method was introduced into our classification model to augment the data and stabilized the training process. We carried out extensive experiments for both UC-Merced and NWPU-RESISC45 datasets with a 5-fold cross-validation protocol using a linear SVM as classifier. We trained the proposed method on UC-Merced dataset and achieve an average overall accuracy of 94.05% under 80% training ratio. When trained on NWPU-RESISC45 dataset, the proposed method reached an average overall accuracy of 83.12% and 92.78% under the training ratios of 20% and 80% respectively, which achieves the state-of-the-art deep learning methods without pre-training.","2169-3536","","10.1109/ACCESS.2020.2981358","Fundamental Research Funds for the Central Universities(grant numbers:HUST-2018KFYYXJJ141); National Natural Science Foundation of China(grant numbers:61871438); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039665","Remote sensing scene classification;semi-supervised learning;generative adversarial networks;data augmentation","Training;Gallium nitride;Remote sensing;Feature extraction;Generative adversarial networks;Generators;Data models","geophysical image processing;image classification;learning (artificial intelligence);remote sensing;support vector machines","semisupervised representation learning;remote sensing image classification;generative adversarial network;remote sensing image scene classification;supervised learning methods;labeled training data;unsupervised learning methods;label information;classification performance;semisupervised learning;discriminative features;unlabeled data;mixup data augmentation method;classification model;training process;NWPU-RESISC45 dataset;cross-validation protocol;UC-Merced dataset;average overall accuracy;training ratios;deep learning methods","","23","","29","CCBY","17 Mar 2020","","","IEEE","IEEE Journals"
"Generative Adversarial Networks to Augment Micro-Doppler Signatures for the Classification of Human Activity","I. Alnujaim; D. Oh; Y. Kim","Department of Electrical and Computer Engineering, California State University, Fresno; Daegu Gyeongbuk Institute of Science & Technology (DGIST), South Korea; Department of Electrical and Computer Engineering, California State University, Fresno","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9459","9461","Collecting a large amount of data for radar requires a significant amount of time, labor, and money. In deep convolutional neural networks, a small dataset causes the problem of overfitting. We herein introduce the employment of data augmentation using generative adversarial networks (GANs) to solve the data deficiency problem. In this study, we tested the feasibility of using generative adversarial networks to generate micro-Doppler signatures for seven human activities. Moreover, we use produced fake images to train deep convolutional neural networks. We found that the use of augmented data improves classification accuracy. In addition, the quality of GAN output was evaluated in terms of classification accuracy.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898073","Human activity classification;microDoppler signatures;deep learning;adversarial generative network","Spectrogram;Gallium nitride;Generative adversarial networks;Radar;Generators;Legged locomotion;Convolutional neural networks","convolutional neural nets;Doppler radar;image classification;learning (artificial intelligence);object detection;radar imaging","human activity;deep convolutional neural networks;generative adversarial networks;data deficiency problem;human activities;classification accuracy;augment microDoppler signatures","","12","","7","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"IMG2DSM: Height Simulation From Single Imagery Using Conditional Generative Adversarial Net","P. Ghamisi; N. Yokoya","Earth Observation Center, SAR Signal Processing, Remote Sensing Technology Institute, Oberpfaffenhofen, Wessling, Germany; RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan","IEEE Geoscience and Remote Sensing Letters","20 Apr 2018","2018","15","5","794","798","This letter proposes a groundbreaking approach in the remote-sensing community to simulating the digital surface model (DSM) from a single optical image. This novel technique uses conditional generative adversarial networks whose architecture is based on an encoder-decoder network with skip connections (generator) and penalizing structures at the scale of image patches (discriminator). The network is trained on scenes where both the DSM and optical data are available to establish an image-to-DSM translation rule. The trained network is then utilized to simulate elevation information on target scenes where no corresponding elevation information exists. The capability of the approach is evaluated both visually (in terms of photographic interpretation) and quantitatively (in terms of reconstruction errors and classification accuracies) on subdecimeter spatial resolution data sets captured over Vaihingen, Potsdam, and Stockholm. The results confirm the promising performance of the proposed framework.","1558-0571","","10.1109/LGRS.2018.2806945","Kayamori Foundation of Information Science Advancement and Japan Society for the Promotion of Science KAKENHI(grant numbers:15K20955); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8306501","Conditional generative adversarial networks (cGANs);convolutional neural network (CNN);deep learning;digital surface model (DSM);encoder–decoder networks;optical images","Generators;Optical imaging;Optical sensors;Remote sensing;Gallium nitride;Training;Adaptive optics","feature extraction;geophysical image processing;image classification;image reconstruction;image resolution;image texture;optical radar;remote sensing;terrain mapping","skip connections;penalizing structures;image patches;optical data;image-to-DSM translation rule;trained network;target scenes;corresponding elevation information;subdecimeter spatial resolution data sets;IMG2DSM;height simulation;single imagery;conditional generative adversarial net;groundbreaking approach;remote-sensing community;digital surface model;single optical image;conditional generative adversarial networks;encoder-decoder network;Vaihingen;Potsdam;Stockholm","","66","","6","IEEE","5 Mar 2018","","","IEEE","IEEE Journals"
"SIGAN: Spectral Index Generative Adversarial Network for Data Augmentation in Multispectral Remote Sensing Images","A. Singh; L. Bruzzone","Department of Information Engineering and Computer Science, Remote Sensing Laboratory (RSLab), University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, Remote Sensing Laboratory (RSLab), University of Trento, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","24 Dec 2021","2022","19","","1","5","Generative models are typically employed to approximate the distribution of deep features. Recently, these state-of-the-art methods have been applied to estimate image transformations by an unsupervised learning approach. In this letter, a novel spectral index generative adversarial network (SIGAN) is proposed for the generation of multispectral (MS) remote sensing images. This network is defined to effectively perform data augmentation starting from a limited number of training samples in the MS remote sensing domain for training deep learning models. The SIGAN model is able to capture class-specific properties in data augmentation, by incorporating the task-specific normalized spectral indices to model class-by-class properties of MS images. Experimental results obtained on a Sentinel 2 dataset show that the proposed model provides better performance than other generative adversarial networks (GANs) in MS data generation.","1558-0571","","10.1109/LGRS.2021.3093238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537161","Data augmentation;labeled sample;multispectral (MS) images;remote sensing","Generators;Training;Indexes;Data models;Remote sensing;Convolution;Generative adversarial networks","","","","5","","14","IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"Synthetic Data Augmentation Using Multiscale Attention CycleGAN for Aircraft Detection in Remote Sensing Images","W. Liu; B. Luo; J. Liu","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","20 Dec 2021","2022","19","","1","5","Deep learning approaches require enough training samples to perform well, but it is a challenge to collect enough real training data and label them manually. In this letter, we propose a practical framework for automatically generating content-rich synthetic images with ground-truth annotations. By rendering 3-D CAD models, we generate two synthetic aircraft image data sets with wide distribution (Syn N and Syn U). For improving the quality of synthetic images, we propose a multiscale attention module which enhances the Cycle-Consistent Adversarial Network (CycleGAN) in spatial and channel dimensions. Then, we compare the synthetic images before and after translation qualitatively and quantitatively. Experiments on Northwestern Polytechnical University (NWPU) very high resolution (VHR)-10, University of Chinese Academy of Sciences, orientation robust object detection in aerial images (UCAS-AOD), and benchmark for object DetectIon in Optical Remote sensing images (DIOR) data sets demonstrate that synthetic data augmentation can improve the performance of aircraft detection in remote sensing images, especially when real data are insufficient. Synthetic data are available at: https://weix-liu.github.io/.","1558-0571","","10.1109/LGRS.2021.3052017","National Natural Science Foundation of China(grant numbers:61571332); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337932","Generative adversarial networks (GANs);image translation;self-attention;synthetic images;object detection","Object detection;Remote sensing;Solid modeling;Training;Feature extraction;Atmospheric modeling;Airplanes","CAD;deep learning (artificial intelligence);image resolution;object detection;remote sensing;rendering (computer graphics);solid modelling","synthetic data augmentation;multiscale attention CycleGAN;aircraft detection;synthetic aircraft image data;Northwestern Polytechnical University very high resolution-10;cycle consistent adversarial network;orientation robust object detection;aerial images;optical remote sensing image data sets;deep learning;3D CAD model rendering","","10","","21","IEEE","28 Jan 2021","","","IEEE","IEEE Journals"
"SiftingGAN: Generating and Sifting Labeled Samples to Improve the Remote Sensing Image Scene Classification Baseline In Vitro","D. Ma; P. Tang; L. Zhao","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","24 Jun 2019","2019","16","7","1046","1050","Lack of annotated samples greatly restrains the direct application of deep learning in remote sensing image scene classification. Although research studies have been done to tackle this issue by data augmentation with various image transformation operations, they are still limited in quantity and diversity. Recently, the advent of the unsupervised learning-based generative adversarial networks (GANs) brings us a new way to generate augmented samples. However, such GAN-generated samples are currently only served for training GANs model itself and for improving the performance of the discriminator in GANs internally (in vivo). It becomes a question of serious doubt whether the GAN-generated samples can help better improve the scene classification performance of other deep learning networks (in vitro), compared with the widely used transformed samples. To answer this question, this letter proposes a SiftingGAN approach to generate more numerous, more diverse, and more authentic labeled samples for data augmentation. SiftingGAN extends traditional GAN framework with an Online-Output method for sample generation, a Generative-Model-Sifting method for model sifting, and a Labeled-Sample-Discriminating method for sample sifting. Experiments on the well-known aerial image data set demonstrate that the proposed SiftingGAN method can not only effectively improve the performance of the scene classification baseline that is achieved without data augmentation but also significantly excels the comparison methods based on traditional geometric/radiometric transformation operations.","1558-0571","","10.1109/LGRS.2018.2890413","Chinese Academy of Sciences(grant numbers:XDA19080301); National Natural Science Foundation of China(grant numbers:41701397,41701399); Major Project of High Resolution Earth Observation System of China(grant numbers:03-Y20A04-9001-17/18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611213","Data augmentation;deep learning;generative adversarial networks (GANs);scene classification","Gallium nitride;Generators;Remote sensing;Training;In vitro;Generative adversarial networks;Deep learning","geophysical image processing;image classification;neural nets;remote sensing;unsupervised learning","remote sensing image scene classification baseline;data augmentation;image transformation operations;unsupervised learning-based generative adversarial networks;augmented samples;GAN-generated samples;GANs model;scene classification performance;deep learning networks;traditional GAN framework;sample generation;aerial image data;SiftingGAN method;labeled-sample-discriminating method;generative-model-sifting method;authentic labeled samples;online-output method;aerial image data set;geometric-radiometric transformation operations;labeled sample sifting","","57","","15","IEEE","13 Jan 2019","","","IEEE","IEEE Journals"
"Semi-Supervised Classification of Hyperspectral Data Based on Generative Adversarial Networks and Neighborhood Majority Voting","Y. Zhan; K. Wu; W. Liu; J. Qin; Z. Yang; Y. Medjadba; G. Wang; X. Yu","School of Software, Nanyang Institute of Technology, Nanyang, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; Libary, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5756","5759","How to classify hyperspectral images using few training samples is an important and challenging problem because the collection of the samples is difficult and expensive. Because semi-supervised approaches can utilize information contained in the unlabeled samples and labeled samples, it is a suitable choice. A novel semi-supervised spectral-spatial classification method for hyperspectral data based on generative adversarial network (GAN) is proposed in this paper. First, we use a custom one-dimensional GAN to train the hyperspectral data to obtain spectral features. After using a new small convolutional neural network (CNN) to classify the spectral features, we use a new classification method based on a majority voting strategy further to improve the classification result. The performance of our method is evaluated on ROSIS image data, and the results show that the proposed method can acquire satisfactory results when compared with traditional methods using a few of labeled samples.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518846","Hyperspectral images classification;semi-supervised learning (SSL);generative adversarial networks (GAN);deep learning;spectral-spatial classification","Generative adversarial networks;Gallium nitride;Hyperspectral imaging;Data models;Feature extraction;Training","convolution;feedforward neural nets;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence)","hyperspectral data;generative adversarial network;neighborhood majority voting;hyperspectral images;unlabeled samples;labeled samples;custom one-dimensional GAN;spectral features;convolutional neural network;ROSIS image data;semisupervised spectral-spatial classification method;CNN","","11","","13","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Remote Sensing Image Translation via Style-Based Recalibration Module and Improved Style Discriminator","T. Zhang; F. Gao; J. Dong; Q. Du","School of Information Science and Engineering, Ocean University of China, Qingdao, China; School of Information Science and Engineering, Ocean University of China, Qingdao, China; School of Information Science and Engineering, Ocean University of China, Qingdao, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Existing remote sensing change detection methods are heavily affected by seasonal variation. Since vegetation colors are different between winter and summer, such variations are inclined to be falsely detected as changes. In this letter, we proposed an image translation method to solve the problem. A style-based recalibration module is introduced to capture seasonal features effectively. Then, a new style discriminator is designed to improve the translation performance. The discriminator can not only produce a decision for the fake or real sample but also return a style vector according to the channel-wise correlations. Extensive experiments are conducted on the season-varying data set. The experimental results show that the proposed method can effectively perform image translation, thereby consistently improving the season-varying image change detection performance. Our codes and data are available at https://github.com/summitgao/RSIT_SRM_ISD.","1558-0571","","10.1109/LGRS.2021.3068558","National Key Research and Development Program of China(grant numbers:2018AAA0100602); National Natural Science Foundation of China(grant numbers:U1706218); Key Technology Research and Development Program of Shandong(grant numbers:2019GHY112048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9391996","Change detection;GAN;image-to-image translation;remote sensing","Gallium nitride;Generators;Remote sensing;Generative adversarial networks;Feature extraction;Correlation;Vegetation mapping","feature extraction;geophysical image processing;remote sensing;vectors","translation performance;style vector;remote sensing image translation;style-based recalibration module;style discriminator;remote sensing change detection;seasonal variation;season-varying image change detection","","2","","17","IEEE","31 Mar 2021","","","IEEE","IEEE Journals"
"Data Augmentation Method of SAR Image Dataset","M. Zhang; Z. Cui; X. Wang; Z. Cao","School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5292","5295","Large-scale high-quality, standardized, measurable and accurate data is the key to promote the progress of the algorithm in the radar remote sensing. Data scaling is a widespread technology that increases the size of a labeled training set dataset through specific data transformations. Synthetic Aperture Radar (SAR) image simulators based on computer-aided mapping models play an important role in SAR applications such as automatic target recognition and image interpretation, but the accuracy of this simulator is due to geometric errors and simplification of electromagnetic calculations. In order to achieve a SAR image datasets with the known target and azimuth angles, we can generate the desired image directly from a known image database. We can realize the augmentation of SAR image data set through linear synthesis and Generative Adversarial Networks, which can generate SAR images for the specified azimuth.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518825","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518825","SAR image;linear synthesis;generative adversarial networks","Azimuth;Synthetic aperture radar;Generative adversarial networks;Target recognition;Training;Training data;Image generation","image recognition;neural nets;radar computing;radar imaging;radar target recognition;remote sensing by radar;synthetic aperture radar;visual databases","generative adversarial networks;linear synthesis;electromagnetic calculations;synthetic aperture radar image simulators;data transformations;radar remote sensing;data augmentation method;SAR image dataset;image interpretation;automatic target recognition;computer-aided mapping models;labeled training set dataset","","3","","10","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks for Change Detection in Multispectral Imagery","M. Gong; X. Niu; P. Zhang; Z. Li","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; College of Information Engineering, Xiangtan University, Xiangtan, China","IEEE Geoscience and Remote Sensing Letters","4 Dec 2017","2017","14","12","2310","2314","Change detection can be treated as a generative learning procedure, in which the connection between bitemporal images and the desired change map can be modeled as a generative one. In this letter, we propose an unsupervised change detection method based on generative adversarial networks (GANs), which has the ability of recovering the training data distribution from noise input. Here, the joint distribution of the two images to be detected is taken as input and an initial difference image (DI), generated by traditional change detection method such as change vector analysis, is used to provide prior knowledge for sampling the training data based on Bayesian theorem and GAN's min-max game theory. Through the continuous adversarial learning, the shared mapping function between the training data and their corresponding image patches can be built in GAN's generator, from which a better DI can be generated. Finally, an unsupervised clustering algorithm is used to analyze the better DI to obtain the desired binary change map. Theoretical analysis and experimental results demonstrate the effectiveness and robustness of the proposed method.","1558-0571","","10.1109/LGRS.2017.2762694","National Natural Science Foundation of China(grant numbers:61772393,61422209); National Program for Support of Top-notch Young Professionals of China; National Key Research and Development Program of China(grant numbers:2017YFB0802200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094357","Change detection;change vector analysis (CVA);generative adversarial networks (GANs);multispectral images","Training data;Gallium nitride;Feature extraction;Redundancy;Principal component analysis;Algorithm design and analysis;Spatial resolution","Bayes methods;game theory;learning (artificial intelligence);object detection;pattern clustering;unsupervised learning","generative adversarial networks;multispectral imagery;generative learning procedure;bitemporal images;unsupervised change detection method;training data distribution;initial difference image;change vector analysis;continuous adversarial learning;shared mapping function;image patches;GAN's min-max game theory;binary change map;Bayesian theorem","","74","","16","IEEE","2 Nov 2017","","","IEEE","IEEE Journals"
"Vision-Inspired Filtering Algorithm for SAR Ship Detection Based on Generative Adversarial Networks","M. Ju; Q. Hu","College of Information Science and Technology, Dalian Maritime University, Dalian, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China","IEEE Geoscience and Remote Sensing Letters","26 Oct 2022","2022","19","","1","5","Ship detection in synthetic aperture radar (SAR) images has been widely applied in the military and civil fields. However, the background environment of SAR images is complex and there are many interferences similar to the ship targets, which is easy to lead fault detection and affect the detection performance. To address this problem, a vision-inspired filtering algorithm (FilterGAN) is proposed to filter out the target-irrelevant information. First, we build a representation model based on filtering mechanism of human brain to guide the design of filtering network. Second, to simulate the adjustment process of the priority map reconstruction in human brain, generative adversarial networks (GAN) are used to learn the optimal filtering mapping function. To train FilterGAN, we introduce the labeling process to generate the ground-truth filtered SAR image. Experimental results on AIR-SARShip-1.0 dataset demonstrate that the detection performance of SAR ships can be improved obviously with FilterGAN.","1558-0571","","10.1109/LGRS.2022.3213804","Fundamental Research Funds for the Central Universities(grant numbers:3132022242); National Natural Science Foundation of China(grant numbers:62201114,62071080); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9916304","Filtering mechanism;generative adversarial networks (GAN);ship detection;synthetic aperture radar (SAR) images","Radar polarimetry;Marine vehicles;Synthetic aperture radar;Feature extraction;Filtering theory;Information filters;Image reconstruction","filtering theory;learning (artificial intelligence);object detection;radar imaging;ships;synthetic aperture radar","SAR ship detection;generative adversarial networks;synthetic aperture radar images;military fields;civil fields;background environment;SAR images;ship targets;fault detection;detection performance;vision-inspired filtering algorithm;FilterGAN;target-irrelevant information;representation model;filtering mechanism;human brain;filtering network;adjustment process;priority map reconstruction;optimal filtering mapping function;ground-truth filtered SAR image;SAR ships","","","","16","IEEE","12 Oct 2022","","","IEEE","IEEE Journals"
"Densely Labeling Large-Scale Satellite Images with Generative Adversarial Networks","Y. Yan; X. Huang; A. Rangarajan; S. Ranka","University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA; University of Florida, Gainesville, Florida, USA","2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)","28 Oct 2018","2018","","","927","934","Building an efficient and accurate pixel-level labeling framework for large-scale and high-resolution satellite imagery is an important machine learning application in the remote sensing area. Due to the very limited amount of the ground-truth data, we employ a well-performing superpixel tessellation approach to segment the image into homogeneous regions and then use these irregular-shaped regions as the foundation for the dense labeling work. A deep model based on generative adversarial networks is trained to learn the discriminating features from the image data without requiring any additional labeled information. In the subsequent classification step, we adopt the discriminator of this unsupervised model as a feature extractor and train a fast and robust support vector machine to assign the pixel-level labels. In the experiments, we evaluate our framework in terms of the pixel-level classification accuracy on satellite imagery with different geographical types. The results show that our dense-labeling framework is very competitive compared to the state-of-the-art methods that heavily rely on prior knowledge or other large-scale annotated datasets.","","978-1-5386-7518-2","10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.000-7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8511999","superpixel segmentation;unsupervised feature learning;generative adversarial networks;remote sensing","Gallium nitride;Labeling;Feature extraction;Generative adversarial networks;Satellites;Remote sensing;Image segmentation","feature extraction;geophysical image processing;image classification;image segmentation;neural nets;remote sensing;support vector machines;unsupervised learning","geographical types;feature discrimination;image segmentation;machine learning application;pixel-level labeling framework;support vector machine;classification step;large-scale annotated datasets;dense-labeling framework;pixel-level classification accuracy;pixel-level labels;feature extractor;unsupervised model;image data;deep model;irregular-shaped regions;homogeneous regions;superpixel tessellation approach;ground-truth data;remote sensing area;high-resolution satellite imagery;generative adversarial networks;densely labeling large-scale satellite images","","","","20","IEEE","28 Oct 2018","","","IEEE","IEEE Conferences"
"DeepExt: A Convolution Neural Network for Road Extraction using RGB images captured by UAV","N. Varia; A. Dokania; J. Senthilnath","Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, Gujarat, IN; Indian Institute of Technology Guwahati, Guwahati, Assam, IN; Agency for Science Technology and Research, Singapore, SG","2018 IEEE Symposium Series on Computational Intelligence (SSCI)","31 Jan 2019","2018","","","1890","1895","In this paper, we propose automatic road extraction using Unmanned Aerial Vehicle (UAV) based Remote Sensing data. Road extraction using UAV data is very useful in traffic management, city planning, GPS based applications, etc. Deep learning techniques namely, Fully Convolutional Network (FCN) and conditional Generative Adversarial Networks (GAN) are used to extract roads from a UAV dataset available in the literature. FCN performs semantic segmentation on the image whereas the GAN generates output images from the model it learns. The results demonstrate the efficiency of the deep learning methods for the task of road extraction.","","978-1-5386-9276-9","10.1109/SSCI.2018.8628717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8628717","road extraction;convolutional neural nenvork;generative adversarial networks;semantic segmentation","Roads;Feature extraction;Image segmentation;Generative adversarial networks;Data mining;Gallium nitride;Remote sensing","autonomous aerial vehicles;convolutional neural nets;feature extraction;geophysical image processing;image colour analysis;image segmentation;learning (artificial intelligence);remote sensing;roads","RGB images;automatic road extraction;Unmanned Aerial Vehicle;Remote Sensing data;UAV;deep learning;DeepExt;generative adversarial networks;GAN;fully convolutional network;FCN;convolution neural network","","16","","32","IEEE","31 Jan 2019","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks for Realistic Synthesis of Hyperspectral Samples","N. Audebert; B. Le Saux; S. Lefevre","UMR 6074, Univ. Bretagne-Sud, Vannes, France; Office National d'Etudes et de Recherches Aerospatiales, Chatillon, ÃŽle-de-France, FR; ONERA, The French Aerospace Lab, Palaiseau, France","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4359","4362","This work addresses the scarcity of annotated hyperspectral data required to train deep neural networks. Especially, we investigate generative adversarial networks and their application to the synthesis of consistent labeled spectra. By training such networks on public datasets, we show that these models are not only able to capture the underlying distribution, but also to generate genuine-looking and physically plausible spectra. Moreover, we experimentally validate that the synthetic samples can be used as an effective data augmentation strategy. We validate our approach on several public hyperspectral datasets using a variety of deep classifiers.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518321","hyperspectral image classification;generative models;deep learning;data augmentation","Hyperspectral imaging;Gallium nitride;Training;Generators;Generative adversarial networks;Interpolation","hyperspectral imaging;image classification;image sampling;learning (artificial intelligence);neural nets","generative adversarial networks;hyperspectral samples;annotated hyperspectral data;public hyperspectral datasets;deep neural network training;data augmentation strategy;consistent labeled spectra synthesis;deep classifiers","","24","","14","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Aerial Image and Map Synthesis Using Generative Adversarial Networks","J. Gu; Y. Zhang; W. Zhang; H. Yu; S. Wang; Y. Wang; L. Wang","Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Department of Electrical and Computer Engineering, Northeastern University; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9803","9806","Accurate automatic conversion between aerial images and maps is a valuable and challenging task in computer vision and computer graphics. Deep convolutional neural networks (CNN) have achieved promising results on this task but the results accuracy is not ideal. In this paper, we propose a solution to improve the precision and quality of the transforming results. The core learning method is based on generative adversarial networks (GANs). A novel generator and a multi-scale discriminator are introduced in our network. The generator operates at the progressive method to gurantee the spatial consistency between the inputs and outputs, and our multi-scale discriminator focuses on increasing the capacity of the network and guides the generator to generate better results. In particular, our architecture can also be used as a general neural network for style translation. Analytic experiments on the aerial-to-map dataset show that our network outperforms the existing method, advancing both accuracy and visual appearance.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900222","Generative adversarial network;image-to-image translation;aerial photos","Generators;Generative adversarial networks;Task analysis;Training;Gallium nitride;Computer vision;Image resolution","computer vision;geophysical image processing;learning (artificial intelligence);neural nets","computer graphics;core learning method;generative adversarial networks;multiscale discriminator;general neural network;aerial image;map synthesis;computer vision;aerial-to-map dataset","","1","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Immune Evolutionary Generative Adversarial Networks for Hyperspectral Image Classification","J. Bai; Y. Zhang; Z. Xiao; F. Ye; Y. Li; M. Alazab; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Digital Mapping and Land Information Application, School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; College of Computer Science and Electronics Engineering, Hunan University, Changsha, China; National Key Laboratory of Remote Sensing Information and Imagery Analysis, Beijing Research Institute of Uranium Geology, Beijing, China; Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen, China; College of Engineering, IT and Environment, Charles Darwin University, Darwin, NT, Australia; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","29 Nov 2022","2022","60","","1","14","In recent years, hyperspectral image classification (HIC) algorithm based on deep learning has been widely studied, and has achieved much better results than traditional algorithms. HIC using small samples has gradually become a research hotspot, and the generative adversarial networks (GANs) have become a brilliant application in this field. However, the HIC results based on GAN methods are poor and volatile, since a single loss function cannot accurately measure the distance between the generated samples and the real samples in different hyperspectral images. To resolve this problem, we propose a novel immune evolutionary generative adversarial network (HIEGAN) via leveraging the evolutionary strategy and immune strategy. Specifically, we enhance the performance of the generator in two ways: 1) HIEGAN uses multiple loss functions for calculation and backpropagation, so as to endow the generator with different parameter values and select the best one as the evolution result each time to enter the next iteration and 2) in the training process, we preserve the optimal generator as memory cells to avoid the performance degradation of the generator. Through these changes, HIEGAN overcame the defects of GAN, improved the stability of GAN, and finally improved classification efficiency. At the same time, in order to alleviate the overfitting problem of depth network under small samples, we change convolution and deconvolution into ghost module to reduce the network parameters. Experiments on three classical datasets validate that HIEGAN has encouraging performance in HIC under small samples.","1558-0644","","10.1109/TGRS.2022.3210280","National Natural Science Foundation of China(grant numbers:62276206); Fundamental Research Funds for the Central Universities(grant numbers:QTZX22088); Key Research and Development Program of Shaanxi(grant numbers:S2022-YF-YBGY-0921,2020GXLHY023); Key Research and Development Project of Hunan Province of China(grant numbers:2022GK2020); Science and Technology Project of Hunan Provincial Water Resources Department(grant numbers:XSKJ2021000-39); Project of SKL of Remote Sensing Information and Image Analysis Technology(grant numbers:6142A010409); Open Research Fund from the Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)(grant numbers:GML-KF-22-22); Open Research Fund Program of the Key Laboratory of Digital Mapping and Land Information Application, Ministry of Natural Resources(grant numbers:ZRZYBWD202205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946422","Generative adversarial networks (GANs);hyperspectral image classification (HIC);immune evolutionary algorithm","Generative adversarial networks;Hyperspectral imaging;Generators;Training;Neural networks;Feature extraction;Deep learning","backpropagation;convolutional neural nets;deep learning (artificial intelligence);evolutionary computation;hyperspectral imaging;image classification","backpropagation;classification efficiency;convolution;deconvolution;deep learning;depth network overfitting problem;evolutionary strategy;GAN methods;ghost module;HIC;HIEGAN;hyperspectral image classification algorithm;immune evolutionary generative adversarial networks;immune strategy;multiple loss functions;network parameter reduction;optimal generator;performance degradation;single loss function;training process","","","","47","IEEE","11 Nov 2022","","","IEEE","IEEE Journals"
"A Supervised Progressive Growing Generative Adversarial Network for Remote Sensing Image Scene Classification","A. Ma; N. Yu; Z. Zheng; Y. Zhong; L. Zhang","Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2022","2022","60","","1","18","Remote sensing image scene classification is a challenging task. With the development of deep learning, methods based on convolutional neural networks (CNNs) have made great achievements in remote sensing image scene classification. Since the training of a CNN requires a large number of labeled samples, a generative adversarial network (GAN) for sample generation represents a new opportunity to solve the problem of the limited samples. However, most of the existing GAN-based sample generation methods can only generate unlabeled samples, instead of samples labeled with the corresponding scene category. In this article, to solve the problem, a supervised progressive growing generative adversarial network (SPG-GAN) is proposed for remote sensing image scene classification. The proposed method can generate labeled samples for the remote sensing image scene classification, significantly improving the classification accuracy in the case of limited samples. The SPG-GAN method has two main improvements. First, a conditional generative framework for labeled samples is proposed, in which the label information is added in the channel dimension as the input. By considering the constraints of the label information in the loss function, the network can be trained in the direction of a specific category. As a result, the network can generate remote sensing image scene classification samples with label categories. Second, a progressive growing sample generation method is introduced. In order to ensure that the generated samples have more spatial details, they are generated by progressively adding modules to the generator and discriminator, thereby ensuring that the generated sample is of better quality. After testing on two benchmark datasets and carrying out a large-scale experiment in the central area of the city of Wuhan in China, it was found that the proposed method can obtain a superior scene classification accuracy in the case of limited samples.","1558-0644","","10.1109/TGRS.2022.3151405","National Natural Science Foundation of China(grant numbers:42171336,42071350); Fundamental Research Funds for the Central Universities(grant numbers:2042020kf0014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713860","Generative adversarial network (GAN);progressive growing;remote sensing image scene classification;sample generation","Generative adversarial networks;Remote sensing;Generators;Training;Feature extraction;Sensors;Deep learning","convolutional neural nets;feature extraction;geophysical image processing;image classification;image representation;remote sensing","labeled samples;remote sensing image scene classification samples;progressive growing sample generation method;supervised progressive growing generative adversarial network;GAN-based sample generation methods;convolutional neural networks","","4","","54","IEEE","14 Feb 2022","","","IEEE","IEEE Journals"
"An Open Set Domain Adaptation Network Based on Adversarial Learning for Remote Sensing Image Scene Classification","J. Zhang; J. Liu; L. Shi; B. Pan; X. Xu","Hebei Province Key Laboratory of Big Data Calculation, Tianjin, China; Hebei Province Key Laboratory of Big Data Calculation, Tianjin, China; Hebei Province Key Laboratory of Big Data Calculation, Tianjin, China; School of Statistics and Data Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1365","1368","Remote sensing image scene classification refers to assigning specific semantic labels for remote sensing images. Due to the lack of labeled remote sensing images, domain adaptation is applied to remote sensing image scene classification. However, recent proposed methods mainly focus on the closed set scenario. In this paper, we explore the open set scenario and introduce an open set domain adaptation network (OSDANet) for remote sensing image scene classification. Inspired by the idea of Generative Adversarial Network (GAN), we design a feature generator as well as a classifier which are learnt in an adversarial way. The purpose of the classifier is to find a boundary between the source and the target samples, while the feature generator attempts to force target samples away from the boundary. Especially, for the target samples, the feature generator will determine whether to align them with source samples or reject them as unknown target samples. The experimental results have indicated the effectiveness of the proposed method.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323944","National Natural Science Foundation of China(grant numbers:41804118); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323944","Remote sensing scene classification;open set domain adaptation;generative adversarial network","Remote sensing;Generators;Generative adversarial networks;Training;Sparse matrices;Gallium nitride;Target recognition","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);remote sensing","labeled remote sensing images;remote sensing image scene classification;open set domain adaptation network","","5","","12","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Self-Supervised Remote Sensing Image Retrieval","K. Walter; M. J. Gibson; A. Sowmya","School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; School of Computer Science and Engineering, University of New South Wales, Sydney, Australia","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1683","1686","Current remote sensing platforms generate a vast amount of imagery but the best current methods to index and retrieve that data require expensive and difficult to procure labels. In this paper, we aim to address this problem by presenting a performant content based image retrieval (CBIR) system that is capable of indexing and retrieval using only unlabelled data. We investigate the use of self-supervised learning, a method for end-to-end learning of visual features from large datasets. In particular, we investigate the performance of four state-of-the-art self-supervised learning methods: variational autoencoders, bidirectional GANs, colourisation networks and DeepCluster, and evaluate the quality of the representations learned on remote sensing CBIR problems. Experiments on two very high resolution datasets show that the best of these methods, DeepCluster, is able to achieve near parity with supervised transfer learning despite not using any label information.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323294","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323294","content-based image retrieval;remote sensing;unsupervised learning;self-supervised learning","Feature extraction;Remote sensing;Task analysis;Generative adversarial networks;Data models;Image retrieval;Image reconstruction","content-based retrieval;feature extraction;geophysical image processing;image retrieval;learning (artificial intelligence);remote sensing","self-supervised remote sensing image retrieval;current remote sensing platforms;performant content based image retrieval system;indexing;unlabelled data;self-supervised learning;end-to-end learning;remote sensing CBIR problems;supervised transfer learning","","1","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"An Approach To Super-Resolution Of Sentinel-2 Images Based On Generative Adversarial Networks","K. Zhang; G. Sumbul; B. Demir",Shanghai Jiao Tong University; Technische Universität Berlin; Technische Universität Berlin,"2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","2 Jun 2020","2020","","","69","72","This paper presents a generative adversarial network based super-resolution (SR) approach (which is called as S2GAN) to enhance the spatial resolution of Sentinel-2 spectral bands. The proposed approach consists of two main steps. The first step aims to increase the spatial resolution of the bands with 20m and 60m spatial resolutions by the scaling factors of 2 and 6, respectively. To this end, we introduce a generator network that performs SR on the lower resolution bands with the guidance of the bands associated to 10m spatial resolution by utilizing the convolutional layers with residual connections and a long skip-connection between inputs and outputs. The second step aims to distinguish SR bands from their ground truth bands. This is achieved by the proposed discriminator network, which alternately characterizes the high level features of the two sets of bands and applying binary classification on the extracted features. Then, we formulate the adversarial learning of the generator and discriminator networks as a min-max game. In this learning procedure, the generator aims to produce realistic SR bands as much as possible so that the discriminator incorrectly classifies SR bands. Experimental results obtained on different Sentinel-2 images show the effectiveness of the proposed approach compared to both conventional and deep learning based SR approaches.","","978-1-7281-2190-1","10.1109/M2GARSS47143.2020.9105165","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105165","Sentinel-2 images;super-resolution;generative adversarial network;remote sensing","Image color analysis;Superresolution;Neural networks;Geoscience and remote sensing;Games;Generative adversarial networks;Feature extraction","convolutional neural nets;feature extraction;game theory;geophysical image processing;image classification;image resolution;learning (artificial intelligence);minimax techniques","generative adversarial network;super-resolution approach;S2GAN;spatial resolution;Sentinel-2 spectral bands;generator network;lower resolution bands;ground truth bands;discriminator network;binary classification;adversarial learning;realistic SR bands;Sentinel-2 images;convolutional layers;feature extraction;min-max game;size 20.0 m;size 10.0 m;size 60.0 m","","3","","10","IEEE","2 Jun 2020","","","IEEE","IEEE Conferences"
"MSNet: A Multiple Supervision Network for Remote Sensing Scene Classification","N. Liu; T. Celik; H. -C. Li","School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Electrical and Information Engineering, University of the Witwatersrand, Johannesburg, South Africa; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Remote sensing scene classification is a complex task due to large intraclass variations in object appearances with a small number of samples per class and high interclass similarities due to shared objects in different classes, which usually cause model overfitting and high interclass confusion. To address these challenges, a multiple supervision approach, called multiple supervision network (MSNet), consisting of the ResNet-50 backbone, a feature discriminative branch (FDB), and a feature confusion branch (FCB) is proposed in this letter. The FDB selects discriminative features per class and suppresses peaks in feature maps to examine more informative regions with lower feature magnitudes. Meanwhile, the FCB reduces overfitting by introducing confusion to the input of a fully connected layer which also enhances the robust features. The FDB and FCB are only used in training of the backbone and not used in inference. Thus, the proposed method does not introduce additional computing time on the backbone while it significantly boosts its performance in scene classification. The experimental results show that MSNet outperforms the methods considered in this letter.","1558-0571","","10.1109/LGRS.2020.3043020","Sichuan Provincial Science and Technology Projects(grant numbers:2019JDJQ0023); National Key Research and Development Program of China(grant numbers:2020YFB0505704); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302612","Convolutional neural network (CNN);deep learning;generative adversarial network (GAN);remote sensing scene classification (RSSC)","Training;Remote sensing;Generative adversarial networks;Feature extraction;Gallium nitride;Visualization;Information science","feature extraction;geophysical image processing;image classification;image representation;learning (artificial intelligence);recurrent neural nets;remote sensing","feature discriminative branch;FDB;feature confusion branch;FCB;discriminative features;feature maps;MSNet;remote sensing scene classification;intraclass variations;object appearances;high interclass similarities;shared objects;multiple supervision approach;ResNet-50 backbone","","1","","16","IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"Data Augmentation Through Spectrally Controlled Adversarial Networks for Classification of Multispectral Remote Sensing Images","A. Singh; L. Bruzzone","Department of Information Engineering and Computer Science, Remote Sensing Laboratory (RSLab), University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, Remote Sensing Laboratory (RSLab), University of Trento, Trento, Italy","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","651","654","Availability of limited training remote sensing datasets is one of the problems in deep learning, as deep architectures require a large number of training samples for proper training. In this paper, we present a technique for data augmentation based on a spectral indexed generative adversarial network to train deep convolutional neural networks. This technique uses the spectral characteristic of multispectral (MS) images to support data augmentation in order to generate realistic training samples with respect to each land-use and land-cover class. The impact of multispectral remote sensing data generated through the spectral indexed GAN are evaluated through classification experiments. Experimental results obtained on the classification of the Sentinel-2 Eurosatallband datasets show that data augmentation through spectral indexed GAN enhances the main accuracy metrics.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884928","Training Data;Data Augmentation;Generative Model;Remote Sensing Image Analysis;Classification Accuracy","Training;Measurement;Deep learning;Analytical models;Generative adversarial networks;Sensors;Convolutional neural networks","geophysical image processing;image classification;learning (artificial intelligence);neural nets;remote sensing","data augmentation;spectrally controlled adversarial networks;training remote sensing datasets;deep learning;deep architectures;proper training;spectral indexed generative adversarial network;deep convolutional neural networks;spectral characteristic;multispectral images;realistic training samples;multispectral remote sensing data;spectral indexed GAN","","1","","14","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"ColorMapGAN: Unsupervised Domain Adaptation for Semantic Segmentation Using Color Mapping Generative Adversarial Networks","O. Tasar; S. L. Happy; Y. Tarabalka; P. Alliez","TITANE Team, Inria, Université Côte d’Azur, Sophia Antipolis, France; HP Inc., Bengaluru, India; LuxCarta Technology, Parc d’Activité l’Argile, Mouans Sartoux, France; TITANE Team, Inria, Université Côte d’Azur, Sophia Antipolis, France","IEEE Transactions on Geoscience and Remote Sensing","24 Sep 2020","2020","58","10","7178","7193","Due to the various reasons, such as atmospheric effects and differences in acquisition, it is often the case that there exists a large difference between the spectral bands of satellite images collected from different geographic locations. The large shift between the spectral distributions of training and test data causes the current state-of-the-art supervised learning approaches to output unsatisfactory maps. We present a novel semantic segmentation framework that is robust to such a shift. The key component of the proposed framework is color mapping generative adversarial networks (ColorMapGANs) that can generate fake training images that are semantically exactly the same as training images, but whose spectral distribution is similar to the distribution of the test images. We then use the fake images and the ground truth for the training images to fine-tune the already trained classifier. Contrary to the existing generative adversarial networks (GANs), the generator in ColorMapGAN does not have any convolutional or pooling layers. It learns to transform the colors of the training data to the colors of the test data by performing only one elementwise matrix multiplication and one matrix-addition operation. Due to the architecturally simple but powerful design of ColorMapGAN, the proposed framework outperforms the existing approaches with a large margin in terms of both accuracy and computational complexity.","1558-0644","","10.1109/TGRS.2020.2980417","ACRI-ST; Centre national d’études spatiales (CNES); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9047180","Convolutional neural networks (CNNs);dense labeling;domain adaptation;generative adversarial networks (GANs);semantic segmentation","Training;Remote sensing;Image segmentation;Training data;Semantics;Image color analysis;Generative adversarial networks","feature extraction;image classification;image colour analysis;image segmentation;unsupervised learning","spectral bands;satellite images;geographic locations;spectral distribution;test data;output unsatisfactory maps;semantic segmentation framework;color mapping generative adversarial networks;ColorMapGAN;fake training images;test images;fake images;trained classifier;training data;unsupervised domain adaptation;atmospheric effects;elementwise matrix multiplication;matrix-addition operation;computational complexity","","58","","73","IEEE","25 Mar 2020","","","IEEE","IEEE Journals"
"FPN-GAN: Multi-class Small Object Detection in Remote Sensing Images","T. Ahmad; X. Chen; A. S. Saqlain; Y. Ma","School of Control and Computer Engineering, North China Electric Power University, Beijing, China; School of Control and Computer Engineering, North China Electric Power University, Beijing, China; School of Control and Computer Engineering, North China Electric Power University, Beijing, China; School of Control and Computer Engineering, North China Electric Power University, Beijing, China","2021 IEEE 6th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA)","2 Jun 2021","2021","","","478","482","Despite the recent dramatic advances in object detection, detecting a small object in general and in remote sensing images is still a challenging problem. One main reason for this is the appearance of small objects in images. Specifically low resolution and noisy representation makes it hard to detect small objects. We tickle down this problem by proposing a novel object detector based on Generative adversarial network (GAN), which we called FPN-GAN in short. The proposed method is composed of GAN, Resnet-50 as a backbone, and Feature Pyramid Network for detection. We combine both of these methods to achieve a single end to end GAN model for multi class-small object detection and image enhancement simultaneously. Extensive experiments on a challenging benchmark DIOR remote sensing dataset demonstrate the superiority of the proposed method for small objects as well as large and the medium size objects.","","978-1-6654-2311-3","10.1109/ICCCBDA51879.2021.9442506","National Key R&D Program of China(grant numbers:2018YFC0831404); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442506","object detection;remote sensing;generative adversarial networks (GANs)","Training;Image resolution;Object detection;Benchmark testing;Generative adversarial networks;Feature extraction;Sensors","geophysical image processing;image enhancement;object detection;remote sensing","FPN-GAN;object detection;remote sensing images;recent dramatic advances;novel object detector;GAN model;challenging benchmark DIOR remote sensing dataset;medium size objects","","2","","38","IEEE","2 Jun 2021","","","IEEE","IEEE Conferences"
"High Resolution SAR Image Synthesis with Hierarchical Generative Adversarial Networks","H. Huang; F. Zhang; Y. Zhou; Q. Yin; W. Hu","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2782","2785","Generative adversarial network (GAN) is an artificial neural network based on unsupervised learning method. Due to its powerful model representation capabilities, GAN has been introduced to synthesize synthetic aperture radar (SAR) image data, for the real sample is difficult to acquire. Large-scale, high-resolution SAR images play an important role in promoting SAR applications, such as automatic target recognition and image interpretation. However, on account of the difficult training problem of GAN network, especially for SAR images with speckle noise, it is difficult to obtain high-resolution SAR images by simply transfer the net from optical image. Recent studies in other image fields have shown that hierarchical structure is an effective and useful way to decompose a generation task into several smaller subtasks. How to obtain more high-resolution SAR images from limited original samples through GAN is the target of our research. Therefore, in this paper, we introduce a hierarchical GAN network model to generate SAR images, through the multi-stage network, gradually improve the quality of the generated image, and finally obtain high-resolution images. The type and aspect of generated images are determined by the input of condition vectors in the last two stages. In addition, we introduce the triple loss, in which the background loss is used to imitating background clutter noise of SAR image, the condition loss is to make the generated images' type and aspect become controllable, and the global loss for getting higher image generation quality. The generated images show high similarity with the real samples.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900494","Generative adversarial network(GAN);synthetic aperture radar (SAR);SAR simulator;automatic target recognition (ATR);triple loss","Generative adversarial networks;Radar polarimetry;Gallium nitride;Image resolution;Synthetic aperture radar;Solid modeling;Data models","image resolution;neural nets;radar imaging;synthetic aperture radar;unsupervised learning","high resolution SAR image synthesis;hierarchical generative adversarial networks;generative adversarial network;artificial neural network;synthetic aperture radar image data;high-resolution SAR images;SAR applications;automatic target recognition;image interpretation;optical image;image fields;hierarchical GAN network model;high-resolution images;generated images;image generation quality;unsupervised learning method","","10","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Remote Sensing Image Super-Resolution Via Attentional Feature Aggregation Generative Adversarial Network","F. Cai; K. -Y. Wu; F. Wang","Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2598","2601","The extraction of high-frequency details is generally neglected in single image super-resolution (SISR) for remote sensing images. In this paper, we propose an attentional feature aggregation generative adversarial network (AFA-GAN) with the capability of strong feature extraction and attentional feature fusion to generate high-resolution remote sensing images. We adopt the residual feature aggregation framework for the feature extraction to make full use of the hierarchical features on the residual branches. To better fuse global and local features with inconsistent scales, an attentional feature fusion mechanism is utilized in residual feature aggregation modules. The comprehensive experiments with state-of-the-art SISR methods on the UC Merced dataset demonstrate the effectiveness and superiority of our AFA-GAN.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884863","National Natural Science Foundation of China(grant numbers:61901122); Natural Science Foundation of Shanghai(grant numbers:20ZR1406300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884863","Remote sensing images;single image super-resolution (SISR);attentional feature aggregation (AFA);generative adversarial network (GAN)","Visualization;Fuses;Superresolution;Feature extraction;Generative adversarial networks;Remote sensing;Image reconstruction","feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","high-resolution remote sensing images;residual feature aggregation framework;hierarchical features;global features;local features;attentional feature fusion mechanism;residual feature aggregation modules;AFA-GAN;remote sensing image super-resolution;high-frequency details;single image super-resolution;attentional feature aggregation generative adversarial network;strong feature extraction","","","","18","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Thin and Thick Cloud Removal on Remote Sensing Image by Conditional Generative Adversarial Network","X. Wang; G. Xu; Y. Wang; D. Lin; P. Li; X. Lin","Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; National nuclear emergency response and technical assistance center, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1426","1429","Cloud removal is an essential step to enhance the quality of cloud-covered remote sensing image. In recent years, conditional Generative Adversarial Network (cGAN) yields promising improvement in plentiful image-to-image translation tasks. In this paper, we propose a novel objective function to upgrade the structural similarity index based on cGAN. We discover that ImageGAN is effective to focus on global information for thick cloud-covered images and Patch-GAN has fewer parameters while maintaining outstanding performance for thin cloud-covered remote sensing images in the experiments. Experimental results demonstrate that our method achieves remarkable performance in both PSNR, SSIM and visual effect on cloud-covered remote sensing images especially thin cloud-covered images.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8897958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897958","Conditional generative adversarial network;remote sensing image;cloud removal","Remote sensing;Generators;Generative adversarial networks;Linear programming;Task analysis;Image reconstruction;Indexes","clouds;geophysical image processing;image enhancement;neural nets;remote sensing","cloud-covered images;cloud removal;cloud-covered remote sensing image;conditional generative adversarial network;image-to-image translation tasks;structural similarity index;cGAN;ImageGAN;Patch-GAN;thin cloud-covered images;thick cloud-covered images","","6","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Typhoon Cloud Prediction Via Generative Adversarial Networks","H. Li; X. Yu; P. Ren","College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3023","3026","We present a novel typhoon cloud prediction method via generative adversarial networks (GANs). Specifically, we develop a adversarial prediction model consisting of a generator and a discriminator. The generator generates the continuous future cloud images by learning the evolution trend of typhoon clouds from multiple continuous historical typhoon cloud images. In this way, the generator completes the visual predictions for typhoon clouds. On the other hand, the discriminator distinguishes the generated future cloud images from the real ones. Furthermore, we adopt a gradient difference loss function and a total variation loss function to improve the quality of the generated cloud images. The proposed method effectively predicts the whole spatial-temporal evolution of the typhoon clouds, resulting in a visual complement for classic typhoon prediction methods. The effectiveness of the proposed method has been evaluated in the real satellite cloud images.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518069","Typhoon cloud prediction;generative adversarial networks (GANs);gradient difference loss;total variation loss","Tropical cyclones;Generators;Predictive models;Training;Clouds;Mathematical model;Testing","atmospheric techniques;clouds;data visualisation;geophysical image processing;geophysical signal processing;prediction theory;storms","generative adversarial networks;adversarial prediction model;continuous future cloud images;typhoon clouds;multiple continuous historical typhoon cloud images;visual predictions;generated future cloud images;generated cloud images;classic typhoon prediction methods;satellite cloud images;typhoon cloud prediction method","","1","","6","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Orchard Areas Segmentation in Remote Sensing Images via Class Feature Aggregate Discriminator","M. Liu; D. Ren; H. Sun; S. X. Yang; P. Shao","College of Computer and Information Technology, China Three Gorges University, Yichang, China; College of Computer and Information Technology, China Three Gorges University, Yichang, China; College of Computer and Information Technology, China Three Gorges University, Yichang, China; School of Engineering, University of Guelph, Guelph, Canada; College of Computer and Information Technology, China Three Gorges University, Yichang, China","IEEE Geoscience and Remote Sensing Letters","25 Oct 2022","2022","19","","1","5","Accurate evaluation of orchard areas from remote sensing images is of great importance in economic and ecological aspects. In practice, the differences in distributions between remote sensing images and the lack of data labels make the semantic segmentation model impossible to use in new data. Unsupervised domain adaptation (UDA) methods can improve the performance of the model in the target domain by aligning the source domain and the target domain. However, due to the class mismatch problem and the interference of high-dimensional feature complexity, most UDA methods cannot achieve satisfactory results in orchard areas segmentation task. To address these issues, we propose an UDA model for orchard areas segmentation by developing a class feature aggregate discriminator (CFUDA). The class feature aggregate discriminator is designed to distinguish intradomain classes and align interdomain classes, and class feature aggregate can represent class information of different domains, which helps the model to avoid the interference of complex information. In addition, adversarial loss reweighting is introduced to the novel model, which makes the segmentation model pay more attention to the orchard areas. To verify the effectiveness of the proposed method, we conducted extensive experiments in three different remote sensing images around Yichang City. Compared to the baseline model, the proposed approach improves intersection over union (IoU) by 27.68%, and we achieve high gains of 6.07% in IoU over other UDA methods. The larger gain indicates that our proposed method has great potential in cross-domain orchard areas segmentation.","1558-0571","","10.1109/LGRS.2022.3213679","National Key Research and Development Program of China(grant numbers:2016YFD0800902); Natural Science Foundation of Hubei Province of China(grant numbers:2021CFB004); National Natural Science Foundation of China(grant numbers:41901341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915593","Generative adversarial networks;remote sensing;semantic segmentation;unsupervised domain adaptation (UDA)","Aggregates;Task analysis;Remote sensing;Adaptation models;Image resolution;Training;Semantics","feature extraction;geophysical image processing;image classification;image segmentation;pattern clustering;remote sensing;unsupervised learning","class feature aggregate discriminator;intradomain classes;align interdomain classes;class information;different remote sensing images;UDA methods;cross-domain orchard areas segmentation;semantic segmentation model;unsupervised domain adaptation methods;target domain;source domain;class mismatch problem;high-dimensional feature complexity;orchard areas segmentation task;UDA model","","","","20","IEEE","10 Oct 2022","","","IEEE","IEEE Journals"
"Low Altitude Aerial Scene Synthesis Using Generative Adversarial Networks for Autonomous Natural Resource Management","S. Sharafi; B. Majidi; A. Movaghar","Department of Computer Engineering, Khatam University, Tehran, Iran; Department of Computer Engineering, Khatam University, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","2019 5th Conference on Knowledge Based Engineering and Innovation (KBEI)","13 Jun 2019","2019","","","322","326","Deep neural networks are currently the best solution for aerial scene interpretation for Unmanned Aerial Vehicle (UAV) based remote sensing. A problem faced by the deep neural networks is that the deep models require significantly large training datasets which should cover almost all of the scenarios. Gathering these datasets is usually very time consuming and expensive. In this paper, data augmentation and generative adversarial network are used for autonomous synthesis of low altitude aerial scenes for creating a training dataset for deep low altitude aerial video interpretation. The proposed system is evaluated using a real world scenario of road following under foliage in a jungle and the experimental results show that the proposed framework is capable of producing high accuracy training datasets for UAV vision system in natural resource management scenarios.","","978-1-7281-0872-8","10.1109/KBEI.2019.8734904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8734904","natural environment management;generative adversarial network;deep learning;remote sensing","Training;Generative adversarial networks;Generators;Neural networks;Visualization;Unmanned aerial vehicles;Roads","autonomous aerial vehicles;geophysical image processing;mobile robots;natural resources;neural nets;remote sensing;robot vision;video signal processing","generative adversarial network;autonomous synthesis;low altitude aerial scenes;deep low altitude aerial video interpretation;natural resource management scenarios;autonomous natural resource management;deep neural networks;aerial scene interpretation;remote sensing;unmanned aerial vehicle;low altitude aerial scene synthesis;UAV vision system","","5","","24","IEEE","13 Jun 2019","","","IEEE","IEEE Conferences"
"Cloud Removal in Remote Sensing Images Using Generative Adversarial Networks and SAR-to-Optical Image Translation","F. N. Darbaghshahi; M. R. Mohammadi; M. Soryani","School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran; School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran; School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran","IEEE Transactions on Geoscience and Remote Sensing","21 Feb 2022","2022","60","","1","9","Satellite images are often contaminated by clouds. Cloud removal has received special attention due to the wide range of satellite image applications. As the clouds thicken, the process of removing them becomes more challenging. In such cases, using auxiliary images, such as near-infrared or synthetic aperture radar (SAR), for reconstructing is common. In this study, we attempt to solve the problem using two generative adversarial networks (GANs): the first translates SAR images to optical images and the second removes clouds using the translated images of prior GAN. Also, we propose dilated residual inception blocks (DRIBs) instead of vanilla U-net in the generator networks and use structural similarity index measure (SSIM) in addition to the L1 loss function. Reducing the number of downsamplings and expanding receptive fields by dilated convolutions increased the quality of output images. We used the SEN1-2 dataset to train and test both GANs, and we made cloudy images by adding synthetic clouds to optical images. In addition, we used the SEN12MS-CR dataset to test network performance to remove real clouds. The restored images are evaluated using PSNR, SSIM, SAM, MAE, RMSE, and  $Q$ . We compared the proposed method with state-of-the-art deep learning models and achieved more accurate results in both SAR-to-optical image translation and cloud removal parts.","1558-0644","","10.1109/TGRS.2021.3131035","Center for Space Research of Iran; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627647","Cloud removal;deep learning;generative adversarial network (GAN);optical imagery;SAR-to-optical translation;synthetic aperture radar (SAR)","Clouds;Generative adversarial networks;Optical imaging;Generators;Optical sensors;Synthetic aperture radar;Radar polarimetry","convolution;deep learning (artificial intelligence);image restoration;radar computing;radar imaging;remote sensing by radar;synthetic aperture radar","satellite image applications;auxiliary images;generative adversarial networks;GAN;optical images;generator networks;structural similarity index measure;cloudy images;synthetic clouds;restored images;SAR-to-optical image translation;cloud removal parts;remote sensing images;dilated convolutions;dilated residual inception blocks","","12","","48","IEEE","25 Nov 2021","","","IEEE","IEEE Journals"
"Semi-Supervised Deep Learning Seismic Impedance Inversion Using Generative Adversarial Networks","D. Meng; B. Wu; N. Liu; W. Chen","School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1393","1396","Deep learning methods have been successfully applied to solve seismic inversion problems in recent years. Though deep learning inversion can obtain results with much higher resolution compared to geophysical inversion, its performance often suffers from the limitation of the well logs which are main source of labels in training data. To overcome this problem, we propose a semi-supervised deep learning workflow based on Generative Adversarial Network (GAN) for seismic impedance inversion. The workflow contains three networks: a generator, a discriminator, and a forward model. The training of the generator and discriminator are guided by well logs and constrained by unlabeled data via the forward model. Test on Marmousi2 model shows that, by making use of both labeled and unlabeled data, the proposed method predicts impedance with better consistency than conventional deep learning inversion.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323119","Fundamental Research Funds for the Central Universities(grant numbers:xjj2018260,xjh012019030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323119","Generative adversarial network;semi-supervised learning;seismic impedance inversion;deep learning","Impedance;Gallium nitride;Training;Generators;Generative adversarial networks;Deep learning;Data models","convolutional neural nets;deep learning (artificial intelligence);geophysical signal processing;seismology;supervised learning;unsupervised learning;well logging","conventional deep learning inversion;unlabeled data;forward model;deep learning workflow;training data;well logs;geophysical inversion;seismic inversion problems;Generative Adversarial networks;semisupervised deep learning seismic impedance inversion","","3","","17","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Structure-Texture Parallel Embedding for Remote Sensing Image Super-Resolution","T. Lu; K. Zhao; Y. Wu; Z. Wang; Y. Zhang","Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, NERCMS, China; Computer School, Hubei University of Arts and Science, Xiangyang, China","IEEE Geoscience and Remote Sensing Letters","27 Sep 2022","2022","19","","1","5","The structure and texture of images are crucial for remote sensing image super-resolution (SR). Generative adversarial networks (GANs) recover image details through adversarial training. However, the recovered images always have structural distortions, on the one hand, and GANs are difficult to train, on the other hand. In addition, some methods assist reconstruction by introducing prior information of the image, but this brings additional computational cost. To address this issue, we propose a novel structure-texture parallel embedding (SPE) method for SR of remote sensing images. Our method does not require additional image priors to reconstruct high-quality images. Specifically, we use the global structure information and local texture information of the image in the ascending space to guide the reconstruction result of the image. First, we design a structure preserving block (SPB) to extract global structural features in the ascending space of the image, so as to obtain global structure information for a priori representation. Then, we design a local texture attention module (LTAM) to restore richer texture details. We have conducted lots of experiments on Draper public dataset. Experimental results show that our proposed method not only achieves a better tradeoff between computational cost and performance, but also outperforms the existing several SR methods in terms of objective index evaluation and subjective visual effects.","1558-0571","","10.1109/LGRS.2022.3206348","National Natural Science Foundation of China(grant numbers:62072350,62171328,U1903214,62071339,61771353); Hubei Technology Innovation Project(grant numbers:2019AAA045); Central Government Guides Local Science and Technology Development Special Projects(grant numbers:2018ZYYD059); High value Intellectual Property Cultivation Project of Hubei Province; Enterprise Technology Innovation Project of Wuhan(grant numbers:202001602011971); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9888134","Remote sensing image super-resolution (SR);structure preserving;texture attention mechanism","Feature extraction;Remote sensing;Image reconstruction;Training;Superresolution;Convolution;Satellites","feature extraction;geophysical image processing;image reconstruction;image resolution;image texture;neural nets;remote sensing","Draper public dataset;structure-texture parallel embedding method;local texture attention module;structure preserving block;local texture information;global structure information;high-quality images;structural distortions;recovered images;image details;generative adversarial networks;remote sensing image super-resolution","","","","21","IEEE","12 Sep 2022","","","IEEE","IEEE Journals"
"SEMI2I: Semantically Consistent Image-to-Image Translation for Domain Adaptation of Remote Sensing Data","O. Tasar; S. L. Happy; Y. Tarabalka; P. Alliez","Université Côte d'Azur, Inria, TITANE team; Inria, STARS team; LuxCarta Technology; Université Côte d'Azur, Inria, TITANE team","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1837","1840","Although convolutional neural networks have been proven to be an effective tool to generate high quality maps from remote sensing images, their performance significantly deteriorates when there exists a large domain shift between training and test data. To address this issue, we propose a new data augmentation approach that transfers the style of test data to training data using generative adversarial networks. Our semantic segmentation framework consists in first training a U-net from the real training data and then fine-tuning it on the test stylized fake training data generated by the proposed approach. Our experimental results prove that our framework outperforms the existing domain adaptation methods.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323711","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323711","Domain adaptation;data augmentation;semantic segmentation;dense labeling;image-to-image translation;generative adversarial networks;GANs","Training;Decoding;Training data;Image color analysis;Image segmentation;Image reconstruction;Roads","convolutional neural nets;data analysis;geophysical image processing;image segmentation;learning (artificial intelligence);remote sensing","consistent image-to-image translation;remote sensing data;convolutional neural networks;remote sensing images;data augmentation approach;generative adversarial networks;semantic segmentation framework;SEMI2I;domain adaptation methods;U-net","","14","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Using Adversarial Network for Multiple Change Detection in Bitemporal Remote Sensing Imagery","W. Zhao; X. Chen; X. Ge; J. Chen","National Geomatics Center of China, Beijing, China; National Geomatics Center of China, Beijing, China; National Geomatics Center of China, Beijing, China; National Geomatics Center of China, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Change detection by comparing two bitemporal images is one of the most challenging tasks in remote sensing. At present, most related studies focus on change area detection while neglecting multiple change type identification. In this letter, an attention gates generative adversarial adaptation network (AG-GAAN) is proposed on multiple change detection. The AG-GAAN has the following contributions: 1) this method can automatically detect multiple changes; 2) it includes attention gates mechanism for spatial constraint and accelerates change area identification with finer contours; and 3) the domain similarity loss is introduced to improve the discriminability of the model so that the model can map out real changes more accurately. To demonstrate the robustness of this approach, we used the Google Earth data sets that include seasonal variations for change detection and understanding. The experimental results demonstrated that the proposed method can accurately detect the multiple change types from bitemporal imagery.","1558-0571","","10.1109/LGRS.2020.3035780","National Key Research and Development Program of China(grant numbers:2018YFC1508903); Fundamental Research Funds for the Central Universities(grant numbers:2018NTST01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9258979","Attention gates (AGs);bitemporal images;domain similarity loss;generative adversarial network (GAN);multiple-change detection","Generative adversarial networks;Remote sensing;Gallium nitride;Generators;Feature extraction;Training;Task analysis","geophysical image processing;remote sensing","change area detection;multiple change type identification;attention gates generative adversarial adaptation network;AG-GAAN;multiple change detection;attention gates mechanism;change area identification;adversarial network;bitemporal remote sensing imagery","","9","","19","IEEE","13 Nov 2020","","","IEEE","IEEE Journals"
"Transfer Learning in Remote Sensing Images with Generative Adversarial Networks","B. Hu; P. Yao; L. Fu; X. Li; K. Dong; T. Zheng","Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing; Institute of Computing Technology, Chinese Academy of Sciences, Beijing","2019 IEEE/ACIS 18th International Conference on Computer and Information Science (ICIS)","27 Dec 2019","2019","","","124","129","In recent years Transfer Learning has become an important research issue and it is employed to solve the lack of data problem in remote sensing field. As one kind of the most popular generative models, Generative Adversarial Networks (GANs) can generate hierarchy representations and realistic textures of objects. Now GANs have become the most successful and usual way to synthesize images. In this paper we train different GANs and generate images on the UC Merced Land Use Dataset and the aircraft carrier dataset compiled by ourselves. Our images are better than the ones generated by MARTA GAN in remote sensing field. We also creatively adopt the Image Similarity Deep Ranking model (ISDR) to evaluate our GANs and reuse high-quality generated images in an objective detection task. We train the object detection model SSD on our dataset. With the augmentation of generated images, the SSD model achieves a better detection accuracy, which means GANs are beneficial for addressing the lack of training data problem in remote sensing field.","","978-1-7281-0801-8","10.1109/ICIS46139.2019.8940298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940298","remote sensing;Generative Adversarial Networks;image similarity","5G mobile communication;Two dimensional displays","geophysical image processing;image texture;learning (artificial intelligence);neural nets;object detection;realistic images;remote sensing","realistic textures;image similarity deep ranking model;UC Merced land use dataset;UC Merced land use dataset;hierarchy representations;MARTA GAN;aircraft carrier dataset;transfer learning;generative adversarial networks;remote sensing images;training data problem;object detection model SSD;objective detection task;high-quality image generation;remote sensing field","","","","19","IEEE","27 Dec 2019","","","IEEE","IEEE Conferences"
"Multispectral Remote Sensing Image Matching via Image Transfer by Regularized Conditional Generative Adversarial Networks and Local Feature","T. Ma; J. Ma; K. Yu; J. Zhang; W. Fu","National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Science and Technology on Complex System Control and Intelligent Agent Cooperation Laboratory, Beijing, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multispectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Science and Technology on Complex System Control and Intelligent Agent Cooperation Laboratory, Beijing, China","IEEE Geoscience and Remote Sensing Letters","20 Jan 2021","2021","18","2","351","355","Multispectral image matching is at the base for many remote sensing and computer vision applications. Due to the different imaging principles and spectra, there are significant nonlinear variations in intensity, texture, and style in multispectral images. This makes it difficult for many classic methods designed for the images of the same spectrum to achieve satisfactory matching performance. To cope with this problem, this letter proposes a new method based on image transfer and local feature for multispectral image matching. First, we propose a new regularized conditional generative adversarial network (GAN) for image transfer to preprocess the multispectral images. This step eliminates the differences in grayscale, texture, and style between the multispectral images. Then, we use a classic local feature to match the generated and original images. We evaluate our method on two commonly used data sets and compare with several state-of-the-art methods. The experiments show that our method performs well by significantly improving the matching accuracy and robustness, and slightly increasing the runtime.","1558-0571","","10.1109/LGRS.2020.2972361","Shanghai Aerospace Science and Technology Innovation Foundation(grant numbers:SAST2016063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9019828","Feature matching;image transfer;multispectral remote sensing images;regularized conditional generative adversarial network (GAN)","Gallium nitride;Generative adversarial networks;Generators;Remote sensing;Image matching;Feature extraction;Robustness","computer vision;image colour analysis;image matching;image texture;neural nets;remote sensing","computer vision applications;imaging principles;image transfer;multispectral image matching;regularized conditional generative adversarial network;classic local feature;multispectral remote sensing image matching;GAN","","9","","19","IEEE","2 Mar 2020","","","IEEE","IEEE Journals"
"A Machine Learning Approach to Clutter Suppression for Marine Surveillance Radar","Z. Wu; J. Pei; W. Huo; Y. Huang; Y. Zhang; H. Yang","Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3137","3140","Marine surveillance radar can monitor the marine environment in all-weather conditions, but the presence of sea clutter will seriously affect its target detection performance. In this paper, we proposes a sea clutter suppression method based on machine learning that contains two pairs of generative adversarial networks (GANs), in which one GAN is used to learn the mapping relationship of sea clutter suppression, and the other is used to ensure the performance of clutter suppression. Matching loss is proposed to preserve clutter suppression performance. Experimental results have shown the superior performance of the proposed method in improving the signal-to-clutter ratio (SCR) and the stability of clutter suppression.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554334","National Natural Science Foundation of China(grant numbers:61901091,61901090,61671117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554334","Marine surveillance radar;sea clutter suppression;machine learning;generative adversarial networks (GAN);matching loss","Radar remote sensing;Surveillance;Radar clutter;Radar detection;Machine learning;Object detection;Generative adversarial networks","interference suppression;learning (artificial intelligence);marine radar;neural nets;object detection;radar clutter;radar computing;radar detection;search radar","marine surveillance radar;marine environment;target detection performance;sea clutter suppression method;machine learning approach;GAN;clutter suppression performance;signal-to-clutter ratio;generative adversarial networks","","1","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Semisupervised Hyperspectral Image Classification Based on Generative Adversarial Networks","Y. Zhan; D. Hu; Y. Wang; X. Yu","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; Beijing Institute of Geology, Beijing, China.; College of Information Science and Technology, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","23 Jan 2018","2018","15","2","212","216","Because the collection of ground-truth labels is difficult, expensive, and time-consuming, classifying hyperspectral images (HSIs) with few training samples is a challenging problem. In this letter, we propose a novel semisupervised algorithm for the classification of hyperspectral data by training a customized generative adversarial network (GAN) for hyperspectral data. The GAN constructs an adversarial game between a discriminator and a generator. The generator generates samples that are not distinguishable by the discriminator, and the discriminator determines whether or not a sample is composed of real data. We design a semisupervised framework for HSI data based on a 1-D GAN (HSGAN). This framework enables the automatic extraction of spectral features for HSI classification. When HSGAN is trained using unlabeled hyperspectral data, the generator can generate hyperspectral samples that are similar to the real data, while the discriminator contains the features, which can be used to classify hyperspectral data with only a small number of labeled samples. The performance of the HSGAN is evaluated on the Airborne Visible Infrared Imaging Spectrometer image data, and the results show that the proposed framework achieves very promising results with a small number of labeled samples.","1558-0571","","10.1109/LGRS.2017.2780890","Ministry of Land and Resources for the Public Welfare Industry Research Special Funds(grant numbers:201511079-02); National Natural Science Foundation of China(grant numbers:41272359,41672323,11471045); Beijing Natural Science Foundation(grant numbers:L172029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8241773","Deep learning;generative adversarial network (GAN);hyperspectral image (HSI) classification;remote sensing;semisupervised learning (SSL)","Gallium nitride;Training;Hyperspectral imaging;Data models;Generators;Feature extraction","hyperspectral imaging;image classification;infrared imaging;learning (artificial intelligence);remote sensing","hyperspectral image classification;generative adversarial networks;adversarial game;semisupervised framework;HSI data;HSGAN;HSI classification;Airborne Visible Infrared Imaging Spectrometer image data;semisupervised algorithm;hyperspectral data;1D GAN","","151","","16","IEEE","29 Dec 2017","","","IEEE","IEEE Journals"
"RSINet: Inpainting Remotely Sensed Images Using Triple GAN Framework","A. Kumar; D. Tamboli; S. Pande; B. Banerjee","Indian Institute of Technology Bombay; School of Electrical and Computer Engineering, Purdue University; Indian Institute of Technology Bombay; Indian Institute of Technology Bombay","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","143","146","We tackle the problem of image inpainting in the remote sensing domain. Remote sensing images possess high resolution and geographical variations, that render the conventional inpainting methods less effective. This further entails the requirement of models with high complexity to sufficiently capture the spectral, spatial and textural nuances within an image, emerging from its high spatial variability. To this end, we propose a novel inpainting method that individually focuses on each aspect of an image such as edges, colour and texture using a task specific GAN. Moreover, each individual GAN also incorporates the attention mechanism that explicitly extracts the spectral and spatial features. To ensure consistent gradient flow, the model uses residual learning paradigm, thus simultaneously working with high and low level features. We evaluate our model, alongwith previous state of the art models, on the two well known remote sensing datasets, Open Cities AI and Earth on Canvas, and achieve competitive performance. The code can be referred here: https://github.com/advaitkumar3107/RSINet.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884330","Image inpainting;remote sensing;generative adversarial networks","Earth;Image edge detection;Urban areas;Generative adversarial networks;Feature extraction;Sensors;Artificial intelligence","feature extraction;geophysical image processing;gradient methods;image classification;image colour analysis;image processing;image resolution;image restoration;image texture;learning (artificial intelligence);remote sensing","triple GAN framework;image inpainting;remote sensing domain;sensing images;geographical variations;render;conventional inpainting methods;spectral nuances;spatial nuances;textural nuances;high spatial variability;novel inpainting method;texture;task specific GAN;individual GAN;attention mechanism;spectral features;spatial features;consistent gradient flow;residual learning paradigm;high level features;low level features;art models;known remote sensing datasets","","","","19","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Compressive Data Gathering With Generative Adversarial Networks for Wireless Geophone Networks","K. Bin; S. Luo; X. Zhang; J. Lin; X. Tong","College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China; College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China; College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China; College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China; College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China","IEEE Geoscience and Remote Sensing Letters","24 Feb 2021","2021","18","3","558","562","In modern seismic data acquisition, real-time data collection is a challenging task due to bandwidth limitations in wireless communications. In this letter, we propose a novel compressive data gathering scheme using generative adversarial networks, named GAN-CDG, to improve the efficiency of data gathering. Instead of collecting the originally acquired data, GAN-CDG gathers data projections in wireless geophone networks. Data compression and load-balanced relay transmission are utilized during the projection process. To speed up the formation of projections, the shortest path routing tree (SPRT) is constructed, which achieves the minimum end-to-end time delay. The sparse domain of seismic signals and its reconstruction mapping are learned by sparsity-constrained adversarial networks. The testing results demonstrate that projections with high compression ratios (e.g., 16) are gathered efficiently with the SPRT. Then, original seismic signals can be reconstructed accurately (over 30 dB) from the projections using the adversarial model, which outperforms the state-of-the-art method.","1558-0571","","10.1109/LGRS.2020.2978520","National Natural Science Foundation of China(grant numbers:41804167); National Key R&D Program of China(grant numbers:2018YFC0603204); Ph.D. Interdisciplinary Research Funding Project of Jilin University(grant numbers:10183201836); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9037122","Compressive data gathering (CDG);generative adversarial networks (GANs);routing;sparse representation and reconstruction;wireless geophone networks (WGNs)","Wireless sensor networks;Routing;Wireless communication;Generative adversarial networks;Task analysis;Data communication;Data models","data acquisition;data compression;geophysical signal processing;neural nets;relay networks (telecommunication);resource allocation;seismometers;signal reconstruction;telecommunication network routing;trees (mathematics);wireless sensor networks","real-time data collection;bandwidth limitations;wireless communications;generative adversarial networks;wireless geophone networks;data compression;load-balanced relay transmission;projection process;shortest path routing tree;minimum end-to-end time delay;sparsity-constrained adversarial networks;high compression ratios;original seismic signals;adversarial model;compressive data gathering;modern seismic data acquisition;GAN-CDG;reconstruction mapping","","10","","15","IEEE","16 Mar 2020","","","IEEE","IEEE Journals"
"Generative Adversarial Network-Based Full-Space Domain Adaptation for Land Cover Classification From Multiple-Source Remote Sensing Images","S. Ji; D. Wang; M. Luo","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 Apr 2021","2021","59","5","3816","3828","The accuracy of remote sensing image segmentation and classification is known to dramatically decrease when the source and target images are from different sources; while deep learning-based models have boosted performance, they are only effective when trained with a large number of labeled source images that are similar to the target images. In this article, we propose a generative adversarial network (GAN) based domain adaptation for land cover classification using new target remote sensing images that are enormously different from the labeled source images. In GANs, the source and target images are fully aligned in the image space, feature space, and output space domains in two stages via adversarial learning. The source images are translated to the style of the target images, which are then used to train a fully convolutional network (FCN) for semantic segmentation to classify the land cover types of the target images. The domain adaptation and segmentation are integrated to form an end-to-end framework. The experiments that we conducted on a multisource data set covering more than 3500 km2 with 51 560 256×256 high-resolution satellite images in Wuhan city and a cross-city data set with 11 383 256×256 aerial images in Potsdam and Vaihingen demonstrated that our method exceeded the recent GAN-based domain adaptation methods by at least 6.1% and 4.9% in the mean intersection over union (mIoU) and overall accuracy (OA) indexes, respectively. We also proved that our GAN is a generic framework that can be implemented for other domain transfer methods to boost their performance.","1558-0644","","10.1109/TGRS.2020.3020804","National Key Research and Development Program of China(grant numbers:2018YFB0505003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9198144","Domain adaptation;generative adversarial network (GAN);image classification;remote sensing","Image segmentation;Remote sensing;Gallium nitride;Training;Decoding;Generative adversarial networks;Feature extraction","feature extraction;geophysical image processing;image classification;image segmentation;learning (artificial intelligence);remote sensing;terrain mapping","generative adversarial network-based full-space domain adaptation;land cover classification;multiple-source remote sensing;remote sensing image segmentation;labeled source images;target remote sensing images;image space;256×256 high-resolution satellite images;11 383 256×256 aerial images;recent GAN-based domain adaptation methods","","41","","28","IEEE","15 Sep 2020","","","IEEE","IEEE Journals"
"DSM Building Shape Refinement from Combined Remote Sensing Images Based on WNET-CGANS","K. Bittner; M. Körner; P. Reinartz","Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Technical University of Munich, Munich, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","783","786","We describe the workflow of a digital surface models (DSMs) refinement algorithm using a hybrid conditional generative adversarial network (cGAN) where the generative part consists of two parallel networks merged at the last stage forming a WNET architecture. The inputs to the so-called WNET-CGAN are stereo DSMs and panchromatic (PAN) half-meter resolution satellite images. Fusing these helps to propagate fine detailed information from a spectral image and complete the missing 3D knowledge from a stereo DSM about building shapes. Besides, it refines the building outlines and edges making them more rectangular and sharp.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8897865","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897865","Conditional generative adversarial networks;digital surface model;3D scene refinement;3D building shape;data fusion;satellite images","Buildings;Three-dimensional displays;Shape;Solid modeling;Data models;Remote sensing;Satellites","buildings (structures);geophysical image processing;image resolution;remote sensing","parallel networks;WNET architecture;stereo DSM;panchromatic half-meter resolution satellite images;fine detailed information;spectral image;building outlines;DSM building shape refinement;combined remote sensing images;WNET-CGANS;digital surface models refinement algorithm;hybrid conditional generative adversarial network;generative part;3D knowledge","","5","","7","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Remote Sensing Data Fusion With Generative Adversarial Networks: State-of-the-art methods and future research directions","P. Liu; J. Li; L. Wang; G. He","Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Magazine","13 Jul 2022","2022","10","2","295","328","In the past decades, remote sensing (RS) data fusion has always been an active research community. A large number of algorithms and models have been developed. Generative adversarial networks (GANs), as an important branch of deep learning, show promising performances in a variety of RS image fusions. This review provides an introduction to GANs for RS data fusion. We briefly review the frequently used architecture and characteristics of GANs in data fusion and comprehensively discuss how to use GANs to realize fusion for homogeneous RS, heterogeneous RS, and RS and ground observation (GO) data. We also analyze some typical applications with GAN-based RS image fusion. This review provides insight into how to make GANs adapt to different types of fusion tasks and summarizes the advantages and disadvantages of GAN-based RS data fusion. Finally, we discuss promising future research directions and make a prediction on their trends.","2168-6831","","10.1109/MGRS.2022.3165967","National Natural Science Foundation of China(grant numbers:61731022,41971397); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779854","","Data integration;Generators;Generative adversarial networks;Computer architecture;Degradation;Data models;Image fusion","data fusion;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;remote sensing;reviews","GO data;deep learning;state-of-the-art methods;active research community;remote sensing;generative adversarial networks;GAN-based RS data fusion;GAN-based RS image fusion;ground observation data;heterogeneous RS;homogeneous RS","","4","","238","IEEE","23 May 2022","","","IEEE","IEEE Magazines"
"Seismic Data Interpolation Using Dual-Domain Conditional Generative Adversarial Networks","D. Chang; W. Yang; X. Yong; G. Zhang; W. Wang; H. Li; Y. Wang","NWIG, PetroChina Research Institute of Petroleum Exploration and Development, Lanzhou, China; NWIG, PetroChina Research Institute of Petroleum Exploration and Development, Lanzhou, China; NWIG, PetroChina Research Institute of Petroleum Exploration and Development, Lanzhou, China; School of Geosciences, China University of Petroleum (East China), Tsingtao, China; Center of Geophysics, Department of Mathematics and Artificial Intelligence Laboratory, Harbin Institute of Technology, Harbin, China; NWIG, PetroChina Research Institute of Petroleum Exploration and Development, Lanzhou, China; NWIG, PetroChina Research Institute of Petroleum Exploration and Development, Lanzhou, China","IEEE Geoscience and Remote Sensing Letters","27 Sep 2021","2021","18","10","1856","1860","Seismic data interpolation is an effective way to reconstruct missing seismic traces and to improve the quality of the seismic data set. In the field of deep learning, generative adversarial networks are capable of data generation and interpolation and have been widely used for high-quality image generations and image interpolations. In this letter, we propose a dual-domain conditional generative adversarial network (DD-CGAN) for seismic data interpolation. The DD-CGAN consists of a generator network and a discriminator network and uses the seismic data set and discrete Fourier transformed data set in the frequency domain as input vectors. The loss function of the DD-CGAN is defined by the generative-adversarial loss, the data loss function, and the total variation loss. Thus, the DD-CGAN can be trained more accurately. The discriminator is used to calculate the feature differences between the interpolated seismic data set and the complete seismic data set to drive the generator network for learning optimal parameters. The numerical results on the test data set and field seismic data set demonstrate the effectiveness of the proposed DD-CGAN.","1558-0571","","10.1109/LGRS.2020.3008478","Scientific Research and Technology Development Project of China National Petroleum Corporation(grant numbers:2019A-3310); National Natural Science Foundation of China(grant numbers:41674130); State Key R&D Program(grant numbers:2017YFB0202905); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145587","Conditional generative adversarial network (CGAN);deep learning;dual-domain feature learning;seismic data interpolation","Interpolation;Generators;Training;Frequency-domain analysis;TV;Generative adversarial networks;Training data","deep learning (artificial intelligence);discrete Fourier transforms;feature extraction;geophysical signal processing;geophysical techniques;interpolation;seismology","discriminator network;discrete Fourier transformed data set;deep learning;data generation;data loss function;seismic data interpolation;DD-CGAN;dual-domain conditional generative adversarial network;image interpolations;image generations","","8","","19","IEEE","21 Jul 2020","","","IEEE","IEEE Journals"
"SD-GAN: Saliency-Discriminated GAN for Remote Sensing Image Superresolution","J. Ma; L. Zhang; J. Zhang","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","26 Oct 2020","2020","17","11","1973","1977","Recently, convolutional neural networks have shown superior performance in single-image superresolution. Although existing mean-square-error-based methods achieve high peak signal-to-noise ratio (PSNR), they tend to generate oversmooth results. Generative adversarial network (GAN)-based methods can provide high-resolution (HR) images with higher perceptual quality, but produce pseudotextures in images, which generally leads to lower PSNR. Besides, different regions in remote sensing images (RSIs) reflect discrepant surface topography and visual characteristics. This means a uniform reconstruction strategy may not be suitable for all targets in RSIs. To solve these problems, we propose a novel saliency-discriminated GAN for RSI superresolution. First, hierarchical weakly supervised saliency analysis is introduced to compute a saliency map, which is subsequently employed to distinguish the diverse demands of regions in the following generator and discriminator part. Different from previous GANs, the proposed residual dense saliency generator takes saliency maps as a supplementary condition in the generator. Simultaneously, combining the characteristic of RSIs, we design a new paired discriminator to enhance the perceptual quality, which measures the distance between generated images and HR images in salient areas and nonsalient areas, respectively. Comprehensive evaluations validate the superiority of the proposed model.","1558-0571","","10.1109/LGRS.2019.2956969","Beijing Natural Science Foundation(grant numbers:L182029); National Natural Science Foundation of China(grant numbers:61571050,41771407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933080","Deep learning;generative adversarial network (GAN);remote sensing;saliency analysis;superresolution","Generators;Gallium nitride;Generative adversarial networks;Image reconstruction;Feature extraction;Visualization","convolutional neural nets;geophysical image processing;image colour analysis;image reconstruction;image representation;image resolution;image texture;mean square error methods;object detection;remote sensing","mean-square-error-based methods;single-image superresolution;convolutional neural networks;remote sensing image superresolution;SD-GAN;paired discriminator;residual dense saliency generator;discriminator part;saliency map;hierarchical weakly supervised saliency analysis;RSI superresolution;novel saliency-discriminated GAN;uniform reconstruction strategy;visual characteristics;discrepant surface topography;RSIs;remote sensing images;higher perceptual quality;high-resolution images;generative adversarial network-based methods;PSNR;high peak signal-to-noise ratio","","6","","17","IEEE","16 Dec 2019","","","IEEE","IEEE Journals"
"Remote Sensing Image Jitter Restoration Based on Deep Generative Adversarial Network","Z. Zhang; Q. Zhou; Y. Xu; L. Ma; A. Iwasaki","Unmanned System Research Institute Northwestern Polytechnical University, Xi'an, China; Unmanned System Research Institute Northwestern Polytechnical University, Xi'an, China; Unmanned System Research Institute Northwestern Polytechnical University, Xi'an, China; Unmanned System Research Institute Northwestern Polytechnical University, Xi'an, China; Department of Aeronautics and Astronautics, Faculty of Engineering, The University of Tokyo","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2683","2686","High stability of the observation satellite platform is increasingly demanded in recent years. However, attitude jitter of observation satellites is a problem that degenerates the development of imaging quality and resolution. In order to reduce the geo-positioning errors and improve the geometric accuracy of remote sensing images, satellite jitter have been studied in recent years. In this work, a generative adversarial network (GAN) architecture is proposed to automatically learn and correct the deformed scene features from a single remote sensing image. In the proposed GAN, a convolutional neural network (CNN) is designed to discriminate the inputs and another CNN is used to generate so-called fake inputs. In order to explore the usefulness and effectiveness of GAN for jitter detection, the proposed GAN are trained on part of PatternNet dataset and tested on three popular remote sensing datasets. Several experiments show that the proposed models provide competitive results compared to other methods. the proposed GAN reveals the huge potential of GAN-based methods for the analysis of attitude jitter from remote sensing images.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554491","Remote sensing;deep learning;GAN;jitter estimation","Satellites;Image retrieval;Imaging;Jitter;Generative adversarial networks;Stability analysis;Image restoration","artificial satellites;backpropagation;feature extraction;feedforward neural nets;geophysical image processing;geophysical signal processing;image classification;jitter;learning (artificial intelligence);neural nets;pattern recognition;remote sensing","geo-positioning errors;remote sensing images;satellite jitter;generative adversarial network architecture;deformed scene features;single remote sensing image;convolutional neural network;CNN;jitter detection;popular remote sensing datasets;GAN-based methods;sensing image jitter restoration;deep generative adversarial network;observation satellite platform;attitude jitter;observation satellites","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"ISAR Images Generation Via Generative Adversarial Networks","R. -Y. Zhou; Z. -L. Yang; F. Wang","Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5267","5270","One of the challenges faced by current intelligent target recognition tasks is the lack of samples, especially in the Inverse Synthetic Aperture Radar (ISAR) images understanding. In this paper, we proposed an ISAR objects generative network to generate multi-aspect ISAR images. A simulated ISAR dataset of six types of aircrafts is produced via, using bidirectional analytic ray tracing (BART) method. Then, the proposed generative network is trained with the simulated ISAR dataset. We evaluated the performance of the proposed network using structural similarity (SSIM). The experimental results show that the generated targets are very close to the real ISAR samples, and the SSIM between generated and real ISAR images of aircrafts is larger than 0.7.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553814","National Natural Science Foundation of China(grant numbers:61901122); Natural Science Foundation of Shanghai(grant numbers:20ZR1406300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553814","Inverse Synthetic Aperture Radar (ISAR);Automatic Target Recognition (ATR);Generative Adversarial Nets (GANs)","Training;Target recognition;Image synthesis;Geoscience and remote sensing;Ray tracing;Generative adversarial networks;Aircraft","","","","1","","12","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Edge Prediction Net for Reconstructing Road Labels Contaminated by Clouds","M. Xu; Y. Li; J. Zhong; Y. Zhang; X. Liu","School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; Avic Leihua Electronic Technology Research Institute, Wuxi, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; Avic Leihua Electronic Technology Research Institute, Wuxi, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6969","6972","Extracting road information from remote sensing images has been a popular issue for decades. However, most studies focus on cloudless datasets, without considering cloud occlusion. The thick clouds especially make it impossible to extract the road information from the blocked parts. To address this problem, we propose a new two-stage method. Since generative adversarial networks (GAN) have powerful image generation capabilities, the two-stage method comprises an edge prediction net relied on GAN and a color filling part. The edge prediction net sketches the contours of the region contaminated by thick clouds in road labels, and the second part fills colors in the missing part by using the edges predicted at the first stage. We evaluate our model over the DeepGlobe Road Extraction dataset. The results show that our model performs excellently on visual effects and evaluation indicators.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323273","Ministry of Industry and Information Technology, China(grant numbers:MJZ-2016-S-44); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323273","Road Extraction;Edge Prediction;GAN;Cloud Removal","Roads;Image edge detection;Image color analysis;Task analysis;Remote sensing;Generators;Generative adversarial networks","feature extraction;geophysical image processing;remote sensing;roads","Road labels contaminated;road information;remote sensing images;popular issue;cloudless datasets;cloud occlusion;blocked parts;two-stage method;generative adversarial networks;GAN;powerful image generation capabilities;color filling part;edge prediction net sketches;missing part;DeepGlobe Road Extraction dataset","","","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Scene Images Diversity Improvement Generative Adversarial Network for Remote Sensing Image Scene Classification","X. Pan; J. Zhao; J. Xu","School of Computer Technology and Engineering, Changchun Institute of Technology, Changchun, China; School of Computer Technology and Engineering, Changchun Institute of Technology, Changchun, China; Key Laboratory of Changbai Mountain Historical Culture and VR Technology Reconfiguration, Changchun Institute of Technology, Changchun, China","IEEE Geoscience and Remote Sensing Letters","24 Sep 2020","2020","17","10","1692","1696","To achieve good remote sensing image scene classification, deep learning models usually require a large number of samples in the training stage. Unfortunately, collecting a large number of training scene images usually involves large acquisition and processing costs. In contrast, after training a generative adversarial network (GAN), scene samples can subsequently be generated automatically by the generator at a low cost. Then, the generated images can be added to the training set. A model with better classification ability will be obtained when these samples include more diverse scene structures and essential features than the original real images. In this letter, we propose the scene images diversity improvement GAN (diversity-GAN). Diversity-GAN has two important advantages. 1) The training process is designed in a progressive manner: the GAN's generator and discriminator progress from coarse- to fine-resolution scene images. This characteristic can guarantee the diversity of generated samples. In particular, it guarantees the diversity of the structure of the generated scene images. 2) The training progress is controllable: by introducing control parameters, diversity-GAN can directly determine the scene image resolution on which the training process should focus. This characteristic allows diversity-GAN to achieve scene image structure diversity at the coarse-resolution training stage with a few iterations. In the experiments, the UC-Merced and AID data sets are introduced. The results show that the samples generated by diversity-GAN can effectively improve the diversity of the sample set, and these generated samples can grant convolutional neural networks (CNNs) better classification ability in the training stage.","1558-0571","","10.1109/LGRS.2019.2953192","National Natural Science Foundation of China(grant numbers:41871236); Foundation of Jilin Provincial Science and Technology Department(grant numbers:20180101020JC,20180622006JC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8915710","Deep learning;diversity improvement;generative adversarial network (GAN);sample generation;scene classification","Training;Gallium nitride;Switches;Generative adversarial networks;Remote sensing;Generators;Machine learning","convolutional neural nets;feature extraction;geophysical image processing;image classification;image resolution;image sampling;learning (artificial intelligence);remote sensing","scene image resolution;scene image structure diversity;coarse-resolution training stage;training scene images;generative adversarial network;scene samples;diverse scene structures;scene images diversity improvement GAN;discriminator progress;fine-resolution scene images;remote sensing image scene classification;AID data sets;convolutional neural networks;UC-Merced data sets;CNNs;deep learning models","","9","","17","IEEE","27 Nov 2019","","","IEEE","IEEE Journals"
"Domain-Agnostic Domain Adaption for Building Footprint Extraction","F. Zhang; Y. Shi; X. X. Zhu","Data Science in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Chair of Remote Sensing Technology (LMF), Technical University of Munich, Munich, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","318","321","For global range satellite imaging mission, images captured from different areas may have large distribution biases due to different illuminations, shooting angles and atmospheric conditions. A straightforward idea to mitigate this problem is to categorize the images into different domains according the cities they belong to, and apply domain adaptation approaches. However, categorization by cities becomes unreasonable with the increase of the city number, and the emergence of inter-city similarity and intra-city discrepancy. With such consideration, this paper proposes a novel domain adaptation method named domain-agnostic domain adaptation (DADA) to reduce the distribution biases without explicitly defining the domain each image belongs to. To implement this, we augment the images to the styles of different domains by Generative Adversarial Networks (GAN) and contrastive learning to increase the generalizability of down-stream tasks. Experiments on Planetscope building footprint extraction datasets verify the effectiveness of our method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883996","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883996","Domain Adaptation;Generative Adversarial Networks;Contrastive Learning","Satellites;Urban areas;Buildings;Lighting;Imaging;Geoscience and remote sensing;Generative adversarial networks","feature extraction;geophysical image processing;learning (artificial intelligence);object detection","domain-agnostic domain adaption;global range satellite imaging mission;distribution biases;different illuminations;domain adaptation approaches;city number;inter-city similarity;intra-city discrepancy;domain adaptation method named domain-agnostic domain adaptation;domain each image;Planetscope building footprint extraction datasets","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"S2-CGAN: Self-Supervised Adversarial Representation Learning for Binary Change Detection in Multispectral Images","J. L. Holgado Alvarez; M. Ravanbakhsh; B. Demir","Faculty of Electrical Engineering and Computer Science, Technische Universität, Berlin, Germany; Faculty of Electrical Engineering and Computer Science, Technische Universität, Berlin, Germany; Faculty of Electrical Engineering and Computer Science, Technische Universität, Berlin, Germany","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2515","2518","Deep Neural Networks have recently demonstrated promising performance in binary change detection (CD) problems in remote sensing (RS), requiring a large amount of labeled multitemporal training samples. Since collecting such data is time-consuming and costly, most of the existing methods rely on pre-trained networks on publicly available computer vision (CV) datasets. However, because of the differences in image characteristics in CV and RS, this approach limits the performance of the existing CD methods. To address this problem, we propose a self-supervised conditional Generative Adversarial Network (S2-cGAN). The proposed S2-cGAN is trained to generate only the distribution of unchanged samples. To this end, the proposed method consists of two main steps: 1) Generating a reconstructed version of the input image as an unchanged image 2) Learning the distribution of unchanged samples through an adversarial game. Unlike the existing GAN based methods (which only use the discriminator during the adversarial training to supervise the generator), the S2-cGAN directly exploits the discriminator likelihood to solve the binary CD task. Experimental results show the effectiveness of the proposed S2-cGAN when compared to the state of the art CD methods.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324345","European Research Council(grant numbers:759764); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324345","Generative adversarial networks;binary change detection;multitemporal images;self-supervised learning;remote sensing","Training;Image reconstruction;Generative adversarial networks;Feature extraction;Generators;Gallium nitride;Task analysis","computer vision;geophysical image processing;image classification;learning (artificial intelligence);neural nets;remote sensing","GAN based methods;adversarial training;binary CD task;multispectral images;deep neural networks;binary change detection problems;remote sensing;labeled multitemporal training samples;computer vision datasets;image characteristics;S 2 -cGAN;adversarial game;self-supervised conditional generative adversarial network;self-supervised adversarial representation learning","","4","","8","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"HFGAN: A Heterogeneous Fusion Generative Adversarial Network for Sar-to-Optical Image Translation","N. Yu; A. Ma; Y. Zhong; X. Gong","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Beijing Institute of Remote Sensing Information, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2864","2867","Due to the influence of the imaging mechanism of SAR images, it is difficult to interpret ground information through SAR images without expert knowledge. On the contrary, optical images have rich spatial and color information, so it is necessary to conduct research on the translation of SAR to optical remote sensing images. In this end, we propose a heterogeneous fusion generative adversarial network (HFGAN) for SAR-to-optical image translation. There are two main improvements: (1) Complementary generation of global structure and texture information. A heterogeneous fusion generator and a multi-scale discriminator are proposed to ensure that the global and detailed features of the generated image are more accurate and rich. (2) Color fidelity. Chromatic aberration loss are introduced to reduce the color difference between the generated image and the real optical image. Through qualitative and quantitative experiments, it is proved that the proposed method not only obtains better visual effects, but also has certain progress in the evaluation metrics, which proves that the proposed method is superior to the previous advanced methods in SAR-to-optical image translation.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883519","Remote sensing;Generative Adversarial Network;Image tranlation","Optical losses;Image color analysis;Optical imaging;Generative adversarial networks;Visual effects;Generators;Radar polarimetry","aberrations;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image texture;optical images;radar imaging;remote sensing;synthetic aperture radar","optical remote sensing images;heterogeneous fusion generative adversarial network;sar-to-optical image translation;heterogeneous fusion generator;imaging mechanism;SAR images;rich spatial;color information","","","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Generative Adversarial Network with Residual Dense Generator for Remote Sensing Image Super Resolution","R. Sustika; A. B. Suksmono; D. Danudirdjo; K. Wikantika","Research Center for Informatics, Indonesian Institute of Sciences (LIPI), Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung (ITB), Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung (ITB), Bandung, Indonesia; Faculty of Earth Sciences and Technology, Institut Teknologi Bandung (ITB), Bandung, Indonesia","2020 International Conference on Radar, Antenna, Microwave, Electronics, and Telecommunications (ICRAMET)","25 Dec 2020","2020","","","34","39","Improving image resolution, especially spatial resolution, has been one of the most important concerns on remote sensing research communities. An efficient solution for improving spatial resolution is by using algorithm, known as super-resolution (SR). The super-resolution technique that received special attention recently is super-resolution based on deep learning. In this paper, we propose deep learning approach based on generative adversarial network (GAN) for remote sensing images super resolution. We used residual dense network (RDN) as generator network. Generally, deep learning with residual dense network (RDN) gives high performance on classical (objective) evaluation metrics meanwhile generative adversarial network (GAN) based approach shows a high perceptual quality. Experiment results show that combination of residual dense network generator with generative adversarial network training is found to be effective. Our proposed method outperforms the baseline method in terms of objective and perceptual quality evaluation metrics.","","978-1-7281-8922-2","10.1109/ICRAMET51080.2020.9298648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9298648","convolutional neural network;generative adversarial network;remote sensing;image;residual dense network;super-resolution","Training;Spatial resolution;Remote sensing;Generators;Generative adversarial networks;PSNR;Measurement","image reconstruction;image resolution;learning (artificial intelligence);remote sensing","generative adversarial network training;residual dense generator;remote sensing image super resolution;image resolution;spatial resolution;remote sensing research communities;super-resolution technique;deep learning approach;remote sensing images super resolution;generator network;residual dense network generator","","2","","15","IEEE","25 Dec 2020","","","IEEE","IEEE Conferences"
"Classification of Hyperspectral Images Based on Multiclass Spatial–Spectral Generative Adversarial Networks","J. Feng; H. Yu; L. Wang; X. Cao; X. Zhang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","22 Jul 2019","2019","57","8","5329","5343","Generative adversarial networks (GANs) are famous for generating samples by training a generator and a discriminator via an adversarial procedure. For hyperspectral image classification, the collection of samples is always difficult. However, directly applying GAN to hyperspectral image classification exists two problems. One is that the generated samples lack discriminative information. Meanwhile, the discriminator has no discriminative ability for multiclassification. Another is that spatial and spectral information requires to be considered in hyperspectral image classification simultaneously. To address these problems, a novel multiclass spatial-spectral GAN (MSGAN) method is proposed. In MSGAN, two generators are devised to generate the samples containing spatial and spectral information, respectively, and the discriminator is devised to extract joint spatial-spectral features and output multiclass probabilities. Moreover, novel adversarial objectives for multiclass are defined. The discriminator is devised to predict training samples belonging to true classes and generated samples belonging to all the classes with the same probability. The generators are devised to make the discriminator mistake. By adversarial learning between the discriminator and generators, the classification performance of the discriminator is promoted with the assistance of discriminative generated samples. Experimental results on hyperspectral images demonstrate that the proposed method achieves encouraging classification performance compared with several state-of-the-art methods, especially with the limited training samples.","1558-0644","","10.1109/TGRS.2019.2899057","National Natural Science Foundation of China(grant numbers:61871306,61772400,61773304); China Postdoctoral Science Foundation(grant numbers:2015M570816,2016T90892); State Key Program of National Natural Science of China(grant numbers:61836009); Chinese Academy of Sciences(grant numbers:LSIT201803D); Fundamental Research Funds for the Central Universities(grant numbers:JBX181707); Postdoctoral Research Program in Shaanxi Province of China; Joint Fund of the Equipment Research of Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661744","Adversarial learning;convolutional neural network (CNN);generative adversarial networks (GANs);hyperspectral images;spatial–spectral information","Hyperspectral imaging;Gallium nitride;Generative adversarial networks;Feature extraction;Generators;Training","feature extraction;hyperspectral imaging;image classification;neural nets","spatial information;spectral information;joint spatial-spectral features;output multiclass probabilities;training samples;discriminator mistake;adversarial learning;discriminative generated samples;hyperspectral images;encouraging classification performance;multiclass spatial-spectral generative adversarial networks;adversarial procedure;hyperspectral image classification;generated samples lack discriminative information;discriminative ability;multiclass spatial-spectral GAN method","","90","","52","IEEE","6 Mar 2019","","","IEEE","IEEE Journals"
"Remote Sensing Image Spatiotemporal Fusion via a Generative Adversarial Network With One Prior Image Pair","Y. Song; H. Zhang; H. Huang; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","17 May 2022","2022","60","","1","17","Spatiotemporal fusion (STF) is an effective solution to promote the application of remote sensing images, given that the tradeoff between the temporal resolution and the spatial resolution is ubiquitous in the production of remote sensing images. However, cloud coverage makes it difficult to obtain dense cloud-free Landsat–Moderate Resolution Imaging Spectroradiometer (MODIS) image pairs on the timeline, which limits the application of existing STF methods. Considering the lack of prior image pairs and the huge spatial resolution gap between Landsat and MODIS images, this article presents a novel remote sensing image STF method based on a generative adversarial network to handle one Landsat–MODIS prior image pair case (OPGAN), which contains a generator and a discriminator simultaneously trained in a min–max game. OPGAN is built based on the STF observation model that learns the base information from the prior Landsat image and then captures temporal change (TC) information from a difference image constructed from MODIS images collected at times 1 and 2 and sensor difference information from the difference image between Landsat and MODIS images at time 1. They are combined together to reconstruct the Landsat image at time 2 at both high spatial and high temporal resolution. Moreover, a change loss is proposed to further improve the accuracy of TC prediction. Extensive experiments on the STF dataset illustrate that the proposed OPGAN method can obtain more accurate prediction of spatial information and TCs in the case of insufficient prior information.","1558-0644","","10.1109/TGRS.2022.3171331","National Natural Science Foundation of China(grant numbers:61871298,42071322); Natural Science Foundation of Hubei Province(grant numbers:2020CFA053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765452","Change loss;generative adversarial network (GAN);one prior image pair;spatiotemporal fusion (STF)","Remote sensing;Earth;Artificial satellites;Spatial resolution;Image resolution;Generative adversarial networks;MODIS","geophysical image processing;image fusion;image resolution;neural nets;radiometry;remote sensing","STF methods;huge spatial resolution gap;MODIS images;novel remote sensing image STF method;generative adversarial network;Landsat-MODIS prior image pair case;STF observation model;prior Landsat image;temporal change information;difference image;high temporal resolution;insufficient prior information;dense cloud-free Landsat-Moderate Resolution Imaging Spectroradiometer;remote sensing image spatiotemporal fusion;OPGAN;high spatial resolution","","1","","54","IEEE","29 Apr 2022","","","IEEE","IEEE Journals"
"Overcoming Missing and Incomplete Modalities with Generative Adversarial Networks for Building Footprint Segmentation","B. Bischke; P. Helber; F. Koenig; D. Borth; A. Dengel","TU Kaiserslautern, Germany; TU Kaiserslautern, Germany; TU Kaiserslautern, Germany; German Research Center for Artificial Intelligence (DFKI), Germany; TU Kaiserslautern, Germany","2018 International Conference on Content-Based Multimedia Indexing (CBMI)","1 Nov 2018","2018","","","1","6","The integration of information acquired with different modalities, spatial resolution and spectral bands has shown to improve predictive accuracies. Data fusion is therefore one of the key challenges in remote sensing. Most prior work focusing on multi-modal fusion, assumes that modalities are always available during inference. This assumption limits the applications of multi-modal models since in practice the data collection process is likely to generate data with missing, incomplete or corrupted modalities. In this paper, we show that Generative Adversarial Networks can be effectively used to overcome the problems that arise when modalities are missing or incomplete. Focusing on semantic segmentation of building footprints with missing modalities, our approach achieves an improvement of about 2% on the Intersection over Union (IoU) against the same network that relies only on the available modality.","","978-1-5386-7021-7","10.1109/CBMI.2018.8516271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516271","Generative Adversarial Networks;Semantic Segmentation;Missing Modalities","Generators;Image segmentation;Gallium nitride;Generative adversarial networks;Training;Buildings;Feature extraction","image fusion;image segmentation;remote sensing","missing modalities;incomplete modalities;spatial resolution;spectral bands;data fusion;remote sensing;multimodal fusion;data collection process;corrupted modalities;semantic segmentation;generative adversarial networks;footprint segmentation;intersection over union;IoU","","7","","30","IEEE","1 Nov 2018","","","IEEE","IEEE Conferences"
"WeGAN: Deep Image Hashing With Weighted Generative Adversarial Networks","Y. Wang; L. Zhang; F. Nie; X. Li; Z. Chen; F. Wang","Shanxi Provincial Key Laboratory of Resources, Environment and Disaster Monitoring, Jinzhong, China; State Key Laboratory of Remote Sensing Science, Faculty of Geographical Science, Beijing Normal University, Beijing, China; Center for Optical Imagery Analysis and Learning, Northwestern Polytechnical University, Xian, China; State Key Laboratory of Remote Sensing Science, Faculty of Geographical Science, Beijing Normal University, Beijing, China; School of Mathematical Sciences, Beijing Normal University, Beijing, China; School of Mathematical Sciences, Beijing Normal University, Beijing, China","IEEE Transactions on Multimedia","21 May 2020","2020","22","6","1458","1469","Image hashing has been widely used in image retrieval tasks. Many existing methods generate hashing codes based on image feature representations. They rarely consider the rich information such as image clustering information contained in the image set as well as uncertain relationships between images and tags simultaneously. In this paper, we develop a Weighted Generative Adversarial Networks (WeGAN) to transfer the clustering information of images to construct the hashing code. WeGAN consists three modules: 1) a hashing learning process for transferring knowledge of the image set to hashing codes of single images; 2) by means of hashing codes, a module to generate image content, tag representation, and their joint information which reflects the correlation between the image and the corresponding tags; 3) a discriminator to distinguish the generated data from the original source, and then formulating three loss functions. Different weights are assigned to these loss functions in order to deal with the uncertainties between images and tags. Through introducing the image set to process the image hashing with different tags, WeGAN can naturally provide the information of clustering results, which is useful for image hashing with multi-tags. The generated hashing code has the ability to dynamically process the uncertain relationships between images and tags. Experiments on three challenging datasets show that WeGAN outperforms the state-of-the-art methods.","1941-0077","","10.1109/TMM.2019.2947197","National Natural Science Foundation of China(grant numbers:41801241); National Basic Research Program of China (973 Program)(grant numbers:2018YFC0213600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867954","Image hashing;generative adversarial networks;image set;uncertainties between images and tags","Gallium nitride;Generative adversarial networks;Deep learning;Uncertainty;Semantics;Task analysis;Linear programming","cryptography;feature extraction;file organisation;image coding;image representation;image retrieval;learning (artificial intelligence);neural nets;pattern clustering","WeGAN;image content;image retrieval tasks;image feature representations;image clustering information;weighted generative adversarial networks;hashing learning process;deep image hashing code;tag representation","","15","","64","IEEE","14 Oct 2019","","","IEEE","IEEE Journals"
"The Denoising of Desert Seismic Data Based on Cycle-GAN With Unpaired Data Training","Y. Li; H. Wang; X. Dong","Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China","IEEE Geoscience and Remote Sensing Letters","26 Oct 2021","2021","18","11","2016","2020","The seismic data with high quality are the essential foundation of imaging and interpretation. However, the real seismic data are inevitably contaminated by noise, which affects the subsequent processing and interpretation of seismic data. In desert seismic data, the energy of noise is stronger. Also, the frequency-band overlap between noise and effective signals is more serious. Recently, some methods based on supervised learning can suppress the desert seismic noise to some extent. Generally, supervised learning-based methods use synthetic noisy data and paired pure data as training sets to train model. However, the difference between synthetic noisy data of training and real seismic data of testing leads to the degradation of the model, and the denoising results often have many false seismic events when dealing with field seismic data. To solve the above problem, we introduce Cycle-generative adversarial networks (GANs) into the denoising of desert seismic records. Cycle-GAN is an unsupervised learning-based method. It can learn the domain mapping from noisy data domain to effective signal data domain through unpaired data training. So we use unpaired real desert common-shot-point data and synthetic pure data to train Cycle-GAN, so as to effectively improve the denoising ability of the method for real seismic data. Finally, the denoising of desert seismic data is realized. The experiment shows that the Cycle-GAN with unpaired data training can effectively suppress desert seismic noise and retain the effective signal amplitude. Also, the denoising result has less false seismic reflection.","1558-0571","","10.1109/LGRS.2020.3011130","National Natural Science Foundations of China(grant numbers:41730422); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153804","Cycle-generative adversarial networks (GANs);desert seismic data;training set;unpaired data training","Noise reduction;Training;Noise measurement;Generators;Gallium nitride;Data models;Generative adversarial networks","geophysical signal processing;seismology;signal denoising;unsupervised learning","desert seismic data denoising;Cycle-GAN;unpaired data training;desert seismic noise;synthetic noisy data;field seismic data;desert seismic records;noisy data domain;effective signal data domain;unpaired real desert common-shot-point data;synthetic pure data;subsequent processing;seismic data interpretation;frequency-band overlap;supervised learning;cycle-generative adversarial networks;domain mapping;effective signal amplitude","","11","","13","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"Synthesis of Multispectral Optical Images From SAR/Optical Multitemporal Data Using Conditional Generative Adversarial Networks","J. D. Bermudez; P. N. Happ; R. Q. Feitosa; D. A. B. Oliveira","Department of Electrical Engineering, Pontifical University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Computer System Engineering, Rio de Janeiro State University, Rio de Janeiro, Brazil; IBM Research Brazil Lab, São Paulo, Brazil","IEEE Geoscience and Remote Sensing Letters","22 Jul 2019","2019","16","8","1220","1224","The synthesis of realistic data using deep learning techniques has greatly improved the performance of classifiers in handling incomplete data. Remote sensing applications that have profited from those techniques include translating images of different sensors, improving the image resolution and completing missing temporal or spatial data such as in cloudy optical images. In this context, this letter proposes a new deep-learning-based framework to synthesize missing or corrupted multispectral optical images using multimodal/multitemporal data. Specifically, we use conditional generative adversarial networks (cGANs) to generate the missing optical image by exploiting the correspondent synthetic aperture radar (SAR) data with a SAR-optical data from the same area at a different acquisition date. The proposed framework was evaluated in two land-cover applications over tropical regions, where cloud coverage is a major problem: crop recognition and wildfire detection. In both applications, our proposal was superior to alternative approaches tested in our experiments. In particular, our approach outperformed recent cGAN-based proposals for cloud removal, on average, by 7.7% and 8.6% in terms of overall accuracy and F1-score, respectively.","1558-0571","","10.1109/LGRS.2019.2894734","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; Conselho Nacional de Desenvolvimento Científico e Tecnológico; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637007","Conditional generative adversarial networks (cGANs);crop recognition;deep learning;remote sensing;wildfire detection","Optical imaging;Optical sensors;Clouds;Nonlinear optics;Adaptive optics;Gallium nitride;Generators","crops;geophysical image processing;image classification;image resolution;land cover;learning (artificial intelligence);neural nets;optical images;radar imaging;radar resolution;remote sensing by radar;synthetic aperture radar","multispectral optical images;conditional generative adversarial networks;deep learning techniques;remote sensing applications;image resolution;spatial data;cloudy optical images;land-cover applications;SAR-optical multitemporal data;multimodal-multitemporal data;SAR optical data;cGAN-based proposals;synthetic aperture radar data","","45","","15","IEEE","7 Feb 2019","","","IEEE","IEEE Journals"
"Physics-Informed Hyperspectral Remote Sensing Image Synthesis With Deep Conditional Generative Adversarial Networks","L. Liu; W. Li; Z. Shi; Z. Zou","State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; Department of Guidance, Navigation and Control, School of Astronautics, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","18 May 2022","2022","60","","1","15","High-resolution hyperspectral remote sensing images are of great significance to agricultural, urban, and military applications. However, collecting and labeling hyperspectral images are time-consuming, expensive, and usually heavily rely on domain knowledge. In this article, we propose a new method for generating high-resolution hyperspectral images and subpixel ground-truth annotations from RGB images. Given a single high-resolution RGB image as its conditional input, unlike previous methods that directly predict spectral reflectance and ignores the physics behind it, we consider both imaging mechanism and spectral mixing, introduce a deep generative network that first recovers the spectral abundance for each pixel, and then generate the final spectral data cube with the standard USGS spectral library. In this way, our method not only synthesizes high-quality spectral data existing in the real world but also generates subpixel-level spectral abundance with well-defined spectral reflectance characteristics. We also introduce a spatial discriminative network and a spectral discriminative network to improve the fidelity of the synthetic output from both spatial and spectral perspectives. The whole framework can be trained end-to-end in an adversarial training paradigm. We refer to our method as “Physics-informed Deep Adversarial Spectral Synthesis (PDASS).” On the IEEE grss_dfc_2018 dataset, our method achieves an MPSNR of 47.56 on spectral reconstruction accuracy and outperforms other state-of-the-art methods. As latent variables, the generated spectral abundance and the atmospheric absorption coefficients of sunlight also suggest the effectiveness of our method.","1558-0644","","10.1109/TGRS.2022.3173532","National Natural Science Foundation of China(grant numbers:62125102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770778","Generation adversarial networks (GANs);hyperspectral image;imaging model;remote sensing;spectral super-resolution (SSR)","Hyperspectral imaging;Superresolution;Atmospheric modeling;Image reconstruction;Absorption;Spatial resolution;Libraries","geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image colour analysis;image reconstruction;image resolution;remote sensing;spectral analysis","Physics-informed hyperspectral remote sensing image synthesis;deep conditional generative adversarial networks;high-resolution hyperspectral remote sensing images;agricultural applications;urban, applications;military applications;high-resolution hyperspectral images;subpixel ground-truth annotations;RGB images;single high-resolution RGB image;conditional input;imaging mechanism;spectral mixing;deep generative network;final spectral data cube;standard USGS spectral library;high-quality spectral data;subpixel-level spectral abundance;well-defined spectral reflectance characteristics;spatial discriminative network;spectral discriminative network;spatial perspectives;spectral perspectives;adversarial training paradigm;Physics-informed Deep Adversarial Spectral Synthesis;spectral reconstruction accuracy;generated spectral abundance","","1","","79","IEEE","9 May 2022","","","IEEE","IEEE Journals"
"Generative Adversarial Networks for Hard Negative Mining in CNN-Based SAR-Optical Image Matching","L. H. Hughes; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4391","4394","In this paper we propose a deep generative framework, based on a generative adversarial network (GAN) and an auto encoder (AE), for generating non-corresponding SAR patches to be used in hard negative mining in situations of limited data quantities. We evaluate the effectiveness of this formulation of hard negative mining for reducing the false positive rate (FPR) and improving network determinability in a SAR-optical patch matching application. Our generative network is trained to generate realistic SAR images using an existing SAR-optical matching dataset. These generated images are then used as non-corresponding, hard negative samples for training a SAR-optical matching network. Our results show that we are able to generate realistic SAR images which exhibit many SAR-like features, such as layover and speckle. We further show that by fine tuning the original matching network using these hard negative samples we are able to improve the overall performance of the original SAR-optical matching network.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517355","generative adversarial networks (GANs);synthetic aperture radar (SAR);multi-modal matching","Synthetic aperture radar;Gallium nitride;Training;Optical imaging;Optical sensors;Remote sensing;Task analysis","data mining;image matching;optical images;radar computing;radar imaging;synthetic aperture radar","hard negative samples;original SAR-optical;generative adversarial network;hard negative mining;CNN-based SAR-optical image matching;deep generative framework;network determinability;generative network;realistic SAR images;original matching network;SAR patches;SAR-optical matching dataset;optical patch matching application;optical patch matching application;false positive rate","","3","","11","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A Distribution Controllable Simulation Method of Remote Sensing Sea-Ice Images","C. Zhao; X. Dong; Y. Yan; N. Su; B. Huang","Harbin Engineering University, Jushri Technologies, INC; Harbin Engineering University, Jushri Technologies, INC; Harbin Engineering University, Jushri Technologies, INC; Harbin Engineering University, Jushri Technologies, INC; Harbin Engineering University, Jushri Technologies, INC","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","3063","3065","In the case of sailing out in the sea-ice areas, it is instructive for route planning to research the distribution characters of ice in the target sea. Existing deep learning methods have shown their strength on sea-ice images processing like image classification. Due to the complex environment around sea-ice area, capturing large quantities of images is not easy. Besides, it's often hard to guarantee the abundance of sea-ice distribution of each different scene class, which causes unsatisfactory classification results. Therefore, it is of considerable practical value to research on sea-ice images simulation. In this paper, a distribution controllable simulation method is proposed based on generative adversarial networks for remote sensing sea-ice images. This research can help settle the problem of small sea-ice samples, as well as can provide a practical method for optical image simulation and similar type problems.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323550","National Natural Science Foundation of China(grant numbers:61801142,61971153); Fundamental Research Funds for the Central Universities(grant numbers:3072020CFJ0804,3072020CFJ0805); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323550","generative adversarial networks;image simulation;remote sensing sea-ice","Sea ice;Generators;Marine vehicles;Feature extraction;Convolution;White noise;Software","geophysical image processing;image classification;oceanographic techniques;remote sensing;sea ice","distribution controllable simulation method;remote sensing sea-ice images;sea-ice area;target sea;deep learning methods;image classification;sea-ice distribution;sea-ice images simulation;sea-ice samples;optical image simulation","","","","7","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Image Generation Based on Texture Guided VAE-AGAN for Regions of Interest Detection in Remote Sensing Images","L. Zhang; Y. Liu","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","2310","2314","Deep learning has shown great strength in regions of interest (ROIs) detection for remote sensing images (RSIs). However, for most of RSIs, the unbalanced distribution of positive and negative samples greatly limits the performance of the deep learning-based methods. To cope with this issue, we propose a novel method based on texture guided variational autoencoder-attention wise generative adversarial network (VAE-AGAN) to augment the training data for ROI detection. First, to generate realistic texture details of RSIs, we propose a texture guidance block to embed texture prior information into encoder and decoder networks. Second, we introduce the channel and spatial-wise attention layers in the discriminator construct to adaptively recalibrate the varying importance of different channels and spatial regions of input RSIs. Finally, we apply the RSI dataset balanced by our proposal to the weakly supervised ROI detection method. Experimental results demonstrate that the proposal can not only improve the performance of ROI detection, but also outperform other competing augmentation methods.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413823","National Natural Science Foundation of China; Beijing Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413823","Image generation;texture guidance;attention;generative adversarial networks","Learning systems;Deep learning;Image synthesis;Training data;Signal processing;Generative adversarial networks;Decoding","deep learning (artificial intelligence);image texture;remote sensing","remote sensing images;unbalanced distribution;positive samples;negative samples;deep learning;texture guidance block;spatial-wise attention layers;RSI;weakly supervised ROI detection method;image generation;texture guided VAE-AGAN;texture guided variational autoencoder;attention wise generative adversarial network","","1","","23","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"GAN-Based Siamese Framework for Landslide Inventory Mapping Using Bi-Temporal Optical Remote Sensing Images","B. Fang; G. Chen; L. Pan; R. Kou; L. Wang","College of Marine Science and Technology, China University of Geosciences, Wuhan, China; College of Marine Science and Technology, China University of Geosciences, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","24 Feb 2021","2021","18","3","391","395","Regarding landslide inventory mapping (LIM) as a task similar to change detection, current methods for LIM using bi-temporal optical remote sensing images are generally derived from change detection methods. In practice, not all changed regions belong to landslides, e.g., new roads, canals, and vegetation. Therefore, an ideal strategy is supposed to present two steps: discriminating changed and unchanged regions, and detecting landslides apart from other changed regions. Owing to the complexity and uncertainty of landslides, it is difficult to simultaneously separate landslides with unchanged and other changed regions by a single model. Addressing this problem, in this letter, we apply a generative adversarial network (GAN) in a Siamese neural network, and then propose a GAN-based Siamese framework (GSF) for LIM. The GSF comprises two cascaded modules, namely, domain adaptation and landslide detection. The former module aims to make a cross-domain mapping between prelandslide and postlandslide images with adversarial learning, then translate paired images into the same domain to suppress the domain discrepancies of bi-temporal remote sensing images. Meanwhile, the latter module aims to perform pixel-level landslide detection with a Siamese model. By training this cascaded framework, our method learns to produce landslide inventory maps without any preprocessing or postprocessing. Extensive experiments and comparison with other state-of-the-art methods verify the efficiency and superiority of our method.","1558-0571","","10.1109/LGRS.2020.2979693","National Natural Science Foundation of China(grant numbers:41674015,U1711266,41925007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042318","Change detection;domain adaptation;generative adversarial network (GAN);landslide inventory mapping (LIM);Siamese network","Terrain factors;Gallium nitride;Feature extraction;Remote sensing;Optical imaging;Optical sensors;Generative adversarial networks","geomorphology;geophysical image processing;geophysical techniques;image classification;neural nets;object detection;remote sensing;remote sensing by radar;terrain mapping","LIM;domain adaptation;cross-domain mapping;postlandslide images;paired images;bi-temporal remote sensing images;pixel-level landslide detection;landslide inventory maps;GAN-based Siamese framework;landslide inventory mapping;bi-temporal optical remote sensing images;change detection methods;unchanged regions;Siamese neural network;generative adversarial network","","17","","13","IEEE","19 Mar 2020","","","IEEE","IEEE Journals"
"Remote Sensing Image Colorization Based on Multiscale SEnet GAN","M. Wu; X. Jin; Q. Jiang; S. -J. Lee; L. Guo; Y. Di; S. Huang; J. Huang","School of Software, Yunnan University, Kunming, Yunnan, China; School of Software, Yunnan University, Kunming, Yunnan, China; School of Software, Yunnan University, Kunming, Yunnan, China; Institute of Technology Management, National Chiao Tung University, Hsinchu, Taiwan, China; School of Software, Yunnan University, Kunming, Yunnan, China; School of Software, Yunnan University, Kunming, Yunnan, China; School of Software, Yunnan University, Kunming, Yunnan, China; School of Software, Yunnan University, Kunming, Yunnan, China","2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","23 Jan 2020","2019","","","1","6","Image colorization technique is to colorize the grayscale images or single-channel images. In the research of image colorization, the coloring of remote sensing images is a challenging problem. This paper proposes a new method of remote sensing image colorization method based on Deep Convolution Generative Adversarial Network (DCGAN). We combine multi-scale convolution with Squeeze-and-Excitation Networks (SEnet) to propose a new model that is applied to the generator of DCGAN. Therefore, the generator not only retains the largest image features in the process of the generating images, but also can adjust the channel weights in the training process. We have compared the proposed method with other image colorization methods, and the results show that the proposed method has a good performance on both human vision and image evaluation indicators on the colorization of remote sensing images.","","978-1-7281-4852-6","10.1109/CISP-BMEI48845.2019.8965902","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965902","Remote sensing;Image Colorization;Squeeze-and-excitation networks;Generative adversarial network;Feature extraction","Image color analysis;Generators;Feature extraction;Gallium nitride;Generative adversarial networks;Remote sensing;Gray-scale","convolutional neural nets;feature extraction;geophysical image processing;image colour analysis;learning (artificial intelligence);remote sensing","human vision;image evaluation indicators;multiscale SEnet GAN;image colorization technique;grayscale images;single-channel images;remote sensing image colorization method;deep convolution generative adversarial network;largest image features;DCGAN;squeeze-and-excitation networks","","2","","25","IEEE","23 Jan 2020","","","IEEE","IEEE Conferences"
"A Method to Create Training Dataset for Dehazing with Cyclegan","H. Zhang; F. Mou; S. Duan; S. Huang; S. Wang; D. Xu; Z. Zheng","Kunming Power Supply Bureau of Yunnan Power Grid Co., Ltd., Kunming, Yunnan, PRC; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, Sichuan, PRC; Kunming Power Supply Bureau of Yunnan Power Grid Co., Ltd., Kunming, Yunnan, PRC; Kunming Power Supply Bureau of Yunnan Power Grid Co., Ltd., Kunming, Yunnan, PRC; Kunming Power Supply Bureau of Yunnan Power Grid Co., Ltd., Kunming, Yunnan, PRC; Kunming Power Supply Bureau of Yunnan Power Grid Co., Ltd., Kunming, Yunnan, PRC; Guangxi Key Laboratory for Spatial Information and Geomatics, Guilin University of Technology, Guilin, Guangxi, PRC","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6997","7000","Haze usually blurs the characteristics of images shotted in adverse weather conditions. It brings many challenges for computer vision such as object detection. Due to the lack of effective training dataset of remote sensing image, the dehazing usually utilize the physical model rather than the deep learning approach based on a large number of training dataset. An effective method to create training dataset may provide some new ideas for remote sensing image dehazing. In this paper, a novel approach based on the visual features rather than the physical counterparts is developed to create a training dataset for remote sensing images dehazing. Firstly, 400 haze images and 400 clear images with size of 240×240 pixels were gathered from Landsat 8. Secondly, the dataset was utilized to train the cycle-consistent generative adversarial network (CycleGAN) to derive an image transform model which can convert a clear image into a haze one. Thirdly, 4000 clear images with size of 240×240 pixels were collected from Landsat 8 and the images were transformed into the haze images with our model. Finally, the ratio of training dataset and testing dataset is set as 4:1. The haze images created by us were selected as the input and the original clear images were chosen as the output to train the convolutional neural networks to derive the dehazing models. The created dataset and the dehazing models derived were tested, and the experimental results showed that our approach is better to keep the brightnessinformation of the original image, but it is not good at keeping the chroma information.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324099","Ministry of Natural Resources(grant numbers:KF-2019-04-074,KF-2019-04-069,KF-2018-03-063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324099","Dehazing;CycleGAN","Atmospheric modeling;Training;Remote sensing;Scattering;Testing;Transforms;Generative adversarial networks","computer vision;feature extraction;geophysical image processing;image classification;image colour analysis;image enhancement;image resolution;image restoration;learning (artificial intelligence);neural nets;object detection;object recognition;remote sensing","create training dataset;effective training dataset;physical model;deep learning approach;remote sensing image dehazing;remote sensing images dehazing;400 haze images;400 clear images;Landsat 8;4000 clear images;testing dataset;original clear images;dehazing models;created dataset","","","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Visual Prediction of Typhoon Clouds With Hierarchical Generative Adversarial Networks","H. Li; S. Gao; G. Liu; D. Guo; C. Grecos; P. Ren","College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; North China Sea Marine Forecasting Center, Ministry of Natural Resources, Qingdao, China; North China Sea Marine Forecasting Center, Ministry of Natural Resources, Qingdao, China; North China Sea Marine Forecasting Center, Ministry of Natural Resources, Qingdao, China; School of Computing, National college of Ireland, Dublin 1, Ireland; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1478","1482","We develop a hierarchical generative adversarial network (HGAN) for generating future typhoon cloud remote sensing images, which enables a visual means to typhoon cloud prediction. The HGAN consists of a global generator and a local discriminator. The global generator aims at producing the future typhoon cloud images as realistic as possible and accordingly reveals the structure and future location of the typhoon clouds. It is constructed in terms of a hierarchical architecture with multiple subnetworks, which capture the overall typhoon variations and favor generating clear future typhoon cloud images. The local discriminator tries its best to distinguish generated typhoon cloud images from ground-truth ones, based on the local patches. The local procedure encourages the discriminator to focus on characterizing the moving typhoon clouds rather than the still background. The global generator and the local discriminator are trained in an adversarial fashion with respect to historical typhoon cloud image sequences. The trained HGAN is capable of producing reliable visual predictions that are not only enabled by the global generator and but also examined by the local discriminator. Experiments validate the effectiveness of the HGAN for typhoon cloud prediction.","1558-0571","","10.1109/LGRS.2019.2950687","National Basic Research Program of China (973 Program)(grant numbers:017YFC1405600); National Natural Science Foundation of China(grant numbers:61971444); Shandong Provincial Key Research and Development Program(grant numbers:2017CXGC0902); Fundamental Research Funds for the Central Universities(grant numbers:18CX05014A); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897004","Future image generation;hierarchical generative adversarial networks (HGANs);typhoon cloud prediction","Tropical cyclones;Generators;Visualization;Generative adversarial networks;Image sequences;Training;Satellites","clouds;geophysical image processing;image sequences;neural nets;remote sensing;storms;weather forecasting","generated typhoon cloud images;local patches;local procedure;moving typhoon clouds;local discriminator;historical typhoon cloud image sequences;reliable visual predictions;HGAN;typhoon cloud prediction;visual prediction;hierarchical generative adversarial network;future typhoon cloud remote sensing images;visual means;global generator;hierarchical architecture;typhoon variations;clear future typhoon cloud images","","5","","15","IEEE","12 Nov 2019","","","IEEE","IEEE Journals"
"An Improved Method for Removal of Thin Clouds in Remote Sensing Images by Generative Adversarial Network","C. Zhang; X. Zhang; Q. Yu; C. Ma","College of Geoexploration Science and Technology, Jilin University, Changchun, China; College of Geoexploration Science and Technology, Jilin University, Changchun, China; College of Geoexploration Science and Technology, Jilin University, Changchun, China; College of Geoexploration Science and Technology, Jilin University, Changchun, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6706","6709","Thin clouds will attenuate surface radiation, causing color distortion in remote sensing images and loss of detailed information. Aiming at the color cast phenomenon of the existing thin cloud removal methods, this paper proposes a remote sensing image thin cloud removal method based on generative adversarial networks with color consistency constraints. The RICE1 thin cloud dataset was used to verify the effectiveness of the method. CIEDE2000 was added to the evaluation index to measure the consistency of the thin cloud removal results and the color of the cloud-free image, and the reasonable value of the color constraint parameter $\lambda$ was explored. The experimental results show that the method in this paper is significantly better than homomorphic filtering, dark channel prior method and non-color-constrained generative adversarial network cloud removal method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884704","Thin cloud removal;Color consistency constraint;Gan;CIEDE2000","Deep learning;Image color analysis;Filtering;Interference;Generative adversarial networks;Distortion;Indexes","casting;filtering theory;geophysical image processing;image colour analysis;remote sensing","thin clouds;remote sensing images;color distortion;color cast phenomenon;existing thin cloud removal methods;remote sensing image thin cloud removal method;color consistency constraints;RICE1 thin cloud dataset;cloud removal results;cloud-free image;color constraint parameter $\lambda;dark channel prior method;noncolor-constrained generative adversarial network cloud removal method","","1","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"GAN-NL: Unsupervised Representation Learning for Remote Sensing Image Classification","Y. Duan; X. Tao; M. Xu; C. Han; J. Lu","Department of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Information Engineering, Beihang University, Beijing, China; Department of Electronic and Information Engineering, Beihang University, Beijing, China","2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","21 Feb 2019","2018","","","375","379","Recently, deep learning methods have greatly enhanced the classification performance because of their strong representation ability in the local receptive field. However, the non-local spatial information always exist in the images. Moreover, the limited amount of the labeled data imposes great challenges on the supervised representation learning model, especially the remote sensing images. With the consideration, we propose a generative adversarial network with non-local spatial information (GAN-NL) for remote sensing image classification. Specifically, a non-local layer is incorporated into a generative adversarial network for unsupervised representation learning. Then, a classification network is designed to infer the labels of the images. The classification results on the challenging NWPU-RESISC45 remote sensing image dataset show that our proposed method performs favorably against the state-of-the-art methods in terms of the classification accuracy without any pre-training.","","978-1-7281-1295-4","10.1109/GlobalSIP.2018.8646414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8646414","Non-local spatial information;generative adversarial networks (GANs);unsupervised representation learning;remote sensing image classification","Remote sensing;Generators;Training;Gallium nitride;Sports;Image classification","geophysical image processing;image classification;image representation;remote sensing;unsupervised learning","GAN-NL;unsupervised representation learning;remote sensing image classification;deep learning methods;classification performance;local receptive field;nonlocal spatial information;supervised representation learning model;remote sensing images;generative adversarial network;nonlocal layer;classification network;classification accuracy","","9","","21","IEEE","21 Feb 2019","","","IEEE","IEEE Conferences"
"Semisupervised Variational Generative Adversarial Networks for Hyperspectral Image Classification","C. Tao; H. Wang; J. Qi; H. Li","School of Geosciences and InfoPhysics, Central South University, Changsha, China; School of Geosciences and InfoPhysics, Central South University, Changsha, China; School of Geosciences and InfoPhysics, Central South University, Changsha, China; School of Geosciences and InfoPhysics, Central South University, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Mar 2020","2020","13","","914","927","Although the hyperspectral image (HSI) classification is extensively investigated, this task remains challenging when the number of labeled samples is extremely limited. In this article, we overcome this challenge by using synthetic samples and proposing semisupervised variational generative adversarial networks (GANs). In contrast to conditional GAN (previously used for the generation of HSI samples), the proposed approach has two novel aspects. First, an encoder-decoder network is extended to the semisupervised context using an ensemble prediction technique. Through this technique, our deep generative model can be trained using limited labeled samples (only five samples per class) with a large number of unlabeled samples. Second, we build a collaborative relationship between the generation network and the classification network. This property enables that our model can produce meaningful samples that can contribute to the final classification. The experiments on four benchmark HSI datasets demonstrate that the proposed model can achieve an increase of >10% in overall classification accuracy compared with the baseline model without using the generated sample. We also show that the proposed model can achieve better and more robust performance for HSI classification than other generative methods as well as semisupervised methods, especially when the labeled data are limited.","2151-1535","","10.1109/JSTARS.2020.2974577","National Natural Science Foundation of China(grant numbers:41771458,41301453,41871364); Natural Science Foundation of Hunan Province(grant numbers:2017JJ3378); Fundamental Research Funds for the Central Universities of Central South University(grant numbers:1053320182684); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007673","Deep learning;generative adversarial networks (GAN);hyperspectral image classification;semisupervised learning;variational auto-encoder (VAE)","Gallium nitride;Training;Generative adversarial networks;Hyperspectral imaging;Data models","geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence)","unlabeled samples;deep generative model;ensemble prediction technique;semisupervised context;encoder-decoder network;HSI samples;conditional GAN;semisupervised variational generative adversarial networks;synthetic samples;labeled samples;hyperspectral image classification;semisupervised methods;generative methods;HSI classification;benchmark HSI datasets;classification network;generation network","","10","","46","CCBY","24 Feb 2020","","","IEEE","IEEE Journals"
"DETGAN: GAN for Arbitrary-oriented Object Detection in Remote Sensing Images","S. Cheng; P. Yao; K. Deng; L. Fu","Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China","2022 Asia Conference on Algorithms, Computing and Machine Learning (CACML)","19 Aug 2022","2022","","","337","341","Object detection in remote sensing images has be-come a research focus in recent years with the development of deep learning. However, due to objective reasons such as weather, cost, etc., we can hardly obtain abundant high-quality remote sensing images, especially for specific targets, which severely limits the training of the object detector, leading to poor detection performance. Thus for the first time, this paper introduces the Generative Adversarial Networks(GANs) for arbitrary-oriented object detection in remote sensing images, by augmenting the dataset to improve the performance of detectors. We construct DETGAN with two-layer self-attention modules to capture long-distance dependence for high-quality image generation. To solve the mismatch between generated slices and the samples for detectors, we propose the GAN-to-Detection transfer strategy, in which the slices are inserted into a background with the same size as the samples for detectors and then added to the training set. Experiments show that the performance of ship detectors is successfully improved with the transfer strategy, and demonstrate that GAN is an effective way to alleviate the problem of data insufficiency in remote sensing image object detection.","","978-1-6654-8290-5","10.1109/CACML55074.2022.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852535","Generative Adversarial Network;remote sensing;arbitrary-oriented object detection;transfer strategy","Training;Machine learning algorithms;Image synthesis;Detectors;Object detection;Generative adversarial networks;Sensors","geophysical image processing;learning (artificial intelligence);object detection;remote sensing;ships","GAN-to-Detection transfer strategy;remote sensing image object detection;arbitrary-oriented object detection;objective reasons;high-quality remote sensing images;object detector;poor detection performance;high-quality image generation","","","","25","IEEE","19 Aug 2022","","","IEEE","IEEE Conferences"
"Remote Sensing Image Generation Based on Attention Mechanism and VAE-MSGAN for ROI Extraction","L. Zhang; Y. Liu","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","A variety of deep learning approaches have been applied to region of interest (ROI) extraction, which is a fundamental task in the field of remote sensing image (RSI) processing. However, the unbalanced distribution of positive and negative samples in most RSIs greatly restricts the performance of these deep learning-based methods. In this study, a data augmentation method based on variational autoencoder-multiscale generative adversarial network (VAE-MSGAN) with spatial and channelwise attention (SCA) is proposed to balance the sample distribution and improve the subsequent ROI extraction results. First, we combine the original multispectral information with handcrafted texture features to make full use of the low-level visual features of RSIs. We then design a VAE-MSGAN to generate realistic RSIs with high quality and diversity. Specifically, in the generator construct, SCA blocks are introduced to adaptively recalibrate the varying importance of different channels and spatial regions. We also build a multiscale discriminator architecture to improve the visual quality of the generated samples. Finally, we compare the ROI extraction results before and after the augmentation. Our experimental results demonstrate that the proposed method can not only improve the performance of ROI extraction but also be superior to other classical generative methods.","1558-0571","","10.1109/LGRS.2021.3068271","Beijing Natural Science Foundation(grant numbers:L182029); National Natural Science Foundation of China(grant numbers:61571050,41771407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9393473","Generative adversarial networks (GANs);region of interest (ROI) extraction;remote sensing;variational autoencoder (VAE)","Training;Generators;Data mining;Task analysis;Remote sensing;Generative adversarial networks;Feature extraction","feature extraction;geophysical image processing;image classification;image segmentation;image texture;learning (artificial intelligence);remote sensing","attention mechanism;VAE-MSGAN;deep learning approaches;interest extraction;remote sensing image processing;unbalanced distribution;positive samples;negative samples;deep learning-based methods;data augmentation method;variational autoencoder-multiscale;spatial attention;channelwise attention;sample distribution;subsequent ROI extraction results;original multispectral information;handcrafted texture features;low-level visual features;realistic RSIs;spatial regions;multiscale discriminator architecture;visual quality;classical generative methods;sensing image generation","","1","","15","IEEE","1 Apr 2021","","","IEEE","IEEE Journals"
"Self-Supervised GANs With Similarity Loss for Remote Sensing Image Scene Classification","D. Guo; Y. Xia; X. Luo","School of Computer and Software, Nanyang Institute of Technology, Nanyang, China; Chongqing Engineering Research Center for Spatial Big Data Intelligent Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Engineering Research Center for Spatial Big Data Intelligent Technology, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","23 Feb 2021","2021","14","","2508","2521","With the development of supervised deep neural networks, classification performance on existing remote sensing scene datasets has been markedly improved. However, supervised learning methods rely heavily on large-scale tagged examples to obtain a better prediction performance. The lack of large-scale tagged remote sensing scene images has become the primary bottleneck in scene classification. To deal with this issue, a novel scene classification method using self-supervised gated self-attention generative adversarial networks (GANs) with similarity loss is proposed. Specifically, the gated self-attention module is first introduced into GANs to focus on key scene areas and filter useless information for strengthening feature representations. Then, the pyramidal convolution block is introduced into the residual block of the discriminator to capture different levels of details in the image using different types of filters with varying sizes and depths for enhancing the feature representations of the discriminator. Additionally, a novel similarity loss item is integrated into the discriminator to leverage self-supervised learning. Besides, spectral normalization is introduced into both the generative network and discriminative network to stabilize training and enhance feature representations. The architecture of multilevel feature fusion is integrated into the discriminative network to achieve more discriminant representation. Experimental results on the AID and NWPU-RESISC45 datasets show that the proposed method achieves the best performance compared to the existing unsupervised classification methods.","2151-1535","","10.1109/JSTARS.2021.3056883","National Natural Science Foundation of China(grant numbers:41 871 226,41 571 401); Special Foundation of Postdoctoral Scientific Research Project of Chongqing(grant numbers:Xm2016081); Science and Technology Research Project of Henan Province(grant numbers:212102210492); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345971","Gated self-attention (GAS) module;generative adversarial networks (GANs);pyramidal convolution (PyConv);remote sensing image scene classification;self-supervised learning;similarity loss;spectral normalization (SN)","Remote sensing;Logic gates;Gallium nitride;Training;Convolution;Generators;Task analysis","convolution;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);neural nets;remote sensing","self-supervised GANs;remote sensing image scene classification;supervised deep neural networks;large-scale tagged examples;large-scale tagged remote sensing scene images;scene classification method;self-supervised gated self-attention generative adversarial networks;gated self-attention module;information filtering;feature representations;pyramidal convolution block;multilevel feature fusion;discriminant representation;unsupervised classification methods;self-supervised learning","","18","","30","CCBYNCND","3 Feb 2021","","","IEEE","IEEE Journals"
"AttentionFGAN: Infrared and Visible Image Fusion Using Attention-Based Generative Adversarial Networks","J. Li; H. Huo; C. Li; R. Wang; Q. Feng","Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Remote Sensing Center of Public Security, People's Public Security University of China, Beijing, China","IEEE Transactions on Multimedia","26 Apr 2021","2021","23","","1383","1396","Infrared and visible image fusion aims to describe the same scene from different aspects by combining complementary information of multi-modality images. The existing Generative adversarial networks (GAN) based infrared and visible image fusion methods cannot perceive the most discriminative regions, and hence fail to highlight the typical parts existing in infrared and visible images. To this end, we integrate multi-scale attention mechanism into both generator and discriminator of GAN to fuse infrared and visible images (AttentionFGAN). The multi-scale attention mechanism aims to not only capture comprehensive spatial information to help generator focus on the foreground target information of infrared image and background detail information of visible image, but also constrain the discriminators focus more on the attention regions rather than the whole input image. The generator of AttentionFGAN consists of two multi-scale attention networks and an image fusion network. Two multi-scale attention networks capture the attention maps of infrared and visible images respectively, so that the fusion network can reconstruct the fused image by paying more attention to the typical regions of source images. Besides, two discriminators are adopted to force the fused result keep more intensity and texture information from infrared and visible image respectively. Moreover, to keep more information of attention region from source images, an attention loss function is designed. Finally, the ablation experiments illustrate the effectiveness of the key parts of our method, and extensive qualitative and quantitative experiments on three public datasets demonstrate the advantages and effectiveness of AttentionFGAN compared with the other state-of-the-art methods.","1941-0077","","10.1109/TMM.2020.2997127","Key Program of High-Resolution Earth Observation System(grant numbers:GFZX0404130307); National Natural Science Foundation of China(grant numbers:41901350); Fundamental Research Funds of People's Public Security University of China(grant numbers:2019JKF330); Fundamental Research Funds for the Central Universities(grant numbers:JZ2019HGBZ0151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103116","Attention mechanism;generative adversarial networks;infrared and visible image fusion","Image fusion;Generative adversarial networks;Feature extraction;Transforms;Generators;Gallium nitride;Fuses","image fusion;image texture;infrared imaging;neural nets;object detection","AttentionFGAN;multiscale attention networks;attention region;infrared image fusion;multiscale attention mechanism;visible image fusion;generative adversarial networks;multimodality images","","61","","47","IEEE","28 May 2020","","","IEEE","IEEE Journals"
"Inpainting of Remote Sensing SST Images With Deep Convolutional Generative Adversarial Network","J. Dong; R. Yin; X. Sun; Q. Li; Y. Yang; X. Qin","Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Technology, Ocean University of China, Qingdao, China; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapoils, MN, USA","IEEE Geoscience and Remote Sensing Letters","21 Jan 2019","2019","16","2","173","177","Cloud occlusion is a common problem in the satellite remote sensing (RS) field and poses great challenges for image processing and object detection. Most existing methods for cloud occlusion recovery extract the surrounding information from the single corrupted image rather than the historical RS image records. Moreover, the existing algorithms can only handle small and regular-shaped obnubilation regions. This letter introduces a deep convolutional generative adversarial network to recover the RS sea surface temperature images with cloud occlusion from the big historical image records. We propose a new loss function for the inpainting network, which adds a supervision term to solve our specific problem. Given a trained generative model, we search for the closest encoding of the corrupted image in the low-dimensional space using our inpainting loss function. This encoding is then passed through the generative model to infer the missing content. We conduct experiments on the RS image data set from the national oceanic and atmospheric administration. Compared with traditional and machine learning methods, both qualitative and quantitative results show that our method has advantages over existing methods.","1558-0571","","10.1109/LGRS.2018.2870880","National Natural Science Foundation of China(grant numbers:41576011,41741007); Key Research and Development Program of Shandong Province(grant numbers:GG201703140154); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8480867","Cloud occlusion images;deep convolutional generative adversarial network (DCGAN);inpainting;sea surface temperature (SST) images","Generative adversarial networks;Ocean temperature;Gallium nitride;Clouds;Land surface temperature;Image color analysis;Generators","geophysical image processing;geophysical signal processing;image processing;learning (artificial intelligence);object detection;ocean temperature;oceanographic techniques;remote sensing","deep convolutional generative adversarial network;common problem;satellite remote sensing field;image processing;object detection;cloud occlusion recovery;single corrupted image;historical RS image records;existing algorithms;regular-shaped obnubilation regions;RS sea surface temperature images;big historical image records;inpainting network;trained generative model;inpainting loss function;RS image data;remote sensing SST images","","39","","18","IEEE","3 Oct 2018","","","IEEE","IEEE Journals"
"Content-Invariant Dual Learning for Change Detection in Remote Sensing Images","B. Fang; G. Chen; G. Ouyang; J. Chen; R. Kou; L. Wang","Key Laboratory of Geological Survey and Evaluation of Ministry of Education, China University of Geosciences, Wuhan, China; Key Laboratory of Geological Survey and Evaluation of Ministry of Education, China University of Geosciences, Wuhan, China; Key Laboratory of Geological Survey and Evaluation of Ministry of Education, China University of Geosciences, Wuhan, China; College of Marine Science and Technology, China University of Geosciences, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","13 Dec 2021","2022","60","","1","17","With the introduction of artificial intelligence (AI), current deep learning-driven change detection methods generally regard changes as a type of specific land cover information and try to detect them simply by using existing semantic labeling models. In practice, the changes that occur on different land cover categories may appear completely different for remote sensing images, making it difficult to detect multiple categories of changes with an end-to-end model. In addition, training such networks requires a large amount of prelabeled references, which are labor-intensive and time-consuming. Motivated by this observation, we integrate dual learning algorithm and disentangled representation theory to develop a novel approach, named content-invariant dual learning (CiDL), for either supervised or unsupervised change detection in remote sensing images. In our framework, two opposite Y-shaped networks, each of which consists of two encoders and one decoder, are introduced to translate bitemporal images from their original domains to each others, where their intrinsic content features are retained, while their style features are consistent. By training our hybrid framework, even without references, this method learns a category-wise cross-domain translation to suppress the discrepancies in paired unchanged regions, and meanwhile highlight those in paired changed regions. The experimental results on two typical change detection data sets and the comparison with other state-of-the-art deep learning-driven methods verify the effectiveness and competitiveness of our proposed CiDL.","1558-0644","","10.1109/TGRS.2021.3064501","National Natural Science Foundation of China(grant numbers:41674015,41925007,U1711266); Scientific Research Project of Hubei Province(grant numbers:1232039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387458","Change detection;disentangled representation;dual learning;generative adversarial networks (GAN);remote sensing","Remote sensing;Geology;Feature extraction;Task analysis;Generative adversarial networks;Training;Redundancy","","","","2","","61","IEEE","26 Mar 2021","","","IEEE","IEEE Journals"
"Attention GANs: Unsupervised Deep Feature Learning for Aerial Scene Classification","Y. Yu; X. Li; F. Liu","Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","27 Dec 2019","2020","58","1","519","531","With the development of deep learning, supervised feature learning methods have achieved prominent performance in the field of aerial scene classification. However, supervised feature learning methods require a large amount of labeled training data. To address this limitation, in this article, a novel unsupervised deep feature learning method, namely, Attention generative adversarial networks (Attention GANs), is proposed for aerial scene classification. First, Attention GANs integrates the attention mechanism into GANs to enhance the representation power of the discriminator. Then, to obtain contextual information, a context-aggregation-based feature fusion architecture is designed in the discriminator. Furthermore, the generator and discriminator losses are improved on basis of the Relativistic GAN. At the same time, a content loss is formed by using the feature representations from the context-aggregation-based feature fusion architecture. In the experiments, our Attention GANs is evaluated via comprehensive experiments with four publicly available remote sensing scene data sets, i.e., the UC-Merced data set with 21 scene classes, the RSSCN7 data set with 7 scene classes, the AID data set with 30 scene classes, and the NWPU-RESISC45 data set with 45 scene classes. Experimental results demonstrate that our Attention GANs can obtain the best performance compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2019.2937830","National Natural Science Foundation of China(grant numbers:71771216,71701209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842616","Aerial scene classification;attention mechanism;context aggregation;generative adversarial networks (GANs);unsupervised deep feature learning","Learning systems;Gallium nitride;Feature extraction;Generators;Task analysis;Remote sensing;Generative adversarial networks","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);remote sensing;unsupervised learning","AID data set;RSSCN7 data set;NWPU-RESISC45 data set;UC-Merced data set;attention GAN;scene classes;attention generative adversarial networks;deep feature learning method;learning methods;aerial scene classification;unsupervised deep feature learning;publicly available remote sensing scene data sets;feature representations;relativistic GAN;context-aggregation-based;attention mechanism","","62","","77","IEEE","17 Sep 2019","","","IEEE","IEEE Journals"
"Remote Sensing Fine-Grained Ship Data Augmentation Pipeline With Local-Aware Progressive Image-to-Image Translation","B. Liu; L. Li; Q. Xiao; W. Ni; Z. Yang","School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Electronics and Information Technology for Space System, National Space Science Center, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Electronics and Information Technology for Space System, National Space Science Center, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Electronics and Information Technology for Space System, National Space Science Center, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","21 Oct 2022","2022","60","","1","16","Remote sensing image ship fine-grained classification is a challenging vision problem due to factors, such as interclass similarity, real-world image scarcity, and class imbalance. Data augmentation aims to solve these problems from the data perspective. Most of these methods cannot work effectively in complex scenes, which restricts their practical application. Here, we propose a novel data augmentation pipeline based on local-aware image translation to achieve the representation mapping between cross-domain corresponding instances. Our pipeline contains three modules: imaging simulation system (ISS), local-aware progressive image-to-image translation (LoPIT), and image harmonization (IH) modules. The ISS module generates simulated images with correct appearance and diverse features based on the input requirement information. To tackle the domain gap between simulated image and real-world image, we propose the local-aware CycleGAN in the LoPIT module to achieve mapping based on local-aware learning and apply two submodules to progressively complete the remote sensing image global cross-domain translation. The IH module uses IH technology to coordinate the visual appearance between the foreground and background to generate sufficient remote sensing ship images with precise representation, photorealistic style, and harmonious features. Moreover, we present a mixed dataset including real-world images and our synthetic images for remote sensing image fine-grained ship classification. Our dataset named RSSA-12 contains 12 categories of ship targets in 3831 images with high-quality annotated category labels, effectively alleviating the long-tail problem of existing datasets. Experimental results demonstrate that our progressive pipeline outperforms the state-of-the-art data augmentation method on the remote sensing fine-grained ship classification task.","1558-0644","","10.1109/TGRS.2022.3211517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9908572","Fine-grained ship classification;image harmonization (IH);image-to-image (I2I) translation;imaging simulation;remote sensing","Remote sensing;Marine vehicles;Pipelines;Imaging;Data models;Task analysis;Generative adversarial networks","computer vision;feature extraction;geophysical image processing;image classification;image segmentation;learning (artificial intelligence);object detection;object recognition;oceanographic techniques;remote sensing;ships","remote sensing fine-grained ship classification task;state-of-the-art data augmentation method;remote sensing image fine-grained ship classification;synthetic images;real-world images;sufficient remote sensing ship images;remote sensing image global cross-domain translation;local-aware learning;local-aware CycleGAN;simulated image;image harmonization modules;simulation system, local-aware;local-aware image translation;novel data augmentation pipeline;real-world image scarcity;sensing image ship fine-grained classification;image-to-image translation;fine-grained ship data augmentation pipeline","","","","63","IEEE","3 Oct 2022","","","IEEE","IEEE Journals"
"GAN-Based One-Class Classification for Remote-Sensing Image Change Detection","P. Jian; K. Chen; W. Cheng","Beijing Engineering Applications Research Center of High Volume Language Information Processing and Cloud Computing, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Beijing Engineering Applications Research Center of High Volume Language Information Processing and Cloud Computing, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Currently, most of the supervised change detection approaches require a training data set that contains samples from both the changed and the unchanged data. However, under certain condition, such as natural disaster and military attack, the changed data samples are very few or even not available but the unchanged data are abundant. In this letter, we develop a generative adversarial networks (GANs)-based one-class classification (OCC) technique for time series remote-sensing image change detection. The proposed method is only trained with the unchanged data instead of both the changed and unchanged data. To achieve this purpose, first, spatial-spectral features are extracted from the time series remote-sensing images. Second, a GAN model is trained to detect the changes only with the extracted features of unchanged data. Remarkably, to offset the outlier errors caused by the incomplete supervision information provided by unchanged data alone, changed data, instead of unchanged data, are generated to improve power of discriminator. Finally, testing data are classified by the trained discriminator of GAN to produce a binary change map. Experimental results obtained on two optical time series remote-sensing data sets confirmed the effectiveness of our proposed method.","1558-0571","","10.1109/LGRS.2021.3066435","National Key Research and Development Program of China(grant numbers:2017YFB1002103); Open Project Program of the National Defense Key Laboratory of Electronic Information Equipment System Research(grant numbers:DXZT-JC-ZZ-2017-009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9394409","Change detection;generative adversarial networks (GANs);one-class classification (OCC);time series","Feature extraction;Remote sensing;Training data;Time series analysis;Training;Gallium nitride;Generators","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);remote sensing;terrain mapping;time series","GAN-based one-class classification;supervised change detection approaches;training data set;unchanged data;changed data samples;generative adversarial networks-based one-class classification;time series remote-sensing image change detection;time series remote-sensing images;testing data;binary change map;optical time series remote-sensing data sets","","3","","20","IEEE","2 Apr 2021","","","IEEE","IEEE Journals"
"Airborne Snow Radar Data Simulation With Deep Learning and Physics-Driven Methods","M. Yari; O. Ibikunle; D. Varshney; T. Chowdhury; A. Sarkar; J. Paden; J. Li; M. Rahnemoonfar","Computer Vision and Remote Sensing Laboratory (Bina Lab), University of Maryland, Baltimore County, MD, USA; University of Kansas, Lawrence, KS, USA; Computer Vision and Remote Sensing Laboratory (Bina Lab), University of Maryland, Baltimore County, MD, USA; Computer Vision and Remote Sensing Laboratory (Bina Lab), University of Maryland, Baltimore County, MD, USA; Computer Vision and Remote Sensing Laboratory (Bina Lab), University of Maryland, Baltimore County, MD, USA; University of Kansas, Lawrence, KS, USA; University of Kansas, Lawrence, KS, USA; Computer Vision and Remote Sensing Laboratory (Bina Lab), University of Maryland, Baltimore County, MD, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Dec 2021","2021","14","","12035","12047","Monitoring properties of ice sheets in polar regions is one of the main challenges in glaciology. There is a large amount of heterogeneous radar data from the polar regions that have been gathered through expensive missions. However, retrieving meaningful information from this large volume of data is still a great challenge. With the advancement of machine learning techniques in recent years, many scientists are eager to take advantage of these algorithms and techniques to explore and mine Arctic and Antarctic data. These advancements, however, have happened mainly in the area of supervised learning where the models are data hungry and require large amounts of annotated data. Generating simulated data can be an effective and inexpensive approach to provide large labeled datasets for training machine learning models. In this work, we explore two approaches to simulate arctic snow radar echogram images, namely a radar scattering physics based approach combined with some statistical measures and a purely data-driven approach based on a conditional generative adversarial network. Using several image comparison metrics, we analyze the utility of both methods for the purpose of simulating echograms. Our results show that the physics simulator generates images with good structural similarities, while the purely data-driven approach achieves better textural similarities for simulated image. Finally, we also show that by augmenting our real dataset by the simulated echograms, we can improve our deep learning model for tracking internal layers of snow.","2151-1535","","10.1109/JSTARS.2021.3126547","NSF BIGDATA(grant numbers:IIS-1838230,IIS-1947584); NSF HDR Institute(grant numbers:OAC-2118285); IBM; Amazon Catalyst; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622135","Generative adversarial networks (GANs);remote sensing;simulation;snow radar","Snow;Radar;Generative adversarial networks;Radar imaging;Radar tracking;Data models;Radar remote sensing","airborne radar;geophysical image processing;glaciology;hydrological techniques;learning (artificial intelligence);radar imaging;remote sensing by radar;snow","airborne snow radar data simulation;physics-driven methods;monitoring properties;ice sheets;polar regions;heterogeneous radar data;Antarctic data;supervised learning;annotated data;effective approach;inexpensive approach;training machine learning models;arctic snow radar echogram images;radar scattering physics;purely data-driven approach;conditional generative adversarial network;simulating echograms;physics simulator;simulated image;simulated echograms;deep learning model","","","","48","CCBY","19 Nov 2021","","","IEEE","IEEE Journals"
"Intelligent Matching Method for Heterogeneous Remote Sensing Images Based on Style Transfer","J. Zhao; D. Yang; Y. Li; P. Xiao; J. Yang","Department of Automatic Control, Xi'an Research Institute of Hi-tech, Xi'an, China; Department of Automatic Control, Xi'an Research Institute of Hi-tech, Xi'an, China; Department of Automatic Control, Xi'an Research Institute of Hi-tech, Xi'an, China; Department of Automatic Control, Xi'an Research Institute of Hi-tech, Xi'an, China; Department of Automatic Control, Xi'an Research Institute of Hi-tech, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Aug 2022","2022","15","","6723","6731","Intelligent matching of heterogeneous remote sensing images is a common basic problem in the field of intelligent remote sensing image processing. Aiming at the difficulty of matching satellite-aerial remote sensing images, this article proposes an intelligent matching method for heterogeneous remote sensing images based on style transfer. First, based on the idea of image style transfer of a generative adversarial networks, this method improves the conversion effect of the model on heterogeneous images by constructing a new generative network loss function and converts satellite images into aerial images. Then, the advanced deep learning-based matching algorithms D2-Net and LoFTR are used to achieve matching between the generated aerial image and the original aerial image. Finally, this transformation relationship is mapped to the corresponding satellite–aerial image pair to obtain the final matching result. The image style transfer experiments and the matching experiments we carry out under different test datasets show that the smooth cycle-consistent generative adversarial networks proposed in this article can effectively reduce the complexity of the algorithm and improve the quality of image generation. In addition, combining it with deep learning-based feature-matching methods can effectively improve the accuracy and robustness of the matching algorithm. Our code and data can be found at: https://gitee.com/AZQZ/intelligent-matching.","2151-1535","","10.1109/JSTARS.2022.3197748","National Natural Science Foundation of China(grant numbers:61673017); Natural Science Foundation of Shaanxi Province(grant numbers:2021JQ-702,2019JM-434); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854096","Heterogeneous remote sensing images;image style transfer;intelligent matching","Remote sensing;Generative adversarial networks;Feature extraction;Satellites;Generators;Image matching;Training","feature extraction;geophysical image processing;image matching;image processing;learning (artificial intelligence);remote sensing","corresponding satellite-aerial image pair;final matching result;image style;image generation;deep learning-based feature-matching methods;matching algorithm;intelligent matching method;heterogeneous remote sensing images;style transfer;intelligent remote sensing image processing;satellite-aerial remote sensing images;heterogeneous images;satellite images;aerial images;generated aerial image;original aerial image","","","","32","CCBY","10 Aug 2022","","","IEEE","IEEE Journals"
"Research on Change Detection Method of High-Resolution Remote Sensing Images Based on Subpixel Convolution","X. Luo; X. Li; Y. Wu; W. Hou; M. Wang; Y. Jin; W. Xu","School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang, China; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2021","2021","14","","1447","1457","Remote sensing image change detection method plays a great role in land cover research, disaster assessment, medical diagnosis, video surveillance, and other fields, so it has attracted wide attention. Based on a small sample dataset from SZTAKI AirChange Benchmark Set, in order to solve the problem that the deep learning network needs a large number of samples, this work first uses nongenerative sample augmentation method and generative sample augmentation method based on deep convolutional generative adversarial networks, and then, constructs a remote sensing image change detection model based on an improved DeepLabv3+ network. This model can realize end-to-end training and prediction of remote sensing image change detection with subpixel convolution. Finally, Landsat 8, Google Earth, and Onera satellite change detection datasets are used to verify the generalization performance of this network. The experimental results show that the improved network accuracy is 95.1% and the generalization performance is acceptable.","2151-1535","","10.1109/JSTARS.2020.3044060","Science and Technology Program of Sichuan(grant numbers:2017GZ0327); Science and Technology Program of Hebei(grant numbers:20355901D); Science and Technology Program of Hebei(grant numbers:19255901D); National Defense Science and Technology; Remote Sensing Information and Image Analysis Technology of China(grant numbers:6142A010301); Chinese Air-Force Equipment Pre-Research Project(grant numbers:10305***02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9291461","Change detection;DeepLabv3+;deep convolutional generative adversarial networks (DCGAN);deep learning;subpixel convolution","Remote sensing;Training;Deep learning;Sensors;Feature extraction;Convolution;Convolutional neural networks","deep learning (artificial intelligence);disasters;geophysical image processing;image classification;image resolution;remote sensing;video surveillance","deep learning network;deep convolutional generative adversarial networks;remote sensing image change detection model;DeepLabv3+ network;end-to-end training;subpixel convolution;Onera satellite change detection datasets;network accuracy;high-resolution remote sensing images;sensing image change detection method;land cover research;disaster assessment;medical diagnosis;video surveillance;sample dataset;SZTAKI AirChange Benchmark Set;generative sample augmentation;nongenerative sample augmentation","","9","","44","CCBY","11 Dec 2020","","","IEEE","IEEE Journals"
"Supporting Aquaculture in the Chesapeake Bay Using Artificial Intelligence to Detect Poor Water Quality with Remote Sensing","S. Schollaert Uz; T. J. Ames; N. Memarsadeghi; S. M. McDonnell; N. V. Blough; A. V. Mehta; J. R. McKay","NASA Goddard Space Flight Center, Greenbelt, MD; NASA Goddard Space Flight Center, Greenbelt, MD; NASA Goddard Space Flight Center, Greenbelt, MD; Department of Chemistry, University of Maryland, College Park, MD; Department of Chemistry, University of Maryland, College Park, MD; University of Maryland Baltimore County, Joint Center for Earth Systems Technology, Baltimore, MD; Maryland Department of Environment, Shellfish Monitoring Section, Annapolis, MD","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","3629","3632","Reliable information on water quality is not currently available at the space and time scales that are required for aquaculture and other resource management needs. For example, shellfish growing areas may be impacted by harmful algal blooms or runoff from land that increases turbidity, lowers salinity, or introduces contaminants. Shellfish resource managers in the Chesapeake Bay are especially concerned with sources of bacteria from land such as failing onsite waste systems, failing wastewater infrastructure, and concentrated animal feeding operations. There is an urgent need for remote sensing of water quality indicators beyond chlorophyll-a and suspended sediments to augment field sampling programs. Artificial Intelligence trained with simultaneous in situ and satellite observations is explored in preparation for future hyperspectral satellite missions, which offer potential to detect additional water quality indicators not previously possible. This first step identifies and develops a method to harmonize disparate, unlinked aquatic datasets to derive information about where water quality is likely degraded.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323465","University of Maryland; NASA Earth Science Division(grant numbers:AIST-18-0007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323465","Artificial Intelligence;machine learning;neural networks;semi-supervised Generative Adversarial Networks;data fusion;aquatic remote sensing;hyperspectral satellite missions;water quality monitoring","Water quality;Satellites;Artificial intelligence;Sensors;Sea measurements;Hyperspectral imaging;Monitoring","aquaculture;artificial intelligence;marine pollution;microorganisms;remote sensing;salinity (geophysical);sediments;water quality","remote sensing;time scales;aquaculture;resource management needs;shellfish growing areas;harmful algal blooms;runoff;turbidity;lowers salinity;shellfish resource managers;Chesapeake Bay;onsite waste systems;wastewater infrastructure;concentrated animal feeding operations;chlorophyll;suspended sediments;field sampling programs;in situ observation;satellite observations;future hyperspectral satellite missions;water quality indicators;artificial Intelligence;contaminants;bacteria","","","","9","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"NIGAN: A Framework for Mountain Road Extraction Integrating Remote Sensing Road-Scene Neighborhood Probability Enhancements and Improved Conditional Generative Adversarial Network","W. Chen; G. Zhou; Z. Liu; X. Li; X. Zheng; L. Wang","Faculty of Computer Science, China University of Geosciences, Wuhan, China; Faculty of Computer Science, China University of Geosciences, Wuhan, China; Faculty of Computer Science, China University of Geosciences, Wuhan, China; Faculty of Computer Science, China University of Geosciences, Wuhan, China; Faculty of Computer Science, China University of Geosciences, Wuhan, China; Faculty of Computer Science, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2022","2022","60","","1","15","Mountain roads are a source of important basic geographic data used in various fields. The automatic extraction of road images through high-resolution remote sensing imagery using deep learning has attracted considerable attention. But the interference of context information limited extraction accuracy, especially for roads in mountain area. Furthermore, when pursuing research in a new district, many algorithms are difficult to train due to a lack of data. To address these issues, a framework based on remote sensing road-scene neighborhood probability enhancement and improved conditional generative adversarial network (NIGAN) is proposed in this article. This framework can be divided into two sections: 1) road scenes classification section. A remote sensing road-scene neighborhood confidence enhancement method was designed for classifying road scenes of the study area to reduce the impact of nonroad information on subsequent fine-road segmentation and 2) fine-road segmentation section. An improved dilated convolution module, which is helpful in extracting small objects such as road, was added into the conditional generative adversarial network (CGAN) to increase the receptive field and pay attention to global information, and segment roads from the results of road scenes classification section. To validate the NIGAN framework, new mountain road-scene and label datasets were constructed, and diverse comparison experiments were performed. The results indicate that the NIGAN framework can improve the integrity and accuracy of mountain road-scene extraction in diverse and complex conditions. The results further confirm the validity of the NIGAN framework in small samples. In addition, the mountain road-scene datasets can serve as benchmark datasets for studying mountain road extraction.","1558-0644","","10.1109/TGRS.2022.3188908","Fundamental Research Funds for the Natural Science Foundation of China(grant numbers:U1803117,41925007,42071430); Department of Natural Resources of Hubei Provincial(grant numbers:ZRZY2021KJ04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9816040","Deep learning;generative adversarial network (GAN);remote sensing;road extraction;scene classification;semantic segmentation;ZiYuan-3","Roads;Feature extraction;Remote sensing;Data mining;Convolution;Generative adversarial networks;Image segmentation","feature extraction;geographic information systems;geophysical image processing;image classification;image resolution;image segmentation;learning (artificial intelligence);probability;remote sensing;traffic engineering computing","road scenes classification section;segment roads;subsequent fine-road segmentation;remote sensing road-scene neighborhood confidence enhancement method;remote sensing road-scene neighborhood probability enhancement;mountain area;context information limited extraction accuracy;high-resolution remote sensing imagery;road images;important basic geographic data;mountain roads;conditional generative adversarial network;mountain road extraction integrating remote sensing road-scene neighborhood probability enhancements;mountain road-scene datasets;NIGAN framework;mountain road-scene extraction","","3","","83","IEEE","6 Jul 2022","","","IEEE","IEEE Journals"
"MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification","D. Lin; K. Fu; Y. Wang; G. Xu; X. Sun","Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","25 Oct 2017","2017","14","11","2092","2096","With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks. However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model G and a discriminative model D. We treat D as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. G can produce numerous images that are similar to the training data; therefore, D can learn better representations of remotely sensed images using the training data provided by G. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.","1558-0571","","10.1109/LGRS.2017.2752750","National Natural Science Foundation of China(grant numbers:41501485,61331017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8059820","Generative adversarial networks (GANs);scene classification;unsupervised representation learning","Generators;Gallium nitride;Training;Remote sensing;Feature extraction;Training data;Computational modeling","feature extraction;geophysical image processing;image classification;image fusion;image matching;image representation;remote sensing;unsupervised learning","global features;training data;unsupervised representation;remote sensing image classification;deep learning;supervised learning;convolutional networks;unsupervised model;multiple-layer feature-matching generative adversarial networks;unlabeled data;generative model G;discriminative model D;feature extractor;remote sensing data;remote sensing image databases;MARTA GAN;fusion layer","","131","","15","IEEE","5 Oct 2017","","","IEEE","IEEE Journals"
"Change Detection of High-Resolution Remote Sensing Image Based on Semi-Supervised Segmentation and Adversarial Learning","S. Yang; S. Hou; Y. Zhang; H. Wang; X. Ma","Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; Faculty of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1055","1058","Change detection, which gives a quantitative analysis of the change information for the target area, is an important technology for lots of remote sensing tasks. Supervised change detection methods can achieve satisfactory performance when there are enough labeled samples, which is a harsh requirement for change detection tasks using remote sensing images. To solve this problem, we propose a change detection method based on semi-supervised segmentation and adversarial learning. The proposed method can learn knowledge from both the limited labeled samples and the abundant unlabeled samples to improve the generalization performance of the model. Firstly, the segmentation maps of both labeled samples and unlabeled samples are obtained through a segmentation network. Then, the labeled samples is used to train the discriminator network, which is responsible for distinguishing the segmentation prediction from the ground truth. Finally, the discriminator output is used as a measurement for self-training to minimize the feature difference between the segmentation prediction and the ground truth. Experimental results on the Sun Yat-Sen University (SYSU) dataset show that the proposed method can use unlabeled samples to improve the quality of prediction, which guarantees the proposed method can achieve good performance with few labeled samples.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884552","National Natural Science Foundation of China(grant numbers:61801078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884552","high resolution image;change detection;semi-supervised learning","Training;Deep learning;Image segmentation;Image resolution;Statistical analysis;Feature extraction;Generative adversarial networks","geophysical image processing;image classification;image segmentation;learning (artificial intelligence);remote sensing","high-resolution remote sensing image;semisupervised segmentation;adversarial learning;change information;remote sensing tasks;supervised change detection methods;labeled samples;change detection tasks;remote sensing images;change detection method;abundant unlabeled samples;segmentation maps;segmentation network;segmentation prediction","","","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Data Augmentation for Land Cover Classification Using Generative Adversarial Networks","A. Kamel; B. Issam","Algerian Space Agency, Space Techniques Center, Arzew, Oran, Algeria; Algerian Space Agency, Space Techniques Center, Arzew, Oran, Algeria","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2309","2312","In machine learning, few shot learning techniques try to generate high prediction accuracy from few training examples. One of those techniques is data augmentation. In this paper, we propose a method for land cover image classification using a conditional generative adversarial network (cGAN) starting from few training samples. First, we train a cGAN with a small dataset to classify images by mapping each pixel of the input image to its class. Second, we augment the images of the dataset by generating new samples using a traditional GAN model, then the labels of the new samples are generated using the previously trained cGAN model. The new augmented labeled data is associated with the original dataset to train the cGAN model again with more samples to improve the classification accuracy. Training experiments were performed on SPARCS publicly available dataset. We tested the proposed method on images of Landsat 8 and Alsat 2 satellites. The classification results show that the performance after data augmentation is better than using the original small dataset.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553374","land cover classification;data augmentation;few shot learning;GANs","Training;Measurement;Satellites;Image color analysis;Machine learning;Generative adversarial networks;Labeling","geophysical image processing;image classification;land cover;learning (artificial intelligence);neural nets","few shot learning;prediction accuracy;data augmentation;land cover image classification;conditional generative adversarial network;cGAN;SPARCS publicly available dataset;machine learning;image augmentation;Alsat 2 satellites;Landsat 8 satellites","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Synthetic Glacier SAR Image Generation from Arbitrary Masks Using Pix2Pix Algorithm","R. Dietrich-Sussner; A. Davari; T. Seehaus; M. Braun; V. Christlein; A. Maier; C. Riess","Department of Computer Science, Friendrich-Alexander University Erlangen, Nürnberg, Germany; Department of Computer Science, Friendrich-Alexander University Erlangen, Nürnberg, Germany; Department of Geography & Geosciences, Friedrich-Alexander University Erlangen, Nürnberg, Germany; Department of Geography & Geosciences, Friedrich-Alexander University Erlangen, Nürnberg, Germany; Department of Computer Science, Friendrich-Alexander University Erlangen, Nürnberg, Germany; Department of Computer Science, Friendrich-Alexander University Erlangen, Nürnberg, Germany; Department of Computer Science, Friendrich-Alexander University Erlangen, Nürnberg, Germany","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4548","4551","Supervised machine learning requires a large amount of labeled data to achieve proper test results. However, generating accurately labeled segmentation maps on remote sensing imagery, including images from synthetic aperture radar (SAR), is tedious and highly subjective. In this work, we propose to alleviate the issue of limited training data by generating synthetic SAR images with the pix2pix algorithm [1]. This algorithm uses conditional Generative Adversarial Networks (cGANs) to generate an artificial image while preserving the structure of the input. In our case, the input is a segmentation mask, from which a corresponding synthetic SAR image is generated. We present different models, perform a comparative study and demonstrate that this approach synthesizes convincing glaciers in SAR images with promising qualitative and quantitative results.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553853","image-to-image translation;conditional GAN;limited training data;synthetic data","Training;Image segmentation;Training data;Machine learning;Generative adversarial networks;Radar polarimetry;Synthetic aperture radar","image segmentation;neural nets;radar computing;radar imaging;remote sensing;supervised learning;synthetic aperture radar","synthetic aperture radar;synthetic SAR images;pix2pix algorithm;conditional Generative Adversarial Networks;artificial image;segmentation mask;synthetic glacier SAR image generation;arbitrary masks;supervised machine learning;remote sensing imagery;labeled segmentation maps","","1","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"An Efficient Seismic Data Acquisition Based on Compressed Sensing Architecture With Generative Adversarial Networks","X. Zhang; S. Zhang; J. Lin; F. Sun; X. Zhu; Y. Yang; X. Tong; H. Yang","College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China; College of Instrumentation and Electrical Engineering, Jilin University, Changchun, China; Key Laboratory of Geophysical Exploration Equipment, Ministry of Education, Jilin University, Changchun, China; Key Laboratory of Geophysical Exploration Equipment, Ministry of Education, Jilin University, Changchun, China; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Sydney, NSW, Australia; Key Laboratory of Geophysical Exploration Equipment, Ministry of Education, Jilin University, Changchun, China; Key Laboratory of Geophysical Exploration Equipment, Ministry of Education, Jilin University, Changchun, China","IEEE Access","12 Aug 2019","2019","7","","105948","105961","Recently, large scale seismic data acquisition has been a critical method for scientific research and industrial production. However, due to the bottleneck on data transmission and the limitation of energy storage, it is hard to conduct large seismic data acquisition in a real-time way. So, in this paper, an efficient seismic data acquisition method, namely, compressed sensing architecture with generative adversarial networks (CSA-GAN), is proposed to tackle the two restrictions of collecting large scale seismic data. In the CSA-GAN, a data collection architecture based on compressed sensing theory is applied to reduce data traffic load of the whole system, as well as balance the data transmission. To make the compressed sensing procedure perform well in both data quality and compression ratio, a kind of generative adversarial networks is designed to learn the recovering map. According to our experiment results, a high data quality (about 30 dB) at the compression ratio of 16 is achieved by the proposed approach, which enables the system to afford 15 times more sensors and reduces the energy cost by means of data collection from N(N + 1)/2 to N2/16. These results show that the CSA-GAN can afford more sensors with the same bandwidth and consume less energy, via improving the efficiency seismic data acquisition.","2169-3536","","10.1109/ACCESS.2019.2932476","National Natural Science Foundation of China(grant numbers:41804167); National Key Research and Development Program of China(grant numbers:2018YFC0603204,2018YFB1501803); Jilin University(grant numbers:10183201836); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784224","Compressed sensing (CS);generative adversarial networks (GAN);dictionary learning;seismic data acquisition","Data acquisition;Compressed sensing;Generative adversarial networks;Sensor systems;Seismology;Dictionaries","compressed sensing;data acquisition;sensors","compressed sensing architecture;generative adversarial networks;data transmission;CSA-GAN;data collection architecture;data traffic load;data quality;energy storage;efficient large scale seismic data acquisition method","","9","","38","CCBY","1 Aug 2019","","","IEEE","IEEE Journals"
"Multiattention Generative Adversarial Network for Remote Sensing Image Super-Resolution","S. Jia; Z. Wang; Q. Li; X. Jia; M. Xu","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","14 Jun 2022","2022","60","","1","15","Image super-resolution (SR) methods can generate remote sensing images with high spatial resolution without increasing the cost of acquisition equipment, thereby providing a feasible way to improve the quality of remote sensing images. Clearly, image SR is a severe ill-posed problem. With the development of deep learning, the powerful fitting ability of deep neural networks has solved this problem to some extent. Since the texture information of various remote sensing images are totally different from each other, in this article, we proposed a network based on generative adversarial network (GAN) to achieve high-resolution remote sensing images, named multiattention GAN (MA-GAN). The main body of the generator in MA-GAN contains three blocks: pyramid convolutional residual dense (PCRD) block, attention-based upsampling (AUP) block, and attention-based fusion (AF) block. Specifically, the developed attention pyramid convolutional (AttPConv) operator in the PCRD block combines multiscale convolution and channel attention (CA) to automatically learn and adjust the scale of residuals for better representation. The established AUP block uses pixel attention (PA) to perform arbitrary scales of upsampling. The AF block uses branch attention (BA) to integrate upsampled low-resolution images with high-level features. Besides, the loss function takes both adversarial loss and feature loss into consideration to guide the learning procedure of the generator. We have compared our MA-GAN approach with several state-of-the-art methods on a number of remote sensing scenes, and the experimental results consistently demonstrate the effectiveness of the proposed MA-GAN. For study replication, the source code will be released at: https://github.com/ZhihaoWang1997/MA-GAN.","1558-0644","","10.1109/TGRS.2022.3180068","National Natural Science Foundation of China(grant numbers:41971300,61901278); Key Project of Department of Education of Guangdong Province(grant numbers:2020ZDZX3045); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515011290); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011413); Shenzhen Scientific Research and Development Funding Program(grant numbers:20200803152531004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787539","Generative adversarial network (GAN);remote sensing image;super-resolution (SR)","Remote sensing;Generators;Convolution;Generative adversarial networks;Task analysis;Spatial resolution;Interpolation","convolution;geophysical image processing;image resolution;image texture;learning (artificial intelligence);neural nets;remote sensing","multiattention generative adversarial network;remote sensing image super-resolution;super-resolution methods;high spatial resolution;image SR;deep neural networks;high-resolution remote sensing images;named multiattention GAN;MA-GAN;attention-based fusion;developed attention pyramid convolutional operator;multiscale convolution;channel attention;established AUP block;low-resolution images;remote sensing scenes","","4","","60","IEEE","3 Jun 2022","","","IEEE","IEEE Journals"
"Road Detection From Remote Sensing Images by Generative Adversarial Networks","Q. Shi; X. Liu; X. Li","Guangdong Key Laboratory for Urbanization and Geo-Simulation, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory for Urbanization and Geo-Simulation, Sun Yat-sen University, Guangzhou, China; School of Geographic Sciences, Key Laboratory of Geographic Information Science, Ministry of Education, East China Normal University, Shanghai, China","IEEE Access","24 May 2018","2018","6","","25486","25494","Road detection with high-precision from very high resolution remote sensing imagery is very important in a huge variety of applications. However, most existing approaches do not automatically extract the road with a smooth appearance and accurate boundaries. To address this problem, we proposed a novel end-to-end generative adversarial network. In particular, we construct a convolutional network based on adversarial training that could discriminate between segmentation maps coming either from the ground truth or generated by the segmentation model. The proposed method could improve the segmentation result by finding and correcting the difference between ground truth and result output by the segmentation model. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art methods greatly on the performance of segmentation map.","2169-3536","","10.1109/ACCESS.2017.2773142","National Key Research and Development Program of China(grant numbers:2017YFA0604402); China Post-Doctoral Science Foundation(grant numbers:2015M580753); National Science Foundation of China(grant numbers:61601522); Key National Natural Science Foundation of China(grant numbers:41531176); National Key Research and Development Program of China(grant numbers:2017YFB1001703); Fundamental Research Funds for the Central Universities(grant numbers:17lgjc40); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8106771","Generative adversarial networks;end-to-end learning;road detection","Roads;Gallium nitride;Feature extraction;Image segmentation;Remote sensing;Training;Data models","artificial intelligence;convolution;geophysical image processing;image resolution;image segmentation;remote sensing;roads","end-to-end generative adversarial network;ground truth;segmentation map;adversarial training;convolutional network;high resolution remote sensing imagery;remote sensing images;road detection","","61","","43","OAPA","13 Nov 2017","","","IEEE","IEEE Journals"
"Skip Attention GAN for Remote Sensing Image Synthesis","K. Deng; K. Zhang; P. Yao; S. Cheng; P. He","Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","2305","2309","High-quality remote sensing images are difficult to obtain due to limited conditions and high cost for data acquisition. With the development of machine vision and deep learning, some image generation methods (e.g., GANs) are introduced into this field, but it’s still hard to generate images with good texture details and structural dependencies. We establish Skip Attention Mechanism to deal with this problem, which learns dependencies between local points on low-resolution feature maps, and then upsample the attention map and combine it with high-resolution feature maps. With this method, long-range dependencies learned from low-resolution are used for generating remote sensing images with more structural details. We name this method as Skip Attention GAN, which is the first method applying cross-scale attention mechanism for unsupervised remote sensing image generation. Experiments show that our method outperforms previous methods under several metrics. Visual and ablation results of attention layers show that Skip Attention has learned long-distance structural dependencies between similar targets.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414701","Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414701","Attention Mechanism;Remote Sensing Image Synthesis;Generative Adversarial Network","Measurement;Deep learning;Visualization;Image synthesis;Machine vision;Generative adversarial networks;Sensors","computer vision;data acquisition;edge detection;geophysical image processing;image classification;image resolution;image texture;learning (artificial intelligence);remote sensing;robot vision","high-quality remote sensing images;data acquisition;machine vision;deep learning;image generation methods;GANs;good texture details;Skip Attention Mechanism;low-resolution feature maps;attention map;high-resolution feature maps;long-range dependencies;Skip Attention GAN;cross-scale attention mechanism;unsupervised remote sensing image generation;attention layers;long-distance structural dependencies;remote sensing image synthesis","","1","","24","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Structure Aware Generative Adversarial Networks for Hyperspectral Image Classification","T. Alipour-Fard; H. Arefi","School of Surveying and Geospatial Engineering, University of Tehran, Tehran, Iran; School of Surveying and Geospatial Engineering, University of Tehran, Tehran, Iran","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Sep 2020","2020","13","","5424","5438","Generative adversarial networks (GANs) have shown striking performances in computer vision applications to augment virtual training samples (VTS). However, the VTS generating by GANs in the context of hyperspectral image classification suffer from structural inconsistency due to the insufficient number of training samples in order to learn high-order features from the discriminator. This work addresses the scarcity of training samples by designing a GAN, in which the performance of discriminator is improved to produce more structurally coherent VTS. In the proposed method, by splitting the discriminator into two parts, GAN undertakes two tasks: the main task is to learn to distinguish between real and fake samples, and the auxiliary task is to learn to distinguish structurally corrupted and real samples. With this setup, GAN will produce real-like VTS with a higher variation than conventional GAN. Furthermore, in order to reduce the computational cost, subspace-based dimension reduction was performed to obtain the dominant features around the training samples to generate meaningful patterns from the original ones to be used in the learning phase. Based on the experimental results on real, and well-known hyperspectral benchmark images, the proposed method improves the performance compared with GANs-related, and conventional data augmentation strategies1.","2151-1535","","10.1109/JSTARS.2020.3022781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187967","Deep learning (DL);hyperspectral images (HSIs);convolutional neural network (CNN);generative adversarial networks (GANs);remote sensing","Training;Gallium nitride;Hyperspectral imaging;Training data;Generators;Task analysis","computer vision;geophysical image processing;image classification;learning (artificial intelligence);neural nets;remote sensing","hyperspectral benchmark images;learning phase;GAN;auxiliary task;structurally coherent VTS;high-order features;structural inconsistency;virtual training samples;computer vision applications;hyperspectral image classification;structure aware generative adversarial networks","","4","","60","CCBY","8 Sep 2020","","","IEEE","IEEE Journals"
"Image Hazing Algorithm Based on Generative Adversarial Networks","J. Xiao; J. Zhou; J. Lei; C. Xu; H. Sui","School of Electrical Information, Wuhan University, Wuhan, China; School of Electrical Information, Wuhan University, Wuhan, China; School of Electrical Information, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Access","28 Jan 2020","2020","8","","15883","15894","Haze is an important factor in photography with a special aesthetic, emotional, or composi- tional meaning, an image hazing method is proposed based on generative adversarial network. The proposed network consists of two parts: the generator has a symmetric encoder and decoder structure with skip connection, which is used to generate hazy images; and the discriminator is a global fully convolutional network, which is used to identify the reality of the generated hazy images. The haze-free image is used as the input of the generative network to obtain a corresponding hazy image. Then the discriminative network judges the similarity between the original haze-free image and the corresponding hazy image to optimize the network parameters, which means that all the parameters of the entire network can be automatically learned through training. The loss function of the network is the combination of the GAN(Generative Adversarial Networks) loss and the REG(Regression) loss with a coefficient $\lambda $ . We design a REG loss which includes feature loss $loss_{FL}$ and image loss $loss_{IL}$ . By calculating the loss, a model mapping the relationship between the two corresponding pixels from the original image and the synthetic hazy image respectively is well trained. In the last part of the paper, we have shown some impressing synthetic hazy images obtained by our model as well as analyzing the effects on images with different scenes. Moreover, we have contrasted the experimental results of our algorithm with that of other classical ones. Experiments demonstrate that the proposed algorithm has a better performance than other state-of-the-art methods on both virtual-scene and real-world images qualitatively and quantitatively.","2169-3536","","10.1109/ACCESS.2019.2962784","National Natural Science Foundation of China(grant numbers:41771457,61471272); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943973","Generative adversarial networks;image hazing;regression loss;mapping from pixels to pixels","Generative adversarial networks;Generators;Atmospheric modeling;Gallium nitride;Training;Solid modeling;Decoding","convolutional neural nets;feature extraction;image denoising;image enhancement;learning (artificial intelligence);optimisation;regression analysis","network parameters optimization;loss function;REG loss;image loss;virtual-scene;image hazing algorithm;generative adversarial network;symmetric encoder;decoder structure;convolutional network;generative network;discriminative network judges;haze-free image;regression loss;GAN;feature loss","","6","","27","CCBY","27 Dec 2019","","","IEEE","IEEE Journals"
"Supervised Generative Adversarial Network Based Sample Generation for Scene Classification","W. Han; R. Feng; L. Wang; J. Chen","School of Computer Science, China University of Geosciences, Wuhan, P.R.China; School of Computer Science, China University of Geosciences, Wuhan, P.R.China; School of Computer Science, China University of Geosciences, Wuhan, P.R.China; School of Computer Science, China University of Geosciences, Wuhan, P.R.China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3041","3044","High-resolution remote sensing (HRRS) image scene classification has been a critical task and greatly important for many applications, wherein convolutional neural network (CNN)-based methods have achieved considerable improvements. However, the CNN-based methods have countered a severe problem that massive annotation samples are required to obtain ideal model for scene classification. There is no dataset with a comparative scale to ImageNet to meet the sample requirement and labelling samples is labor-intensive and time-consuming. To solve the problem of insufficient annotation samples, a new generative adversarial network (GAN)-based sample generation method for scene classification is implemented. The proposed method is able to generate HRRS images with specific label and improve scene classification performance for the CNN-based methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900525","Deep Learning;High-Resolution Remote Sensing;Scene Classification","Training;Generators;Remote sensing;Generative adversarial networks;Linear programming;Geology;Gallium nitride","convolutional neural nets;image classification;image resolution;image sampling;remote sensing;supervised learning","insufficient annotation samples;generative adversarial network-based sample generation method;HRRS images;CNN-based methods;high-resolution remote sensing image scene classification;convolutional neural network-based methods;massive annotation samples;labelling samples","","1","","17","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Retro-Remote Sensing: Generating Images From Ancient Texts","M. B. Bejiga; F. Melgani; A. Vascotto","Department of Computer Science and Information Engineering, University of Trento, Trento, Italy; Department of Computer Science and Information Engineering, University of Trento, Trento, Italy; Department of Computer Science and Information Engineering, University of Trento, Trento, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Mar 2019","2019","12","3","950","960","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area.","2151-1535","","10.1109/JSTARS.2019.2895693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660422","Convolutional neural networks (CNN);deep learning;generative adversarial networks (GAN);multimodal learning;remote sensing;text-to-image synthesis","Remote sensing;Gallium nitride;Earth;Sensors;Generators;Satellites;Technological innovation","convolutional neural nets;geophysical image processing;image retrieval;learning (artificial intelligence);remote sensing;statistical analysis;text analysis","retro-remote sensing;ancient texts;data modality;multimodal learning models;text-to-image synthesis;ancient text descriptions;generative adversarial networks;GANs;deep neural network;satellite images;image generation;statistical properties;image captioning;information extraction","","11","","34","IEEE","5 Mar 2019","","","IEEE","IEEE Journals"
"Simulating Tropical Cyclone Passive Microwave Rainfall Imagery Using Infrared Imagery via Generative Adversarial Networks","F. Meng; T. Song; D. Xu","School of Geosciences, China University of Petroleum, Qingdao, China; College of Computer Science and Technology, China University of Petroleum, Qingdao, China; Guangdong Laboratory of Marine Science and Engineering (Zhuhai), Zhuhai, China","IEEE Geoscience and Remote Sensing Letters","17 Mar 2022","2022","19","","1","5","Tropical cyclones (TCs) generally carry large amounts of water vapor and can cause large-scale extreme rainfall. Passive microwave (PMW) rainfall (PMR) estimation of TC with high spatial and temporal resolution is crucial for disaster warning of TC, but remains a challenging problem due to low temporal resolution of microwave sensors. This study attempts to solve this problem by directly predicting PMW rainfall images (PMRIs) from satellite infrared (IR) images of TC. We develop a generative adversarial network (GAN) to simulate PMRI using IR images and establish the mapping relationship between TC cloud-top brightness temperature and PMR, and the algorithm is named tropical cyclone rainfall (TCR)-GAN. Meanwhile, a new dataset that is available as a benchmark, Dataset of TC IR-to-Rainfall Prediction (TCIRRP), was established, which is expected to advance the development of artificial intelligence in this direction. The experimental results show that the algorithm can effectively extract key features from IR. The end-to-end deep learning approach shows potential as a technique that can be applied globally and provides a new perspective TC precipitation prediction via satellite, which is expected to provide important insights for real-time visualization of TC rainfall globally in operations.","1558-0571","","10.1109/LGRS.2022.3152847","Natural Science Foundation of China(grant numbers:U1811464); Natural Science Foundation of Shandong Province(grant numbers:405 ZR2019MF012); Taishan Scholars Fund(grant numbers:ZX20190157); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9717263","Deep learning;generative adversarial networks (GANs);passive microwave~(PMW) rainfall;remote sensing;tropical cyclones (TCs)","Magnetic resonance imaging;Clouds;Generative adversarial networks;Microwave imaging;Generators;Spatial resolution;Microwave theory and techniques","atmospheric techniques;hydrological techniques;infrared imaging;learning (artificial intelligence);rain;storms","perspective TC precipitation prediction;TC rainfall;tropical cyclone passive microwave Rainfall imagery;infrared imagery;generative adversarial network;tropical cyclones;water vapor;large-scale extreme rainfall;PMR;high spatial resolution;low temporal resolution;microwave sensors;PMW rainfall images;IR images;TC cloud-top brightness temperature;tropical cyclone rainfall-GAN;TC IR-to-Rainfall Prediction","","1","","17","IEEE","18 Feb 2022","","","IEEE","IEEE Journals"
"Cycle-Consistent Adversarial Networks for Realistic Pervasive Change Generation in Remote Sensing Imagery","C. X. Ren; A. Ziemann; J. Theiler; A. M. S. Durieux","Intelligence and Space Research Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Intelligence and Space Research Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Intelligence and Space Research Division, Los Alamos National Laboratory, Los Alamos, NM, USA; Descartes Labs, Inc., Santa Fe, NM, USA","2020 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)","18 May 2020","2020","","","42","45","This paper introduces a new method of generating realistic pervasive changes in the context of evaluating the effectiveness of change detection algorithms in controlled settings. The method - a cycle-consistent adversarial network (CycleGAN) - requires low quantities of training data to generate realistic changes. Here we show an application of CycleGAN in creating realistic snow-covered scenes of multispectral Sentinel-2 imagery, and demonstrate how these images can be used as a test bed for anomalous change detection algorithms.","2473-3598","978-1-7281-5745-0","10.1109/SSIAI49293.2020.9094603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9094603","remote sensing;multispectral;generative adversarial networks;deep learning;change detection;image analysis","Remote sensing;Change detection algorithms;Robustness;Gallium nitride;Terrain factors;Detection algorithms;Data models","geophysical image processing;remote sensing;snow","CycleGAN;realistic snow-covered scenes;multispectral Sentinel-2 imagery;anomalous change detection algorithms;cycle-consistent adversarial network;remote sensing imagery;pervasive change generation","","5","","15","IEEE","18 May 2020","","","IEEE","IEEE Conferences"
"Domain Adaptation via a Task-Specific Classifier Framework for Remote Sensing Cross-Scene Classification","Z. Zheng; Y. Zhong; Y. Su; A. Ma","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing and the Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing and the Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing and the Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing and the Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","14 Apr 2022","2022","60","","1","13","The scene classification of high spatial resolution (HSR) imagery involves labeling an HSR image with a specific high-level semantic class according to the composition of the semantic objects and their spatial relationships. As such, scene classification has attracted increased attention in recent years, and many different algorithms have now been proposed for the cross-scene classification task. However, the recently proposed scene classification methods based on deep convolutional neural networks (CNNs) still suffer from domain shift problems, because of the training data and validation data not following the assumption of independent and identical distributions. The employment of generative adversarial networks has been found to be an effective way to bridge the domain shift/gap. However, the existing cross-scene classification methods do not use the classification information in the target domain, and the domain classifier is task-independent for different scene classification tasks. In this article, to solve this problem, domain adaptation via a task-specific classifier (DATSNET) framework is proposed for HSR image scene classification. Task-specific classifiers and minimizing and maximizing, “ i.e., minimaxing,” of the classifier discrepancy are integrated in the DATSNET framework. The task-specific classifiers are proposed to align the distributions of the source domain features and target domain features by utilizing task-specific decision boundaries in the target domain. In order to align the two task-specific classifiers’ feature distributions, minimaxing the defined discrepancy between the different classifiers in an adversarial manner is proposed to obtain better task-specific classifier boundaries in the target domain and a better-aligned feature distribution in both domains. The experimental results obtained with different remote sensing cross-scene classification tasks demonstrate that the proposed method achieves a significantly improved performance compared with the other state-of-the-art remote sensing cross-scene classification algorithms.","1558-0644","","10.1109/TGRS.2022.3151689","National Natural Science Foundation of China(grant numbers:42071350); State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS) Special Research Funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714321","Convolutional neural network (CNN);discrepancy;domain adaptation;scene classification;task specific","Task analysis;Feature extraction;Remote sensing;Training;Image analysis;Semantics;Adaptation models","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);neural nets;pattern classification","domain adaptation;task-specific classifier framework;high spatial resolution imagery;specific high-level semantic class;cross-scene classification task;domain shift problems;existing cross-scene classification methods;classification information;domain classifier;task-independent;different scene classification tasks;HSR image scene classification;task-specific classifiers;classifier discrepancy;source domain features;target domain features;task-specific decision boundaries;task-specific classifier boundaries;different remote sensing cross-scene classification tasks;state-of-the-art remote sensing cross-scene classification algorithms","","4","","60","IEEE","15 Feb 2022","","","IEEE","IEEE Journals"
"Cloud Removal Using Multimodal GAN With Adversarial Consistency Loss","Y. Zhao; S. Shen; J. Hu; Y. Li; J. Pan","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","31 Dec 2021","2022","19","","1","5","In the field of remote sensing image processing, clouds heavily affect the quality of the remote sensing images and their application potential. Thus, in recent years, with the prevalence of deep learning techniques used in the field of image processing, many methods have been proposed for cloud removal using single remote sensing images. The existing single-image cloud removal methods suffer from poor generalization capabilities that prevent them from being applied to diverse remote sensing images. Thus, a novel method using a multimodal architecture is proposed which provides multiple most likely outputs for the image and selects the best one through perception-based image quality evaluator (PIQE). In addition, adversarial consistency loss is used to replace cycle consistency loss, which encourages the model to retain more texture information of the original image, and thus the quality of the generated image increases. Experiments demonstrate that the presented method can easily achieve a considerable increase in the peak signal-to-noise ratio and the structural similarity index compared with other methods.","1558-0571","","10.1109/LGRS.2021.3093887","National Natural Science Foundation of China(grant numbers:41971422); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9481173","Cloud removal;generative adversarial network (GAN);multimodal;remote sensing","Clouds;Remote sensing;Generative adversarial networks;Generators;Task analysis;Deep learning;Decoding","data compression;geophysical image processing;image classification;image coding;image texture;learning (artificial intelligence);remote sensing","multimodal GAN;adversarial consistency loss;remote sensing image processing;deep learning techniques;single remote sensing images;poor generalization capabilities;diverse remote sensing images;multimodal architecture;perception-based image quality evaluator;cycle consistency loss;generated image increases;single-image cloud removal methods","","1","","23","IEEE","12 Jul 2021","","","IEEE","IEEE Journals"
"Adaptive DropBlock-Enhanced Generative Adversarial Networks for Hyperspectral Image Classification","J. Wang; F. Gao; J. Dong; Q. Du","Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Laboratory of Mixed Reality and Virtual Ocean, School of Information Science and Engineering, Ocean University of China, Qingdao, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","5040","5053","In recent years, the hyperspectral image (HSI) classification based on generative adversarial networks (GANs) has achieved great progress. GAN-based classification methods can mitigate the limited training sample dilemma to some extent. However, several studies have pointed out that existing GAN-based HSI classification methods are heavily affected by the imbalanced training data problem. The discriminator in GAN always contradicts itself and tries to associate fake labels to the minority-class samples and, thus, impair the classification performance. Another critical issue is the mode collapse in GAN-based methods. The generator is only capable of producing samples within a narrow scope of the data space, which severely hinders the advancement of GAN-based HSI classification methods. In this article, we proposed an Adaptive DropBlock-enhanced Generative Adversarial Networks (ADGANs) for HSI classification. First, to solve the imbalanced training data problem, we adjust the discriminator to be a single classifier, and it will not contradict itself. Second, an adaptive DropBlock (AdapDrop) is proposed as a regularization method employed in the generator and discriminator to alleviate the mode collapse issue. The AdapDrop generated drop masks with adaptive shapes instead of a fixed size region, and it alleviates the limitations of DropBlock in dealing with ground objects with various shapes. Experimental results on three HSI data sets demonstrated that the proposed ADGAN achieved superior performance over state-of-the-art GAN-based methods. Our codes are available at https://github.com/summitgao/HC_ADGAN.","1558-0644","","10.1109/TGRS.2020.3015843","National Key Research and Development Program of China(grant numbers:2018AAA0100602); National Natural Science Foundation of China(grant numbers:U1706218); Key Technology Research and Development Program of Shandong(grant numbers:2019GHY112048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173809","Adaptive DropBlock (AdapDrop);deep learning;generative adversarial network (GAN);hyperspectral image (HSI) classification","Generative adversarial networks;Generators;Training;Feature extraction;Gallium nitride;Shape;Hyperspectral sensors","hyperspectral imaging;image classification;learning (artificial intelligence);neural nets;shape recognition","fixed size region;AdapDrop generated drop masks;AdapDrop regularization method;adaptive DropBlock regularization method;ADGAN;fake labels;HSI;adaptive dropblock-enhanced generative adversarial networks;adaptive shapes;mode collapse issue;minority-class samples;imbalanced training data problem;GAN-based classification methods;hyperspectral image classification","","23","","52","IEEE","21 Aug 2020","","","IEEE","IEEE Journals"
"DML-GANR: Deep Metric Learning With Generative Adversarial Network Regularization for High Spatial Resolution Remote Sensing Image Retrieval","Y. Cao; Y. Wang; J. Peng; L. Zhang; L. Xu; K. Yan; L. Li","School of Land Science and Technology, China University of Geosciences, Beijing, China; State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Shanxi Provincial Key Laboratory of Resources, Environment and Disaster Monitoring, Jinzhong, China; Faculty of Geographical Science, State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Shanxi Provincial Key Laboratory of Resources, Environment and Disaster Monitoring, Jinzhong, China; Shanxi Provincial Key Laboratory of Resources, Environment and Disaster Monitoring, Jinzhong, China; Shanxi Provincial Key Laboratory of Resources, Environment and Disaster Monitoring, Jinzhong, China","IEEE Transactions on Geoscience and Remote Sensing","24 Nov 2020","2020","58","12","8888","8904","With a small number of labeled samples for training, it can save considerable manpower and material resources, especially when the amount of high spatial resolution remote sensing images (HSR-RSIs) increases considerably. However, many deep models face the problem of overfitting when using a small number of labeled samples. This might degrade HSR-RSI retrieval accuracy. Aiming at obtaining more accurate HSR-RSI retrieval performance with small training samples, we develop a deep metric learning approach with generative adversarial network regularization (DML-GANR) for HSR-RSI retrieval. The DML-GANR starts from a high-level feature extraction (HFE) to extract high-level features, which includes convolutional layers and fully connected (FC) layers. Each of the FC layers is constructed by deep metric learning (DML) to maximize the interclass variations and minimize the intraclass variations. The generative adversarial network (GAN) is adopted to mitigate the overfitting problem and validate the qualities of extracted high-level features. DML-GANR is optimized through a customized approach, and the optimal parameters are obtained. The experimental results on the three data sets demonstrate the superior performance of DML-GANR over state-of-the-art techniques in HSR-RSI retrieval.","1558-0644","","10.1109/TGRS.2020.2991545","National Natural Science Foundation of China(grant numbers:41801241,41574011); Fundamental Research Funds for the Central Universities(grant numbers:292018029,375201906); Key Research and Development Projects of Shanxi Province(grant numbers:201903D121142); Open Fund of the State Key Laboratory of Remote Sensing Science(grant numbers:OFSLRSS201923); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090971","Convolutional neural network (CNN);generative adversarial network (GAN);deep metric learning (DML);image retrieval;deep learning","Feature extraction;Measurement;Generative adversarial networks;Gallium nitride;Image retrieval;Generators;Training","convolutional neural nets;feature extraction;geophysical image processing;image resolution;image retrieval;remote sensing;unsupervised learning;visual databases","generative adversarial network regularization;high spatial resolution remote sensing image retrieval;high spatial resolution remote sensing images;deep models;HSR-RSI retrieval accuracy;DML-GANR;high-level feature extraction;FC layers;deep metric learning;convolutional layers;fully connected layers","","19","","60","IEEE","11 May 2020","","","IEEE","IEEE Journals"
"Remote Sensing Image Super-Resolution via Saliency-Guided Feedback GANs","H. Wu; L. Zhang; J. Ma","School of Artificial Intelligence, BNU, Beijing, China; School of Artificial Intelligence, BNU, Beijing, China; School of Artificial Intelligence, BNU, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","2 Dec 2021","2022","60","","1","16","In remote sensing images (RSIs), the visual characteristics of different regions are versatile, which poses a considerable challenge to single image super-resolution (SISR). Most existing SISR methods for RSIs ignore the diverse reconstruction needs of different regions and thus face a serious contradiction between high perception quality and less spatial distortion. The mean square error (MSE) optimization-based methods produce results of unsatisfactory visual quality, while generative adversarial networks (GANs) can produce photo-realistic but severely distorted results caused by pseudotextures. In addition, increasingly deeper networks, although providing powerful feature representations, also face problems of overfitting and occupying too much storage space. In this article, we propose a new saliency-guided feedback GAN (SG-FBGAN) to address these problems. The proposed SG-FBGAN applies different reconstruction principles for areas with varying levels of saliency and uses feedback (FB) connections to improve the expressivity of the network while reducing parameters. First, we propose a saliency-guided FB generator with our carefully designed paired-feedback block (PFBB). The PFBB uses two branches, a salient and a nonsalient branch, to handle the FB information and generate powerful high-level representations for salient and nonsalient areas, respectively. Then, we measure the visual perception quality of salient areas, nonsalient areas, and the global image with a saliency-guided multidiscriminator, which can dramatically eliminate pseudotextures. Finally, we introduce a curriculum learning strategy to enable the proposed SG-FBGAN to handle complex degradation models. Comprehensive evaluations and ablation studies validate the effectiveness of our proposal.","1558-0644","","10.1109/TGRS.2020.3042515","Beijing Natural Science Foundation(grant numbers:L182029); National Natural Science Foundation of China(grant numbers:61571050,41771407); Beijing Normal University (BNU) Interdisciplinary Research Foundation for the First-Year Doctoral Candidates(grant numbers:BNUXKJC1926); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301233","Deep learning (DL);generative adversarial network (GAN);remote sensing;saliency detection;super-resolution (SR)","Visualization;Image reconstruction;Generative adversarial networks;Distortion;Gallium nitride;Sensors;Optimization","geophysical image processing;image classification;image reconstruction;image resolution;image texture;learning (artificial intelligence);mean square error methods;remote sensing","SG-FBGAN;feedback connections;saliency-guided FB generator;paired-feedback block;PFBB;nonsalient branch;FB information;high-level representations;nonsalient areas;visual perception quality;global image;saliency-guided multidiscriminator;pseudotextures;remote sensing image super-resolution;RSI;visual characteristics;single image super-resolution;SISR methods;high perception quality;spatial distortion;mean square error optimization-based methods;generative adversarial networks;feature representations;storage space;saliency-guided feedback GAN","","10","","62","IEEE","21 Dec 2020","","","IEEE","IEEE Journals"
"Class-Aware Domain Adaptation for Semantic Segmentation of Remote Sensing Images","Q. Xu; X. Yuan; C. Ouyang","School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China; Bell Labs, Murray Hill, NJ, USA; CAS Center for Excellence in Tibetan Plateau Earth Sciences, Chinese Academy of Sciences (CAS), Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","2 Dec 2021","2022","60","","1","17","Unsupervised domain adaptation (UDA) for the semantic segmentation of remote sensing images is challenging since the same class of objects may have different spectra while the different class of objects may have the same spectrum. To address this issue, we propose a class-aware generative adversarial network (CaGAN) for UDA semantic segmentation of multisource remote sensing images, which explicitly models the discrepancies of intraclass and the interclass between the source domain images with labels and the target domain images without labels. Specifically, first, to enhance the global domain alignment (GDA), we propose a transferable attention alignment (TAA) procedure to add more fine-grained features into the adversarial learning framework. Then, we propose a novel class-aware domain alignment (CDA) approach in semantic segmentation. CDA mainly includes two parts: the first one is adaptive category selection, which is to alleviate the class imbalance and select the reliable per-category centers in the source and target domains; the second one is adaptive category alignment, which is to model the intraclass compactness and interclass separability from source-only, target-only, and joint source and target images. Finally, the CDA plays as a penalty of GDA to train GaGAN in an alternating and iterative manner. Experiments on domain adaptation of space to space, spectrum to spectrum, both space-to-space and spectrum-to-spectrum data sets demonstrate that CaGAN outperforms the current state-of-the-art methods, which may serve as a starting point and baseline for the comprehensive applications of semantic segmentation in cross-space and cross-spectrum remote sensing images.","1558-0644","","10.1109/TGRS.2020.3031926","Strategic Priority Research Program of CAS(grant numbers:XDA23090303); National Key Research and Development Program of China(grant numbers:2017YFC1501000); NSFC(grant numbers:42022054); CAS Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9262039","Class-aware domain alignment (CDA);class-aware generative adversarial network (CaGAN);cross-scene and cross-spectrum remote sensing images;global domain alignment (GDA);unsupervised domain adaptation (UDA) semantic segmentation","Semantics;Image segmentation;Remote sensing;Adaptation models;Generative adversarial networks;Gallium nitride;Training","geophysical image processing;image classification;image segmentation;learning (artificial intelligence);remote sensing;unsupervised learning","class-aware domain adaptation;unsupervised domain adaptation;different spectra;class-aware generative adversarial network;UDA semantic segmentation;multisource remote sensing images;source domain images;target domain images;global domain alignment;transferable attention alignment procedure;class-aware domain alignment approach;CDA;adaptive category selection;class imbalance;target domains;adaptive category alignment;target images;spectrum-to-spectrum data sets;cross-spectrum remote sensing images","","10","","53","IEEE","17 Nov 2020","","","IEEE","IEEE Journals"
"Real-World DEM Super-Resolution Based on Generative Adversarial Networks for Improving InSAR Topographic Phase Simulation","Z. Wu; Z. Zhao; P. Ma; B. Huang","Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong, Hong Kong; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong, Hong Kong; Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Sep 2021","2021","14","","8373","8385","Topographic phase simulation is important for deformation estimation in differential synthetic aperture radar (SAR) interferometry. The most commonly used 30 m resolution shuttle radar topography mission (SRTM) digital elevation model (DEM) is usually required to be resampled due to its relatively low resolution (LR) comparing to the high resolution (HR) SAR images. Although the WorldDEM with a 12 m resolution achieves global coverage, it is not available freely. Consequently, it is useful to evaluate the practicability of the super-resolution (SR) from LR SRTM DEMs to HR WorldDEM ones, which has not been investigated. Most existing DEM SR models are trained with synthetic datasets in which the LR DEMs are downsampled from their HR counterparts. However, these models become less effective when applied to real-world scenarios due to the domain gap between the synthetic and real LR DEMs. In this article, we constructed a real-world DEM SR dataset, where the LR and HR DEMs were collected from SRTM and WorldDEM, respectively. An enhanced SR generative adversarial network model was adapted to train on the dataset. Considering that the real LR-HR pairs may suffer from misalignment, we introduced the perceptual loss for better optimizing the model. Moreover, a logarithmic normalization was proposed to compress the wide elevation range and adjust the uneven distribution. We also pretrained the model using natural images since collecting sufficient HR DEMs is costly. Experiments demonstrate that the proposed method achieves near 0.69 dB improvement of peak signal-to-noise ratio. In addition, our method is also validated to improve the topographic phase simulation by 23.42% of MSE.","2151-1535","","10.1109/JSTARS.2021.3105123","National Natural Science Foundation of China(grant numbers:41971278); Research Grants Council of Hong Kong(grant numbers:CUHK14504219,CUHK14206818,AoE/E-603/18); National Key R&D Program of China(grant numbers:2019YFC1510400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516888","Deep learning;digital elevation model;generative adversarial network;InSAR topographic phase simulation;super resolution (SR)","Generative adversarial networks;Synthetic aperture radar;Adaptation models;Training;Superresolution;Earth;Satellites","digital elevation models;geophysical image processing;image resolution;radar imaging;radar interferometry;remote sensing by radar;synthetic aperture radar;topography (Earth)","synthetic datasets;LR DEMs;real-world DEM;SR dataset;enhanced SR generative adversarial network model;LR-HR pairs;world DEM super-resolution;generative adversarial networks;improving InSAR topographic phase simulation;deformation estimation;differential synthetic aperture radar interferometry;relatively low resolution comparing;high resolution SAR images;LR SRTM DEMs;HR WorldDEM ones;DEM SR models;30 m resolution shuttle radar topography mission digital elevation model;HR DEMs","","5","","54","CCBY","18 Aug 2021","","","IEEE","IEEE Journals"
"Background Subtraction Based on GAN and Domain Adaptation for VHR Optical Remote Sensing Videos","W. Yu; J. Bai; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xi’an, China","IEEE Access","6 Jul 2020","2020","8","","119144","119157","The application of deep learning techniques in background subtraction for VHR optical remote sensing videos holds the potential to facilitate multiple intelligent remote sensing processing tasks. However, existing methods on background subtraction for VHR optical remote sensing videos are still facing technical challenges. First, conventional CNN and other networks are limited by performance constraints. Second, existing background subtraction methods are mostly trained by natural videos due to the lack of VHR optical remote sensing video datasets. Third, VHR optical remote sensing videos have large scene sizes. In our article, we design a novel deep learning network via fully utilizing GAN and domain adaptation, which has the ability to measure and minimize the discrepancy between feature distributions of natural videos and VHR optical remote sensing videos so that the background subtraction performance for VHR optical remote sensing videos is improved significantly. Numerous experiments are conducted on the CDnet 2014 dataset and VHR optical remote sensing video dataset. Tremendous experiments demonstrate that our proposed method achieves an average FM of 0.8533, which reveals excellent performance on background subtraction.","2169-3536","","10.1109/ACCESS.2020.3004495","National Natural Science Foundation of China(grant numbers:61772401); Fundamental Research Funds for the Central Universities(grant numbers:5035-20109206747,RW180177); Aeronautical Science Foundation of China(grant numbers:20170181003); Open Fund of Key Laboratory of Computer Network and Information Integration of Ministry of Education, Southeast University(grant numbers:K93-9-2018-09); Open Fund of Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education(grant numbers:IPIU2019007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9123388","Background subtraction;generative adversarial networks (GAN);domain adaptation;very high resolution (VHR) optical remote sensing videos","Videos;Remote sensing;Optical sensors;Optical imaging;Integrated optics;Nonlinear optics;Generative adversarial networks","feature extraction;geophysics computing;learning (artificial intelligence);neural nets;remote sensing;video signal processing","background subtraction;VHR optical remote sensing video dataset;GAN;domain adaptation;deep learning","","8","","48","CCBY","23 Jun 2020","","","IEEE","IEEE Journals"
"Learning to Translate for Cross-Source Remote Sensing Image Retrieval","W. Xiong; Y. Lv; X. Zhang; Y. Cui","Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2020","2020","58","7","4860","4874","Content-based remote sensing image retrieval (CBRSIR) is one of the important techniques for the mining and analysis of big remote sensing data. Recently, unified-source CBRSIR (US-CBRSIR) has been extensively studied and explored, but there has been little attention on cross-source CBRSIR (CS-CBRSIR). Although there is motivation for exploration of CS-CBRSIR for the continually increasing multisource remote sensing data, CS-CBRSIR suffers data drift due to multisource data. In this article, we explore to explicitly address the problem by mapping the source domain to target domain and propose an image translation-based framework for CS-CBRSIR. On the one hand, a novel cycle-identity-generative adversarial network (CI-GAN) is proposed based on the cycle-GAN. In addition to the generator and discriminator, a pretrained classifier, identity module, is designed to further boost the discriminative ability of translated images and facilitate the implementation of feature extraction and similarity measure. On the other hand, to alleviate the impact of style difference between the generated and real images, translated image augmentations and label smoothing regularization (LSR) are adopted to enhance training and contribute toward generation of a robust feature extractor. Extensive experiments on a public data set and a detailed ablation study confirm the effectiveness of our approach.","1558-0644","","10.1109/TGRS.2020.2968096","National Natural Science Foundation of China(grant numbers:61790550,61790554,91538201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8985543","Cross-source content-based remote sensing image retrieval (CS-CBRSIR);domain adaptation;generative adversarial networks (GANs);image translation","Remote sensing;Feature extraction;Image retrieval;Training;Gallium nitride;Data mining;Task analysis","content-based retrieval;feature extraction;geophysical image processing;image classification;image retrieval;learning (artificial intelligence);remote sensing","similarity measure;feature extraction;multisource remote sensing data;cross-source CBRSIR;US-CBRSIR;unified-source CBRSIR;big remote sensing data;cross-source remote sensing image retrieval;translated image augmentations;generated images;cycle-identity-generative adversarial network;CS-CBRSIR;image translation-based framework;source domain;data drift","","25","","48","IEEE","6 Feb 2020","","","IEEE","IEEE Journals"
"Cropland Change Detection With Harmonic Function and Generative Adversarial Network","J. Chen; W. Zhao; X. Chen","National Geomatics Center of China, Beijing, China; Beijing Engineering Research Center for Global Land Remote Sensing Products, Faculty of Geographical Science, Institute of Remote Sensing Science and Engineering, Beijing Normal University, Beijing, China; School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China","IEEE Geoscience and Remote Sensing Letters","15 Dec 2021","2022","19","","1","5","Time-series image change detection is one of the most challenging tasks to remote sensing society. Due to complex phenological patterns of cropland, it is difficult to design an efficient strategy for cropland change detection. In this work, an integrated framework is proposed to perform change detection with a limited number of training samples. There are two improvements in this proposed cropland change detection method: 1) the harmonic function is utilized to fill the missing data within a time-series image stack by considering phenological patterns of cropland and 2) the CropGAN was developed to generate realistic samples for training data set enrichment. Compared to the traditional change detection methods, the proposed strategy able to detect different kinds of cropland changes even with few number of samples. Experiments on a Landsat time-series image stack demonstrated that the proposed CropGAN can significantly improve change detection accuracies, given a limited number of labeled samples.","1558-0571","","10.1109/LGRS.2020.3023137","National Key Research and Development Program of China(grant numbers:2016YFB0501401); Ministry of Science and Technology, China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205210","Change detection;generative adversarial network (GAN);time-series imagery","Harmonic analysis;Remote sensing;Generative adversarial networks;Gallium nitride;Training;Agriculture;Generators","","","","1","","12","IEEE","24 Sep 2020","","","IEEE","IEEE Journals"
"U-IMG2DSM: Unpaired Simulation of Digital Surface Models With Generative Adversarial Networks","M. E. Paoletti; J. M. Haut; P. Ghamisi; N. Yokoya; J. Plaza; A. Plaza","Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Geoscience and Remote Sensing Letters","23 Jun 2021","2021","18","7","1288","1292","High-resolution digital surface models (DSMs) provide valuable height information about the Earth's surface, which can be successfully combined with other types of remotely sensed data in a wide range of applications. However, the acquisition of DSMs with high spatial resolution is extremely time-consuming and expensive with their estimation from a single optical image being an ill-possed problem. To overcome these limitations, this letter presents a new unpaired approach to obtain DSMs from optical images using deep learning techniques. Specifically, our new deep neural model is based on variational autoencoders (VAEs) and generative adversarial networks (GANs) to perform image-to-image translation, obtaining DSMs from optical images. Our newly proposed method has been tested in terms of photographic interpretation, reconstruction error, and classification accuracy using three well-known remotely sensed data sets with very high spatial resolution (obtained over Potsdam, Vaihingen, and Stockholm). Our experimental results demonstrate that the proposed approach obtains satisfactory reconstruction rates that allow enhancing the classification results for these images. The source code of our method is available from: https://github.com/mhaut/UIMG2DSM.","1558-0571","","10.1109/LGRS.2020.2997295","Spanish Ministry(grant numbers:FPU15/02090); Junta de Extremadura(grant numbers:GR18060); European Union(grant numbers:734541- EXPOSURE); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108295","Digital surface models (DSMs);generative adversarial networks (GANs);image-to-image problems;optical imaging;variational autoencoder (VAEs)","Optical imaging;Gallium nitride;Optical sensors;Data models;Surface topography;Adaptive optics;Optical network units","feature extraction;geophysical image processing;image classification;image reconstruction;image segmentation;learning (artificial intelligence);neural nets;remote sensing;terrain mapping","DSMs;valuable height information;Earth's surface;high spatial resolution;single optical image;ill-possed problem;unpaired approach;optical images;deep learning techniques;deep neural model;generative adversarial networks;image-to-image translation;remotely sensed data sets;u-IMG2DSM;unpaired simulation;high-resolution digital surface models","","8","","24","IEEE","4 Jun 2020","","","IEEE","IEEE Journals"
"Fusion Detection of Closed Water in Medium-Low Resolution Remote Sensing Imagery","Y. Ning; Y. You; J. Cao; F. Liu; Q. Yan; Y. Zhang","School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4027","4030","Aiming at the closed water detection in remote sensing imagery at medium-low resolution, this paper proposes a novel method for closed water detection based on fusion detection which conducts detection via informative fused images blended by Synthetic Aperture Radar (SAR) and optical images. Firstly, it utilizes SAR and optical image pairs containing the same closed water object to generate aligned image pairs according to latitude and longitude information. Next, generative adversarial network (GAN) is adopted to fuse two categories of images. At last, a target detection network driven by optical image samples is used to detect the closed water on the fused image. The experiment result on Sentinel-1&2 shows that the proposed method can effectively make up for the shortage of SAR image in closed water detection and improve the detection performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553554","Beijing Natural Science Foundation, China(grant numbers:4214058); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553554","Closed water detection;Image fusion;GAN","Image resolution;Object detection;Optical fiber networks;Optical imaging;Generative adversarial networks;Adaptive optics;Radar polarimetry","geophysical image processing;geophysical techniques;image fusion;image resolution;object detection;optical images;radar imaging;remote sensing;synthetic aperture radar","target detection network;optical image samples;fused image;SAR image;closed water detection;detection performance;fusion detection;medium-low resolution remote sensing imagery;informative fused images;optical images;optical image pairs;closed water object;aligned image pairs","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Super-Resolution for Cross-Sensor Optical Remote Sensing Images","S. Ambudkar; R. Raj; K. Billa; R. Hukumchand","Pixxel.space, Bengaluru, Karnataka, India; Pixxel.space, Bengaluru, Karnataka, India; Pixxel.space, Bengaluru, Karnataka, India; Pixxel.space, Bengaluru, Karnataka, India","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1880","1883","Generative adversarial network (GAN) models are becoming popular in the field of remote sensing for generating high spatial resolution images from their low resolution versions. In this study, four models including two basic Super-resolution GAN models and two non-GAN Deep Learning models were trained and tested to achieve 2.5m, and 5m spatial resolution from their 10m spatial resolution satellite data. The comparison of results showed that the SRGAN model performed better than the other deep learning models. The performance metrics were also found to be consistent with available literature.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883182","Super-resolution;generative adversarial network;resolution enhancement","Deep learning;Satellites;Superresolution;Optical fiber networks;Generative adversarial networks;Optical imaging;Data models","geophysical image processing;image resolution;learning (artificial intelligence);remote sensing","low resolution versions;basic Super-resolution GAN models;nonGAN Deep Learning models;5m spatial resolution;10m spatial resolution satellite data;SRGAN model;cross-sensor optical remote sensing;generative adversarial network models;high spatial resolution images;size 2.5 m;size 10.0 m;size 5.0 m","","","","15","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Memory-Oriented Unpaired Learning for Single Remote Sensing Image Dehazing","X. Chen; Y. Huang","College of Electronic and Information Engineering, Shenyang Aerospace University, Shenyang, China; College of Electronic and Information Engineering, Shenyang Aerospace University, Shenyang, China","IEEE Geoscience and Remote Sensing Letters","28 Apr 2022","2022","19","","1","5","Remote sensing image dehazing (RSID) is an extremely challenging problem due to the irregular and nonuniform distribution of haze. The existing RSID methods achieve excellent performance using deep learning; however, relying on paired synthetic data is limited to their generality in various haze distribution. In this letter, we present a memory-oriented generative adversarial network (MO-GAN), which tries to capture the desired hazy features in an unpaired learning manner toward single RSID. For better extracting the haze-relevant features, a novel multistage attentive-recurrent memory module is designed to guide an autoencoder neural network, which can record the various appearances of haze distribution at different stages. To well differentiate fake images from real ones, a dual region discriminator is constructed to handle spatially varying haze densities in global and local regions. Extensive experiments demonstrate that our designed MO-GAN outperforms the recent comparing approaches on the various frequently used datasets, especially in real world nonuniform haze conditions. The source code is released in https://github.com/cxtalk/MO-GAN.","1558-0571","","10.1109/LGRS.2022.3167476","Liaoning Education Department of Science and Technology research project(grant numbers:JYT2020030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757219","Attention mechanism;generative adversarial networks (GANs);haze removal;memory network;remote sensing (RS) image;unpaired learning","Feature extraction;Generative adversarial networks;Training;Memory modules;Sensors;Image reconstruction;Task analysis","feature extraction;geophysical image processing;image classification;image denoising;learning (artificial intelligence);neural nets;remote sensing","global regions;local regions;MO-GAN;world nonuniform haze conditions;memory-oriented unpaired learning;single remote sensing image dehazing;extremely challenging problem;existing RSID methods;deep learning;paired synthetic data;haze distribution;memory-oriented generative adversarial network;desired hazy features;unpaired learning manner;single RSID;haze-relevant features;novel multistage attentive-recurrent memory module;autoencoder neural network;differentiate fake images;dual region discriminator;haze densities","","2","","27","IEEE","14 Apr 2022","","","IEEE","IEEE Journals"
"Classification of Hyperspectral Images via Multitask Generative Adversarial Networks","R. Hang; F. Zhou; Q. Liu; P. Ghamisi","Jiangsu Key Laboratory of Big Data Analysis Technology, the School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, the School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, the School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Freiberg, Germany","IEEE Transactions on Geoscience and Remote Sensing","20 Jan 2021","2021","59","2","1424","1436","Deep learning has shown its huge potential in the field of hyperspectral image (HSI) classification. However, most of the deep learning models heavily depend on the quantity of available training samples. In this article, we propose a multitask generative adversarial network (MTGAN) to alleviate this issue by taking advantage of the rich information from unlabeled samples. Specifically, we design a generator network to simultaneously undertake two tasks: the reconstruction task and the classification task. The former task aims at reconstructing an input hyperspectral cube, including the labeled and unlabeled ones, whereas the latter task attempts to recognize the category of the cube. Meanwhile, we construct a discriminator network to discriminate the input sample coming from the real distribution or the reconstructed one. Through an adversarial learning method, the generator network will produce real-like cubes, thus indirectly improving the discrimination and generalization ability of the classification task. More importantly, in order to fully explore the useful information from shallow layers, we adopt skip-layer connections in both reconstruction and classification tasks. The proposed MTGAN model is implemented on three standard HSIs, and the experimental results show that it is able to achieve higher performance than other state-of-the-art deep learning models.","1558-0644","","10.1109/TGRS.2020.3003341","Natural Science Foundation of China(grant numbers:61825601,61532009,61906096); Natural Science Foundation of Jiangsu Province, China(grant numbers:BK20180786,18KJB520032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9125995","Convolutional neural network (CNN);generative adversarial networks (GANs);hyperspectral image (HSI) classification;multitask learning","Task analysis;Generative adversarial networks;Generators;Gallium nitride;Training;Deep learning;Image reconstruction","hyperspectral imaging;image classification;learning (artificial intelligence);neural nets","adversarial learning method;generator network;deep learning models;hyperspectral images;multitask generative adversarial network;hyperspectral image classification;unlabeled samples;reconstruction task;input hyperspectral cube;discriminator network;input sample","","64","","44","IEEE","25 Jun 2020","","","IEEE","IEEE Journals"
"HPGAN: Hyperspectral Pansharpening Using 3-D Generative Adversarial Networks","W. Xie; Y. Cui; Y. Li; J. Lei; Q. Du; J. Li","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","463","477","Hyperspectral (HS) pansharpening, as a special case of the superresolution (SR) problem, is to obtain a high-resolution (HR) image from the fusion of an HR panchromatic (PAN) image and a low-resolution (LR) HS image. Though HS pansharpening based on deep learning has gained rapid development in recent years, it is still a challenging task because of the following requirements: 1) a unique model with the goal of fusing two images with different dimensions should enhance spatial resolution while preserving spectral information; 2) all the parameters should be adaptively trained without manual adjustment; and 3) a model with good generalization should overcome the sensitivity to different sensor data in reasonable computational complexity. To meet such requirements, we propose a unique HS pansharpening framework based on a 3-D generative adversarial network (HPGAN) in this article. The HPGAN induces the 3-D spectral-spatial generator network to reconstruct the HR HS image from the newly constructed 3-D PAN cube and the LR HS image. It searches for an optimal HR HS image by successive adversarial learning to fool the introduced PAN discriminator network. The loss function is specifically designed to comprehensively consider global constraint, spectral constraint, and spatial constraint. Besides, the proposed 3-D training in the high-frequency domain reduces the sensitivity to different sensor data and extends the generalization of HPGAN. Experimental results on data sets captured by different sensors illustrate that the proposed method can successfully enhance spatial resolution and preserve spectral information.","1558-0644","","10.1109/TGRS.2020.2994238","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Young Talent fund of University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097446","Generative adversarial networks;global constraint;hyperspectral pansharpening;spectral–spatial constraints;3-D high-frequency block","Generators;Generative adversarial networks;Spatial resolution;Gallium nitride;Bayes methods;Data models","computational complexity;deep learning (artificial intelligence);image fusion;image resolution;spectral analysis","spectral constraint;spatial constraint;sensor data;HPGAN;spatial resolution;spectral information;hyperspectral pansharpening;3D generative adversarial network;superresolution problem;high-resolution image;HR panchromatic image;low-resolution HS image;deep learning;unique HS pansharpening framework;LR HS image;optimal HR HS image;3D PAN cube;3D spectral-spatial generator network;3D training;PAN discriminator network;successive adversarial learning;image fusion;computational complexity","","32","","36","IEEE","20 May 2020","","","IEEE","IEEE Journals"
"Research on GAN-based Image Super-Resolution Method","X. Xue; X. Zhang; H. Li; W. Wang","College of Information Science and Technology, Northeast Normal University, Changchun, China; College of Information Science and Technology, Northeast Normal University, Changchun, China; College of Information Science and Technology, Northeast Normal University, Changchun, China; College of Information Science and Technology, Northeast Normal University, Changchun, China","2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)","1 Sep 2020","2020","","","602","605","Super-Resolution (SR) refers to the reconstruction of high-resolution image from low-resolution image, which has important application value in object detection, medical imaging, satellite remote sensing and other fields. In recent years, with the rapid development of deep learning, the image super-resolution reconstruction method based on deep learning has made remarkable progress. In this paper, R-SRGAN (Residual Super-Resolution Generative Adversarial Networks) is used to build the model and realize image super-resolution. By adding residual blocks between adjacent convolutional layers of the GAN generator, more detailed information is retained. At the same time, the Wassertein distance is used as a loss function to enhance the training effect and achieve image super-resolution.","","978-1-7281-7005-3","10.1109/ICAICA50127.2020.9182617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9182617","Super-Resolution;Image Processing;Generative Adversarial Networks","Image resolution;Signal resolution;Training;Generative adversarial networks;Generators;Interpolation;Gallium nitride","image reconstruction;image resolution;learning (artificial intelligence);neural nets;object detection","deep learning;image superresolution reconstruction method;GAN-based image superresolution method;residual superresolution generative adversarial networks;R-SRGAN","","3","","17","IEEE","1 Sep 2020","","","IEEE","IEEE Conferences"
"Semi-Supervised Remote-Sensing Image Scene Classification Using Representation Consistency Siamese Network","W. Miao; J. Geng; W. Jiang","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","8 Mar 2022","2022","60","","1","14","Deep learning has achieved excellent performance in remote-sensing image scene classification, since a large number of datasets with annotations can be applied for training. However, in actual applications, there is just a few annotated samples and a large number of unannotated samples in remote-sensing images, which leads to overfitting of the deep model and affects the performance of scene classification. In order to address these problems, a semi-supervised representation consistency Siamese network (SS-RCSN) is proposed for remote-sensing image scene classification. First, considering intraclass diversity and interclass similarity of remote-sensing images, Involution-generative adversarial network (GAN) is utilized to extract the discriminative features from remote-sensing images via unsupervised learning. Then, Siamese network with a representation consistency loss is proposed for semi-supervised classification, which aims to reduce the differences of labeled and unlabeled data. Experimental results on UC Merced dataset, RESICS-45 dataset, aerial image dataset (AID), and RS dataset demonstrate that our method yields superior classification performance compared with other semi-supervised learning (SSL) methods.","1558-0644","","10.1109/TGRS.2022.3140485","National Natural Science Foundation of China(grant numbers:61901376); China Postdoctoral Science Foundation(grant numbers:2021TQ0271,2021M700110); Shaanxi Key Research and Development Program(grant numbers:2021ZDLGY01-04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9672102","Involution-generative adversarial network (GAN);remote-sensing image;scene classification;semi-supervised learning (SSL);Siamese network","Remote sensing;Feature extraction;Semisupervised learning;Sensors;Convolutional neural networks;Generative adversarial networks;Training","feature extraction;geophysical image processing;image classification;image representation;remote sensing;semi-supervised learning (artificial intelligence);unsupervised learning","representation consistency loss;aerial image dataset;semisupervised learning methods;semisupervised remote-sensing image scene classification;deep learning;annotated samples;unannotated samples;semisupervised representation consistency siamese network;involution-generative adversarial network;unsupervised learning;UC Merced dataset;RESICS-45 dataset;RS dataset","","6","","61","IEEE","6 Jan 2022","","","IEEE","IEEE Journals"
"Generative Adversarial Networks for Spectral Super-Resolution and Bidirectional RGB-To-Multispectral Mapping","K. G. Lore; K. K. Reddy; M. Giering; E. A. Bernal","United Technologies Research Center, E. Hartford, CT; United Technologies Research Center, E. Hartford, CT; United Technologies Research Center, E. Hartford, CT; University of Rochester, Rochester, NY","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","9 Apr 2020","2019","","","926","933","Acquisition of multi-and hyperspectral imagery imposes significant requirements on the hardware capabilities of the sensors involved. In order to keep costs manageable, and due to limitations in the sensing technology, tradeoffs between the spectral and the spatial resolution of hyperspectral images are usually made. Such tradeoffs are usually not necessary when considering acquisition of traditional RGB imagery. We investigate the use of statistical learning, and in particular, of conditional generative adversarial networks (cGANs) to estimate mappings from three-channel RGB to 31-band multispectral imagery. We demonstrate the application of the proposed approach to (i) RGB-to-multispectral image mapping, (ii) spectral super-resolution of image data, and (iii) recovery of RGB imagery from multispectral data.","2160-7516","978-1-7281-2506-0","10.1109/CVPRW.2019.00122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025450","","Spatial resolution;Image reconstruction;Task analysis;Training;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image colour analysis;image resolution;image sensors;remote sensing;spectral analysis","sensing technology;hardware capabilities;significant requirements;hyperspectral imagery;Bidirectional RGB-To-Multispectral;multispectral data;spectral super-resolution;RGB-to-multispectral image mapping;31-band multispectral imagery;three-channel RGB;conditional generative adversarial networks;traditional RGB imagery;considering acquisition;hyperspectral images;spatial resolution","","10","","30","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Super-Resolution Reconstruction of Remote Sensing Images Based on GAN","L. Zhou; Y. Xia; Z. Liu","Jiangsu University of Science and Technology, Nanjing, China; Jiangsu University of Science and Technology, Nanjing, China; Jiangsu University of Science and Technology, Nanjing, China","2021 IEEE International Conference on Data Science and Computer Application (ICDSCA)","23 Dec 2021","2021","","","270","274","In recent years, the methods of super-resolution image reconstruction that based on deep learning have become a hot topic in research of computer vision. The methods of super-resolution image reconstruction that based on the Generative Adversarial Network (GAN) are not controlled in network generation, the models are easy to collapse, the generalization ability is undesirable, and the time complexity degree is too high. To fill these gaps, we propose a super-resolution image reconstruction method based on the GAN of encoding and decoding, which improves the quality of image reconstruction. First of all, our approach uses a design network with regularized structure to avoid model collapse. Then we build a generation network structure that based on encoding and decoding to suppress the uncontrollable defects of GAN network generated images. Finally, in the last layer of the generator, $\mathrm{N}^{\star}\mathrm{N}$ convolutional feature layer is included to replace the Softmax layer, which speeds up the training of the model. The experimental results show that the super-resolution remote sensing image reconstructed by the proposed method has higher reconstruction quality and better generalization ability in the DOTA training data sets. At the same time, the image reconstruction process can take much less time.","","978-1-6654-4054-7","10.1109/ICDSCA53499.2021.9649727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649727","GAN;Coding and Decoding;Remote Sensing Images;Super Resolution Reconstruction","Training;Deep learning;Image coding;Superresolution;Generative adversarial networks;Visual effects;Generators","computational complexity;computer vision;convolutional neural nets;geophysical image processing;image reconstruction;image resolution;remote sensing","generative adversarial network;generalization ability;superresolution image reconstruction method;design network;generation network structure;superresolution remote sensing image;deep learning;computer vision;time complexity degree;encoding;decoding;GAN network generated image;convolutional feature layer;softmax layer;DOTA training data sets","","","","11","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"GAN Evaluation Method Based on Remote Sensing Image Information","W. JinYu; L. Yang; Y. HaiTao; Z. FengJie; G. YuGe; L. GaoYuan","School of Space Information, Space Engineering University, BeiJing, China; School of Space Information, Space Engineering University, BeiJing, China; School of Space Information, Space Engineering University, BeiJing, China; School of Space Information, Space Engineering University, BeiJing, China; School of Space Information, Space Engineering University, BeiJing, China; School of Space Information, Space Engineering University, BeiJing, China","2021 6th International Conference on Image, Vision and Computing (ICIVC)","14 Sep 2021","2021","","","295","300","To address the problem of poor applicability of the existing generative adversarial network (GAN) evaluation system, this paper introduces the remote sensing image information quantity calculation system into the generative adversarial network evaluation indicator system, proposes an end-to-end GAN evaluation method based on the single-pixel image information quantity indicator, and uses EDS, ENDS, and InfoDS to evaluate the stability of the generator training process, mode collapse, diversity generation, and training failure problem is effectively monitored, which independent of loss function, learning rate.","","978-1-6654-4368-5","10.1109/ICIVC52351.2021.9526935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526935","remote sensing image information;information indicators;GAN","Training;Visualization;Uncertainty;Stability criteria;Optimization methods;Generative adversarial networks;Reflection","geophysical image processing;learning (artificial intelligence);neural nets;remote sensing","generative adversarial network evaluation indicator system;single-pixel image information quantity indicator;generator training process;diversity generation;training failure problem;GAN evaluation method;poor applicability;remote sensing image information quantity calculation system;generative adversarial network evaluation system;ENDS;InfoDS;loss function;learning rate","","","","14","IEEE","14 Sep 2021","","","IEEE","IEEE Conferences"
"UFN-GAN: An unsupervised generative adversarial network for remote sensing image fusion","H. Dai; X. Liu; Y. Qiao; K. Zheng; X. Xiao; Z. Cai","School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Computer Science, China University of Geosciences, Whuhan, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1803","1808","Different sensors acquire different images in the same area, such as multi-spectral (MS) images and panchromatic (PAN) images. Normally, the MS images possess high spectral resolution but low spatial resolution, while PAN images are opposite in the distribution of spectral and spatial information. Image fusion is a common method to obtain the information of PAN and MS images simultaneously. To generate clearer fusion image with abundant information, we design an unsupervised fusion net based on generative adversarial network (GAN), named UFNGAN for remote sensing image fusion. In our proposed UFNGAN, an adversarial net is designed between our generator and two discriminators to adequately retain the spectral and spatial information of original images without supervision. MS images and PAN images are fused by our generator, which consists of an encoder and a decoder. Our encoder is used to extract deeper feature maps of the original images, and the decoder is applied to rebuild images. Furthermore, the Spatial-Information-Enhancement (SIE) model is utilized to obtain spatial information of MS images for enhancing PAN image, and the Edge-Detection-Registration (EDR) method is applied to register the original images to avoid fused images distortion. At last, experiments are performed on QuickBird and GaoFen-2 datasets.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727490","Unsupervised learning;Image fusion;Generative adversarial network;Deep learning;Remote sensing images","Image edge detection;Generative adversarial networks;Feature extraction;Distortion;Generators;Decoding;Sensors","geophysical image processing;image fusion;image resolution;neural nets;remote sensing;unsupervised learning","remote sensing image fusion;multispectral images;panchromatic images;MS images;high spectral resolution;PAN images;spatial information;fusion image;unsupervised fusion net;spectral information;original images;fused images distortion;unsupervised generative adversarial network;spatial-information-enhancement model;UFNGAN;QuickBird;GaoFen-2 datasets;encoder","","","","10","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Coupled Adversarial Training for Remote Sensing Image Super-Resolution","S. Lei; Z. Shi; Z. Zou","State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; Department of Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor, USA","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2020","2020","58","5","3633","3643","Generative adversarial network (GAN) has made great progress in recent natural image super-resolution tasks. The key to its success is the integration of a discriminator which is trained to classify whether the input is a real high-resolution (HR) image or a generated one. Arguably, learning a strong discriminative prior is essential for generating high-quality images. However, in remote sensing images, we discover, through extensive statistical analysis, that there are more low-frequency components than natural images, which may lead to a “discrimination-ambiguity” problem, i.e., the discriminator will become “confused” to tell whether its input is real or not when dealing with those low-frequency regions, and therefore, the quality of generated HR images may be deeply affected. To address this problem, we propose a novel GAN-based super-resolution algorithm named coupled-discriminated GANs (CDGANs) for remote sensing images. Different from the previous GAN-based super-resolution models in which their discriminator takes in a single image at one time, in our model, the discriminator is specifically designed to take in a pair of images: a generated image and its HR ground truth, to make better discrimination of the inputs. We further introduce a dual pathway network architecture, a random gate, and a coupled adversarial loss to learn better correspondence between the discriminative results and the paired inputs. Experimental results on two public data sets demonstrate that our model can obtain more accurate super-resolution results in terms of both visual appearance and local details compared with other state of the arts. Our code will be made publicly available.","1558-0644","","10.1109/TGRS.2019.2959020","National Basic Research Program of China (973 Program)(grant numbers:2017YFC1405605); National Natural Science Foundation of China(grant numbers:61671037); Beijing Natural Science Foundation(grant numbers:4192034); National Defense Science and Technology Innovation Special Zone Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946581","Coupled adversarial training;deep convolutional neural networks;generative adversarial networks (GANs);remote sensing images;super-resolution","Remote sensing;Training;Generators;Gallium nitride;Task analysis","geophysical image processing;image representation;image resolution;learning (artificial intelligence);remote sensing;statistical analysis","CDGAN;coupled-discriminated GAN;GAN-based super-resolution;low-frequency components;high-quality images;strong discriminative;high-resolution image;natural image super-resolution tasks;generative adversarial network;remote sensing image super-resolution;coupled adversarial training;accurate super-resolution results;paired inputs;discriminative results;coupled adversarial loss;previous GAN-based super-resolution models;remote sensing images;generated HR images;low-frequency regions","","54","","58","IEEE","31 Dec 2019","","","IEEE","IEEE Journals"
"Void Filling of Digital Elevation Models With Deep Generative Models","K. Gavriil; G. Muntingh; O. J. D. Barrowclough","Institute of Discrete Mathematics and Geometry, Vienna University of Technology, Vienna, Austria; SINTEF Digital, Oslo, Norway; SINTEF Digital, Oslo, Norway","IEEE Geoscience and Remote Sensing Letters","25 Sep 2019","2019","16","10","1645","1649","In recent years, advances in machine learning algorithms, cheap computational resources, and the availability of big data have spurred the deep learning revolution in various application domains. In particular, supervised learning techniques in image analysis have led to a superhuman performance in various tasks, such as classification, localization, and segmentation, whereas unsupervised learning techniques based on increasingly advanced generative models have been applied to generate high-resolution synthetic images indistinguishable from real images. In this letter, we consider a state-of-the-art machine learning model for image inpainting, namely, a Wasserstein Generative Adversarial Network based on a fully convolutional architecture with a contextual attention mechanism. We show that this model can be successfully transferred to the setting of digital elevation models for the purpose of generating semantically plausible data for filling voids. Training, testing, and experimentation are done on GeoTIFF data from various regions in Norway, made openly available by the Norwegian Mapping Authority.","1558-0571","","10.1109/LGRS.2019.2902222","H2020 Marie Skłodowska-Curie Actions(grant numbers:675789); Norges Forskningsråd(grant numbers:270922); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669876","Digital elevation models (DEMs);predictive models;remote sensing;unsupervised learning","Remote sensing;Data models;Adaptation models;Digital elevation models;Generative adversarial networks;Unsupervised learning;Generative adversarial networks;Image reconstruction;Learning systems","convolutional neural nets;digital elevation models;image resolution;image restoration;supervised learning;unsupervised learning","supervised learning techniques;image analysis;unsupervised learning techniques;high-resolution synthetic images;image inpainting;digital elevation models;GeoTIFF data;void filling;machine learning algorithms;Wasserstein generative adversarial network;Big Data;deep learning;deep generative models;contextual attention mechanism;fully convolutional architecture","","9","","21","IEEE","19 Mar 2019","","","IEEE","IEEE Journals"
"Towards Creating Exotic Remote Sensing Datasets using Image Generating AI","M. Abduljawad; A. Alsalmani","National Space Science and Technology Center, United Arab Emirates University, Al Ain, UAE; National Space Science and Technology Center, United Arab Emirates University, Al Ain, UAE","2022 International Conference on Electrical and Computing Technologies and Applications (ICECTA)","26 Dec 2022","2022","","","84","88","Over the past few years, neural networks have been used more often to solve long lasting challenges. Remote sensing and data classification were some of the fields that have widely depended on this continuously developing technology. In this context, remote sensing data related to places with harsh conditions have been missing, especially the ones related to SAR imagery. Such conditions include deserts, glaciers, and icebergs, where lots of people have lost their lives in, due to the lack of efficient methods of searching and finding these people in such critical timing. Training AI models on similar scenarios to fasten the process can be beneficial, but the lack of data is an obstacle in the way of development such models. In this paper, we propose using image generating AI systems to generate remote sensing datasets that are difficult to collect using normal imagery, thus creating more efficient image classification systems that can be used in scenarios such as locating missing people. Several AI models are discussed in this paper: Dall-E 2, Stable Diffusion and Midjourney, where they are found to vary a lot in terms of the generated images, that could be because of the architecture of the model, and the data they trained on. The overall performance of the AI models is promising. Dall-E 2 performed the best in our tests, followed by Stable Diffusion, and finally Midjourney. This research could open the door to using such models in generating lots of datasets, which might solve crucial problems.","","978-1-6654-5600-5","10.1109/ICECTA57148.2022.9990245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9990245","Generative Adversarial Networks;Diffusion Models;Remote Sensing;Datasets","Training;Satellites;Neural networks;Computer architecture;Data models;Radar polarimetry;Timing","artificial intelligence;image classification;image recognition;neural nets;radar imaging;remote sensing;synthetic aperture radar","Dall-E 2;data classification;efficient image classification systems;exotic remote sensing datasets;image generating AI;Midjourney;missing people;neural networks;normal imagery;remote sensing data;SAR imagery;stable diffusion;training AI models","","","","28","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Hyperspectral Image Classification Approach Based on Wasserstein Generative Adversarial Networks","N. Chen; C. Li","Anhui Administration of Linhuaigang Flood Control Engineering, Hefei, China; College of Computer and Information, Hohai University, Nanjing, China","2020 International Conference on Machine Learning and Cybernetics (ICMLC)","5 Jul 2021","2020","","","53","63","Hyperspectral image classification is an important research direction in the application of remote sensing technology. In the process of labeling different types of objects based on spectral information and geometric spatial characteristics, noise interference often exists in continuous multi-band spectral information, which brings great troubles to spectral feature extraction. Besides, far from enough spectral samples will restrict the classification performance of the algorithm to some extent. In order to solve the problem of small amount of original spectral sample data and noisy signal, Wasserstein generative adversarial networks (WGAN) is used to generate samples similar to the original spectrum, and spectral features are extracted from the samples. In the case of small samples, the original materials are provided for the classification of hyperspectral images and a semi-supervised classification model WGAN-CNN for hyperspectral images based on Wasserstein generation antagonistic network is proposed in this paper. This model combines with CNN classifier and completes the classification of terrain objects according to the label for the synthesized samples. The proposed method is compared with several classical hyperspectral image classification methods in classification accuracy. WGAN-CNN can achieve higher classification accuracy in the case of small sample size, which proves the effectiveness of the proposed method.","2160-1348","978-1-6654-1943-7","10.1109/ICMLC51923.2020.9469586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9469586","Hyperspectral images classification;WGAN;CNN;Semisupervised lea1rning","Training;Support vector machines;Machine learning;Interference;Feature extraction;Generative adversarial networks;Noise measurement","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;remote sensing","remote sensing technology;geometric spatial characteristics;noise interference;continuous multiband spectral information;spectral feature extraction;classification performance;Wasserstein generative adversarial networks;spectral features;hyperspectral images;semisupervised classification model WGAN-CNN;Wasserstein generation antagonistic network;hyperspectral image classification approach;spectral sample data;CNN classifier","","","","12","IEEE","5 Jul 2021","","","IEEE","IEEE Conferences"
"Semisupervised Multiscale Generative Adversarial Network for Semantic Segmentation of Remote Sensing Image","J. Wang; B. Liu; Y. Zhou; J. Zhao; S. Xia; Y. Yang; M. Zhang; L. M. Ming","Engineering Research Center of Mine Digitization, Ministry of Education of the Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the Peoples Republic of China, Xuzhou, China; School of Mechatronic Engineering, Jiangsu Normal University, Xuzhou, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Semantic segmentation of remote sensing images based on deep neural networks has gained wide attention recently. Although many methods have achieved amazing performance, they need large amounts of labeled images to distinguish the differences in angle, color, size, and other aspects for small targets in remote sensing data sets. However, with a few labeled images, it is difficult to extract the key features of small targets. We propose a semisupervised multiscale generative adversarial network (GAN), which not only utilizes the multipath input and atrous spatial pyramid pooling (ASPP) module but leverages unlabeled images and semisupervised learning strategy to improve the performance of small target segmentation in semantic segmentation when labeled data amount is small. Experimental results show that our model outperforms state-of-the-art methods with insufficient labeled data.","1558-0571","","10.1109/LGRS.2020.3036823","National Natural Science Foundation of China(grant numbers:61801198); Natural Science Foundation of Jiangsu Province, China(grant numbers:BK20180174); Jiangsu Province Post-Doctoral Fund(grant numbers:2020Z241); Natural Science Foundation of China(grant numbers:61806206); Natural Science Foundation of Jiangsu Province, China(grant numbers:BK20180639); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274510","Generative adversarial network (GAN);multiscale;remote sensing;semantic segmentation;semisupervised","Image segmentation;Semantics;Remote sensing;Feature extraction;Generative adversarial networks;Gallium nitride;Training","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image colour analysis;image segmentation;remote sensing;semi-supervised learning (artificial intelligence)","remote sensing datasets;semisupervised multiscale generative adversarial network;atrous spatial pyramid pooling module;semisupervised learning strategy;small target segmentation;semantic segmentation;remote sensing image;deep neural networks;ASPP module;feature extraction","","1","","30","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"Face Inpainting via Nested Generative Adversarial Networks","Z. Li; H. Zhu; L. Cao; L. Jiao; Y. Zhong; A. Ma","School of Printing and Packaging, Wuhan University, Wuhan, China; School of Printing and Packaging, Wuhan University, Wuhan, China; School of Printing and Packaging, Wuhan University, Wuhan, China; Department of Information and Communication Technology, University of Agder, Grimstad, Norway; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Access","1 Nov 2019","2019","7","","155462","155471","Face inpainting aims to repaired damaged images caused by occlusion or cover. In recent years, deep learning based approaches have shown promising results for the challenging task of image inpainting. However, there are still limitation in reconstructing reasonable structures because of over-smoothed and/or blurred results. The distorted structures or blurred textures are inconsistent with surrounding areas and require further post-processing to blend the results. In this paper, we present a novel generative model-based approach, which consisted by nested two Generative Adversarial Networks (GAN), the sub-confrontation GAN in generator and parent-confrontation GAN. The sub-confrontation GAN, which is in the image generator of parent-confrontation GAN, can find the location of missing area and reduce mode collapse as a prior constraint. To avoid generating vague details, a novel residual structure is designed in the sub-confrontation GAN to deliver richer original image information to the deeper layers. The parent-confrontation GAN includes an image generation part and a discrimination part. The discrimination part of parent-confrontation GAN includes global and local discriminator, which benefits the reconstruction of overall coherency of the repaired image while obtaining local details. The experiments are executed over the publicly available dataset CelebA, and the results show that our method outperforms current state-of-the-art techniques quantitatively and qualitatively.","2169-3536","","10.1109/ACCESS.2019.2949614","National Basic Research Program of China (973 Program)(grant numbers:2018YFB10046,2017YFB0504202); Northwestern Polytechnical University(grant numbers:2042018kf0229); National Natural Science Foundation of China(grant numbers:41671441); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8883161","Face inpainting;deep neural network;nested GAN","Generative adversarial networks;Generators;Semantics;Face;Image reconstruction;Gallium nitride;Image restoration","face recognition;image reconstruction;image restoration;image texture;learning (artificial intelligence);neural nets","CelebA dataset;global discriminator;local discriminator;image discrimination part;residual structure design;mode collapse reduction;generator GAN;blurred textures;distorted structures;structure reconstruction;nested generative adversarial networks;image generation part;image generator;parent-confrontation GAN;sub-confrontation GAN;generative model-based approach;deep learning based approaches;face inpainting","","7","","44","CCBY","25 Oct 2019","","","IEEE","IEEE Journals"
"Can We Generate Good Samples for Hyperspectral Classification? — A Generative Adversarial Network Based Method","Y. Xu; B. Du; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P. R. China; School of Computer, Wuhan University, Wuhan, P. R. China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, P. R. China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5752","5755","The insufficiency of training samples is really a great challenge for hyperspectral image (HSI) classification. Samples generation is a commonly used technique in deep learning based remote sensing field which can extend the training set. However, previous methods ignore the real distribution of the training samples in the feature space and thus can hardly ensure that the generated samples possess the same patterns with the real ones. In this paper, we propose a generative adversarial network based method (SpecGAN) to handle this problem. Different from traditional GAN framework where the generated samples have no categories, for the first time we take the label information into consideration for hyperspectral images. Feeding a random noise z and a class label vector y into the generator, we can get a spectral sample of the corresponding category. The experiments on the Pavia University data set demonstrate the potential of the proposed SpecGAN in spectral samples generation.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519295","Sample generation;deep learning;generative adversarial network;hyperspectral image classification","Training;Generators;Generative adversarial networks;Data visualization;Gallium nitride;Hyperspectral imaging","geophysical image processing;image classification;learning (artificial intelligence);pattern classification;remote sensing","hyperspectral image classification;remote sensing field;hyperspectral images;spectral sample;generative adversarial network based method;deep learning based remote sensing field;SpecGAN","","7","","10","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"On the possibility of conditional adversarial networks for multi-sensor image matching","N. Merkle; P. Fischer; S. Auer; R. Müller","German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2633","2636","A major research area in remote sensing is the problem of multi-sensor data fusion. Especially the combination of images acquired by different sensor types, e.g. active and passive, is a difficult task. Over the last years deep learning methods have proven their high potential for remote sensing applications. In this paper we will show how a deep learning method can be valuable for the problem of optical and SAR image matching. We investigate the possible of conditional generative adversarial networks (cGANs) for the generation of artificial templates. Contrary to common template generation approaches for image matching, the generation of templates using cGANs does not require the extraction of features. Our results show the possibility of realistic SAR-like template generation from optical images through cGANs and the potential of these templates for enhancing the matching of optical and SAR images by means of reliability and accuracy.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127535","conditional GANs;deep learning;image matching;multi-sensor;artificial template generation","Optical imaging;Adaptive optics;Optical sensors;Feature extraction;Synthetic aperture radar;Image matching;Gallium nitride","feature extraction;image fusion;image matching;learning (artificial intelligence);radar imaging;remote sensing;sensor fusion;synthetic aperture radar","artificial templates;optical images;multisensor image matching;research area;multisensor data fusion;remote sensing applications;deep learning method;optical SAR image matching;conditional generative adversarial networks;template generation;cGAN","","21","","10","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"SMAPGAN: Generative Adversarial Network-Based Semisupervised Styled Map Tile Generation Method","X. Chen; S. Chen; T. Xu; B. Yin; J. Peng; X. Mei; H. Li","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","21 Apr 2021","2021","59","5","4388","4406","Traditional online map tiles, which are widely used on the Internet, such as by Google Maps and Baidu Maps, are rendered from vector data. The timely updating of online map tiles from vector data, for which generation is time-consuming, is a difficult mission. Generating map tiles over time from remote sensing images is relatively simple and can be performed quickly without vector data. However, this approach used to be challenging or even impossible. Inspired by image-to-image translation (img2img) techniques based on generative adversarial networks (GANs), we proposed a semisupervised generation of styled map tiles based on the GANs (SMAPGAN) model to generate styled map tiles directly from remote sensing images. In this model, we designed a semisupervised learning strategy to pretrain SMAPGAN on rich unpaired samples and fine-tune it on limited paired samples in reality. We also designed the image gradient L1 loss and the image gradient structure loss to generate a styled map tile with global topological relationships and detailed edge curves for objects, which are important in cartography. Moreover, we proposed the edge structural similarity index (ESSI) as a metric to evaluate the quality of the topological consistency between the generated map tiles and ground truth. The experimental results show that SMAPGAN outperforms state-of-the-art (SOTA) works according to the mean squared error, the structural similarity index, and the ESSI. Also, SMAPGAN gained higher approval than SOTA in a human perceptual test on the visual realism of cartography. Our work shows that SMAPGAN is a new tool with excellent potential for producing styled map tiles. Our implementation of SMAPGAN is available at https://github.com/imcsq/SMAPGAN.","1558-0644","","10.1109/TGRS.2020.3021819","Advance Research Projects of Civil Aerospace Technology, Intelligent Distribution Technology of Domestic Satellite Information(grant numbers:B0301); National Natural Science Foundation of China(grant numbers:41871276,41871364,41871302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200723","Generative adversarial networks (GANs);map tiles generation;quality assessment;semisupervised","Remote sensing;Gallium nitride;Generators;Measurement;Indexes;Generative adversarial networks;Image edge detection","cartography;geographic information systems;geophysical image processing;Internet;mean square error methods;neural nets;remote sensing;supervised learning","remote sensing images;SMAPGAN;image gradient structure loss;generated map tiles;Google Maps;Baidu Maps;vector data;image-to-image translation techniques;online map tiles;generative adversarial network-based semisupervised styled map tile generation method;Internet;semisupervised learning strategy;image gradient L1 loss;global topological relationships;detailed edge curves;edge structural similarity index;mean squared error;ESSI;SOTA","","9","","44","IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"SWCGAN: Generative Adversarial Network Combining Swin Transformer and CNN for Remote Sensing Image Super-Resolution","J. Tu; G. Mei; Z. Ma; F. Piccialli","School of Engineering and Technology, China University of Geosciences, Beijing, China; School of Engineering and Technology, China University of Geosciences, Beijing, China; School of Engineering and Technology, China University of Geosciences, Beijing, China; Department of Mathematics and Applications “R. Caccioppoli”, University of Naples Federico II, Napoli, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Jul 2022","2022","15","","5662","5673","Easy and efficient acquisition of high-resolution remote sensing images is of importance in geographic information systems. Previously, deep neural networks composed of convolutional layers have achieved impressive progress in super-resolution reconstruction. However, the inherent problems of the convolutional layer, including the difficulty of modeling the long-range dependency, limit the performance of these networks on super-resolution reconstruction. To address the abovementioned problems, we propose a generative adversarial network (GAN) by combining the advantages of the swin transformer and convolutional layers, called SWCGAN. It is different from the previous super-resolution models, which are composed of pure convolutional blocks. The essential idea behind the proposed method is to generate high-resolution images by a generator network with a hybrid of convolutional and swin transformer layers and then to use a pure swin transformer discriminator network for adversarial training. In the proposed method, first, we employ a convolutional layer for shallow feature extraction that can be adapted to flexible input sizes; second, we further propose the residual dense swin transformer block to extract deep features for upsampling to generate high-resolution images; and third, we use a simplified swin transformer as the discriminator for adversarial training. To evaluate the performance of the proposed method, we compare the proposed method with other state-of-the-art methods by utilizing the UCMerced benchmark dataset, and we apply the proposed method to real-world remote sensing images. The results demonstrate that the reconstruction performance of the proposed method outperforms other state-of-the-art methods in most metrics.","2151-1535","","10.1109/JSTARS.2022.3190322","National Natural Science Foundation of China(grant numbers:11602235); Fundamental Research Funds for China Central Universities(grant numbers:2652021053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829280","Convolutional layers;generative adversarial network (GAN);remote sensing images;super-resolution reconstruction;swin transformer","Feature extraction;Superresolution;Transformers;Remote sensing;Image reconstruction;Generative adversarial networks;Task analysis","convolution;feature extraction;geographic information systems;geophysical image processing;image reconstruction;image resolution;learning (artificial intelligence);neural nets;remote sensing;unsupervised learning","real-world remote sensing images;simplified swin transformer;residual dense swin transformer block;adversarial training;pure swin transformer discriminator network;transformer layers;generator network;high-resolution images;pure convolutional blocks;previous super-resolution models;super-resolution reconstruction;convolutional layer;deep neural networks;high-resolution remote sensing images;efficient acquisition;easy acquisition;remote sensing image super-resolution;generative adversarial network combining swin transformer","","3","","44","CCBYNCND","13 Jul 2022","","","IEEE","IEEE Journals"
"Agriculture Land Appraisal with Use of Remote Sensing and Infrastructure Data","N. Kussul; A. Shelestov; H. Yailymova; L. Shumilo; S. Drozd","Space Research Institute NASU-SSAU, Kyiv, Ukraine; Space Research Institute NASU-SSAU, Kyiv, Ukraine; Space Research Institute NASU-SSAU, Kyiv, Ukraine; Department of Geographical Sciences, University of Maryland, College Park, MD, USA; National Technical University of Ukraine “Igor Sikorsky Kiev Polytechnic Institute”, Kyiv, Ukraine","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2785","2788","1st July 2021 the law on the creation of land market start effect in Ukraine. As a result, land appraisal became cornerstone task in Ukrainian agriculture sector. The official methodology on land appraisal includes use of soil fertility characteristics combined with coefficients related to the distance to the infrastructure objects or settlements and placing of field in specific functional areas, like recreational, or areas with high level of radiation pollution. In this study we collected open source infostructure geospatial information and characteristics of fields obtained from remote sensing data - crop types and Normalized Difference Vegetation Index to build land price predictive model trained on the official land market information. This work designed to investigate potential of geo-informational technologies and remote sensing in the land appraisal use. We separated all available ground truth land price data into three groups by fields size - very small, small, medium and big. We found different relationships between field characteristics and prices. For very small fields the most important features are area, altitude, slope, bonitet and distances to elevators, villages and roads. For small fields the most important are bonitet, altitude, area and distances to cities and roads. For medium and big field's area, slope, distance to cities, roads and historical NDVI.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884045","deep learning;Generative Adversarial Networks;GAN;super-resolution;Sentinel-2","Pollution;Roads;Urban areas;Vegetation mapping;Soil;Predictive models;Agriculture","agriculture;crops;geographic information systems;land use planning;pricing;remote sensing;soil;vegetation;vegetation mapping","cornerstone task;Ukrainian agriculture sector;official methodology;soil fertility characteristics;infrastructure objects;settlements;specific functional areas;open source infostructure geospatial information;remote sensing data - crop types;Normalized Difference Vegetation Index;land price predictive model;official land market information;geo-informational technologies;land appraisal use;available ground truth land price data;fields size;field characteristics;bonitet;big field;agriculture land appraisal;infrastructure data;st July;land market start effect","","","","15","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Using Gans to Augment Data for Cloud Image Segmentation Task","M. Jain; C. Meegan; S. Dev","School of Computer Science, University College Dublin, Ireland; School of Computer Science, University College Dublin, Ireland; School of Computer Science, University College Dublin, Ireland","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3452","3455","While cloud/sky image segmentation has extensive real-world applications, a large amount of labelled data is needed to train a highly accurate models to perform the task. Scarcity of such volumes of cloud/sky images with corresponding ground-truth binary maps makes it highly difficult to train such complex image segmentation models. In this paper, we demonstrate the effectiveness of using Generative Adversarial Networks (GANs) to generate data to augment the training set in order to increase the prediction accuracy of image segmentation model. We further present a way to estimate ground-truth binary maps for the GAN-generated images to facilitate their effective use as augmented images. Finally, we validate our work with different statistical techniques.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554993","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554993","WSI;Cloud Image Segmentation;GAN;Data Augmentation","Training;Image segmentation;Geoscience and remote sensing;Predictive models;Generative adversarial networks;Data models;Task analysis","image segmentation;neural nets;statistical analysis","cloud image segmentation task;real-world applications;generative adversarial networks;GAN-generated images;images augmentation;ground-truth binary maps;cloud-sky images;statistical techniques","","4","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Conditional Generative Adversarial Network-Based Training Sample Set Improvement Model for the Semantic Segmentation of High-Resolution Remote Sensing Images","X. Pan; J. Zhao; J. Xu","Jilin Provincial Key Laboratory of Changbai Historical Culture and VR Reconstruction Technology, Changchun, China; Jilin Provincial Key Laboratory of Changbai Historical Culture and VR Reconstruction Technology, Changchun, China; Jilin Provincial Key Laboratory of Changbai Historical Culture and VR Reconstruction Technology, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2021","2021","59","9","7854","7870","To achieve high segmentation quality, deep semantic segmentation neural networks (DSSNNs) need to be trained on diverse direction, location, and neighboring category combinations for each pixel in the input-output image patches. Achieving this goal requires a large sample set. However, in many practical application scenarios, a very large training sample set is too expensive to achieve, or insufficient remote sensing image data are available. These limitations directly affect the quality of the results of DSSNNs. To address the above-mentioned problem, this article proposes a conditional generative adversarial network (CGAN)-based training sample set improvement model (CGAN-TSIM) for the semantic segmentation of high-resolution remote sensing images. In CGAN-TSIM, the generator model of the CGAN can generate a sample image when a ground-truth image is an input as a “condition.” A condition generation mechanism is designed to create ground-truth images, and these ground-truth conditions are used to drive the CGAN to generate samples containing more diverse object combinations, directions, and locations. These generated images can be added to the original training sample set to improve their spatial information diversity. Rather than simply relying on passively finding samples that contain diverse spatial information, CGAN-TSIM extracts high-level spatial information from the original training images and actively generates new sample images. Experiments show that the samples generated by CGAN-TSIM can improve the quality of the sample set. Compared with other traditional methods, CGAN-TSIM enables better classification accuracy.","1558-0644","","10.1109/TGRS.2020.3033816","National Natural Science Foundation of China(grant numbers:41871236,41971193); Foundation of Jilin Provincial Science and Technology Department(grant numbers:20180101020JC,20200403174SF); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9250622","Condition generation;generative adversarial network (GAN);high-resolution remote sensing classification;sample generation;semantic segmentation","Remote sensing;Image segmentation;Training;Semantics;Generative adversarial networks;Feature extraction;Generators","deep learning (artificial intelligence);feature extraction;image enhancement;image resolution;image segmentation;remote sensing","deep semantic segmentation neural networks;input-output image patches;conditional generative adversarial network-based training sample set improvement model;high-resolution remote sensing images;ground-truth image;condition generation mechanism;ground-truth conditions;CGAN-TSIM;spatial information extraction","","7","","51","IEEE","6 Nov 2020","","","IEEE","IEEE Journals"
"Hyperspectral Remote Sensing Imagery Generation From RGB Images Based on Joint Discrimination","L. Liu; S. Lei; Z. Shi; N. Zhang; X. Zhu","Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China; Shanghai Aerospace Electronic Technology Institute, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Aug 2021","2021","14","","7624","7636","Spatial resolution and spectral resolution both play an important role in the recognition of objects in hyperspectral remote sensing. However, the imaging characteristics of hyperspectral images (HSIs) result in a mutually restrictive relationship between the spatial and spectral resolutions. Generative adversarial networks (GANs) have achieved significant success in image generation. The introduce of the discriminators plays a key role in improving the reality. In this article, we propose an RGB to multiband hyperspectral imagery (150 bands) generation method based on GAN (R2HGAN). The method solves the high ill-posed problem and introduces high spectral resolution into RGB images by learning from multiple scenes of HSI. In R2HGAN, we extend the adversarial learning from spatial to spectral dimensions and joint discrimination is designed to generate HSIs closer to the real ones, where two discriminators (the conditional D and the spectral D) are put forward to supervise the spectral similarity and the conditional reality of the HSI jointly. In detail, the conditional discriminator comprehensively judges the quality of each area in the reconstructed HSI. At the same time, to ensure that the generated spectra are close to the real ones, a spectral discriminator based on multilayer perceptron is designed. Through the experiments on GF-5 imagery, the method has significantly improved the quality of the generated images over other state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2021.3099242","National Key R&D Program of China(grant numbers:2019YFC1510905); National Natural Science Foundation of China(grant numbers:61671037); Beijing Natural Science Foundation(grant numbers:4192034); Shanghai Association for Science and Technology(grant numbers:SAST2020077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495279","Generation adversarial network (GAN);hyperspectral image (HSI);remote sensing;spectral superresolution (SSR)","Spatial resolution;Superresolution;Generative adversarial networks;Image reconstruction;Hyperspectral imaging;Imaging;Earth","geophysical image processing;hyperspectral imaging;image classification;image colour analysis;image sensors;learning (artificial intelligence);multilayer perceptrons;remote sensing","hyperspectral remote sensing imagery generation;RGB images;joint discrimination;spatial resolution;imaging characteristics;hyperspectral images result;mutually restrictive relationship;spatial resolutions;spectral resolutions;generative adversarial networks;GAN;significant success;image generation;discriminators;hyperspectral imagery generation method;R2HGAN;high spectral resolution;HSI;spectral dimensions;spectral similarity;conditional reality;conditional discriminator comprehensively judges;generated spectra;spectral discriminator;GF-5 imagery","","5","","85","CCBY","26 Jul 2021","","","IEEE","IEEE Journals"
"Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","M. Jiang; H. Shen; J. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2022","2022","60","","1","15","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal–spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.","1558-0644","","10.1109/TGRS.2022.3188998","National Natural Science Foundation of China(grant numbers:42130108,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829539","Deep residual cycle generative adversarial network (GAN);heterogeneous integrated framework;land-cover change;thick cloud cover","Generators;Remote sensing;Spatial resolution;Generative adversarial networks;Feature extraction;Image fusion;Optical sensors","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;sensor fusion;spatiotemporal phenomena","imaging degradation process;heterogeneous integrated fusion framework;complementary spatial information;temporal, information;spectral information;multisource heterogeneous observations;heterogeneous spatiospectral fusion;spatiotemporal fusion;heterogeneous spatiotemporal-spectral fusion;low-resolution imaging;cloud contamination;different remote sensing satellites;Moderate Resolution Imaging Spectroradiometer;image fusion method;spatio-temporal-spectral integrated fusion;heterogeneous remote sensing images;multisource remote sensing images;heterogeneous images;heterogeneous integrated framework;deep residual cycle generative adversarial network;forward fusion part;backward degeneration feedback part;forward part;desired fusion result","","","","54","IEEE","14 Jul 2022","","","IEEE","IEEE Journals"
"An Unsupervised Remote Sensing Single-Image Super-Resolution Method Based on Generative Adversarial Network","N. Zhang; Y. Wang; X. Zhang; D. Xu; X. Wang","School of Optoelectronics, University of Chinese Academy of Sciences, Beijing, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun, China; School of Optoelectronics, University of Chinese Academy of Sciences, Beijing, China; School of Optoelectronics, University of Chinese Academy of Sciences, Beijing, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun, China","IEEE Access","14 Feb 2020","2020","8","","29027","29039","Image super-resolution (SR) technique can improve the spatial resolution of images without upgrading the imaging system. As a result, SR promotes the development of high resolution (HR) remote sensing image applications. Many remote sensing image SR algorithms based on deep learning have been proposed recently, which can effectively improve the spatial resolution under the constraints of HR images. However, images acquired by remote sensing imaging devices typically have lower resolution. Hence, an insufficient number of HR remote sensing images are available for training deep neural networks. In view of this problem, we propose an unsupervised SR method that does not require HR remote sensing images. The proposed method introduces a generative adversarial network (GAN) that obtains SR images through the generator; then, the SR images are downsampled to train the discriminator with low resolution (LR) images. Our method outperformed several methods in terms of the quality of the obtained SR images as measured by 6 evaluation metrics, which proves the satisfactory performance of the proposed unsupervised method for improving the spatial resolution of remote sensing images.","2169-3536","","10.1109/ACCESS.2020.2972300","National Natural Science Foundation of China(grant numbers:11703027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8986554","Image super-resolution;unsupervised learning;remote sensing;generative adversarial network","Remote sensing;Image reconstruction;Generative adversarial networks;Training;Gallium nitride","convolutional neural nets;geophysical image processing;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","HR remote sensing images;generative adversarial network;SR images;spatial resolution;unsupervised remote sensing single-image super-resolution method;super-resolution technique;imaging system;high resolution remote sensing image applications;remote sensing image SR algorithms;HR images;remote sensing imaging devices;unsupervised SR method","","10","","90","CCBY","7 Feb 2020","","","IEEE","IEEE Journals"
"Localizing Microseismic Events Using Semi-Supervised Generative Adversarial Networks","Q. Feng; L. Han; B. Zhao","College of Geo-Exploration Science and Technology, Jilin University, Changchun, China; College of Geo-Exploration Science and Technology, Jilin University, Changchun, China; College of Geo-Exploration Science and Technology, Jilin University, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","8 Dec 2022","2022","60","","1","8","The performance of the microseismic monitoring technique depends greatly on the accuracy of microseismic event localization. Recently, machine learning (ML) methods have been extensively implemented for the localization of microseismic events. These neural networks are typically trained using numerous microseismic events labeled with known source locations. Obtaining enough microseismic events with good source locations can be difficult and costly. To overcome this shortcoming, we present a microseismic events localization method using semi-supervised generative adversarial networks (GANs). We utilize limited labeled seismograms and large amounts of unlabeled seismograms to train the semi-supervised GANs, thus improving the prediction ability of the networks. Finally, we evaluate the performance of the proposed method using synthetic microseismic data and field data. Comparison with the supervised learning methods on the same microseismic data shows that the proposed method can significantly improve the accuracy of locating microseismic sources in the lack of sufficient source labels.","1558-0644","","10.1109/TGRS.2022.3225415","National Natural Science Foundation of China(grant numbers:42130805,42074154); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965437","Event location;generative adversarial networks (GANs);microseismic monitoring;semi-supervised learning","Location awareness;Position measurement;Generators;Data models;Training;Predictive models;Neural networks","geophysical techniques;geophysics computing;learning (artificial intelligence);neural nets;seismology;supervised learning;unsupervised learning","machine learning methods;microseismic data shows;microseismic event localization;microseismic events localization method;microseismic monitoring technique;microseismic sources;neural networks;semisupervised GANs;semisupervised generative adversarial networks;supervised learning methods;synthetic microseismic data","","1","","33","IEEE","28 Nov 2022","","","IEEE","IEEE Journals"
"Temporal Mapping of Hyperspectral Data","R. Fick; P. Gader; A. Zare; S. Meerdink","Department of Computer and Information Science and Engineering, University of Florida; Department of Computer and Information Science and Engineering, University of Florida; Department of Electrical and Computer Engineering, University of Florida; Department of Computer and Information Science and Engineering, Department of Electrical and Computer Engineering, University of Florida","2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","5 Dec 2019","2019","","","1","4","The increasing popularity of hyperspectral sensors is dramatically increasing the temporal availability of data. To date, algorithms struggle to compare hyperspectral data collected across dates due to different environmental conditions during collection. In this work, we develop a temporal mapping in order to map data collected from one year to a different year. We investigated both conditional generative adversarial networks (cGANs) as well as affine transformations to perform this mapping. Both methods showed an improvement over using data from past collections without mapping, with cGANs outperforming the affine transformation.","2158-6276","978-1-7281-5294-3","10.1109/WHISPERS.2019.8921373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8921373","","Training;Generative adversarial networks;Generators;Testing;Hyperspectral imaging;Gallium nitride","affine transforms;geophysical image processing;remote sensing","temporal mapping;hyperspectral data;hyperspectral sensors;temporal availability;algorithms struggle;different year;conditional generative adversarial networks;affine transformation","","2","","9","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Dismon-Gan: 24×7 All-Weather Optical Domain Surveillance Using Progressively Growing Adversarial Networks with Patch Discriminator","R. Gandikota; D. Mishra","National Remote Sensing Center, Indian Space Research Organization, Hyderabad, India; Department of Space, Indian Institute of Space Science and Technology (IIST), Thiruvananthapuram, India","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","501","504","Cyclones and floods are significant disasters seen by south- Asian countries. With cloud occlusions on the affected regions, monitoring the disaster becomes tedious. Synthetic Aperture Radars (SAR) can penetrate through the clouds (due to their microwave frequencies) and image the area under- Being active sensors, they also can image around the clock. These properties enable the domain experts to use SAR images for disaster monitoring; however, even professionals find it challenging to interpret the data since the human eye is unfamiliar with the impact of distance-dependent imaging, signal intensities observed in the radar spectrum, and image features associated to speckle or postprocessing procedures. This manuscript exploits the valuable imaging properties of SAR images to propose a Generative Adversarial Network (GAN) to synthesize realistic and semantic optical images by conditioning them over the microwave satellite images. It en- the model to synthesize optical images of an affected area in all weather and around the clock conditions, making it a critical disaster monitoring assistance tool.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884051","Generative Adversarial Networks;Synthetic Aperture Radar;Disaster Monitoring;Risat;Cartosat","Satellite broadcasting;Radar imaging;Optical imaging;Generative adversarial networks;Adaptive optics;Radar polarimetry;Optical sensors","disasters;geophysical image processing;geophysical techniques;meteorological radar;neural nets;optical images;radar imaging;search radar;synthetic aperture radar","all-weather optical domain surveillance;patch discriminator;cloud occlusions;Synthetic Aperture Radars;microwave frequencies;active sensors;SAR images;distance-dependent imaging;radar spectrum;image features;imaging properties;Generative Adversarial Network;realistic images;semantic optical images;microwave satellite images;critical disaster monitoring assistance tool;Dismon-GAN;south-Asian countries","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Fully Convolutional Semi-Supervised Gan for Polsar Classification","M. Liu; Y. Hu; S. Wang; Y. Guo; B. Hou; L. Jiao; X. Hou","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","621","624","We propose a novel semi -supervised fully convolutional network for Polarimetric synthetic aperture radar (PoISAR) terrain classification. First, by designing a fully convolutional structure, we can perform pixel-based classification tasks. Then, by applying semi -supervised generative adversarial networks (GANs), we utilize both labeled and unlabeled samples and aim to obtain higher classification accuracy. Through a mini-max two-player game, GAN has better performance than other “single-player” classifiers. Finally, we combine the fully convolutional structure with the semi-supervised GAN. Our fully convolutional semi-supervised GAN (FC-SGAN) has excellent spatial feature learning ability and can perform end-to-end pixel-based classification tasks. Experimental results show that compared with existing works, the proposed method has better performances. Even when the training set gets smaller, our method keeps high accuracy.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519541","terrain classification;fully convolutional network;semi -supervised learning;generative adversarial network","Gallium nitride;Task analysis;Generative adversarial networks;Feature extraction;Training;Convolution;Generators","game theory;geophysical image processing;image classification;learning (artificial intelligence);radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar;terrain mapping","Polarimetric synthetic aperture radar terrain classification;fully convolutional structure;mini-max two-player game;end-to-end pixel-based classification tasks;semisupervised generative adversarial networks;classification accuracy;polSAR classification;FC;spatial feature learning ability;FC-SGAN;single-player classifiers;semisupervised fully convolutional network;convolutional semisupervised GAN","","","","13","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"DE-CycleGAN: An Object Enhancement Network for Weak Vehicle Detection in Satellite Images","P. Gao; T. Tian; L. Li; J. Ma; J. Tian","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Apr 2021","2021","14","","3403","3414","Vehicle detection is a very important application of remote sensing. However, suffering from the low acutance and insufficient color information, the detection of weak vehicles in satellite imagery still remains a challenge. Image enhancement can improve the visual effects of remote sensing images. Nevertheless, most existing image enhancement methods aim to improve the quality of the entire image without target guidance, which have ambiguous contributions to the detection performance. Methods based on generative adversarial networks (GANs) have realized image enhancement with target guidance by the addition of target-guided branches, but paired training data is not available in some scenarios. In this article, a novel model of detection-guided CycleGAN (DE-CycleGAN) is proposed to enhance the weak targets for the purpose of accurate vehicle detection, where a backbone GAN with a target-guided branch is learned in the absence of paired images. Specifically, enhancements of two levels are mutually executed. At the image level, the color information of the entire satellite image is enriched by refined CycleGAN, and its sharpness is enhanced by the gradient enhancement model. At the object level, the target-guided branch for detection is added to enhance features of the target. The experimental results validate that the detection performance has been significantly improved on the images enhanced by the proposed DE-CycleGAN model, which shows a positive effect on weak target detection.","2151-1535","","10.1109/JSTARS.2021.3062057","National Natural Science Foundation of China(grant numbers:42071339); Fund of Key Laboratory(grant numbers:6142113180201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363513","CycleGAN;generative adversarial networks (GANs);object enhancement;target-guided branch;weak vehicle detection","Task analysis;Generative adversarial networks;Image enhancement;Image color analysis;Vehicle detection;Satellites;Object detection","image colour analysis;image enhancement;neural nets;object detection;remote sensing","weak target detection;DE-CycleGAN model;gradient enhancement model;image level;detection-guided CycleGAN;target-guided branch;generative adversarial networks;image enhancement;remote sensing images;satellite images;weak vehicle detection;object enhancement network","","11","","34","CCBY","25 Feb 2021","","","IEEE","IEEE Journals"
"Extension of Image Data Using Generative Adversarial Networks and Application to Identification of Aurora","A. Uchino; M. Matsumoto","NTT DOCOMO, Inc., Tokyo, Japan; Department of Informatics, The University of Electro-Communications, Tokyo, Japan","IEEE Geoscience and Remote Sensing Letters","26 Oct 2021","2021","18","11","1941","1945","In recent years, automatic auroral image classification has been actively investigated. The baseline method has relied on supervised learning. As this approach requires a large amount of labeled teacher data, it is necessary to collect the data manually and label them, which is a time-consuming task. In this study, we proposed a method to extend an image data set by inputting training images into a deep convolutional generative adversarial network (DCGAN) and generating images in this manner. The proposed approach implied using both generated and original images to train the classifier. It could reduce the number of labeling operations performed manually. As an evaluation experiment, we performed classifier learning on the data sets before and after extension and confirmed that the classification accuracy was improved because of training on the data set after the extension.","1558-0571","","10.1109/LGRS.2020.3012620","Support Center for Advanced Telecommunications Technology Research Foundation; Foundation for the Fusion of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165808","Data extension;generative adversarial networks (GANs);image classification","Magnetosphere;Ion radiation effects;Gallium nitride;Feature extraction;Training;Supervised learning;Training data","aurora;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);neural nets;pattern classification","automatic auroral image classification;baseline method;supervised learning;labeled teacher data;image data;inputting training images;deep convolutional generative adversarial network;generated images;original images;labeling operations;classifier learning;data sets;generative adversarial networks","","2","","13","IEEE","12 Aug 2020","","","IEEE","IEEE Journals"
"Selective Adversarial Adaptation-Based Cross-Scene Change Detection Framework in Remote Sensing Images","M. Yang; L. Jiao; B. Hou; F. Liu; S. Yang","School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","24 Feb 2021","2021","59","3","2188","2203","Supervised change detection methods always face a big challenge that the current scene (target domain) is fully unlabeled. In remote sensing, it is common that we have sufficient labels in another scene (source domain) with a different but related data distribution. In this article, we try to detect changes in the target domain with the help of the prior knowledge learned from multiple source domains. To achieve this goal, we propose a change detection framework based on selective adversarial adaptation. The adaptation between multisource and target domains is fulfilled by two domain discriminators. First, the first domain discriminator regards each scene as an individual domain and is designed for identifying the domain to which each input sample belongs. According to the output of the first domain discriminator, a subset of important samples is selected from multisource domains to train a deep neural network (DNN)-based change detection model. As a result, not only the positive transfer is enhanced but also the negative transfer is alleviated. Second, as for the second domain discriminator, all the selected samples are thought from one domain. Adversarial learning is introduced to align the distributions of the selected source samples and the target ones. Consequently, it further adapts the knowledge of change from the source domain to the target one. At the fine-tuning stage, target samples with reliable labels and the selected source ones are used to jointly fine-tune the change detection model. As the target domain is fully unlabeled, homogeneity- and boundary-based strategies are exploited to make the pseudolabels from a preclassification map reliable. The proposed method is evaluated on three SAR and two optical data sets, and the experimental results have demonstrated its effectiveness and superiority.","1558-0644","","10.1109/TGRS.2020.3001584","National Natural Science Foundation of China(grant numbers:U1701267); State Key Program of National Natural Science of China(grant numbers:61836009); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201); Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121326","Adaptation;adversarial learning;change detection;deep neural networks (DNNs);domain discriminator;remote sensing images","Feature extraction;Remote sensing;Radar polarimetry;Adaptation models;Synthetic aperture radar;Generative adversarial networks;Reliability","geophysical image processing;image classification;learning (artificial intelligence);neural nets;remote sensing","selective adversarial adaptation-based cross-scene change detection framework;remote sensing;supervised change detection methods;current scene;target domain;source domain;multiple source domains;domain discriminator;individual domain;multisource domains;deep neural network-based change detection model;selected source samples;target samples;selected source ones","","11","","62","IEEE","19 Jun 2020","","","IEEE","IEEE Journals"
"Single-Image Super-Resolution for Remote Sensing Images Using a Deep Generative Adversarial Network With Local and Global Attention Mechanisms","Y. Li; S. Mavromatis; F. Zhang; Z. Du; J. Sequeira; Z. Wang; X. Zhao; R. Liu","Zhejiang Provincial Laboratory of Geographic Information System (GIS), School of Earth Sciences, Zhejiang University, Hangzhou, China; Laboratory of Computer Science and Systems (LIS), The French National Centre for Scientific Research (CNRS), Aix-Marseille University, Marseille, France; Zhejiang Provincial Laboratory of Geographic Information System (GIS), School of Earth Sciences, Zhejiang University, Hangzhou, China; Zhejiang Provincial Laboratory of Geographic Information System (GIS), School of Earth Sciences, Zhejiang University, Hangzhou, China; Laboratory of Computer Science and Systems (LIS), The French National Centre for Scientific Research (CNRS), Aix-Marseille University, Marseille, France; City Intelligence, Cloud & AI, Huawei Technologies Company Ltd., Shenzhen, China; City Intelligence, Cloud & AI, Huawei Technologies Company Ltd., Shenzhen, China; Zhejiang Provincial Laboratory of Geographic Information System (GIS), School of Earth Sciences, Zhejiang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","13 Jan 2022","2022","60","","1","24","Super-resolution (SR) technology is an important way to improve spatial resolution under the condition of sensor hardware limitations. With the development of deep learning (DL), some DL-based SR models have achieved state-of-the-art performance, especially the convolutional neural network (CNN). However, considering that remote sensing images usually contain a variety of ground scenes and objects with different scales, orientations, and spectral characteristics, previous works usually treat important and unnecessary features equally or only apply different weights in the local receptive field, which ignores long-range dependencies; it is still a challenging task to exploit features on different levels and reconstruct images with realistic details. To address these problems, an attention-based generative adversarial network (SRAGAN) is proposed in this article, which applies both local and global attention mechanisms. Specifically, we apply local attention in the SR model to focus on structural components of the earth’s surface that require more attention, and global attention is used to capture long-range interdependencies in the channel and spatial dimensions to further refine details. To optimize the adversarial learning process, we also use local and global attentions in the discriminator model to enhance the discriminative ability and apply the gradient penalty in the form of hinge loss and loss function that combines <inline-formula> <tex-math notation=""LaTeX"">$L1$ </tex-math></inline-formula> pixel loss, <inline-formula> <tex-math notation=""LaTeX"">$L1$ </tex-math></inline-formula> perceptual loss, and relativistic adversarial loss to promote rich details. The experiments show that SRAGAN can achieve performance improvements and reconstruct better details compared with current state-of-the-art SR methods. A series of ablation investigations and model analyses validate the efficiency and effectiveness of our method.","1558-0644","","10.1109/TGRS.2021.3093043","National Key Research and Development Project(grant numbers:2018YFB0505000); Fundamental Research Funds for the Central Universities(grant numbers:2019QNA3013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9479919","Convolutional neural networks (CNNs);generative adversarial network (GAN);local and global attention module;remote sensing;single-image super super-resolution (SISR)","Remote sensing;Feature extraction;Image reconstruction;Spatial resolution;Signal processing algorithms;Biological system modeling;Generative adversarial networks","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image reconstruction;image resolution;remote sensing","single-image super-resolution;remote sensing images;deep generative adversarial network;global attention mechanisms;spatial resolution;deep learning;SR model;convolutional neural network;ground scenes;spectral characteristics;local receptive field;attention-based generative adversarial network;spatial dimensions;adversarial learning process;discriminator model;L1 pixel loss;L1 perceptual loss;relativistic adversarial loss;SRAGAN;gradient penalty;hinge loss;loss function","","4","","78","IEEE","12 Jul 2021","","","IEEE","IEEE Journals"
"Semisupervised Remote Sensing Image Fusion Using Multiscale Conditional Generative Adversarial Network With Siamese Structure","X. Jin; S. Huang; Q. Jiang; S. -J. Lee; L. Wu; S. Yao","Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Institute of Technology Management, National Chiao Tung University, Hsinchu, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Jul 2021","2021","14","","7066","7084","Remote sensing image fusion (RSIF) can generate an integrated image with high spatial and spectral resolution. The fused remote sensing image is conducive to applications including disaster monitoring, ecological environment investigation, and dynamic monitoring. However, most existing deep learning based RSIF methods require ground truths (or reference images) to train a model, and the acquisition of ground truths is a difficult problem. To address this, we propose a semisupervised RSIF method based on the multiscale conditional generative adversarial networks by combining the multiskip connection and pseudo-Siamese structure. This new method can simultaneously extract the features of panchromatic and multispectral images to fuse them without a ground truth; the adopted multiskip connection contributes to presenting image details. In addition, we propose a composite loss function, which combines the least squares loss, L1 loss, and peak signal-to-noise ratio loss to train the model; the composite loss function can help to retain the spatial details and spectral information of the source images. Moreover, we verify the proposed method by extensive experiments, and the results show that the new method can achieve outstanding performance without relying on the ground truth.","2151-1535","","10.1109/JSTARS.2021.3090958","National Natural Science Foundation of China(grant numbers:61863036,62002313); China Postdoctoral Science Foundation(grant numbers:2020T130564,2019M653507); Key Areas Research Program of Yunnan Province in China(grant numbers:202001BB050076); Key Laboratory in Software Engineering of Yunnan Province in China(grant numbers:2020SE408); Yunnan Provincial Postdoctoral Science Foundation; Yunnan University's Research Innovation Fund for Graduate Students(grant numbers:2020230,2020231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9461404","Conditional generative adversarial network (cGAN);deep learning (DL);image fusion;loss function;remote sensing image fusion (RSIF)","Image fusion;Spatial resolution;Remote sensing;Feature extraction;Generative adversarial networks;Sensors;Fuses","geophysical image processing;geophysical techniques;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing","semisupervised remote sensing image fusion;multiscale conditional generative adversarial network;Siamese structure;spatial resolution;spectral resolution;ecological environment investigation;dynamic monitoring;deep learning;RSIF methods;ground truth;semisupervised RSIF method;pseudoSiamese structure;panchromatic images;multispectral images;image details;composite loss function;signal-to-noise ratio loss;multiskip connection","","8","","50","CCBY","21 Jun 2021","","","IEEE","IEEE Journals"
"Deep Learning Super Resolution of Sea Surface Temperature on South China Sea","J. J. D. Khoo; K. H. Lim; P. K. Pang","Department of Electrical and Computer Engineering, Curtin University Malaysia CDT 250, Miri, Malaysia; Department of Electrical and Computer Engineering, Curtin University Malaysia CDT 250, Miri, Malaysia; Department of Electrical and Computer Engineering, Curtin University Malaysia CDT 250, Miri, Malaysia","2022 International Conference on Green Energy, Computing and Sustainable Technology (GECOST)","12 Jan 2023","2022","","","176","180","Surface temperature is one of the key observations to analyse the greenhouse effect on the Earth. The surface of the ocean can be captured using satellite sensors and transmitted to a meteorological center for real-time analysis. The use of the deep learning paradigm in super resolution has its potential in geoscience applications to increase the data transmission latency and enhance low-quality observation from remote sensing data. In this paper, the deployment of Generative Adversarial Network (GAN) architecture is studied to apply resolution reconstruction using the South China Sea sea surface temperature data. In addition, the development of spectral normalization is added to the Enhanced Super Resolution Generative Adversarial Network (ESRGAN) architecture to improve the training mechanism of generator and discriminator. This improved ESRGAN is compared with its super resolution performance against peak signal-to-noise ratio and structural similarity index evaluation metrics. The experiment shows that the low resolution of South China Sea data can be inferred to obtain a higher resolution with a more realistic resolution as compared to the conventional upsampling approaches.","","978-1-6654-8663-7","10.1109/GECOST55694.2022.10010371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10010371","Sea Surface Temperature;Deep Learning;Super Resolution;Generative Adversarial Networks","Temperature sensors;Deep learning;Sea surface;Temperature distribution;Surface reconstruction;Computer architecture;Generative adversarial networks","image enhancement;image reconstruction;image resolution;learning (artificial intelligence);ocean temperature;oceanographic regions;oceanographic techniques;remote sensing","data transmission;deep learning paradigm;Enhanced Super Resolution Generative Adversarial Network architecture;greenhouse effect;key observations;low-quality observation;realistic resolution;remote sensing data;resolution reconstruction;South China Sea data;South China Sea sea surface temperature data;super resolution performance","","","","23","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Semisupervised Semantic Segmentation of Remote Sensing Images With Consistency Self-Training","J. Li; B. Sun; S. Li; X. Kang","Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province and the College of Electrical and Information Engineering, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province and the College of Electrical and Information Engineering, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province and the College of Electrical and Information Engineering, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province and the College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","1 Mar 2022","2022","60","","1","11","Semisupervised semantic segmentation is an effective way to reduce the expensive manual annotation cost and take advantage of the unlabeled data for remote sensing (RS) image interpretation. Recent related research has mainly adopted two strategies: self-training and consistency regularization. Self-training tries to acquire accurate pseudo-labels to explicitly expand the train set. However, the existing methods cannot accurately identify false pseudo-labels, suffering from their negative impact on model optimization. The consistency regularization constrains the model by producing consistent predictions robust to the perturbations introduced in the sample or feature domain but requires a sufficient number of training data. Therefore, we propose a strategy for the semisupervised semantic segmentation of the RS images. The proposed model in the generative adversarial network (GAN) framework is optimized by consistency self-training, learning the distributions of both labeled and unlabeled data. The discriminator is optimized by accurate pixel-level training labels instead of the image-level ones, thereby assessing the confidence for the prediction of each pixel, which is then used to reweight the loss of the unlabeled data in self-training. The generator is optimized with the consistency constraint with respect to all random perturbations on the unlabeled data, which increases the sample diversity and prompts the model to learn the underlying distribution of the unlabeled data. Experimental results on the the large-scale and densely annotated Instance Segmentation in Aerial Images Dataset (iSAID) datasets and the International Society for Photogrammetry and Remote Sensing (ISPRS) datasets show that our framework outperforms several state-of-the-art semisupervised semantic segmentation methods.","1558-0644","","10.1109/TGRS.2021.3134277","Science and Technology Project of Guangdong Province(grant numbers:2018B010107001); National Natural Science Fund of China(grant numbers:61890962,61801178,61871179); Science and Technology Talents Program of Hunan Association for Science and Technology(grant numbers:2017TJ-Q09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645575","Consistency self-training;generative adversarial network (GAN);remote sensing (RS) image;semantic segmentation;semisupervised learning","Semantics;Predictive models;Image segmentation;Generative adversarial networks;Perturbation methods;Training;Semisupervised learning","deep learning (artificial intelligence);geophysical image processing;image classification;image segmentation;learning (artificial intelligence);neural nets;object detection;photogrammetry;remote sensing;supervised learning;unsupervised learning","semisupervised semantic segmentation methods;consistency self-training;remote sensing image interpretation;consistency regularization;RS images;generative adversarial network framework;pixel-level training labels;instance segmentation in aerial images dataset;International Society for Photogrammetry and Remote Sensing dataset;iSAID dataset;ISPRS dataset","","1","","53","IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"Semi Supervised Change Detection Method of Remote Sensing Image","W. Nie; P. Gou; Y. Liu; B. Shrestha; T. Zhou; N. Xu; P. Wang; Q. Du","Beijing Big Data Advanced Technology Institute, Beijing; Beijing Big Data Advanced Technology Institute, Beijing; Beijing Big Data Advanced Technology Institute, Beijing; State Key Laboratory of Remote Sensing Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Beijing Big Data Advanced Technology Institute, Beijing; Department of Geography, University of California, Los Angles, USA; Beijing Big Data Advanced Technology Institute, Beijing; Jiaxing Nanhu University, Jiaxing","2022 IEEE 6th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC )","9 Nov 2022","2022","","","1013","1019","Change detection based on deep learning is an important research direction in intelligent interpretation of remote sensing images. It has developed rapidly in recent years, but it is also a long-term challenge in remote sensing applications. This is mainly because the production of labeled data for training requires a lot of labor costs, and the currently available change detection labeled data is relatively small. While the complexity of high-resolution remote sensing imagery greatly increases the difficulty for deep learning models to learn robust and discriminative representations from scenes and objects, in this case, training deep learning models with a small amount of labeled data is still a huge challenge. To address this issue, this paper proposes a semi-supervised learning change detection method based on Generative Adversarial Networks (GAN). Compared with previous techniques, this paper combines a typical GAN framework with a Siamese network and applies it to change detection in remote sensing images. We introduce residual networks and atrous convolutions into Siamese networks, and employ a flow alignment module (FAM) to learn semantic flow between adjacent hierarchical feature maps. The connected discriminator formulates the training of the generator as a min-max optimization problem. Comprehensive quantitative and qualitative evaluations of multiple models show that our proposed method outperforms state-of-the-art change detection algorithms.","2689-6621","978-1-6654-5864-1","10.1109/IAEAC54830.2022.9930050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930050","semi-supervised;generative adversarial network;remote sensing;change detection","Training;Deep learning;Semantics;Semisupervised learning;Generative adversarial networks;Data models;Robustness","feature extraction;geophysical image processing;image classification;image representation;image resolution;learning (artificial intelligence);object detection;remote sensing;terrain mapping","long-term challenge;remote sensing applications;currently available change detection;high-resolution remote sensing imagery;training deep learning models;semisupervised learning change detection method;Siamese network;remote sensing image;state-of-the-art change detection algorithms;semisupervised change detection method","","","","30","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks for Classification of Micro-Doppler Signatures of Human Activity","I. Alnujaim; D. Oh; Y. Kim","Electrical and Computer Engineering Department, California State University, Fresno, USA; Advanced Radar Research Division, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea; Electrical and Computer Engineering Department, California State University, Fresno, USA","IEEE Geoscience and Remote Sensing Letters","26 Feb 2020","2020","17","3","396","400","We propose using generative adversarial networks (GANs) for the classification of micro-Doppler signatures measured by the radar. Despite Deep Convolutional Neural Networks (DCNNs) having been used extensively in radar image classification in recent years, their performance could not be fully implemented in the radar field because of the deficiency of the training data set. This is a key issue because of the extremely high labor and monetary costs involved in obtaining radar images. As such, attempts have been made to resolve this issue via the production of radar data by simulation or by the use of transfer learning. In this letter, we propose the use of GANs to produce a large number of micro-Doppler signatures with which to increase the training data set. Once the GANs are trained, a large amount of similar data, with the same distribution as the original data, can be easily generated. The generated fake micro-Doppler images can then be included in the DCNN training process. The proposed method is applied to classifying human activities measured by the Doppler radar. For each human activity, corresponding GANs that generate micro-Doppler signatures for a particular activity are constructed. Using the micro-Doppler signatures produced by the GANs along with the original data, the DCNN is trained. According to the results, the use of GANs improves the accuracy of classification. Moreover, the use of GANs was found to be more effective than the use of transfer learning.","1558-0571","","10.1109/LGRS.2019.2919770","Daegu Gyeongbuk Institute of Science and Technology(grant numbers:18-ST-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8738820","Deep convolutional neural networks DCNNs;generative adversarial networks GANs;human activity classification;micro-Doppler signatures","Gallium nitride;Radar imaging;Doppler radar;Generators;Neural networks;Training","convolutional neural nets;Doppler radar;image classification;learning (artificial intelligence);object detection;radar computing;radar imaging","microDoppler signatures;human activity;generative adversarial networks;GANs;Deep Convolutional Neural Networks;radar image classification;radar field;training data set;radar data;microDoppler images;Doppler radar","","37","","26","IEEE","18 Jun 2019","","","IEEE","IEEE Journals"
"Super-Resolution Reconstruction Method of Remote Sensing Image Based on Multi-Feature Fusion","Z. -X. Huang; C. -W. Jing","Zhejiang Yijia Geographic Information Technology Company, Ltd., Hangzhou, China; Hangzhou Normal University, Hangzhou, China","IEEE Access","30 Jan 2020","2020","8","","18764","18771","The acquisition of remote sensing images is affected by imaging equipment and environmental conditions. Usually on lower performance devices, the resolution of the acquired images is also low. Among many methods, the super-resolution reconstruction method based on generative adversarial networks has obvious advantages over previous network models in reconstructing image texture details. However, it is found in experiments that not all of these reconstructed textures exist in the image itself. Aiming at the problem of whether the texture details of the reconstructed image are accurate and clear, we propose a super-resolution reconstruction method combining wavelet transform and generative adversarial network. Using wavelet multi-resolution analysis, training wavelet decomposition coefficients in the generative adversarial network can effectively improve the local detail information of the reconstructed image. Experimental results show that our method can effectively reconstruct more natural image textures and make the images more visually clear. In the remote sensing image test set, the four indicators of the algorithm, peak signal to noise ratio (PSNR), structural similarity (SSIM), Feature Similarity (FSIM) and Universal Image Quality (UIQ) are slightly better than the algorithms mentioned in the article.","2169-3536","","10.1109/ACCESS.2020.2967804","Key Special Projects through the Provincial Scientific Research Institutes Program of Zhejiang Province, China(grant numbers:2014F50022); Construction of Agricultural Science Park Program of Zhejiang Province, China(grant numbers:2019E70002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963714","Remote sensing image;super-resolution;self-correlation;image texture","Image reconstruction;Wavelet transforms;Reconstruction algorithms;Remote sensing;Generative adversarial networks","image fusion;image reconstruction;image resolution;image texture;neural nets;remote sensing;wavelet transforms","wavelet multiresolution analysis;generative adversarial network;reconstructed image;natural image textures;remote sensing image test;super-resolution reconstruction method;remote sensing images;imaging equipment;image texture details;reconstructed textures;universal image quality;multi-feature fusion;environmental conditions;wavelet transform;training wavelet decomposition coefficients;peak signal to noise ratio;structural similarity;feature similarity","","11","","27","CCBY","20 Jan 2020","","","IEEE","IEEE Journals"
"Remote Sensing Data Augmentation Through Adversarial Training","N. Lv; H. Ma; C. Chen; Q. Pei; Y. Zhou; F. Xiao; J. Li","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Ministry of Water Resources of China, Beijing, China; Ministry of Water Resources of China, Beijing, China; Ministry of Water Resources of China, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Sep 2021","2021","14","","9318","9333","The lack of remote sensing images and poor quality limit the performance improvement of follow-up research such as remote sensing interpretation. In this article, a generative adversarial network (GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangxi and Anhui Provinces in China, i.e., deeply supervised GAN (D-sGAN). D-sGAN can generate high-quality images that are rich in changes, greatly shorten the generation time, and provide data support for applications such as semantic interpretation of remote sensing images. First, to modulate the layer activations, a downsampling scheme is designed based on the segmentation map. Then, the architecture of the generator is Unet++ with the proposed downsampling module. Next, the generator of this net is deeply supervised by the discriminator using deep convolutional neural network. This article further proved that the proposed downsampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with the faster generation speed compared to the CoGAN, SimGAN, and CycleGAN models. Furthermore, the remote sensing data generated by the model helped the interpretation network to increase the accuracy by 9%, meeting actual generation requirements.","2151-1535","","10.1109/JSTARS.2021.3110842","National Key R&D Program of China(grant numbers:2020YFB1807500); National Natural Science Foundation of China(grant numbers:62072360,62001357,61672131,61901367); Key Research and Development Plan of Shaanxi province(grant numbers:2021ZDLGY02-09,2020JQ-844); Key Laboratory of Embedded System and Service Computing(grant numbers:Tongji University,ESSCKF2019-05); Ministry of Education, Xi'an Science and Technology Plan(grant numbers:20RGZN0005); Xi'an Key Laboratory of Mobile Edge Computing and Security(grant numbers:201805052-ZD3CG36); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9531488","Data augmentation;deep supervision;downsampling;GAN","Remote sensing;Generative adversarial networks;Generators;Semantics;Training;Task analysis;Image synthesis","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image segmentation;learning (artificial intelligence);remote sensing","remote sensing images;remote sensing data;sensing data augmentation;remote sensing interpretation;generative adversarial network;high-quality images;downsampling module;adversarial training;Jiangxi provinces;Anhui provinces;China;deeply supervised GAN;D-sGAN;Unet++;deep convolutional neural network;CoGAN;SimGAN;CycleGAN models;interpretation network;generation requirements","","15","","47","CCBY","8 Sep 2021","","","IEEE","IEEE Journals"
"Stochastic Super-Resolution for Downscaling Time-Evolving Atmospheric Fields With a Generative Adversarial Network","J. Leinonen; D. Nerini; A. Berne","Federal Office of Meteorology and Climatology MeteoSwiss, Locarno-Monti, Switzerland; Federal Office of Meteorology and Climatology MeteoSwiss, Locarno-Monti, Switzerland; Environmental Remote Sensing Laboratory, École Polytechnique fédérale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2021","2021","59","9","7211","7223","Generative adversarial networks (GANs) have been recently adopted for super-resolution, an application closely related to what is referred to as “downscaling” in the atmospheric sciences: improving the spatial resolution of low-resolution images. The ability of conditional GANs to generate an ensemble of solutions for a given input lends itself naturally to stochastic downscaling, but the stochastic nature of GANs is not usually considered in super-resolution applications. Here, we introduce a recurrent, stochastic super-resolution GAN that can generate ensembles of time-evolving high-resolution atmospheric fields for an input consisting of a low-resolution sequence of images of the same field. We test the GAN using two data sets: one consisting of radar-measured precipitation from Switzerland; the other of cloud optical thickness derived from the Geostationary Earth Observing Satellite 16 (GOES-16). We find that the GAN can generate realistic, temporally consistent super-resolution sequences for both data sets. The statistical properties of the generated ensemble are analyzed using rank statistics, a method adapted from ensemble weather forecasting; these analyses indicate that the GAN produces close to the correct amount of variability in its outputs. As the GAN generator is fully convolutional, it can be applied after training to input images larger than the images used to train it. It is also able to generate time series much longer than the training sequences, as demonstrated by applying the generator to a three-month data set of the precipitation radar data. The source code to our GAN is available at https://github.com/jleinonen/downscaling-rnn-gan.","1558-0644","","10.1109/TGRS.2020.3032790","Swiss National Science Foundation(grant numbers:#200020_175700); Swiss National Supercomputing Centre (CSCS)(grant numbers:sm35); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246532","Atmosphere;clouds;image processing;meteorological radar;neural networks;remote sensing","Gallium nitride;Generators;Meteorology;Generative adversarial networks;Training","atmospheric optics;atmospheric precipitation;atmospheric techniques;clouds;geophysics computing;image resolution;remote sensing;remote sensing by radar;stochastic processes;time series;weather forecasting","input images;GAN generator;generated ensemble;super-resolution sequences;data sets;low-resolution sequence;time-evolving high-resolution;stochastic super-resolution GAN;recurrent resolution GAN;super-resolution applications;stochastic nature;stochastic downscaling;conditional GANs;low-resolution images;spatial resolution;atmospheric sciences;generative adversarial network;downscaling time-evolving atmospheric fields","","20","","55","IEEE","2 Nov 2020","","","IEEE","IEEE Journals"
"Multifeature Collaborative Adversarial Attack in Multimodal Remote Sensing Image Classification","C. Shi; Y. Dang; L. Fang; M. Zhao; Z. Lv; Q. Miao; C. -M. Pun","School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, Shaanxi, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, Shaanxi, China; Chinese Academy of Sciences, Quanzhou Institute of Equipment Manufacturing, Haixi Institutes, Quanzhou, Fujian, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, Shaanxi, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, Shaanxi, China; School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China; Department of Computer and Information Science, University of Macau, Macau, China","IEEE Transactions on Geoscience and Remote Sensing","25 Oct 2022","2022","60","","1","15","Deep neural networks have strong feature learning ability, but their vulnerability cannot be ignored. Current research shows that deep learning models are threatened by adversarial examples in remote sensing (RS) classification tasks, and their robustness drops sharply in the face of adversarial attacks. Therefore, many adversarial attack methods have been studied to predict the risks faced by a network. However, the existing adversarial attack methods mainly focus on single-modal image classification networks, and the rapid growth of RS data makes multimodal RS image classification a research hotspot. Generating multimodal adversarial examples needs to consider a high attack success rate, subtle perturbation, and collaborative attack ability between different modalities. In this article, we investigate the vulnerability of multimodal RS classification networks and propose a multifeature collaborative adversarial network (MFCANet) for generating multimodal adversarial examples. Two modality-specific generators are designed to generate the multimodal collaborative perturbations with strong attack ability, and two modality-specific discriminators make the generated multimodal adversarial examples closer to the real instances. In addition, a modality-specific generative loss and a modality-specific discriminative loss are proposed, and an alternating optimization strategy is designed for training the proposed MFCANet. Extensive experiments are carried out on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen 2D dataset and ISPRS Potsdam 2D dataset. The results show that the attack performance of the proposed method is stronger than that of the fast gradient sign method (FGSM), project gradient descent (PGD), and Carlini and Wagner (C&W) attack methods.","1558-0644","","10.1109/TGRS.2022.3208337","National Natural Science Foundation of China(grant numbers:61902313,61973250,42101359,62002272,61902296); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9899410","Generative adversarial networks (GANs);multimodal adversarial attack;multimodal remote sensing (RS) image classification","Perturbation methods;Collaboration;Task analysis;Training;Mathematical models;Generators;Deep learning","feature extraction;geophysical image processing;gradient methods;image classification;learning (artificial intelligence);neural nets;photogrammetry;remote sensing","multifeature collaborative adversarial network;modality-specific generators;multimodal collaborative perturbations;strong attack ability;modality-specific discriminators;generated multimodal adversarial examples;modality-specific generative loss;modality-specific discriminative loss;Remote Sensing Vaihingen 2D dataset;attack performance;collaborative adversarial attack;multimodal Remote Sensing image classification;deep neural networks;strong feature learning ability;vulnerability;deep learning models;remote sensing classification tasks;adversarial attacks;existing adversarial attack methods;single-modal image classification networks;RS data;multimodal RS image classification;high attack success rate;collaborative attack ability;multimodal RS classification networks","","","","29","IEEE","21 Sep 2022","","","IEEE","IEEE Journals"
"Deep Convolutional Generative Adversarial Network With Autoencoder for Semisupervised SAR Image Classification","Z. Zhang; J. Yang; Y. Du","Urban and Rural Innovation Design Research Center, Zhejiang University, Hangzhou, China; Southern Marine Science and Engineering Guangdong Laboratory, Zhuhai, China; Urban and Rural Innovation Design Research Center, Zhejiang University, Hangzhou, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2021","2022","19","","1","5","Effective and efficient classification of synthetic aperture radar (SAR) images represents an important step toward image interpretation and knowledge discovery. Keys to classification performance include feature extraction and availability of class labels for training. Generative adversarial networks (GANs) are a recently advanced powerful framework to offer both automatic feature extraction and semisupervised and unsupervised learning. For its known drawbacks such as mode collapse and training instability, noteworthy improvements include the deep convolutional GANs (DCGANs). In the letter, we proposed a new approach, which further advances DCGANs with autoencoder (AE) for intermediate feature extraction and reconstruction, in combination with multiclassifier (MC) for better context treatment. The resulting improvement on training stability and mode preservation over DCGAN is clearly demonstrated by the classification results across multifrequency bands (L-, C-, and X-bands).","1558-0571","","10.1109/LGRS.2020.3018186","China National Science Foundation(grant numbers:41771367); National Key Research and Development Program of China(grant numbers:2016YFC1401007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194329","Autoencoder (AE);classification;feature;generative adversarial network (GAN);synthetic aperture radar (SAR)","Feature extraction;Gallium nitride;Synthetic aperture radar;Training;Generative adversarial networks;Generators;Image reconstruction","data mining;feature extraction;geophysical image processing;image classification;learning (artificial intelligence);pattern classification;radar computing;radar imaging;synthetic aperture radar;unsupervised learning","known drawbacks;mode collapse;noteworthy improvements;deep convolutional GANs;DCGAN;advances DCGANs;autoencoder;intermediate feature extraction;training stability;mode preservation;deep convolutional generative adversarial network;semisupervised SAR image classification;effective classification;synthetic aperture radar images;image interpretation;knowledge discovery;classification performance;class labels;generative adversarial networks;recently advanced powerful framework;automatic feature extraction;semisupervised learning;unsupervised learning","","2","","14","IEEE","10 Sep 2020","","","IEEE","IEEE Journals"
"Incorporating Metric Learning and Adversarial Network for Seasonal Invariant Change Detection","W. Zhao; L. Mou; J. Chen; Y. Bo; W. J. Emery","Beijing Engineering Research Center for Global Land Remote Sensing Products, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, China; German Aerospace Center (DLR), Remote Sensing Technology Institute (IMF), Weßling, Germany; National Geomatics Center of China, Beijing, China; Beijing Engineering Research Center for Global Land Remote Sensing Products, Institute of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, China; Colorado Center for Astrodynamics Research, University of Colorado, Boulder, USA","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2020","2020","58","4","2720","2731","Change detection by comparing two bitemporal images is one of the most fundamental challenges for dynamic monitoring of the Earth surface. In this article, we propose a metric learning-based generative adversarial network (GAN) (MeGAN) to automatically explore seasonal invariant features for pseudochange suppressing and real change detection. To achieve this purpose, a seasonal invariant term is introduced to maximally suppress pseudochanges, whereas the MeGAN explores the transition patterns between adjacent images in a self-learning fashion. Different from the previous works on bitemporal imagery change detection, the proposed MeGAN have the following contributions: 1) it automatically explores change patterns from the complex bitemporal background without human intervention and 2) it aims to maximally exclude pseudochanges from the seasonal transition term and map out real changes efficiently. To our best knowledge, this is the first time we incorporate the seasonal transition term and GAN for change detection between bitemporal images. At last, to demonstrate the robustness of the proposed method, we included two data sets which are the Google Earth data and the Landsat data, for bitemporal change detection and evaluation. The experimental results indicated that the proposed method is able to perform change detection with precision can be as high as 81% and 88% for the Google Earth and Landsat data set, respectively.","1558-0644","","10.1109/TGRS.2019.2953879","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0501502); China Postdoctoral Science Foundation(grant numbers:2018M640087,2019T120063); Fundamental Research Funds for the Central Universities(grant numbers:2018NTST01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937747","Bitemporal images;change detection;metric learning;metric learning-based generative adversarial network (GAN) (MeGAN);pseudochanges","Gallium nitride;Remote sensing;Generative adversarial networks;Measurement;Generators;Earth;Training","feature extraction;geophysical image processing;learning (artificial intelligence);object detection","Google Earth;Landsat data;transition patterns;seasonal invariant features;metric learning-based generative adversarial network;seasonal invariant change detection;bitemporal images;seasonal transition term;complex bitemporal background;change patterns;bitemporal imagery change detection;MeGAN;pseudochanges;seasonal invariant term","","27","","43","IEEE","20 Dec 2019","","","IEEE","IEEE Journals"
"Adversarial Shape Learning for Building Extraction in VHR Remote Sensing Images","L. Ding; H. Tang; Y. Liu; Y. Shi; X. X. Zhu; L. Bruzzone","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Technology and Electrical Engineering, ETH Zürich, Zürich, Switzerland; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Chair of Remote Sensing Technology, Technical University of Munich (TUM), Munich, Germany; Data Science in Earth Observation (SiPEO, formerly Signal Processing in Earth Observation), Technical University of Munich (TUM), Munich, Germany; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Image Processing","28 Dec 2021","2022","31","","678","690","Building extraction in VHR RSIs remains a challenging task due to occlusion and boundary ambiguity problems. Although conventional convolutional neural networks (CNNs) based methods are capable of exploiting local texture and context information, they fail to capture the shape patterns of buildings, which is a necessary constraint in the human recognition. To address this issue, we propose an adversarial shape learning network (ASLNet) to model the building shape patterns that improve the accuracy of building segmentation. In the proposed ASLNet, we introduce the adversarial learning strategy to explicitly model the shape constraints, as well as a CNN shape regularizer to strengthen the embedding of shape features. To assess the geometric accuracy of building segmentation results, we introduced several object-based quality assessment metrics. Experiments on two open benchmark datasets show that the proposed ASLNet improves both the pixel-based accuracy and the object-based quality measurements by a large margin. The code is available at: https://github.com/ggsDing/ASLNet.","1941-0042","","10.1109/TIP.2021.3134455","Scholarship from the China Scholarship Council(grant numbers:201703170123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653801","Building extraction;generative adversarial networks (GANs);image segmentation;convolutional neural network;deep learning;remote sensing","Buildings;Shape;Image segmentation;Feature extraction;Adversarial machine learning;Semantics;Convolutional neural networks","feature extraction;geophysical image processing;image classification;image resolution;image segmentation;image texture;learning (artificial intelligence);neural nets;object detection;remote sensing","adversarial shape learning network;building shape patterns;adversarial learning strategy;shape constraints;CNN shape regularizer;shape features;geometric accuracy;building segmentation results;object-based quality assessment metrics;pixel-based accuracy;object-based quality measurements;building extraction;vhr remote sensing images;VHR RSIs;occlusion;boundary ambiguity problems;conventional convolutional neural networks based methods;local texture;context information;necessary constraint;human recognition","Humans;Image Processing, Computer-Assisted;Neural Networks, Computer;Remote Sensing Technology","8","","54","IEEE","16 Dec 2021","","","IEEE","IEEE Journals"
"Thermal to Visible Facial Image Translation Using Generative Adversarial Networks","Z. Wang; Z. Chen; F. Wu","School of Computer Science, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Signal Processing Letters","28 Jun 2018","2018","25","8","1161","1165","Thermal cameras can capture images invariant to illumination conditions. However, thermal facial images are difficult to be recognized by human examiners. In this letter, an end-to-end framework, which consists of a generative network and a detector network, is proposed to translate thermal facial images into visible ones. The generative network aims at generating visible images given the thermal ones. The detector can locate important facial landmarks on visible faces and help the generative network to generate more realistic images that are easier to be recognized. As demonstrated in the experiments, the faces generated by our method have good visual quality and maintain identity preserving features.","1558-2361","","10.1109/LSP.2018.2845692","National Key R&D Program of China(grant numbers:2017YFB1002202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8375645","Face;generative adversarial network (GAN);image translation;infrared;thermal","Detectors;Face recognition;Mathematical model;Gallium nitride;Training;Shape;Visualization","cameras;face recognition;feature extraction;infrared imaging","visible facial image translation;generative adversarial networks;thermal cameras;thermal facial image translation;facial landmarks;identity preserving features","","34","","23","IEEE","8 Jun 2018","","","IEEE","IEEE Journals"
"Cloud Removal in Unpaired Sentinel-2 Imagery Using Cycle-Consistent GAN and SAR-Optical Data Fusion","P. Ebel; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2065","2068","The majority of optical images acquired via spaceborne remote sensing are affected by clouds. Recent advances in cloud removal combine multimodal data with deep neural networks recovering the affected areas. To relax the requirements on the data the network is trained on previous approaches utilized generative models no longer necessitating strict pixel-wise correspondences between cloudy input and cloud-free target images. However, such models are often-times prone to fiction, i.e. the generation of content systematically differing from the structure of the target images. In this work we combine the fusion of optical and radar imagery with the advantages of generative models trainable on unpaired optical data, while reducing fiction by reconstructing optical information only where it need be-over cloud-covered areas. We evaluate our approach qualitatively and quantitatively and demonstrate its effectiveness.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324060","synthetic aperture radar (SAR);optical imagery;cloud removal;data fusion;deep learning","Clouds;Optical imaging;Optical sensors;Adaptive optics;Generative adversarial networks;Synthetic aperture radar;Optical fiber networks","geophysical image processing;image reconstruction;neural nets;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","generative models;longer necessitating strict pixel-wise correspondences;cloudy input;cloud-free target images;fiction;content systematically differing;optical radar imagery;unpaired optical data;optical information;cloud-covered areas;unpaired sentinel-2 imagery;cycle-consistent GAN;SAR-optical data fusion;optical images;spaceborne remote sensing;cloud removal combine multimodal data;deep neural networks","","8","","15","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Deep Learning Framework for Fusion of Sar and Optical Satellite Imagery","N. Gupta; H. S. Srivastava; T. Sivasankar; P. Patel","NIIT University, Neemrana, India; Indian Institute of Remote Sensing, ISRO, Dehradun, India; NIIT University, Neemrana, India; Space Applications Centre, ISRO, Ahmedabad, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","488","491","In remote sensing, image fusion is the process of converting information from various source images to a single image such that the features of the source are preserved and relevant information is being highlighted. Through this research work, we propose an unsupervised deep learning Generative Adversarial Network (GAN) for the fusion process of SAR and optical Images. For SAR image, we chose VV, VH, VV-VH bands and for optical image we did Principal Component Analysis (PCA) on its image bands to extract the top three principal components and compose an image out of it. Images were then converted into HSV space. The GAN is primarily trained to capture the maximum gradient features from both the images and secondarily to capture other noticeable features. Experimental results on both training and test samples indicate that the proposed method is able to preserve gradient features and other details of the images with respect to input images.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9792062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792062","SAR;optical;image fusion;deep learning;GAN","Deep learning;Training;Neural networks;Optical computing;Optical imaging;Generative adversarial networks;Optical sensors","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;principal component analysis;remote sensing;synthetic aperture radar;unsupervised learning","remote sensing;image fusion;GAN;optical Images;SAR image;VV-VH bands;optical image;principal component analysis;image bands;maximum gradient features;optical satellite imagery;unsupervised deep learning generative adversarial network;PCA","","","","18","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Edge-Enhanced GAN for Remote Sensing Image Superresolution","K. Jiang; Z. Wang; P. Yi; G. Wang; T. Lu; J. Jiang","National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; Hubei Province Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","22 Jul 2019","2019","57","8","5799","5812","The current superresolution (SR) methods based on deep learning have shown remarkable comparative advantages but remain unsatisfactory in recovering the high-frequency edge details of the images in noise-contaminated imaging conditions, e.g., remote sensing satellite imaging. In this paper, we propose a generative adversarial network (GAN)-based edge-enhancement network (EEGAN) for robust satellite image SR reconstruction along with the adversarial learning strategy that is insensitive to noise. In particular, EEGAN consists of two main subnetworks: an ultradense subnetwork (UDSN) and an edge-enhancement subnetwork (EESN). In UDSN, a group of 2-D dense blocks is assembled for feature extraction and to obtain an intermediate high-resolution result that looks sharp but is eroded with artifacts and noises as previous GAN-based methods do. Then, EESN is constructed to extract and enhance the image contours by purifying the noise-contaminated components with mask processing. The recovered intermediate image and enhanced edges can be combined to generate the result that enjoys high credibility and clear contents. Extensive experiments on Kaggle Open Source Data set, Jilin-1 video satellite images, and Digitalglobe show superior reconstruction performance compared to the state-of-the-art SR approaches.","1558-0644","","10.1109/TGRS.2019.2902431","National Natural Science Foundation of China(grant numbers:61671332,U1736206,61671336,61501413,61502354); National Key Research and Development Project(grant numbers:2016YFE0202300); Hubei Province Technological Innovation Major Project(grant numbers:2017AAA123); Central Government Guided Local Science and Technology Development Projects(grant numbers:2018ZYYD059); Basic Research Program of Shenzhen City(grant numbers:JCYJ20170306171431656); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8677274","Adversarial learning;dense connection;edge enhancement;remote sensing imagery;superresolution","Image edge detection;Image reconstruction;Satellites;Feature extraction;Image resolution;Gallium nitride;Generative adversarial networks","edge detection;feature extraction;gallium compounds;geophysical image processing;image enhancement;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","edge-enhanced GAN;remote sensing image superresolution;current superresolution methods;deep learning;remarkable comparative advantages;high-frequency edge details;remote sensing satellite imaging;generative adversarial network-based edge-enhancement network;EEGAN;robust satellite image SR reconstruction;adversarial learning strategy;main subnetworks;ultradense subnetwork;UDSN;edge-enhancement subnetwork;EESN;intermediate high-resolution result;artifacts;image contours;noise-contaminated components;Jilin-1 video satellite images;mask processing","","211","","63","IEEE","29 Mar 2019","","","IEEE","IEEE Journals"
"Deep Covariance Alignment for Domain Adaptive Remote Sensing Image Segmentation","L. Wu; M. Lu; L. Fang","State Key Laboratory of Integrated Services Networks, Xidian University, Xian, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","19 Apr 2022","2022","60","","1","11","Unsupervised domain adaptive (UDA) image segmentation has recently gained increasing attention, aiming to improve the generalization capability for transferring knowledge from the source domain to the target domain. However, in high spatial resolution remote sensing image (RSI), the same category from different domains (e.g., urban and rural) can appear to be totally different with extremely inconsistent distributions, which heavily limits the UDA accuracy. To address this problem, in this article, we propose a novel deep covariance alignment (DCA) model for UDA RSI segmentation. The DCA can explicitly align category features to learn shared domain-invariant discriminative feature representations, which enhance the ability of model generalization. Specifically, a category feature pooling (CFP) module is first used to extract category features by combining coarse outputs and deep features. Then, we leverage a novel covariance regularization (CR) to enforce the intracategory features to be closer and the intercategory features to be further separate. Compared with the existing category alignment methods, our CR aims to regularize the correlation between different dimensions of the features, and thus performs more robustly when dealing with divergent category features of imbalanced and inconsistent distributions. Finally, we propose a stagewise procedure to train the DCA to alleviate error accumulation. Experiments on both rural-to-urban and urban-to-rural scenarios of the LoveDA dataset demonstrate the superiority of our proposed DCA over other state-of-the-art UDA segmentation methods. Code is available at https://github.com/Luffy03/DCA.","1558-0644","","10.1109/TGRS.2022.3163278","National Natural Science Fund of China(grant numbers:61922029); Science and Technology Plan Project Fund of Hunan Province(grant numbers:2019RS2016); Key Research and Development Project of Science and Technology Plan of Hunan Province(grant numbers:2021SK2039); Natural Science Fund of Hunan Province(grant numbers:2021JJ30003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745130","Deep covariance alignment (DCA);remote sensing image (RSI);semantic segmentation;unsupervised domain adaptive (UDA)","Feature extraction;Training;Image segmentation;Generative adversarial networks;Task analysis;Semantics;Adaptation models","feature extraction;geophysical image processing;image classification;image representation;image segmentation;learning (artificial intelligence);remote sensing;unsupervised learning","DCA;segmentation methods;domain adaptive remote sensing image segmentation;unsupervised domain adaptive image segmentation;generalization capability;source domain;target domain;high spatial resolution remote sensing image;extremely inconsistent distributions;UDA accuracy;deep covariance alignment model;UDA RSI segmentation;shared domain-invariant;model generalization;category feature pooling module;deep features;novel covariance regularization;intracategory features;intercategory features;existing category alignment methods;CR aims;divergent category features;imbalanced distributions;urban-to-rural scenarios","","2","","42","IEEE","30 Mar 2022","","","IEEE","IEEE Journals"
"A Data Augmentation Strategy Combining a Modified pix2pix Model and the Copy-Paste Operator for Solid Waste Detection With Remote Sensing Images","X. Xu; B. Zhao; X. Tong; H. Xie; Y. Feng; C. Wang; C. Xiao; X. Ke; J. Du","Frontiers Science Center for Intelligent Autonomous Systems, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; Frontiers Science Center for Intelligent Autonomous Systems, Shanghai, China; Frontiers Science Center for Intelligent Autonomous Systems, Shanghai, China; Frontiers Science Center for Intelligent Autonomous Systems, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; Frontiers Science Center for Intelligent Autonomous Systems, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Oct 2022","2022","15","","8484","8491","Solid waste detection is of great significance for environmental protection. In recent years, object detection methods based on deep learning have progressed rapidly. However, it is often extremely difficult to collect sufficient data to train a model with a good performance. In this article, a data augmentation strategy was introduced to generate sufficient synthetic high-quality images for solid waste detection. First, a modified pix2pix model was proposed, in which a local-global discriminator was designed to improve the detailed and global information of the generated images, which are commonly fuzzy with the original pix2pix model. Second, a copy-paste operator was utilized, which simply pastes the bounding box of the generated objects into different images to enhance the diversity of the samples. In this manner, the expanded dataset can be utilized to train different object detection models, for which FPN and Yolo-v4 were introduced as the validation models in this article. The experimental results show that the proposed strategy outperforms the traditional pix2pix method and the generated synthetic images can effectively improve the performance of object detection methods.","2151-1535","","10.1109/JSTARS.2022.3209967","National key research and development program of China(grant numbers:2018YFB0505400); National Natural Science Foundation of China(grant numbers:41971299,42221002); Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0100); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9904838","Copy-paste;data augmentation;local-global discriminator (LGD);object detection;pix2pix","Waste materials;Training;Generative adversarial networks;Remote sensing;Object detection;Image resolution;Environmental monitoring","","","","","","34","CCBYNCND","28 Sep 2022","","","IEEE","IEEE Journals"
"Unsupervised Domain Adaptation for Semantic Segmentation of High-Resolution Remote Sensing Imagery Driven by Category-Certainty Attention","J. Chen; J. Zhu; Y. Guo; G. Sun; Y. Zhang; M. Deng","School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","8 Mar 2022","2022","60","","1","15","Semantic segmentation is an important task of analysis and understanding of high-resolution remote sensing images (HRSIs). The deep convolutional neural network (DCNN)-based model shows their excellent performance in remote sensing image semantic segmentation. Most of the existing HRSI semantic segmentation methods are only designed for a very limited data domain, that is, the training and test images are from the same dataset. The accuracy drops sharply once a model trained on a certain dataset is used for cross-domain prediction due to the difference in feature distribution of the dataset. To this end, this article proposes an unsupervised domain adaptation framework based on adversarial learning for HRSI semantic segmentation. This framework uses high-level feature alignment to narrow the difference between the source and target domains at the semantic level. It uses the category-certainty attention module to reduce the attention of the classifier on category-level aligned features and increase the attention on category-level unaligned features. Experimental results show that the proposed method performs favorably against the state-of-the-art methods in cross-domain segmentation.","1558-0644","","10.1109/TGRS.2021.3140108","National Key Research and Development Program of China(grant numbers:2020YFA0713503); National Natural Science Foundation of China(grant numbers:42071427,41671357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667523","Category-certainty attention;domain adaptation;generative adversarial networks;semantic segmentation","Semantics;Image segmentation;Feature extraction;Adaptation models;Task analysis;Remote sensing;Training","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image resolution;image segmentation;learning (artificial intelligence);remote sensing;unsupervised learning","data domain;cross-domain prediction;unsupervised domain adaptation framework;high-level feature alignment;semantic level;category-certainty attention module;category-level aligned features;category-level unaligned features;cross-domain segmentation;high-resolution remote sensing imagery;deep convolutional neural network-based model;remote sensing image semantic segmentation;HRSI semantic segmentation","","6","","53","IEEE","4 Jan 2022","","","IEEE","IEEE Journals"
"MapGen-GAN: A Fast Translator for Remote Sensing Image to Map Via Unsupervised Adversarial Learning","J. Song; J. Li; H. Chen; J. Wu","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","18 Feb 2021","2021","14","","2341","2357","Map is an essential medium for people to understand our changing planet. Recently, research on generating and updating maps through remote sensing images has been an important and challenging task in geographic information. Traditional methods for map generation are time-consuming and labor-intensive. Besides, most supervised learning methods for map generation lack labeled training samples. It is challenging to generate maps quickly and efficiently for emergency rescue operations such as earthquakes, fire disasters, or tsunami. In this article, we propose an unsupervised domain mapping model based on adversarial learning called MapGen-GAN. MapGen-GAN is a generative adversarial network (GAN) that can do end-to-end translation from remote sensing images to general map quickly, and trained with no human annotation data. In order to improve the fidelity and the geometry precision of generated maps, we employ circularity-consistency and geometrical-consistency constraints as a part of the loss function of the proposed model. And then, an improved residual block Unet is designed and adopted as the generator of MapGen-GAN to capture the geographic structure information of buildings, roads, and topography outlines under different resolutions in the map generation. By applying the proposed model to two distinct datasets, experiments demonstrate that our model can generate maps efficiently and quickly and outperform the state-of-the-art approaches.","2151-1535","","10.1109/JSTARS.2021.3049905","Natural Science Foundation of Henan Province(grant numbers:2020JJ4103); National Natural Science Foundation of China(grant numbers:41871284,61806211,4971362,U19A2058); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316788","Adversarial learning;map generation;remote sensing images;unsupervised domain mapping","Remote sensing;Generative adversarial networks;Deep learning;Task analysis;Training;Internet;Semantics","disasters;geophysical image processing;image classification;neural nets;remote sensing;unsupervised learning","MapGen-GAN;fast translator;remote sensing image;unsupervised adversarial learning;supervised learning methods;unsupervised domain mapping model;generative adversarial network;end-to-end translation;general map;generated maps;updating maps generation;geometrical-consistency constraints;loss function;residual block Unet;geographic structure information","","10","","57","CCBY","8 Jan 2021","","","IEEE","IEEE Journals"
"Using Landsat-8 imagery and Generative Adversarial Network for Glacial Lakes Mapping in High Mountain Regions","H. Zhao; X. Liu; S. Wang","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence (ICIBA)","3 Feb 2022","2021","2","","877","881","As an essential resource in High Mountain Regions (HMR), evaluating the glacial lake dynamics is of great significance to explore the impacts of climate changes and predict the risks of Glacial Lake Outburst Floods (GLOFs). However, complicated and laborious methods for glacial lake mapping are unpractically applied in automatically monitoring glacial lakes at a large-scale region. In this work, we explored the mapping efficiency of glacial lakes by combing Generative Adversarial Networks and multi-level feature pyramid (GANMFP) in HMR. We first sampled the image patches containing glacial lakes from Landsat-8 raw data to evaluate the model performance. Totally, 6583 patches with 256x256x7 pixels are randomly cropped from 62 Landsat-8 images. Then we employed these data to train and test the GAN model, which integrated a multi-level feature fusion module in generator and a Resnet-152 networks in discriminator. From the validation results and result visualization, our method achieved good performances in Precision (88.42), Recall (59.61), and Overall Accuracy (99.28), which shows excellent potential in glacial lake mapping at a large-scale region.","","978-1-6654-2877-4","10.1109/ICIBA52610.2021.9687901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9687901","Glacial Lake;High Mountain Regions;Generative Adversarial Networks;Landsat-8 OLI","Earth;Artificial satellites;Data visualization;Lakes;Generative adversarial networks;Generators;Data models;Climate change","crops;feature extraction;floods;geophysical image processing;glaciology;hydrological techniques;image classification;lakes;remote sensing;terrain mapping","glacial lake mapping;large-scale region;Landsat-8 raw data;62 Landsat-8 images;landsat-8 imagery;generative adversarial network;glacial lakes mapping;high mountain regions;glacial lake dynamics;Glacial Lake Outburst Floods","","","","14","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Conditional GAN with Effective Attention for SAR-to-Optical Image Translation","T. Yu; J. Zhang; J. Zhou","College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2021 3rd International Conference on Advances in Computer Technology, Information Science and Communication (CTISC)","6 Sep 2021","2021","","","7","11","Synthetic aperture radar (SAR) is an effective observation technology, which is widely used in industry and agriculture. However, SAR images have speckle noise because of its imaging mechanism, so it is difficult to obtain useful information from them directly. Generative adversarial networks (GANs) have great performance in image translation with the development of deep learning, SAR images can be translated into optical images. However, due to the complex scene, low resolution and speckle noise, the generated images obtained by the existing methods are not satisfactory. In this paper, we propose a method based on conditional GAN (CGAN) for image translation from SAR images to optical images. We use the attention mechanism, which means that the network attaches importance to useful features and ignores unimportant ones. We apply discrete cosine transform (DCT) as loss function to extract the low frequency features in the image. Our experiments show that the quality of the images generated by our method is better than that of some famous methods.","","978-1-6654-1868-3","10.1109/CTISC52352.2021.00009","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527653","component;Remote sensing;Image translation;Generative adversarial networks;Attention mechanism","Optical losses;Speckle;Optical fiber networks;Optical imaging;Generative adversarial networks;Radar polarimetry;Adaptive optics","discrete cosine transforms;learning (artificial intelligence);optical images;radar imaging;speckle;synthetic aperture radar","conditional GAN;effective attention;SAR-to-optical image translation;effective observation technology;SAR images;speckle noise;imaging mechanism;optical images","","2","","10","IEEE","6 Sep 2021","","","IEEE","IEEE Conferences"
"An Attention Encoder-Decoder Network Based on Generative Adversarial Network for Remote Sensing Image Dehazing","L. Zhao; Y. Zhang; Y. Cui","Key Laboratory of Modern Power System Simulation and Control and Renewable Energy Technology, Ministry of Education, Northeast Electric Power University, Jilin, China; Key Laboratory of Modern Power System Simulation and Control and Renewable Energy Technology, Ministry of Education, Northeast Electric Power University, Jilin, China; Guangdong Electric Power Corporation Zhuhai Power Supply Bureau, Zhuhai, China","IEEE Sensors Journal","30 May 2022","2022","22","11","10890","10900","Remote sensing image dehazing is a difficult problem for its complex characteristics. It can be regarded as the preprocessing of high-level tasks of remote sensing images. To remove haze from the hazy remote sensing image, an encoder-decoder based on generative adversarial network is proposed. It first learns the low-frequency information of the image, and then learns the high-frequency information of the image. The skip connection is also added in the network to avoid losing information. To further improve the ability of learning more useful information, a multi-scale attention module is proposed. Meanwhile, a CBlock module is also designed to extract more feature information. It can capture different size of receptive fields. In order to reduce the computational pressure of the network, a distillation module is used in the network. Inspired by multi-scale network, an enhance module is designed and introduced it in the end of the network to further improve the dehazing ability of the network by integrating context information on multi-scale. We compared with five methods and our proposed method on RICE dataset. Experimental results show that our method achieves the best effect, both qualitatively and quantitatively.","1558-1748","","10.1109/JSEN.2022.3172132","Research Foundation of Education Bureau of Jilin Province(grant numbers:JJKH20220054KJ,JJKH20210095KJ); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9766320","Remote sensing image dehazing;generative adversarial network;encoder-decoder;multi-scale attention module","Atmospheric modeling;Feature extraction;Remote sensing;Generative adversarial networks;Training;Scattering;Learning systems","data mining;decoding;encoding;feature extraction;geophysical image processing;image colour analysis;image denoising;image enhancement;image restoration;learning (artificial intelligence);remote sensing","remote sensing image dehazing;high-level tasks;remote sensing images;hazy remote sensing image;generative adversarial network;low-frequency information;high-frequency information;losing information;multiscale attention module;feature information;multiscale network;dehazing ability;context information;attention encoder-decoder network","","15","","40","IEEE","3 May 2022","","","IEEE","IEEE Journals"
"Very Short-Term Rainfall Prediction Using Ground Radar Observations and Conditional Generative Adversarial Networks","Y. Kim; S. Hong","Department of Environment, Energy and Geoinfomatics, Sejong University, Seoul, Gwangjin-gu, Republic of Korea; DeepThoTh Company Ltd., Seoul, Gwangjin-gu, Republic of Korea","IEEE Transactions on Geoscience and Remote Sensing","2 Feb 2022","2022","60","","1","8","Weather radars play an important role in in situ rainfall monitoring owing to their ability to measure instantaneous rain rates and rainfall distributions. Currently, the Korea Meteorological Administration (KMA) provides instantaneous radar observation data and predictions based on the McGill algorithm for precipitation nowcasting by Lagrangian extrapolation (MAPLE) for up to 6 h, for short-term forecasting. This study presents a conditional generative adversarial network (CGAN)-based radar rainfall prediction method for very short-range weather forecasts from 10 min to 4 h. The CGAN-predicted model was trained and tested using KMA’s constant altitude plan position indicator (CAPPI) observation data. The qualitative comparison between the radar observation and the CGAN-predicted rain rates displayed high statistical scores, such as the probability of detection (POD) = 0.8442, false alarm ratio (FAR) = 0.2913, and critical success index (CSI) = 0.6268, in the case of a 1-h prediction for rainfall on September 5, 2019, 15:20 KST. This study demonstrates the capability of the CGAN model for short-term rainfall forecasting. Consequently, the CGAN-generated radar-based rainfall prediction could complement the KMA MAPLE system and be useful in various forecasting applications.","1558-0644","","10.1109/TGRS.2021.3108812","National Institute of Environment Research through the Ministry of Environment of the Republic of Korea(grant numbers:NIER-2021-01-02-068); Korea Meteorological Administration (KMA) Research and Development Program “Development of AI Techniques for Weather Forecasting”(grant numbers:KMA2021-00121); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9532007","Artificial intelligence;conditional generative adversarial network (CGAN);prediction;radar;rainfall","Radar;Rain;Predictive models;Meteorological radar;Radar imaging;Data models;Spaceborne radar","atmospheric techniques;geophysics computing;meteorological radar;neural nets;rain;remote sensing by radar;weather forecasting","short-term rainfall prediction;ground radar observations;conditional generative adversarial networks;weather radars;situ rainfall monitoring;instantaneous rain rates;rainfall distributions;Korea Meteorological Administration;instantaneous radar observation data;short-term forecasting;conditional generative adversarial network-based radar rainfall prediction method;weather forecasts;CGAN-predicted model;KMA's constant altitude plan position indicator observation data;CGAN-predicted rain rates;1-h prediction;CGAN model;short-term rainfall forecasting;CGAN-generated radar-based rainfall prediction","","4","","62","IEEE","9 Sep 2021","","","IEEE","IEEE Journals"
"Boosting Small Ship Detection in Optical Remote Sensing Images via Image Super-Resolution","L. Li; Z. Zhou; S. Cui","School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing; School of Automation, Beijing Institute of Technology, Beijing","2021 33rd Chinese Control and Decision Conference (CCDC)","30 Nov 2021","2021","","","1508","1512","Small ships in optical remote sensing images are hard to detect due to the lack of sufficient detail information. In this paper, we adopt the image super-resolution technology to solve this problem. Specifically, an effective super-resolution network is designed to generate clear super-resolution ship images from small blurry ones produced by the ship detector. Inspired by the idea of generative adversarial network (GAN), the super-resolution network is trained together with a discriminator network in an adversarial way, aiming at generating more realistic super-resolution images. Moreover, to eliminate false detections, the discriminator network is also used to distinguish ship and non-ship images via an additional classification branch. Experimental results demonstrate the effectiveness of the proposed method.","1948-9447","978-1-6654-4089-9","10.1109/CCDC52312.2021.9601674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9601674","Ship detection;Image super-resolution;Generative adversarial network","Training;Superresolution;Detectors;Optical detectors;Optical imaging;Generative adversarial networks;Boosting","image classification;image resolution;marine radar;object detection;radar imaging;remote sensing;ships","discriminator network;super-resolution images;false detections;nonship images;boosting small ship detection;optical remote sensing images;image super-resolution technology;super-resolution network;clear super-resolution ship images;ship detector;generative adversarial network","","","","16","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"WGAN-GP-Based Synthetic Radar Spectrogram Augmentation in Human Activity Recognition","L. Qu; Y. Wang; T. Yang; L. Zhang; Y. Sun","College of Electronic Information Engineering, Shenyang Aerospace University; College of Electronic Information Engineering, Shenyang Aerospace University; College of Electronic Information Engineering, Shenyang Aerospace University; College of Electronic Information Engineering, Shenyang Aerospace University; College of Electronic Information Engineering, Shenyang Aerospace University","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2532","2535","Despite deep convolutional neural networks (DCNNs) having been used extensively in radar-based human activity recognition in recent years, their performance could not be fully implemented because of the lack of radar dataset. However, radar data acquisition is difficult to achieve due to the high cost of its measurement. Generative adversarial networks (GANs) can be utilized to generate a large number of similar micro-Doppler signatures with which to increase the training data set. For the training of DCNNs, the quality and diversity of data set generated by GANs is particularly important. In this paper, we propose using a more stable and effective Wasserstein generative adversarial network with gradient penalty (WGAN-GP) to augment the training data set. The classification results from the experimental data have shown the proposed method can improve the classification accuracy of human activity.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554556","National Natural Science Foundation of China(grant numbers:61671310); Aeronautical Science Foundation of China(grant numbers:2019ZC054004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554556","micro-Doppler;human activity recognition;generative adversarial networks (GANs);deep convolutional neural networks (DCNNs)","Training;Radar measurements;Neural networks;Data acquisition;Training data;Radar;Activity recognition","data acquisition;Doppler radar;image classification;image denoising;learning (artificial intelligence);neural nets;radar imaging","stable Wasserstein generative adversarial network;effective Wasserstein generative adversarial network;training data set;WGAN-GP-based synthetic radar spectrogram augmentation;deep convolutional neural networks;DCNNs;radar-based human activity recognition;radar dataset;radar data acquisition;generative adversarial networks;GANs;microDoppler signatures","","2","","13","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Exploring the Potential of Conditional Adversarial Networks for Optical and SAR Image Matching","N. Merkle; S. Auer; R. Müller; P. Reinartz","Remote Sensing Technology Institute of the German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute of the German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute of the German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute of the German Aerospace Center (DLR), Wessling, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Jul 2018","2018","11","6","1811","1820","Tasks such as the monitoring of natural disasters or the detection of change highly benefit from complementary information about an area or a specific object of interest. The required information is provided by fusing high accurate coregistered and georeferenced datasets. Aligned high-resolution optical and synthetic aperture radar (SAR) data additionally enable an absolute geolocation accuracy improvement of the optical images by extracting accurate and reliable ground control points (GCPs) from the SAR images. In this paper, we investigate the applicability of a deep learning based matching concept for the generation of precise and accurate GCPs from SAR satellite images by matching optical and SAR images. To this end, conditional generative adversarial networks (cGANs) are trained to generate SAR-like image patches from optical images. For training and testing, optical and SAR image patches are extracted from TerraSAR-X and PRISM image pairs covering greater urban areas spread over Europe. The artificially generated patches are then used to improve the conditions for three known matching approaches based on normalized cross-correlation (NCC), scale-invariant feature transform (SIFT), and binary robust invariant scalable key (BRISK), which are normally not usable for the matching of optical and SAR images. The results validate that a NCC-, SIFT-, and BRISK-based matching greatly benefit, in terms of matching accuracy and precision, from the use of the artificial templates. The comparison with two state-of-the-art optical and SAR matching approaches shows the potential of the proposed method but also revealed some challenges and the necessity for further developments.","2151-1535","","10.1109/JSTARS.2018.2803212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8328024","Artificial image generation;conditional generative adversarial networks (cGANs);multisensor image matching;optical satellite images;synthetic aperture radar (SAR)","Optical sensors;Optical imaging;Adaptive optics;Synthetic aperture radar;Optical distortion;Biomedical optical imaging;Geology","geophysical image processing;geophysical techniques;image matching;image registration;synthetic aperture radar","conditional adversarial networks;SAR image matching;high accurate coregistered datasets;georeferenced datasets;high-resolution optical;synthetic aperture radar data;absolute geolocation accuracy improvement;optical images;extracting accurate ground control points;reliable ground control points;matching concept;precise GCPs;accurate GCPs;SAR satellite images;conditional generative adversarial networks;SAR-like image patches;PRISM image pairs;artificially generated patches;known matching approaches;scale-invariant feature;BRISK-based matching;matching accuracy;state-of-the-art optical;SAR matching approaches;natural disasters;deep learning based matching concept;TerraSAR-X image pair;urban areas;Europe;normalized cross-correlation;binary robust invariant scalable key;artificial templates","","90","","35","IEEE","29 Mar 2018","","","IEEE","IEEE Journals"
"Class-Wise Distribution Adaptation for Unsupervised Classification of Hyperspectral Remote Sensing Images","Z. Liu; L. Ma; Q. Du","School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; Key Laboratory of Spectral Imaging Technology, Chinese Academy of Sciences, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","508","521","Class-wise adversarial adaptation networks are investigated for the classification of hyperspectral remote sensing images in this article. By adversarial learning between the feature extractor and the multiple domain discriminators, domain-invariant features are generated. Moreover, a probability-prediction-based maximum mean discrepancy (MMD) method is introduced to the adversarial adaptation network to achieve a superior feature-alignment performance. The class-wise adversarial adaptation in conjunction with the class-wise probability MMD is denoted as the class-wise distribution adaptation (CDA) network. The proposed CDA does not require labeled information in the target domain and can achieve an unsupervised classification of the target image. The experimental results using the Hyperion and Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) hyperspectral data demonstrated its efficiency.","1558-0644","","10.1109/TGRS.2020.2997863","National Natural Science Foundations of China(grant numbers:61771437,61102104,91442201); Open Research Fund of Key Laboratory of Spectral Imaging Technology, Chinese Academy of Sciences(grant numbers:LSIT201702D); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9112308","Adversarial learning;classification;domain adaptation;remote sensing","Feature extraction;Hyperspectral imaging;Neural networks;Generative adversarial networks;Gallium nitride","feature extraction;geophysical image processing;hyperspectral imaging;image classification;infrared imaging;probability;remote sensing;unsupervised learning","unsupervised classification;hyperspectral remote sensing image classification;class-wise adversarial adaptation networks;adversarial learning;feature extractor;multiple domain discriminators;domain-invariant features;probability-prediction-based maximum mean discrepancy method;class-wise probability MMD;class-wise distribution adaptation network;feature-alignment performance;airborne visible-infrared imaging spectrometer hyperspectral data;Hyperion hyperspectral data;CDA network","","27","","43","IEEE","9 Jun 2020","","","IEEE","IEEE Journals"
"Generating Natural Adversarial Remote Sensing Images","J. -C. Burnel; K. Fatras; R. Flamary; N. Courty","CNRS, IRISA, UMR 6074, University Bretagne Sud, Vannes, France; CNRS, IRISA, UMR 6074, University Bretagne Sud, Vannes, France; Centre de Mathématiques Appliquées (CMAP), École Polytechnique, Palaiseau, France; CNRS, IRISA, UMR 6074, University Bretagne Sud, Vannes, France","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","14","Over the last years, remote sensing image (RSI) analysis has started resorting to using deep neural networks to solve most of the commonly faced problems, such as detection, land cover classification, or segmentation. As far as critical decision-making can be based upon the results of RSI analysis, it is important to clearly identify and understand potential security threats occurring in those machine learning algorithms. Notably, it has recently been found that neural networks are particularly sensitive to carefully designed attacks, generally crafted given the full knowledge of the considered deep network. In this article, we consider the more realistic but challenging case where one wants to generate such attacks in the case of a black-box neural network. In this case, only the prediction score of the network is accessible, on a specific dataset. Examples that lure away the network’s prediction, while being perceptually similar to real images, are called natural or unrestricted adversarial examples. We present an original method to generate such examples based on a variant of the Wasserstein generative adversarial network. We demonstrate its effectiveness on natural adversarial hyperspectral image generation and image modification for fooling a state-of-the-art detector. Among others, we also conduct a perceptual evaluation with human annotators to better assess the effectiveness of the proposed method. Our code is available for the community: https://github.com/PythonOT/ARWGAN.","1558-0644","","10.1109/TGRS.2021.3110601","OATMIL(grant numbers:ANR-17-CE23-0012); 3IA Cote d’Azur Investments of French National Research Agency (ANR)(grant numbers:ANR-19-P3IA-0002); 3rd Programme d’Investissements d’Avenir(grant numbers:ANR-18-EUR-0006-02); Chair “Challenging Technology for Responsible Energy” led by l’X—Ecole polytechnique and the Fondation de l’Ecole polytechnique through Total; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9542938","Adversarial examples;deep learning;generative models;remote sensing","Generators;Training;Perturbation methods;Neural networks;Generative adversarial networks;Security;Inverters","cartography;deep learning (artificial intelligence);feature extraction;geophysical image processing;hyperspectral imaging;image classification;image segmentation;remote sensing;security of data","hyperspectral image generation;image modification;remote sensing image analysis;deep neural networks;decision making;RSI analysis;black-box neural network;Wasserstein generative adversarial network;generating natural adversarial;machine learning algorithms","","2","","57","IEEE","21 Sep 2021","","","IEEE","IEEE Journals"
"Weakly Supervised Region of Interest Extraction Based on Uncertainty-Aware Self-Refinement Learning for Remote Sensing Images","Y. Liu; L. Zhang","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","30 Aug 2022","2022","60","","1","16","Region of interest (ROI) extraction plays a significant role in the field of remote sensing image (RSI) processing. Recently, weakly supervised ROI extraction methods have attracted considerable attention due to low labeling cost. Most of them follow the pipeline of first generating pseudo labels and then using the pseudo labels to train a segmentation model. However, there remain problems to be solved: 1) the unbalanced distribution of foreground and background samples in the RSI dataset influences the network performance; 2) the pseudo labels mainly cover the most discriminative part of object regions that are incomplete; and 3) training with pseudo labels inevitably causes noise issues that degrade the model performance. To solve these issues, we propose a weakly supervised uncertainty-aware self-refinement learning (UASRL) method, where the initial unbalanced image-level labels are progressively refined to high-quality pixel-level annotations. In the proposed UASRL, we first present a deep generative model combined with self-attention modules to improve the unbalanced distribution in the weakly labeled dataset. Then, we design a confidence-weighted complementary erasing-based weakly supervised method to generate pseudo labels with high integrity. Finally, for training with noisy pseudo labels, we develop an uncertainty-aware joint optimization (UAJO) training strategy to reduce the negative effect caused by noisy labels and further refine pixelwise labels in a coarse-to-accurate manner, which in turn jointly promotes the model’s performance. Extensive experiments on three types of RSI datasets reveal that our proposed method is superior to other competing methods and shows a preferable tradeoff between annotation cost and detection performance.","1558-0644","","10.1109/TGRS.2022.3199028","Beijing Natural Science Foundation(grant numbers:4222046); National Natural Science Foundation of China(grant numbers:61571050,41771407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857944","Region of interest (ROI) extraction;remote sensing images (RSIs);saliency analysis;weakly supervised learning","Training;Annotations;Data models;Generative adversarial networks;Noise measurement;Costs;Analytical models","feature extraction;geophysical image processing;image classification;image segmentation;learning (artificial intelligence);object detection;remote sensing","RSI dataset;self-refinement learning;deep generative model;confidence-weighted complementary erasing-based weakly supervised method;uncertainty-aware joint optimization training strategy;region of interest extraction;remote sensing image processing;weakly supervised ROI extraction methods;unbalanced image-level labels;weakly supervised uncertainty-aware self-refinement learning;UAJO;UASRL;self-attention modules","","","","63","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"SI-SA GAN: A Generative Adversarial Network Combined With Spatial Information and Self-Attention for Removing Thin Cloud in Optical Remote Sensing Images","J. Liu; W. Hou; X. Luo; J. Su; Y. Hou; Z. Wang","School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang, China; School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang, China; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang, China; School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang, China; School of Information Science and Engineering, Hebei University of Science and Technology, Shijiazhuang, China","IEEE Access","7 Nov 2022","2022","10","","114318","114330","In agricultural remote sensing monitoring, climate often affects the quality of optical remote sensing image data acquisition. The acquired satellite imagery results usually contain cloud information, leading to a lack of ground data information. Unlike thick clouds, the semi-transparent nature of thin clouds prevents thin clouds from completely obscuring the ground scene. In order to remove thin clouds in the cultivated land and restore the actual ground information as much as possible, we proposed a cloud removal method of spatial information fusion self-attention generative adversarial network (SI-SA GAN) based on multi-directional perceptual attention and self-attention mechanism. The proposed method identifies and focuses on cloud regions using spatial attention, channel attention, and self-attention mechanism, which can enhance image information. The modules of the discriminator utilize residual networks and self-attention non-local neural networks to guide image information output. The generative adversarial network (GAN) is applied to remove clouds and restore the corresponding irregular occlusion area according to the depth characteristics of the input information. A gradient penalty is applied to improve the robustness of the generative network. In this paper, we compared the evaluation indexes of other advanced models. The qualitative and quantitative results of Sentinel-2A and public RICE datasets confirmed that the proposed method could enhance image quality effectively after cloud removal. The model has excellent thin cloud removal performance with small-scale training data.","2169-3536","","10.1109/ACCESS.2022.3213354","Science and Technology Program of Hebei(grant numbers:21355901D,20355901D,19255901D); Science and Technology Program of Sichuan(grant numbers:2020YFG0240,2020YFG0055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915386","Remote sensing;thin cloud removal;GAN;self-attention;spatial multi-directional perception","Generative adversarial networks;Spatial resolution;Agriculture;Remote sensing;Feature extraction;Optical sensors","agriculture;atmospheric optics;clouds;data acquisition;geophysical image processing;image enhancement;neural nets;remote sensing","image information output;image quality;generative adversarial network;agricultural remote sensing monitoring;optical remote sensing image data acquisition;cloud information;ground data information;cloud removal method;spatial information fusion self-attention generative adversarial network;multidirectional perceptual attention;cloud regions;SI-SA GAN;thin cloud;satellite imagery;cultivated land;residual networks;self-attention nonlocal neural networks","","","","39","CCBY","10 Oct 2022","","","IEEE","IEEE Journals"
"SAR Image Ship Object Generation and Classification With Improved Residual Conditional Generative Adversarial Network","L. Li; C. Wang; H. Zhang; B. Zhang","College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2021","2022","19","","1","5","Image generation by the conditional generative adversarial network (CGAN) can provide rich samples for supervised classification tasks. However, the unstable gradient updating problem makes it difficult to be used for image generation, especially for the high-resolution SAR image. In this letter, an improved residual condition generation network is proposed to enhance the quality of generated images and classification accuracy for hard negative examples. First, a residual convolutional-based block is built to clear the detail texture of different types of targets. Then, the gradient penalty and Wasserstein loss are used as discriminators to improve the similarity between real samples and the intra diversity of a generated images. We test the proposed method on generation and classification of three kinds of commercial ships using C-band 3-m Gaofen-3 (GF-3) SAR images. Experimental results show that our method can generate high-quality ship samples and achieve good classification accuracy.","1558-0571","","10.1109/LGRS.2020.3016692","National Natural Science Foundation of China(grant numbers:41930110); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9180260","Generative adversarial network (GAN);high-resolution image generation;SAR;ship classification","Marine vehicles;Synthetic aperture radar;Gallium nitride;Generators;Generative adversarial networks;Image generation;Training","image classification;image resolution;image texture;neural nets;object detection;radar imaging;ships;synthetic aperture radar","CGAN;SAR image ship object classification;residual convolutional-based block;improved residual condition generation network;high-resolution SAR image;supervised classification tasks;image generation;improved residual conditional generative adversarial network;SAR image ship object generation;good classification accuracy;high-quality ship samples","","4","","24","IEEE","31 Aug 2020","","","IEEE","IEEE Journals"
"LiDAR Data Classification Based on Improved Conditional Generative Adversarial Networks","A. Wang; D. Xue; H. Wu; Y. Iwahori","Higher Educational Key Laboratory for Measuring and Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Higher Educational Key Laboratory for Measuring and Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Higher Educational Key Laboratory for Measuring and Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Department of Computer Science, Chubu University, Aichi, Japan","IEEE Access","1 Dec 2020","2020","8","","209674","209686","Light detection and ranging (LiDAR) data contains the height of different objects and records the elevation information of ground objects, so it plays an important role in land classification. In recent years, deep learning has been widely used in LiDAR data classification due to its strong ability to extract features. However, deep learning methods usually need sufficient training data to achieve better classification results. In order to solve this problem, a new classification method combined conditional generative adversarial network (CGAN) with residual unit and DropBlock, is proposed here for the classification of LiDAR data, called as RDB-CGAN. CGAN expands the generated samples to training data to improve the classification performance when the training samples are relatively small. Residual unit increases the network depth of the generator to improve its generation capability and utilizes shortcut connection to transfer the input information directly to the output to solve degradation caused by increased network depth. DropBlock improved the generalization of the network by dropping a whole area with spatial information correlation so that the network can learn the remaining features. The experimental results on two different LiDAR datasets show that RDB-CGAN significantly improved the classification performance of LiDAR data compared to several state-of-the-art classification methods.","2169-3536","","10.1109/ACCESS.2020.3039211","National Natural Science Foundation of China(grant numbers:NSFC-61671190); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264189","Data classification;light detection and ranging (LiDAR);conditional generative adversarial network (CGAN);residual unit;DropBlock","Laser radar;Generative adversarial networks;Training;Generators;Feature extraction;Data models;Hyperspectral imaging","feature extraction;image classification;learning (artificial intelligence);neural nets;optical radar;pattern classification;radar imaging","LiDAR data classification;improved conditional generative adversarial networks;ground objects;land classification;deep learning methods;classification method;conditional generative adversarial network;residual unit;RDB-CGAN;classification performance;generation capability;increased network depth;LiDAR datasets;state-of-the-art classification methods","","2","","33","CCBY","19 Nov 2020","","","IEEE","IEEE Journals"
"A Stepwise Domain Adaptive Segmentation Network With Covariate Shift Alleviation for Remote Sensing Imagery","J. Li; S. Zi; R. Song; Y. Li; Y. Hu; Q. Du","Key Laboratory of Spectral Imaging Technology, Chinese Academy of Sciences, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; CVLab, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2022","2022","60","","1","15","Semantic segmentation for remote sensing images (RSI) is critical for the Earth monitoring system. However, the covariate shift between RSI datasets under different capture conditions cannot be alleviated by directly using the unsupervised domain adaptation (UDA) method, which negatively affects the segmentation accuracy in RSI. We propose a stepwise domain adaptive segmentation network with covariate shift alleviation (Cov-DA) for RSI parsing to solve this issue. Specifically, to alleviate domain shift generated by different sensors, both the source and target domains are projected into a colorspace with normalized distribution through an elaborate colorspace mapping unified module (CMUM). The color distributions of these two domains tend to be more uniform. Furthermore, in the target domain, the multistatistics joint evaluation module (MJEM) is proposed to capture different statistical characteristics of subscenarios for selecting plain scenarios regarded as high-confidence segmentation results to assist the further improvement of segmentation performance. In addition, a pyramid perceptual attention module (PPAM) containing omnidirectional features without computational burdens is added to our network for effectively enhancing the multiscale feature capture ability. In the cross-city DA experiments based on the International Society for Photogrammetry and Remote Sensing (ISPRS) and aerial benchmarks, the superiority of our algorithm is significantly demonstrated. Furthermore, we release a large-scale Martian terrain dataset noted as “Mars-Seg” containing 5 K images with pixel-level accurate annotations regarding issues, such as the lack of semantic segmentation datasets for unknown scenes.","1558-0644","","10.1109/TGRS.2022.3152587","National Key Research and Development Program of China(grant numbers:2018AAA0102702); National Nature Science Foundation of China(grant numbers:61901343,61801359,61701360,61671383,61571345); Science and Technology on Space Intelligent Control Laboratory(grant numbers:ZDSYS-2019-03); China Postdoctoral Science Special Foundation(grant numbers:2018T111019); Open Research Fund of CAS Key Laboratory of Spectral Imaging Technology(grant numbers:LSIT201924W); Fundamental Research Funds for the Central Universities; Innovation Fund of Xidian University(grant numbers:5001-20109215456); Higher Education Discipline Innovation Project(grant numbers:B08038); Innovation Fund of Xidian University(grant numbers:10221150004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716091","Covariate shift alleviation;semantic segmentation;stepwise;unsupervised domain adaptation (UDA)","Image segmentation;Semantics;Feature extraction;Training;Complexity theory;Adaptive systems;Generative adversarial networks","feature extraction;geophysical image processing;image colour analysis;image segmentation;object detection;photogrammetry;remote sensing;statistical analysis;unsupervised learning","target domain;aerial benchmarks;semantic segmentation datasets;stepwise domain adaptive segmentation network;covariate shift alleviation;remote sensing imagery;remote sensing images;RSI datasets;capture conditions;unsupervised domain adaptation method;RSI parsing;omnidirectional features;multiscale feature capture ability;cross-city DA experiments;colorspace mapping unified module;CMUM;color distribution;multistatistics joint evaluation module;statistical characteristics;high-confidence segmentation","","2","","59","IEEE","17 Feb 2022","","","IEEE","IEEE Journals"
"Cloud Sample Amplification Method Based on Improved CycleGAN","C. Yuan; H. Zou","School of Aerospace Information, Space Engineering University of PLA, Beijing, China; School of Aerospace Information, Space Engineering University of PLA, Beijing, China","2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Aug 2022","2022","10","","790","794","Cloud is one of the primary noise sources of optical satellite remote sensing images, which can obscure or interfere with ground object information in the image to varying degrees. Remote sensing scenes are complex and changeable, and the accurate labeling of cloud areas directly affects the learning performance of cloud pixel classification and segmentation network model. Aiming at the problem that it is challenging to train an automatic annotation model with good stability due to the small number of remote sensing image samples containing cloud and unbalanced samples of different cloud conditions, this paper constructed a cloud sample augmentation strategy model based on improved CycleGAN network. The generated image is a sample image with high visual consistency with the actual image. This strategy can rapidly expand the limited sample set and solve the training problem of a deep learning network under the condition of a small sample.","2693-2865","978-1-6654-2207-9","10.1109/ITAIC54216.2022.9836713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836713","remote sensing images;cloud;eycleGAN;generated image","Training;Visualization;Satellites;Generative adversarial networks;Data models;Stability analysis;Reliability","geophysical image processing;image classification;image segmentation;learning (artificial intelligence);neural nets;remote sensing","cloud sample amplification method;primary noise sources;optical satellite remote sensing images;ground object information;remote sensing scenes;accurate labeling;cloud areas;learning performance;cloud pixel classification;segmentation network model;automatic annotation model;remote sensing image samples;unbalanced samples;different cloud conditions;cloud sample augmentation strategy model;improved CycleGAN network;sample image;actual image;training problem;deep learning network","","","","14","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","Q. Liu; H. Zhou; Q. Xu; X. Liu; Y. Wang","Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of the Computer Science and Engineering, Beihang University, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2021","2021","59","12","10227","10242","This article addresses the problem of remote sensing image pan-sharpening from the perspective of generative adversarial learning. We propose a novel deep neural network-based method named pansharpening GAN (PSGAN). To the best of our knowledge, this is one of the first attempts at producing high-quality pan-sharpened images with generative adversarial networks (GANs). The PSGAN consists of two components: a generative network (i.e., generator) and a discriminative network (i.e., discriminator). The generator is designed to accept panchromatic (PAN) and multispectral (MS) images as inputs and maps them to the desired high-resolution (HR) MS images, and the discriminator implements the adversarial training strategy for generating higher fidelity pan-sharpened images. In this article, we evaluate several architectures and designs, namely, two-stream input, stacking input, batch normalization layer, and attention mechanism to find the optimal solution for pan-sharpening. Extensive experiments on QuickBird, GaoFen-2, and WorldView-2 satellite images demonstrate that the proposed PSGANs not only are effective in generating high-quality HR MS images and superior to state-of-the-art methods but also generalize well to full-scale images.","1558-0644","","10.1109/TGRS.2020.3042974","NSFC(grant numbers:41871283,61601011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306912","Convolutional neural network (CNN);deep learning;generative adversarial network (GAN);pan-sharpening;residual learning","Generative adversarial networks;Generators;Neural networks;Computer architecture;Training;Spatial resolution;Data models","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","PSGAN;generative adversarial network;remote sensing image pan-sharpening;generative adversarial learning;deep neural network-based method named pansharpening GAN;high-quality pan-sharpened images;generative network;discriminative network;MS images;adversarial training strategy;higher fidelity pan-sharpened images;WorldView-2 satellite images;full-scale images","","49","","77","IEEE","24 Dec 2020","","","IEEE","IEEE Journals"
"ECGAN: An Improved Conditional Generative Adversarial Network With Edge Detection to Augment Limited Training Data for the Classification of Remote Sensing Images With High Spatial Resolution","B. Sui; T. Jiang; Z. Zhang; X. Pan","College of Geodesy and Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geodesy and Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geodesy and Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geodesy and Geomatics, Shandong University of Science and Technology, Qingdao, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2021","2021","14","","1311","1325","The classification of remote sensing images with high spatial resolution requires considerable training samples, but the process of sample making is slow and laborious. How to guarantee the accuracy of supervised classification under the condition of limited samples is an urgent problem to be solved in the field of supervised classification. For addressing this problem, we propose an improved conditional generative adversarial network with edge feature (ECGAN) to augment limited training data for the classification of remote sensing images with high spatial resolution in this article. On the basis of conditional generative adversarial network, feature factors of interclass boundaries and intraclass edges are added to networks, and an objective function with multiscale and multilevel features is constructed. The ISPRS potsdam and Vaihingen remote sensing datasets are regarded as examples. Results indicate that the high-resolution remote sensing images generated by using the network proposed in this article have abundant texture, accurate edges, and are highly similar to real images. The generated images are used to augment training samples, and an experiment for classifying high-resolution remote sensing images is conducted. The classification results of the proposed augmentation method perform better than that of the traditional sample augmentation method. We prove that ECGAN as a means of sample augmentation can effectively solves the problem that the classification effect is unideal when the supervised classification sample is insufficient.","2151-1535","","10.1109/JSTARS.2020.3033529","National Natural Science Foundation of China(grant numbers:41801385); Natural Science Foundation of Shandong Province(grant numbers:ZR2018BD004,ZR2019QD010); Key Technology Research and Development Program of Shandong(grant numbers:2019GGX101049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240038","Conditional generative adversarial network (CGAN);edge feature extraction;high spatial resolution remote sensing image;image generation;sample augment","Image edge detection;Remote sensing;Spatial resolution;Generative adversarial networks;Training;Generators;Linear programming","edge detection;geophysical image processing;image classification;image resolution;image texture","high spatial resolution;high-resolution remote sensing images;traditional sample augmentation method;ISPRS potsdam;objective function;intraclass edges;interclass boundaries;image classification;edge feature;sample making;edge detection;improved conditional generative adversarial network;ECGAN;supervised classification sample","","3","","42","CCBY","26 Oct 2020","","","IEEE","IEEE Journals"
"Unsupervised Domain Adaptation Augmented by Mutually Boosted Attention for Semantic Segmentation of VHR Remote Sensing Images","X. Ma; X. Zhang; Z. Wang; M. -O. Pun","Shenzhen Research Institute of Big Data, Shenzhen, China; School of Information Science and Engineering, Wuhan University of Science and Technology, Wuhan, China; College of Mathematics, Sichuan University, Chengdu, Sichuan, China; Shenzhen Research Institute of Big Data, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","10 Feb 2023","2023","61","","1","15","This work investigates unsupervised domain adaptation (UDA)-based semantic segmentation of very high-resolution (VHR) remote sensing (RS) images from different domains. Most existing UDA methods resort to generative adversarial networks (GANs) to cope with the domain shift problem caused by the discrepancies across different domains. However, these GAN-based UDA methods directly align two domains in the appearance, latent, or output space based on convolutional neural networks (CNNs), making them ineffective in exploiting long-range dependencies across the high-level feature maps derived from different domains. Unfortunately, such high-level features play an essential role in characterizing RS images with complex content. To circumvent this obstacle, a mutually boosted attention transformer (MBATrans) is proposed to capture cross-domain dependencies of semantic feature representations in this work. Compared with conventional UDA methods, MBATrans can significantly reduce domain discrepancies by capturing transferable features using global attention. More specifically, MBATrans utilizes a novel mutually boosted attention (MBA) module to align cross-domain feature maps while enhancing domain-general features. Furthermore, a novel GAN-based network with improved discriminative capability is devised by integrating an additional discriminator to learn domain-specific features. Extensive experiments on two large-scale VHR RS datasets, namely, International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen, confirm the superior performance of the proposed MBATrans-augmented GAN (MBATA-GAN) architecture. The source code in this work is available at https://github.com/sstary/SSRS.","1558-0644","","10.1109/TGRS.2023.3240982","National Key Research and Development Program of China(grant numbers:2018YFB1800800); Basic Research of Hetao Shenzhen-HK S&T Cooperation Zone; Shenzhen Outstanding Talents Training Fund 202002; Guangdong Research; Guangdong Provincial Key Laboratory of Future Networks of Intelligence(grant numbers:2022B1212010001); National Natural Science Foundation of China(grant numbers:41801323,62203313); China Postdoctoral Science Foundation(grant numbers:2020M682038); National Key Research and Development Program of China(grant numbers:2020YFA0714003); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC1853); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032584","Generative adversarial networks (GANs);mutually boosted attention (MBA);remote sensing (RS) image;unsupervised domain adaptation (UDA)","Remote sensing;Feature extraction;Task analysis;Semantics;Transformers;Training;Semantic segmentation","","","","","","47","IEEE","31 Jan 2023","","","IEEE","IEEE Journals"
"Infrared Image Super Resolution with Deep Neural Networks","K. Vassilo; T. Taha; A. Mehmood","University of Dayton, Dayton, OH; University of Dayton, Dayton, OH; Air Force Research Laboratory, Wright-Patterson AFB, OH","2021 11th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Jul 2021","2021","","","1","5","Recent studies have shown that Deep Learning (DL) algorithms can significantly improve Super Resolution (SR) performance. Single image SR is useful in producing High Resolution (HR) images from their Low Resolution (LR) counterparts. The motivation for SR is the potential to assist algorithms such as object detection, localization, and classification. Insufficient work has been conducted using Generative Adversarial Networks (GANs) for SR on infrared (IR) images despite its promising ability to increase object detection accuracy by extracting more precise features from a given image. This work adopts the idea of a relativistic GAN that utilizes Residual in Residual Dense blocks (RRDBs) for feature ex- traction, a novel residual image addition, and a Pixel Transposed Convolutional Layer (PixelTCL) for up-sampling. Recent work has validated the use of GANs for Visible Light (VL) images, making them a strong candidate. The inclusion of these components produce more realistic and natural features while also receiving superior metric values.","2158-6276","978-1-6654-3601-4","10.1109/WHISPERS52202.2021.9484045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484045","Deep Learning;Super Resolution;Generative Adversarial Network;Infrared Imaging","Deep learning;Image resolution;Object detection;Generative adversarial networks;Feature extraction;Generators;Classification algorithms","convolutional neural nets;deep learning (artificial intelligence);image resolution;object detection","residual in residual dense blocks;PixelTCL;VL images;residual image addition;pixel transposed convolutional layer;natural features;realistic features;visible light images;feature extraction;residual dense blocks;relativistic GAN;object detection accuracy;infrared images;GANs;generative adversarial networks;low resolution images;high resolution images;single image SR;super resolution performance;deep neural networks;image super resolution","","","","14","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"SAR Image Super-Resolution Based on Noise-Free Generative Adversarial Network","F. Gu; H. Zhang; C. Wang; F. Wu","Key Laboratory of Digital Earth Science, of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China; Key Laboratory of Digital Earth Science, of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China; Key Laboratory of Digital Earth Science, of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China; Key Laboratory of Digital Earth Science, of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2575","2578","Deep learning has been successfully applied to the ordinary image super-resolution (SR). However, since the synthetic aperture radar (SAR) images are often disturbed by multiplicative noise known as speckle and more blurry than ordinary images, there are few deep learning methods for the SAR image SR. In this paper, a deep generative adversarial network (DGAN) is proposed to reconstruct the pseudo high-resolution (HR) SAR images. First, a generator network is constructed to remove the noise of low-resolution SAR image and generate HR SAR image. Second, a discriminator network is used to differentiate between the pseudo super-resolution images and the realistic HR images. The adversarial objective function is introduced to make the pseudo HR SAR images closer to real SAR images. The experimental results show that our method can maintain the SAR image content with high-level noise suppression. The performance evaluation based on peak signal-to-noise-ratio and structural similarity index shows the superiority of the proposed method to the conventional CNN baselines.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899202","Synthetic aperture radar;super-resolution;generative adversarial network","Radar polarimetry;Image reconstruction;Generators;Training;Generative adversarial networks","image denoising;image resolution;learning (artificial intelligence);neural nets;radar computing;radar imaging;radar resolution;synthetic aperture radar","deep generative adversarial network;realistic HR images;adversarial objective function;synthetic aperture radar images;SAR image superresolution;noise free generative adversarial network;deep learning;DGAN;pseudo high-resolution SAR images","","7","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"End-to-End Semantic Segmentation and Boundary Regularization of Buildings from Satellite Imagery","Q. Li; S. Zorzi; Y. Shi; F. Fraundorfer; X. X. Zhu","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany; Institute of Computer Graphics and Vision, Graz University of Technology (TU Graz), Graz, Austria; Remote Sensing Technology, Technical University of Munich (TUM), Munich, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2508","2511","Building footprint generation is a vital task of satellite imagery interpretation. However, the segmentation masks of buildings obtained by existing semantic segmentation networks often have blurred boundaries and irregular shapes. In this research, we propose a new boundary regularization network for building footprint generation in satellite images. More specifically, we consider semantic segmentation and boundary regularization in an end-to-end generative adversarial network (GAN). The learned building footprints are regularized by the interplay between the generator and discriminator. By doing so, the straight boundaries and geometric details of the building could be preserved. Experiments are conducted on a collected dataset of Planetscope satellite imagery (spatial resolution: 4.77 m/pixel). Our approach is much superior to the state-of-the-art methods in both quantitative and qualitative results.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555147","European Research Council (ERC)(grant numbers:ERC-2016-StG-714087); Helmholtz Association(grant numbers:VH-NG-1018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555147","semantic segmentation;boundary regularization;building;satellite imagery;generative adversarial network","Image segmentation;Visualization;Satellites;Shape;Buildings;Semantics;Generative adversarial networks","feature extraction;geophysical image processing;image segmentation;neural nets;object detection;solid modelling","end-to-end generative adversarial network;learned building footprints;straight boundaries;end-to-end semantic segmentation;building footprint generation;vital task;satellite imagery interpretation;semantic segmentation networks;boundary regularization network;satellite images;planetscope satellite imagery;GAN","","1","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Gan-Based Domain Adaptation for Object Classification","M. B. Bejiga; F. Melgani","Department of Information Engineering and Computer science, University of Trento, Trento, Italy; Department of Information Engineering and Computer science, University of Trento, Trento, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1264","1267","Recent trends in image classification focus on training deep neural networks that require having a large amount of training images related to the considered task. However, obtaining enough labeled image samples is often time-consuming and expensive. An alternative solution proposed is to transfer the knowledge learned while solving one problem to another but related problem, also called transfer learning. Domain adaptation is a type of transfer learning that deals with learning a model that performs well on two datasets that have different (but somehow correlated) data distributions. In this work, we present a new domain adaptation method based on generative adversarial networks (GANs) in the context of aerial image classification. Experimental results obtained on two datasets for a single object scenario show that the proposed method is particularly promising.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518649","Deep learning;domain adaptation;GAN;transfer learning","Gallium nitride;Training;Generators;Hidden Markov models;Cost function;Generative adversarial networks;Adaptation models","image classification;learning (artificial intelligence);neural nets","gan-based domain adaptation;object classification;image classification focus;deep neural networks;training images;labeled image samples;time-consuming;alternative solution;domain adaptation method;generative adversarial networks;aerial image classification;GAN","","7","2","18","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"GAN with ASPP for SAR Image to Optical Image Conversion","Z. Shao; X. Zhang; T. Zhang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3355","3358","Researchers can gain more intuitive information by converting synthetic aperture radar (SAR) images to optical images using generative adversarial networks (GANs). However, their GANs have poor feature extraction ability, which leads to color conversion errors and loss of details. Therefore, to solve this problem, we add an atrous spatial pyramid pooling (ASPP) module to GAN to enhance the feature extraction ability, i.e., ASPP-GAN. ASPP module can extract multi-resolution feature responses, enabling the network to focus on both overall and detailed features for better feature extraction ability. The experimental results on public SEN1-2 datasets show that ASPP-GAN has a significant improvement over the traditional GAN, i.e., Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM) values are improved by about 20%.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883157","Synthetic aperture radar (SAR);optical image;generative adversarial network (GAN);atrous spatial pyramid pooling (ASPP);image conversion","Optical losses;PSNR;Optical fiber networks;Generative adversarial networks;Feature extraction;Optical imaging;Radar polarimetry","feature extraction;image resolution;optical images;radar imaging;synthetic aperture radar;wavelet transforms","multiresolution feature responses;detailed features;ASPP-GAN;SAR image;optical image conversion;intuitive information;synthetic aperture radar images;optical images;generative adversarial networks;poor feature extraction ability;conversion errors;atrous spatial pyramid pooling module;ASPP module","","","","7","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"CycleGAN-STF: Spatiotemporal Fusion via CycleGAN-Based Image Generation","J. Chen; L. Wang; R. Feng; P. Liu; W. Han; X. Chen","Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Aerospace Information Research Institute (AIR), Chinese Academy of Sciences (CAS), Beijing, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2021","2021","59","7","5851","5865","Due to the trade-off of temporal resolution and spatial resolution, spatiotemporal image-fusion uses existing high-spatial-low-temporal (HSLT) and high-temporal-low-spatial (HTLS) images as prior knowledge to reconstruct high-temporal-high-spatial (HTHS) images. However, some existing spatiotemporal image-fusion algorithms ignore the issue that the spatial information of HTLS images is insufficient to support the acquisition of spatial information, which leads to the unsatisfactory accuracy of the fusion result. To introduce more spatial information, the algorithm in this article uses Cycle-generative adversarial networks (GANs) to simulate the change process of two HSLT images at k-1 and k+1, and to generate some simulated images between k-1 and k+1. Then, the generated images are selected under the help of HTLS images, and the selected ones are then enhanced with wavelet transform. Finally, the image with spatial information is introduced into the Flexible Spatiotemporal DAta Fusion (FSDAF) framework to improve the performance of spatiotemporal image-fusion. Extensive experiments on two real data sets demonstrate that our proposed method outperforms current state-of-the-art spatiotemporal image-fusion methods.","1558-0644","","10.1109/TGRS.2020.3023432","National Natural Science Foundation of China(grant numbers:U1711266,41925007,41701429,41371344); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206067","Generative adversarial network (GAN);image-generation;remote sensing;spatiotemporal image-fusion;wavelet transform","Spatiotemporal phenomena;Spatial resolution;Remote sensing;Wavelet transforms;Generative adversarial networks","image enhancement;image fusion;image resolution;neural nets;wavelet transforms","cycle generative adversarial networks;HSLT images;HTLS images;temporal resolution;spatial resolution;spatiotemporal image fusion;high temporal high spatial images;CycleGAN based image generation;flexible spatiotemporal data fusion framework;high temporal low spatial images;image enhancement;wavelet transform","","19","","52","IEEE","25 Sep 2020","","","IEEE","IEEE Journals"
"Bridging the Gap Between Geophysics and Geology With Generative Adversarial Networks","S. Song; T. Mukerji; J. Hou","Department of Energy Resources Engineering, Stanford University, Stanford, CA, USA; Department of Energy Resources Engineering, Stanford University, Stanford, CA, USA; College of Geoscience, China University of Petroleum (Beijing), Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","15 Dec 2021","2022","60","","1","11","Inverse mapping from geophysics to geology is a difficult problem due to the inherent uncertainty of geophysical data and the spatially heterogeneous patterns (structure) in geology. We improve conditional facies simulation based on GANs (GANSim), a new Earth model simulation method based on generative adversarial networks (GANs), to bridge the gap between remotely sensed geophysical information and geology, by introducing a specially designed loss function and an input architecture for probability maps representing geophysical interpretation. After training, the GANSim is then used to produce multiple geological facies models conditioned to the input geophysics-interpreted probability maps alone or together with input well observations and global features. By evaluation, the generated facies models are realistic, diversified, and consistent with all input conditions. We demonstrate that the GAN learns the implicit geological pattern knowledge from the training data set and the knowledge of conditioning to inputs from human-defined explicit functions. Given the commonality of remotely sensed geophysical information, sparse measurements, and global features, GANSim should be applicable to many problems of geosciences.","1558-0644","","10.1109/TGRS.2021.3066975","National Natural Science Foundation of China(grant numbers:42072146); Stanford Center for Earth Resources Forecasting and the Dean of the Stanford School of Earth, Energy, and Environmental Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399180","Generative adversarial networks (GANs);geological facies models;geological pattern;geomodeling;geophysics;probability maps","Geology;Gallium nitride;Generators;Training;Data models;Probability;Pipelines","","","","5","","23","IEEE","8 Apr 2021","","","IEEE","IEEE Journals"
"MLFF-GAN: A Multilevel Feature Fusion With GAN for Spatiotemporal Remote Sensing Images","B. Song; P. Liu; J. Li; L. Wang; L. Zhang; G. He; L. Chen; J. Liu","Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Guangdong Key Laboratory for Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory for Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","30 May 2022","2022","60","","1","16","Due to the limitation of technology and budget, it is often difficult for sensors of a single remote sensing satellite to have both high-temporal and high-spatial (HTHS) resolution at the same time. In this article, we proposed a new multilevel feature fusion with generative adversarial network (MLFF-GAN) for generating fusion HTHS images. The MLFF-GAN mainly uses U-net-like architecture, and its generator is composed of three stages: feature extraction, feature fusion, and image reconstruction. In the feature extraction and reconstruction stage, the generator employs the encoding and decoding structure to extract three groups of multilevel features (MLFs), which can cope with the huge difference of resolution between high-resolution images and low-resolution images. In the feature fusion stage, adaptive instance normalization (AdaIN) block is designed to learn the global distribution relationship between multitemporal images, and an attention module (AM) is used to learn the local information weights for the change of small areas. The proposed MLFF-GAN was tested on two Landsat and Moderate-Resolution Imaging Spectroradiometer (MODIS) datasets. Some state-of-the-art algorithms are comprehensively compared with MLFF-GAN. We also carried on the ablation experiment to test the effectiveness of different submodules in the MLFF-GAN. The experiment results and ablation analysis show the better performances of the proposed method when compared with other methods. The code is available at https://github.com/songbingze/MLFF-GAN.","1558-0644","","10.1109/TGRS.2022.3169916","National Natural Science Foundation of China(grant numbers:61731022,41971397); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781347","Adaptive instance normalization (AdaIN);generative adversarial network (GAN);spatial attention mechanism;U-net","Spatial resolution;Feature extraction;Generators;Generative adversarial networks;Spatiotemporal phenomena;Image resolution;Image reconstruction","feature extraction;geophysical image processing;image coding;image fusion;image reconstruction;image resolution;neural nets;remote sensing","multitemporal images;spatiotemporal remote sensing images;single remote sensing satellite;feature extraction;image reconstruction;high-resolution images;multilevel feature fusion with generative adversarial network;HTHS images;MLFF-GAN;adaptive instance normalization block;attention module;moderate-resolution imaging spectroradiometer;high-temporal and high-spatial resolution","","3","","58","IEEE","24 May 2022","","","IEEE","IEEE Journals"
"Remote-Sensing Image Scene Classification With Deep Neural Networks in JPEG 2000 Compressed Domain","A. Preethy Byju; G. Sumbul; B. Demir; L. Bruzzone","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Faculty of Electrical Engineering and Computer Science, Technische Universität Berlin, Berlin, Germany; Faculty of Electrical Engineering and Computer Science, Technische Universität Berlin, Berlin, Germany; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Geoscience and Remote Sensing","24 Mar 2021","2021","59","4","3458","3472","To reduce the storage requirements, remote-sensing (RS) images are usually stored in compressed format. Existing scene classification approaches using deep neural networks (DNNs) require to fully decompress the images, which is a computationally demanding task in operational applications. To address this issue, in this article, we propose a novel approach to achieve scene classification in Joint Photographic Experts Group (JPEG) 2000 compressed RS images. The proposed approach consists of two main steps: 1) approximation of the finer resolution subbands of reversible biorthogonal wavelet filters used in JPEG 2000 and 2) characterization of the high-level semantic content of approximated wavelet subbands and scene classification based on the learned descriptors. This is achieved by taking codestreams associated with the coarsest resolution wavelet subband as input to approximate finer resolution subbands using a number of transposed convolutional layers. Then, a series of convolutional layers models the high-level semantic content of the approximated wavelet subband. Thus, the proposed approach models the multiresolution paradigm given in the JPEG 2000 compression algorithm in an end-to-end trainable unified neural network. In the classification stage, the proposed approach takes only the coarsest resolution wavelet subbands as input, thereby reducing the time required to apply decoding. Experimental results performed on two benchmark aerial image archives demonstrate that the proposed approach significantly reduces the computational time with similar classification accuracies when compared with traditional RS scene classification approaches (which requires full image decompression).","1558-0644","","10.1109/TGRS.2020.3007523","European Research Council (ERC) through the ERC-2017-STG BigEarth Project(grant numbers:759764); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9144368","Compressed image domain;deep neural networks (DNNs);joint photographic experts group (JPEG) 2000;remote sensing (RS);scene classification","Transform coding;Image coding;Image resolution;Gallium nitride;Generative adversarial networks;Neural networks;Semantics","data compression;feature extraction;geophysical image processing;image classification;image coding;image representation;image resolution;learning (artificial intelligence);neural nets;wavelet transforms","image decompression;traditional RS scene classification approaches;similar classification accuracies;benchmark aerial image archives;classification stage;neural network;JPEG 2000 compression algorithm;convolutional layers models;transposed convolutional layers;approximate finer resolution subbands;coarsest resolution wavelet subband;approximated wavelet subband;high-level semantic content;reversible biorthogonal wavelet filters;Joint Photographic Experts Group 2000 compressed RS images;computationally demanding task;compressed format;remote-sensing images;storage requirements;JPEG 2000 compressed domain;deep neural networks;remote-sensing image scene classification","","10","","44","IEEE","20 Jul 2020","","","IEEE","IEEE Journals"
"Thin Cloud Removal Fusing Full Spectral and Spatial Features for Sentinel-2 Imagery","J. Li; Y. Zhang; Q. Sheng; Z. Wu; B. Wang; Z. Hu; G. Shen; M. Schmitt; M. Molinier","College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; College of Astronautics, Nanjing University of Aeronautics and Astronautics, Nanjing, China; MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area, Shenzhen University, Shenzhen, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Aerospace Engineering, University of the Bundeswehr Munich, Neubiberg, Germany; VTT Technical Research Centre of Finland, Ltd., Espoo, Finland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Oct 2022","2022","15","","8759","8775","Multispectral remote sensing images are widely used for monitoring the globe. Although thin clouds can affect all optical bands, the influences of thin clouds differ with band wavelength. When processing multispectral bands at different resolutions, many methods only remove thin clouds in visible/near-infrared bands or rescale multiresolution bands to the same resolution and then process them together. The former cannot make full use of multispectral information, and in the latter, the rescaling process will introduce noise. In this article, a deep-learning-based thin cloud removal method that fuses full spectral and spatial features in original Sentinel-2 bands is proposed, named CR4S2. A multi-input and output architecture is designed for better fusing information in all bands and reconstructing the background at original resolutions. In addition, two parallel downsampling residual blocks are designed to transfer features extracted from different depths to the bottom of the network. Experiments were conducted on a new globally distributed Sentinel-2 thin cloud removal dataset called WHUS2-CRv. The results show that the best averaged peak signal-to-noise ratio, structural similarity index measurement, normalized root-mean-square error, and spectral angle mapper of the proposed method over 12 bands in all 20 testing images were 39.55, 0.9443, 0.0245, and 2.5676°, respectively. Compared with baseline methods, the proposed CR4S2 method can better restore not only the spatial features but also spectral features. This indicates that the proposed method is very promising for removing thin clouds in multispectral remote sensing images at different resolutions.","2151-1535","","10.1109/JSTARS.2022.3211857","Natural Science Foundation of Jiangsu Province(grant numbers:BK20220888); National Key Laboratory Foundation of China(grant numbers:2021-JCJQ-LB-006,8676142411442120); National Key Laboratory of Science and Technology on Space Microwave(grant numbers:HTKJ2022KL504018); Academy of Finland; Finnish Flagship Programme FCAI: Finnish Center for Artificial Intelligence(grant numbers:320183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910392","Deep learning (DL);multifeature fusion;parallel downsample residual block (PDRB);Sentinel-2;thin cloud removal","Clouds;Cloud computing;Remote sensing;Spatial resolution;Feature extraction;Vegetation mapping;Generative adversarial networks","feature extraction;geophysical image processing;image resolution;learning (artificial intelligence);mean square error methods;remote sensing","thin cloud removal fusing;spatial features;Sentinel-2 imagery;multispectral remote sensing images;optical bands;band wavelength;multispectral bands;rescale multiresolution bands;multispectral information;rescaling process;cloud removal method;original Sentinel-2 bands;named CR4S2;output architecture;fusing information;original resolutions;cloud removal dataset;WHUS2-CRv;peak signal-to-noise ratio;spectral angle mapper;baseline methods;spectral features","","","","63","CCBY","4 Oct 2022","","","IEEE","IEEE Journals"
"Lidar Data Classification Algorithm Based on Generative Adversarial Network","A. Wang; Y. Li; K. Jiang; L. Zhao; Y. Iwahori","Higher Education Key Lab for Measuring & Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Higher Education Key Lab for Measuring & Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Higher Education Key Lab for Measuring & Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Higher Education Key Lab for Measuring & Control Technology and Instrumentations of Heilongjiang, Harbin University of Science and Technology, Harbin, China; Computer Science, Chubu University, Aichi, Japan","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2487","2490","In this paper, the Generative Adversarial Network (GAN) is applied to LiDAR data classification. Generative Adversarial Network usually includes a generating network and a discriminant network. In GAN, a convolutional neural network (CNN) is designed to distinguish inputs. Another CNN is used to generate so-called false inputs. Combining with the actual training samples, the discriminant CNN is fine-tuned to improve the final classification performance. The proposed classifier is implemented on real data sets. The results show that the accuracy of the proposed network is higher than that of the classification method based on CNN, which shows that the features extracted by the network have better discrimination and stronger competitiveness.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899182","LiDAR;GAN;CNN;fine-tune","Training;Gallium nitride;Laser radar;Generators;Feature extraction;Generative adversarial networks;Remote sensing","convolutional neural nets;feature extraction;geophysical image processing;image classification;optical radar;remote sensing","generating network;discriminant network;convolutional neural network;generative adversarial network;LiDAR data classification algorithm;discriminant CNN;classifier;feature extraction","","1","","13","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"3D Point Cloud Generation Using Adversarial Training for Large-Scale Outdoor Scene","T. Shinohara; H. Xiu; M. Matsuoka","Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan; Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan; Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2935","2938","Three-dimensional (3D) point clouds are becoming an important part of the geospatial domain. During research on 3D point clouds, deep-learning models have been widely used for the classification and segmentation of 3D point clouds observed by airborne LiDAR. However, most previous studies used discriminative models, whereas few studies used generative models. Specifically, one unsolved problem is the synthesis of large-scale 3D point clouds, such as those observed in outdoor scenes, because of the 3D point clouds' complex geometric structure. In this paper, we propose a generative model for generating large-scale 3D point clouds observed from airborne LiDAR. Generally, because the training process of the famous generative model called generative adversarial network (GAN) is unstable, we combine a variational autoen-coder and GAN to generate a suitable 3D point cloud. We experimentally demonstrate that our framework can generate high-density 3D point clouds by using data from the 2018 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554523","KAKENHI(grant numbers:19H02408); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554523","Generative Adversarial Network;Variational Autoencoder;Deep Learning;Point Clouds;Airborne LiDAR","Training;Solid modeling;Three-dimensional displays;Laser radar;Atmospheric modeling;Geoscience and remote sensing;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;learning (artificial intelligence);optical radar;remote sensing by laser beam;sensor fusion;terrain mapping","airborne LiDAR;large-scale 3D point clouds;famous generative model;generative adversarial network;suitable 3D point cloud;high-density 3D point clouds;3D point cloud generation;large-scale outdoor scene;three-dimensional point clouds;deep-learning models","","","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Detail-Preservation Method of Deep Learning One-Step Phase Unwrapping","X. Ye; J. Qian; Y. Wang; H. Yu; L. Wang","School of Resources and Environment, UESTC, Chengdu, Sichuan, China; School of Information and Communication Engineering, UESTC, Chengdu, Sichuan, China; Dept. of Geography, Planning, and Environment, East Carolina University, Greenville, NC, USA; School of Resources and Environment, UESTC, Chengdu, Sichuan, China; Institute for Infocomm Research (I2R), Agency for Science, Technology, and Research (A*STAR), Singapore","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1115","1118","Phase unwrapping is essential in interferometric synthetic aperture radar (InSAR) data processing. Currently, deep learning is widely used in the phase unwrapping process. For instance, the one-step phase unwrapping method is excellent because of its strong noise adaptability. The method treats the unwrapping process as a regression problem, which uses the l1 or l2 loss function to constrain the reconstructed phase to be close to the ground truth of the absolute phase. However, no matter whether the l1 or l2 loss function is used, the result may lack details in texture, and the details cannot be well preserved. This is because the l1 or l2 smoothens the output greatly, and the texture detail loss is not intentionally considered. Due to the noise of the wrapped phase, there is speckle noise in the valley part of the unwrapping result. To solve these problems, we study the generative adversarial network (GAN) with mixed loss functions. The texture details are preserved with the trained GAN, and the speckle noise in the valley is significantly reduced.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883456","National Natural Science Foundation of China(grant numbers:61401077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883456","Deep learning;Digital elevation model;InSAR phase unwrapping;texture","Deep learning;Geoscience and remote sensing;Speckle;Generative adversarial networks;Data processing;Synthetic aperture radar","radar imaging;radar interferometry;remote sensing by radar;speckle;synthetic aperture radar","speckle noise;detail-preservation method;deep learning;interferometric synthetic aperture radar;data processing;phase unwrapping process;one-step phase unwrapping method;strong noise adaptability;regression problem;l2 loss function;reconstructed phase;absolute phase;wrapped phase;unwrapping result;mixed loss functions;texture details","","","","7","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Cycle GAN Based Heterogeneous Spatial-Spectral Fusion for Soil Moisture Downscaling","M. Jiang; H. Shen; J. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","4819","4822","Soil moisture (SM) downscaling aims to solve the coarse resolution problem of passive microwave SM products. On the basis of SMAP SM products and related MODIS products, this study develops a deep residual cycle generative adversarial network (GAN) based heterogeneous spatial-spectral fusion method to downscale SMAP SM from 36km to 9km. On the one hand, the proposed method creatively regards the MODIS products that can reflect the SM state as the spectral features of SM in a broad sense and performs the heterogeneous spatial-spectral fusion between the low-resolution (LR) SM product and high-resolution (HR) MODIS products. On the other hand, considering the spatial correlation of SM, the proposed method utilizes a deep residual cycle generative adversarial network (GAN) to extract and fuse features of heterogeneous images through convolutions. Both qualitative and quantitative evaluation of experimental results shows that the proposed method can generate high accuracy SM products.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884702","Soil moisture;downscale;heterogeneous spatial-spectral fusion;cycle GAN","Fuses;Soil moisture;Geoscience and remote sensing;Generative adversarial networks;Feature extraction;Microwave theory and techniques;Spatial resolution","hydrological techniques;image fusion;image resolution;microwave measurement;moisture;remote sensing;soil","cycle GAN;heterogeneous spatial-spectral fusion;soil moisture downscaling;coarse resolution problem;passive microwave SM products;SMAP SM products;related MODIS products;deep residual cycle generative adversarial network;spatial-spectral fusion method;downscale SMAP SM;SM state;spectral features;low-resolution SM product;heterogeneous images;high accuracy SM products;size 9.0 km to 36.0 km","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Unsupervised Change Detection in Satellite Images With Generative Adversarial Network","C. Ren; X. Wang; J. Gao; X. Zhou; H. Chen","UBRI, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China; UBRI, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China; StarGIS Technology Development Co., Ltd., Tianjin, China; Iflytek Co., Ltd., Hefei, China; UBRI, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2021","2021","59","12","10047","10061","Detecting changed regions in paired satellite images plays a key role in many remote sensing applications. The evolution of recent techniques could provide satellite images with a very high spatial resolution (VHR) but made it challenging to apply image coregistration, and many change detection methods are dependent on its accuracy. Two images of the same scene taken at different times or from different angles would introduce unregistered objects and the existence of both unregistered areas and actual changed areas would lower the performance of many change detection algorithms in unsupervised conditions. To alleviate the effect of unregistered objects in the paired images, we propose a novel change detection framework utilizing a special neural network architecture—Generative Adversarial Network (GAN) to generate many better coregistered images. In this article, we show that the GAN model can be trained upon a pair of images by using the proposed expanding strategy to create a training set and optimizing designed objective functions. The optimized GAN model would produce better coregistered images where changes can be easily spotted and then the change map can be presented through a comparison strategy using these generated images explicitly. Compared to other deep learning-based methods, our method is less sensitive to the problem of unregistered images and makes most of the deep learning structure. Experimental results on synthetic images and real data with many different scenes could demonstrate the effectiveness of the proposed approach.","1558-0644","","10.1109/TGRS.2020.3043766","National Key Research and Development Program of China(grant numbers:2016YFB1000905); National Natural Science Foundation of China(grant numbers:91746209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311699","Change detection;deep learning;generative adversarial networks (GANs);satellite images;unsupervised","Feature extraction;Generative adversarial networks;Deep learning;Satellites;Task analysis;Gallium nitride;Generators","geophysical image processing;image registration;image resolution;image segmentation;learning (artificial intelligence);neural nets;object detection;remote sensing;terrain mapping","special neural network architecture-Generative Adversarial Network;coregistered images;optimizing designed objective functions;optimized GAN model;change map;unregistered images;synthetic images;unsupervised change detection;changed regions;paired satellite images;remote sensing applications;high spatial resolution;image coregistration;change detection methods;unregistered objects;unregistered areas;actual changed areas;change detection algorithms;unsupervised conditions;paired images;novel change detection framework","","13","","54","IEEE","31 Dec 2020","","","IEEE","IEEE Journals"
"A Conditional Generative Adversarial Network to Fuse Sar And Multispectral Optical Data For Cloud Removal From Sentinel-2 Images","C. Grohnfeldt; M. Schmitt; X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1726","1729","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and haze-free MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-l SAR data confirm that our extended SAR-Opt-cGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519215","SAR;optical remote sensing;data fusion;deep learning;generative adversarial network (GAN);cloud-removal","Synthetic aperture radar;Optical sensors;Optical imaging;Clouds;Remote sensing;Adaptive optics;Generative adversarial networks","geophysical image processing;image fusion;radar imaging;synthetic aperture radar","cloud-free MS optical data;Sentinel-l SAR data;single-sensor MS data;reconstruct MS images;auxiliary SAR information;extended SAR-Opt-cGAN model;auxiliary SAR image;cloud-corrupted MS input;haze-free MS optical data;multispectral image data;synthetic aperture radar;conditional generative adversarial network architecture;Sentinel-2;cloud removal;multispectral optical data","","51","3","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Road Segmentation for Remote Sensing Images Using Adversarial Spatial Pyramid Networks","P. Shamsolmoali; M. Zareapoor; H. Zhou; R. Wang; J. Yang","Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; School of Informatics, University of Leicester, Leicester, U.K; School of Natural and Computational Sciences, Massey University, Auckland, New Zealand; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","4673","4688","Road extraction in remote sensing images is of great importance for a wide range of applications. Because of the complex background, and high density, most of the existing methods fail to accurately extract a road network that appears correct and complete. Moreover, they suffer from either insufficient training data or high costs of manual annotation. To address these problems, we introduce a new model to apply structured domain adaption for synthetic image generation and road segmentation. We incorporate a feature pyramid (FP) network into generative adversarial networks to minimize the difference between the source and target domains. A generator is learned to produce quality synthetic images, and the discriminator attempts to distinguish them. We also propose a FP network that improves the performance of the proposed model by extracting effective features from all the layers of the network for describing different scales' objects. Indeed, a novel scale-wise architecture is introduced to learn from the multilevel feature maps and improve the semantics of the features. For optimization, the model is trained by a joint reconstruction loss function, which minimizes the difference between the fake images and the real ones. A wide range of experiments on three data sets prove the superior performance of the proposed approach in terms of accuracy and efficiency. In particular, our model achieves state-of-the-art 78.86 IOU on the Massachusetts data set with 14.89M parameters and 86.78B FLOPs, with 4× fewer FLOPs but higher accuracy (+3.47% IOU) than the top performer among state-of-the-art approaches used in the evaluation.","1558-0644","","10.1109/TGRS.2020.3016086","NSFC(grant numbers:61806125,61977046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173823","Adversarial network;domain adaptation;feature pyramid (FP);remote sensing (RS) images;road segmentation","Roads;Image segmentation;Feature extraction;Adaptation models;Task analysis;Training;Generators","computer vision;feature extraction;geophysical image processing;image segmentation;neural nets;remote sensing","multilevel feature maps;fake images;road segmentation;remote sensing images;adversarial spatial pyramid networks;road extraction;road network;manual annotation;structured domain adaption;synthetic image generation;feature pyramid network;generative adversarial networks;target domains;quality synthetic images;FP network;scale-wise architecture;joint reconstruction loss function;feature extraction","","50","","48","IEEE","21 Aug 2020","","","IEEE","IEEE Journals"
"Stagewise Unsupervised Domain Adaptation With Adversarial Self-Training for Road Segmentation of Remote-Sensing Images","L. Zhang; M. Lan; J. Zhang; D. Tao","School of Computer Science, Institute of Artificial Intelligence, Wuhan University, Wuhan, China; School of Computer Science, Institute of Artificial Intelligence, Wuhan University, Wuhan, China; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; JD Explore Academy, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","13","Road segmentation from remote-sensing images is a challenging task with wide ranges of application potentials. Deep neural networks have advanced this field by leveraging the power of large-scale labeled data, which, however, are extremely expensive and time-consuming to acquire. One solution is to use cheap available data to train a model and deploy it to directly process the data from a specific application domain. Nevertheless, the well-known domain shift (DS) issue prevents the trained model from generalizing well on the target domain. In this article, we propose a novel stagewise domain adaptation model called RoadDA to address the DS issue in this field. In the first stage, RoadDA adapts the target domain features to align with the source ones via generative adversarial networks (GANs)-based interdomain adaptation. Specifically, a feature pyramid fusion module is devised to avoid information loss of long and thin roads and learn discriminative and robust features. Besides, to address the intradomain discrepancy in the target domain, in the second stage, we propose an adversarial self-training method. We generate the pseudo labels of the target domain using the trained generator and divide it to labeled easy split and unlabeled hard split based on the road confidence scores. The features of hard split are adapted to align with the easy ones using adversarial learning and the intradomain adaptation process is repeated to progressively improve the segmentation performance. Experiment results on two benchmarks demonstrate that RoadDA can efficiently reduce the domain gap and outperforms state-of-the-art methods. The code is available at https://github.com/LANMNG/RoadDA.","1558-0644","","10.1109/TGRS.2021.3104032","National Natural Science Foundation of China(grant numbers:62076188); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)(grant numbers:2019AEA170); Fundamental Research Funds for the Central Universities(grant numbers:2042021kf0196); supercomputing system in the Supercomputing Center of Wuhan University; Australian Research Council (ARC)(grant numbers:FL-170100117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516689","Remote sensing (RS);road segmentation;self-training;unsupervised domain adaptation (UDA)","Roads;Image segmentation;Adaptation models;Task analysis;Data models;Feature extraction;Predictive models","feature extraction;image fusion;image segmentation;learning (artificial intelligence);neural nets;remote sensing;road traffic;traffic engineering computing","deep neural networks;RoadDA;DS;generative adversarial networks-based interdomain adaptation;feature pyramid fusion module;road confidence scores;adversarial learning;intradomain adaptation process;road segmentation;remote-sensing images;stagewise unsupervised domain adaptation;adversarial self-training","","32","","49","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Unsupervised Cycle-Consistent Generative Adversarial Networks for Pan Sharpening","H. Zhou; Q. Liu; D. Weng; Y. Wang","State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China; School of Biomedical Engineering, Capital Medical University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","26 Apr 2022","2022","60","","1","14","Deep learning-based pan sharpening has received significant research interest in recent years. Most of the existing methods fall into the supervised learning framework in which they downsample the multispectral (MS) and panchromatic (PAN) images and regard the original MS images as ground truths to form training samples based on Wald’s protocol. Although impressive performance could be achieved, they have difficulties when generalizing to the original full-scale images due to the scale gap, which makes them lack of practicability. In this article, we propose an unsupervised generative adversarial framework that learns from the full-scale images without the ground truths to alleviate this problem. We first extract the modality-specific features from the PAN and MS images with a two-stream generator, perform fusion in the feature domain, and then reconstruct the pan-sharpened images. Furthermore, we introduce a novel hybrid loss based on the cycle-consistency and adversarial scheme to improve the performance. Comparison experiments with the state-of-the-art methods are conducted on GaoFen-2 (GF-2) and WorldView-3 satellites. Results demonstrate that the proposed method can greatly improve the pan-sharpening performance on the full-scale images, which clearly shows its practical value. Codes are available at https://github.com/zhysora/UCGAN.","1558-0644","","10.1109/TGRS.2022.3166528","NSFC(grant numbers:62176017,41871283,U1804157); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755137","Cycle consistency;generative adversarial network (GAN);image fusion;pan sharpening;unsupervised learning","Feature extraction;Deep learning;Training;Task analysis;Generators;Generative adversarial networks;Spatial resolution","","","","3","","53","IEEE","11 Apr 2022","","","IEEE","IEEE Journals"
"SAR Image Urban Scene Classification based on an Optimized Conditional Generative Adversarial Network","L. Li; C. Wang; H. Zhang; K. Zhang","Key Laboratory of Digital Earth Science, of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China; Key Laboratory of Digital Earth Science, of Institute of Remote Sensing and Digital Earth, University of Chinese Academy of Science, Beijing, China; Key Laboratory of Digital Earth Science, of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China; Information science academy, CETC, Beijing, China","2019 SAR in Big Data Era (BIGSARDATA)","7 Oct 2019","2019","","","1","4","Classification of urban scenes in SAR images has been challenging in the complex and various behaviors of urban areas in SAR images. Generative Adversarial networks (GANs), which can learn distribution characteristics of image scenes in an alternative training style, have attracted attention. However, these networks all suffer from difficulties in network training and stability. To overcome this issue and extract proper features, this paper introduces two effective solutions. Firstly, employing the residual structure and an extra classifier into the traditional conditional adversarial network to achieve scenes classification. Then gradient penalty is used to optimize the loss convergence during training stage. Last, we select GF-3 and Sentinel-1 SAR images to test the network. The experiment results show the usefulness of our proposed optimization.","","978-1-7281-2653-1","10.1109/BIGSARDATA.2019.8858163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858163","scene classification;urban areas;GANs;SAR images","Radar polarimetry;Training;Buildings;Rivers;Remote sensing;Urban areas;Generative adversarial networks","feature extraction;image classification;neural nets;optimisation;radar imaging;synthetic aperture radar","SAR image urban scene classification;optimized conditional generative adversarial network;distribution characteristics;Sentinel-1 SAR images;loss convergence;GF-3 images","","2","","16","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"Multitask Learning Mechanism for Remote Sensing Image Motion Deblurring","J. Fang; X. Cao; D. Wang; S. Xu","Center for Image and Information Processing, School of Communications and Information Engineering & School of Artificial Intelligence, Xi’an University of Posts & Telecommunications, Xi’an, P. R. China; School of Electric and Control Engineering, Shaanxi University of Science and Technology, Xi’an, P. R. China; Center for Image and Information Processing, School of Communications and Information Engineering & School of Artificial Intelligence, Xi’an University of Posts & Telecommunications, Xi’an, P. R. China; AI & DE, Guangzhou, P. R. China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","8 Feb 2021","2021","14","","2184","2193","As a fundamental preprocessing technique, remote sensing image motion deblurring is important for visual understanding tasks. Most conventional approaches formulate the image motion deblurring task as a kernel estimation. Because the kernel estimation is a highly ill-posed problem, many priors have been applied to model the images and kernels. Even though these methods have obtained relatively better performances, they are usually time-consuming and not robust for different conditions. To address this problem, we propose a multitask learning mechanism for remote sensing image motion deblurring in this article, which contains an image restoration subtask and an image texture complexity recognition one. First, we consider the image motion deblurring problem as a domain transformation problem, from the blurred domain to a clear one. Specifically, the blurred domain represents the data space consisted of blurring images, and the definition of clear domain is similar. Second, we design a novel weighted attention map loss to enhance the reconstruction capability of the restoration subbranch for difficult local regions. Third, based on the restoration subbranch, a recognition subbranch is incorporated into the framework to guide the deblurring process, which provides the auxiliary texture complexity information to help the optimization of restoration subbranch. Additionally, in order to optimize the proposed network, we construct three large-scale datasets, and each sample in the dataset contains a clear image, a blurred image, and its texture label obtained by corresponding texture complexity. Finally, the experimental results on three constructed datasets demonstrate the robustness and the effectiveness of the proposed method.","2151-1535","","10.1109/JSTARS.2020.3047636","Yanan University(grant numbers:IPBED6); Science and Technology of Ministry of Public Security(grant numbers:2019GABJC42); Education Department of Shaanxi Province(grant numbers:19JK0140); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2020JM472,2020JM473,2019JQ760); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309366","Domain transformation;image deblurring;multitask learning mechanism","Complexity theory;Image restoration;Task analysis;Kernel;Remote sensing;Mathematical model;Generative adversarial networks","image motion analysis;image restoration;image texture;learning (artificial intelligence);remote sensing","remote sensing image motion deblurring;kernel estimation;multitask learning mechanism;image restoration subtask;image texture complexity recognition","","3","","60","CCBY","28 Dec 2020","","","IEEE","IEEE Journals"
"Single Satellite Optical Imagery Dehazing using SAR Image Prior Based on conditional Generative Adversarial Networks","B. Huang; Z. Li; C. Yang; F. Sun; Y. Song","Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Tsinghua University; Shanghai University Of Engineering Science; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Tsinghua University; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Tsinghua University; Beijing National Research Center for Information Science and Technology (BNRist), State Key Lab on Intelligent Technology and Systems, Tsinghua University","2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","1795","1802","Satellite image dehazing aims at precisely retrieving the real situations of the obscured parts from the hazy remote sensing (RS) images, which is a challenging task since the hazy regions contain both ground features and haze components. Many approaches of removing haze focus on processing multi-spectral or RGB images, whereas few of them utilize multi-sensor data. The multi-sensor data fusion is significant to provide auxiliary information since RGB images are sensitive to atmospheric conditions. In this paper, a dataset called SateHaze1k is established and composed of 1200 pairs clear Synthetic Aperture Radar (SAR), hazy RGB, and corresponding ground truth images, which are divided into three degrees of the haze, i.e. thin, moderate, and thick fog. Moreover, we propose a novel fusion dehazing method to directly restore the haze-free RS images by using an end-to-end conditional generative adversarial network(cGAN). The proposed network combines the information of both RGB and SAR images to eliminate the image blurring. Besides, the dilated residual blocks of the generator can also sufficiently improve the dehazing effects. Our experiments demonstrate that the proposed method, which fuses the information of different sensors applied to the cloudy conditions, can achieve more precise results than other baseline models.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093471","","Synthetic aperture radar;Satellites;Remote sensing;Gallium nitride;Task analysis;Optical imaging;Optical sensors","image colour analysis;image enhancement;image restoration;neural nets;radar imaging;remote sensing by radar;sensor fusion;spaceborne radar;synthetic aperture radar","synthetic aperture radar;hazy RGB;fusion dehazing method;haze-free RS images;image blurring;dehazing effects;cloudy conditions;single satellite optical imagery dehazing;SAR image;conditional generative adversarial networks;hazy remote sensing images;ground features;RGB images;multisensor data fusion;ground truth images;cGAN;SateHaze1k;multispectral images","","14","","39","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Unsupervised Remote Sensing Image Super-Resolution Method Based on Adaptive Domain Distance Measurement Network","Y. Hou; J. Zhang","Aerospace Information Research Institute, University of Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, University of Chinese Academy of Sciences, Beijing, China","2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","2 Jul 2020","2020","","","256","259","Compared with supervised learning, unsupervised learning is more practical; however, the associated training process is more difficult and complex. To solve the problems of unstable training and insufficient diversity of generative adversarial networks (GAN), which are widely used to realize unsupervised learning, we propose a novel unsupervised remote sensing image super-resolution method based on a reverse generating network module and the adaptive domain distance measurement network. The discriminant network of GAN is considered as a tool to measure a certain image attribute instead of the original GAN binary classification network. Furthermore, the adaptive domain distance measurement network is used to back feed the information of a high-resolution image to guide the optimization of the generating network. The results of experiments performed on various datasets demonstrate the effectiveness of the proposed method.","","978-1-7281-8143-1","10.1109/AEMCSE50948.2020.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9131243","GAN;remote senseng;super resolution;domain","","geophysical image processing;image resolution;pattern classification;remote sensing;unsupervised learning","unsupervised remote sensing image super-resolution method;adaptive domain distance measurement network;supervised learning;unsupervised learning;generative adversarial networks;reverse generating network module;discriminant network;original GAN binary classification network;high-resolution image","","3","","15","IEEE","2 Jul 2020","","","IEEE","IEEE Conferences"
"Relative Radiation Correction Based on CycleGAN for Visual Perception Improvement in High-Resolution Remote Sensing Images","X. Yu; J. Fan; M. Zhang; Q. Liu; Y. Li; D. Zhang; Y. Zhou","XAG Company Ltd., Guangzhou, China; School of Civil and Architectural Engineering, Shandong University of Technology, Zibo, China; School of Civil and Architectural Engineering, Shandong University of Technology, Zibo, China; School of Civil and Architectural Engineering, Shandong University of Technology, Zibo, China; School of Civil and Architectural Engineering, Shandong University of Technology, Zibo, China; School of Civil and Architectural Engineering, Shandong University of Technology, Zibo, China; Ecology Observing Network and Modeling Laboratory, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China","IEEE Access","4 Aug 2021","2021","9","","106627","106640","The differences between the imaging environments of sensors lead to great differences in remote sensing images of the same area in different seasons. Relative radiation correction has high practical value as the main method to reduce such differences. However, the differences in vegetation radiation caused by seasonal changes are difficult to correct by traditional radiation correction methods. The corrected results also have difficulty achieving better results at the level of human eye visual perception. Moreover, the traditional measurement of the relative radiation correction result image quality index is not consistent with the human eye visual perception effect. To address the above two problems, this paper performs seasonal relative radiation correction on high-resolution remote sensing images by CycleGAN based on a convolutional neural network, including two transformations: 1) the transformation of remote sensing images from autumn-winter to spring-summer and 2) the transformation of remote sensing images from spring-summer to autumn-winter. The similarity between the relative radiation-corrected image and the reference image is measured by the convolutional neural network model with the ability to discriminate distances. The results show that the visual effect of this method is significantly better than that of other relative radiation correction methods, and the visual perception distance is consistent with the human eye visual perception judgment. The changed area still retains its original feature characteristics. The visual perception distance of the conversion from autumn-winter to spring-summer images is improved by 9% compared with other state-of-the-art methods. The visual perception distance of spring-summer images to autumn-winter images is improved by 3%. We expect that the method in this paper can be used for preprocessing to improve the accuracy of algorithms for remote sensing image classification, image change detection, etc.","2169-3536","","10.1109/ACCESS.2021.3101110","National Key Research and Development Program of China(grant numbers:2017YFB0503500,2018YFB0505301); Shandong Provincial Natural Science Foundation(grant numbers:ZR2020MD015,ZR2020MD018); Major Science and Technology Innovation Project of Shandong Province(grant numbers:2019JZZY020103); Young Teacher Development Support Program of Shandong University of Technology(grant numbers:4072-115016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9500223","GAN;image similarity;seasonal transform;visual perception distance","Remote sensing;Radiometry;Visual perception;Generative adversarial networks;Visualization;Indexes;Image color analysis","geophysical image processing;image classification;neural nets;remote sensing;visual perception","visual perception improvement;high-resolution remote sensing images;great differences;high practical value;vegetation radiation;traditional radiation correction methods;corrected results;relative radiation correction result image quality index;human eye visual perception effect;paper performs seasonal relative radiation correction;relative radiation-corrected image;reference image;visual effect;relative radiation correction methods;visual perception distance;human eye visual perception judgment;spring-summer images;autumn-winter images;remote sensing image classification;image change detection","","1","","50","CCBY","28 Jul 2021","","","IEEE","IEEE Journals"
"K-Means Clustering Guided Generative Adversarial Networks for SAR-Optical Image Matching","W. -L. Du; Y. Zhou; J. Zhao; X. Tian","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the People’s Republic of China, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau","IEEE Access","11 Dec 2020","2020","8","","217554","217572","Synthetic Aperture Radar and optical (SAR-optical) image matching is a technique of finding correspondences between SAR and optical images. SAR-optical image matching can be simplified to single-mode image matching through image synthesis. However, the existing SAR-optical image synthesis methods are unable to provide qualified images for SAR-optical image matching. In this work, we present a K-means Clustering Guide Generative Adversarial Networks (KCG-GAN) to improve the image quality of synthesizing by constraining spatial information synthesis. KCG-GAN uses k-means segmentations as one of the image generator's inputs and introduces feature matching loss, segmentation loss, and L1 loss to the objective function. Meanwhile, to provide repeatable k-means segmentations, we develop a straightforward 1D k-means algorithm. We compare KCG-GAN with a leading image synthesis method-pix2pixHD. Qualitative results illustrate that KCG-GAN preserves more spatial structures than pix2pixHD. Quantitative results show that, compared with pix2pixHD, images synthesized by KCG-GAN are more similar to original optical images, and SAR-optical image matching based on KCG-GAN obtains at most 3.15 times more qualified matchings. Robustness tests demonstrate that SAR-optical image matching based on KCG-GAN is robust to rotation and scale changing. We also test three SIFT-like algorithms on matching original SAR-optical image pairs and matching KCG-GAN synthesized optical-optical image pairs. Experimental results show that our KCG-GAN significantly improves the performances of the three algorithms on SAR-optical image matching.","2169-3536","","10.1109/ACCESS.2020.3042213","National Natural Science Foundation of China(grant numbers:61572505,62002360,61806206,61772530,U1610124); Six Talent Peaks Project in Jiangsu Province(grant numbers:2015-DZXX-010); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180639,BK20171192); China Postdoctoral Science Foundation(grant numbers:2018M642359); Science and Technology Development Fund of Macau(grant numbers:0038/2020/A1); Science and Technology Development Fund, Macu SAR under the open project of State Key Laboratory of Lunar and Planetary Science; Fundamental Research Funds for the Central Universities(grant numbers:2020ZDPY0305); Xuzhou Science and Technology Program(grant numbers:KC18061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9279214","Image matching;image synthesis;synthetic aperture radar (SAR);generative adversarial networks (GANs)","Optical imaging;Optical sensors;Nonlinear optics;Image segmentation;Image matching;Optical distortion;Adaptive optics","image matching;image segmentation;neural nets;optical information processing;pattern clustering;radar computing;radar imaging;synthetic aperture radar;transforms","KCG-GAN;image synthesis method;SAR-optical image matching;optical-optical image pairs;SAR-optical image synthesis methods;k-means clustering guide generative adversarial networks;synthetic aperture radar;constraining spatial information synthesis;k-means segmentations;1D k-means algorithm;pix2pixHD;SIFT-like algorithms","","7","","69","CCBY","3 Dec 2020","","","IEEE","IEEE Journals"
"Synthesis of Micro-Doppler Signatures of Human Activities From Different Aspect Angles Using Generative Adversarial Networks","I. Alnujaim; S. S. Ram; D. Oh; Y. Kim","Department of Electrical and Computer Engineering, California State University, Fresno, CA, USA; Department of Electrical and Computer Engineering, Indraprastha Institute of Information Technology Delhi, New Delhi, India; Advanced Radar Research Division, Daegu Gyeongbuk Institute of Science and Technology, Daegu, South Korea; Department of Electrical and Computer Engineering, California State University, Fresno, CA, USA","IEEE Access","29 Mar 2021","2021","9","","46422","46429","In this paper, we propose to produce synthesized micro-Doppler signatures from different aspect angles through conditional generative adversarial networks (cGANs). Micro-Doppler signatures of non-rigid human body motions vary considerably as a function of the radar's aspect angle. Because the direction of the human motion can be arbitrary, a large volume of training data across diverse aspects is needed for practical target activity classification through machine learning. As measurements can require significant monetary and labor costs, the synthesis of micro-Doppler signatures can be an alternate solution. Therefore, we investigate the feasibility of data augmentation through synthesizing micro-Doppler signatures of human activities from diverse radar aspect angles with input data from a single aspect angle. For the training data, the micro-Doppler radar signatures of 12 human activities are generated from different angles ranging from 0 to 315 degrees, at 45-degree increments, through simulations. For each angle, cGANs are trained to synthesize the micro-Doppler signatures for that specific angle given micro-Doppler signatures from another angle. The output of each model is evaluated by calculating mean-square errors and structural similarity indexes between the synthesized micro-Doppler signatures and the ground-truth ones obtained from simulations. We test three different scenarios, and report the respective results.","2169-3536","","10.1109/ACCESS.2021.3068075","Daegu Gyeongbuk Institute of Science and Technology DGIST Research and Development Program of the Ministry of Science and Information and Communications Technology(grant numbers:20-ST-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383276","Generative adversarial networks (GAN);deep learning;micro-Doppler signatures;monostatic radar;multistatic radar;human activity","Radar;Radar imaging;Radar cross-sections;Radar antennas;Doppler radar;Spectrogram;Generators","Doppler radar;learning (artificial intelligence);mean square error methods;neural nets;radar computing","structural similarity indexes;mean-square error calculation;data augmentation;machine learning;nonrigid human body motions;conditional generative adversarial networks;cGAN;microDoppler radar signature synthesis;diverse radar aspect angles","","7","","20","CCBY","23 Mar 2021","","","IEEE","IEEE Journals"
"Learning Spectral and Spatial Features Based on Generative Adversarial Network for Hyperspectral Image Super-Resolution","R. Jiang; X. Li; A. Gao; L. Li; H. Meng; S. Yue; L. Zhang","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; Department of Electronic and Computer Engineering, Brunel University London, UK; School of Computer Science, University of Lincoln, Lincoln, UK; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3161","3164","Super-resolution (SR) of hyperspectral images (HSIs) aims to enhance the spatial/spectral resolution of hyperspectral imagery and the super-resolved results will benefit many remote sensing applications. A generative adversarial network for HSIs super-resolution (HSRGAN) is proposed in this paper. Specifically, HSRGAN constructs spectral and spatial blocks with residual network in generator to effectively learn spectral and spatial features from HSIs. Furthermore, a new loss function which combines the pixel-wise loss and adversarial loss together is designed to guide the generator to recover images approximating the original HSIs and with finer texture details. Quantitative and qualitative results demonstrate that the proposed HSRGAN is superior to the state of the art methods like SRCNN and SRGAN for HSIs spatial SR.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900228","Hyperspectral images;super-resolution;generative adversarial network;residual network","Spatial resolution;Generative adversarial networks;Generators;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image enhancement;image resolution;image texture;learning (artificial intelligence);remote sensing","spectral features;spatial features;generative adversarial network;hyperspectral image super-resolution;hyperspectral imagery;super-resolved results;remote sensing applications;HSIs super-resolution;HSRGAN;spatial blocks;residual network;loss function;pixel-wise loss;adversarial loss;original HSIs;HSIs spatial SR","","7","","7","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"A Cycle Gan Approach for Heterogeneous Domain Adaptation in Land Use Classification","C. Voreiter; J. -C. Burnel; P. Lassalle; M. Spigai; R. Hugues; N. Courty","Université de Bretagne Sud - IRISA, Vannes, France; Université de Bretagne Sud - IRISA, Vannes, France; CNES, Toulouse Cedex 9, France; Thales Alenia Space, Toulouse, France; Thales Alenia Space, Toulouse, France; Université de Bretagne Sud - IRISA, Vannes, France","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1961","1964","In the field of remote sensing and more specifically in Earth Observation, new data are available every day, coming from different sensors. Leveraging on those data in classification tasks comes at the price of intense labelling tasks that are not realistic in operational settings. While domain adaptation could be useful to counterbalance this problem, most of the usual methods assume that the data to adapt are comparable (they belong to the same metric space), which is not the case when multiple sensors are at stake. Heterogeneous domain adaptation methods are a particular solution to this problem. We present a novel method to deal with such cases, based on a modified cycleGAN version that incorporates classification losses and a metric space alignment term. We demonstrate its power on a land use classification tasks, with images from both Google Earth and Sentinel-2.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324264","Centre National d’Études Spatiales (CNES); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324264","Heterogeneous Domain Adaptation;Cycle GAN;Land Use Classification","Gallium nitride;Measurement;Generators;Task analysis;Generative adversarial networks;Agriculture;Earth","geophysical image processing;image classification;learning (artificial intelligence);pattern classification;remote sensing","cycle gan approach;land use classification;remote sensing;Earth Observation;classification tasks;intense labelling tasks;operational settings;usual methods;multiple sensors;heterogeneous domain adaptation methods;classification losses;metric space alignment term;Google Earth","","3","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Domain Adaptation for Semantic Segmentation of Aerial Imagery Using Cycle-Consistent Adversarial Networks","F. Schenkel; W. Middelmann",Fraunhofer IOSB; Fraunhofer IOSB,"IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1448","1451","Semantic segmentation is an important computer vision task for the analysis of aerial imagery in many remote sensing applications. Due to the large availability of data it is possible to design efficient convolutional neural network based deep learning models for this purpose. But these methods usually show a weak performance when they are applied without any modifications to data from another domain with different characteristics relating to aspects concerning the sensor or environmental influences. To improve the performance of these methods domain adaptation approaches can be employed. In the following work, we want to present a method for unsupervised domain adaptation for semantic segmentation. We trained an encoder-decoder model on the source domain dataset as task application and adjusted the network to the target domain. The adaptation process is based on a style transfer component, which is realized using a cycle-consistent adversarial network. Through a continuous adaptation of the task model we achieved a higher generalization of the network and increased the task method performance on the target domain.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323650","Semantic Segmentation;Domain Adaptation;Unsupervised Learning;Cycle-Consistent Adversarial Networks","Task analysis;Adaptation models;Training;Semantics;Gallium nitride;Image segmentation;Generative adversarial networks","computer vision;convolutional neural nets;geophysical image processing;remote sensing;unsupervised learning","semantic segmentation;aerial imagery;cycle-consistent adversarial network;computer vision task;remote sensing applications;convolutional neural network;deep learning models;unsupervised domain adaptation;domain adaptation approach;encoder-decoder model;source domain dataset;task application;target domain;task model;task method performance","","2","","17","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Dual-Fusion Semantic Segmentation Framework with Gan for SAR Images","D. Li; J. Liu; F. Liu; W. Zhang; A. Zhang; W. Gao; J. Shi","Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Northwestern Poly technical University, Xian, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","991","994","Deep learning based semantic segmentation is one of the popular methods in remote sensing image segmentation. In this paper, a network based on the widely used encoder-decoder architecture is proposed to accomplish the synthetic aperture radar (SAR) images segmentation. With the better representation capability of optical images, we propose to enrich SAR images with generated optical images via the generative adversative network (GAN) trained by numerous SAR and optical images. These optical images can be used as expansions of original SAR images, thus ensuring robust result of segmentation. Then the optical images generated by the GAN are stitched together with the corresponding real images. An attention module following the stitched data is used to strengthen the representation of the objects. Experiments indicate that our method is efficient compared to other commonly used methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884931","Open Research Fund in 2021 of Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense(grant numbers:JSGP202101); Fundamental Research Funds for the Central Universities(grant numbers:JSGP202204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884931","Semantic segmentation;SAR images;encoder-decoder framework","Semantics;Transforms;Optical fiber networks;Optical imaging;Generative adversarial networks;Adaptive optics;Radar polarimetry","feature extraction;image processing;image segmentation;learning (artificial intelligence);optical images;radar imaging;remote sensing;synthetic aperture radar","generated optical images;GAN;numerous SAR;original SAR images;corresponding real images;dual-fusion semantic segmentation framework;gan;remote sensing image segmentation;synthetic aperture radar images segmentation","","1","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Toward Faster and Accurate Post-Disaster Damage Assessment: Development of End-to-End Building Damage Detection Framework with Super-Resolution Architecture","X. Fu; T. Kouyama; H. Yang; R. Nakamura; I. Yoshikawa","National Institute of Advance Industrial Science and Technology, Tokyo, Japan; National Institute of Advance Industrial Science and Technology, Tokyo, Japan; The University of Tokyo, Tokyo, Japan; National Institute of Advance Industrial Science and Technology, Tokyo, Japan; The University of Tokyo, Tokyo, Japan","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1588","1591","Building damage detection (BDD) with satellite images has been frequently adopted as an essential reference for post-disaster rescue, whereas its timeliness is significantly impacted by the long revisit time of high-resolution remote sensing satellites. Therefore, a reliable super-resolution method which is optimized for accurate and detail BDD is fundamental one for advancing the BDD analysis even when we can use only low-resolution images after a disaster. Based on Super-Resolution Generative Adversarial Network (SRGAN) and U-Net convolutional network, an efficient and novel BDD framework is proposed in this paper for obtaining upsampled BDD results from low-resolution post-disaster images. We trained the framework using two disasters from the xBD dataset and tested three different structures. The results show that our training structure based on an end-to-end framework successfully generated super-resolution BDD maps from low-resolution images, which performed significantly better than those from a two-stage training structure.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883317","Super-resolution;building damage;xBD dataset;end-to-end network;deep learning","Training;Satellites;Architecture;Superresolution;Buildings;Generative adversarial networks;Reliability","buildings (structures);convolutional neural nets;disasters;geophysical image processing;image resolution;remote sensing","Super-Resolution Generative Adversarial Network;U-Net convolutional network;low-resolution post-disaster images;super-resolution BDD maps;post-disaster damage assessment;satellite images;post-disaster rescue;high-resolution remote sensing satellites;end-end building damage detection framework;SRGAN","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Style Transformation-Based Change Detection Using Adversarial Learning with Object Boundary Constraints","X. Zhang; W. Yu; M. -O. Pun; M. Liu","School of Mathematical Sciences, University of Science and Technology of China, Hefei, P.R.China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Shenzhen, P.R. China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, Shenzhen, P.R. China; Shanghai CAS-NOVA Satellite Technology Company Limited, Shanghai, P.R.China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3117","3120","Deep learning has shown promising results on change detection (CD) from bi-temporal remote sensing imagery in recent years. However, it still remains challenging to cope with the pseudo-changes caused by seasonal differences and style variations of bi-temporal images. In this paper, an object-level boundary-preserving generative adversarial network (BPGAN) is developed for style transformation-based CD of bi-temporal images. To achieve this purpose, image objects derived in the spectral domain are incorporated into the image translation to generate object-level target-style-like images. In particular, constraints on object boundary consistency and object homogeneity are established in the adversarial learning to maintain the style and content consistency while regularizing the network training. Furthermore, the Superpixel-Based Fast Fuzzy c-Means (SF-FCM) algorithm is utilized for efficient CD from the object-level style-transformed images. Extensive experiments on SPOT5 and GF1 data confirm the effectiveness of the proposed approach.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554645","National Natural Science Foundation of China(grant numbers:41801323); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554645","Change detection;style transformation;cGAN;object-level","Training;Deep learning;Image analysis;Change detection algorithms;Generative adversarial networks;Adversarial machine learning;Spectral analysis","deep learning (artificial intelligence);geophysical image processing;geophysical techniques;object detection;remote sensing","style transformation-based change detection;adversarial learning;object boundary constraints;deep learning;bi-temporal remote sensing imagery;style variations;bi-temporal images;object-level boundary-preserving generative adversarial network;style transformation-based CD;image objects;image translation;object-level target-style-like images;object boundary consistency;object homogeneity;content consistency;object-level style-transformed images;superpixel-based fast fuzzy c-means","","","","3","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Interpolating Seismic Data With Conditional Generative Adversarial Networks","D. A. B. Oliveira; R. S. Ferreira; R. Silva; E. Vital Brazil","IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","10 Dec 2018","2018","15","12","1952","1956","Having dense and regularly sampled data is becoming increasingly important in seismic processing. However, due to physical or financial constraints, seismic data sets can be often undersampled. Occasionally, these data sets may also present bad or dead traces the geoscientist must deal with. Many works have tackled this problem using prestack data and can be classified in three main categories: wave-equation, domain transform, and prediction-error-filter methods. In this letter, we assess the performance of a conditional generative adversarial network for the interpolation problem in poststack seismic data sets. To the best of our knowledge, this is the first work to evaluate a deep learning approach in this context. Quantitative and qualitative evaluations of our experiments indicate that deep networks may present an interesting alternative to classical methods.","1558-0571","","10.1109/LGRS.2018.2866199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8465958","Geoscience;geophysical image processing;multilayer neural network","Training;Generators;Interpolation;Gallium nitride;Testing;Transforms;Decoding","geophysical techniques;seismic waves;seismology","conditional generative adversarial network;dense sampled data;regularly sampled data;seismic processing;physical constraints;financial constraints;bad traces;dead traces;prestack data;prediction-error-filter methods;interpolation problem;poststack seismic data sets;deep networks;wave-equation;domain transform;deep learning approach","","64","","22","IEEE","14 Sep 2018","","","IEEE","IEEE Journals"
"Improving Satellite Image Fusion via Generative Adversarial Training","X. Luo; X. Tong; Z. Hu","College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, China; College of Surveying and Geo-informatics, Tongji University, Shanghai, China; MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area & Guangdong Key Laboratory of Urban Informatics & Shenzhen Key Laboratory of Spatial Smart Sensing and Services, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6969","6982","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications.","1558-0644","","10.1109/TGRS.2020.3025821","National Natural Science Foundation of China(grant numbers:41631178); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212572","Deep learning;generative adversarial networks (GANs);Landsat 8;remote sensing image fusion;residual dense blocks;Sentinel-2","Image fusion;Satellites;Training;Spatial resolution;Remote sensing","geophysical image processing;image fusion;image resolution;optical images;remote sensing","satellite image fusion;generative adversarial training;optical images;satellite platforms;multiresolution satellite images;higher-resolution images;image quality;generative adversarial network;fusion GAN approach;remote sensing image fusion problem;adversarial training strategy;generative network;G-Net;discriminative network;original multiresolution images;residual dense blocks;FusGAN approach;Landsat Operational Land Imager multiresolution images;common resolution disparities;intersatellite images;quality improvement","","7","","74","IEEE","5 Oct 2020","","","IEEE","IEEE Journals"
"An Improved Method for Pan-Sharpening Based on Pan-GAN","Y. Li; J. Li; X. Du; Y. Huang; J. Lei","School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China","2022 7th International Conference on Image, Vision and Computing (ICIVC)","19 Sep 2022","2022","","","282","286","Pan-sharpening refers to the fusion of pan images and low-resolution multispectral remote sensing images to obtain high-resolution multispectral images. Generative adversarial network (GAN)-based pan-sharpening methods have recently became popular due to the lack of ground-truth data during training. However, GAN-based methods suffer from training instability and convergence difficulties. To deal the issues, we propose a novel GAN-based pan-sharpening method using additional constraint. First, we adopt the geometric consistency constraint to enforce the network to preserve the spatial structure of image. Second, we introduce an attention mechanism in the generator to extract useful information through the features of pan image and multispectral images and pay more attention to meaningful regions. Experimental results show the effectiveness of our method in terms of the quantitative and visual results.","","978-1-6654-6734-6","10.1109/ICIVC55077.2022.9887169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887169","pan-sharpening;geometric consistency;attention mechanism;Generative adversarial network","Training;Visualization;Feature extraction;Generative adversarial networks;Generators;Data mining;Remote sensing","geophysical image processing;image enhancement;image fusion;image resolution;remote sensing","GAN-based methods;training instability;convergence difficulties;novel GAN-based pan-sharpening method;pan image;pan-GAN;low-resolution multispectral remote sensing images;high-resolution multispectral images;generative adversarial network-based pan-sharpening methods;ground-truth data","","","","19","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"From Artifact Removal to Super-Resolution","J. Wang; Z. Shao; X. Huang; T. Lu; R. Zhang; Y. Li","State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Department of Geosciences, University of Arkansas, Fayetteville, AR, USA; School of Computer Science and Engineering, Wuhan Institute of Technology, Wuhan, China; Institute of Photogrammetry and Remote Sensing, Chinese Academy of Surveying and Mapping, Beijing, China; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","23 Aug 2022","2022","60","","1","15","Deep-learning-based super-resolution (SR) methods have been extensively studied and have achieved significant performance with deep convolutional neural networks. However, the results still suffer from the ringing effect, especially in satellite image SR tasks, due to the loss of image details in the satellite degradation process. In this article, we build a novel satellite SR framework by decomposing a high-resolution image into three components, i.e., low-resolution (LR), artifact, and high-frequency information. Specifically, we propose an artifact removal network with a self-adaption difference convolution (SDC) to fully exploit the structure prior in the LR image and predict the artifact map. Considering that the artifact map and the high-frequency map share a similar pattern, we introduce the supervised structure correction (SSC) block that establishes a bridge between the high-frequency generation process and the artifact removal process. Experimental results on satellite images demonstrate that the proposed method owns an improved tradeoff between the performance and the computational cost compared to existing state-of-the-art satellite and natural SR methods. The source code is available at https://github.com/jiaming-wang/ARSRN.","1558-0644","","10.1109/TGRS.2022.3196709","National Natural Science Foundation of China(grant numbers:42090012); Guangxi Science and Technology Program(grant numbers:GuiKe 2021AB30019); 03 Special Research and 5G Project of Jiangxi Province in China(grant numbers:20212ABC03A09); Zhuhai Industry University Research Cooperation Project of China(grant numbers:ZH22017001210098PWC); Sichuan Science and Technology Program(grant numbers:2022YFN0031); Zhizhuo Research Fund on Spatial-Temporal Artificial Intelligence(grant numbers:ZZJJ202202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851467","Artifact removal;difference convolution;remote sensing;super-resolution (SR)","Satellites;Image edge detection;Task analysis;Convolution;Superresolution;Image reconstruction;Generative adversarial networks","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image resolution","deep convolutional neural networks;ringing effect;satellite image SR tasks;image details;satellite degradation process;high-resolution image;high-frequency information;artifact removal network;self-adaption difference convolution;LR image;artifact map;supervised structure correction block;high-frequency generation process;deep-learning-based super-resolution methods;low-resolution","","","","59","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"A Correlation Context-Driven Method for Sea Fog Detection in Meteorological Satellite Imagery","Y. Huang; M. Wu; J. Guo; C. Zhang; M. Xu","School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Geoscience and Remote Sensing Letters","15 Dec 2021","2022","19","","1","5","Sea fog detection is a challenging and essential issue in satellite remote sensing. Although conventional threshold methods and deep learning methods can achieve pixel-level classification, it is difficult to distinguish ambiguous boundaries and thin structures from the background. Considering the correlations between neighbor pixels and the affinities between superpixels, a correlation context-driven method for sea fog detection is proposed in this letter, which mainly consists of a two-stage superpixel-based fully convolutional network (SFCNet), named SFCNet. A fully connected Conditional Random Field (CRF) is utilized to model the dependencies between pixels. To alleviate the problem of high cloud occlusion, an attentive Generative Adversarial Network (GAN) is implemented for image enhancement by exploiting contextual information. Experimental results demonstrate that our proposed method achieves 91.65% mIoU and obtains more refined segmentation results, performing well in detecting fogs in small, broken bits and weak contrast thin structures, as well as detects more obscured parts.","1558-0571","","10.1109/LGRS.2021.3095731","National Key Research and Development Program of China(grant numbers:2019YFF0303300,2019YFF0303302); Ministry of Education (MoE)-China Mobile Communications Group Company Ltd., (CMCC) Artificial Intelligence Project(grant numbers:MCM20190701); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9494720","Deep learning;satellite imagery;sea fog detection;superpixel","Feature extraction;Image segmentation;Generative adversarial networks;Remote sensing;Correlation;Satellites;Semantics","","","","2","","15","CCBYNCND","26 Jul 2021","","","IEEE","IEEE Journals"
"Hybrid Gan and Spectral Angular Distance for Cloud Removal","O. Ghozatlou; M. Datcu","Research Center for Spatial Information (CEOSpaceTech), University POLITEHNICA of Bucharest (UPB), Bucharest, Romania; Earth Observation Center (EOC), German Aerospace Center (DLR), Oberpfaffenhofen, Germany","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2695","2698","This paper aims to present a new algorithm to remove thin clouds and retain information in corrupted images without the use of auxiliary data. By injecting physical properties into the cycle consistent generative adversarial network (GAN), we were able to convert a cloudy multispectral image to a cloudless image. To recover information beneath clouds and shadows we create a synthetic multispectral space to obtain illumination invariant features. Multispectral vectors were transformed from Cartesian coordinates to Polar coordinates to obtain spectral angular distance (SAD) then we employed them as input to train the deep neural network (DNN). Afterward, the outputs of DNN were transformed to Cartesian coordinates to obtain shadow and cloud-free multispectral images. The proposed method, Hybrid GAN-SAD yields trustworthy reconstructed results because of exploiting transparent information from certain multispectral bands to recover uncorrupted images.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554891","Cloud Removal;Generative Adversarial Networks (GANs);Polar Coordinates;Multispectral Satellite Images","Deep learning;Training;Satellites;Image recognition;Clouds;Lighting;Geoscience and remote sensing","clouds;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image reconstruction;remote sensing","cloud-free multispectral images;hybrid GAN-SAD;cloud removal;multispectral bands;transparent information;deep neural network;spectral angular distance;synthetic multispectral space;cloudless image;cloudy multispectral image;generative adversarial network","","2","","5","EU","12 Oct 2021","","","IEEE","IEEE Conferences"
"ESeismic-GAN: A Generative Model for Seismic Events From Cotopaxi Volcano","F. Grijalva; W. Ramos; N. Pérez; D. Benítez; R. Lara-Cueva; M. Ruiz","Faculty of Engineering and Applied Sciences (FICA), Telecommunications Engineering, Universidad de Las Américas (UDLA), Quito, Ecuador; Departamento de Electrónica, Telecomunicaciones y Redes de Información (DETRI), Escuela Politécnica Nacional, Quito, Ecuador; Colegio de Ciencias e Ingenierías “El Politécnico,”, Universidad San Francisco de Quito (USFQ), Quito, Ecuador; Colegio de Ciencias e Ingenierías “El Politécnico,”, Universidad San Francisco de Quito (USFQ), Quito, Ecuador; Centro de Investigaciones de Redes Ad-Hoc (CIRAD), Departamento de Eléctrica, Electrónica y Telecomunicaciones, Universidad de las Fuerzas Armadas ESPE, Sangolquí, Ecuador; Instituto Geofísico, Escuela Politécnica Nacional, Quito, Ecuador","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","29 Jul 2021","2021","14","","7111","7120","With the growing ability to collect large volumes of volcano seismic data, the detection and labeling process of these records is increasingly challenging. Clearly, analyzing all available data through manual inspection is no longer a viable option. Supervised machine learning models might be considered to automatize the analysis of data acquired by in situ monitoring stations. However, the direct application of such algorithms is defiant, given the high complexity of waveforms and the scarce and often imbalanced amount of labeled data. In light of this and motivated by the wide success that generative adversarial networks (GANs) have seen at generating images, we present ESeismic-GAN, a GAN model to generate the magnitude frequency response of volcanic events. Our experiments demonstrate that ESeismic-GAN learns to generate the frequency components that characterize long-period and volcano-tectonic events from Cotopaxi volcano. We evaluate the performance of ESeismic-GAN during the training stage using Fréchet distance, and, later on, we reconstruct the signals into time-domain to be finally evaluated with Frechet inception distance.","2151-1535","","10.1109/JSTARS.2021.3095270","Universidad San Francisco de Quito(grant numbers:10100,12494,16916); Universidad de las Fuerzas Armadas ESPE(grant numbers:2013-PIT-014,2016-EXT-038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477001","Adversarial learning;Cotopaxi;generative adversarial networks (GANs);generative model;seismic;volcano","Recurrent neural networks;Computer architecture;Inspection;Generative adversarial networks;Data models;Volcanoes;Labeling","geophysical image processing;neural nets;seismic waves;seismology;supervised learning;tectonics;volcanology","seismic events;Cotopaxi volcano;volcano seismic data;supervised machine learning;generative adversarial networks;volcano-tectonic events;in situ monitoring stations;Ecuador;ESeismic-GAN model;Frechet inception distance","","4","","60","CCBY","7 Jul 2021","","","IEEE","IEEE Journals"
"Single Underwater Image Restoration by Contrastive Learning","J. Han; M. Shoeiby; T. Malthus; E. Botha; J. Anstee; S. Anwar; R. Wei; L. Petersson; M. A. Armin","Australian National University (ANU), Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia; Australian National University (ANU), Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia; Commonwealth Scientific and Industrial Research Organisation (CSIRO), Australia","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2385","2388","Underwater image restoration attracts significant attention due to its importance in unveiling the underwater world. This paper elaborates on a novel method that achieves state-of-the-art results for underwater image restoration based on the unsupervised image-to-image translation framework. We design our method by leveraging from contrastive learning and generative adversarial networks to maximize mutual information between raw and restored images. Additionally, we release a large-scale real underwater image dataset to support both paired and unpaired training modules. Extensive experiments with comparisons to recent approaches further demonstrate the superiority of our proposed method.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553857","Underwater image restoration;contrastive learning;underwater image dataset;image-to-image translation","Learning systems;Training;Design methodology;Geoscience and remote sensing;Generative adversarial networks;Image restoration;Mutual information","image restoration;unsupervised learning","underwater image restoration;contrastive learning;underwater world;unsupervised image-to-image translation framework;raw restored images;underwater image dataset","","12","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Super-Resolution Reconstruction of Remote Sensing Images Using Generative Adversarial Network With Shallow Information Enhancement","Y. Fu; X. Zhang; M. Wang","College of Information and Computer Engineering, Northeast Forestry University, Harbin, China; College of Economics and Business Administration, Heilongjiang Institute of Technology, Harbin, China; College of Information and Computer Engineering, Northeast Forestry University, Harbin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Oct 2022","2022","15","","8529","8540","The super-resolution (SR) reconstruction method based on deep learning can significantly improve the spatial SR of remote sensing images. However, the current methods make insufficient use of the remote context information and channel information in shallow feature extraction, resulting in the limited effect of SR reconstruction. This article proposed a new SR reconstruction model, SIEGAN, which uses generative adversarial network with shallow information enhancement to improve the effect of SR reconstruction of remote sensing images. Similar to other generative adversarial models, SIEGAN is composed of generator and discriminator. But SIEGAN enhances the generator's ability to extract shallow information by using three different scale convolution operations. Specifically, a depthwise convolution is used to extract the local context information of each band of the image. A depthwise dilation convolution is used to capture the remote context information in the image. Finally, a 1×1 convolution is used to extract the correlation features between different channels in remote sensing images. In addition, SIEGAN uses U-Net network as its discriminator to provide detailed feedback per pixel to the generator to improve the model's ability to identify image details. And the spectral–spatial total variation loss function is introduced to ensure the spectral–spatial reliability of the reconstructed images. The experimental results on Gaofen-1 data proved that compared with the state-of-the-art models, SIEGAN has achieved better SR reconstruction performance. Furthermore, the reconstructed images by SIEGAN demonstrate better performance in land cover classification.","2151-1535","","10.1109/JSTARS.2022.3209819","National Natural Science Foundation of China(grant numbers:71473034); Natural Science Foundation of Heilongjiang Province(grant numbers:LH2019G001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903573","Generative adversarial network (GAN);multiscale shallow information;remote sensing images;super-resolution (SR) reconstruction","Feature extraction;Image reconstruction;Superresolution;Spatial resolution;Remote sensing;Generative adversarial networks","","","","","","37","CCBY","26 Sep 2022","","","IEEE","IEEE Journals"
"An Adversarial Super-Resolution Remedy for Radar Design Trade-offs","K. Armanious; S. Abdulatif; F. Aziz; U. Schneider; B. Yang","Institute of Signal Processing and System Theory, University of Stuttgart, Germany; Institute of Signal Processing and System Theory, University of Stuttgart, Germany; Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany; Fraunhofer Institute for Manufacturing Engineering and Automation IPA, Stuttgart, Germany; Institute of Signal Processing and System Theory, University of Stuttgart, Germany","2019 27th European Signal Processing Conference (EUSIPCO)","18 Nov 2019","2019","","","1","5","Radar is of vital importance in many fields, such as autonomous driving, safety and surveillance applications. However, it suffers from stringent constraints on its design parametrization leading to multiple trade-offs. For example, the bandwidth in FMCW radars is inversely proportional with both the maximum unambiguous range and range resolution. In this work, we introduce a new method for circumventing radar design trade-offs. We propose the use of recent advances in computer vision, more specifically generative adversarial networks (GANs), to enhance low-resolution radar acquisitions into higher resolution counterparts while maintaining the advantages of the low-resolution parametrization. The capability of the proposed method was evaluated on the velocity resolution and range-azimuth trade-offs in micro-Doppler signatures and FMCW uniform linear array (ULA) radars, respectively.","2076-1465","978-9-0827-9703-9","10.23919/EUSIPCO.2019.8902510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902510","Radar;Super-resolution;Micro-Doppler;MIMO;Range-azimuth;Convolutional neural network;CNN;Generative adversarial networks;GAN;Remote sensing","Radar imaging;Legged locomotion;Generators;Generative adversarial networks;Sensors","computer vision;CW radar;direction-of-arrival estimation;Doppler radar;FM radar;radar imaging;radar resolution","safety;surveillance applications;design parametrization;multiple trade-offs;FMCW radars;maximum unambiguous range;range resolution;radar design trade-offs;generative adversarial networks;low-resolution radar acquisitions;low-resolution parametrization;velocity resolution;FMCW uniform linear array radars;adversarial super-resolution remedy;range-azimuth trade-offs;computer vision","","15","","28","","18 Nov 2019","","","IEEE","IEEE Conferences"
"A Two-Layers Super-Resolution Based Generation Adversarial Spatiotemporal Fusion Model","S. Fang; Q. Guo; Y. Cao; J. Zhang","Anhui Province Key Laboratory of Industry Safety and Emergency Technology, Hefei, China; Key Laboratory of Knowledge Engineering with Big Data, Hefei University of Technology, Ministry of Education, Hefei, China; University of Science and Technology of China, Hefei, Anhui, China; Key Laboratory of Knowledge Engineering with Big Data, Hefei University of Technology, Ministry of Education, Hefei, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","891","894","Remote sensing image spatiotemporal fusion (STF) algorism plays an important role by supplementing the lack of original high-resolution remote sensing satellite images in the study scenarios of dense time-series data. In recent years, the deep-learning-based STF algorithm has become a research hotspot with comparatively higher accuracy and robustness. However, due to the lack of sufficient high-quality images for training and the huge resolution gap between low-resolution images and high-resolution images, it is difficult to recover detailed information, especially for areas of land-cover change. In this paper, we propose a two-layers super-resolution based generation adversarial spatiotemporal fusion model(TLSRSTF) using smaller inputs to reduce pressure on data requirements and a mutual affine convolution to reduce model parameters. Specifically, we only use a pair of high-resolution and low-resolution images and a high-resolution image at any time. A spatial degradation consistency is constructed to adaptively determine the ratio of two layers of the super-resolution STF model. The quantitative and qualitative experimental results on public spatiotemporal fusion datasets demonstrate our superiority over the state-of-the-art methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883547","National Natural Science Foundation of China(grant numbers:61872327,61175033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883547","Spatiotemporal Fusion;Generative Adversarial Networks(GAN);Mutual Affine Convolution","Training;Adaptation models;Satellites;Convolution;Superresolution;Data models;Robustness","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;spatiotemporal phenomena","sensing image spatiotemporal fusion algorism;high-resolution remote sensing satellite images;dense time-series data;STF algorithm;comparatively higher accuracy;robustness;high-quality images;huge resolution gap;low-resolution images;high-resolution image;super-resolution STF model;public spatiotemporal fusion datasets","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"GAN-Generated Elevation Models in Computational Fluid Dynamics: A Feasibility Study for Complex Urban Terrain","M. Langheinrich; K. Bittner; P. Reinartz","Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","5302","5305","Recently developed methods to simulate very high-resolution (VHR) wind fields over complex urban terrain rely on high-quality three-dimensional vector representations of building information. Unfortunately data of that kind is sparsely available on a worldwide scale. In this work, we investigate the applicability of computational fluid dynamics (CFD) on 2.5D digital surface models (DSMs) automatically generated by generative adversarial network (GAN) from globally available satellite data which includes photogrammetric DSMs and pan-chromatic (PAN) images. The obtained results demonstrate that the GAN-based DSMs are reasonable alternatives to rarely available level of detail 2 (LoD2) vector data, promoting large coverage, continuous wind field derivation over complex terrain.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324564","computational fluid dynamic;Reynolds-averaged Navier-Stokes;detached-eddy simulation;Open-FOAM;complex urban terrain;digital surface model;generative adversarial network","Computational modeling;Buildings;Atmospheric modeling;Gallium nitride;Solid modeling;Generative adversarial networks;Geometry","computational fluid dynamics;digital elevation models;feature extraction;geophysical image processing;image matching;image reconstruction;photogrammetry;pose estimation;solid modelling;terrain mapping;video surveillance;wind","GAN-generated elevation models;computational fluid dynamics;complex urban terrain;recently developed methods;high-resolution wind fields;high-quality three-dimensional vector representations;unfortunately data;worldwide scale;2.5D digital surface models;generative adversarial network;globally available satellite data;photogrammetric DSMs;pan-chromatic images;GAN-based DSMs;rarely available level;detail 2 vector data;continuous wind field derivation;complex terrain","","","","11","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Gan-based SAR to Optical Image Translation in Fire-Disturbed Regions","X. Hu; P. Zhang; Y. Ban","Division of Geoinformatics, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Geoinformatics, KTH Royal Institute of Technology, Stockholm, Sweden; Division of Geoinformatics, KTH Royal Institute of Technology, Stockholm, Sweden","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1456","1459","Climate change by anthropogenic warming leads to increases in dry fuels and promotes forest fires. Multispectral images' quality is easily affected by poor atmospheric conditions. SAR satellite sensors can penetrate through clouds and image day and night. However, the burned area mapping methods widely used for optical data are not feasible to be applied for SAR data owing to the differences in imaging mechanisms. Recent advances in deep image translation can fill this gap by using Generative Adversarial Networks (GAN). In this research, we apply a GAN-based model for SAR to optical image translation over fire-disturbed regions. Specifically, Sentinel-1 SAR images are translated into Sentinel-2 images using the ResNet-based Pix2Pix model, which is trained on 281 large fire events and tested on the other 23 events in Canada. The generated images preserve the spectral characteristics well and show high similarity to the real images with Structure Similarity Index Measure (SSIM) over 0.59.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884234","Sentinel-2 MSI;Sentinel-1 C band SAR;image Translation;Generative Adversarial Network (GAN)","Optical losses;Visualization;Satellites;Soil moisture;Forestry;Optical imaging;Generative adversarial networks","fires;geophysical image processing;optical images;radar imaging;remote sensing by radar;synthetic aperture radar","gan-based SAR;optical image translation;fire-disturbed regions;climate change;anthropogenic warming;dry fuels;promotes forest fires;multispectral images;poor atmospheric conditions;SAR satellite sensors;image day;burned area mapping methods;optical data;SAR data;imaging mechanisms;deep image translation;GAN-based model;Sentinel-1 SAR images;Sentinel-2 images;ResNet-based Pix2Pix model;281 large fire events","","","","12","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks for Hyperspectral Image Classification","L. Zhu; Y. Chen; P. Ghamisi; J. A. Benediktsson","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","26 Aug 2018","2018","56","9","5046","5063","A generative adversarial network (GAN) usually contains a generative network and a discriminative network in competition with each other. The GAN has shown its capability in a variety of applications. In this paper, the usefulness and effectiveness of GAN for classification of hyperspectral images (HSIs) are explored for the first time. In the proposed GAN, a convolutional neural network (CNN) is designed to discriminate the inputs and another CNN is used to generate so-called fake inputs. The aforementioned CNNs are trained together: the generative CNN tries to generate fake inputs that are as real as possible, and the discriminative CNN tries to classify the real and fake inputs. This kind of adversarial training improves the generalization capability of the discriminative CNN, which is really important when the training samples are limited. Specifically, we propose two schemes: 1) a well-designed 1D-GAN as a spectral classifier and 2) a robust 3D-GAN as a spectral-spatial classifier. Furthermore, the generated adversarial samples are used with real training samples to fine-tune the discriminative CNN, which improves the final classification performance. The proposed classifiers are carried out on three widely used hyperspectral data sets: Salinas, Indiana Pines, and Kennedy Space Center. The obtained results reveal that the proposed models provide competitive results compared to the state-of-the-art methods. In addition, the proposed GANs open new opportunities in the remote sensing community for the challenging task of HSI classification and also reveal the huge potential of GAN-based methods for the analysis of such complex and inherently nonlinear data.","1558-0644","","10.1109/TGRS.2018.2805286","National Natural Science Foundation of China(grant numbers:61771171); National Natural Science Foundation of Key International Cooperation(grant numbers:61720106002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8307247","Convolutional neural network (CNN);deep learning;generative adversarial network (GAN);hyperspectral image (HSI) classification","Gallium nitride;Training;Hyperspectral imaging;Feature extraction;Generators","convolution;feedforward neural nets;gallium compounds;hyperspectral imaging;image classification;remote sensing","generative adversarial network;hyperspectral image classification;generative network;discriminative network;hyperspectral images;convolutional neural network;generative CNN;discriminative CNN;adversarial training;1D-GAN;3D-GAN;spectral-spatial classifier;generated adversarial samples;GAN-based methods;classification performance;remote sensing community;HSI classification","","378","","52","IEEE","6 Mar 2018","","","IEEE","IEEE Journals"
"Single Image Cloud Removal Using U-Net and Generative Adversarial Networks","J. Zheng; X. -Y. Liu; X. Wang","Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Department of Electrical Engineering, Columbia University, New York, NY, USA; Department of Electrical Engineering, Columbia University, New York, NY, USA","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6371","6385","Cloud removal is a ubiquitous and important task in remote sensing image processing, which aims at restoring the ground regions shadowed by clouds. It is challenging to remove the clouds for a single satellite image due to the difficulty of distinguishing clouds from white objects on the ground and filling the irregular missing regions with visual consistency. In this article, we propose a novel two-stage cloud removal method. The first stage is cloud segmentation, i.e., extracting the clouds and removing the thin clouds directly using U-Net. The second stage is image restoration, i.e., removing the thick cloud and recovering the corresponding irregular missing regions using generative adversarial network (GAN). We evaluate the proposed scheme on both synthetic images and real satellite images (over  $20\,000\, \times \,20\,000$  pixels). On synthetic images for cloud coverage less than 40%, the proposed scheme achieves improvements of 0.049–0.078 in Structural SIMilarity (SSIM) and 3.8–6.2 dB in peak signal-to-noise ratio (PSNR), while the  $\ell _{1}$ -norm error reduces by 49%–78%, compared with a state-of-the-art deep learning method Pix2Pix. On real satellite images, we demonstrate the consistent visual results of the proposed scheme.","1558-0644","","10.1109/TGRS.2020.3027819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224941","Encoder-decoder;generative adversarial network (GAN);single image cloud removal;U-Net","Clouds;Image restoration;Satellites;Cloud computing;Task analysis;Visualization;Sensors","clouds;geophysical image processing;image classification;image processing;image restoration;image segmentation;image sensors;learning (artificial intelligence);remote sensing","single satellite image;two-stage cloud removal method;cloud segmentation;image restoration;irregular missing regions;generative adversarial network;synthetic images;satellite images;cloud coverage;single image cloud removal;remote sensing image processing","","14","","54","IEEE","15 Oct 2020","","","IEEE","IEEE Journals"
"A STUDY ON GENERALIZING BUILDING EXTRACTION MODELS TO UNSEEN DATASETS USING SOURCE DOMAIN TRANSFER","M. Parusinski; S. Ibrahim; T. Lampert","ICube-SERTIT, Université de Strasbourg; ICube-SERTIT, Université de Strasbourg; ICube, Université de Strasbourg","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1352","1355","Building footprint detection from remote sensing imagery remains an activate field of research. In particular, training models that generalise well remains challenging. A common approach to building generic models is to leverage domain adaptation approaches based on style transfer to adapt labelled datasets to an unlabelled domain. These assume the availability of labelled and unlabelled data during training. In applications such as emergency mapping however, the target domain is not always known in advance, and there is therefore a need to build models that generalise to domains unseen at training time. Using SpaceNet data various domain adaptation approaches are evaluated to improve model gen-eralisation. This article demonstrates that domain adaptation improves model generalisation, and the choice of source and target domain can be more significant than the choice of style transfer algorithm.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883354","Building extraction;deep learning;optical satellite images;domain adaptation;data augmentation;semantic segmentation;generative adversarial networks","Training;Adaptation models;Image segmentation;Satellites;Buildings;Performance gain;Data models","generalisation (artificial intelligence);geophysical image processing;learning (artificial intelligence);remote sensing","target domain;generalise;training time;SpaceNet data;model gen-eralisation;model generalisation;style transfer algorithm;generalizing building extraction models;unseen datasets;source domain transfer;building footprint detection;remote sensing imagery;training models;building generic models;leverage domain adaptation approaches;labelled datasets;unlabelled domain;labelled data;unlabelled data","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"An Ensemble Wasserstein Generative Adversarial Network Method for Road Extraction From High Resolution Remote Sensing Images in Rural Areas","C. Yang; Z. Wang","China Highway Engineering Consultants Corporation Data Company Ltd., Beijing, China; Department of Computer Science, University of Auckland, Auckland, New Zealand","IEEE Access","29 Sep 2020","2020","8","","174317","174324","Road extraction from high resolution remote sensing (HR-RS) images is an important yet challenging computer vision task. In this study, we propose an ensemble Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP) method called E-WGAN-GP for road extraction from HR-RS images in rural areas. The WGAN-GP model modifies the standard GANs with Wasserstein distance and gradient penalty. We add a spatial penalty term in the loss function of the WGAN-GP model to solve the class imbalance problem typically in road extraction. Parameter experiments are undertaken to determine the best spatial penalty and the weight term in the new loss function based on GaoFen-2 dataset. In addition, we execute an ensemble strategy in which we first train two WGAN-GP models using the U-Net and BiSeNet as generator respectively, and then intersect their inferred outputs to yield better road vectors. We train our new model with GaoFen-2 HR-RS images in rural areas from China and also the DeepGlobe Road Extraction dataset. Compared with the U-Net, BiSeNet, D-LinkNet and WGAN-GP methods without ensemble, our new method makes a good trade-off between precision and recall with F1-score = 0.85 and IoU = 0.73.","2169-3536","","10.1109/ACCESS.2020.3026084","Grand Special of High resolution On Earth Observation: Application demonstration system of high resolution remote sensing and transportation(grant numbers:07-Y30B03-9001-19/21); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204705","Deep Learningwi;Road Extraction;High Resolution Remote Sensing;Wasserstein GAN;Ensemble Learning","Roads;Gallium nitride;Feature extraction;Generative adversarial networks;Image segmentation;Computer architecture;Image resolution","feature extraction;geophysical image processing;gradient methods;image resolution;learning (artificial intelligence);neural nets;remote sensing;roads","loss function;WGAN-GP model;road vectors;GaoFen-2 HR-RS images;rural areas;DeepGlobe Road Extraction dataset;WGAN-GP methods;high resolution remote sensing images;computer vision task;gradient penalty method;E-WGAN-GP;spatial penalty term;ensemble Wasserstein generative adversarial network method;Wasserstein distance;U-Net;BiSeNet","","10","","44","CCBY","23 Sep 2020","","","IEEE","IEEE Journals"
"SRARNet: A Unified Framework for Joint Superresolution and Aircraft Recognition","W. Tang; C. Deng; Y. Han; Y. Huang; B. Zhao","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2021","2021","14","","327","336","Aircraft recognition in high-resolution remote sensing images has rapidly progressed with the advance of convolutional neural networks (CNNs). However, the previous CNN-based methods may not work well for recognizing aircraft in low-resolution remote sensing images because the blurred aircraft in these images offer insufficient details to distinguish them from similar types of targets. An intuitive solution is to introduce superresolution preprocessing. However, conventional superresolution methods mainly focus on reconstructing natural images with detailed texture rather than constructing a high-resolution object with strong discriminative information for the recognition task. To address these problems, we propose a unified framework for joint superresolution and aircraft recognition (Joint-SRARNet) that tries to improve the recognition performance by generating discriminative, high-resolution aircraft from low-resolution remote sensing images. Technically, this network integrates superresolution and recognition tasks into the generative adversarial network (GAN) framework through a joint loss function. The generator is constructed as a joint superresolution and refining subnetwork that can upsample small blurred images into high-resolution ones and restore high-frequency information. In the discriminator, we introduce a new classification loss function that forces the discriminator to distinguish between real and fake images while recognizing the type of aircraft. In addition, the classification loss function is back-propagated to the generator to obtain high-resolution images with discriminative information for easier recognition. Extensive experiments on the challenging multitype aircraft of remote sensing images (MTARSI) dataset demonstrate the effectiveness of the proposed method in restoring a clear super-resolved image from a small blurred image and significant improvement in the recognition performance. To our knowledge, this is the first work on joint superresolution and aircraft recognition tasks.","2151-1535","","10.1109/JSTARS.2020.3037225","National Natural Science Foundation of China(grant numbers:91838303,91738302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9254000","Aircraft recognition;multitask GAN;superresolution","Aircraft;Image recognition;Task analysis;Remote sensing;Generative adversarial networks;Aircraft manufacture;Generators","backpropagation;convolutional neural nets;image classification;image resolution;image restoration;image sampling;object recognition;remote sensing","convolutional neural networks;SRARNet;GAN framework;CNN-based methods;generative adversarial network framework;high-resolution aircraft;natural images;conventional superresolution methods;blurred aircraft;low-resolution remote sensing images;high-resolution remote sensing images;aircraft recognition;blurred image","","4","","42","CCBY","10 Nov 2020","","","IEEE","IEEE Journals"
"Generating Sketch-Based Synthetic Seismic Images With Generative Adversarial Networks","R. S. Ferreira; J. Noce; D. A. B. Oliveira; E. V. Brazil","IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","21 Jul 2020","2020","17","8","1460","1464","The characterization of the subsurface is paramount for the exploration and production life cycle and, more specifically, for the identification of potential hydrocarbon accumulations. In recent years, the oil and gas industry has increased its interest in applying machine learning to accelerate the seismic interpretation process, which is regarded as a time-consuming and human-centered task. Although machine learning has been successfully used in many applications ranging from stratigraphic segmentation to salt dome detection, a usual bottleneck is the need for a large amount of high-quality annotated data. To overcome this, data augmentation approaches are commonly used, and one of the most powerful is synthetic data generation. In addition, sketch-based synthetic seismic images can be used to support image retrieval applications to help oil companies leverage petabytes of seismic data sets. In this context, this letter investigates the generation of synthetic seismic images based on sketches using generative adversarial networks (GANs). To the best of our knowledge, this is the first work to propose such an approach. Experiments with five different sketch types in a public seismic data set indicate that realistic seismic images can be synthesized using rather simple sketches.","1558-0571","","10.1109/LGRS.2019.2945680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8870208","Artificial neural networks;geophysical image processing;image generation","Training;Image edge detection;Generators;Gallium nitride;Machine learning;Geology;Oils","gas industry;geophysical image processing;geophysical techniques;hydrocarbon reservoirs;image retrieval;learning (artificial intelligence);petroleum industry;production engineering computing;seismology","machine learning;high-quality annotated data;data augmentation approaches;synthetic data generation;sketch-based synthetic seismic images;image retrieval applications;oil companies leverage petabytes;seismic data sets;generative adversarial networks;public seismic data;realistic seismic images;production life cycle;potential hydrocarbon accumulations;gas industry;seismic interpretation process","","2","","25","IEEE","16 Oct 2019","","","IEEE","IEEE Journals"
"MLFcGAN: Multilevel Feature Fusion-Based Conditional GAN for Underwater Image Color Correction","X. Liu; Z. Gao; B. M. Chen","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1488","1492","Color correction for underwater images has received increasing interest, due to its critical role in facilitating available mature vision algorithms for underwater scenarios. Inspired by the stunning success of deep convolutional neural network (DCNN) techniques in many vision tasks, especially the strength in extracting features in multiple scales, we propose a deep multiscale feature fusion net based on the conditional generative adversarial network (GAN) for underwater image color correction. In our network, multiscale features are extracted first, followed by augmenting local features in each scale with global features. This design was verified to facilitate more effective and faster network learning, resulting in better performance in both color correction and detail preservation. We conducted extensive experiments and compared the results with state-of-the-art approaches quantitatively and qualitatively, showing that our method achieves significant improvements.","1558-0571","","10.1109/LGRS.2019.2950056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894129","Conditional generative adversarial network (cGAN);feature extraction and fusion;image enhancement;underwater image color correction","Feature extraction;Image color analysis;Gallium nitride;Loss measurement;Generative adversarial networks;Image restoration;Generators","computer vision;convolutional neural nets;feature extraction;image classification;image colour analysis;image fusion;learning (artificial intelligence)","feature fusion-based conditional GAN;underwater image color correction;underwater scenarios;deep convolutional neural network;deep multiscale feature fusion net;conditional generative adversarial network;network learning;mature vision algorithms;DCNN","","38","","34","IEEE","7 Nov 2019","","","IEEE","IEEE Journals"
"Deep Learning for Automatic Colorization of Legacy Grayscale Aerial Photographs","Q. Poterek; P. -A. Herrault; G. Skupinski; D. Sheeren","CNRS UMR 7362, Laboratoire Image Ville Environnement, University of Strasbourg, Strasbourg, France; CNRS UMR 7362, Laboratoire Image Ville Environnement, University of Strasbourg, Strasbourg, France; CNRS UMR 7362, Laboratoire Image Ville Environnement, University of Strasbourg, Strasbourg, France; INRA UMR 1201, DYNAFOR, University of Toulouse, Castanet-Tolosan, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 Jun 2020","2020","13","","2899","2915","Legacy grayscale aerial photographs represent one of the main available sources for studying the past state of the environment and its relationship to the present. However, these photographs lack spectral information thereby hindering their use in current remote sensing approaches that rely on spectral data for characterizing surfaces. This article proposes a conditional generative adversarial network, a deep learning model, to enrich legacy photographs by predicting color channels for an input grayscale image. The technique was used to colorize two orthophotographs (taken in 1956 and 1978) covering the entire Eurométropole de Strasbourg. To assess the model's performances, two strategies were proposed: first, colorized photographs were evaluated with metrics such as peak signal-to-noise ratio (PSNR), and structural similarity index (SSIM); second, random forest classifications were performed to extract land cover classes from grayscale and colorized photographs, respectively. The results revealed strong performances, with PSNR = 25.56 ± 2.20 and SSIM = 0.93 ± 0.06 indicating that the model successfully learned the mapping between grayscale and color photographs over a large territory. Moreover, land cover classifications performed on colorized data showed significant improvements over grayscale photographs, respectively, +6% and +17% for 1956 and 1978. Finally, the plausibility of outputs images was evaluated visually. We conclude that deep learning models are powerful tools for improving radiometric properties of old aerial grayscale photographs and land cover mapping. We also argue that the proposed approach could serve as a basis for further developments aiming to promote the use of aerial photographs archives for landscapes reconstruction.","2151-1535","","10.1109/JSTARS.2020.2992082","Zone Atelier Environnementale Urbaine; French Long Term Ecosystem Research Network; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103274","Colorization;deep learning;generative adversarial network (GAN);grayscale imagery;historical aerial photograph;remote sensing","Gray-scale;Image color analysis;Deep learning;Remote sensing;Generative adversarial networks;Sensors;Photography","crops;geophysical image processing;image classification;image colour analysis;learning (artificial intelligence);remote sensing;terrain mapping","automatic colorization;legacy grayscale aerial photographs;photographs lack spectral information;current remote sensing approaches;conditional generative adversarial network;deep learning model;legacy photographs;color channels;input grayscale image;colorized photographs;land cover classes;color photographs;land cover classifications;colorized data;old aerial grayscale photographs;land cover mapping;aerial photographs archives","","13","","81","CCBY","28 May 2020","","","IEEE","IEEE Journals"
"A High-Performance Multispectral Adaptation GAN for Harmonizing Dense Time Series of Landsat-8 and Sentinel-2 Images","R. Sedona; C. Paris; G. Cavallaro; L. Bruzzone; M. Riedel","University of Iceland, Reykjavik, Iceland; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Jülich Supercomputing Centre, Jülich, Germany; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; University of Iceland, Reykjavik, Iceland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Oct 2021","2021","14","","10134","10146","The combination of data acquired by Landsat-8 and Sentinel-2 earth observation missions produces dense time series (TSs) of multispectral images that are essential for monitoring the dynamics of land-cover and land-use classes across the earth's surface with high temporal resolution. However, the optical sensors of the two missions have different spectral and spatial properties, thus they require a harmonization processing step before they can be exploited in remote sensing applications. In this work, we propose a workflow-based on a deep learning approach to harmonize these two products developed and deployed on an high-performance computing environment. In particular, we use a multispectral generative adversarial network with a U-Net generator and a PatchGan discriminator to integrate existing Landsat-8 TSs with data sensed by the Sentinel-2 mission. We show a qualitative and quantitative comparison with an existing physical method [National Aeronautics and Space Administration (NASA) Harmonized Landsat and Sentinel (HLS)] and analyze original and generated data in different experimental setups with the support of spectral distortion metrics. To demonstrate the effectiveness of the proposed approach, a crop type mapping task is addressed using the harmonized dense TS of images, which achieved an overall accuracy of 87.83% compared to 81.66% of the state-of-the-art method.","2151-1535","","10.1109/JSTARS.2021.3115604","CoE RAISE project; European Union's Horizon 2020 Research and Innovation Framework Programme(grant numbers:951733); ADMIRE project; European Union's Horizon 2020 JTI-EuroHPC research and innovation programme(grant numbers:956748); DEEP-EST project; European Union's Horizon 2020 research and innovation programme(grant numbers:754304); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548804","Deep learning (DL);dense time series (TSs);generative adversarial network (GAN);harmonization;high performance computing (HPC);Landsat-8;remote sensing (RS);sentinel-2;virtual constellation","Remote sensing;Earth;Artificial satellites;Spatial resolution;Optical sensors;Generators;Generative adversarial networks","deep learning (artificial intelligence);geophysical image processing;land cover;land use;remote sensing by laser beam;time series","spectral distortion metrics;high-performance multispectral adaptation GAN;dense time series;Sentinel-2 earth observation missions;multispectral images;land-cover;land-use classes;optical sensors;spatial properties;harmonization processing step;remote sensing applications;deep learning approach;multispectral generative adversarial network;U-Net generator;Sentinel-2 mission;qualitative comparison;Landsat-8 images","","3","","63","CCBY","27 Sep 2021","","","IEEE","IEEE Journals"
"Generative Adversarial Network for Deblurring of Remote Sensing Image","Y. Zhang; Y. Xiang; L. Bai","Department of Computer Science, Yunnan Normal University, Kunming, China; Department of Computer Science, Yunnan Normal University, Kunming, China; Department of Computer Science, Yunnan Normal University, Kunming, China","2018 26th International Conference on Geoinformatics","6 Dec 2018","2018","","","1","4","Deblurring is a classical problem for remote sensing images, which is known to be difficult as an ill-posed problem. A feasible solution for the problem is incorporating various priors into restoration procedure as constrained conditions. However, the learning of priors usually assumes that the blurs in an image are produced by fixed types of reasons, and thus a possible decrease in model's description ability. In this paper, an end-to-end learned method based on generative adversarial networks (GANs) is proposed to tackle the deblurring problem for remote sensing images. The proposed deblurring model does not need any prior assumptions for the blurs. The proposed method was evaluated on a satellite map image data set and state-of-the-art performance was obtained.","2161-0258","978-1-5386-7619-6","10.1109/GEOINFORMATICS.2018.8557110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8557110","Generative Adversatial Network (GAN);image deblurring;remote sensing image;loss function","","geophysical image processing;image restoration;learning (artificial intelligence);neural nets;remote sensing","remote sensing image;deblurring model;satellite map image data set;generative adversarial network;end-to-end learned method","","4","","29","IEEE","6 Dec 2018","","","IEEE","IEEE Conferences"
"A Generative Discriminatory Classified Network for Change Detection in Multispectral Imagery","M. Gong; Y. Yang; T. Zhan; X. Niu; S. Li","School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Science and Engineering, Southeast University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Jan 2019","2019","12","1","321","333","Multispectral image change detection based on deep learning generally needs a large amount of training data. However, it is difficult and expensive to mark a large amount of labeled data. To deal with this problem, we propose a generative discriminatory classified network (GDCN) for multispectral image change detection, in which labeled data, unlabeled data, and new fake data generated by generative adversarial networks are used. The GDCN consists of a discriminatory classified network (DCN) and a generator. The DCN divides the input data into changed class, unchanged class, and extra class, i.e., fake class. The generator recovers the real data from input noises to provide additional training samples so as to boost the performance of the DCN. Finally, the bitemporal multispectral images are input to the DCN to get the final change map. Experimental results on the real multispectral imagery datasets demonstrate that the proposed GDCN trained by unlabeled data and a small amount of labeled data can achieve competitive performance compared with existing methods.","2151-1535","","10.1109/JSTARS.2018.2887108","National Natural Science Foundation of China(grant numbers:61772393); National Key Research and Development Program of China(grant numbers:2017YFB0802200); Key Research and Development Program of Shaanxi Province(grant numbers:2018ZDXM-GY-045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600384","Change detection;deep learning;generative adversarial networks (GANs);multispectral imagery","Gallium nitride;Training;Remote sensing;Feature extraction;Deep learning;Generators;Task analysis","image classification;learning (artificial intelligence);remote sensing","GDCN;DCN;bitemporal multispectral images;multispectral imagery datasets;generative discriminatory classified network;multispectral image change detection;generative adversarial networks;discriminatory classified network;deep learning","","44","","44","IEEE","3 Jan 2019","","","IEEE","IEEE Journals"
"Unsupervised cross-sensor domain adaptation using adversarial network for land cover classification","I. Kalita; N. Mugganawar; M. Roy","Indian Institute of Information Technology, Guwahati, India; Indian Institute of Information Technology, Guwahati, India; Indian Institute of Information Technology, Guwahati, India","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5724","5727","Remote sensing data gathered by various satellites (cross-sensor data having) are used to have a significant impact to lower down the performance of the stand-alone land cover classification model. The collected data from source and target regions have different probability distributions due to the different resolution of images and different geographical locations. To deal with this problem, an adversarial network-based unsupervised cross-sensor domain network has been investigated by considering two source → target scenarios using hyperspectral and aerial image datasets. Initially, an unsupervised generative adversarial network (GAN) has been implemented to minimize the distribution between both domains. Following that, the transformed target images are obtained using the trained GAN architecture. Thereafter, a deep convolutional neural network (DCNN) has been trained using the source images and finally, the trained DCNN is used to predict the land cover classes under a multi-sensor framework. The effectiveness of the proposed scheme has been compared with the state-of-the-art techniques, and the results are found to be promising to handle the issues under an unsupervised cross-sensor environment.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884404","Land cover classification;Cross-sensor domain adaptation;Convolutional neural network;Active learning","Satellites;Image resolution;Generative adversarial networks;Probability distribution;Data models;Convolutional neural networks;Hyperspectral imaging","geophysical image processing;geophysical signal processing;image classification;neural nets;probability;remote sensing;sensor fusion;terrain mapping;unsupervised learning","unsupervised cross-sensor domain adaptation;land cover classification;sensing data;cross-sensor data;target regions;different probability distributions;different geographical locations;adversarial network-based unsupervised cross-sensor domain network;source → target scenarios;hyperspectral image datasets;aerial image datasets;unsupervised generative adversarial network;transformed target images;trained GAN architecture;deep convolutional neural network;source images;land cover classes;multisensor framework;unsupervised cross-sensor environment","","","","14","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"PSTAF-GAN: Progressive Spatio-Temporal Attention Fusion Method Based on Generative Adversarial Network","Q. Liu; X. Meng; F. Shao; S. Li","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","14 Apr 2022","2022","60","","1","13","Spatio-temporal fusion aims to integrate multisource remote sensing images with complementary high spatial and temporal resolutions, so as to obtain time-series high spatial resolution fused images. Currently, deep learning (DL)-based spatio-temporal fusion methods have received broad attention. However, on one hand, most of the existing DL-based methods train the model in a band-by-band manner, ignoring the correlations among bands. On the other hand, the general coarse spatio-temporal changes in low spatial resolution images (e.g., MODIS) calculated at the pixel domain cannot completely cover the fine spatio-temporal changes in high spatial resolution images (e.g., Landsat), due to complex surface features and the general large spatial resolution ratio between fine and coarse images. Besides, the existing DL-based spatio-temporal fusion methods are insufficient in exploring multiscale information by only stacking convolutional kernels with different sizes. To alleviate the above challenges, we propose a progressive spatio-temporal attention fusion model in a multiband training manner based on generative adversarial network (PSTAF-GAN). Specifically, we design a flexible multiscale feature extraction architecture to extract multiscale feature hierarchies. Then, spatio-temporal changes are calculated on the feature domain in different feature hierarchies. Besides, a spatio-temporal attention fusion architecture is proposed to fuse the spatio-temporal changes and ground details in a coarse-to-fine manner, which can explore multiscale information more sufficient and gradually recover the target image. The results of quantitative and qualitative experiments on two publicly available benchmark datasets show that the proposed PSTAF-GAN can achieve the best performance compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3161563","National Natural Science Foundation of China(grant numbers:42171326,41801252,62071261); Natural Science Foundation of Zhejiang Province(grant numbers:LY22F010014); Fellowship of China Postdoctoral Science Foundation(grant numbers:2020M672490); K. C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740577","Generative adversarial network (GAN);multilevel feature;remote sensing;spatio-temporal attention fusion;weight sharing","Feature extraction;Spatial resolution;Image resolution;Generative adversarial networks;Convolution;Predictive models;Convolutional neural networks","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image resolution;object detection;remote sensing;spatiotemporal phenomena;time series","multiscale information;progressive spatio-temporal attention fusion model;multiband training manner;generative adversarial network;PSTAF-GAN;flexible multiscale feature extraction architecture;multiscale feature hierarchies;spatio-temporal attention fusion architecture;coarse-to-fine manner;target image;multisource remote sensing images;temporal resolutions;time-series high spatial resolution;deep learning-based spatio-temporal fusion methods;DL-based methods;band-by-band manner;low spatial resolution images;high spatial resolution images;complex surface features","","1","","53","IEEE","23 Mar 2022","","","IEEE","IEEE Journals"
"Improving Road Semantic Segmentation Using Generative Adversarial Network","A. Abdollahi; B. Pradhan; G. Sharma; K. N. A. Maulud; A. Alamri","Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and IT, School of Information, Systems and Modelling, University of Technology Sydney (UTS), Sydney, NSW, Australia; Earth Observation Centre, Institute of Climate Change, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical and Computer Engineering, University of Rochester, Rochester, NY, USA; Department of Civil Engineering, Faculty of Engineering and Built Environment, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Geology and Geophysics, College of Science, King Saud University, Riyadh, Saudi Arabia","IEEE Access","30 Apr 2021","2021","9","","64381","64392","Road network extraction from remotely sensed imagery has become a powerful tool for updating geospatial databases, owing to the success of convolutional neural network (CNN) based deep learning semantic segmentation techniques combined with the high-resolution imagery that modern remote sensing provides. However, most CNN approaches cannot obtain high precision segmentation maps with rich details when processing high-resolution remote sensing imagery. In this study, we propose a generative adversarial network (GAN)-based deep learning approach for road segmentation from high-resolution aerial imagery. In the generative part of the presented GAN approach, we use a modified UNet model (MUNet) to obtain a high-resolution segmentation map of the road network. In combination with simple pre-processing comprising edge-preserving filtering, the proposed approach offers a significant improvement in road network segmentation compared with prior approaches. In experiments conducted on the Massachusetts road image dataset, the proposed approach achieves 91.54% precision and 92.92% recall, which correspond to a Mathews correlation coefficient (MCC) of 91.13%, a Mean intersection over union (MIOU) of 87.43% and a F1-score of 92.20%. Comparisons demonstrate that the proposed GAN framework outperforms prior CNN-based approaches and is particularly effective in preserving edge information.","2169-3536","","10.1109/ACCESS.2021.3075951","Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and IT, University of Technology Sydney (UTS); Researchers Supporting Project, King Saud University, Riyadh, Saudi Arabia(grant numbers:RSP-2020/14); Universiti Kebangsan Malaysia, DANA IMPAK PERDANA(grant numbers:DIP-2018-030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9416669","GAN;road segmentation;remote sensing;deep learning;U-Net","Roads;Image segmentation;Generative adversarial networks;Semantics;Remote sensing;Generators;Feature extraction","feature extraction;geophysical image processing;image classification;image segmentation;learning (artificial intelligence);neural nets;remote sensing;roads","high-resolution aerial imagery;generative part;presented GAN approach;high-resolution segmentation map;simple pre-processing comprising edge-preserving filtering;road network segmentation;prior approaches;Massachusetts road image dataset;prior CNN-based approaches;improving road semantic segmentation;road network extraction;remotely sensed imagery;geospatial databases;convolutional neural network;deep learning semantic segmentation techniques;high-resolution imagery;modern remote sensing;CNN approaches;high precision segmentation maps;high-resolution remote sensing imagery;generative adversarial network-based deep learning approach;road segmentation","","24","","50","CCBY","27 Apr 2021","","","IEEE","IEEE Journals"
"Multimodal GANs: Toward Crossmodal Hyperspectral–Multispectral Image Segmentation","D. Hong; J. Yao; D. Meng; Z. Xu; J. Chanussot","GIPSA-laboratory, CNRS, Grenoble INP, University of Grenoble Alpes, Grenoble, France; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Macau Institute of Systems Engineering, Macau University of Science and Technology, Taipa, Macau; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","5103","5113","This article addresses the problem of semantic segmentation with limited cross-modality data in large-scale urban scenes. Most prior works have attempted to address this issue by using multimodal deep neural networks (DNNs). However, their ability to effectively blending different properties across multimodalities and robustly learning representations from complex scenes remains limited, particularly in the absence of sufficient and well-annotated training images. This leads to a challenge related to cross-modality learning with multimodal DNNs. To this end, we introduce two novel plug-and-play units in the network: self-generative adversarial networks (GANs) module and mutual-GANs module, to learn perturbation-insensitive feature representations and to eliminate the gap between multimodalities, respectively, yielding more effective and robust information transfer. Furthermore, a patchwise progressive training strategy is devised to enable effective network learning with limited samples. We evaluate the proposed network on two multimodal (hyperspectral and multispectral) overhead image data sets and achieve a significant improvement in comparison with several state-of-the-art methods.","1558-0644","","10.1109/TGRS.2020.3020823","National Natural Science Foundation of China(grant numbers:U1811461,11690011,61721002); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9198910","Classification;convolutional neural network (CNN);cross modality;deep learning (DL);generative adversarial networks (GANs);hyperspectral (HS);multispectral (MS);remote sensing;semantic segmentation;single image training","Image segmentation;Semantics;Gallium nitride;Robustness;Training;Feature extraction;Imaging","feature extraction;hyperspectral imaging;image representation;image segmentation;learning (artificial intelligence);neural nets","self-generative adversarial networks module;mutual-GANs module;perturbation-insensitive feature representations;information transfer;patchwise progressive training strategy;multimodal GANs;semantic segmentation;urban scenes;multimodal deep neural networks;cross-modality learning;multimodal DNNs;plug-and-play units;hyperspectral-multispectral image segmentation","","42","","56","IEEE","16 Sep 2020","","","IEEE","IEEE Journals"
"Unsupervised Deep Transfer Learning-Based Change Detection for HR Multispectral Images","S. Saha; Y. T. Solano-Correa; F. Bovolo; L. Bruzzone","Department of Information Engineering and Computer science, University of Trento, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Department of Information Engineering and Computer science, University of Trento, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","22 Apr 2021","2021","18","5","856","860","To overcome the limited capability of most state-of-the-art change detection (CD) methods in modeling spatial context of multispectral high spatial resolution (HR) images and exploiting all spectral bands jointly, this letter presents a novel unsupervised deep-learning-based CD method that can effectively model contextual information and handle the large number of bands in multispectral HR images. This is achieved by exploiting all spectral bands after grouping them into spectral-dedicated band groups. To eliminate the necessity of multitemporal training data, the proposed method exploits a data set targeted for image classification to train spectral-dedicated Auxiliary Classifier Generative Adversarial Networks (ACGANs). They are used to obtain pixelwise deep change hypervector from multitemporal images. Each feature in deep change hypervector is analyzed based on the magnitude to identify changed pixels. An ensemble decision fusion strategy is used to combine change information from different features. Experimental results on the urban, Alpine, and agricultural Sentinel-2 data sets confirm the effectiveness of the proposed method.","1558-0571","","10.1109/LGRS.2020.2990284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089195","Change detection (CD);deep learning;generative adversarial network;high resolution;Sentinel-2","Feature extraction;Spatial resolution;Training;Gallium nitride;Generative adversarial networks;Generators","feature extraction;geophysical image processing;geophysical signal processing;image classification;image segmentation;learning (artificial intelligence);pattern classification;remote sensing;unsupervised learning","unsupervised deep transfer learning-based change detection;HR multispectral;state-of-the-art change detection methods;modeling spatial context;multispectral high spatial resolution images;spectral bands;CD method;contextual information;multispectral HR images;spectral-dedicated band groups;multitemporal training data;data set;image classification;Auxiliary Classifier Generative Adversarial Networks;pixelwise deep change hypervector;multitemporal images;changed pixels;change information","","29","","12","IEEE","7 May 2020","","","IEEE","IEEE Journals"
"DRL-GAN: Dual-Stream Representation Learning GAN for Low-Resolution Image Classification in UAV Applications","Y. Xi; W. Jia; J. Zheng; X. Fan; Y. Xie; J. Ren; X. He","School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; School of Computer Science, Northwestern Polytechnical University, Shaanxi, China; School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia; School of Computer Science, Northwestern Polytechnical University, Shaanxi, China; School of Computer Sciences, Guangdong Polytechnic Normal University, Guangzhou, China; School of Electrical and Data Engineering, University of Technology Sydney, Ultimo, NSW, Australia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","18 Jan 2021","2021","14","","1705","1716","Identifying tiny objects from extremely low-resolution (LR) unmanned-aerial-vehicle-based remote sensing images is generally considered as a very challenging task, because of very limited information in the object areas. In recent years, there have been very limited attempts to approach this problem. These attempts intend to deal with LR image classification by enhancing either the poor image quality or image representations. In this article, we argue that the performance improvement in LR image classification is affected by the inconsistency of the information loss and learning priority on low-frequency (LF) components and high-frequency (HF) components. To address this LF-HF inconsistency problem, we propose a dual-stream representation learning generative adversarial network (DRL-GAN). The core idea is to produce enhanced image representations optimal for LR recognition by simultaneously recovering the missing information in LF and HF components, respectively, under the guidance of high-resolution (HR) images. We evaluate the performance of DRL-GAN on the challenging task of LR image classification. A comparison of the experimental results on the LR benchmark, namely HRSC and CIFAR-10, and our newly collected `WIDER-SHIP' dataset demonstrates the effectiveness of our DRL-GAN, which significantly improves the classification performance, with up to 10% gain on average.","2151-1535","","10.1109/JSTARS.2020.3043109","Creating the Dataset(grant numbers:ONR-G N62909-18-1-2169); National Natural Science Foundation of China(grant numbers:61972321); Research and Development Plan of Shaanxi Province(grant numbers:2017ZDXM-GY-094,2015KTZDGY04-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286580","Convolutional neural networks (CNNs);generative adversarial networks;low-resolution (LR) image classification;representation learning;unmanned aerial vehicle (UAV)-based remote sensing","Feature extraction;Task analysis;Remote sensing;Hafnium;Image recognition;Neural networks;Image representation","autonomous aerial vehicles;geophysical image processing;image classification;image enhancement;image representation;image resolution;learning (artificial intelligence);neural nets;remote sensing;robot vision","DRL-GAN;dual-stream representation;low-resolution image classification;low-resolution unmanned-aerial-vehicle-based remote sensing images;LR image classification;information loss;low-frequency components;high-frequency components;enhanced image representations;LR recognition;high-resolution images;HRSC benchmark;CIFAR-10 benchmark","","13","","54","CCBY","8 Dec 2020","","","IEEE","IEEE Journals"
"Graph-Based Semisupervised Learning With Weighted Features for Hyperspectral Remote Sensing Image Classification","Q. Wang; Q. Zhang; J. Zhang; S. Kang; Y. Wang","School of Measurement-Control and Communication Engineering, Harbin University of Science and Technology, Harbin, China; School of Measurement-Control and Communication Engineering, Harbin University of Science and Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Measurement-Control and Communication Engineering, Harbin University of Science and Technology, Harbin, China; School of Measurement-Control and Communication Engineering, Harbin University of Science and Technology, Harbin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Aug 2022","2022","15","","6356","6370","Graph neural network has an excellent performance in obtaining the similarity relationship of samples, so it has been widely used in computer vision. But the hyperspectral remote sensing image (HSI) has some problems, such as data redundancy, noise, lack of labeled samples, and insufficient utilization of spatial information. These problems affect the accuracy of HSI classification using graph neural networks. To solve the aforementioned problems, this article proposes graph-based semisupervised learning with weighted features for HSI classification. The method proposed in this article first uses the stacked autoencoder network to extract features, which is used to remove the redundancy of HSI data. Then, the similarity attenuation coefficient is introduced to improve the original feature weighting scheme. In this way, the contribution difference of adjacent pixels to the center pixel is reflected. Finally, to obtain more generalized spectral features, a shallow feature extraction mechanism is added to the stacked autoencoder network. And features that have good generalization can solve the problem of the lack of labeled samples. The experiment on three different types of datasets demonstrates that the proposed method in this article can get better classification performance in the case of the scarcity of labeled samples than other classification methods.","2151-1535","","10.1109/JSTARS.2022.3195639","Natural Foundation of Heilongjiang Province(grant numbers:LH2019E058); Scientific Research Foundation of Harbin University of Science and Technology(grant numbers:217045332); National Natural Science Foundation of China(grant numbers:61871150); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847091","Convolutional neural network (CNN);graph with weighted features;hyperspectral image classification;stacked autoencoder","Feature extraction;Merging;Data mining;Convolution;Hyperspectral imaging;Generative adversarial networks;Decoding","computer vision;feature extraction;geophysical image processing;graph theory;hyperspectral imaging;image classification;image resolution;neural nets;remote sensing;semi-supervised learning (artificial intelligence)","weighted features;hyperspectral remote sensing image classification;graph neural network;computer vision;data redundancy;spatial information;HSI classification;stacked autoencoder network;similarity attenuation coefficient;feature weighting scheme;generalized spectral features;shallow feature extraction mechanism;graph-based semisupervised learning","","","","40","CCBY","1 Aug 2022","","","IEEE","IEEE Journals"
"Deep Learning for SAR-Optical Image Matching","L. H. Hughes; N. Merkle; T. Bürgmann; S. Auer; M. Schmitt","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; Airbus Defence and Space GmbH, Immenstaad, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4877","4880","The automatic matching of corresponding regions in remote sensing imagery acquired by synthetic aperture radar (SAR) and optical sensors is a crucial pre-requesite for many data fusion endeavours such as target recognition, image registration, or 3D-reconstruction by stereogrammetry. Driven by the success of deep learning in conventional optical image matching, we have carried out extensive research with regard to deep matching for SAR-optical multi-sensor image pairs in the recent past. In this paper, we summarize the achieved findings, including different concepts based on (pseudo-)siamese convolutional neural network architectures, hard negative mining, alternative formulations of the underlying loss function, and creation of artificial images by generative adversarial networks. Based on data from state-of-the-art remote sensing missions such as TerraSAR-X, Prism, Worldview-2, and Sentinel-1/2, we show what is already possible today, while highlighting challenges to be tackled by future research endeavors.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898635","Deep Learning;Image Matching;Optical Images;SAR Images;Data Fusion","Optical imaging;Optical sensors;Image matching;Optical fiber networks;Training data;Training;Deep learning","convolutional neural nets;image fusion;image matching;learning (artificial intelligence);optical images;optical sensors;radar imaging;remote sensing;synthetic aperture radar","remote sensing imagery;optical sensors;data fusion;deep learning;SAR-optical multisensor image pairs;artificial images;SAR-optical image matching;synthetic aperture radar;Siamese convolutional neural network architectures","","23","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"PTGAN: A Proposal-Weighted Two-Stage GAN with Attention for Hyperspectral Target Detection","H. Qin; W. Xie; Y. Li; K. Jiang; J. Lei; Q. Du","State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; The Department of Electrical and Computer Engineering, Mississippi State University, USA","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4428","4431","In this paper, a proposal-weighted two-stage generative adversarial network (GAN) with attention mechanism is proposed for hyperspectral target detection (HTD). PTGAN leverages GAN to estimate spectral background distribution and realize mapping from the latent space to the spectral space. Meanwhile, PTGAN conducts the reversed mapping through latent-spectral-latent and spectral-latent-spectral learning. On this basis, PTGAN implements accurate reconstruction of background spectrum via latent space. Therefore, targets of interest can be detected through larger pixel-level reconstruction error. In particular, the variance attention module is designed to make full use of global information among spectral bands to selectively emphasize channel-wise spectral features. Furthermore, a proposal-weighted strategy in a two-stage manner reduces the false alarm of detection by refining the previous detection proposal. Finally, exponential nonlinear fusion combines the discriminative feature from two stages to suppress the background. Extensive experiments on two real hyperspectral images (HSIs) verify the effectiveness of PTGAN.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553721","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); China Postdoctoral Science Foundation(grant numbers:2019T120878,2017M620440); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553721","Hyperspectral target detection;generative adversarial network;background reconstruction;two-stage","Refining;Geoscience and remote sensing;Object detection;Generative adversarial networks;Feature extraction;Proposals;Data mining","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;learning (artificial intelligence);neural nets;object detection","reversed mapping;spectral-latent-spectral learning;latent space;pixel-level reconstruction error;variance attention module;spectral bands;channel-wise spectral features;detection proposal;hyperspectral images;proposal-weighted two-stage GAN;hyperspectral target detection;attention mechanism;PTGAN;spectral background distribution;spectral space;proposal-weighted two-stage generative adversarial network;HTD;latent-spectral-latent learning;background spectrum reconstruction;exponential nonlinear fusion;discriminative feature;HSIs","","2","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Few-Shot Hyperspectral Image Classification Based on Domain Adaptation of Class Balance","Q. Zhen; X. Zhang; Z. Li; B. Hou; X. Tang; L. Gao; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, Shaanxi Province, China; State Key Laboratory of Geo-Information Engineering, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, Shaanxi Province, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2255","2258","Hyperspectral image (HSI) classification has attracted ever-rising attention to better performance based on limited labeled data. In this paper, a domain adaptation method of class balance based on few-shot learning is proposed, which obtains the classification results of target HSI by training the dataset in the source domain containing sufficient labeled data. We use a random weighted sampling strategy in the source domain and the generative adversarial network (GAN) in the target domain to reduce the label distribution shift caused by unbalanced classes. Then, the conditional maximum mean discrepancy (CMMD) is presented for a more comprehensive domain alignment by considering the posterior data distribution. In addition, the double cross non-local block and multi-scale strategy are adopted in the feature extraction stage to get a refined classification result. Experimental results on public HSI datasets demonstrate that our method is efficient and outperforms other baselines.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883302","Hyperspectral image classification;domain adaptation;label distribution shift;few-shot learning","Training;Head;Geoscience and remote sensing;Tail;Generative adversarial networks;Feature extraction;Hyperspectral imaging","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);pattern classification","ever-rising attention;domain adaptation method;class balance;few-shot learning;target HSI;source domain;sufficient labeled data;random weighted sampling strategy;generative adversarial network;target domain;label distribution shift;unbalanced classes;conditional maximum mean discrepancy;comprehensive domain alignment;posterior data distribution;double cross nonlocal block;multiscale strategy;refined classification result;public HSI datasets;shot hyperspectral image classification","","1","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Subsurface Voids Detection from Limited Ground Penetrating Radar Data Using Generative Adversarial Network and YOLOV5","G. Chen; X. Bai; G. Wang; L. Wang; X. Luo; M. Ji; P. Feng; Y. Zhang","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, Heilongjiang, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","8600","8603","Recently, preventing road collapse caused by subsurface voids became an urgent problem that needs to be solved in the urban road safety area. While the most study in the field of subsurface object detection by deep learning method has only focused on the objects that can be acquired GPR B-scan data easily. This paper aims to realize the subsurface voids detection under the condition that lacks verified GPR B-Scan data. In this paper, a GPR B-Scan image augmentation method by SinGAN is proposed and the YOLOv5 object detection algorithm is applied correspondingly to detect subsurface voids from both real collected and generated GPR B-Scan data. The detection results show that the proposed technique realized subsurface voids detection in limited verified GPR B-scan data samples and become an inspiration for similar tasks that lacking training samples.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554954","Natural Science Foundation of China(grant numbers:62071147); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554954","Ground Penetrating Radar (GPR);object detection;image augumentation;deep learning;Yolov5;Generative Adversarial Nets (GANs)","Training;Deep learning;Ground penetrating radar;Geoscience and remote sensing;Object detection;Generative adversarial networks;Road safety","ground penetrating radar;neural nets;object detection;radar computing;radar imaging;road safety;voids (solid)","limited ground penetrating radar data;generative adversarial network;urban road safety area;subsurface object detection;deep learning method;GPR B-Scan image augmentation method;YOLOv5;subsurface voids detection;verified GPR B-scan data samples;GPR B-Scan data;SinGAN","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Learning Transformations between Heterogeneous SAR and Optical Images for Change Detection","Z. Chen; J. Liu; F. Liu; W. Zhang; L. Xiao; J. Shi","Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Northwestern Polytechnical University, Xian, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","3243","3246","Change detection based on heterogeneous images is challenging because of the distribution variance caused by imaging properties of different types of sensor. Most methods deal with this problem by transforming features into a common space. However, the lack of available labeled data limits the training of complex models and representation of heterogeneous distributions. In this paper, we propose to train a network via abundant unlabeled data by adopting cyclic adversarial pre-training in order to learn the relationship between heterogeneous distributions. After pre-training, for change detection, we introduce a constraint to maintain consistency of image content, to avoid the participation of changed pixels in training. Experiments on heterogeneous optical and SAR images prove the effectiveness of our proposed method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884752","Open Research Fund in 2021 of Jiangsu Key Laboratory of Spectral Imaging & Intelligent Sense(grant numbers:JSGP202101); Fundamental Research Funds for the Central Universities(grant numbers:JSGP202204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884752","Change detection;heterogeneous images;cycle-consistent adversarial networks (Cycle GAN);optical images;synthetic aperture radar (SAR)","Training;Knowledge engineering;Radar remote sensing;Radar detection;Radar imaging;Optical imaging;Generative adversarial networks","geophysical image processing;image classification;image recognition;learning (artificial intelligence);radar imaging;synthetic aperture radar","distribution variance;imaging properties;common space;available labeled data;heterogeneous distributions;abundant unlabeled data;cyclic adversarial pre-training;change detection;image content;changed pixels;heterogeneous optical images;SAR images;heterogeneous SAR;heterogeneous images","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A New Quantitative Evaluation Strategy for Deep Generated Sar Images","Z. Yu; G. Dong","National Lab of Rader Signal Processing, Xidian University; National Lab of Rader Signal Processing, Xidian University","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2626","2629","Synthetic Aperture Radar Automatic Target Recognition (SAR-ATR) is the core application of SAR technology. The shortage of training data is a constraint for SAR-ATR. One of the valid methods to solve the problem is SAR image simulation. A large quantity of generative models have achieved impressive performances on SAR image simulation. Therefore, it is absolutely necessary to evaluate whether the simulated images fulfill the requirement of application. This challenging problem has not attracted plenty of attention. Very few studies have been done. To fill the blank, we propose a new evaluation strategy. Two quantitative measurements are proposed to evaluate the simulated images from the local and the global perspective respectively. The local measurement, Fréchet Inception Distance score (FID) is used to measure the distance of feature vector between the real images and the simulated images. Contrarily, the global measurement, Hybrid Recognition Rate curve (HRR) is developed to assess the application capability of the simulated images from a global perspective. Multiple comparative experiments are performed on real SAR dataset. The experimental results demonstrate the effectiveness of the proposed method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884006","image quality evaluation;SAR;simulated images;deep generative network","Image quality;Image recognition;Target recognition;Fitting;Training data;Geoscience and remote sensing;Generative adversarial networks","radar imaging;radar target recognition;synthetic aperture radar","simulated images;SAR dataset;new quantitative evaluation strategy;deep generated sar images;Synthetic Aperture Radar Automatic Target Recognition;SAR-ATR;SAR technology;SAR image simulation;quantitative measurements","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Anomaly Detection in Aerial Images VIA Semi-Supervised Adversarial Training","C. -C. Yu; P. -H. Wang; H. -Y. Cheng","Department of Information and Computer Engineering, Chung Yuan Christian University; Department of Information and Computer Engineering, Chung Yuan Christian University; Department of Computer Science and Information Engineering, National Central University","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5035","5038","Most of the deep neural networks require a large amount of data for training to achieve good results. However, in practical anomaly detection applications, we usually only have few labeled anomalous samples that have various types. This study proposed an innovative hybrid architecture that aims to detect anomalies with a small number of labeled anomalous samples. The proposed method consists of two stages. First, the network learns the distribution of normal data with a generative adversarial network (GAN). Second, the discriminator of the network is combined with a classifier and the training process updates different network components depending on whether the labeled samples are normal or anomalies. From the experiments on CIFAR-10 and UC Merced datasets, we demonstrate that our method yields significant performance improvements then another GAN-based approach even when few labeled samples are provided.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884118","anomaly detection;semi-supervised learning;generative adversarial network","Training;Deep learning;Neural networks;Geoscience and remote sensing;Network architecture;Benchmark testing;Generative adversarial networks","deep learning (artificial intelligence);image classification;semi-supervised learning (artificial intelligence)","UC Merced dataset;CIFAR-10 dataset;semisupervised adversarial training;training process updates;generative adversarial network;normal data;hybrid architecture;labeled anomalous samples;anomaly detection applications;deep neural networks;aerial images;GAN;labeled samples;network components","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Conditional Generation of Cloud Fields","N. Mahfouz; Y. Ming; K. Smith","Division of Atmospheric Physics, Princeton University Atmospheric and Oceanic Sciences NOAA Geophys, Fluid Dynamics Lab, Princeton, NJ, USA; Division of Atmospheric Physics, Princeton University Atmospheric and Oceanic Sciences NOAA Geophys, Fluid Dynamics Lab, Princeton, NJ, USA; NVIDIA Corporation Higher Education Research, USA","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6741","6744","Processes related to cloud physics constitute the largest remaining scientific uncertainty in climate models and projections. This uncertainty stems from the coarse nature of current climate models and relatedly the lack of understanding of detailed physics. We train a generative adversarial network to generate realistic cloud fields conditioned on meterological reanalysis data for both climate model outputs as well as satellite imagery. While our network is able to generate realistic cloud fields, especially their large-scale patterns, more work is needed to refine its accuracy to resolve finer textural details of cloud masses to improve its predictions.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883181","clouds;satellite imagery;generative mod-eling;climate;weather","Uncertainty;Satellites;Image resolution;Clouds;Geoscience and remote sensing;Generative adversarial networks;Data models","climatology;clouds","realistic cloud fields;meterological reanalysis data;climate model;cloud masses;cloud physics;coarse nature;current climate models;generative adversarial network","","","","26","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A Temporal-Spectral Generative Adversarial Fusion Network for Improving Satellite Hyperspectral Temporal Resolution","K. Ren; W. Sun; J. Zhou; X. Meng; G. Yang; J. Peng","Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Mathematics and Statistics, Hubei University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","899","902","The improvement of temporal resolution of hyperspectral (HS) data is a fundamental and challenging problem. In this paper, we propose a Temporal-Spectral fusion method based on Generative Adversarial Network (TSF-GAN). First, the generator is used to train the nonlinear relationship between multispectral (MS) and HS data pairs at time T1 and T3, and we map the relationship to the MS data at T2 to obtain the HS data. Second, the discriminator is used to identify whether the differential image of HS data at different times is consistent with that of MS data, and whether the HS data at time T2 after spectral down-sampling is consistent with that of MS data at time T2. Preliminary experimental results demonstrate that the proposed TSF-GAN achieves comparative fidelity and has strong practicability.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884778","National Natural Science Foundation of China(grant numbers:42122009,42171351,41971296,61871177,42171326,41801256,41801252); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884778","Temporal-Spectral fusion;TSF-GAN;hyperspectral;multispectral","Satellites;Image resolution;Geoscience and remote sensing;Feature extraction;Generative adversarial networks;Generators;Hyperspectral imaging","hyperspectral imaging;image fusion;image resolution;neural nets","TSF-GAN;data pairs;MS data;HS data;spectral down-sampling;hyperspectral data;temporal-spectral generative adversarial fusion network;satellite hyperspectral temporal resolution;differential image","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Polsar Image Classification Via Auxiliary Classifier Generative Adversarial Network","W. Xie; X. Yang; R. Wang; F. Zhao","School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, China; School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, China; School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, China; School of Communications and Information Engineering, Xi'an University of Posts and Telecommunications, Xi'an, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1205","1208","When we use deep learning to classify PolSAR images, the lack of labeled samples will affect the classification performance. This paper uses the ACGAN model to expand the training samples of PolSAR data, utilizing the generated samples and original samples together to train CNN for PolSAR image classification. In addition to random noise, the ACGAN model also inputs additional relevant guidance information to ensure that the new generated samples are more similar to the original data. Compared with GAN, the discriminator of ACGAN can distinguish not only whether the data is real or not, but also the class label of the data. Subsequently, the validity of our proposed method is verified on the San Francisco data set. Compared with other classical PolSAR classification methods, the accuracy of our proposed method is improved.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884179","National Natural Science Foundation of China(grant numbers:61901365,62071379); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884179","PolSAR image classification;ACGAN;CNN","Deep learning;Training;Geoscience and remote sensing;Generative adversarial networks;Data models;Image classification","geophysical image processing;image classification;learning (artificial intelligence);radar imaging;radar polarimetry;synthetic aperture radar","polsar image classification;auxiliary classifier generative adversarial network;deep learning;PolSAR images;labeled samples;classification performance;ACGAN model;training samples;PolSAR data;original samples;additional relevant guidance information;San Francisco data set;classical PolSAR classification methods","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Building Facade Completion Using Semantic-Synchronized GAN","Z. Cai; Y. Lin; J. Li; Z. Zhang; X. Huang",Jimei University; Jimei University; Xiamen University; Jimei University; Jimei University,"2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","6387","6390","Due to the difference in high-rise distance, laser incident angle, and occlusion, incomplete point cloud of building facades is incomplete. The existing point cloud facade completion work does not fully explore the semantic constraints. Thus, it is difficult to deal with the facades with large-area missing. This paper proposes a novel facade completion framework for point clouds. By introducing semantic constraints and semantic-synchronized GAN, we solve the facade completion problem with large-area missing while the semantic consistency is augmented.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554453","Facade completion;Point cloud;GAN","Semantics;Buildings;Lasers;Geoscience and remote sensing;Generative adversarial networks","buildings (structures);image reconstruction;image segmentation","semantic constraints;large-area missing;novel facade completion framework;point clouds;semantic-synchronized GAN;facade completion problem;semantic consistency;building facade completion;high-rise distance;laser incident angle;incomplete point cloud;building facades;existing point cloud facade completion work","","","","23","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multi-Category SAR Images Generation Based on Improved Generative Adversarial Network","S. Du; J. Hong; Y. Wang; K. Xing; T. Qiu","National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4260","4263","The generative adversarial network (GAN) provides a different way for SAR data augmentation. The traditional GAN model is mainly based on the Jensen-Shannon (JS) divergence or Wasserstein distance. The former faces mode collapse, while the latter is not suitable for multi-category image generation. In this paper, an improved model based on WGAN-GP is proposed. An encoder is used to learn the features of real samples as the input of the generator to control training to a certain extent and make the generated image quality better. In addition, a pre-trained classifier is introduced as the constraint of the generator to ensure the generated images have the correct category information. MSTAR dataset is used to verify the generation capability of the proposed model. The results show that the proposed model has the stable generation capability to provide high-quality SAR images as a supplementary training dataset, which could assist in achieving good classification accuracy.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554120","P-band;calibration;BIOMASS;parabolic antenna;target of opportunity","Training;Image quality;Image synthesis;Geoscience and remote sensing;Generative adversarial networks;Generators;Radar polarimetry","image classification;learning (artificial intelligence);maximum likelihood estimation;pattern classification;radar imaging;synthetic aperture radar","SAR data augmentation;traditional GAN model;faces mode collapse;multicategory image generation;generated image quality;pre-trained classifier;correct category information;stable generation capability;high-quality SAR images;supplementary training dataset;multicategory SAR images generation;improved generative adversarial network","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Cross-Scene Hyperspectral Image Classification Based on Cycle-Consistent Adversarial Networks","Z. Meng; M. Ye; F. Yao; F. Xiong; Y. Qian","Key Laboratory of Electromagnetic Wave Information Technology and Metrology of Zhejiang Province, College of Information Engineering, China Jiliang University, China; Key Laboratory of Electromagnetic Wave Information Technology and Metrology of Zhejiang Province, College of Information Engineering, China Jiliang University, China; Key Laboratory of Electromagnetic Wave Information Technology and Metrology of Zhejiang Province, College of Information Engineering, China Jiliang University, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, China; College of Computer Science, Zhejiang University, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1912","1915","Lack of labeled training samples is a challenge in hyperspectral image (HSI) classification. Cross-scene classification is a valid solution to few-shot learning problem. In cross-scene classification, two strongly related HSI scenes are considered, one with sufficient labeled samples is called source scene, while the other one containing limited labeled samples is called target scene. By establishing connections between two scenes, abundant labeled samples in source scene can benefit the classification of target scene. In this paper, a novel model named cycle auxiliary classifier generative adversarial network (Cycle-AC-GAN) is proposed for heterogeneous transfer learning across source and target scenes. In Cycle-AC-GAN, a source-to-target generator and a target-to-source generator are simultaneously built. Thus, a two-way mapping can be effectively established between source and target scenes with the adversarial training. In addition, different from existing CycleGAN, in Cycle-AC-GAN, each discriminator contains a binary domain classifier and an auxiliary land-cover classifier. The auxiliary classifiers can align the class-conditional distributions between source and target HSIs. Inspiring experimental results on two real-world cross-scene HSI datasets demonstrate the effectiveness of the proposed approach.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883513","National Natural Science Foundation of China(grant numbers:62071421,62002169); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883513","Hyperspectral image classification;heterogeneous transfer learning;cycle-consistent adversarial networks","Training;Transfer learning;Geoscience and remote sensing;Generative adversarial networks;Generators;Hyperspectral imaging;Image classification","geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;learning (artificial intelligence);pattern classification","cross-scene hyperspectral image classification;Cycle-consistent adversarial networks;labeled training samples;cross-scene classification;few-shot learning problem;strongly related HSI scenes;sufficient labeled samples;source scene;target scene;abundant labeled samples;cycle auxiliary classifier generative adversarial network;Cycle-AC-GAN;target scenes;source-to-target generator;target-to-source generator;adversarial training;real-world cross-scene HSI datasets","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Siamese Generative Adversarial Network for Change Detection Under Different Scales","M. Liu; Q. Shi; P. Liu; C. Wan","Guangdong Provincial Key Laboratory for Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory for Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory for Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory for Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2543","2546","Change detection methods based on low-resolution (LR) images with higher temporal resolution often lead to fuzzy results, while high-resolution images (HRIs) can provide more detailed information to solve this problem. However, it's hard to obtain two tiles of HRIs with high-quality for rapid change detection in actual production due to low temporal resolution and high cost. Therefore, it is necessary to explore a change detection method combing low- and high-resolution images to acquire urban change areas more accurately and quickly. In this paper, an end-to-end siamese generative adversarial network (SiamGAN) integrating a super resolution network and the siamese structure was proposed for change detection under different scales. The super-resolution network is used to reconstruct low-resolution images into high-resolution images, while the siamese structure is adopted as the classification network to detect changes. In the experiments, SiamGAN achieved an F1 of 76.06% and an IoU of 61.52% in the test set, which is respectively 5.68% and 6.92% higher than the CNN-based methods using LR images after bicubic interpolation. The results show that our proposed method can effectively overcome difference in scale between low- and high-resolution images and perform change detection more precisely and rapidly.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323499","Change detection;siamese network;super-resolution;high resolution images","Generators;Feature extraction;Training;Superresolution;Generative adversarial networks;Spatial resolution;Remote sensing","geophysical image processing;image reconstruction;image resolution;interpolation","change detection method;low-resolution images;higher temporal resolution;high-resolution images;rapid change detection;low temporal resolution;urban change areas;end-to-end siamese generative adversarial network;super resolution network;siamese structure;super-resolution network","","","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel-2 datasets.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747268","Image fusion;generative adversarial network;convolutional network;Sentinel-2","Image resolution;Image fusion;Generators;Gallium nitride;Training;Generative adversarial networks;Signal resolution","geophysical image processing;geophysical techniques;image fusion;image resolution;image sensors;remote sensing","deep convolutional Generative Adversarial Network;multispectral images;Sentinel-2 constellation;multispectral bands;electromagnetic spectrum;fine resolution bands;coarse spatial resolution bands;deep convolutional design;Sentinel-2 datasets;multispectral sensors;visible band;short-wave infrared band;single sensor image fusion methods;near-infrared band;wavelength 10.0 m;wavelength 60.0 m;wavelength 20.0 m","","7","","16","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic Images via GANs","F. Ozcelik; U. Alganci; E. Sertel; G. Unal","Artificial Intelligence and Data Science Research and Application Center, Istanbul Technical University (ITU), Istanbul, Turkey; Research Center for Satellite Communications and Remote Sensing (CSCRS), Istanbul Technical University (ITU), Istanbul, Turkey; Research Center for Satellite Communications and Remote Sensing (CSCRS), Istanbul Technical University (ITU), Istanbul, Turkey; Artificial Intelligence and Data Science Research and Application Center, Istanbul Technical University (ITU), Istanbul, Turkey","IEEE Transactions on Geoscience and Remote Sensing","24 Mar 2021","2021","59","4","3486","3501","Convolutional neural network (CNN)-based approaches have shown promising results in the pansharpening of the satellite images in recent years. However, they still exhibit limitations in producing high-quality pansharpening outputs. To that end, we propose a new self-supervised learning framework, where we treat pansharpening as a colorization problem, which brings an entirely novel perspective and solution to the problem compared with the existing methods that base their solution solely on producing a super-resolution version of the multispectral image. Whereas the CNN-based methods provide a reduced-resolution panchromatic image as the input to their model along with the reduced-resolution multispectral images and, hence, learn to increase their resolution together, we instead provide the grayscale transformed multispectral image as the input and train our model to learn the colorization of the grayscale input. We further address the fixed downscale ratio assumption during training, which does not generalize well to the full-resolution scenario. We introduce a noise injection into the training by randomly varying the downsampling ratios. Those two critical changes, along with the addition of adversarial training in the proposed PanColorization generative adversarial network (PanColorGAN) framework, help overcome the spatial-detail loss and blur problems that are observed in CNN-based pansharpening. The proposed approach outperforms the previous CNN-based and traditional methods, as demonstrated in our experiments.","1558-0644","","10.1109/TGRS.2020.3010441","Research Fund of the Istanbul Technical University Project(grant numbers:MGA-2017-40811); Turkcell-ITU Researcher Funding Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153037","AI;colorization;convolutional neural networks (CNNs);deep learning;generative adversarial networks (GANs);image fusion;PanColorization generative adversarial network (PanColorGAN);pansharpening;self-supervised learning;super-resolution (SR)","Task analysis;Spatial resolution;Training;Standards;Sensors;Multiresolution analysis","convolutional neural nets;geophysical image processing;image colour analysis;image resolution;supervised learning","adversarial training;grayscale transformed multispectral image;blur problems;PanColorGAN;panchromatic image resolution;PanColorization generative adversarial network;CNN based pansharpening;panchromatic image colorization;self supervised learning;satellite images;convolutional neural network;multispectral image resolution","","37","","44","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"A Semisupervised GAN-Based Multiple Change Detection Framework in Multi-Spectral Images","F. Jiang; M. Gong; T. Zhan; X. Fan","School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","23 Jun 2020","2020","17","7","1223","1227","Effectively highlighting multiple changes in the earth surface from multi-temporal remote sensing images is a meaningful but challenging task. In order to reduce costs and ensure the performance, it is advisable to employ a semisupervised strategy to achieve this goal. As a discriminative joint classification task, semisupervised change detection aims to extract useful and discriminative features from a large amount of unlabeled data in addition to limited labeled samples. The discriminator of a well-trained generative adversarial network (GAN) is just right for this. Therefore, in this letter, we proposed a semisupervised GAN-based multiple change detection framework for multi-spectral images. First, the GAN is trained by all data without any prior information. Then, we combine two identical trained discriminators to construct a dual-pipeline joint classifier. Finally, the classifier is fine-tuned by a very small amount of labeled data to detect multiple changes. The superior performance of the proposed model over both real multi-spectral data sets demonstrates its robustness and effectiveness.","1558-0571","","10.1109/LGRS.2019.2941318","National Natural Science Foundation of China(grant numbers:61772393); Key Research and Development Program of Shaanxi Province(grant numbers:2018ZDXM-GY-045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8854295","Generative adversarial network (GAN);multi-spectral images;multiple change detection;semisupervised","Feature extraction;Gallium nitride;Task analysis;Training;Remote sensing;Data mining;Generative adversarial networks","feature extraction;geophysical image processing;image classification;neural nets;object detection;remote sensing","multispectral images;multitemporal remote sensing images;discriminative joint classification task;semisupervised change detection;discriminative features;identical trained discriminators;multispectral data sets;semisupervised GAN-based multiple change detection","","14","","19","IEEE","1 Oct 2019","","","IEEE","IEEE Journals"
"Capsule Feature Pyramid Network for Building Footprint Extraction From High-Resolution Aerial Imagery","Y. Yu; Y. Ren; H. Guan; D. Li; C. Yu; S. Jin; L. Wang","Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; School of Energy and Power Engineering, Nanjing Institute of Technology, Nanjing, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China","IEEE Geoscience and Remote Sensing Letters","22 Apr 2021","2021","18","5","895","899","Building footprint extraction plays an important role in a wide range of applications. However, due to size and shape diversities, occlusions, and complex scenarios, it is still challenging to accurately extract building footprints from aerial images. This letter proposes a capsule feature pyramid network (CapFPN) for building footprint extraction from aerial images. Taking advantage of the properties of capsules and fusing different levels of capsule features, the CapFPN can extract high-resolution, intrinsic, and semantically strong features, which perform effectively in improving the pixel-wise building footprint extraction accuracy. With the use of signed distance maps as ground truths, the CapFPN can extract solid building regions free of tiny holes. Quantitative evaluations on an aerial image data set show that a precision, recall, intersection-over-union (IoU), and F-score of 0.928, 0.914, 0.853, and 0.921, respectively, are obtained. Comparative studies with six existing methods confirm the superior performance of the CapFPN in accurately extracting building footprints.","1558-0571","","10.1109/LGRS.2020.2986380","Natural Science Foundation of Jiangsu Province(grant numbers:BK20160427,BK20191214); National Natural Science Foundation of China(grant numbers:61603146,51975239,41971414,41671454); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075171","Aerial imagery;building extraction;building footprint;capsule feature pyramid network (CapFPN);capsule network","Feature extraction;Buildings;Remote sensing;Training;Shape;Generative adversarial networks;Gallium nitride","building information modelling;cartography;feature extraction;geophysical image processing;image classification;image fusion;image resolution","IoU;intersection-over-union;quantitative evaluations;ground truths;signed distance maps;semantically strong feature extraction;intrinsic feature extraction;high-resolution feature extraction;occlusions;shape diversities;aerial image data set;solid building region extraction;pixel-wise building footprint extraction accuracy;CapFPN;high-resolution aerial imagery;capsule feature pyramid network","","18","","28","IEEE","21 Apr 2020","","","IEEE","IEEE Journals"
"SDFL-FC: Semisupervised Deep Feature Learning With Feature Consistency for Hyperspectral Image Classification","Y. Cao; Y. Wang; J. Peng; C. Qiu; L. Ding; X. X. Zhu","School of Land Science and Technology, China University of Geosciences, Beijing, China; State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; School of Land Science and Technology, China University of Geosciences, Beijing, China; Data Science in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Weßling, Germany","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2021","2021","59","12","10488","10502","Semisupervised deep learning methods (DLMs) can mitigate the dependence on large amounts of labeled samples using a small number of labeled samples. However, for semisupervised deep feature learning (SDFL), the quality of extracted features cannot be well ensured without a certain amount of labeled samples. To address this issue, we develop the SDFL method with feature consistency (SDFL-FC) for the hyperspectral image (HSI) classification. The SDFL-FC first adopts the convolutional neural network (CNN) to extract spectral–spatial features of HSI and then uses the fully connected layers (FCLs) to model the feature consistency. Moreover, two constraints that enforce both the feature consistency of single pixel (FCS) and feature consistency of group pixels (FCG) are introduced to obtain the representative and discriminative features. The FCS is achieved by the generative adversarial network (GAN) regularization, which can reconstruct the original data from extracted features. The FCG is based on the assumption that the features of group pixels should have similar characteristics within a superpixel, which is embedded in each FCL. The final FCL outputs the class labels, and the cross-entropy (CE) loss is calculated with the labeled samples, while the two losses of FCS and FCG are calculated with all the training samples (both labeled and unlabeled). SDFL-FC integrates the FCS, FCG, and CE loss into a unified objective function and uses a customized iterative optimization algorithm to optimize it. Experiments demonstrate that the SDFL-FC can outperform the related state-of-the-art HSI classification methods.","1558-0644","","10.1109/TGRS.2020.3044094","National Natural Science Foundation of China(grant numbers:41801241,41711411); Fundamental Research Funds for the Central Universities(grant numbers:292018029,375201906); Key Research and Development Projects of Shanxi Province(grant numbers:201903D121142); Open Fund of the State Key Laboratory of Remote Sensing Science(grant numbers:OFSLRSS201923); Guizhou Science and Technology Plan Project(grant numbers:Qiankehezhicheng[2020] 4Y022); European Research Council (ERC) through the European Union’s Horizon 2020 Research and Innovation Program (Acronym: So2Sat)(grant numbers:ERC-2016-StG-714087); German Federal Ministry of Education and Research (BMBF) in the framework of the International Future AI Lab “AI4EO”(grant numbers:01DD20001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9307259","Convolutional neural network (CNN);feature consistency;fully connected network;hyperspectral image (HSI) classification;optimization","Feature extraction;Gallium nitride;Image reconstruction;Generative adversarial networks;Training;Optimization;Linear programming","convolutional neural nets;entropy;feature extraction;hyperspectral imaging;image classification;image reconstruction;iterative methods;optimisation;supervised learning","discriminative features;SDFL-FC;semisupervised deep feature learning;semisupervised deep learning methods;SDFL method;hyperspectral image classification;spectral-spatial features;fully connected layers;FCL;feature consistency of single pixel;FCS;feature consistency of group pixels;FCG;enerative adversarial network;GAN regularization;convolutional neural network;CNN;HSI classification;cross-entropy loss;CE loss","","4","","60","IEEE","24 Dec 2020","","","IEEE","IEEE Journals"
"Oil Spill Detection from SAR Images by Deep Learning","F. Ronci; C. Avolio; M. di Donna; M. Zavagli; V. Piccialli; M. Costantini","e-GEOS, an Italian Space Agency and Telespazio Company, Rome, Italy; e-GEOS, an Italian Space Agency and Telespazio Company, Rome, Italy; e-GEOS, an Italian Space Agency and Telespazio Company, Rome, Italy; e-GEOS, an Italian Space Agency and Telespazio Company, Rome, Italy; University of Rome Tor Vergata, Italy; e-GEOS, an Italian Space Agency and Telespazio Company, Rome, Italy","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2225","2228","Oil spills, caused by accidents or by ships cleaning their tanks, represent big threats for maritime and coastal ecosystems health. A very effective detection of oil spills can be performed using satellite synthetic aperture radar (SAR) systems, operating regardless of cloud coverage and sunlight and capable of discriminating oil from regular sea surface. However, discriminating between real oil spills and lookalikes (such as natural oils and seepages, often occurring in upwelling sea areas), although well performed by expert SAR image interpreters, poses a great challenge for automatic processes. In addition, a visual check performed by human operators on a great number of images would be too expensive. Therefore, many solutions for automatic detection have been tried in the last few years, using probabilistic models and, more recently, machine learning. This work presents an innovative solution based on image-to-image translation using convolutional neural networks (CNNs) trained with an adversarial loss function. The proposed approach has been tested, with very promising results, using Radarsat-2 and Sentinel-1 SAR data over the Mediterranean Sea and some areas of the Atlantic Ocean and the North Sea.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323590","SAR;Oil Spill;Deep Learning;Semantic Segmentation;U-Net;GAN","Oils;Synthetic aperture radar;Indexes;Image segmentation;Radar polarimetry;Gallium nitride;Generative adversarial networks","geophysical signal processing;learning (artificial intelligence);marine pollution;neural nets;oceanographic techniques;oil pollution;radar imaging;remote sensing by radar;ships;synthetic aperture radar","seepages;expert SAR image interpreters;automatic detection;image-to-image translation;Sentinel-1 SAR data;oil spill detection;SAR images;maritime;coastal ecosystems health;satellite synthetic aperture radar systems;regular sea surface;natural oils","","2","","9","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"An Unsupervised Domain Adaptation Method Towards Multi-Level Features and Decision Boundaries for Cross-Scene Hyperspectral Image Classification","C. Zhao; B. Qin; S. Feng; W. Zhu; L. Zhang; J. Ren","College of Information and Communication Engineering and the Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; College of Information and Communication Engineering and the Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; State Key Laboratory of Remote Sensing Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; College of Information and Communication Engineering and the Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; State Key Laboratory of Remote Sensing Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; National Subsea Centre, Robert Gordon University, Aberdeen, U.K.","IEEE Transactions on Geoscience and Remote Sensing","30 Dec 2022","2022","60","","1","16","Despite success in the same-scene hyperspectral image classification (HSIC), for the cross-scene classification, samples between source and target scenes are not drawn from the independent and identical distribution, resulting in significant performance degradation. To tackle this issue, a novel unsupervised domain adaptation (UDA) framework toward multilevel features and decision boundaries (ToMF-B) is proposed for the cross-scene HSIC, which can align task-related features and learn task-specific decision boundaries in parallel. Based on the maximum classifier discrepancy, a two-stage alignment scheme is proposed to bridge the interdomain gap and generate discriminative decision boundaries. In addition, to fully learn task-related and domain-confusing features, a convolutional neural network (CNN) and Transformer-based multilevel features extractor (generator) is developed to enrich the feature representation of two domains. Furthermore, to alleviate the harm even the negative transfer to UDA caused by task-irrelevant features, a task-oriented feature decomposition method is leveraged to enhance the task-related features while suppressing task-irrelevant features, and enabling the aligned domain-invariant features can be contributed to the classification task explicitly. Extensive experiments on three cross-scene HSI benchmarks have validated the effectiveness of the proposed framework.","1558-0644","","10.1109/TGRS.2022.3230378","National Natural Science Foundation of China(grant numbers:62002083,61971153); Open Fund of State Key Laboratory of Remote Sensing Science(grant numbers:OFSLRSS202210); Heilongjiang Provincial Natural Science Foundation(grant numbers:LH2021F012); Fundamental Research Funds for the Central Universities(grant numbers:3072021CFT0801,3072022QBZ0805,3072022CF0808); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991175","Cross-scene classification;hyperspectral image (HSI);task irrelevant;task specific;unsupervised domain adaptation (UDA)","Task analysis;Feature extraction;Transformers;Generators;Hyperspectral imaging;Bridges;Generative adversarial networks","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image representation;unsupervised learning","aligned domain-invariant features;classification task;cross-scene HSI benchmarks;cross-scene HSIC;cross-scene hyperspectral image classification;discriminative decision boundaries;domain-confusing features;feature representation;identical distribution;independent distribution;maximum classifier discrepancy;multilevel features;novel unsupervised domain adaptation framework;same-scene hyperspectral image classification;target scenes;task-irrelevant features;task-oriented feature decomposition method;task-related features;task-specific decision boundaries;two-stage alignment scheme;UDA;unsupervised domain adaptation method","","17","","65","IEEE","16 Dec 2022","","","IEEE","IEEE Journals"
"PLFM: Pixel-Level Merging of Intermediate Feature Maps by Disentangling and Fusing Spatial and Temporal Data for Cloud Removal","A. Sebastianelli; E. Puglisi; M. P. Del Rosso; J. Mifdal; A. Nowakowski; P. P. Mathieu; F. Pirri; S. L. Ullo","Department of Engineering, University of Sannio, Benevento, Italy; Department of Automation Robotics and Control for Aerospace, La Sapienza University, Rome, Italy; Department of Engineering, University of Sannio, Benevento, Italy; Φ-Lab, European Space Agency, Frascati, Italy; Department of Mathematics and Computer Science, Warsaw University of Technology, Warsaw, Poland; Φ-Lab, European Space Agency, Frascati, Italy; Department of Automation Robotics and Control for Aerospace, La Sapienza University, Rome, Italy; Department of Engineering, University of Sannio, Benevento, Italy","IEEE Transactions on Geoscience and Remote Sensing","13 Oct 2022","2022","60","","1","16","Cloud removal is a relevant topic in remote sensing, fostering medium- and high-resolution optical (OPT) image usability for Earth monitoring and study. Recent applications of deep generative models and sequence-to-sequence-based models have proved their capability to advance the field significantly. Nevertheless, there are still some gaps: the amount of cloud coverage, the landscape temporal changes, and the density and thickness of clouds need further investigation. We fill some of these gaps in this work by introducing an innovative deep model. The proposed model is multimodal, relying on both spatial and temporal sources of information to restore the whole optical scene of interest. We use the outcomes of both temporal-sequence blending and direct translation from synthetic aperture radar (SAR) to optical images to obtain a pixel-wise restoration of the whole scene. The reconstructed images preserve scene details without resorting to a considerable portion of a clean image. Our approach’s advantage is demonstrated across various atmospheric conditions tested on different datasets. Quantitative and qualitative results prove that the proposed method obtains cloud-free images coping with landscape changes.","1558-0644","","10.1109/TGRS.2022.3208694","ongoing Open Space Innovation Platform (OSIP) Project “Al Powered Cross-Modal Adaptation Techniques Applied to Sentinel-1 and Sentinel-2 Data,” started in June 2020 under a joint collaboration between the Φ-Lab, European Space Agency, and the University of Sannio, Benevento, Italy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9899477","Cloud removal (CR);conditional generative adversarial networks (cGANs);convolutional long short-term memory (ConvLSTM);deep hierarchical model;multitemporal remote sensing (RS) images;synthetic aperture radar (SAR)-optical (OPT) data fusion","Clouds;Optical imaging;Optical sensors;Image reconstruction;Radar polarimetry;Adaptation models;Image restoration","geophysical image processing;image reconstruction;image resolution;optical images;radar imaging;remote sensing;synthetic aperture radar","PLFM;pixel-level merging;intermediate feature maps;fusing spatial;temporal data;cloud removal;relevant topic;remote sensing;high-resolution optical image usability;deep generative models;sequence-to-sequence-based models;cloud coverage;landscape temporal changes;density;innovative deep model;optical scene;temporal-sequence blending;optical images;pixel-wise restoration;reconstructed images;scene details;clean image;method obtains cloud-free images;landscape changes","","3","","90","IEEE","22 Sep 2022","","","IEEE","IEEE Journals"
"SSCV-GANs: Semi-Supervised Complex-Valued GANs for PolSAR Image Classification","X. Li; Q. Sun; L. Li; X. Liu; H. Liu; L. Jiao; F. Liu","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Joint International Research Laboratory of Intelligent Perception and Computation, International Research Center for Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Access","17 Aug 2020","2020","8","","146560","146576","Polarimetric synthetic aperture radar (PolSAR) image classification has been widely applied in many fields, such as agriculture, meteorology and military. However, some problems, such as the deficiency of labeled data and the underutilization of data information, are always the challenges that can not be ignored in PolSAR image classification. In this paper, a semi-supervised complex-valued generative adversarial networks (SSCV-GANs) is proposed for the first time to address the two issues mentioned above simultaneously. On the one hand, the complex-valued model conforms with the physical mechanism of PolSAR data and it plays an important role for retaining and utilizing amplitude and phase information of PolSAR data. On the other hand, we also present a new complex-valued GANs together with semi-supervised learning to alleviate the problem of insufficient labeled data. Specifically, our complex-valued GANs expands the training data set by generating fake data. Flevoland data and San Francisco data are used to validate the effectiveness of our model. Experimental results show that our model outperforms existing state-of-the-art models in terms of classification accuracy, especially for conditions with fewer labeled data. In particular, the analysis of the statistical distribution of the generated fake data and the real data further demonstrate the effectiveness of the proposed SSCV-GANs.","2169-3536","","10.1109/ACCESS.2020.3004591","State Key Program of National Natural Science of China(grant numbers:61836009); National Natural Science Foundation of China(grant numbers:U1701267,61871310,61977052,61502369); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201); Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT_15R53); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9123901","Polarimetric synthetic aperture;image classification;complex-valued operations;generative adversarial networks (GANs);semi-supervised learning","Data models;Biological neural networks;Gallium nitride;Generative adversarial networks;Semisupervised learning;Recurrent neural networks","image classification;neural nets;radar imaging;radar polarimetry;statistical distributions;synthetic aperture radar","complex-valued model;PolSAR data;semisupervised learning;insufficient labeled data;training data;San Francisco data;classification accuracy;fewer labeled data;generated fake data;PolSAR image classification;polarimetric synthetic aperture radar image classification;data information;complex-valued GAN;SSCV-GAN;semi-supervised complex-valued generative adversarial networks;amplitude information;phase information;Flevoland data;statistical distribution","","4","","66","CCBY","24 Jun 2020","","","IEEE","IEEE Journals"
"SAR-to-Optical Image Translating Through Generate-Validate Adversarial Networks","H. Shi; B. Zhang; Y. Wang; Z. Cui; L. Chen","Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China; Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China; Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China; Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China; Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China","IEEE Geoscience and Remote Sensing Letters","2 May 2022","2022","19","","1","5","Synthetic aperture radar (SAR) has the advantages of high resolution in all-weather and all-day. However, SAR images are hard to be understood, due to their unique imaging mechanism. The SAR to optical image translation can assist in interpreting and has become a topic of growing interest in the field of remote sensing. In this letter, a SAR to optical image translation network is proposed, called generate-validate adversarial networks (GVANs). More specifically, there are two Pix2Pix networks form the cyclic structure. The validate module is employed to increase the training process and improve the edge retention ability. In order to improve multidomain images adaptability, the embedded layer is proposed. Additionally, the dilation convolution layer is employed in the generator, which is more suitable for the characteristics of SAR images. The proposed method has experimented on the SEN1-2 dataset. The result demonstrates the superiority of the proposed method over state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3168391","National Natural Science Foundation of China(grant numbers:6210010891); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9759326","Generative adversarial networks (GVANs);synthetic aperture radar (SAR);SAR-to-optical image translating;U-Net","Generators;Optical imaging;Radar polarimetry;Convolution;Synthetic aperture radar;Optical sensors;Image edge detection","convolutional neural nets;image processing;image resolution;optical images;radar imaging;radar resolution;synthetic aperture radar","synthetic aperture radar;SAR images;optical image translation network;Pix2Pix networks;multidomain images adaptability;SAR-to-optical image translation;generate-validate adversarial networks;remote sensing;GVAN;cyclic structure;edge retention ability;dilation convolution layer","","","","13","IEEE","18 Apr 2022","","","IEEE","IEEE Journals"
"Self-Normalizing Generative Adversarial Network for Super-Resolution Reconstruction of SAR Images","C. Zheng; X. Jiang; Y. Zhang; X. Liu; B. Yuan; Z. Li","School of Electronic Information and Electrical Engineering; School of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering; School of Electronic Information and Electrical Engineering; Beijing Institute of Remote Sensing Information, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1911","1914","High-resolution images with abundant detailed information are necessary elements for various applications of synthetic aperture radar (SAR). In this paper, a novel super-resolution image reconstruction method based on self-normalizing generative adversarial network (SNGAN) is proposed. Compared with other published GAN-based super-resolution algorithms, the proposed method reflects its superiority in two aspects. First, the scaled exponential linear units (SeLU) is introduced as the activation function of generator to give the GAN system self-normalization ability and make it more suitable for SAR images. Second, the batch normalization layers after convolution are canceled to reduce the computational requirement and model oscillation. Experiment results on the images of TerraSAR and MSTAR dataset demonstrate that the proposed method acquires satisfactory performance on the resolution enhancement and target recognition of SAR images.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900084","Generative adversarial network (GAN);super-resolution image reconstruction;target recognition","Image reconstruction;Radar polarimetry;Gallium nitride;Generative adversarial networks;Target recognition","image reconstruction;image resolution;neural nets;radar imaging;synthetic aperture radar","self-normalizing generative adversarial network;super-resolution reconstruction;SAR images;high-resolution images;synthetic aperture radar;batch normalization layers;resolution enhancement;super-resolution image reconstruction method;activation function;TerraSAR dataset;MSTAR dataset;target recognition","","9","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Uncertainty, Edge, and Reverse-Attention Guided Generative Adversarial Network for Automatic Building Detection in Remotely Sensed Images","S. Chattopadhyay; A. C. Kak","Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA; Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 May 2022","2022","15","","3146","3167","Despite recent advances in deep-learning-based semantic segmentation, automatic building detection from remotely sensed imagery is still a challenging problem owing to large variability in the appearance of buildings across the globe. The errors occur mostly around the boundaries of the building footprints, in shadow areas, and when detecting buildings whose exterior surfaces have reflectivity properties that are very similar to those of the surrounding regions. To overcome these problems, we propose a generative adversarial network-based segmentation framework with uncertainty attention unit and refinement module embedded in the generator. The refinement module, composed of edge and reverse-attention units, is designed to refine the predicted building map. The edge attention enhances the boundary features to estimate building boundaries with greater precision, and the reverse attention allows the network to explore the features missing in the previously estimated regions. The uncertainty attention unit assists the network in resolving uncertainties in classification. As a measure of the power of our approach, as of December 4, 2021, it ranks at the second place on DeepGlobe’s public leaderboard despite the fact that main focus of our approach—refinement of the building edges—does not align exactly with the metrics used for leaderboard rankings. Our overall $F$1-score on DeepGlobe’s challenging dataset is 0.745. We also report improvements on the previous-best results for the challenging INRIA validation dataset for which our network achieves an overall IoU of 81.28% and an overall accuracy of 97.03%. Along the same lines, for the official INRIA test dataset, our network scores 77.86% and 96.41% in overall IoU and accuracy. We have also improved upon the previous best results on two other datasets: the WHU building dataset and the Massachusetts buildings dataset.","2151-1535","","10.1109/JSTARS.2022.3166929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756257","Attention;deep learning;generative adversarial networks (GANs);semantic segmentation","Buildings;Image segmentation;Measurement;Uncertainty;Training;Semantics;Generative adversarial networks","buildings (structures);deep learning (artificial intelligence);edge detection;image classification;image segmentation;object detection;remote sensing","reverse-attention units;predicted building map;edge attention;boundary features;building boundaries;reverse attention;estimated regions;uncertainty attention unit;DeepGlobe's challenging dataset;network scores;WHU building dataset;Massachusetts buildings dataset;automatic building detection;deep-learning-based semantic segmentation;remotely sensed imagery;building footprints;exterior surfaces;generative adversarial network-based segmentation framework;refinement module;INRIA validation dataset;leaderboard ranking","","1","","92","CCBY","12 Apr 2022","","","IEEE","IEEE Journals"
"Pan-Sharpening Based on Joint Visual Saliency Analysis and Parallel Bidirectional Network","W. Zhu; L. Zhang","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","12 Oct 2022","2022","19","","1","5","In remote sensing (RS) images, the demands for spectral and spatial quality of different regions are different, which means that the unified fusion strategy on the whole image is not suitable for pan-sharpening task. Saliency, derived from visual attention mechanism, provides an effective way to satisfy these demands. Inspired by this, we propose a novel pan-sharpening method based on joint visual saliency analysis and parallel bidirectional network (JSPBN). First, considering the complex scenes and uneven distribution of targets in RS images, we develop a Bayesian optimization-based joint visual saliency analysis (B-JVSA) method that integrates prior saliency based on global color contrast with likelihood saliency based on joint co-occurrence histogram, which can highlight common salient regions while suppressing individual ones and irrelevant background by exploring the correlation among multiple RS images. Second, we construct a parallel bidirectional feature pyramid (PBFP) network to obtain coarse fusion features, fully considering individual characteristics of panchromatic (PAN) images and multispectral (MS) images. Finally, we design a saliency-aware layer (SAL) according to B-JVSA to further refine the fusion effect in salient regions and nonsalient regions. With the help of SAL, diverse strategies for certain regions are learned through two independent residual dense networks (RDNs) and thereby generating accurate fusion results. Experimental results show that our proposal performs better than the competing methods in both spatial quality enhancement and spectral fidelity preservation.","1558-0571","","10.1109/LGRS.2022.3209787","National Natural Science Foundation of China(grant numbers:62271060,61571050,41771407); Beijing Natural Science Foundation(grant numbers:4222046); BNU Interdisciplinary Research Foundation for the First-Year Doctoral Candidates(grant numbers:BNUXKJC2120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903428","Joint visual saliency analysis;pan-sharpening;parallel bidirectional network;remote sensing (RS)","Visualization;Image color analysis;Feature extraction;Bayes methods;Histograms;Convolution;Generative adversarial networks","Bayes methods;deep learning (artificial intelligence);feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;optimisation;remote sensing","visual attention mechanism;pan-sharpening method;parallel bidirectional network;Bayesian optimization-based joint visual saliency analysis method;prior saliency;likelihood saliency;joint co-occurrence histogram;multiple RS images;parallel bidirectional feature pyramid network;coarse fusion features;panchromatic images;multispectral images;saliency-aware layer;independent residual dense networks;remote sensing images;spectral quality;spatial quality;unified fusion strategy;B-JVSA;RDNs;SAL;PAN;MS;global color contrast","","2","","16","IEEE","26 Sep 2022","","","IEEE","IEEE Journals"
"Green Band Generation for Advanced Baseline Imager Sensor Using Pix2Pix With Advanced Baseline Imager and Advanced Himawari Imager Observations","J. -E. Park; G. Kim; S. Hong","Department of Environment, Energy and Geoinfomatics, Sejong University, Seoul, South Korea; National Institute of Environmental Research, Incheon, South Korea; DeepThoTh Company Ltd., Seoul, South Korea","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6415","6423","Green bands in satellite remote sensing play an important role in monitoring water and vegetation information. Due to the lack of observed green band, the Geostationary Operational Environmental Satellite (GOES-16) Advanced Baseline Imager (ABI) sensor uses a synthetic one. This study presents an ABI green band generation method using the Pix2Pix based on conditional generative adversarial networks (CGANs) and convolutional neural network techniques with data observed in the visible range of the GOES-16/ABI sensor. Our model was constructed from the radiance data sets in the red, blue, and green bands of the Advanced Himawari Imager (AHI) onboard Himawari-8/9 satellites from August 27, 2018 to May 1, 2019, and applied to generate a GOES-16 ABI green band using the ABI blue band radiance data. A comparison between the AHI and the Pix2Pix-generated AHI green bands displayed high accuracy, evaluated through bias = 0.120, root mean square error (RMSE) = 0.983 in digital number (DN) units, and correlation coefficient (CC) = 0.999. Furthermore, comparison between the Pix2Pix-generated and synthetic ABI green bands resulted in a good agreement (bias = 1.029 and RMSE = 2.892 in DN units, CC = 0.993). The statistical comparison between the green band, and red or blue band resulted in the exceptional performance of the Pix2Pix-generated ABI green band compared to the synthetic ABI green band. Consequently, our Pix2Pix-based model can be effectively used to generate nonexistent green band of ABI sensor and be applied in a variety of scientific applications requiring green band.","1558-0644","","10.1109/TGRS.2020.3032732","Korea Meteorological Administration Research and Development Program(grant numbers:KMI2020-00510); National Institute of Environment Research (NIER), funded by the Ministry of Environment (MOE), South Korea(grant numbers:NIER-2020-01-01-004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247535","Artificial intelligence (AI);green band;Himawari;Pix2Pix;radiance;satellite remote sensing","Green products;Satellites;Air pollution;Data models;Training;Vegetation mapping;Indexes","atmospheric temperature;data assimilation;geophysical signal processing;image sensors;neural nets;radiative transfer;remote sensing;weather forecasting","Advanced Himawari Imager observations;satellite remote sensing;Geostationary Operational Environmental Satellite Advanced Baseline Imager sensor;ABI green band generation method;conditional generative adversarial networks;GOES-16 ABI green band;ABI blue band radiance data;Pix2Pix-generated AHI green bands;synthetic ABI green band;red band;Pix2Pix-generated ABI green band;Pix2Pix-based model;nonexistent green band","","3","","41","IEEE","3 Nov 2020","","","IEEE","IEEE Journals"
"Fully Convolutional Change Detection Framework with Generative Adversarial Network for Unsupervised, Weakly Supervised and Regional Supervised Change Detection","C. Wu; B. Du; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, and Institute of Artificial Intelligence, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence, School of Computer Science and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2023","PP","99","1","15","Deep learning for change detection is one of the current hot topics in the field of remote sensing. However, most end-to-end networks are proposed for supervised change detection, and unsupervised change detection models depend on traditional pre-detection methods. Therefore, we proposed a fully convolutional change detection framework with generative adversarial network, to unify unsupervised, weakly supervised, regional supervised, and fully supervised change detection tasks into one end-to-end framework. A basic Unet segmentor is used to obtain change detection map, an image-to-image generator is implemented to model the spectral and spatial variation between multi-temporal images, and a discriminator for changed and unchanged is proposed for modeling the semantic changes in weakly and regional supervised change detection task. The iterative optimization of segmentor and generator can build an end-to-end network for unsupervised change detection, the adversarial process between segmentor and discriminator can provide the solutions for weakly and regional supervised change detection, the segmentor itself can be trained for fully supervised task. The experiments indicate the effectiveness of the propsed framework in unsupervised, weakly supervised and regional supervised change detection. This paper provides new theorical definitions for unsupervised, weakly supervised and regional supervised change detection tasks with the proposed framework, and shows great potentials in exploring end-to-end network for remote sensing change detection.","1939-3539","","10.1109/TPAMI.2023.3237896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10019593","Remote Sensing;Change Detection;Fully Covolutional Network;Generative Adversarial Network;Weakly Supervised Segmentation","Task analysis;Image segmentation;Generators;Remote sensing;Generative adversarial networks;Predictive models;Training","","","","","","","IEEE","18 Jan 2023","","","IEEE","IEEE Early Access Articles"
"HyperViTGAN: Semisupervised Generative Adversarial Network With Transformer for Hyperspectral Image Classification","Z. He; K. Xia; P. Ghamisi; Y. Hu; S. Fan; B. Zu","Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; School of Electronics and Information Engineering, Hebei University of Technology, Tianjin, China; Institute of Advanced Research in Artificial Intelligence, Vienna, Austria; Department of Electrical and Computer Engineering, University of Wisconsin-Madison, Madison, WI, USA; School of Electronics and Information Engineering, Hebei University of Technology, Tianjin, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Aug 2022","2022","15","","6053","6068","Generative adversarial networks (GANs) have achieved many excellent results in hyperspectral image (HSI) classification in recent years, as GANs can effectively solve the dilemma of limited training samples in HSI classification. However, due to the class imbalance problem of HSI data, GANs always associate minority-class samples with fake label. To address this issue, we first propose a semisupervised generative adversarial network incorporating a transformer, called HyperViTGAN. The proposed HyperViTGAN is designed with an external semisupervised classifier to avoid self-contradiction when the discriminator performs both classification and discrimination tasks. The generator and discriminator with skip connection are utilized to generate HSI patches by adversarial learning. The proposed HyperViTGAN captures semantic context and low-level textures to reduce the loss of critical information. In addition, the generalization ability of the HyperViTGAN is improved through the use of data augmentation. Experimental results on three well-known HSI datasets, Houston 2013, Indian Pines 2010, and Xuzhou, show that the proposed model achieves competitive HSI classification performance in comparison with the current state-of-the-art classification models.","2151-1535","","10.1109/JSTARS.2022.3192127","National Scholarship for Building High Level Universities; China Scholarship Council(grant numbers:201906700002); National Natural Science Foundation of China(grant numbers:U1813222,42075129); Hebei Province Natural Science Foundation(grant numbers:E2021202179); Key Research and Development Project from Hebei Province(grant numbers:19210404D,20351802D,21351803D); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832721","Generative adversarial network (GAN);hyperspectral image (HSI) classification;semisupervised learning;transformer","Generative adversarial networks;Transformers;Task analysis;Data models;Hyperspectral imaging;Generators;Training","geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);pattern classification","discrimination tasks;HSI patches;adversarial learning;low-level textures;HSI datasets;competitive HSI classification performance;current state-of-the-art classification models;semisupervised generative adversarial network;transformer;hyperspectral image classification;generative adversarial networks;GANs;training samples;class imbalance problem;HSI data;minority-class samples;called HyperViTGAN;external semisupervised classifier","","1","","59","CCBY","18 Jul 2022","","","IEEE","IEEE Journals"
"SlimRGBD: A Geographic Information Photography Noise Reduction System for Aerial Remote Sensing","C. Wu; B. Ju; Y. Wu; N. Xiong","School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China; School of Optical-Electrical and Computer Engineering, University of Shanghai for Science and Technology, Shanghai, China; O’Neill School of Public and Environmental Affairs, Indiana University Bloomington, Bloomington, USA; Department of Mathematics and Computer Science, Northeastern State University, Tahlequah, USA","IEEE Access","28 Jan 2020","2020","8","","15144","15158","In the past ten years, civil drone technology has developed rapidly, and UAV (Unmanned Aerial Vehicle) has been widely used in various industries. Especially in the field of aerial remote sensing, the emergence of UAV technology has enabled the geographical information of remote areas that are not concerned to be quickly presented. However, UAV aerial photography is greatly affected by the weather. Pictures that use aerial drones for aerial photography in rainy weather will appear noise. In this paper, how to eliminate the noise of aerial image is to be talked, the multi-channel pruning technology is used to pruning the RnResNet network. Based on this, a new anti-convergence-convolution neural network noise reduction system for the operation of UAV airborne embedded equipment is proposed. The system is used to eliminate noise in the aerial image. This type of noise reducer has got rid of the current situation that the neural network noise reducer consumes too much power and is inefficient, and has certain advantages.","2169-3536","","10.1109/ACCESS.2020.2966497","National Basic Research Program of China (973 Program)(grant numbers:2018YFC0810204); National Natural Science Foundation of China(grant numbers:61872242,61502220); Shanghai Science and Technology Innovation Action Plan Project(grant numbers:17511107203,16111107502); Shanghai Key Lab of Modern Optical System; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8959217","SlimRGBD;ResNet;generative adversarial networks;image noise reduction;UAV;channel pruning;sparse training","Drones;Noise reduction;Remote sensing;Photography;Image denoising;Real-time systems","autonomous aerial vehicles;geophysical image processing;image denoising;neural nets;remote sensing;robot vision","geographic information photography noise reduction system;aerial remote sensing;civil drone technology;unmanned aerial vehicle;UAV technology;geographical information;UAV aerial photography;aerial drones;aerial image;multichannel pruning technology;anti-convergence-convolution neural network noise reduction system;UAV airborne embedded equipment;neural network noise reducer;RnResNet network","","4","","51","CCBY","14 Jan 2020","","","IEEE","IEEE Journals"
"Can SAR Images and Optical Images Transfer with Each Other?","L. Liu; B. Lei","Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7019","7022","Synthetic aperture radar (SAR) and optical imaging are different remote sensing methods. Given a SAR image, is it possible to predict what the observed scene looks like in an optical image? Transfer between SAR data and optical data seems to be impossible. However, this article shows examples that by applying deep learning techniques on high resolution airborne SAR images and GoogleEarth optical images, the SAR images and optical images can transfer with each other. The transferring help us to better understand the relationship between SAR and optical image, and can be potentially used to transfer detection or classification algorithms for optical image straightforwardly to be applied on SAR image.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518921","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518921","Image transfer between optical and SAR images;deep learning;CycleGAN;generative adversarial networks;transfer learning","Optical imaging;Optical sensors;Synthetic aperture radar;Adaptive optics;Optical distortion;Optical computing;Optical losses","geophysical image processing;learning (artificial intelligence);optical images;radar imaging;remote sensing by radar;synthetic aperture radar","SAR image;optical images transfer;optical imaging;optical image;SAR data;optical data;high resolution airborne SAR images;GoogleEarth optical images","","13","","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Characterization of Background-Anomaly Separability With Generative Adversarial Network for Hyperspectral Anomaly Detection","J. Zhong; W. Xie; Y. Li; J. Lei; Q. Du","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Science and Technology on Electro-Optic Control Laboratory, Luoyang, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2021","2021","59","7","6017","6028","Hyperspectral images (HSIs) have unique advantages in distinguishing subtle spectral differences of different materials. However, due to complex and diverse backgrounds, unknown prior knowledge, and imbalanced samples, it is challenging to separate background and anomaly. In this article, we present a novel characterization of background-anomaly separability with a generative adversarial network (BASGAN) for hyperspectral anomaly detection. The key contribution is the proposal to explicitly constrain the background and anomaly separability by characterizing background spectral samples while avoiding anomaly reconstruction. First, we use a class saliency map extraction algorithm to obtain pseudobackground and anomaly samples for adversarial training. To further mitigate the suffering of anomaly contamination in background distribution estimation, we introduce background-anomaly separability constrained loss function to enhance the reconstruction of the background while weakening the anomaly reconstruction in a semisupervised way. Additionally, a discriminator is induced into the latent space to make the encoded representation resemble Gaussian distribution during adversarial training. The other is adversarial training in the reconstruction space so that the background estimation can be improved. Experiments conducted on real data sets illustrate the superior background-anomaly separability of the proposed method.","1558-0644","","10.1109/TGRS.2020.3013022","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Young Talent Fund of the University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; Science and Technology on Electro-Optic Control Laboratory and Aeronautical Science Foundation of China(grant numbers:6142504190206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173691","Background-anomaly separability;generative adversarial networks (GANs);hyperspectral anomaly detection","Anomaly detection;Hyperspectral imaging;Generative adversarial networks;Training;Gallium nitride;Image reconstruction","feature extraction;Gaussian distribution;geophysical image processing;hyperspectral imaging;learning (artificial intelligence);neural nets;object detection","generative adversarial network;hyperspectral anomaly detection;distinguishing subtle spectral differences;complex backgrounds;diverse backgrounds;separate background;background spectral samples;anomaly reconstruction;anomaly samples;adversarial training;anomaly contamination;background distribution estimation;background estimation;superior background-anomaly separability;hyperspectral images;BASGAN;Gaussian distribution;encoded representation","","18","","45","IEEE","21 Aug 2020","","","IEEE","IEEE Journals"
"Self-Supervised Divide-and-Conquer Generative Adversarial Network for Classification of Hyperspectral Images","J. Feng; N. Zhao; R. Shang; X. Zhang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","20 Sep 2022","2022","60","","1","17","Generative adversarial network (GAN) has been rapidly developed because of its powerful generating ability. However, imbalanced class distribution of hyperspectral images (HSIs) easily causes mode collapse in GAN. Moreover, limited training samples in HSIs restrict the generating ability of GAN. These issues may further deteriorate the classification performance of the discriminator. To conquer these issues, a novel self-supervised divide-and-conquer (SDC)-GAN is proposed for HSI classification. In SDC-GAN, a pretext cluster task with an encoder–decoder architecture is designed by leveraging abundant unlabeled samples. By transferring the learned cluster representation from the cluster task, limited labeled samples are divided effectively in the downstream classification. According to the division of clustering, SDC-GAN constructs a generic and several specific branches for both the generator and discriminator. The generator generates all- and specific-class samples by using the generic and specific branches separately and combines them adaptively. It can weaken the generation preference for the classes with large sample sizes and alleviate the mode collapse problem. Meanwhile, the classification ability of the discriminator is improved by integrating the judgment of specific branches into the generic branch. Experimental results show that SDC-GAN achieves competitive results for HSI classification compared with several state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3202908","National Natural Science Foundation of China(grant numbers:61871306,61836009,62172600,62077038); Innovation Capability Support Program of Shaanxi(grant numbers:2021KJXX-08); Natural Science Basic Research Program of Shaanxi(grant numbers:2022JC-45,2022GY-065); Association for Science and Technology Youth Talent Support Program Project(grant numbers:095920201301); Fundamental Research Funds for the Central Universities(grant numbers:JB211901); Aeronautical Science Fund of China(grant numbers:2019ZC081002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869893","Divide and conquer;generative adversarial networks (GANs);hyperspectral image (HSI) classification;self-supervised learning","Generative adversarial networks;Task analysis;Feature extraction;Generators;Training;Self-supervised learning;Hyperspectral imaging","","","","6","","53","IEEE","29 Aug 2022","","","IEEE","IEEE Journals"
"Small Object Detection Leveraging on Simultaneous Super-resolution","H. Ji; Z. Gao; X. Liu; Y. Zhang; T. Mei","School of Remote Sensing and Information Engineering Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering Wuhan University, Wuhan, China; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Remote Sensing and Information Engineering Wuhan University, Wuhan, China; Electronic and Information School, Wuhan University, Wuhan, China","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","803","810","Despite the impressive advancement achieved in object detection, the detection performance of small object is still far from satisfactory due to the lack of sufficient detailed appearance to distinguish it from similar objects. Inspired by the positive effects of super-resolution for object detection, we propose a framework that can be incorporated with detector networks to improve the performance of small object detection, in which the low-resolution image is super-resolved via generative adversarial network (GAN) in an unsupervised manner. In our method, the super-resolution network and the detection network are trained jointly. In particular, the detection loss is back-propagated into the super-resolution network during training to facilitate detection. Compared with available simultaneous super-resolution and detection methods which heavily rely on low-/high-resolution image pairs, our work breaks through such restriction via applying the CycleGAN strategy, achieving increased generality and applicability, while remaining an elegant structure. Extensive experiments on datasets from both computer vision and remote sensing communities demonstrate that our method obtains competitive performance on a wide range of complex scenarios.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9413058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413058","","Training;Image segmentation;Computer vision;Superresolution;Object detection;Detectors;Generative adversarial networks","computer vision;image resolution;object detection;remote sensing","low-resolution image;generative adversarial network;super-resolution network;detection network;simultaneous super-resolution;small object detection;detector networks;CycleGAN;computer vision;remote sensing","","1","","36","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Building Damage Assessment From Post-Hurricane Imageries Using Unsupervised Domain Adaptation With Enhanced Feature Discrimination","C. Lin; Y. Li; Y. Liu; X. Wang; S. Geng","School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","6 Dec 2021","2022","60","","1","10","Rapid damage assessment after hurricane disasters is crucial for initiating effective emergency response actions. Unsupervised domain adaptation (UDA), which improves the classification accuracy of the target domain with leveraging abundant labeled data of the source domain, exhibits the potential to solve the issue of lacking labeled data in the task of damage building assessment. However, the application of UDA to building damage detection remains a challenge due to the complexity of post-hurricane imageries. The image characteristics of the source domain differ from those of the target domain, and the similarity of interclass samples is high, thereby degrading the transfer performance of UDA. We propose Duplex Alignment Networks with enhanced feature discrimination, which consists of a pair of generative adversarial networks (GANs) and a dedicated classifier, to address this issue. Our approach can be highlighted in two aspects: 1) a pair of GANs are used to eliminate the discrepancy between the source and the target domains by the alignments at the feature and pixel levels and 2) a dedicated classifier is integrated into the second discriminator to improve the discrimination of the extracted features of the target domain, which can alleviate performance deterioration caused by the high similarity of interclass samples. The proposed approach is validated by two challenging transfer tasks by using the data sets of Hurricanes Sandy, Maria, and Irma. Average accuracy rates of 85.1% (Hurricane Sandy  $\to $  Maria) and 98.3% (Hurricane Sandy  $\to $  Irma) are achieved, thereby leading to superior performance compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2021.3054869","National Natural Science Foundation of China(grant numbers:62071006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354434","Building damage assessment;domain adaptation (DA);generative adversarial network (GAN)","Feature extraction;Buildings;Generative adversarial networks;Training;Deep learning;Hurricanes;Gallium nitride","buildings (structures);disasters;feature extraction;image classification;neural nets;storms;structural engineering computing;unsupervised learning","building damage assessment;post-hurricane imageries;unsupervised domain adaptation;feature discrimination;hurricane disasters;emergency response actions;building damage detection;dedicated classifier;Hurricanes Sandy;Hurricane Maria;Hurricane Irma;target classification;duplex alignment networks;generative adversarial networks;feature extraction","","2","","47","IEEE","15 Feb 2021","","","IEEE","IEEE Journals"
"TEM-NLnet: A Deep Denoising Network for Transient Electromagnetic Signal With Noise Learning","M. Wang; F. Lin; K. Chen; W. Luo; S. Qiang","College of Computer Science and Cyber Security, Chengdu University of Technology, Chengdu, China; College of Mechanical and Electrical Engineering, Chengdu University of Technology, Chengdu, China; Key Laboratory of Earth Exploration and Information Technology, Ministry of Education, Chengdu University of Technology, Chengdu, China; College of Geophysics, Chengdu University of Technology, Chengdu, China; Faculty of Information Technology, Macau University of Science and Technology, Taipa, China","IEEE Transactions on Geoscience and Remote Sensing","22 Mar 2022","2022","60","","1","14","Transient electromagnetic (TEM) method is a widely adopted technology in geophysics. TEM signals received by coils will be disturbed by complex noises. Compared with traditional filtering-based methods, deep-learning-based TEM signal denoising methods achieved impressive denoising performance. However, the existing deep-learning-based methods rely heavily on simulated noise with a certain distribution to construct paired datasets for supervised learning. In real scenarios, if the noise distribution of acquired TEM signals has a huge difference (e.g., the type of noise distribution, the level of noise) with that of the simulated datasets, the trained model may not always be valid. To address this issue, a novel noise-learning-inspired deep denoising network (namely, TEM-NLnet) is proposed for TEM signal denoising. Specifically, instead of inserting the simulated noise, we first learn the noise appeared in real-world signals through generative adversarial networks (GANs), such that the generator can produce the learned noise to construct paired datasets for training. Then, a deep-neural-network-based denoiser is imposed to learn mapping from the noise TEM signal to the corresponding noise-free one. Extensive experiments on the simulated and actual geological datasets show that compared with other state-of-the-art TEM denoising methods, our proposed method achieves better performance in terms of quantitative and visual results. Models and code are available at https://github.com/wmyCDUT/TEM-NLnet_demo.","1558-0644","","10.1109/TGRS.2022.3148340","Sichuan Science and Technology Program(grant numbers:2021ZHCG0014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698089","Deep learning;denoising;noise learning;transient electromagnetic (TEM) signal","Noise reduction;Noise measurement;Signal denoising;Generative adversarial networks;Wavelet transforms;Neural networks;Generators","deep learning (artificial intelligence);signal denoising","transient electromagnetic signal;noise learning;transient electromagnetic method;complex noises;filtering-based methods;deep-learning-based TEM signal;impressive denoising performance;deep-learning-based methods;simulated noise;paired datasets;supervised learning;noise distribution;acquired TEM signals;real-world signals;generative adversarial networks;learned noise;deep-neural-network-based denoiser;noise TEM signal;actual geological datasets;noise-learning-inspired deep denoising network","","1","","42","IEEE","31 Jan 2022","","","IEEE","IEEE Journals"
"FGF-GAN: A Lightweight Generative Adversarial Network for Pansharpening via Fast Guided Filter","Z. Zhao; J. Zhan; S. Xu; K. Sun; L. Huang; J. Liu; C. Zhang","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Pansharpening is a widely used image enhancement technique for remote sensing. Its principle is to fuse the input high-resolution single-channel panchromatic (PAN) image and low-resolution multi-spectral image and to obtain a high-resolution multi-spectral (HRMS) image. The existing deep learning pansharpening method has two shortcomings. First, features of two input images need to be concatenated along the channel dimension to reconstruct the HRMS image, which makes the importance of PAN images not prominent, and also leads to high computational cost. Second, the implicit information of features is difficult to extract through the manually designed loss function. To this end, we propose a generative adversarial network via the fast guided filter (FGF) for pansharpening. In generator, traditional channel concatenation is replaced by FGF to better retain the spatial information while reducing the number of parameters. Meanwhile, the fusion objects can be highlighted by the spatial attention module. In addition, the latent information of features can be preserved effectively through adversarial training. Numerous experiments illustrate that our network generates high-quality HRMS images that can surpass existing methods, and with fewer parameters.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428272","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428272","Pansharpening;Fast guided filter;Generative adversarial network;Image fusion","Training;Earth;Fuses;Pansharpening;Feature extraction;Information filters;Generative adversarial networks","deep learning (artificial intelligence);geophysical image processing;image enhancement;image fusion;image resolution;remote sensing","lightweight generative adversarial network;fast guided filter;image enhancement technique;remote sensing;input high-resolution single-channel panchromatic image;low-resolution multispectral image;high-resolution multispectral image;channel dimension;PAN images;high computational cost;channel concatenation;adversarial training;high-quality HRMS images;FGF-GAN;deep learning pansharpening method","","3","","20","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"FADN: Features Attention with Deep Networks for Remote-Image Classification","Y. Zhang; H. Ren; W. Yang; J. Lv; C. -Z. Xu; K. Ye","Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China; Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China; Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China; Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China; Dept of Computer & Information Science, University of Macau, Macau SAR, China; Shenzhen Institutes of Advanced Technology, CAS, Shenzhen, China","2019 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS)","13 Jun 2019","2019","","","79","84","Convolutional neural networks have become the major method in the field of image classification, and the algorithm based on neural networks already has achieved remarkable achievements in a lot of scenarios in the real world. Now, we consider to solve the problem that semantic segmentation and classification in remote sensing images. The recognition in this field is based on image clipping and then passed the images to the traditional classifiers in the early stage. However, this non-end-to-end network has poor edge recognition result. Today, End-to-end methods can accurately identity edges in images, but limited by the numbers of labeled satellite images. In this paper, we proposed an end-to-end remote-image classification method which focus on recovering detailed information on the pixel level and features themselves. Instead of increasing the number of labeled samples, we put more effort on improving the quality of features. This method takes into account both high-dimensional features and low-dimensional features. Meanwhile, our designed network is an effective algorithm which combined the U-net and ResNet together. This new method can learn more useful features and achieve a better performance. Finally, we did a series of experiments and it showed our network has the good capacity to do the classification job.","","978-1-7281-0467-6","10.1109/HPBDIS.2019.8735452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735452","convolutional neural networks;remote-image;end-to-end;classification","Satellites;Neural networks;Generative adversarial networks;Classification algorithms;Convolution;Image classification;Image segmentation","convolutional neural nets;edge detection;image classification;image sampling;image segmentation;remote sensing","convolutional neural networks;semantic segmentation;remote sensing images;image clipping;labeled satellite images;high-dimensional features;low-dimensional features;edge recognition;remote-image classification method","","2","","28","IEEE","13 Jun 2019","","","IEEE","IEEE Conferences"
"DCGAN: Deep Convolutional GAN with Attention Module for Remote View Classification","A. Patil; Venkatesh","Department of Computer Science and Engineering, University Visvesvaraya College of Engineering, Bangalore University, Bengaluru-560001, India; Department of Computer Science and Engineering, University Visvesvaraya College of Engineering, Bangalore University, Bengaluru-560001, India","2021 International Conference on Forensics, Analytics, Big Data, Security (FABS)","9 Feb 2022","2021","1","","1","10","In recent times, the development of Deep Learning Techniques for Image Classification has increased. The deep learning module uses an unsupervised learning technique. The supervised learning requires an adequate and outsized dataset with labels to train a machine. This paper proposes a unique Unsupervised Deep Feature Learning Method called Deep Convolutional GAN (DCGAN) with Attention Module for Remote Scene Classification. The Attention module is integrated with DCGAN to optimize the power of feature extraction. To extract the contextual information, a feature fusion architecture is proposed and it is integrated with Discriminator. The proposed module optimizes the discriminator and generator losses. The extracted features are given as input to SVM for the classification. The proposed module DCGAN with Attention module is implemented with publicly available remote sensing scene UC-Merced dataset which have 21 different scene classes and the RSSCN7 dataset which have 7 different scene classes. The experimental results obtained by the proposed model DCGAN with Attention module are better than the state-of-art machines or methods results. The proposed model achieves an accuracy of 91.67% and 84.29 % for the RSSCN7 and UC-Merced datasets respectively.","","978-1-6654-2005-1","10.1109/FABS52071.2021.9702655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9702655","Attention Module;Generative Adversarial networks;SVM;Loss Function;feature fusion;Cross-Entropy","Support vector machines;Deep learning;Representation learning;Supervised learning;Feature extraction;Generative adversarial networks;Security","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);pattern classification;remote sensing;support vector machines;unsupervised learning","Deep Convolutional GAN;Attention Module;Remote Scene Classification;Attention module;feature extraction;module DCGAN;publicly available remote sensing scene UC-Merced dataset;21 different scene classes;7 different scene classes;Remote view Classification;Deep Learning Techniques;Image Classification;deep learning module;unsupervised learning technique;supervised learning;unique Unsupervised Deep Feature Learning Method","","1","","22","IEEE","9 Feb 2022","","","IEEE","IEEE Conferences"
"FG-GAN: A Fine-Grained Generative Adversarial Network for Unsupervised SAR-to-Optical Image Translation","X. Yang; Z. Wang; J. Zhao; D. Yang","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Xi’an Institute of Space Radio Technology, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","21 Apr 2022","2022","60","","1","11","Synthetic aperture radar (SAR) and optical sensing are two important means of Earth observation. SAR can be used for all-day and all-weather Earth observation, but it has the disadvantages of speckle noise and geometric distortion, which are not conducive to human eye recognition. Optical image conforms to the characteristics of human visual observation, but it is easily affected by climate and time. Therefore, to integrate the advantages of the two, researchers have carried out extensive work on SAR-to-optical (S2O) image translation. Most of the existing methods for S2O image translation are supervised and need paired training samples, limiting its large-scale application in remote sensing field. Thus, we give priority to an unsupervised S2O image translation method. Meanwhile, we find that the images generated by unsupervised methods suffer from significant detail deficiencies. To solve this problem, we propose a fine-grained generative adversarial network (FG-GAN) introducing three strategies to enhance the detailed information in generated optical images. First, we design an unbalanced generator (UBG) with complex encoder networks and relatively simple decoder networks. The complex encoder extracts abundant feature information, while the decoder obtains key details by filtering these features. Second, to match the learning ability of the generator, we present a multiscale discriminator (MSD) to enhance the discriminant ability of the network. Third, we propose a comprehensive normalization group (CNG) to promote the physical representation consistency of SAR and optical images. Extensive experiments have been conducted, and the results show that our method is superior to the state-of-the-art (SOTA) methods on both subjective and objective evaluation indicators. Moreover, our FG-GAN has a significant improvement on classification accuracy, indicating its potential in facilitating the performance of practical remote sensing tasks.","1558-0644","","10.1109/TGRS.2022.3165371","National Natural Science Foundation of China(grant numbers:61976166); Key Research and Development Program of Shaanxi(grant numbers:2021GY-030); Fundamental Research Funds for the Central Universities(grant numbers:JB210115); Open Research Projects of Laboratory of Pinghu; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9750140","Comprehensive normalization group (CNG);multiscale discriminator (MSD);synthetic aperture radar (SAR)-to-optical (S2O) image translation;unbalanced generator (UBG)","Optical imaging;Optical sensors;Optical distortion;Adaptive optics;Synthetic aperture radar;Generative adversarial networks;Generators","codecs;feature extraction;geophysical image processing;geophysical techniques;image classification;image coding;image filtering;learning (artificial intelligence);neural nets;optical images;radar imaging;remote sensing by radar;speckle;synthetic aperture radar","SOTA methods;CNG;comprehensive normalization group;MSD;multiscale discriminator;geometric distortion;speckle noise;Earth observation;synthetic aperture radar;state-of-the-art methods;decoder networks;complex encoder networks;unbalanced generator;unsupervised S2O image translation method;remote sensing field;human visual observation;optical image;human eye recognition;all-weather Earth observation;optical sensing;unsupervised SAR-to-optical image translation;fine-grained generative adversarial network;FG-GAN","","","","40","IEEE","6 Apr 2022","","","IEEE","IEEE Journals"
"Controlled Multi-modal Image Generation for Plant Growth Modeling","M. Miranda; L. Drees; R. Roscher","German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","5118","5124","Predicting plant development is an important task in precision farming and an essential metric for decision-making by researchers and farmers. In this work, we propose a novel generative modeling technique for plant growth prediction based on conditional generative adversarial networks. We formulate plant growth as an image-to-image translation task and predict the appearance of a plant growth stage as a function of its previous stage. We take into account that plant growth is inherently multi-modal, depending on numerous and highly variable environmental factors, and thus a single input belongs to a distribution of potential outputs. We encode the ambiguity in an interpretable and low-dimensional latent vector space representing the various factors of variation that are influencing plant growth. We use a novel encoder-based data fusion technique and combine information contained in remote sensing imagery of different cropping systems with data containing the factors of variation to adequately model plant growth. This offers several advantages over existing methods: (1) we show that we can model a distribution of potential appearances and simultaneously outperform existing methods in providing more realistic predictions, (2) the complexity of plant growth is more adequately captured, as various factors influencing plant growth can be included, (3) predictions are controllable by being conditioned by an interpretable latent vector representing the factors of variation along with an input image of a previous growth stage.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956115","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956115","","Measurement;Image synthesis;Ecosystems;Predictive models;Generative adversarial networks;Pattern recognition;Task analysis","agriculture;crops;encoding;environmental factors;geophysical image processing;image fusion;regression analysis;remote sensing;sensor fusion;vectors","controlled multimodal image generation;image-to-image translation task;plant growth modeling;plant growth prediction;plant growth stage;predicting plant development;previous growth stage","","","","36","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"JAGAN: A Framework for Complex Land Cover Classification Using Gaofen-5 AHSI Images","W. Chen; S. Ouyang; J. Yang; X. Li; G. Zhou; L. Wang","School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","10 Feb 2022","2022","15","","1591","1603","Owing to their powerful feature extraction capabilities, deep learning-based methods have achieved significant progress in hyperspectral remote sensing classification. However, several issues still exist in these methods, including a lack of hyperspectral datasets for specific complicated scenarios and the need to improve the classification accuracy of land cover with limited samples. Thus, to highlight and distinguish effective features, we propose a hyperspectral classification framework based on a joint channel-space attention mechanism and generative adversarial network (JAGAN). To relearn feature-based weights, a higher priority was assigned to important features, which was developed by integrating a two-joint channel-space attention model to obtain the most valuable feature via the attention weight map. Additionally, two classifiers were designed in JAGAN: sigmoid was used to determine whether the input data were real or fake samples produced by the generator, while Softmax was adopted as a land cover classifier to yield the prediction type labels of the input samples. To test the classification performance of the JAGAN model, we used a self-constructed complex land cover dataset based on GaoFen-5 AHSI images, which consists of mixed landscapes of mining and agricultural areas from the urban-rural fringe. Compared with other methods, the proposed model achieved the highest overall classification accuracy of 86.09%, the highest kappa amount of 79.41%, the highest F1 score of 85.86%, and the highest average accuracy of 82.30%, indicating the JAGAN can effectively improve the classification accuracy for limited samples in complex regional environments using GF-5 AHSI images.","2151-1535","","10.1109/JSTARS.2022.3144339","Fundamental Research Funds for the Natural Science Foundation of China(grant numbers:U1803117,U21A2013); Aerospace Research Fund(grant numbers:D040104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691861","Attention mechanism;generative adversarial network;Gaofen-5 (GF-5);hyperspectral remote sensing;land cover classification","Hyperspectral imaging;Generative adversarial networks;Training;Image classification;Feature extraction;Land surface;Roads","agriculture;feature extraction;geophysical image processing;geophysical signal processing;image classification;learning (artificial intelligence);pattern classification;remote sensing;terrain mapping","complex land cover classification;gaofen-5 AHSI images;powerful feature extraction capabilities;deep learning-based methods;hyperspectral remote sensing classification;hyperspectral datasets;specific complicated scenarios;classification accuracy;hyperspectral classification framework;joint channel-space attention mechanism;generative adversarial network;feature-based weights;two-joint channel-space attention model;valuable feature;attention weight map;land cover classifier;classification performance;JAGAN model;complex land cover dataset;GaoFen-5 AHSI images;highest average accuracy;GF-5 AHSI images","","11","","71","CCBY","25 Jan 2022","","","IEEE","IEEE Journals"
"Unsupervised Domain Adaptation Based on Progressive Transfer for Ship Detection: From Optical to SAR Images","Y. Shi; L. Du; Y. Guo; Y. Du","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Academy of Advanced Interdisciplinary Research, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","4 Jul 2022","2022","60","","1","17","In recent years, synthetic aperture radar (SAR) ship detection methods based on convolutional neural networks (CNNs) have attracted wide attention in remote sensing fields. However, these methods require a large number of labeled SAR images to train the network, where labeling for SAR images is more expensive and time-consuming than that for optical images. To address the problem of lacking labeled SAR images, in this article, we proposed an unsupervised domain adaptation (UDA) framework based on progressive transfer for SAR ship detection by transferring knowledge from the optical domain to the SAR domain. Due to the prominent difference between the optical and SAR images, our approach progressively transfers knowledge from three levels: pixel level, feature level, and prediction level. At the pixel level, considering the difference in imaging mechanism, we propose a special data augmentation method for ship targets and build the generator with skip connection based on generative adversarial networks (GANs) to generate transition domain, which can reduce the appearance discrepancy between the optical and SAR images. At the feature level, the detector is trained to learn the domain-invariant features with adversarial alignment. At the prediction level, we further use the predicted pseudo-labels obtained by the feature-aligned detector to learn more discriminative features of the SAR images directly and propose the robust self-training (RST) method to reduce the influence of noisy pseudo-labels on detector training. Specially, RST is formulated as a loss minimization problem for object detection. The experimental results based on the domain adaptation from optical dataset to SAR dataset demonstrate that our approach achieves superior SAR ship detection performance with unlabeled SAR images.","1558-0644","","10.1109/TGRS.2022.3185298","National Science Foundation of China(grant numbers:U21B2039,61771362); Higher Education Discipline Innovation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803220","Generative adversarial networks (GANs);optical to synthetic aperture radar (SAR);robust self-training (RST);SAR;ship detection;unsupervised domain adaptation (UDA)","Optical imaging;Marine vehicles;Synthetic aperture radar;Radar polarimetry;Adaptive optics;Detectors;Optical detectors","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);neural nets;object detection;radar imaging;ships;synthetic aperture radar;unsupervised learning","optical SAR images;SAR domain;optical domain;unsupervised domain adaptation framework;labeled SAR images;convolutional neural networks;synthetic aperture radar ship detection methods;progressive transfer;unlabeled SAR images;superior SAR ship detection performance;robust self-training method;predicted pseudolabels;domain-invariant features;feature level;optical images;transition domain;generative adversarial networks;special data augmentation method;imaging mechanism;pixel level;prediction level","","","","50","IEEE","22 Jun 2022","","","IEEE","IEEE Journals"
"SAR Image Data Augmentation via Residual and Attention-Based Generative Adversarial Network for Ship Detection","Y. -S. Guo; H. -C. Li; W. -S. Hu; W. -Y. Wang","School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; National Engineering Laboratory of Integrated Transportation Big Data Application Technology, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Software Engineering, Chengdu University of Information Technology, Chengdu, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","439","442","In recent years, generative adversarial networks (GANs) have been successfully applied to generate the SAR images. However, due to the fact that it is more difficult to generate the images than to distinguish the real or fake, GANs usually suffer from the problems of unstable training and mode collapse. As such, a residual and attention-based generative adversarial network (RAGAN) is proposed for SAR data augmentation. Firstly, the directional bounding box is used as a constraint in the RAGAN to limit the position of ship in the generated SAR image, which can be further set as the annotation of the SAR image for ship detection directly. After that, inspired by the residual and attention learning, a residual and attention block (RABlock) and a transposed RABlock (TRABlock) are designed to improve the generator of the RAGAN, thus preventing the whole model from gradient vanishing and suppressing the effects of speckle noise and background to enhance the quality of the generated SAR images. Experimental results on the HRSID data set demonstrate the effectiveness of our RAGAN model in SAR data augmentation for ship detection.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884798","National Natural Science Foundation of China(grant numbers:61871335); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884798","Synthetic aperture radar;generative ad-versarial networks;residual learning;attention mechanism;data augmentation;target detection","Training;Annotations;Speckle;Generative adversarial networks;Radar polarimetry;Generators;Data models","learning (artificial intelligence);radar imaging;ships;synthetic aperture radar","SAR image data augmentation;ship detection;GANs;unstable training;mode collapse;attention-based generative adversarial network;RAGAN;SAR data augmentation;generated SAR image;attention learning;attention block","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Combination of Images and Point Clouds in a Generative Adversarial Network for Upsampling Crack Point Clouds","N. H. T. Nguyen; S. Perry; D. Bone; H. Le Thanh; M. Xu; T. T. Nguyen","Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; VNU University of Engineering and Technology, Hanoi, Vietnam; Faculty of Engineering and Information Technology, University of Technology Sydney, Ultimo, NSW, Australia; Faculty of Information Technology, Vietnam National University of Agriculture, Hanoi, Vietnam","IEEE Access","30 Jun 2022","2022","10","","67198","67209","Point cloud data of cracks can be used for various purposes such as crack detection, depth calculation and crack segmentation. Upsampling low-density point clouds can help to improve the performance of those tasks. Building on existing methods that upsample point clouds from low-resolution point cloud input, to improve feature definition, this paper proposes a new method for upsampling low-density point clouds using a combination of these point clouds and corresponding 2D images of the original objects as input data. We use an architecture based on Generative Adversarial Networks (GAN) for training input point clouds with additional information from the corresponding 2D images. The key idea is to exploit features from both 2D images and point clouds to enrich point clouds in both the training and testing phases. Our method takes advantage of the combination of 2D images and point clouds using a GAN framework. Experimental results show our proposed method achieves a higher effectiveness compared with previous upsampling methods.","2169-3536","","10.1109/ACCESS.2022.3182697","Joint Technology and Innovation Research Centre—a partnership between the University of Technology Sydney and Vietnam National University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795005","Point cloud upsampling;generative adversarial network;crack point cloud;enrichment point cloud;combining point clouds and images","Point cloud compression;Three-dimensional displays;Feature extraction;Generative adversarial networks;Solid modeling;Information technology;Data models","computational geometry;computer graphics;crack detection;cracks;feature extraction;geophysical image processing;image reconstruction;image resolution;image sampling;image segmentation;remote sensing by laser beam;solid modelling","crack segmentation;low-density point clouds;methods that upsample point clouds;low-resolution point cloud input;corresponding 2D images;Generative Adversarial Networks;training input point;enrich point clouds;Generative Adversarial network;upsampling crack point clouds;depth calculation","","","","47","CCBY","13 Jun 2022","","","IEEE","IEEE Journals"
"3-D Hybrid CNN Combined With 3-D Generative Adversarial Network for Wetland Classification With Limited Training Data","A. Jamali; M. Mahdianpari; F. Mohammadimanesh; B. Brisco; B. Salehi","Civil Engineering Department, Faculty of Engineering, University of Karabük, Karabük, Turkey; Department of Electrical and Computer Engineering, Memorial University of Newfoundland, St. John's, NL, Canada; Canada Centre for Mapping and Earth Observation, Ottawa, ON, Canada; Canada Centre for Mapping and Earth Observation, Ottawa, ON, Canada; Department of Environmental Resources Engineering, State University of New York College of Environmental Science and Forestry (SUNY ESF), Syracuse, NY, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Sep 2022","2022","15","","8095","8108","Recently, deep learning algorithms, specifically convolutional neural networks (CNNs), have played an important role in remote sensing image classification, including wetland mapping. However, one limitation of deep CNN for classification is its requirement for a great number of training samples. This limitation is particularly enhanced when the classes of interest are spectrally similar, such as that of wetland types, and the training samples are limited. This article presents a novel approach named 3-D hybrid generative adversarial network (3-D hybrid GAN) that addresses the limited training sample issue in the classification of remote sensing imagery with a focus on complex wetland classification. We used a conditional map unit that generates synthetic training samples for only classes with a lower number of training samples to improve the per-class accuracy of wetlands. This procedure overcomes the issue of imbalanced data in conventional wetland mapping. Based on the achieved results, better classification accuracy is obtained by integrating a 3-D generative adversarial network (3-D GAN) and the CNN network of a 3-D hybrid CNN using both 3-D and 2-D convolutional filters. Experimental results on the avalon pilot site located in eastern Newfoundland, Canada, and covering five wetland types of bog, fen, marsh, swamp, and shallow water demonstrate that our model significantly outperforms other CNN models, including the HybridSN, SpectralNet, MLP-mixer, as well as a conventional algorithm of random forest for complex wetland classification by approximately 1% to 51% in terms of F-1 score.","2151-1535","","10.1109/JSTARS.2022.3206143","Natural Sciences and Engineering Research Council(grant numbers:RGPIN-2022-04766); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903275","Convolutional neural network (CNN);deep convolutional neural network (DCNN);generative adversarial network (GAN);random forest (RF);wetland mapping","Training;Solid modeling;Biological system modeling;Generative adversarial networks;Hybrid power systems;Data models;Classification algorithms","geophysical image processing;geophysics computing;image classification;learning (artificial intelligence);multilayer perceptrons;neural nets;pattern classification;remote sensing","CNN combined;3-D generative adversarial network;limited training data;deep learning algorithms;specifically convolutional neural networks;remote sensing image classification;deep CNN;wetland types;3-D hybrid generative adversarial network;hybrid GAN;training sample issue;remote sensing imagery;complex wetland classification;conditional map unit;synthetic training samples;conventional wetland mapping;classification accuracy;CNN network;3-D hybrid CNN;CNN models","","1","","66","CCBY","26 Sep 2022","","","IEEE","IEEE Journals"
"Learning to Generate SAR Images With Adversarial Autoencoder","Q. Song; F. Xu; X. X. Zhu; Y. -Q. Jin","German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Data Science in Earth Observation, Technical University of Munich, Munich, Germany; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2022","2022","60","","1","15","Deep learning-based synthetic aperture radar (SAR) target recognition often suffers from sparsely distributed training samples and rapid angular variations due to scattering scintillation. Thus, data-driven SAR target recognition is considered a typical few-shot learning (FSL) task. This article first reviews the key issues of FSL and provides a definition of the FSL task. A novel adversarial autoencoder (AAE) is then proposed as an SAR representation and generation network. It consists of a generator network that decodes target knowledge to SAR images and an adversarial discriminator network that not only learns to discriminate “fake” generated images from real ones but also encodes the input SAR image back to target knowledge. The discriminator employs progressively expanding convolution layers and a corresponding layer-by-layer training strategy. It uses two cyclic loss functions to enforce consistency between the inputs and outputs. Moreover, rotated cropping is introduced as a mechanism to address the challenge of representing the target orientation. The moving and stationary Target recognition (MSTAR) 7-target dataset is used to evaluate the AAE’s performance, and the results demonstrate its ability to generate SAR images with aspect angular diversity. Using only 90 training samples with at least 25° of orientation interval, the trained AAE is able to generate the remaining 1748 samples of other orientation angles with an unprecedented level of fidelity. Thus, it can be used for data augmentation in SAR target recognition FSL tasks. Our experimental results show that the AAE could boost the test accuracy by 5.77%.","1558-0644","","10.1109/TGRS.2021.3086817","Natural Science Foundation of China(grant numbers:61991422,61822107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9461232","Adversarial autoencoder (AAE);deep learning (DL);few-shot learning (FSL);image representation;synthetic aperture radar (SAR)","Radar polarimetry;Training;Synthetic aperture radar;Task analysis;Target recognition;Generative adversarial networks;Generators","deep learning (artificial intelligence);image recognition;radar computing;radar imaging;radar target recognition;synthetic aperture radar","deep learning;sparsely distributed training samples;rapid angular variations;data-driven SAR target recognition;few-shot learning task;FSL task;adversarial autoencoder;generation network;generator network;adversarial discriminator network;input SAR image;target knowledge;target orientation;training samples;trained AAE;layer-by-layer training strategy;synthetic aperture radar target recognition;scattering scintillation;SAR representation;convolution layers;cyclic loss functions;rotated cropping;moving and stationary target recognition 7-target dataset;MSTAR 7-target dataset;data augmentation","","5","","34","IEEE","21 Jun 2021","","","IEEE","IEEE Journals"
"Poststack Seismic Data Compression Using a Generative Adversarial Network","K. S. d. S. Ribeiro; A. P. Schiavon; J. P. Navarro; M. B. Vieira; S. M. Villela; P. M. C. e Silva","Departamento de Ciência da Computação, UFJF—Universidade Federal de Juiz de Fora, Juiz de Fora—MG, Brazil; Departamento de Ciência da Computação, UFJF—Universidade Federal de Juiz de Fora, Juiz de Fora—MG, Brazil; NVIDIA, São Paulo, Brazil; Departamento de Ciência da Computação, UFJF—Universidade Federal de Juiz de Fora, Juiz de Fora—MG, Brazil; Departamento de Ciência da Computação, UFJF—Universidade Federal de Juiz de Fora, Juiz de Fora—MG, Brazil; NVIDIA, São Paulo, Brazil","IEEE Geoscience and Remote Sensing Letters","28 Dec 2021","2022","19","","1","5","This work presents a method for volumetric seismic data compression by coupling a 3-D convolution-based autoencoder to a generative adversarial network (GAN). The main challenge of 3-D convolutional autoencoders for data compression is how to fully exploit volumetric redundancy while keeping reasonable latent representation dimensions. Our method is based on a convolutional neural network for seismic data compression called 3DSC. Its encoder and decoder use 3-D convolutions and are connected by a latent representation with the same dimensions as its 2-D network counterparts. Our main hypothesis is that the 3DSC architecture can be improved by adversarial training. We, thus, propose a new 3-D-based seismic data compression method (3DSC-GAN) by coupling the 3DSC network to a GAN. The seismic data decoder is used as a generator of poststack data that are integrated with a discriminator module to better exploit 3-D redundancy. Results show that our method outperforms previous seismic data compression methods for very low target bit rates, increasing the peak signal-to-noise ratio (PSNR) with fairly high visual quality.","1558-0571","","10.1109/LGRS.2021.3103663","CAPES, FAPEMIG, and NVIDIA Corporation for the GPU Grant Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9515684","3-D poststack data;deep learning;generative adversarial networks (GANs);seismic data compression","Generators;Data compression;Generative adversarial networks;Training;Redundancy;Bit rate;Image coding","convolutional neural nets;data compression;deep learning (artificial intelligence);geophysical image processing;geophysical techniques;image coding;image representation;seismology;stereo image processing","3DSC-GAN;3DSC network;seismic data decoder;poststack seismic data compression;generative adversarial network;volumetric seismic data compression;3D convolutional autoencoders;volumetric redundancy;convolutional neural network;encoder;3DSC architecture;adversarial training;2D network counterparts;latent representation dimensions;3D-based seismic data compression;discriminator module;3D redundancy;peak signal-to-noise ratio;PSNR;visual quality;deep learning","","","","17","IEEE","17 Aug 2021","","","IEEE","IEEE Journals"
"Asymmetric Training of Generative Adversarial Network for High Fidelity SAR Image Generation","Y. Huang; W. Mei; S. Liu; T. Li","School of Software Engineering Chongqing University of Posts and Telecommunications, Chongqing, China; School of Software Engineering Chongqing University of Posts and Telecommunications, Chongqing, China; School of Software Engineering Chongqing University of Posts and Telecommunications, Chongqing, China; School of Software Engineering Chongqing University of Posts and Telecommunications, Chongqing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1576","1579","In practical application, the research of synthetic aperture radar (SAR) target recognition has fallen into a bottle-neck due to the lack of samples. Data argumentation methods based on generative adversarial networks (GAN) have received widespread attention in solving this type of few-shot sample problem. However, the generated images suffer from various shortcomings, such as lack of diversity, low signal-to-noise ratio, blur, etc. In this article, the VAE-WGANGP is proposed, which combines GAN and variational autoencoder (VAE) to alleviate these shortcomings. The innovations of this paper are as follows: firstly, the generator of GAN is replaced with VAE, which constructs an asymmetric network ensuring the stability of GAN training; secondly, the asymmetric loss function is composed of four parts, including reconstruction loss, divergence loss, adversarial loss, and gradient penalty. In this way, the problem of gradient explosion or gradient disappearance is alleviated. The experimental results with the MSTAR dataset show that the images generated by the proposed model outperform the advanced technology with many similar deep features and achieve significant improvement in the target recognition accuracy rate.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884284","Generative adversarial network (GAN);variational autoencoder (VAE);synthetic aperture radar (SAR);data augmentation;automatic target recognition (ATR)","Training;Technological innovation;Target recognition;Azimuth;Generative adversarial networks;Radar polarimetry;Stability analysis","image denoising;radar imaging;radar resolution;synthetic aperture radar","low signal-to-noise ratio;VAE-WGANGP;asymmetric network;GAN training;asymmetric loss function;adversarial loss;target recognition accuracy rate;asymmetric training;generative adversarial network;high fidelity SAR image generation;synthetic aperture radar target recognition;bottle-neck;data argumentation methods;few-shot sample problem","","1","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Generative Adversarial Network for SAR-to-Optical Image Translation with Feature Cross-Fusion Inference","J. Wei; H. Zou; L. Sun; X. Cao; M. Li; S. He; S. Liu","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6025","6028","The translation of synthetic aperture radar (SAR) to optical images provides a new solution for the interpretation of SAR images. Most of the existing translation networks are based on generative adversarial networks and use 9-residual blocks or U-Net structures in the feature inference phase. Both structures cause a large amount of information lost during the conversion of SAR image features to optical features, making the outline of the translated image blurred or semantic information lost. Aiming at this problem, this paper proposes a cross-fusion inference network structure, which preserves both high-resolution features and low-resolution features in the whole process of feature inference. Our proposed method broadens the network horizontally while deepening it vertically and improving the image translation performance. The experiments conducted on the public dataset sen1-2 show that the proposed method is superior to other networks.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884166","National Natural Science Foundation of China(grant numbers:62071474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884166","SAR-to-optical image translation;Generative adversarial network (GAN);Cross-fusion inference structure","Optical losses;Laser radar;Semantics;Optical fiber networks;Optical imaging;Generative adversarial networks;Radar polarimetry","feature extraction;image fusion;image resolution;image restoration;inference mechanisms;neural nets;optical images;radar imaging;radar resolution;synthetic aperture radar","generative adversarial network;SAR-to-optical image translation;synthetic aperture radar;optical images;U-Net structures;SAR image features;cross-fusion inference network structure;high-resolution features;low-resolution features;translated image blurred;semantic information lost","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"GAN-Based SAR-to-Optical Image Translation with Region Information","K. Doi; K. Sakurada; M. Onishi; A. Iwasaki","National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; The University of Tokyo, Japan","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2069","2072","In this paper, we propose a SAR-to-optical image translation method based on conditional generative adversarial networks (cGANs). Though cGANs have achieved great success in image translation, some problems remain in SAR-to-optical image translation. One of the problems is the colorization error owing to the lack of color information in SAR data. Since the colors of optical images are varied, while SAR images have no color information, the generator network is confused and fail to generate correctly colorized optical images. To prevent it, we introduce a region information to the image translation network. Specifically, the feature vector from the pre-trained classification network is fed to the generator and discriminator network. Experimental results with SEN1-2 dataset show the advantage of our proposed method over the baseline method that does not use any additional information.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323085","SAR;Optical remote sensing;Generative adversarial network (GAN);Image translation","Optical imaging;Adaptive optics;Radar polarimetry;Optical sensors;Generators;Synthetic aperture radar;Integrated optics","feature extraction;image colour analysis;learning (artificial intelligence);optical images;radar imaging;synthetic aperture radar","pre-trained classification network;discriminator network;GAN-based SAR-to-optical image translation;region information;SAR-to-optical image translation method;conditional generative adversarial networks;cGANs;problems remain;colorization error;color information;SAR data;SAR images;generator network;correctly colorized optical images;image translation network","","12","","12","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Synthesizing Optical and SAR Imagery From Land Cover Maps and Auxiliary Raster Data","G. Baier; A. Deschemps; M. Schmitt; N. Yokoya","RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; SERPICO Team of Inria, Bretagne-Atlantique, Rennes, France; Department of Geoinformatics, Munich University of Applied Sciences, Munich, Germany; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan","IEEE Transactions on Geoscience and Remote Sensing","20 Dec 2021","2022","60","","1","12","We synthesize both optical RGB and synthetic aperture radar (SAR) remote sensing images from land cover maps and auxiliary raster data using generative adversarial networks (GANs). In remote sensing, many types of data, such as digital elevation models (DEMs) or precipitation maps, are often not reflected in land cover maps but still influence image content or structure. Including such data in the synthesis process increases the quality of the generated images and exerts more control on their characteristics. Spatially adaptive normalization layers fuse both inputs and are applied to a full-blown generator architecture consisting of encoder and decoder to take full advantage of the information content in the auxiliary raster data. Our method successfully synthesizes medium (10 m) and high (1 m) resolution images when trained with the corresponding data set. We show the advantage of data fusion of land cover maps and auxiliary information using mean intersection over unions (mIoUs), pixel accuracy, and Fréchet inception distances (FIDs) using pretrained U-Net segmentation models. Handpicked images exemplify how fusing information avoids ambiguities in the synthesized images. By slightly editing the input, our method can be used to synthesize realistic changes, i.e., raising the water levels. The source code is available at https://github.com/gbaier/rs_img_synth, and we published the newly created high-resolution data set at https://ieee-dataport.org/open-access/geonrw.","1558-0644","","10.1109/TGRS.2021.3068532","Japan Society for the Promotion of Science through KAKENHI(grant numbers:18K18067,20K19834); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406194","Deep learning;generative adversarial network (GAN);image synthesis;synthetic aperture radar (SAR)","Generators;Semantics;Remote sensing;Image synthesis;Radar polarimetry;Image segmentation;Training","geophysical image processing;image segmentation;land cover;synthetic aperture radar;terrain mapping","land cover maps;auxiliary raster data;synthetic aperture radar remote sensing images;precipitation maps;image content;high resolution images;data fusion;synthesized images;high-resolution data;optical Imagery;SAR Imagery;optical RGB;generative adversarial networks;size 1.0 m;size 10.0 m","","7","","49","IEEE","16 Apr 2021","","","IEEE","IEEE Journals"
"A semi-supervised image segmentation method based on generative adversarial network","W. Nie; P. Gou; Y. Liu; T. Zhou; N. Xu; P. Wang; Q. Du","Beijing Big Data Advanced Technology Institute, Beijing; Beijing Big Data Advanced Technology Institute, Beijing; Beijing Big Data Advanced Technology Institute, Beijing; Beijing Big Data Advanced Technology Institute, Beijing; Department of Geography, University of California, Los Angles, USA; Beijing Big Data Advanced Technology Institute, Beijing; State Key Laboratory of Tibetan Plateau Earth System, Resources and Environment, Institute of Tibetan Plateau Research, Chinese Academy of Sciences, Beijing, China","2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Aug 2022","2022","10","","1217","1223","Semantic segmentation based on deep learning is an important research direction in computer vision, and it has developed rapidly in intelligent interpretation of remote sensing images in recent years. However, semantic segmentation based on deep learning is also a long-standing challenge in remote sensing image applications. The main reason is that training a neural network requires a large amount of pixel-level labeled data, and these publicly available labeled data are usually few. It requires a lot of labor costs. To address this issue, this paper proposes a semi-supervised semantic segmentation method based on Generative adversarial networks (GAN). Compared with previous techniques, we extends typical GAN networks to pixel-level prediction and discrimination, and applies it to remote sensing image semantic segmentation. We introduce residual networks and dilated convolutions into the generator, and employ a flow alignment module (FAM) to learn the semantic flow between adjacent hierarchical feature maps. The connected discriminator formulates the training as the min-maximum optimization problem. Comprehensive quantitative and qualitative evaluations on multiple models show that our proposed method outperforms state-of-the-art semi-supervised image segmentation algorithms.","2693-2865","978-1-6654-2207-9","10.1109/ITAIC54216.2022.9836504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836504","semi-supervised;generative adversarial network;remote sensing;image segmentation","Training;Deep learning;Image segmentation;Semantics;Generative adversarial networks;Robustness;Sensors","computer vision;deep learning (artificial intelligence);geophysical image processing;image classification;image segmentation;minimax techniques;remote sensing","residual networks;semantic flow;semisupervised image segmentation method;generative adversarial network;deep learning;computer vision;neural network training;pixel-level labeled data;semisupervised semantic segmentation method;typical GAN networks;pixel-level prediction;remote sensing image semantic segmentation;dilated convolution;flow alignment module;FAM;adjacent hierarchical feature map;connected discriminator;min-maximum optimization problem","","","","31","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"Enhancing ISAR Resolution by a Generative Adversarial Network","D. Qin; X. Gao","College of Electronic Science, National University of Defense Technology, Changsha, China; College of Electronic Science, National University of Defense Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","23 Dec 2020","2021","18","1","127","131","Recent studies have shown the superiority of neural networks on imaging equality and efficiency in inverse synthetic aperture radar (ISAR) resolution enhancement, but a central problem remains largely unsolved: all recent studies based on neural networks focused on minimizing the mean-squared reconstruction error (MSE), causing limited enhancing factors and inaccurate recovery of weak point scatters. In order to address this problem, a framework based on a generative adversarial network (GAN) using a combined loss composed of the absolute loss and the adversarial loss is proposed in this letter. The absolute loss ensures that reconstructed high-resolution ISAR images achieve higher enhancing factors and lower sidelobes. The adversarial loss pushes this framework to recover accurate amplitude and position of weak point scatters by a discriminator that is trained to differentiate reconstructed high-resolution ISAR images and real high-resolution ISAR images. Compared to some state-of-the-art methods, our GAN-based framework provides superior reconstruction with higher enhancing factors and more target details.","1558-0571","","10.1109/LGRS.2020.2965743","National Natural Science Foundation of China(grant numbers:61571450); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972359","Combined loss;generative adversarial network (GAN);higher enhancing factor;inverse synthetic aperture radar (ISAR) resolution enhancement;lower sidelobe;weak point scatters","Image resolution;Training;Generators;Image reconstruction;Generative adversarial networks;Imaging;Gallium nitride","image enhancement;image reconstruction;image resolution;mean square error methods;neural nets;radar imaging;synthetic aperture radar","GAN-based framework;ISAR resolution;generative adversarial network;neural networks;inverse synthetic aperture radar resolution enhancement;mean-squared reconstruction error;absolute loss;adversarial loss;reconstructed high-resolution ISAR images","","13","","21","IEEE","28 Jan 2020","","","IEEE","IEEE Journals"
"Superresolution Land Cover Mapping Using a Generative Adversarial Network","C. Shang; X. Li; G. M. Foody; Y. Du; F. Ling","University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; School of Geography, University of Nottingham, Nottingham, U.K.; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","24 Dec 2021","2022","19","","1","5","Superresolution mapping (SRM) is a commonly used method to cope with the problem of mixed pixels when predicting the spatial distribution within low-resolution pixels. Central to the popular SRM method is the spatial pattern model, which is utilized to represent the land cover spatial distribution within mixed pixels. The use of an inappropriate spatial pattern model limits such SRM analyses. Alternative approaches, such as deep-learning-based algorithms, which learn the spatial pattern from training data through a convolutional neural network, have been shown to have considerable potential. Deep learning methods, however, are limited by issues such as the way the fraction images are utilized. Here, a novel SRM model based on a generative adversarial network (GAN), GAN-SRM, is proposed that uses an end-to-end network to address the main limitations of existing SRM methods. The potential of the proposed GAN-SRM model was assessed using four land cover subsets and compared to hard classification and several popular SRM methods. The experimental results show that of the set of methods explored, the GAN-SRM model was able to generate the most accurate high-resolution land cover maps.","1558-0571","","10.1109/LGRS.2020.3020395","Innovation Group Project of Hubei Natural Science Foundation(grant numbers:2019CFA019); Hubei Province Natural Science Fund for Distinguished Young Scholars(grant numbers:2018CFA062); Natural Science Foundation of China(grant numbers:61671425); Youth Innovation Promotion Association CAS(grant numbers:2017384); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9195742","Deep learning;generative adversarial network (GAN);super-resolution mapping (SRM)","Generative adversarial networks;Training data;Layout;Spatial resolution;Training;Gallium nitride","","","","4","","26","IEEE","14 Sep 2020","","","IEEE","IEEE Journals"
"Large-Scale Point Cloud Contour Extraction via 3-D-Guided Multiconditional Residual Generative Adversarial Network","Y. Zhang; Z. Liu; T. Liu; B. Peng; X. Li; Q. Zhang","School of Electronic Science, National University of Defense Technology, Changsha, China; School of Electronic Science, National University of Defense Technology, Changsha, China; School of Electronic Science, National University of Defense Technology, Changsha, China; School of Electronic Science, National University of Defense Technology, Changsha, China; School of Electronic Science, National University of Defense Technology, Changsha, China; School of Business, University of Leeds, Leeds, U.K.","IEEE Geoscience and Remote Sensing Letters","27 Dec 2019","2020","17","1","142","146","As one of the most important features for human perception, contours are widely applied in graphics and mapping applications. However, it is considerably challenging to extract contours from large-scale point clouds due to the irregular distribution of point clouds. In this letter, we propose a 3-D-guided multiconditional residual generative adversarial network (3-D-GMRGAN), the first deep-learning framework to generate contours for large-scale outdoor point clouds. To make the network handle huge amounts of points, we operate contours in the parametric space rather than raw point space, associated with a parametric chamfer distance. Then, to gather contour features from potential positions and avoid the huge solution space, we propose a guided residual generative adversarial framework, by utilizing a simple feature-based method to get the “over extraction” potential contour distribution. Experiments demonstrate that the proposed method is able to generate contours efficiently for large-scale point clouds, with fewer outliers and pseudo contours compared with state-of-the-art approaches.","1558-0571","","10.1109/LGRS.2019.2917319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733065","Contour extraction;generative adversarial network;large-scale point clouds","Three-dimensional displays;Feature extraction;Generators;Surface reconstruction;Task analysis;Training;Generative adversarial networks","edge detection;feature extraction;geophysical image processing;learning (artificial intelligence);solid modelling","pseudocontours;over extraction potential contour distribution;guided residual generative adversarial framework;huge solution space;contour features;raw point space;large-scale outdoor point clouds;large-scale point clouds;mapping applications;3-D-guided multiconditional residual generative adversarial network;scale point cloud contour extraction","","4","","26","IEEE","10 Jun 2019","","","IEEE","IEEE Journals"
"Seismic Surface-Related Multiples Suppression Based on SAGAN","L. Tao; H. Ren; Y. Ye; J. Jiang","Key Laboratory of Geoscience Big Data and Deep Resource of Zhejiang Province, School of Earth Sciences, Zhejiang University, Hangzhou, China; Key Laboratory of Geoscience Big Data and Deep Resource of Zhejiang Province, School of Earth Sciences, Zhejiang University, Hangzhou, China; Petro China HangZhou Research Institute of Geology, Hangzhou, China; Key Laboratory of Geoscience Big Data and Deep Resource of Zhejiang Province, School of Earth Sciences, Zhejiang University, Hangzhou, China","IEEE Geoscience and Remote Sensing Letters","29 Apr 2022","2022","19","","1","5","Suppressing multiples from seismic records is necessary to improve imaging quality. Deep neural networks (DNNs) can automatically mine features from data. Once a network is successfully trained, it can process data with extremely high efficiency. In this letter, a generative adversarial network (GAN) framework is proposed to remove surface-related multiples in both synthetic and field datasets, where the generator is U-Net with Markov discriminator. Adding self-attention (SA) blocks to GAN improves processing precision. Improved signal noise ratio (SNR), and accurate reverse time migration (RTM) images implemented by network’s outputs of synthetic datasets, jointly support that this network is effective on surface-related multiple suppression. Based on the results from field application, deep learning method in this letter is comparable to conventional adaptive surface-related multiple elimination (SRME) method but time-saving. By constructing an end-to-end workflow for seismic surface-related multiples suppression, small batches dataset can be used to train the network, and large batches of datasets can be processed accurately and efficiently.","1558-0571","","10.1109/LGRS.2022.3168143","National Natural Science Foundation of China(grant numbers:42074135,41874164); Natural Science Foundation of Zhejiang Province, China(grant numbers:LY19D040002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9758713","Data processing;deep neural network (DNN);generative adversarial network (GAN);self-attention (SA);surface-related multiple","Training;Mathematical models;Surface impedance;Generative adversarial networks;Surface waves;Surface treatment;Signal to noise ratio","data analysis;deep learning (artificial intelligence);geophysical image processing;geophysical techniques;seismic waves;seismology","generative adversarial network framework;GAN;synthetic field datasets;improved signal noise ratio;reverse time migration images;conventional adaptive surface-related;seismic surface-related multiples suppression;seismic records;deep neural networks;U-Net;Markov discriminator;synthetic datasets;deep learning method;conventional adaptive surface-related multiple elimination","","3","","20","IEEE","18 Apr 2022","","","IEEE","IEEE Journals"
"An Improved SRGAN Based Ambiguity Suppression Algorithm for SAR Ship Target Contrast Enhancement","J. Ai; G. Fan; Y. Mao; J. Jin; M. Xing; H. Yan","Key Laboratory of Radar Imaging and Microwave Photonics, Nanjing University of Aeronautics and Astronautics, Ministry of Education, Nanjing, China; Intelligent Interconnected Systems Laboratory of Anhui, Hefei University of Technology, Hefei, China; Intelligent Interconnected Systems Laboratory of Anhui, Hefei University of Technology, Hefei, China; Intelligent Interconnected Systems Laboratory of Anhui, Hefei University of Technology, Hefei, China; Academy of Advanced Interdisciplinary Research, Xidian University, Xi’an, China; Key Laboratory of Radar Imaging and Microwave Photonics, Nanjing University of Aeronautics and Astronautics, Ministry of Education, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Due to the specific characteristics of synthetic aperture radar (SAR), there will be ambiguity interference in SAR images, resulting in low contrast of the ship target to the clutter. This letter proposes an improved super-resolution generative adversarial network (ISRGAN) based ambiguity suppression algorithm for SAR ship target contrast enhancement. The proposed ISRGAN is the first attempt of using GAN for SAR ambiguity suppression. As a post-processing procedure, it does not need prior information of SAR systems, so it can be applied to various observation scenes and different acquisition modes. The generator of ISRGAN embeds the residual dense network (RDN) to optimally fuse the global and local features of the image, and it effectively improves the completeness of the feature information used for SAR ship target contrast enhancement. The superiority of ISRGAN on ambiguity suppression is validated on the Chinese Gaofen-3 imagery.","1558-0571","","10.1109/LGRS.2021.3111553","National Natural Science Foundation of China(grant numbers:62071164,61701157); China Postdoctoral Science Foundation(grant numbers:2020T130165); open fund of Key Laboratory of Radar Imaging and Microwave Photonics, Nanjing University of Aeronautics and Astronautics, Ministry of Education(grant numbers:NJ20210008); Fundamental Research Funds for the Central Universities of China(grant numbers:JZ2020HGTB0012,PA2021AKSK0113); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9539247","Azimuth ambiguity suppression;improved super-resolution generative adversarial network (ISRGAN);synthetic aperture radar (SAR);target contrast enhancement","Feature extraction;Synthetic aperture radar;Convolution;Marine vehicles;Radar polarimetry;Generators;Generative adversarial networks","image enhancement;image resolution;object detection;radar imaging;ships;synthetic aperture radar","SAR ship target contrast enhancement;ISRGAN;improved SRGAN based ambiguity suppression algorithm;SAR images;super-resolution generative adversarial network based ambiguity suppression algorithm;SAR ambiguity suppression;SAR systems","","3","","15","IEEE","16 Sep 2021","","","IEEE","IEEE Journals"
"DeepDT: Generative Adversarial Network for High-Resolution Climate Prediction","J. Cheng; J. Liu; Q. Kuang; Z. Xu; C. Shen; W. Liu; K. Zhou","School of Computer, Wuhan University, Wuhan, China; School of Computer, Wuhan University, Wuhan, China; Public Weather Services Center, China Meteorological Administration (CMA), Beijing, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, China; School of Computer, Wuhan University, Wuhan, China; School of Computer, Wuhan University, Wuhan, China; School of Computer, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","15 Dec 2021","2022","19","","1","5","Climate prediction is susceptible to a variety of meteorological factors, and downscaling technology is used for high-resolution climate prediction. This technology can generate small-scale regional climate prediction from large-scale climate output information. Inspired by the concept of image super resolution, we propose to apply the convolutional neural network (CNN) to downscaling technology. However, some unpleasant artifacts always appear in the final climate images generated by existing CNN-based models. To further eliminate these unpleasant artifacts, we present a new training strategy for the generative adversarial network, termed DeepDT. The key idea of our DeepDT is to train a generator and a discriminator separately. More specifically, we apply the residual-in-residual dense block as the basic frame structure to fully extract the features of the input. Additionally, we innovatively use a CNN model to fuse multiple climate elements to generate trainable climate images, and build a high-quality climate data set. Finally, we evaluate the DeepDT using the proposed climate data sets, and the experiments indicate that DeepDT performs best compared to most CNN-based models in climate prediction.","1558-0571","","10.1109/LGRS.2020.3041760","National Key Research and Development Program of China(grant numbers:2018YFC1507801); National Science Foundation of China (NSFC)(grant numbers:61972290); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0226); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383802","Climate prediction;generative adversarial network;image super resolution","Meteorology;Generators;Predictive models;Feature extraction;Data models;Training;Generative adversarial networks","","","","3","","15","IEEE","23 Mar 2021","","","IEEE","IEEE Journals"
"Deep Mutual GAN for Life-Detection Radar Super Resolution","H. Xing; M. Bao; Y. Li; L. Shi; M. Xing","School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2021","2022","19","","1","5","To improve the life-detection radar resolution under certain hardware conditions, in this letter, a deep mutual learning generative adversarial network model (Deep Mutual GAN) is proposed. In the proposed model, the generator can improve the angular resolution of the input low-resolution radar image by five times, which is enough to meet our requirements for the resolution of life detection. We innovatively use two generators in GAN with the same network structure and make the two generators learn from each other. In this way, the learning process of a generator is not only achieved by its confrontation with the discriminator but also guided by another generator. As a result, the knowledge of the generator is no longer only obtained through its own learning; each generator learns knowledge from another generator while learning knowledge by itself. The proposed model can effectively make the convergence of GAN more stable and improves the super resolution effect. We also introduce the details of the network structure of generator and discriminator, in which residual learning and a symmetrical network structure are applied. The experimental results show that the proposed method can achieve state-of-the-art imaging effect, which is meaningful for subsequent target detection and recognition.","1558-0571","","10.1109/LGRS.2021.3065696","National Key Research and Development Program of China(grant numbers:2018YFC0810202); Fundamental Research Funds for the Central Universities(grant numbers:JB190204); Science Foundation for Distinguished Young Scholars of Shaanxi Province(grant numbers:2020JC-25); Shaanxi Innovative Talents Promotion Plan-Science and Technology Innovation Team(grant numbers:2019TD-002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9389759","Deep mutual learning;generative adversarial network (GAN);life-detection radar imaging;super resolution (SR)","Generators;Radar imaging;Gallium nitride;Generative adversarial networks;Radar;Radar antennas;Receiving antennas","image resolution;learning (artificial intelligence);neural nets;object detection;radar imaging;radar resolution","life detection;learning process;super resolution effect;residual learning;symmetrical network structure;subsequent target detection;Deep Mutual GAN;life-detection radar super resolution;life-detection radar resolution;hardware conditions;deep mutual learning generative adversarial network model;angular resolution;input low-resolution radar image","","1","","17","IEEE","30 Mar 2021","","","IEEE","IEEE Journals"
"Recognition-Aware HRRP Generation With Generative Adversarial Network","Y. Huang; Y. Wen; L. Shi; X. Ding","School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China","IEEE Geoscience and Remote Sensing Letters","15 Dec 2021","2022","19","","1","5","Existing works on radar high-resolution range profile (HRRP) recognition commonly focus on utilizing data and various deep learning models in achieving high classification accuracy. However, in practical applications, it is often difficult to obtain HRRP signals, especially for noncooperative targets. Such lack of data dramatically decreases the recognition performance, so this letter applies data augmentation to address small-sample problems. A recognition-aware HRRP generation framework based on a generative adversarial network is proposed for data augmentation, which generates discriminative samples by decomposing and reorganizing signal’s characteristics. The proposed model increases the generated signals’ discriminative power, thus meeting the application requirements. Experiments show that the generated HRRP signals can not only accurately expand the data set but also improve the recognition system’s performance. Besides, the developed model outperforms traditional data augmentation methods and other generative methods. To the best of our knowledge, this is the first work on HRRP signal generation in radar automatic target recognition systems.","1558-0571","","10.1109/LGRS.2021.3056192","National Natural Science Foundation of China(grant numbers:81671766,61971369,U19B2031,U1605252,61671309); Open Fund of Science and Technology on Automatic Target Recognition Laboratory(grant numbers:6142503190202); Fundamental Research Funds for the Central Universities(grant numbers:20720180059,20720190116,20720200003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354214","Data generation;generative adversarial network (GAN);high-resolution range profile (HRRP);target recognition","Data models;Target recognition;Semantics;Image reconstruction;Generative adversarial networks;Encoding;Training","","","","1","","22","IEEE","12 Feb 2021","","","IEEE","IEEE Journals"
"LDGAN: A Synthetic Aperture Radar Image Generation Method for Automatic Target Recognition","C. Cao; Z. Cao; Z. Cui","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Center for Information Geoscience, UESTC, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2020","2020","58","5","3495","3508","Under the framework of a supervised learning-based automatic target recognition (ATR) approach, recognition performance is primarily dependent on the amount of training samples. However, shortage in training samples is a consistent issue for ATR. In this article, we propose a new image to image generation method, called label-directed generative adversarial networks (LDGANs), which will provide labeled samples to be used for recognition model training. We define an entirely new loss function for the LDGAN, which utilizes the Wasserstein distance to replace the original distance measurement of the conventional generative adversarial networks (GANs), thus efficiently avoiding the collapse mode problem. The label information is also added to the loss function of the LDGAN to avoid generating a large number of unlabeled target images. More importantly, the proposed method also makes corresponding changes to the network architecture regarding the new GANs. At the same time, the detailed algorithm about the LDGAN is also introduced in this article to deal with the issue that characteristically GANs are not easy to train. Based on comparisons with other directed generation methods, the experimental results show comparative results of several types of generated images in statistical features, gradient features, classic features of synthetic aperture radar (SAR) targets and the independence from the real image. While demonstrating that the images generated by the LDGAN produced better results using the assumptions of independent and identical distribution, the experiment also explores the performance of the generated image in the ATR. A comparison of these experimental results demonstrates a better way to use the generated image for ATR. The experimental results also prove that the proposed method does have the ability to supplement information for ATR when the training sample information is insufficient.","1558-0644","","10.1109/TGRS.2019.2957453","National Natural Science Foundation of China(grant numbers:61801098); Fundamental Research Funds for the Central Universities(grant numbers:2672018ZYGX2018J013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938701","Automatic target recognition (ATR);information supplement;label-directed generative adversarial network (LDGAN);synthetic aperture radar (SAR) image","Gallium nitride;Radar polarimetry;Image recognition;Synthetic aperture radar;Target recognition;Training;Image synthesis","feature extraction;learning (artificial intelligence);neural nets;radar computing;radar imaging;radar target recognition;synthetic aperture radar","label-directed generative adversarial networks;supervised learning-based automatic target recognition approach;synthetic aperture radar image generation method;training sample information;synthetic aperture radar targets;directed generation methods;unlabeled target images;label information;GAN;conventional generative adversarial networks;loss function;recognition model training;labeled samples;LDGAN;training samples;recognition performance;ATR","","22","","32","IEEE","23 Dec 2019","","","IEEE","IEEE Journals"
"Residual Encoder–Decoder Conditional Generative Adversarial Network for Pansharpening","Z. Shao; Z. Lu; M. Ran; L. Fang; J. Zhou; Y. Zhang","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1573","1577","Due to the limitation of the satellite sensor, it is difficult to acquire a high-resolution (HR) multispectral (HRMS) image directly. The aim of pansharpening (PNN) is to fuse the spatial in panchromatic (PAN) with the spectral information in multispectral (MS). Recently, deep learning has drawn much attention, and in the field of remote sensing, several pioneering attempts have been made related to PNN. However, the big size of remote sensing data will produce more training samples, which require a deeper neural network. Most current networks are relatively shallow and raise the possibility of detail loss. In this letter, we propose a residual encoder-decoder conditional generative adversarial network (RED-cGAN) for PNN to produce more details with sharpened images. The proposed method combines the idea of an autoencoder with generative adversarial network (GAN), which can effectively preserve the spatial and spectral information of the PAN and MS images simultaneously. First, the residual encoder-decoder module is adopted to extract the multiscale features from the last step to yield pansharpened images and relieve the training difficulty caused by deepening the network layers. Second, to further enhance the performance of the generator to preserve more spatial information, a conditional discriminator network with the input of PAN and MS images is proposed to encourage that the estimated MS images share the same distribution as that of the referenced HRMS images. The experiments conducted on the Worldview2 (WV2) and Worldview3 (WV3) images demonstrate that our proposed method provides better results than several state-of-the-art PNN methods.","1558-0571","","10.1109/LGRS.2019.2949745","National Natural Science Foundation of China(grant numbers:61671312,61922029); Sichuan Science and Technology Program(grant numbers:2018HH0070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894479","Deep learning;generative adversarial network (GAN);multispectral (MS) image;panchromatic (PAN);pansharpening (PNN)","Feature extraction;Generative adversarial networks;Gallium nitride;Training;Generators;Task analysis;Decoding","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","residual encoder-decoder conditional generative adversarial network;high-resolution multispectral image;PNN;PAN;remote sensing data;deeper neural network;spectral information;residual encoder-decoder module;conditional discriminator network;referenced HRMS images;Worldview3 images;MS image estimation;Worldview2 images;RED-cGAN;satellite sensor;pansharpening;spatial fusion;feature extraction","","36","","22","IEEE","8 Nov 2019","","","IEEE","IEEE Journals"
"Robust Signature-Based Hyperspectral Target Detection Using Dual Networks","Y. Gao; Y. Feng; X. Yu; S. Mei","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","6 Feb 2023","2023","20","","1","5","The training of deep networks for hyperspectral target detection (HTD) is usually confronted with the problem of limited samples and in extreme cases, there might be only one target sample available. To address this challenge, we propose a novel approach with dual networks in this letter. First, a training set that is not fully accurate but representative enough regarding both targets and backgrounds is built through predetection and clustering. Then, two types of neural networks, that is, one generative adversarial network (GAN) and one convolutional neural network (CNN), which focus on spectral and spatial features of hyperspectral images (HSIs), are utilized for target detection. After that, the results of the two networks are fused, with the final detection result obtained. Experiments on real HSIs indicate that the proposed approach manages to perform HTD with only one target sample and is able to yield a more robust detection performance compared to other approaches.","1558-0571","","10.1109/LGRS.2023.3237746","National Natural Science Foundation of China(grant numbers:62171381); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018390","Convolutional neural network (CNN);generative adversarial network (GAN);hyperspectral target detection (HTD)","Training;Detectors;Object detection;Neural networks;Generative adversarial networks;Hyperspectral imaging;Feature extraction","","","","","","15","IEEE","16 Jan 2023","","","IEEE","IEEE Journals"
"Open-Set Patient Activity Recognition With Radar Sensors and Deep Learning","G. Bhavanasi; L. Werthen-Brabants; T. Dhaene; I. Couckuyt","Department of Information Technology (INTEC), Internet Technology and Data Science Lab (IDLab) Research Group, Ghent University-imec, Ghent, Belgium; Department of Information Technology (INTEC), Internet Technology and Data Science Lab (IDLab) Research Group, Ghent University-imec, Ghent, Belgium; Department of Information Technology (INTEC), Internet Technology and Data Science Lab (IDLab) Research Group, Ghent University-imec, Ghent, Belgium; Department of Information Technology (INTEC), Internet Technology and Data Science Lab (IDLab) Research Group, Ghent University-imec, Ghent, Belgium","IEEE Geoscience and Remote Sensing Letters","2 Feb 2023","2023","20","","1","5","Open-set recognition (OSR) has achieved significant importance in recent years. For a robust recognition system, we need to identify the right class from a myriad of knowns and unknowns. In this work, we build and compare OSR systems for patient activity recognition (PAR) using compact radar sensors in a hospital setting. Radar sensors are an important part of a privacy-preserving monitoring system. Specifically, the proposed approach is based on a deep discriminative representation network (DDRN) trained using the large margin cosine loss (LMCL) and triplet loss (TL). A probability of an inclusion model in the embedding space based on the Weibull distribution is able to separate knowns from unknowns. This overall approach limits the risk of open space and enables us to easily identify any unknown activities. Our experiments show that the proposed approach is significantly better for open-set human activity recognition (HAR) with radar when compared with the state-of-the-art open-set approaches.","1558-0571","","10.1109/LGRS.2023.3235243","Ghent University–imec and the Flemish Government under the “Onderzoeksprogramma Artificiële Intelligentie (AI) Vlaanderen” program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10011414","Deep learning (DL);extreme value theory (EVT);human activity recognition (HAR);large margin cosine loss (LMCL);open-set recognition (OSR);radar sensors;triplet loss (TL)","Radar;Sensors;Human activity recognition;Weibull distribution;Biomedical imaging;Generative adversarial networks;Training","","","","","","18","IEEE","9 Jan 2023","","","IEEE","IEEE Journals"
"Turbidite Probability Scenarios Generation Combining Generative Models and Geostatistical Techniques","E. Sarruf; A. N. Caseri; S. Pesco","Mathematics Department, Pontifícia Universidade Católica do Rio de Janeiro, Rio de Janeiro, Brazil; Mathematics Department, Pontifícia Universidade Católica do Rio de Janeiro, Rio de Janeiro, Brazil; Mathematics Department, Pontifícia Universidade Católica do Rio de Janeiro, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","13 Jul 2022","2022","19","","1","5","The identification of characteristics of turbidite deposits is important for the oil and gas industry, as they represent possible oil reservoirs. The indirect measurement of turbidites through sonars can be a cause of several uncertainties in the reservoir data. This work aims to improve the data of turbidity reservoirs and quantify their uncertainties. For this, a probabilistic sandstone map of a turbiditical basin was used and a method based on geostatistics, kriging, and machine learning techniques, especially SinGAN, was developed to generate an ensemble of probabilistic sandstone maps. In order to analyze the performance of the solution, evaluation metrics such as continuous ranked probability score (CRPS) and receiver operator characteristic’s area under the curve (ROC AUC) were used. The results showed that the method developed has high potential of producing trustworthy maps from a single original data and that it can be a solution to improve the measurement of probabilistic reservoirs.","1558-0571","","10.1109/LGRS.2022.3188219","CNPq, CAPES, and Petrobras(grant numbers:10.13039/501100004225-PETROBRAS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9813694","Computational modeling;geostatistics;machine learning;turbidite reservoirs","Uncertainty;Reservoirs;Generative adversarial networks;Reliability;Probabilistic logic;Oils;Neural networks","gas industry;geophysical techniques;geophysics computing;hydrocarbon reservoirs;learning (artificial intelligence);neural nets;probability;production engineering computing;statistical analysis","geostatistical techniques;turbidite deposits;gas industry;oil reservoirs;reservoir data;turbidity reservoirs;probabilistic sandstone map;turbiditical basin;geostatistics;machine learning techniques;single original data;probabilistic reservoirs;turbidite probability scenarios generation;generative models;ROC AUC;receiver operator characteristics area under the curve;continuous ranked probability score;CRPS;SinGAN","","","","20","IEEE","4 Jul 2022","","","IEEE","IEEE Journals"
"D-SRCAGAN : DEM Super-resolution Generative Adversarial Network","X. Deng; W. Hua; X. Liu; S. Chen; W. Zhang; J. Duan","School of Geography and Information Engineering, China University of Geosciences (Wuhan), Wuhan, China; School of Geography and Information Engineering, China University of Geosciences (Wuhan), Wuhan, China; School of Geography and Information Engineering, China University of Geosciences (Wuhan), Wuhan, China; School of Geography and Information Engineering, China University of Geosciences (Wuhan), Wuhan, China; School of Geography and Information Engineering, China University of Geosciences (Wuhan), Wuhan, China; School of Geography and Information Engineering, China University of Geosciences (Wuhan), Wuhan, China","IEEE Geoscience and Remote Sensing Letters","","2022","PP","99","1","1","High-resolution digital elevation models (DEMs) are widely used in many fields such as mapping, hydrology, meteorology and geology, where they can improve the accuracy and reliability of many geographic analysis applications as an input. However, due to the high cost and difficulty of acquiring high-resolution DEMs, as well as the problems of edge smoothing, data distortion and fractures in reconstructed ground surfaces with traditional super-resolution DEM reconstruction techniques. Inspired by the excellence of generative adversarial neural networks in super-resolution image analysis, this paper investigates an approach for DEM super-resolution reconstruction with deep residual generative adversarial network. An advanced DEM Super-resolution Generative Adversarial Network (D-SRCAGAN) is proposed in this paper, which can reconstruct a quadruple higher resolution DEM by using low-resolution DEM. Compared with the bicubic and SRGAN methods, the D-SRCAGAN method reconstruction results can retain more topographic features and obtain higher RMSE values.","1558-0571","","10.1109/LGRS.2022.3224296","National Key Research and Development Project(grant numbers:2019YFC0605102); National Natural Science Foundation of China(grant numbers:NSFC 41972307); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961205","Digital elevation model (DEM);Super-resolution reconstruction;Improved super-resolution generative adversarial network (D-SRCAGAN);Attention mechanism","Image reconstruction;Superresolution;Training;Convolution;Image resolution;Generative adversarial networks;Generators","","","","","","","IEEE","23 Nov 2022","","","IEEE","IEEE Early Access Articles"
"A GAN-based Method for SAR Image Despeckling","F. Gu; H. Zhang; C. Wang","Key Laboratory of Digital Earth Science of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China; Key Laboratory of Digital Earth Science of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China; Key Laboratory of Digital Earth Science of Institute of Remote Sensing and Digital Earth, Chinese Academy of Science, Beijing, China","2019 SAR in Big Data Era (BIGSARDATA)","7 Oct 2019","2019","","","1","5","Synthetic Aperture Radar (SAR) images are contaminated by multiplicative noise known as speckle. Most filtering methods are limited to noise statistics and requires complex parameter tuning to achieve the desired visual effects. To solve the above problem, a generative adversarial network (GAN) based method is proposed for SAR image despeckling. Firstly, homogeneous regions are selected manually and speckle samples are produced. Then, GAN is trained to learn the distribution of the speckle samples and generate the “realistic-looking” ones. Third, a convolutional neural network (CNN) is designed that specializes in removing the speckle. Experiments on simulated SAR images and real SAR images show the good performance of the proposed method with respect to both the visual effect and quantitative analysis.","","978-1-7281-2653-1","10.1109/BIGSARDATA.2019.8858487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8858487","SAR image despeckling;generative adversarial network;convolutional neural network","Radar polarimetry;Gallium nitride;Training;Generative adversarial networks;Noise reduction;Synthetic aperture radar;Speckle","convolutional neural nets;image denoising;radar imaging;speckle;synthetic aperture radar;telecommunication computing","GAN-based method;SAR image despeckling;Synthetic Aperture Radar images;multiplicative noise;filtering methods;noise statistics;complex parameter tuning;generative adversarial network based method;speckle samples;convolutional neural network;simulated SAR images;visual effect;real SAR images;homogeneous regions","","1","","19","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"Hybrid cGAN: Coupling Global and Local Features for SAR-to-Optical Image Translation","Z. Wang; Y. Ma; Y. Zhang","School of Information Science and Engineering, Lanzhou University, Lanzhou, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; National Cryosphere Desert Data Center, Northwest Institute of Eco-Environment and Resources, Chinese Academy of Sciences, Lanzhou, China","IEEE Transactions on Geoscience and Remote Sensing","21 Oct 2022","2022","60","","1","16","Synthetic aperture radar (SAR) has the advantage of all-weather observation, but its imaging principle based on the backscattering of electromagnetic waves makes its information less interpretable. One feasible approach is to convert SAR images into optical images, which not only improves the interpretability of SAR images but also fills the gaps in information captured by optical sensors due to weather and light limitations. Since conditional generative adversarial network (cGAN) has the powerful ability to generate images, many studies have started to apply it to image translation tasks. For SAR-to-optical translation, some specialized cGAN models have been proposed, but most of them struggle to process SAR images with widely varying styles, often generating images with poor quality. To this end, we propose a hybrid cGAN that combines the advantages of convolutional neural network (CNN) and vision transformer (ViT). With the advantage of ViT to capture long-distance feature dependencies, the global features can be extracted and then fused with the local features extracted by CNN to improve the representation capabilities of our generator. Moreover, we expand the receptive field of the residual blocks in CNN by hierarchical convolution. Perceptual loss and classification loss are added for training to further improve the fidelity of the generated images. Finally, we introduce the multiscale strategy into the discriminator to balance its learning ability with that of the generator. Both visual and quantitative experiments are conducted with other state-of-the-art methods. The results show that our method not only achieves the optimal results in all the evaluation metrics but also generates images that are more consistent with the human visual system. In addition, the potential of our method to process multitype SAR images with significant style differences is also experimentally demonstrated.","1558-0644","","10.1109/TGRS.2022.3212208","National Key Research and Development Program of China(grant numbers:2022YFF0711700); National Natural Science Foundation of China(grant numbers:61201421); National Cryosphere Desert Data Center(grant numbers:E01Z7902); Capability Improvement Project for Cryosphere Desert Data Center of the Chinese Academy of Sciences(grant numbers:Y9298302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9911667","Conditional generative adversarial networks (cGANs);image translation;remote sensing;synthetic aperture radar (SAR);transformer","Feature extraction;Task analysis;Optical sensors;Optical imaging;Transformers;Adaptive optics;Generators","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);neural nets;optical images;radar imaging;remote sensing by radar;synthetic aperture radar","hybrid cGAN;local features;SAR-to-optical image translation;imaging principle;optical images;optical sensors;conditional generative adversarial network;image translation tasks;SAR-to-optical translation;specialized cGAN models;CNN;vision transformer;long-distance feature dependencies;global features;multitype SAR images","","","","66","IEEE","5 Oct 2022","","","IEEE","IEEE Journals"
"Biophysical Parameter Estimation Using Earth Observation Data in a Multi-Sensor Data Fusion Approach: CycleGAN","N. Efremova; E. Erten","University of Oxford Deepplanet, Oxford, UK; Department of Geomatics Engineering, Istanbul Technical University, Istanbul, Turkey","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5965","5968","Water management and up-to-date soil moisture (SM) information are crucial to ensure agricultural activities in dry-land farming regions. In this context, remote sensing imagery coupled with machine learning techniques can provide large scale SM information if there is enough data for training, which is really limited in reality. In this paper, we explored the potential of cycle-consistent Generative Adversarial Network (GAN) for data augmentation for training machine learning algorithms, which try to model spatial and temporal dependencies between the SM prediction (output) and the remote sensing imagery (input features). Specifically, the freely available SAR (Sentinel-1) and optical (Sentinel-2) time series data were evaluated together to predict SM using GANs. The experiments demonstrate that the proposed methodology outperforms the compared state-of-the-art methods if there is not enough data to train a regression convolutional neural networks (CNN) to predict SM content.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553561","Research Fund of the Istanbul Technical University(grant numbers:43018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553561","soil moisture;Sentinel-1;Sentinel-2;cycleGAN;CNN;PCA;autoencoders;support vector regression;ridge regression","Training;Time series analysis;Soil moisture;Pipelines;Estimation;Predictive models;Generative adversarial networks","","","","1","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"3-D Auxiliary Classifier GAN for Hyperspectral Anomaly Detection via Weakly Supervised Learning","H. Luo; H. Zhu; S. Liu; Y. Liu; X. Zhu; J. Lai","State Key Laboratory of ASIC and System, School of Microelectronics, Fudan University, Shanghai, China; School of Aeronautics and Astronautics, Zhejiang University, Hangzhou, China; China Aerospace Corporation Organization, Shanghai, China; Department of Micro/Nano Electronics, Shanghai Jiaotong University, Shanghai, China; China Aerospace Corporation Organization, Shanghai, China; State Key Laboratory of ASIC and System, School of Microelectronics, Fudan University, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","30 May 2022","2022","19","","1","5","Hyperspectral anomaly detection (AD) is important in Earth observation and remote sensing. However, the low spatial resolution of hyperspectral images (HSIs), insufficient samples, and lack of prior information limit the detection accuracy. To solve these problems, in this letter, we propose an auxiliary classifier generative adversarial network model based on a 3-D convolutional neural network named 3-D Auxiliary Classifier generative adversarial network (AC-GAN). First, the model is based on a 3-D convolutional neural network design, with 3-D tensors as samples. The network maintains valuable image spatial spectrum joint features to achieve good detection results. It can also generate sufficient samples to achieve dataset augmentation, solving the overfitting problem in GAN training. Second, we train the model with a weakly supervised method. The label of the samples is obtained through the coarse scanning method. Then, the AC-GAN is trained with the bootstrapping method to mitigate the impact of noise labels. The experimental results show that our proposed algorithm outperforms state-of-the-art AD algorithms.","1558-0571","","10.1109/LGRS.2022.3175836","National Natural Science Foundation of China(grant numbers:U20A20202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775984","Anomaly detection (AD);auxiliary classifier generative adversarial network (AC-GAN);hyperspectral images (HSIs);weakly supervised learning (WSL)","Training;Three-dimensional displays;Classification algorithms;Solid modeling;Generative adversarial networks;Supervised learning;Hyperspectral imaging","convolutional neural nets;geophysical image processing;hyperspectral imaging;image classification;image resolution;object detection;remote sensing;stereo image processing;supervised learning","hyperspectral anomaly detection;supervised learning;Earth observation;remote sensing;low spatial resolution;hyperspectral images;AC-GAN;3-D convolutional neural network design;3-D auxiliary classifier GAN;3-D auxiliary classifier generative adversarial network;AD algorithms;image spatial spectrum joint features;HSIs;dataset augmentation;bootstrapping method","","3","","26","IEEE","17 May 2022","","","IEEE","IEEE Journals"
"Robust SAR Automatic Target Recognition Via Adversarial Learning","Y. Guo; L. Du; D. Wei; C. Li","National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Jan 2021","2021","14","","716","729","The traditional denoising methods in noise robust synthetic aperture radar (SAR) automatic target recognition research are independent of the recognition model, which limits the robust recognition performance. In this article, we present a robust SAR automatic target recognition method via adversarial learning, which could integrate data denoising, feature extraction, and classification into a unified framework for joint learning. Different from the common recognition methods of directly inputting the SAR data into the classifiers, we add a dual-generative-adversarial-network (GAN) model between the SAR data and the classifier for data translation from a noise-polluted style to a relatively clean style to reduce the noise from SAR data. In order to ensure the target information in the SAR data can be retained during the data style translation, reconstruction constraint and label constraint are also used in the dual-GAN model. Then, the more reliable transferred SAR data are fed into the classifier. The parameters of the dual-GAN and classifier are learned through joint optimization in our method. Thus, the data separability is guaranteed in the process of denoising and feature extraction, which greatly improves the recognition performance of the method. In addition, our method can be easily extended to a semisupervised method by using different objective functions for labeled and unlabeled training data, which is more suitable for practical application. Experimental results on MSTAR dataset and Gotcha dataset show that our method can get the encouraging performance in the case of low signal-to-noise ratio and small labeled data size.","2151-1535","","10.1109/JSTARS.2020.3039235","National Natural Science Foundation of China(grant numbers:61771362,U1833203,61671354); Higher Education Discipline Innovation Project(grant numbers:B18039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264634","Adversarial learning;automatic target recognition (ATR);generative adversarial networks (GAN);noise robust;semisupervised learning;synthetic aperture radar (SAR)","Synthetic aperture radar;Feature extraction;Generative adversarial networks;Gallium nitride;Training;Deep learning;Target recognition","feature extraction;image classification;image denoising;learning (artificial intelligence);neural nets;radar computing;radar imaging;radar target recognition;synthetic aperture radar","reliable transferred SAR data;dual-GAN model;data style translation;noise-polluted style;data translation;dual-generative-adversarial-network model;common recognition methods;data denoising;robust SAR automatic target recognition method;robust recognition performance;recognition model;noise robust synthetic aperture radar automatic target recognition research;traditional denoising methods;adversarial learning;labeled data size;unlabeled training data;labeled training data;data separability","","9","","51","CCBYNCND","19 Nov 2020","","","IEEE","IEEE Journals"
"A Classification-Aware HSI Denoising Model With Neural Adversarial Network","W. Liu; J. You; J. Lee","Blockchain Laboratory, Xiong’an Intelligent City Innovation Federation, Xiong’an, China; Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea; Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea","IEEE Geoscience and Remote Sensing Letters","13 Sep 2022","2022","19","","1","5","Denoising is the representative task in hyperspectral image (HSI) processing. This letter proposes to produce a classification-driven HSI denoiser, which is capable of simultaneously reducing noise and preserving semantic-aware detail. The conditional neural adversarial network and multiple loss functions are introduced to produce visually pleasing images by enforcing an additional constraint that the denoised image must be indistinguishable from its corresponding ground-truth clean image. Experiments are performed on hyperspectral remote sensing images containing both the simulated hybrid noise and real noise. The results show that the proposed model outperforms many state-of-the-art denoising methods for HSIs.","1558-0571","","10.1109/LGRS.2022.3196698","Natural Science Foundation of Hebei Province(grant numbers:F2020403030); Overseas Returnees Program of Hebei Province(grant numbers:C20220104); Youth Science and Technology Foundation of Hebei GEO University(grant numbers:QN202103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851402","Classification;denoising;hyperspectral image (HSI);neural adversarial network","Noise reduction;Training;Feature extraction;Neural networks;Noise measurement;Generative adversarial networks;Task analysis","geophysical image processing;hyperspectral imaging;image denoising;remote sensing","hyperspectral image processing;classification-driven HSI denoiser;conditional neural adversarial network;multiple loss functions;visually pleasing images;denoised image;corresponding ground-truth clean image;hyperspectral remote sensing images;simulated hybrid noise;state-of-the-art denoising methods;classification-aware HSI denoising model;representative task","","","","17","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"IR-MSDNet: Infrared and Visible Image Fusion Based On Infrared Features and Multiscale Dense Network","A. Raza; J. Liu; Y. Liu; J. Liu; Z. Li; X. Chen; H. Huo; T. Fang","Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Apr 2021","2021","14","","3426","3437","Infrared (IR) and visible images are heterogeneous data, and their fusion is one of the important research contents in the remote sensing field. In the last decade, deep networks have been widely used in image fusion due to their ability to preserve high-level semantic information. However, due to the lower resolution of IR images, deep learning-based methods may not be able to retain the salient features of IR images. In this article, a novel IR and visible image fusion based on IR Features & Multiscale Dense Network (IR-MSDNet) is proposed to preserve the content and key target features from both visible and IR images in the fused image. It comprises an encoder, a multiscale decoder, a traditional processing unit, and a fused unit, and can capture incredibly rich background details in visible images and prominent target details in IR features. When the dense and multiscale features are fused, the background details are obtained by utilizing attention strategy, and then combined with complimentary edge features. While IR features are extracted by traditional quadtree decomposition and Bezier interpolation, and further intensified by refinement. Finally, both the decoded multiscale features and IR features are used to reconstruct the final fused image. Experimental evaluation with other state-of-the-art fusion methods validates the superiority of our proposed IR-MSDNet in both subjective and objective evaluation metrics. Additional objective evaluation conducted on the object detection (OD) task further verifies that the proposed IR-MSDNet has greatly enhanced the details in the fused images, which bring the best OD results.","2151-1535","","10.1109/JSTARS.2021.3065121","National Key Research and Development Program of China(grant numbers:2018YFB0505000); National Science and Technology Major Project(grant numbers:21-Y20A06-9001-17/18); National Natural Science Foundation of China(grant numbers:61221003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376604","Feature attention;image fusion;multiscale feature fusion;object detection (OD);remote sensing","Feature extraction;Image fusion;Decoding;Remote sensing;Generative adversarial networks;Transforms;Object detection","feature extraction;image fusion;infrared imaging;learning (artificial intelligence);object detection;quadtrees","Bezier interpolation;quadtree decomposition;multiscale dense network;fusion methods;fused image;multiscale features;complimentary edge features;dense features;IR features;salient features;deep learning-based methods;IR images;visible images;infrared features;visible image fusion;IR-MSDNet","","10","","42","CCBY","11 Mar 2021","","","IEEE","IEEE Journals"
"Enhanced Leaf Area Index Estimation with CROP-DualGAN Network","X. Li; Y. Dong; Y. Zhu; W. Huang","Aerospace Information Research Institute, State Key Laboratory of Remote Sensing Science, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; School of Mathematical Sciences, Capital Normal University, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2022","PP","99","1","1","Quantitative estimation of regional leaf area index (LAI) is an important basis for large-scale crop growth monitoring and yield estimation. With the development of deep learning, theoretically, the use of neural networks can effectively improve the accuracy of LAI estimation, but sufficient training samples are often required due to a large number of network parameters. In an actual regional LAI quantitative estimation, there are only a few samples, which is difficult to train in networks. Therefore, a crop dual-learning generative adversarial network (CROP-DualGAN) was proposed in this article for data enhancement of small samples to estimate regional LAI. The method uses dual learning to generate hyperspectral reflectance and corresponding LAI, including two groups of generative adversarial networks, in which the generator is used to generate data that conforms to the distribution of the training set, and the discriminator is used to judge the true or false generated samples. The generators and discriminators are constantly optimized in the confrontation so that the distribution of generated data is closer to that of training samples. In single crop type experiments, 30 training samples with enhanced in VGG16 achieved the R2 of cereal, maize and rape seed as 0.921, 0.990 and 0.956, and in SSLLAI-Net achieved the R2 of cereal, maize and rape seed as 0.971, 0.991 and 0.962. In multiple crop types experiments, the result is lower than individual crop estimation, but higher than that of without enhancement. Finally, non-parametric test is used to prove that most improvement in LAI estimation is significant, and the accuracy won’t decrease when improvement is not significant. In all, proposed method is universal and can effectively help benchmark models to improve regional LAI estimation accuracy with neural networks.","1558-0644","","10.1109/TGRS.2022.3230354","National Natural Science Foundation of China(grant numbers:42071320,42071423); SINO-EU, Dragon 5 Proposal: Application Of Sino-Eu Optical Data Into Agronomic Models To Predict Crop Performance And To Monitor And Forecast Crop Pests And Diseases(grant numbers:ID 57457); National Key Research and Development Program of China(grant numbers:2021YFB3901303); Alliance of International Science Organizations Grant(grant numbers:ANSO-CR-KP-2021-06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9992186","Leaf Area Index;Hyperspectral;Remote Sensing;CROP-DualGAN;data enhancement","Estimation;Hyperspectral imaging;Reflectivity;Training;Crops;Generators;Biological system modeling","","","","","","","CCBY","19 Dec 2022","","","IEEE","IEEE Early Access Articles"
"Remote Sensing Target Tracking in UAV Aerial Video Based on Saliency Enhanced MDnet","F. Bi; M. Lei; Y. Wang; D. Huang","School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China; School of Information Science and Technology, North China University of Technology, Beijing, China; China Research and Development Academy of Machinery Equipment, Beijing, China","IEEE Access","24 Jun 2019","2019","7","","76731","76740","Remote sensing target tracking in the aerial video from unmanned aerial vehicles (UAV) plays a key role in public security. As the UAV aerial video has rapid changes in scale and perspective, few pixels in the target region, and multiple similar disruptors, and the main tracking methods in this research field generally have relatively low tracking performance and timeliness, we propose a remote sensing target tracking method for the UAV aerial video based on a saliency enhanced multi-domain convolutional neural network (SEMD). First, in the pre-training stage, we combine the least squares generative adversarial networks (LSGANs) with a multi-orientation Gaussian Pyramid to augment typical easily confused negative samples for enhancing the capacity to distinguish between targets and the background. Then, a saliency module was integrated into our tracking network architecture to boost the saliency of the feature map, which can improve the representation power of a rapid dynamic change target. Finally, in the stage for generating tracking samples, we implemented a local weight allocation model to screen for hard negative samples. This approach can not only improve the stability in tracking but also boost efficiency. The comprehensive evaluations of public and homemade hard datasets demonstrate that the proposed method can achieve high accuracy and efficiency results compared with state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2019.2921315","National Natural Science Foundation of China(grant numbers:61601006); Natural Science Foundation of Beijing Municipality(grant numbers:4192021); Equipment Pre-Research Foundation(grant numbers:61404130312); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8732335","Visual tracking;multi-domain learning;saliency enhanced;sample augmentation","Target tracking;Remote sensing;Unmanned aerial vehicles;Resource management;Visualization;Convolution;Convolutional neural networks","autonomous aerial vehicles;convolutional neural nets;feature extraction;Gaussian processes;learning (artificial intelligence);object detection;object tracking;robot vision;target tracking;video signal processing","UAV aerial video;unmanned aerial vehicles;remote sensing target tracking method;saliency enhanced multidomain convolutional neural network;tracking network architecture;rapid dynamic change target;least squares generative adversarial networks;SEMD;LSGAN;multiorientation Gaussian pyramid;saliency module","","10","","24","OAPA","6 Jun 2019","","","IEEE","IEEE Journals"
"Image Data Augmentation for SAR Automatic Target Recognition Using TripleGAN","J. Hwang; Y. Shin","School of Electronic Engineering, Soongsil University, Seoul, Korea; School of Electronic Engineering, Soongsil University, Seoul, Korea","2021 International Conference on Information and Communication Technology Convergence (ICTC)","7 Dec 2021","2021","","","312","314","Synthetic aperture radar (SAR) images can be observed in all weather conditions. In addition, SAR automatic target recognition (ATR) is an important technology in the field of remote sensing image analysis. However, it is expensive to acquire SAR images, which limits the construction of additional datasets. In this paper, we propose a method to enhance ATR performance of the corresponding SAR images based on triple-generative adversarial networks (GANs). We added a classifier to learn together so that generator converges into the real data distribution. Experiments on the MSTAR dataset confirmed that the proposed model is applicable through the classification model.","2162-1233","978-1-6654-2383-0","10.1109/ICTC52510.2021.9621194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9621194","Synthetic aperture radar (SAR);automatic target recognition (ATR);generative adversarial network (GAN);data augmentation","Image analysis;Target recognition;Minimization;Radar polarimetry;Generators;Information and communication technology;Synthetic aperture radar","image classification;learning (artificial intelligence);neural nets;object recognition;radar imaging;remote sensing;synthetic aperture radar","image data augmentation;SAR automatic target recognition;synthetic aperture radar images;remote sensing image analysis;additional datasets;SAR images;TripleGAN;weather conditions;ATR performance;triple-generative adversarial networks;MSTAR dataset","","","","10","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"A Point Clouds Framework for 3-D Reconstruction of SAR Images Based on 3-D Parametric Electromagnetic Part Model","Z. -L. Yang; R. -Y. Zhou; F. Wang; F. Xu","Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4818","4821","3-D reconstruction is a hot topic in remote sensing as well as computer vision. The particularity and complexity of the microwave scattering mechanism bring great challenges to the 3D reconstruction of SAR images, and the applicability of existing methods need to be improved. This study proposes an efficient and explainable point clouds framework for three-dimensional reconstruction of SAR images based on three-dimensional parametric electromagnetic part models. This 3-D SAR reconstruction framework consists of two parts: a feature extraction generative adversarial network and a 3-D reconstruction generative network. The feature extraction generative adversarial network has 5 convolutional layers to extract the features of single SAR image and save them in the form of graph, then input this graph to the 3-D reconstruction generative network and we can get the main shape of the target from a SAR image. This framework effectively reduces the numbers of observation for 3-D reconstruction and make the 3-D reconstruction from single SAR image possible.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553789","National Natural Science Foundation of China(grant numbers:61901122,61991421,61991422); Natural Science Foundation of Shanghai(grant numbers:20ZR1406300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553789","3-D reconstruction;parametric electromagnetic part model;point clouds generation;deep learning;SAR","Training;Solid modeling;Three-dimensional displays;Electromagnetic scattering;Feature extraction;Generative adversarial networks;Radar polarimetry","feature extraction;image reconstruction;radar imaging;synthetic aperture radar","three-dimensional reconstruction;three-dimensional parametric electromagnetic part models;3-D SAR reconstruction framework;feature extraction generative adversarial network;3-D reconstruction generative network;single SAR image possible;3-D parametric electromagnetic part model;efficient point clouds framework;explainable point clouds framework","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"High-Resolution Refocusing for Defocused ISAR Images by Complex-Valued Pix2pixHD Network","H. Yuan; H. Li; Y. Zhang; Y. Wang; Z. Liu; C. Wei; C. Yao","School of Electronic and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronic and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronic and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronic and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronic and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronic and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronic and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","14 Oct 2022","2022","19","","1","5","Inverse synthetic aperture radar (ISAR) is an effective detection method for targets. However, for the maneuvering targets, the Doppler frequency induced by an arbitrary scatterer on the target is time-varying, which will cause defocus on ISAR images and bring difficulties for the further recognition process. It is hard for traditional methods to well refocus all positions on the target well. In recent years, generative adversarial networks (GANs) achieve great success in image translation. However, the current refocusing models ignore the information of high-order terms containing in the relationship between real and imaginary parts of the data. To this end, an end-to-end refocusing network, named complex-valued pix2pixHD (CVPHD), is proposed to learn the mapping from defocus to focus, which utilizes complex-valued (CV) ISAR images as an input. A CV instance normalization layer is applied to mine the deep relationship between the complex parts by calculating the covariance of them and accelerate the training. Subsequently, an innovative adaptively weighted loss function is put forward to improve the overall refocusing effect. Finally, the proposed CVPHD is tested with the simulated and real dataset, and both can get well-refocused results. The results of comparative experiments show that the refocusing error can be reduced if extending the pix2pixHD network to the CV domain and the performance of CVPHD surpasses other autofocus methods in refocusing effects. The code and dataset have been available online (https://github.com/yhx-hit/CVPHD).","1558-0571","","10.1109/LGRS.2022.3210036","National Natural Science Foundation of China(grant numbers:61201308,61671490); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903624","Complex-valued (CV) network;generative adversarial networks (GANs);inverse synthetic aperture radar (ISAR);radar image refocusing","Radar imaging;Radar;Imaging;Image reconstruction;Frequency modulation;Feature extraction;Time-frequency analysis","image resolution;radar imaging;synthetic aperture radar","high-resolution refocusing;defocused ISAR images;complex-valued pix2pixHD network;inverse synthetic aperture radar;effective detection method;maneuvering targets;Doppler frequency;arbitrary scatterer;time-varying;defocus;recognition process;generative adversarial networks;image translation;current refocusing models;high-order terms;end-to-end refocusing network;CV instance normalization layer;deep relationship;innovative adaptively weighted loss function;refocusing effect;well-refocused results;refocusing error;CVPHD surpasses other autofocus methods","","","","18","IEEE","26 Sep 2022","","","IEEE","IEEE Journals"
"Simulated Intensity Rendering of 3D LiDAR using Generative Adversarial Network","S. -c. Mok; G. -w. Kim","Chungbuk National University,Department of Electronics Engineering,Cheongju,South Korea; Chungbuk National University,Department of Electronics Engineering,Cheongju,South Korea","2021 IEEE International Conference on Big Data and Smart Computing (BigComp)","10 Mar 2021","2021","","","295","297","In autonomous vehicle, LiDAR is one of the most importent sensor for measurement range. In particular, LiDAR is widely used in mobile mapping systems (MMS) for building high-definition (HD) map. Not only range but also intensity are useful data for extracting road marking and surface data from sensor. However, LiDAR intensity is affected by surface reflecting characteristic as well as range, incidence angle, atmospheric transmittance and others. Therefore, LiDAR intensity model is difficult to reproduce in simulation environments. In this paper, we proposed realistic intensity rendering method with simulator by generative adversarial network (GAN) trained with unannotated real data. We develop LiDAR projection preprocessing method for operating convolution networks. Our approach have advantage to preserve sensor characteristic and to reduce projection scale loss. We modified CycleGAN architecture to render intensity with unpaired real and synthetic data. Our rendered results show possibility of image-based approach for LiDAR and potential for generating LiDAR dataset by using simulator. Especially, we expect to utilize for military vehicle that has difficulty to reproduce the environments.","2375-9356","978-1-7281-8924-6","10.1109/BigComp51126.2021.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373175","Autonomous Vehicle;LiDAR;Generative Adversarial Network;Data Augmentation;Simulator","Laser radar;Three-dimensional displays;Atmospheric modeling;Roads;Generative adversarial networks;Rendering (computer graphics);Particle measurements","geophysical image processing;military vehicles;neural nets;optical radar;radar imaging;remote sensing by laser beam;remote sensing by radar;rendering (computer graphics)","generative adversarial network;unannotated real data;LiDAR projection;convolution networks;sensor characteristic;unpaired real data;synthetic data;rendered results;LiDAR dataset;simulated intensity rendering;autonomous vehicle;importent sensor;measurement range;mobile mapping systems;high-definition map;extracting road marking;LiDAR intensity model;simulation environments;realistic intensity rendering method","","2","","17","IEEE","10 Mar 2021","","","IEEE","IEEE Conferences"
"CTGAN : Cloud Transformer Generative Adversarial Network","G. -L. Huang; P. -Y. Wu","Graduate Institute of Communication Engineering, Electrical Engineering, National Taiwan University, Taiwan; Graduate Institute of Communication Engineering, Electrical Engineering, National Taiwan University, Taiwan","2022 IEEE International Conference on Image Processing (ICIP)","18 Oct 2022","2022","","","511","515","Cloud occlusions obstruct some applications of remote sensing imagery, such as environment monitoring, land cover classification, and poverty prediction. In this paper, we propose the Cloud Transformer Generative Adversarial Network (CTGAN), taking three temporal cloudy images as input and generating a corresponding cloud-free image. Unlike previous work using generative networks, we design the feature extractor to maintain the weight of the cloudless region while reducing the weight of the cloudy region, and we pass the extracted features to a conformer module to find the most critical representations. Mean-while, to address the lack of datasets, we collected a new dataset named Sen2_MTC from the Sentinel-2 satellite and manually labeled each cloudy and cloud-free image. Finally, we conducted extensive experiments on FS-2, the STGAN dataset, and Sen2_MTC. Our proposed CTGAN demonstrates higher qualitative and quantitative performance than the previous work and achieves state-of-the-art performance on these three datasets. The code is available at https://github.com/come880412/CTGAN","2381-8549","978-1-6654-9620-9","10.1109/ICIP46576.2022.9897229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897229","Cloud removal for multi-temporal cloudy images;generative adversarial network;conformer;Sentinel-2 satellite;FormoSat-2 satellite","Satellites;Codes;Clouds;Feature extraction;Transformers;Generative adversarial networks;Remote sensing","","","","","","21","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"Restoration of Antenna Radiation Pattern Using Conditional Generative Adversarial Network","Y. -H. Chen; T. D. Ha; D. Erricolo; P. -Y. Chen","Department of Electrical and Computer Engineering, University of Illinois, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Illinois, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Illinois, Chicago, IL, USA; Department of Electrical and Computer Engineering, University of Illinois, Chicago, IL, USA","2022 IEEE International Symposium on Antennas and Propagation and USNC-URSI Radio Science Meeting (AP-S/URSI)","21 Sep 2022","2022","","","481","482","A deep learning model based on the conditional generative adversarial network (c-GAN) is proposed to accelerate the measurement of radiation and scattering patterns. Recently, GAN has been proposed to effectively restore an image from its fragments and blank areas. Here, we further leverage the GAN model to fully restore the antenna radiation pattern with sparse solid angles, thus greatly saving the characterization time and cost. We envision that the proposed GAN-assisted measurement method may be beneficial for various applications, including radar, remote sensing, and optimal deployment of 5G/6G networks.","1947-1491","978-1-6654-9658-2","10.1109/AP-S/USNC-URSI47032.2022.9887123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887123","","Antenna measurements;Solid modeling;Radar remote sensing;Scattering;Radar imaging;Generative adversarial networks;Solids","5G mobile communication;antenna radiation patterns;gallium compounds;III-V semiconductors;learning (artificial intelligence)","antenna radiation pattern;conditional generative adversarial network;deep learning model;c-GAN;scattering patterns;GAN model;GAN-assisted measurement method","","","","6","IEEE","21 Sep 2022","","","IEEE","IEEE Conferences"
"Adversarial Machine Learning in Text Processing: A Literature Survey","I. Alsmadi; N. Aljaafari; M. Nazzal; S. Alhamed; A. H. Sawalmeh; C. P. Vizcarra; A. Khreishah; M. Anan; A. Algosaibi; M. A. Al-Naeem; A. Aldalbahi; A. Al-Humam","Department of Computing and Cyber Security, Texas A&M University, College Station, TX, USA; Department of Computer Science, King Faisal University, Hofuf, Saudi Arabia; Department of Electrical and Computer Engineering, Newark College of Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Department of Computer Science, King Faisal University, Hofuf, Saudi Arabia; Remote Sensing Unit, Northern Border University, Arar, Saudi Arabia; Department of Computer Science, King Faisal University, Hofuf, Saudi Arabia; Department of Electrical and Computer Engineering, Newark College of Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Software Engineering Department, College of Engineering, Alfaisal University, Riyadh, Saudi Arabia; Department of Computer Science, King Faisal University, Hofuf, Saudi Arabia; Department of Computer Networks and Communications, College of Computer Sciences and Information Technology, King Faisal University, Hofuf, Saudi Arabia; Department of Computer Science, King Faisal University, Hofuf, Saudi Arabia; Department of Computer Science, King Faisal University, Hofuf, Saudi Arabia","IEEE Access","16 Feb 2022","2022","10","","17043","17077","Machine learning algorithms represent the intelligence that controls many information systems and applications around us. As such, they are targeted by attackers to impact their decisions. Text created by machine learning algorithms has many types of applications, some of which can be considered malicious especially if there is an intention to present machine-generated text as human-generated. In this paper, we surveyed major subjects in adversarial machine learning for text processing applications. Unlike adversarial machine learning in images, text problems and applications are heterogeneous. Thus, each problem can have its own challenges. We focused on some of the evolving research areas such as: malicious versus genuine text generation metrics, defense against adversarial attacks, and text generation models and algorithms. Our study showed that as applications of text generation will continue to grow in the near future, the type and nature of attacks on those applications and their machine learning algorithms will continue to grow as well. Literature survey indicated an increasing trend in using pre-trained models in machine learning. Word/sentence embedding models and transformers are examples of those pre-trained models. Adversarial models may utilize same or similar pre-trained models as well. In another trend related to text generation models, literature showed effort to develop universal text perturbations to be used in both black-and white-box attack settings. Literature showed also using conditional GANs to create latent representation for writing types. This usage will allow for a seamless lexical and grammatical transition between various writing styles. In text generation metrics, research trends showed developing successful automated or semi-automated assessment metrics that may include human judgement. Literature showed also research trends of designing and developing new memory models that increase performance and memory utilization efficiency without validating real-time constraints. Many research efforts evaluate different defense model approaches and algorithms. Researchers evaluated different types of targeted attacks, and methods to distinguish human versus machine generated text.","2169-3536","","10.1109/ACCESS.2022.3146405","Deputyship for Research and Innovation, Ministry of Education, Saudi Arabia(grant numbers:1120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693527","Adversarial machine learning;generative adversarial networks;GAN;text generation","Generators;Training;Security;Market research;Machine learning algorithms;Generative adversarial networks;Adversarial machine learning","invasive software;learning (artificial intelligence);natural language processing;neural nets;text analysis","machine learning algorithms;text processing;malicious;adversarial attacks;text generation models;transformers;word embedding models;black-and white-box attack;GAN;semi-automated assessment metrics","","1","","181","CCBY","26 Jan 2022","","","IEEE","IEEE Journals"
"A Conditional Adversarial Network for Change Detection in Heterogeneous Images","X. Niu; M. Gong; T. Zhan; Y. Yang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","27 Dec 2018","2019","16","1","45","49","Due to the distinct statistical properties in cross-sensor images, change detection in heterogeneous images is much more challenging than in homogeneous images. In this letter, we adopt a conditional generative adversarial network (cGAN) to transform the heterogeneous synthetic aperture radar (SAR) and optical images into some space where their information has a more consistent representation, making the direct comparison feasible. Our proposed framework contains a cGAN-based translation network that aims to translate the optical image with the SAR image as a target, and an approximation network that approximates the SAR image to the translated one by reducing their pixelwise difference. The two networks are updated alternately and when they are both trained well, the two translated and approximated images can be considered as homogeneous, from which the final change map can be acquired by direct comparison. Theoretical analysis and experimental results demonstrate the effectiveness and robustness of the proposed framework.","1558-0571","","10.1109/LGRS.2018.2868704","National Natural Science Foundation of China(grant numbers:61772393); National Program for Support of Top-Notch Young Professionals of China; National Key Research and Development Program of China(grant numbers:2017YFB0802200); Key Research and Development Program of Shaanxi Province(grant numbers:2018ZDXM-GY-045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8471225","Change detection;conditional generative adversarial networks (cGANs);heterogeneous images;synthetic aperture radar (SAR)","Optical imaging;Adaptive optics;Synthetic aperture radar;Optical fiber networks;Optical sensors;Linear programming;Approximation algorithms","approximation theory;image representation;optical images;radar detection;radar imaging;synthetic aperture radar","translated approximated images;conditional adversarial network;change detection;conditional generative adversarial network;cGAN-based translation network;approximation network;statistical properties;optical imaging;SAR imaging;heterogeneous imaging;cross-sensor imaging;homogeneous imaging;heterogeneous synthetic aperture radar imaging;image representation;pixelwise difference reduction","","88","","14","IEEE","23 Sep 2018","","","IEEE","IEEE Journals"
"A SAR-to-Optical Image Translation Method Based on PIX2PIX","Z. Zuo; Y. Li","School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3026","3029","Optical remote sensing images are susceptible to adverse weather effects, such as cloud occlusion, which lead to low availability of optical image data. However, synthetic aperture radar (SAR) can well overcome these shortcomings of optical imaging because of SAR working in an all-weather environment. Due to the imaging mechanism of SAR image is essentially different from optical image, the interpretation of SAR image is a huge challenge. Inspired by the powerful image-to-image translation capability of Generative Adversarial Networks (GANs), this paper proposes an improved Pix2Pix network to achieve the translation task from SAR image to optical image. In order to better maintain the structural information after image translation, this paper introduces a constraint based on phase consistency as the consideration of the loss function. Experimental results show that the proposed method has obvious advantages for multimodal remote sensing data translation tasks in comparison with the current state-of-the-art methods.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555111","SAR-to-optical translation;Pix2Pix;SAR;GAN s;phase consistency","Optical losses;Radar imaging;Optical fiber networks;Optical imaging;Adaptive optics;Radar polarimetry;Optical sensors","geophysical image processing;optical images;radar imaging;remote sensing;synthetic aperture radar","SAR image;multimodal remote sensing data translation tasks;SAR-to-optical image translation method;optical remote sensing images;optical image data;optical imaging;SAR working;imaging mechanism;powerful image-to-image translation capability;improved Pix2Pix network","","3","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Spatiotemporal Reflectance Fusion Using a Generative Adversarial Network","C. Shang; X. Li; Z. Yin; X. Li; L. Wang; Y. Zhang; Y. Du; F. Ling","College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","13 Dec 2021","2022","60","","1","15","The spatiotemporal reflectance fusion method is used to blend high-temporal and low-spatial resolution images with their low-temporal and high-spatial resolution counterparts that were previously acquired by various satellite sensors. Recently, a wide variety of learning-based solutions have been developed, but challenges remain. These solutions usually require two sets of data acquired before and after the prediction time, making them unsuitable for near-real-time predicting. The solutions are always trained band by band and thus do not consider the spectral correlation. High-resolution temporal changes are difficult to reconstruct accurately with the network structure used, which lowers the accuracy of the fusion result. To address these problems, this study proposes a novel spatiotemporal adaptive reflectance fusion model using a generative adversarial network (GASTFN). In GASTFN, an end-to-end network, including a generative and discriminative network, is simultaneously trained for all spectral bands. The proposed model can be applied to the one-pair case, consider the spectral correlation of each band, and improve the process of producing super-resolution imagery by adopting the discriminative network for image reflectance values rather than temporal changes in reflectance. The proposed model has been verified with two actual satellite data sets acquired in heterogeneous landscapes and areas with abrupt changes, with a comparison of the state-of-art methods. The results show that GASTFN can generate the most accurate fusion images with more detailed textures, more realistic spatial shapes, and higher accuracy, demonstrating that the GASTFN is effective for predicting near-real-time changes in image reflectance and preserves the most valuable spatial information.","1558-0644","","10.1109/TGRS.2021.3065418","Hubei Provincial Natural Science Foundation for Innovation Groups(grant numbers:2019CFA019); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA2003030201); Natural Science Foundation of China(grant numbers:62071457); Application Foundation Frontier Project of Wuhan(grant numbers:2020020601012283); Hubei Province Natural Science Fund for Distinguished Young Scholars(grant numbers:2018CFA062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383451","Generative adversarial network (GAN);spatiotemporal fusion;super-resolution;temporal changes","Spatiotemporal phenomena;Remote sensing;Generative adversarial networks;Superresolution;Spatial resolution;Gallium nitride;Layout","","","","5","","51","IEEE","23 Mar 2021","","","IEEE","IEEE Journals"
"Cross-Sensor Adversarial Domain Adaptation of Landsat-8 and Proba-V Images for Cloud Detection","G. Mateo-García; V. Laparra; D. López-Puigdollers; L. Gómez-Chova","Image Processing Laboratory (IPL), University of Valencia, Valencia, Spain; Image Processing Laboratory (IPL), University of Valencia, Valencia, Spain; Image Processing Laboratory (IPL), University of Valencia, Valencia, Spain; Image Processing Laboratory (IPL), University of Valencia, Valencia, Spain","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","747","761","The number of Earth observation satellites carrying optical sensors with similar characteristics is constantly growing. Despite their similarities and the potential synergies among them, derived satellite products are often developed for each sensor independently. Differences in retrieved radiances lead to significant drops in accuracy, which hampers knowledge and information sharing across sensors. This is particularly harmful for machine learning algorithms, since gathering new ground-truth data to train models for each sensor is costly and requires experienced manpower. In this work, we propose a domain adaptation transformation to reduce the statistical differences between images of two satellite sensors in order to boost the performance of transfer learning models. The proposed methodology is based on the cycle consistent generative adversarial domain adaptation framework that trains the transformation model in an unpaired manner. In particular, Landsat-8 and Proba-V satellites, which present different but compatible spatio-spectral characteristics, are used to illustrate the method. The obtained transformation significantly reduces differences between the image datasets while preserving the spatial and spectral information of adapted images, which is, hence, useful for any general purpose cross-sensor application. In addition, the training of the proposed adversarial domain adaptation model can be modified to improve the performance in a specific remote sensing application, such as cloud detection, by including a dedicated term in the cost function. Results show that, when the proposed transformation is applied, cloud detection models trained in Landsat-8 data increase cloud detection accuracy in Proba-V.","2151-1535","","10.1109/JSTARS.2020.3031741","Ministerio de Ciencia e Innovación(grant numbers:TEC2016-77741-R,PID2019-109026RB-I00); European Space Agency(grant numbers:CCN008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234601","Generative adversarial networks;convolutional neural networks;domain adaptation;Landsat-8;Proba-V;cloud detection","Remote sensing;Earth;Artificial satellites;Cloud computing;Adaptation models;Satellites;Data models","clouds;geophysical image processing;learning (artificial intelligence);optical sensors;remote sensing","cross-sensor adversarial domain adaptation;Earth observation satellites;optical sensors;potential synergies;derived satellite products;retrieved radiances;information sharing;machine learning algorithms;ground-truth data;experienced manpower;domain adaptation transformation;statistical differences;satellite sensors;transfer learning models;cycle consistent generative adversarial domain adaptation framework;transformation model;Proba-V satellites;image datasets;spatial information;spectral information;adapted images;general purpose cross-sensor application;adversarial domain adaptation model;cloud detection models;Landsat-8 data;cloud detection accuracy;compatible spatio-spectral characteristics","","10","","65","CCBY","21 Oct 2020","","","IEEE","IEEE Journals"
"Discriminative Semi-Supervised Generative Adversarial Network for Hyperspectral Anomaly Detection","T. Jiang; W. Xie; Y. Li; Q. Du","State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, USA","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2420","2423","Hyperspectral anomaly detection has been facing great challenges in the field of deep learning due to high dimensions and limited samples. To address these challenges, a novel discriminative semi-supervised generative adversarial network (GAN) method with dual RX (Reed-Xiaoli), called semiDRX, is proposed in this paper. The main contribution of the proposed method is to learn a reconstruction of background homogenization and anomaly saliency through a semi-supervised GAN. To achieve this goal, firstly, the coarse RX detection is performed to obtain a background sample set with potential anomalous pixels being removed. Secondly, the obtained coarse background set learns more comprehensive background characteristics through the network. The original hyperspectral image (HSI) is fed into the learned network to obtain reconstructions with homogeneous backgrounds and salient anomalies. The refined detection results are generated by a second RX detector. Experiments on three HSIs over different scenes demonstrate its advancement and effectiveness.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323688","National Natural Science Foundation of China(grant numbers:61801359); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323688","Hyperspectral anomaly detection (HAD);dual RX;semi-supervised;background homogenization;generative adversarial network (GAN)","Generative adversarial networks;Gallium nitride;Anomaly detection;Detectors;Hyperspectral imaging;Generators;Decoding","feature extraction;geophysical image processing;geophysical signal processing;hyperspectral imaging;learning (artificial intelligence);object detection","refined detection results;salient anomalies;homogeneous backgrounds;learned network;original hyperspectral image;comprehensive background characteristics;coarse background;background sample set;coarse RX detection;semisupervised GAN;anomaly saliency;background homogenization;dual RX;novel discriminative semisupervised;limited samples;deep learning;hyperspectral anomaly detection;discriminative semisupervised generative adversarial network","","9","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Semi-Supervised Complex-Valued GAN for Polarimetric SAR Image Classification","Q. Sun; X. Li; L. Li; X. Liu; F. Liu; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xian, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xian, Shaanxi Province, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3245","3248","Polarimetric synthetic aperture radar (PolSAR) images are widely used in disaster detection and military reconnaissance and so on. However, their interpretation faces some challenges, e.g., deficiency of labeled data, inadequate utilization of data information and so on. In this paper, a complex-valued generative adversarial network (GAN) is proposed for the first time to address these issues. The complex number form of model complies with the physical mechanism of PolSAR data and in favor of utilizing and retaining amplitude and phase information of PolSAR data. GAN architecture and semi-supervised learning are combined to handle deficiency of la-beled data. GAN expands training data and semi-supervised learning is used to train network with generated, labeled and unlabeled data. Experimental results on two benchmark data sets show that our model outperforms existing state-of-the-art models, especially for conditions with fewer labeled data.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898217","PolSAR image classification;complex-valued operations;semi-supervised learning;generative adversarial network","Gallium nitride;Data models;Semisupervised learning;Generative adversarial networks;Covariance matrices;Feature extraction;Generators","image classification;neural nets;radar imaging;radar polarimetry;synthetic aperture radar","polarimetric SAR image classification;polarimetric synthetic aperture radar images;complex-valued generative adversarial network;semisupervised complex-valued GAN;PolSAR","","7","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Multi-Discriminator Generative Adversarial Network for High Resolution Gray-Scale Satellite Image Colorization","F. Li; L. Ma; J. Cai","Institute of Automation, Chinese Academy of Science; Institute of Automation, Chinese Academy of Science; Institute of Automation, Chinese Academy of Science","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3489","3492","Automatic colorization for grayscale satellite images can help with eliminating lighting differences between multi-spectral captures, and provides strong prior information for ground type classification and object detection. In this paper, we introduced a novel generative adversarial network with multiple discriminators for colorizing gray-scale satellite images with pseudo-natural appearances. Although being powerful, deep generative model in its common form with a single discriminator could be unstable for achieving spatial consistency on local textured regions, especially highly textured ones. To address this issue, the generator in our proposed structure produces a group of colored outputs from feature maps at different scale levels of the network, each being supervised by an independent discriminator to fit the original colored training input in discrete Lab color space. The final colored output is a cascaded ensemble of these preceding by-products via summation, thus the fitting errors are reduced by a geometric series form. Quantitative and qualitative comparisons with the sole-discriminator version have been performed on high-resolution satellite images in experiments, where significant reductions in prediction errors have been observed.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517930","Pseudo-natural colorization;gray-scale satellite images;generative adversarial network;multiple discriminators","Image color analysis;Generative adversarial networks;Gallium nitride;Training;Satellites;Generators;Visualization","image colour analysis;image resolution;image segmentation;image texture;object detection","scale levels;object detection;ground type classification;strong prior information;multispectral captures;lighting differences;grayscale satellite images;automatic colorization;high resolution gray-scale satellite image colorization;multidiscriminator generative adversarial network;high-resolution satellite images;sole-discriminator version;final colored output;discrete Lab color space;original colored training input;independent discriminator;colored outputs;local textured regions;single discriminator;deep generative model;pseudonatural appearances;gray-scale satellite images","","5","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Hyperspectral Nonlinear Unmixing via Generative Adversarial Network","M. Tang; Y. Qu; H. Qi","Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, USA","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2404","2407","Hyperspectral nonlinear unmixing (HNU) is an extremely challenging problem as it is very difficult, if possible at all, to derive an explicit model to describe the underlying nonlinear mixing process. This paper gives the first attempt to tackle this problem by taking advantage of recent advances in deep learning, in specific, the development in generative adversarial network (GAN). The biggest contribution of GAN is that upon training, the network can generate samples with the same probabilistic distribution as that of the training samples, without explicitly knowing what the distribution actually is. Hence, we ask a similar question: can we unmix a hyperspectal image without explicitly knowing the nonlinear mixing model? In order to test this hypothesis, this paper proposes a data-driven supervised HNU method as compared to the traditional model-based approaches and uses a specific GAN framework, CycleGAN to solve the challenging nonlinear unmixing problem. We exploit the linkage between the cycle consistency loss used in CycleGAN and the spectral reconstruction loss used in traditional methods. We make the essential discovery that the usage of the cycle consistency loss enables the learning of the mixing and unmixing processes to be dependent on the training data only, without the need of an explicit mixing model. We refer to the proposed approach as CycleGAN unmixing net, or CGU net. Experimental results indicate that the proposed CGU net exhibits stable and competitive performance on different datasets as compared to traditional HNU methods that are model-based.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324087","Hyperspectral Image;Nonlinear Unmixing;Data Driven;Generative Network","Gallium nitride;Image reconstruction;Mixture models;Hyperspectral imaging;Training;Generative adversarial networks;Data models","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging","hyperspectral nonlinear unmixing;generative adversarial network;nonlinear mixing process;deep learning;probabilistic distribution;training samples;nonlinear mixing model;data-driven supervised HNU method;model-based approaches;nonlinear unmixing problem;cycle consistency loss;unmixing processes;training data;explicit mixing model;CycleGAN unmixing net;hyperspectal image;HNU methods;GAN framework","","5","","11","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Multi-Frame Super-Resolution Algorithm Based on a WGAN","K. Ning; Z. Zhang; K. Han; S. Han; X. Zhang","School of Computer Science and Technology, North China University of Technology, Beijing, China; School of Computer Science and Technology, North China University of Technology, Beijing, China; School of Computer Science and Technology, North China University of Technology, Beijing, China; School of Computer Science and Technology, North China University of Technology, Beijing, China; School of Science, Institute of Photoelectronics Technology, Beijing Jiaotong University, Beijing, China","IEEE Access","18 Jun 2021","2021","9","","85839","85851","Image super-resolution reconstruction has been widely used in remote sensing, medicine and other fields. In recent years, due to the rise of deep learning research and the successful application of convolutional neural networks in the image field, the super-resolution reconstruction technology based on deep learning has also achieved great development. However, there are still some problems that need to be solved. For example, the current mainstream image super-resolution algorithms based on single or multiple frames pursue high performance indicators such as PSNR and SSIM, while the reconstructed image is relatively smooth and lacks many high-frequency details. It is not conducive to application in a real environment. To address such problem, this paper proposes a super-resolution reconstruction model of sequential images based on Generative Adversarial Networks (GAN). The proposed approach combines the registration module to fuse adjacent frames, effectively use the detailed information in multiple consecutive frames, and enhances the spatio-temporality of low-resolution images in sequential images. While the GAN was used to improve the effect of image high-frequency texture detail reconstruction, WGAN was introduced to optimize model training. The reconstruction results not only improved the PSNR and SSIM indexes but also reconstructed more high-frequency detail textures. Finally, in order to further improve the perception effect, an additional registration loss item RLT is introduced in the GAN network perception loss. Through extensive experiments, it shows that the model proposed in this paper effectively obtains the information between the sequence images. When the PSNR and SSIM indicators are improve, it can reconstruct better high-frequency texture details than the current advanced multi-frame algorithms.","2169-3536","","10.1109/ACCESS.2021.3088128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452141","Super-resolution reconstruction;sequential images;convolutional neural network;Wasserstein generative adversarial network (WGAN)","Image reconstruction;Superresolution;Generative adversarial networks;Spatial resolution;Generators;Visual perception","edge detection;image denoising;image reconstruction;image resolution;image sampling;image sequences;image texture;learning (artificial intelligence);maximum likelihood estimation;neural nets;wavelet transforms","remote sensing;deep learning research;convolutional neural networks;image field;super-resolution reconstruction technology;current mainstream image super-resolution algorithms;single frames;high performance indicators;reconstructed image;high-frequency details;super-resolution reconstruction model;sequential images;Generative Adversarial Networks;multiple consecutive frames;low-resolution images;image high-frequency texture;WGAN;high-frequency detail textures;GAN network perception loss;sequence images;high-frequency texture details;current advanced multiframe algorithms;multiframe super-resolution algorithm","","3","","39","CCBY","11 Jun 2021","","","IEEE","IEEE Journals"
"Mental Retrieval of Large-Scale Satellite Images Via Learned Sketch-Image Deep Features","F. Xu; R. Zhang; W. Yang; G. -S. Xia","School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; LIESMARS CAPTAIN, Wuhan University, Wuhan, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3356","3359","Searching targets of interest in large-scale satellite images is an imperative task, which becomes a challenging issue when the targets reside only in the mind of the user as a set of subjective visual patterns. In this paper, we take the advantage of hand-drawn sketches' strong intuition of describing mental target to address the problem of no available exemplar query. We introduce a multi-level-of-detail model to learn a cross-domain representation for bridging the gap between sketches and satellite images. To train the model, we propose a novel method of generating satellite images with corresponding level of details based on generative adversarial network. Experiments on both large-scale satellite images and commonly used RS datasets demonstrate the effectiveness and superiority of our method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900605","mental search;satellite images;sketch;multi-level-of-detail model","Satellites;Image edge detection;Task analysis;Image retrieval;Search methods;Visualization;Generative adversarial networks","feature extraction;image retrieval;learning (artificial intelligence)","mental retrieval;satellite images;searching targets;hand-drawn sketches;mental target;sketch-image deep features;cross-domain representation;generative adversarial network;RS datasets","","3","","13","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Semi-Supervised SAR ATR via Conditional Generative Adversarial Network with Multi-Discriminator","X. Liu; Y. Huang; C. Wang; J. Pei; W. Huo; Y. Zhang; J. Yang","School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2361","2364","Convolutional neural networks (CNN) show superior potential in synthetic aperture radar automatic target recognition (SAR ATR). However, due to the difficulty of obtaining SAR images and the scarcity of labeled SAR images, supervised learning has poor performance in this area and is not widely applicable. To address this problem, a semi-supervised conditional generative adversarial network with a multi-discriminator (SCGAN-MD) is proposed in this paper. In our method, a conditional generative adversarial network (CGAN) is adopted with two discriminators for training the generated images and predicting the labels for unlabeled samples. Compared with other semi-supervised learning-based methods, our proposed method has more accurate image generation capability and can achieve improved recognition accuracy of SAR ATR. Experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) database indicate that the proposed method can effectively improve the recognition accuracy and robustness of the network with a small number of labeled samples.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554365","National Natural Science Foundation of China(grant numbers:61901091,61901090,61671117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554365","SAR ATR;semi-supervised;conditional generative adversarial network","Training;Image recognition;Target recognition;Image synthesis;Supervised learning;Generative adversarial networks;Feature extraction","image recognition;neural nets;radar computing;radar imaging;radar target recognition;supervised learning;synthetic aperture radar","recognition accuracy;semisupervised SAR ATR;conditional generative adversarial network;multidiscriminator;convolutional neural networks;synthetic aperture radar automatic target recognition;labeled SAR images;supervised learning;semisupervised learning-based methods;image generation capability;CNN;SCGAN-MD;CGAN;moving and stationary target acquisition and recognition database;MSTAR database","","3","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Adaptive Neighborhood Strategy Based Generative Adversarial Network for Hyperspectral Image Classification","H. Liang; W. Bao; B. Lei; J. Zhang; K. Qu","School of Computer Science and Engineering, North Minzu University, Yinchuan, China; School of Computer Science and Engineering, North Minzu University, Yinchuan, China; School of Computer Science and Engineering, North Minzu University, Yinchuan, China; School of Computer Science and Engineering, North Minzu University, Yinchuan, China; School of Computer Science and Engineering, North Minzu University, Yinchuan, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","862","865","Hyperspectral image (HSI) is usually composed of hundreds of continuous bands, leading a challenge task for pixel-level classification owing to high-dimensional spectral features and insufficient labeled samples. In this paper, an adaptive neighborhood strategy based generative adversarial network with (AN-GAN) for semi-supervised HSI classification is proposed. The proposed AN-GAN approach firstly uses superpixel algorithm, e.g., simple linear iterative clustering (SLIC), to generate multiple spatially homogeneous regions. Furthermore, each superpixel is merged with its spectrally similar neighbor superpixels. Then, for the reconstructed superpixels, the limited labeled samples are used to train discriminator, and a large number of unlabeled samples are utilized to generate noise using sparse autoencoder and also used to train discriminator for purpose of improving discriminator performance. Experiments were conducted on both Pavia University and Indian Pines datasets, which show that AN-GAN could provide better classification performance comparing with state-of-the-art classification models.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324088","National Natural Science foundation of Ningxia Province of China(grant numbers:2020AAC02028); Natural Science Foundation of Ningxia Province of China(grant numbers:NZ17111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324088","Hyperspectral classification;generative adversarial network;neighborhood adaptive;spectral-spatial features;semi-supervised learning","Generative adversarial networks;Hyperspectral imaging;Image classification;Generators;Feature extraction;Classification algorithms;Kernel","feature extraction;geophysical image processing;hyperspectral imaging;image classification;iterative methods;learning (artificial intelligence);pattern classification","spectrally similar neighbor superpixels;reconstructed superpixels;unlabeled samples;classification performance;state-of-the-art classification models;adaptive neighborhood strategy;generative adversarial network;hyperspectral image classification;continuous bands;challenge task;pixel-level classification owing;high-dimensional spectral features;insufficient labeled samples;semisupervised HSI classification;AN-GAN approach;simple linear iterative clustering;spatially homogeneous regions;superpixel","","3","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Generative Adversarial Network with Folded Spectrum for Hyperspectral Image Classification","W. Li; J. Yin; B. Han; H. Zhu","School of Astronautics, Beihang University, Beijing, China; School of Astronautics, Beihang University, Beijing, China; School of Astronautics, Beihang University, Beijing, China; School of Astronautics, Beihang University, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","883","886","Hyperspectral image (HSIs) with abundant spectral information but limited labeled dataset endows the rationality and necessity of semi-supervised spectral-based classification methods. Where, the utilizing approach of spectral information is significant to classification accuracy. In this paper, we propose a novel semi-supervised method based on generative adversarial network (GAN) with folded spectrum (FS-GAN). Specifically, the original spectral vector is folded to 2D square spectrum as input of GAN, which can generate spectral texture and provide larger receptive field over both adjacent and non-adjacent spectral bands for deep feature extraction. The generated fake folded spectrum, the labeled and unlabeled real folded spectrum are then fed to the discriminator for semi-supervised learning. A feature matching strategy is applied to prevent model collapse. Extensive experimental comparisons demonstrate the effectiveness of the proposed method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899034","Hyperspectral classification;generative adversarial network;semi-supervised learning","Generative adversarial networks;Feature extraction;Classification algorithms;Generators;Shape;Hyperspectral imaging","feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence);neural nets","classification accuracy;generative adversarial network;FS-GAN;original spectral vector;2D square spectrum;spectral texture;nonadjacent spectral bands;fake folded spectrum;semisupervised learning;hyperspectral image classification;abundant spectral information;labeled dataset;spectral-based classification methods;folded spectrum;feature matching strategy","","2","","9","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"SYNTHETIC MINORITY CLASS DATA BY GENERATIVE ADVERSARIAL NETWORK FOR IMBALANCED SAR TARGET RECOGNITION","Z. Luo; X. Jiang; X. Liu","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2459","2462","The deep convolutional neural networks (CNNs) have achieved the state of art performance in synthetic aperture radar (SAR) automatic target recognition (ATR). However, these networks often provide sub-optimal recognition results in the case of imbalanced SAR data distribution. In this paper, a synthetic minority class data method for improving imbalanced SAR target recognition using the generative adversarial network (GAN) is proposed. The minority class SAR data is first over-sampled by optimized data augmentation policies from automatic search method, which enlarge the training set for GAN. The progressive growing of GANs (PGGAN) is then trained on these data and generates high quality and diverse minority class SAR data to alleviate imbalanced data distribution. Experimental results on the designed imbalanced distributed Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset indicate that our method can effectively improve the recognition accuracy of minority class by approximately 11.68%.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323439","National Natural Science Foundation of China(grant numbers:61971279); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323439","SAR;imbalanced target recognition;generative adversarial network","Training;Synthetic aperture radar;Gallium nitride;Target recognition;Generative adversarial networks;Image resolution;Training data","image recognition;neural nets;radar computing;radar imaging;radar target recognition;supervised learning;synthetic aperture radar","Stationary Target Acquisition;imbalanced data distribution;diverse minority class SAR data;automatic search method;optimized data augmentation policies;GAN;synthetic minority class data method;imbalanced SAR data distribution;sub-optimal recognition results;synthetic aperture radar automatic target recognition;deep convolutional neural networks;imbalanced sar target recognition;generative adversarial network;recognition accuracy;Recognition dataset","","1","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Modeling Mountain Snowpack Dynamics with CGANS: A Validation Study","A. Manepalli; M. Mudigonda; A. Albert","terrafuse, inc., Berkeley, CA; terrafuse, inc., Berkeley, CA; terrafuse, inc., Berkeley, CA","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","3995","3998","In our previous work [1] we show the effectiveness of learning based schemes to emulate numerical simulations. Specifically, we explore the use of GANs to model Snow Water Equivalent (SWE). In this work, we present further analysis of these results, with a model now trained on 30 years of SWE data compared to 10 years for the previous. First, we study the model outputs with station data and find the outputs to closely match the station data. Second, we study the uncertainty of the cGAN based results. We study the correlation between noise and the outputs generated. We hope these contributions can help the community leverage the power of these deep learning models for future works.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324517","","Data models;Training;Time series analysis;Predictive models;Numerical models;Generative adversarial networks;Snow","learning (artificial intelligence);snow","CGANS;numerical simulations;GANs;model Snow Water Equivalent;SWE data;station data;cGAN based results;deep learning models;mountain snowpack dynamics","","","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Implementing New Feature Extraction Techniques for Characterization of Complex Mineral Signatures of Salty Regions on Mars","J. L. Bishop; M. Parente; A. M. Saranathan; Y. Itoh; C. M. Weitz; J. Flahaut; C. Gross; J. M. Danielsen; G. S. Usabal; J. K. Miura","SETI Institute & NASA-Ames, Mountain View, CA; University of Massachusetts at Amherst, MA; University of Massachusetts at Amherst, MA; University of Massachusetts at Amherst, MA; Planetary Science Institute, Tucson, AZ; CRPG-CNRS, France; Free University of Berlin, Berlin, Germany; San Jose State University, San Jose, CA; Brown University, Providence, RI; California Institute of Technology, Pasadena, CA","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","5147","5150","This study exploits recent advances in image calibration and feature extraction techniques for analysis of hyperspectral images acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) to characterize subtle geologic outcrops at the border of phyllosilicate-bearing and sulfate-bearing regions of Mars. Specifically, a unique spectral “doublet” feature at 2.21-2.23 and 2.26-2.28 μm is isolated to characterize salty regions that may represent a changing climate on Mars. The martian locations exhibiting these spectral features are identified and compared with terrestrial settings with similar geologic compositions.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324608","NASA(grant numbers:NNX15BB01,NNX19K1230,NNX16AG48G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324608","Mars;CRISM;feature extraction;minerals;water","Mars;Minerals;Geology;Feature extraction;Moon;Generative adversarial networks;Reconnaissance","feature extraction;Mars;minerals;planetary surfaces","salty regions;Mars;spectral features;new feature extraction techniques;complex mineral signatures;image calibration;hyperspectral images;Compact Reconnaissance Imaging Spectrometer;subtle geologic outcrops;phyllosilicate-bearing;sulfate-bearing regions","","","","19","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Urban Surface Simulation Through Image-to-Image Translation Deep Learning Algorithm using Optical Aerial Imagery","S. K. Das; P. PS; A. C. Pandey; B. H. A","Department of Geoinformatics, Central University of Jharkhand, Ranchi; RCG School of Infrastructure Design and Management, Indian Institute of Technology, Kharagpur; Department of Geoinformatics, Central University of Jharkhand, Ranchi; RCG School of Infrastructure Design and Management, Indian Institute of Technology, Kharagpur","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1500","1503","Digital Surface Model (DSM) provides the detailed structure and geometry of an urban environment. This paper proposes an approach of using a type of image-to-image translation deep learning model called cycle consistent adversarial networks for reconstructing DSM from monocular aerial imagery. The cycleGAN architecture consisted of two generators with an encoder-decoder network with skip connections and two discriminators that punishes structures at the scale of patches. The cycleGAN objective function was adapted for training on paired images. The evaluation was performed using mean square error (MSE) and zero normalized cross-correlation (ZNCC) for errors in reconstruction. cGAN model was considered as a baseline model for comparison of the proposed approach. The results using the proposed approach confirmed a higher reconstruction accuracy than previous studies that utilized conditional GAN.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323915","cycle consistent adversarial networks;deep learning;image-to-image translation;optical aerial imagery","Generators;Gallium nitride;Image reconstruction;Training;Generative adversarial networks;Surface reconstruction;Adaptation models","correlation methods;feature extraction;geophysical image processing;image matching;image reconstruction;image resolution;image sensors;learning (artificial intelligence);mean square error methods;stereo image processing","cycle consistent adversarial networks;DSM;monocular aerial imagery;cycleGAN architecture;encoder-decoder network;punishes structures;cycleGAN objective function;paired images;cGAN model;baseline model;urban Surface simulation;image-to-image translation deep learning algorithm;optical aerial imagery;Digital Surface Model;geometry;urban environment","","","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Seismic Signal Synthesis by Generative Adversarial Network with Gated Convolutional Neural Network Structure","Y. Li; B. Ku; G. Kim; J. -K. Ahn; H. Ko","School of Electrical Engineering, Korea University, Seoul, Rep. Korea; School of Electrical Engineering, Korea University, Seoul, Rep. Korea; School of Electrical Engineering, Korea University, Seoul, Rep. Korea; Korea Meteorological Administration, Rep. Korea; School of Electrical Engineering, Korea University, Seoul, Rep. Korea","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","3857","3860","Detecting earthquake events from seismic time series signal is a challenging task. Recently, detection methods based on machine learning have been developed to improve the accuracy and efficiency. However, accuracy of those methods rely on sufficient amount of high-quality training data. In many situations, the high-quality data is difficulty to obtain. We address and resolve this issue by using a Generative Adversarial Network (GAN) model for seismic signal synthesis. GAN already shows its powerful capability in generating high quality synthetic samples in multiple domains. In this paper, we propose a GAN model with gated CNN which can excellently capture sequential structure of seismic time series. We demonstrate its effectiveness via earthquake classification performance. The results show the synthetic data generated by our model indeed can improve the classification performance over the one trained with only real samples.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323670","deep learning;data augmentation;earthquake detection;generative adversarial network","Earthquakes;Data models;Generators;Logic gates;Generative adversarial networks;Gallium nitride;Deep learning","convolutional neural nets;earthquakes;learning (artificial intelligence);seismology","seismic signal synthesis;high quality synthetic samples;GAN model;gated CNN;sequential structure;earthquake classification performance;synthetic data;gated convolutional neural Network structure;earthquake events;seismic time series signal;detection methods;machine learning;high-quality training data;Generative Adversarial Network model","","","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Design and Evaluation of an Electronic Eye for Fire Detection in Human Space Capsule","B. Monish Moger; H. Abhishek; B. Pranav; P. Roy; P. Bharadwaj; A. Gupta; P. Sharma; J. Manikandan","Department of CSE and CORI, PES University, Bangalore, India; CORI PES University Bangalore, India; Department of ECE and CORI, PES University, Bangalore, India; Department of ECE and CORI, PES University, Bangalore, India; Department of ECE and CORI, PES University, Bangalore, India; Human Space Flight Technology Space Applications Centre ISRO, Ahmedabad, India; Human Space Flight Technology Space Applications Centre ISRO, Ahmedabad, India; Department of ECE and CORI, PES University, Bangalore, India","2022 IEEE International Conference on Aerospace Electronics and Remote Sensing Technology (ICARES)","30 Dec 2022","2022","","","1","7","Fire is often considered as a good friend, as it helps human in several ways, but the same is also considered as a dangerous foe, once it gets out of control. Fire can burn almost anything in no time and also depletes oxygen from the surrounding atmosphere, thus leading to casualties due to lack of oxygen, smoke and suffocation. Hence there is an urge to design early fire detection systems that can sense the initiation of fire and in turn activate the extinguishers to extinguish the fire, reducing the loss of property and life. In this paper an attempt is made to design and evaluate Electronic eye (E-Eye) for early fire detection in a human space capsule. Two variants of E-eye are proposed and their performances are compared. An attempt is also made to employ the concepts of machine learning using Generative Adversarial Network (GAN) models for fire detection. Maximum recognition accuracy of 100% with a prediction time of around 80ms was achieved using the proposed model. The proposed work is an outcome of a funded project from Indian Space Research Organization. The proposed prototypes can be easily employed in other places too such as houses, offices, storeroom, garage, etc.","","978-1-6654-6191-7","10.1109/ICARES56907.2022.9993503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9993503","Fire Detection;Electronic Eye;Embedded System Design;Human Space Capsule","Temperature measurement;Surveillance;Prototypes;Machine learning;Aerospace electronics;Predictive models;Generative adversarial networks","aerospace computing;aerospace safety;fires;learning (artificial intelligence);neural nets;object recognition;space vehicles","E-eye;early fire detection systems;electronic eye;GAN;generative adversarial network models;human space capsule;Indian space research organization;oxygen;recognition accuracy","","","","16","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"Generative Adversarial Network for Pansharpening With Spectral and Spatial Discriminators","A. Gastineau; J. -F. Aujol; Y. Berthoumieu; C. Germain","CNRS, UMR 5251, Bordeaux INP, Institut de Mathématique de Bordeaux (IMB), University of Bordeaux, Talence, France; CNRS, UMR 5251, Bordeaux INP, Institut de Mathématique de Bordeaux (IMB), University of Bordeaux, Talence, France; Laboratoire de Intégration du Matériau au Systéme (IMS), CNRS, UMR 5218, Bordeaux INP, University of Bordeaux, Talence, France; Laboratoire de Intégration du Matériau au Systéme (IMS), CNRS, UMR 5218, Bordeaux INP, University of Bordeaux, Talence, France","IEEE Transactions on Geoscience and Remote Sensing","8 Dec 2021","2022","60","","1","11","The pansharpening problem amounts to fusing a high-resolution panchromatic image with a low-resolution multispectral image so as to obtain a high-resolution multispectral image. Therefore, the preservation of the spatial resolution of the panchromatic image and the spectral resolution of the multispectral image is of key importance for the pansharpening problem. To cope with it, we propose a new method based on a bidiscriminator in a generative adversarial network (GAN) framework. The first discriminator is optimized to preserve textures of images by taking as input the luminance and the near-infrared band of images, and the second discriminator preserves the color by comparing the chroma components Cb and Cr. Thus, this method allows to train two discriminators, each one with a different and complementary task. Moreover, to enhance these aspects, the proposed method based on bidiscriminator, and called MDSSC-GAN SAM, considers a spatial and a spectral constraint in the loss function of the generator. We show the advantages of this new method on experiments carried out on Pléiades and World View 3 satellite images.","1558-0644","","10.1109/TGRS.2021.3060958","French State through the French National Research Agency (ANR) in the Framework of the Investments for the Future Programme IdEx Bordeaux—SysNum(grant numbers:ANR-10-IDEX-03-02); ANR(grant numbers:ANR-18-CE92-0050 SUPREMATIM); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9371303","Bidiscriminator;deep learning;generative adversarial network (GAN);pansharpening;remote sensing","Spatial resolution;Pansharpening;Gallium nitride;Generative adversarial networks;Vegetation mapping;Satellites;Generators","image colour analysis;image fusion;image resolution;image texture;neural nets;spectral analysis","pansharpening;high-resolution panchromatic image;low-resolution multispectral image;high-resolution multispectral image;spatial resolution;spectral resolution;generative adversarial network;spectral discriminator;spatial discriminator;loss function;image texture","","11","","29","IEEE","5 Mar 2021","","","IEEE","IEEE Journals"
"Improving Seismic Data Resolution With Deep Generative Networks","D. A. B. Oliveira; R. S. Ferreira; R. Silva; E. V. Brazil","IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil; IBM Research, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","22 Nov 2019","2019","16","12","1929","1933","Noisy traces, gaps in coverage, or irregular/inadequate trace spacing are common problems in both land and marine surveys, possibly hindering the geological interpretation of an area of interest. This problem has been typically addressed in the literature using prestack data; however, prestack data are not always available. As an alternative, poststack interpolations may aid the geological interpretation by increasing the spatial density of a seismic section and can also be used to reconstruct entire sections by interpolating neighboring traces, reducing field costs. In this letter, we evaluate the performance of conditional Generative Adversarial Networks (cGANs) as an interpolation tool for improving seismic data resolution on a public poststack seismic data set and compare our results with the traditional cubic interpolation. To perform the comparisons, we used structural similarity (SSIM), mean squared error (mse), and local binary patterns (LBPs) texture descriptor. The results show that cGANs outperform traditional algorithms by up to 72% and that the texture descriptor was able to better capture image similarities, producing results more coherent with the visual perception.","1558-0571","","10.1109/LGRS.2019.2913593","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715421","Artificial neural networks;geophysical image processing;image resolution","Interpolation;Generators;Training;Gallium nitride;Image resolution;Image reconstruction;Decoding","geophysical image processing;geophysical techniques;image reconstruction;image resolution;image texture;interpolation;neural nets;seismology","seismic data resolution;geological interpretation;poststack interpolations;seismic section;conditional generative adversarial networks;poststack seismic data;deep generative networks;traces interpolation","","14","","16","IEEE","15 May 2019","","","IEEE","IEEE Journals"
"SAR-to-Optical Image Translation Using Supervised Cycle-Consistent Adversarial Networks","L. Wang; X. Xu; Y. Yu; R. Yang; R. Gui; Z. Xu; F. Pu","Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Collaborative Sensing Laboratory, School of Electronic Information, Wuhan University, Wuhan, China; Electrical Engineering Department, Stanford University, Stanford, CA, USA; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Access","19 Sep 2019","2019","7","","129136","129149","Optical remote sensing (RS) data suffer from the limitation of bad weather and cloud contamination, whereas synthetic aperture radar (SAR) can work under all weather conditions and overcome this disadvantage of optical RS data. However, due to the imaging mechanism of SAR and the speckle noise, untrained people are difficult to recognize the land cover types visually from SAR images. Inspired by the excellent image-to-image translation performance of Generative Adversarial Networks (GANs), a supervised Cycle-Consistent Adversarial Network (S-CycleGAN) was proposed to generate large optical images from the SAR images. When the optical RS data are unavailable or partly unavailable, the generated optical images can be alternative data that aid in land cover visual recognition for untrained people. The main steps of SAR-to-optical image translation were as follows. First, the large SAR image was split to small patches. Then S-CycleGAN was used to translate the SAR patches to optical image patches. Finally, the optical image patches were stitched to generate the large optical image. A paired SAR-optical image dataset which covered 32 Chinese cities was published to evaluate the proposed method. The dataset was generated from Sentinel-1 (SEN-1) SAR images and Sentinel-2 (SEN-2) multi-spectral images. S-CycleGAN was applied to two experiments, which were SAR-to-optical image translation and cloud removal, and the results showed that S-CycleGAN could keep both the land cover and structure information well, and its performance was superior to some famous image-to-image translation models.","2169-3536","","10.1109/ACCESS.2019.2939649","National Basic Research Program of China (973 Program)(grant numbers:2016YFB0502600); Thirteen-Five Civil Aerospace Planning Project — Integration of Communication, Navigation and Remote Sensing Comprehensive Application Technology; Chinese Technology Research and Development of the Major Project of High-Resolution Earth Observation System(grant numbers:03-Y20A10-9001-15/16); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825802","SAR-to-optical image translation;visualization;GAN;Sentinel;cloud removal","Radar polarimetry;Optical imaging;Clouds;Optical sensors;Adaptive optics;Optical polarization;Gallium nitride","geophysical image processing;optical images;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","SAR-to-optical image translation;supervised Cycle-Consistent Adversarial Networks;optical remote sensing data;optical RS data;imaging mechanism;generated optical images;SAR patches;optical image patches;paired SAR-optical image dataset;Sentinel-1 SAR images;multispectral images;image-to-image translation models;S-CycleGAN;Chinese cities;land cover;Generative Adversarial Networks;weather conditions;cloud contamination;bad weather","","59","","58","CCBY","5 Sep 2019","","","IEEE","IEEE Journals"
"A Sea Clutter Suppression Method Based on Machine Learning Approach for Marine Surveillance Radar","J. Pei; Y. Yang; Z. Wu; Y. Ma; W. Huo; Y. Zhang; Y. Huang; J. Yang","Department of Electrical Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 May 2022","2022","15","","3120","3130","Marine surveillance radar is widely used in marine monitoring for its ability of observing sea surface all-time and all-weather. However, the radar target detection performance is seriously affected by the existence of sea clutter. In this article, we propose a new sea clutter suppression method based on machine learning approach. We first employ a cyclic structure network with a pair of generative adversarial networks to sufficiently learn the characteristics of sea clutter, which converts the problem of sea clutter suppression as a transformation from the clutter radar data domain to the clutter-free radar data domain. In addition, we propose a target-consistency loss for the cost function of the network to effectively preserve the target information while suppressing the sea clutter. Therefore, the proposed method can not only effectively remove the sea clutter from the radar data but also protect the target information from being damaged during sea clutter suppression, thereby achieving excellent sea clutter suppression performance. Experimental results have shown the superiorities of the proposed sea clutter suppression method on both simulated and measured marine surveillance radar data.","2151-1535","","10.1109/JSTARS.2022.3167410","National Natural Science Foundation of China(grant numbers:61901091,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757870","Generative adversarial networks (GAN);machine learning;marine surveillance radar;sea clutter suppression;target-consistency loss","Clutter;Radar;Radar clutter;Radar detection;Radar cross-sections;Surveillance;Sea surface","interference suppression;learning (artificial intelligence);marine radar;object detection;radar clutter;radar computing;radar detection;search radar","machine learning approach;clutter radar data domain;clutter-free radar data domain;target information;sea clutter suppression method;sea surface;radar target detection performance;sea clutter suppression performance;marine surveillance radar data;marine monitoring;generative adversarial networks;target-consistency loss","","3","","35","CCBY","14 Apr 2022","","","IEEE","IEEE Journals"
"A Latent Encoder Coupled Generative Adversarial Network (LE-GAN) for Efficient Hyperspectral Image Super-Resolution","Y. Shi; L. Han; L. Han; S. Chang; T. Hu; D. Dancey","Department of Computing and Mathematics, Faculty of Science and Engineering, Manchester Metropolitan University, Manchester, U.K; Department of Computing and Mathematics, Faculty of Science and Engineering, Manchester Metropolitan University, Manchester, U.K; Department of Computer Science, Brunel University London, Uxbridge, U.K; State Key Laboratory of Remote Sensing Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; College of Plant Protection, Hebei Agriculture University, Baoding, China; Department of Computing and Mathematics, Faculty of Science and Engineering, Manchester Metropolitan University, Manchester, U.K","IEEE Transactions on Geoscience and Remote Sensing","10 Aug 2022","2022","60","","1","19","Realistic hyperspectral image (HSI) super-resolution (SR) techniques aim to generate a high-resolution (HR) HSI with higher spectral and spatial fidelity from its low-resolution (LR) counterpart. The generative adversarial network (GAN) has proven to be an effective deep learning framework for image SR. However, the optimization process of existing GAN-based models frequently suffers from the problem of mode collapse, leading to the limited capacity of spectral–spatial invariant reconstruction. This may cause the spectral–spatial distortion to the generated HSI, especially with a large upscaling factor. To alleviate the problem of mode collapse, this work has proposed a novel GAN model coupled with a latent encoder (LE-GAN), which can map the generated spectral–spatial features from the image space to the latent space and produce a coupling component to regularize the generated samples. Essentially, we treat an HSI as a high-dimensional manifold embedded in a latent space. Thus, the optimization of GAN models is converted to the problem of learning the distributions of HR HSI samples in the latent space, making the distributions of the generated SR HSIs closer to those of their original HR counterparts. We have conducted experimental evaluations on the model performance of SR and its capability in alleviating mode collapse. The proposed approach has been tested and validated based on two real HSI datasets with different sensors (i.e., AVIRIS and UHD-185) for various upscaling factors (i.e.,  $\times 2$ ,  $\times 4$ , and  $\times 8$ ) and added noise levels (i.e.,  $\infty $ , 40, and 80 dB) and compared with the state-of-the-art SR models (i.e., hyperspectral coupled network (HyCoNet), low tensor-train rank (LTTR), band attention GAN (BAGAN), SR-GAN, and WGAN). Experimental results show that the proposed model outperforms the competitors on the SR quality, robustness, and alleviation of mode collapse. The proposed approach is able to capture spectral and spatial details and generate more faithful samples than its competitors. It has also been found that the proposed model is more robust to noise and less sensitive to the upscaling factor and has been proven to be effective in improving the convergence of the generator and the spectral–spatial fidelity of the SR HSIs.","1558-0644","","10.1109/TGRS.2022.3193441","Biotechnology and Biological Sciences Research Council (BBSRC)(grant numbers:BB/R019983/1,BB/S020969/1); Newton Fund Institutional Links through the Newton-Ungku Omar Fund Partnership [the grant is funded by the U.K. Department of Business, Energy, and Industrial Strategy (BEIS)](grant numbers:ID 332438911); Open Research Fund of Key Laboratory of Digital Earth Science, Chinese Academy of Sciences(grant numbers:2019LDE003); Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/W007762/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837938","Deep learning (DL);generative adversarial network (GAN);hyperspectral image (HSI) super-resolution (SR)","Superresolution;Generative adversarial networks;Generators;Spatial resolution;Optimization;Data models;Distortion","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image coding;image denoising;image reconstruction;image resolution;tensors","realistic hyperspectral image super resolution techniques;spectral spatial invariant reconstruction;spectral spatial distortion;generated spectral spatial features;spectral spatial fidelity;SR HSI;spatial details;spectral details;SR quality;SR-GAN;hyperspectral coupled network;state-of-the-art SR models;HSI datasets;generated SR;HR HSI samples;high dimensional manifold;coupling component;latent space;image space;LE-GAN;GAN model;generated HSI;GAN-based models;optimization process;image SR;deep learning framework;higher spectral fidelity;high resolution HSI;generative adversarial network;latent encoder","","","","69","IEEE","25 Jul 2022","","","IEEE","IEEE Journals"
"Semi-Supervised Fine-Grained Image Categorization Using Transfer Learning With Hierarchical Multi-Scale Adversarial Networks","P. Chen; P. Li; Q. Li; D. Zhang","Key Laboratory of Knowledge Automation for Industrial Processes, Ministry of Education, University of Science and Technology Beijing, Beijing, China; Beijing Key Laboratory of Knowledge Engineering for Materials Science, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Knowledge Automation for Industrial Processes, Ministry of Education, University of Science and Technology Beijing, Beijing, China; Beijing Key Laboratory of Knowledge Engineering for Materials Science, University of Science and Technology Beijing, Beijing, China","IEEE Access","30 Aug 2019","2019","7","","118650","118668","Fine-grained image categorization is still a challenging computer vision problem in recent years. Most of existing methods highly rely on massive labeled data which are scarce in many real world applications. It should also be noticed that progressive learning demands of existing data is very common today. That is, we may pay attention to more fine-grained information (like arctic tern, black tern, buttercup or tulip) in an existing data set with labels like “bird” and “flower”. It is reasonable to believe that the existing labels and model with transferable knowledge would be helpful to another related but different, fine-grained recognition task. In this context, an improved transfer deep learning approach with hierarchical multi-adversarial networks is proposed in this paper. With this approach, cross domain features are extracted by advanced deep encoders coarsely. After that, we annotate a small amount of images in the target domain, thereby creating the “active labels” which can provide instructions for adversarial learning. Then, the GAN-based hierarchical model is utilized to select cross domain categories and enhance related features so as to facilitate an effective transfer. In order to exploit useful local features, a novel adaptive attention mechanism, Region Adversarial Network (RAN) which can select attention regions during adversarial learning and generate valuable fine-grained features, is introduced in the article. We call the proposed hierarchical framework “Attentional Multi-Adversarial Networks (AMAN)”. Experimental results show that AMAN is able to augment cross domain features well-directly and build an effective classifier for fine-grained categorization in the target domain with fewer training samples and higher accuracies.","2169-3536","","10.1109/ACCESS.2019.2934476","Key Research and Development Program of Ningxia Hui Autonomous Region (Research and Application Demonstration of Key Technologies in High-Resolution Remote Sensing based Intelligent Monitoring for Spatial Planning)(grant numbers:2019BFG02009); Ministry of Information Industry of the People's Republic of China; National Natural Science Foundation of China(grant numbers:61801019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8794503","Fine-grained classification;transfer learning;deep learning;domain adaption;generative adversarial networks;attention","Feature extraction;Generative adversarial networks;Task analysis;Training;Computational modeling;Deep learning;Generators","computer vision;feature extraction;image recognition;learning (artificial intelligence);neural nets","transfer learning;massive labeled data;progressive learning demands;fine-grained information;transferable knowledge;fine-grained recognition task;improved transfer deep learning approach;cross domain features;advanced deep encoders;target domain;active labels;adversarial learning;GAN-based hierarchical model;cross domain categories;fine-grained features;hierarchical framework;computer vision problem;local features;adaptive attention mechanism;semisupervised fine-grained categorization;hierarchical multiscale adversarial networks;region adversarial network;attentional multiadversarial networks;AMAN","","5","","71","CCBY","12 Aug 2019","","","IEEE","IEEE Journals"
"Building Footprint Extraction from High Resolution Aerial Images Using Generative Adversarial Network (GAN) Architecture","A. Abdollahi; B. Pradhan; S. Gite; A. Alamri","Centre for Advanced Modeling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and IT, University of Technology Sydney, Sydney, NSW, Australia; Earth Observation Center, Institute of Climate Change, Universiti Kebangsaan Malaysia, Selangor, Malaysia; Computer Science and Information Technology Department, Symbiosis Institute of Technology, Symbiosis International (Deemed) University, Pune, India; Department of Geology and Geophysics, College of Science, King Saud University, Riyadh, Saudi Arabia","IEEE Access","1 Dec 2020","2020","8","","209517","209527","Building extraction with high accuracy using semantic segmentation from high-resolution remotely sensed imagery has a wide range of applications like urban planning, updating of geospatial database, and disaster management. However, automatic building extraction with non-noisy segmentation map and obtaining accurate boundary information is a big challenge for most of the popular deep learning methods due to the existence of some barriers like cars, vegetation cover and shadow of trees in the high-resolution remote sensing imagery. Thus, we introduce an end-to-end convolutional neural network called Generative Adversarial Network (GAN) in this study to tackle these issues. In the generative model, we utilized SegNet model with Bi-directional Convolutional LSTM (BConvLSTM) to generate the segmentation map from Massachusetts building dataset containing high-resolution aerial imagery. BConvLSTM combines encoded features (containing of more local information) and decoded features (containing of more semantic information) to improve the performance of the model even with the presence of complex backgrounds and barriers. The adversarial training method enforces long-range spatial label vicinity to tackle with the issue of covering building objects with the existing occlusions such as trees, cars and shadows and achieve high-quality building segmentation outcomes under the complex areas. The quantitative results obtained by the proposed technique with an average F1-score of 96.81% show that the suggested approach could achieve better results through detecting and adjusting the difference between the segmentation model output and the reference map compared to other state-of-the-art approaches such as autoencoder method with 91.36%, SegNet+BConvLSTM with 95.96%, FCN-CRFs with 95.36%% SegNet with 94.77%, and GAN-SCA model with 96.36% accuracy.","2169-3536","","10.1109/ACCESS.2020.3038225","Centre for Advanced Modeling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and IT; University of Technology Sydney (UTS); Researchers Supporting Project(grant numbers:RSP-2020/14); King Saud University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9260150","Building extraction;GAN;remote sensing;SegNet","Training;Image segmentation;Buildings;Semantics;Generative adversarial networks;Feature extraction;Gallium nitride","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image classification;image segmentation;learning (artificial intelligence);object detection;recurrent neural nets;remote sensing","building footprint extraction;high resolution aerial images;generative adversarial network;semantic segmentation;high-resolution remotely;urban planning;geospatial database;disaster management;automatic building extraction;nonnoisy segmentation map;boundary information;popular deep learning methods;cars;vegetation cover;trees;high-resolution remote sensing imagery;end-to-end convolutional neural network;generative model;SegNet model;Massachusetts building dataset;high-resolution aerial imagery;local information;semantic information;adversarial training method;building objects;high-quality building segmentation outcomes;segmentation model output;reference map;SegNet+BConvLSTM;GAN-SCA;bi-directional convolutional LSTM;long-range spatial label vicinity","","34","","53","CCBY","16 Nov 2020","","","IEEE","IEEE Journals"
"Stingray Detection of Aerial Images Using Augmented Training Images Generated by a Conditional Generative Model","Y. -M. Chou; C. -H. Chen; K. -H. Liu; C. -S. Chen","MOST Joint Research Center for AI Technology and All Vista Healthcare; MOST Joint Research Center for AI Technology and All Vista Healthcare; Department of Mechanical Electro-mechanical Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan; MOST Joint Research Center for AI Technology and All Vista Healthcare","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","1484","14846","In this paper, we present an object detection method that tackles the stingray detection problem based on aerial images. In this problem, the images are aerially captured on a sea-surface area by using an Unmanned Aerial Vehicle (UAV), and the stingrays swimming under (but close to) the sea surface are the target we want to detect and locate. To this end, we use a deep object detection method, faster RCNN, to train a stingray detector based on a limited training set of images. To boost the performance, we develop a new generative approach, conditional GLO, to increase the training samples of stingray, which is an extension of the Generative Latent Optimization (GLO) approach. Unlike traditional data augmentation methods that generate new data only for image classification, our proposed method that mixes foreground and background together can generate new data for an object detection task, and thus improve the training efficacy of a CNN detector. Experimental results show that satisfiable performance can be obtained by using our approach on stingray detection in aerial images.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575348","","Training;Gallium nitride;Object detection;Generators;Sea surface;Generative adversarial networks;Detectors","autonomous aerial vehicles;geophysical image processing;image classification;learning (artificial intelligence);object detection;oceanographic techniques;optimisation;recurrent neural nets;remote sensing","training set;generative approach;traditional data augmentation methods;image classification;object detection task;training efficacy;aerial images;stingray detection problem;sea-surface area;Unmanned Aerial Vehicle;deep object detection method;stingray detector;generative latent optimization approach;augmented training images;conditional generative model;target detection;faster RCNN;conditional GLO approach;CNN detector","","2","","20","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"SAR-to-Optical Image Translation Using SSIM Loss Based Unpaired GAN","J. Hwang; Y. Shin","School of Electronic Engineering, Soongsil University, Seoul, Korea; School of Electronic Engineering, Soongsil University, Seoul, Korea","2022 13th International Conference on Information and Communication Technology Convergence (ICTC)","25 Nov 2022","2022","","","917","920","The synthetic aperture radar (SAR) image can be acquired regardless of weather condition and time through radar reflection. However, visual effects and observations of the SAR image is difficult due to single channel information and speckle noise. In this paper, we present the disadvantage that cycle-consistency loss focuses only on texture based on the CycleGAN, and propose to apply the dual contrastive learning GAN (DCLGAN) that maximizes mutual information between SAR and optical images by learning the correlation of image patches. Also, we propose a method to apply the structural similarity index measure (SSIM) loss instead of cycle-consistency loss to address the lack of expressiveness by emphasizing structural features. The proposed DCLGAN eliminates cycle-consistency loss of CycleGAN and maximizes mutual information through contrastive learning to translates unpaired SAR-optical images. In addition, the proposed SSIM loss helps translate the SAR-optimal image by recognizing perceived changes in structural information. We confirm the performance through comparative experiments with existing models to verify unpaired SAR-to-optical image translation.","2162-1241","978-1-6654-9939-2","10.1109/ICTC55196.2022.9952649","IITP; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952649","Synthetic aperture radar (SAR);SAR-to-optical image translation;structure similarity index measure (SSIM);cycle-consistent generative adversarial network (CycleGAN);contrastive learning","Optical losses;Integrated optics;Generative adversarial networks;Optical imaging;Loss measurement;Radar polarimetry;Optical reflection","image texture;optical images;radar imaging;remote sensing by radar;speckle;synthetic aperture radar","cycle-consistency loss;dual contrastive learning GAN;image patches;mutual information;radar reflection;SAR image;SAR-optimal image;SAR-to-optical image translation;single channel information;speckle noise;SSIM loss;structural information;structural similarity index measure loss;synthetic aperture radar image;translates unpaired SAR-optical images;visual effects;weather condition","","","","17","IEEE","25 Nov 2022","","","IEEE","IEEE Conferences"
"A Weakly-Supervised Change Detection for Multispectral Earth Observation Imagery using a Long Short- Term Memory Classifier with a Virtual Training Data Neural Generator","I. -A. Gîrlă; V. -E. Neagoe","Department of Applied Electronics and Information Engineering, Polytechnic University of Bucharest, Bucharest, Romania; Department of Applied Electronics and Information Engineering, Polytechnic University of Bucharest, Bucharest, Romania","2022 14th International Conference on Communications (COMM)","13 Jul 2022","2022","","","1","6","This paper proposes a novel approach to improve accuracy of weakly-supervised change detection for multispectral Earth Observation (EO) imagery. The method is based on the idea to use an initial small-size EO labeled dataset to generate a larger set of virtual data. We have considered two variants of virtual data-generators based on the general architecture called Generative Adversarial Network (GAN): MLPGAN and LSGAN. The resulting virtual dataset is used to train a simple Long Short-Term Memory (LSTM) classifier. The proposed method is evaluated using the Mexico dataset acquired by the Thematic Mapper (TM) sensor of the Landsat 5 satellite. For each acquisition date, two spectral bands are considered (B4, B5). The two images have been acquired in April 2000 and May 2002, respectively. We have evaluated the change detection performances (OA, Kappa, MAR, and FAR) using two virtual data generators corresponding to considered GAN architectures. As a benchmark method, we have considered the case when the LSTM classifier is trained with the original small-size dataset without synthetic data generation.","","978-1-6654-9485-4","10.1109/COMM54429.2022.9817344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817344","Weakly-supervised change detection;synthetic training data generation (STDG);Generative Adversarial Network (GAN);Long Short Term Memory (LSTM) classifier","Earth;Satellites;Artificial satellites;Training data;Benchmark testing;Generative adversarial networks;Generators","geophysical image processing;geophysical signal processing;geophysical techniques;image classification;learning (artificial intelligence);neural nets;remote sensing","weakly-supervised change detection;multispectral Earth Observation imagery;Long Short- Term Memory classifier;virtual training data neural generator;initial small-size EO;virtual data-generators;Generative Adversarial Network;resulting virtual dataset;Short-Term Memory classifier;Mexico dataset;change detection performances;virtual data generators;considered GAN architectures;LSTM classifier;original small-size dataset;synthetic data generation","","","","21","IEEE","13 Jul 2022","","","IEEE","IEEE Conferences"
"Fine-grained Adversarial Image Inpainting with Super Resolution","Y. Li; B. Jiang; Y. Lu; L. Shen",Beijing Institute of Remote Sensing Information; Beijing Institute of Remote Sensing Information; Beijing Institute of Remote Sensing Information; Beijing Institute of Remote Sensing Information,"2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","8","Image inpainting refers to synthesizing plausible contents for images with missing regions. However, current methods often create blurry textures, distorted structures and loss of details, especially when the image has complex scenes or large missing regions. We propose a fine-grained adversarial image inpainting model with super resolution. It performs a coarse-to-fine inpainting procedure in two stages. The proposed generator first synthesizes initial predictions of the missing regions with a novel encoder-decoder structure. Then it refines the predicted missing regions by generating high-frequency details via super resolution. We evaluate the proposed from both pixel level and semantic level. Experiments demonstrate that the proposed can generate higher quality inpainting results than the baseline models in both metrics.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852241","Image Inpainting;Image Completion;Super Resolution","Image resolution;Generators;Feature extraction;Generative adversarial networks;Image restoration;Decoding;Neural networks","image coding;image reconstruction;image resolution;image restoration;image texture","super resolution;plausible contents;blurry textures;distorted structures;complex scenes;fine-grained adversarial image inpainting model;coarse-to-fine inpainting procedure;predicted missing regions;high-frequency details;encoder-decoder structure;higher quality inpainting","","3","","29","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"A Residual Dense Generative Adversarial Network For Pansharpening With Geometrical Constraints","A. GASTINEAU; J. -F. AUJOL; Y. BERTHOUMIEU; C. GERMAIN","Bordeaux INP, CNRS, IMB, UMR 5251, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMB, UMR 5251, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMS, UMR 5218, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMS, UMR 5218, Univ. Bordeaux, Talence, France","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","493","497","The pansharpening problem consists in fusing a high resolution panchromatic image with a low resolution multispectral image in order to obtain a high resolution multispectral image. In this paper, we adapt a Residual Dense architecture for the generator in a Generative Adversarial Network framework. Indeed, this type of architecture avoids the vanishing gradient problem faced when training a network by re-injecting previous information thanks to dense and residual connections. Moreover, an important point for the pansharpening problem is to preserve the geometry of the image. Hence, we propose to add a regularization term in the loss function of the generator: it preserves the geometry of the target image so that a better solution is obtained. In addition, we propose geometrical measures that illustrate the advantages of this new method.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9191230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191230","Pansharpening;Generative Adversarial Network;residual dense network;regularization;remote sensing","Generators;Gallium nitride;Spatial resolution;Generative adversarial networks;Geometry;Satellites","geometry;hyperspectral imaging;image colour analysis;image fusion;image resolution;neural nets","geometrical measures;Residual Dense Generative Adversarial Network;geometrical constraints;pansharpening problem;low resolution multispectral image;high resolution multispectral image;residual dense architecture;vanishing gradient problem;residual connections;high resolution panchromatic image fusion;regularization term;loss function;target image geometry","","3","","22","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"A modified Generative Adversarial Network (GAN) architecture for land use classification","S. Ansith; A. A. Bini","Department of Electronics and Communication Engineering, Indian Institute of Information Technology Kottayam, Kerala, India; Department of Electronics and Communication Engineering, Indian Institute of Information Technology Kottayam, Kerala, India","2021 IEEE Madras Section Conference (MASCON)","19 Oct 2021","2021","","","1","6","Land cover and usage classification with satellite images plays an important role in many applications such as land resource management, urban planning, precision agriculture and environmental protection. Faster and easier land cover and usage classification without the prior knowledge of the terrain and training sample assignment can be done using deep learning algorithms. Early works in land use classification are mainly focused on machine learning algorithms. In the past few years some deep learning (DL) architectures are also used in the land cover and usage classification purposes. But these DL architectures need large amounts of training samples to get higher accuracy. In this paper, a modified Generative Adversarial Network (GAN) architecture has been proposed for land use classification. This deep learning model performs physical atmospheric corrections as a pre-processing step. Moreover, the model has shown to perform an efficient classification based on the statistical qualifications performed herein with a limited training dataset acquired from UC Merced land use dataset.","","978-1-6654-0405-1","10.1109/MASCON51689.2021.9563609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563609","land use classification;remote sensing;GAN;deep learning algorithm;UC Merced land use dataset","Deep learning;Machine learning algorithms;Satellites;Atmospheric modeling;Urban planning;Generative adversarial networks;Feature extraction","agriculture;geophysical image processing;image classification;land cover;learning (artificial intelligence);pattern classification","modified Generative Adversarial Network architecture;land use classification;land resource management;easier land cover;terrain;training sample assignment;deep learning algorithms;machine learning algorithms;years some deep learning architectures;usage classification purposes;deep learning model performs physical atmospheric corrections;UC Merced land use dataset","","","","25","IEEE","19 Oct 2021","","","IEEE","IEEE Conferences"
"DAugNet: Unsupervised, Multisource, Multitarget, and Life-Long Domain Adaptation for Semantic Segmentation of Satellite Images","O. Tasar; A. Giros; Y. Tarabalka; P. Alliez; S. Clerc","Alliez are with Computer Science Department, the Université Côte d’Azur and INRIA, Project-Team TITANE, Sophia Antipolis, France; Center National d’Études Spatiales, Toulouse, France; LuxCarta Technology, Mouans Sartoux, France; Alliez are with Computer Science Department, the Université Côte d’Azur and INRIA, Project-Team TITANE, Sophia Antipolis, France; ACRI-ST, Biot, France","IEEE Transactions on Geoscience and Remote Sensing","20 Jan 2021","2021","59","2","1067","1081","The domain adaptation of satellite images has recently gained increasing attention to overcome the limited generalization abilities of machine learning models when segmenting large-scale satellite images. Most of the existing approaches seek for adapting the model from one domain to another. However, such single-source and single-target setting prevents the methods from being scalable solutions since, nowadays, multiple sources and target domains having different data distributions are usually available. Besides, the continuous proliferation of satellite images necessitates the classifiers to adapt to continuously increasing data. We propose a novel approach, coined DAugNet, for unsupervised, multisource, multitarget, and life-long domain adaptation of satellite images. It consists of a classifier and a data augmentor. The data augmentor, which is a shallow network, is able to perform style transfer between multiple satellite images in an unsupervised manner, even when new data are added over time. In each training iteration, it provides the classifier with diversified data, which makes the classifier robust to large data distribution difference between the domains. Our extensive experiments prove that DAugNet significantly better generalizes to new geographic locations than the existing approaches.","1558-0644","","10.1109/TGRS.2020.3006161","ACRI-ST; Centre National d’Études Spatiales (CNES); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137717","Convolutional neural networks (CNNs);dense labeling;domain adaptation;generative adversarial networks (GANs);life-long adaption;multisource adaption;multitarget adaption;semantic segmentation","Satellites;Training;Semantics;Image segmentation;Adaptation models;Remote sensing;Standardization","image classification;image segmentation;learning (artificial intelligence)","DAugNet;life-long domain adaptation;machine learning models;segmenting large-scale satellite images;single-target setting;target domains;classifier;data augmentor;multiple satellite images;data distribution difference","","34","","61","IEEE","9 Jul 2020","","","IEEE","IEEE Journals"
"An Aero-Engine RUL Prediction Method Based on VAE-GAN","Y. Peng; X. Pan; S. Wang; C. Wang; J. Wang; J. Wu","College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; Beijing Institute of Remote Sensing Information, Beijing, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Computer Science and Engineering, Northeastern University, Shenyang, China","2021 IEEE 24th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","28 May 2021","2021","","","953","957","As an important index of aero-engine, Remaining Useful Life (RUL) is the key content of prediction. Due to the good generation characteristics of Variational Auto-encoder (VAE) and Generation Adversarial Network (GAN) networks, this paper proposes a Health Index (HI) curve generation method based on VAE-GAN. After that, sensor sequence prediction is carried out through Bidirectional Long Short-Term Memory Network (BLSTM). The two networks are parallel, and then RUL prediction is carried out by synthesizing the data of the two networks. As far as the author knows, this is the first use of VAE-GAN in Prognostics Health Management (PHM). It is verified on the Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dataset. Finally, the results show that the VAE-GAN network is effective and superior in RUL prediction. At the same time, the proposed parallel network is superior to other RUL prediction methods by generating HI curves.","","978-1-7281-6597-4","10.1109/CSCWD49262.2021.9437836","National Key Research and Development Program of China(grant numbers:2018YFB1701600); National Natural Science Foundation of China(grant numbers:61871107); China Academy of Space Technology(grant numbers:20180105); Fundamental Research Funds for the Central Universities(grant numbers:N171612014,N170308028,N180708009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437836","aero-engine;RUL;VAE-GAN;BLSTM;C-MAPSS","Conferences;Prediction methods;Generative adversarial networks;Collaborative work;Systems simulation;Indexes;Prognostics and health management","aerospace computing;aerospace engines;aerospace propulsion;condition monitoring;recurrent neural nets;remaining life assessment","variational auto-encoder;bidirectional long short-term memory network;commercial modular aero-propulsion system simulation;VAE-GAN network;RUL prediction methods;aero-engine RUL prediction method;generation adversarial network networks;health index curve generation method;remaining useful life","","2","","15","IEEE","28 May 2021","","","IEEE","IEEE Conferences"
"Infrared and Visible Image Fusion via Multi-discriminators Wasserstein Generative Adversarial Network","J. Li; H. Huo; K. Liu; C. Li; S. Li; X. Yang","School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; Remote sensing center of public security, People’s Public Security University of China, BeiJing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China","2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)","17 Feb 2020","2019","","","2014","2019","Generative adversarial network (GAN) has been widely applied to infrared and visible image fusion. However, the existing GAN-based image fusion methods only establish one discriminator in the network to make the fused image capture gradient information from the visible image, which may result in the loss of some infrared intensity information and texture information on the fused images. To solve this problem and improve the performance of GAN, we extend GAN to multiple discriminators and propose an end-to-end multi-discriminators Wasserstein generative adversarial network (MD-WGAN). In this framework, the fused image can preserve major infrared intensity and detail information from the first discriminator, and keep more texture information that existing in visible image from the second discriminator. We also design a texture loss function via local binary patterns to preserve more texture from visible image. The extensive qualitative and quantitative experiments show the advantages of our method compared with other state-of-the-art fusion methods.","","978-1-7281-4550-1","10.1109/ICMLA.2019.00322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999112","Wasserstein generative adversarial network, image fusion, infrared image, visible image","Generators;Generative adversarial networks;Image fusion;Gallium nitride;Machine learning;Security;Information technology","gradient methods;image fusion;image texture;infrared imaging;neural nets","visible image;fused image capture gradient information;infrared intensity information;texture information;end-to-end multidiscriminators Wasserstein generative adversarial network;detail information;GAN-based image fusion methods","","1","","29","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"The Reconstruction Method of SAR Image Ambiguous Area based on Deep Learning","Y. Gao; F. Zou; W. Yang; J. Chen","School of Electronic and Information Engineering, BeiHang University, Beijing, China; Beijing Institute of Remote Sensing Information, Beijing, China; School of Electronic and Information Engineering, BeiHang University, Beijing, China; School of Electronic and Information Engineering, BeiHang University, Beijing, China","2022 3rd China International SAR Symposium (CISS)","12 Dec 2022","2022","","","1","6","The ambiguities of Synthetic Aperture Radar (SAR) images have a serious impact on image quality and object detection accuracy. In recent years, Generative Adversarial Network (GAN) models based on Convolution Neural Networks (CNN) have been widely used in image generation and inpainting in the field of Computer Vision, making it possible to reconstruct SAR ambiguous regions using Deep Learning technology. Based on deep learning technology, the SAR image ambiguous area reconstruction method adopts the GAN model, which can achieve an ideal reconstruction of the marked area with SAR ambiguities. Compared with the SAR image ambiguous area reconstruction method based on traditional image processing, new method based on GAN has faster processing efficiency and better reconstruction result, can fully extract the image information, and learn the reconstruction from a large scale of area within complex background. To deal with the problem that the image reconstruction model based on deep learning has limited Receptive Field and difficult to reconstruct a large area of image, this paper introduces a conditional GAN structure based on the Fast Fourier Convolution layer (FFC layer) and studies the effect of its improvement to the feature extraction ability after the introduction of FFC. Finally, using a large-mask inpainting model based on FFC structure, a SAR image ambiguous area reconstruction method based on deep learning is purposed in this paper.","","979-8-3503-9871-7","10.1109/CISS57580.2022.9971258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9971258","Synthetic Aperture Radar;Generative Adversarial Network;Image Inpainting;Fast Fourier Convolution layer","Deep learning;Analytical models;Convolution;Reconstruction algorithms;Maintenance engineering;Generative adversarial networks;Feature extraction","computer vision;feature extraction;image processing;image reconstruction;learning (artificial intelligence);neural nets;object detection;radar imaging;synthetic aperture radar","deep learning technology;Generative Adversarial Network models;image generation;image quality;image reconstruction model;inpainting;object detection accuracy;SAR ambiguities;SAR ambiguous regions;SAR image ambiguous area reconstruction method;Synthetic Aperture Radar images","","","","10","IEEE","12 Dec 2022","","","IEEE","IEEE Conferences"
"Impact of Satellite Sounding Data on Virtual Visible Imagery Generation Using Conditional Generative Adversarial Network","J. -H. Kim; S. Ryu; J. Jeong; D. So; H. -J. Ban; S. Hong","Department of Environment, Energy and Geoinfomatics, Sejong University, Seoul, South Korea; Department of Environment, Energy and Geoinfomatics, Sejong University, Seoul, South Korea; National Institute of Environmental Research, Incheon, South Korea; Department of Environment, Energy and Geoinfomatics, Sejong University, Seoul, South Korea; Department of Environment, Energy and Geoinfomatics, Sejong University, Seoul, South Korea; Department of Research and Development, DeepThoTh Co., Ltd., Seoul, Republic of Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Aug 2020","2020","13","","4532","4541","The visible band of satellite sensors is of limited use during the night due to a lack of solar reflection. This study presents an improved conditional generative adversarial networks (CGANs) model to generate virtual nighttime visible imagery using infrared (IR) multiband satellite observations and the brightness temperature difference between the two IR bands in the communication, ocean, and meteorological satellite. For the summer daytime case study with visible band imagery, our multiband CGAN model showed better statistical results [correlation coefficient (CC) = 0.952, bias = -1.752 (in a digital number (DN) unit from 0 to 255, converted from reflectance from 0 to 1), and root-mean-square-error (RMSE) = 26.851 DN] than the single-band CGAN model using a pair of visible and IR bands (CC = 0.916, bias = -4.073 DN, and RMSE = 35.349 DN). The proposed multiband CGAN model performed better than the single-band CGAN model, particularly, in convective clouds and typhoons, because of the sounding effects from the water vapor band. In addition, our multiband CGAN model provided detailed patterns for clouds and typhoons at twilight. Therefore, our results could be used for visible-based nighttime weather analysis of convective clouds and typhoons, using data from next-generation geostationary meteorological satellites.","2151-1535","","10.1109/JSTARS.2020.3013598","Korea Meteorological Administration Research and Development Program(grant numbers:KMI2020-00510); National Institute of Environment Research (NIER); Ministry of Environment(grant numbers:NIER-2020-01-01-004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9154540","Clouds;conditional generative adversarial network (CGAN);deep learning;multiband;nighttime;typhoon;visible (VIS)","Cloud computing;Satellites;Meteorology;Gallium nitride;Ocean temperature;Generative adversarial networks;Sea surface","artificial satellites;atmospheric techniques;clouds;convection;data acquisition;infrared imaging;remote sensing by laser beam","twilight;next-generation geostationary meteorological satellites;visible-based nighttime weather analysis;water vapor band;typhoons;convective clouds;single-band CGAN model;multiband CGAN model;visible band imagery;summer daytime case study;IR bands;brightness temperature difference;infrared multiband satellite observations;virtual nighttime visible imagery;CGANs;solar reflection;satellite sensors;conditional generative adversarial network;virtual visible imagery generation;satellite sounding data","","8","","35","CCBY","3 Aug 2020","","","IEEE","IEEE Journals"
"Delving Into Classifying Hyperspectral Images via Graphical Adversarial Learning","G. Wang; P. Ren","College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 May 2020","2020","13","","2019","2031","Recent remote sensing literature has seen generative adversarial network (GAN)-based models developed for hyperspectral image classification, especially in a spatiospectral manner. The intuition is that training classifiers with additional generated hyperspectral data improves model generalization and hence increases classification accuracy. Existing GAN-based hyperspectral image classification methods tend to straightforwardly characterize spatiospectral characteristics of hyperspectral data subject to basic distributions (e.g., Gaussian or uniform distributions). However, hyperspectral imageries have high dimensions in both spectral and spatial representations, which are possibly derived from a latent space following more sophisticated distributions. In this scenario, we believe that comprehensively modeling the latent space would favor accurate hyperspectral classification. To this end, we develop a graphical adversarial learning (GAL) framework that explores the latent variable structure for generating diversified hyperspectral samples. The comprehensive modeling strategy enables GAL to be capable of accurately characterizing full-depth hyperspectral data such that it establishes an end-to-end framework that does not require reduction on spectral bands. We conduct extensive experiments on three public hyperspectral datasets in terms of processing full-depth hyperspectral images without dimension reduction. The experimental results validate that, first, our GAL excels at full-depth hyperspectral image classification, and second, our GAL is competitive with state-of-the-art methods which use global data (i.e., both training and testing data) for learning spectral reduction.","2151-1535","","10.1109/JSTARS.2020.2992310","National Key R&D Program of China(grant numbers:2019YFC1408400); National Natural Science Foundation of China(grant numbers:61971444); Innovative Research Team Program for Young Scholars at Universities in Shandong Province(grant numbers:2020KJN010); Natural Science Foundation of Shandong Province(grant numbers:ZR2019MF019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086159","Generative adversarial networks (GANs);graphical learning;hyperspectral image classification","Hyperspectral imaging;Gallium nitride;Data models;Training;Graphical models","hyperspectral imaging;image classification;learning (artificial intelligence);neural nets;remote sensing","remote sensing literature;GAN-based hyperspectral image classification methods;spatiospectral characteristics;hyperspectral data;spectral representations;spatial representations;graphical adversarial learning framework;GAL;generative adversarial network","","2","","47","CCBY","4 May 2020","","","IEEE","IEEE Journals"
"CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds","A. H. Nielsen; A. Iosifidis; H. Karstoft","Department of Engineering, Aarhus University, Aarhus N, Denmark; Department of Engineering, Aarhus University, Aarhus N, Denmark; Department of Engineering, Aarhus University, Aarhus N, Denmark","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Apr 2021","2021","14","","3485","3494","Forecasting the formation and development of clouds is a central element of modern weather forecasting systems. Incorrect cloud forecasts can lead to major uncertainty in the overall accuracy of weather forecasts due to their intrinsic role in the Earth's climate system. Few studies have tackled this challenging problem from a machine learning point-of-view due to a shortage of high-resolution datasets with many historical observations globally. In this article, we present a novel satellite-based dataset called “CloudCast.” It consists of 70 080 images with 10 different cloud types for multiple layers of the atmosphere annotated on a pixel level. The spatial resolution of the dataset is 928 × 1530 pixels (3 × 3 km per pixel) with 15-min intervals between frames for the period January 1, 2017 to December 31, 2018. All frames are centered and projected over Europe. To supplement the dataset, we conduct an evaluation study with current state-of-the-art video prediction methods such as convolutional long short-term memory networks, generative adversarial networks, and optical flow-based extrapolation methods. As the evaluation of video prediction is difficult in practice, we aim for a thorough evaluation in the spatial and temporal domain. Our benchmark models show promising results but with ample room for improvement. This is the first publicly available global-scale dataset with high-resolution cloud types on a high temporal granularity to the authors' best knowledge.","2151-1535","","10.1109/JSTARS.2021.3062936","Danske Commodities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366908","Atmospheric forecasting;remote sensing datasets;spatiotemporal deep learning","Clouds;Atmospheric modeling;Forecasting;Spatial resolution;Satellites;Weather forecasting;Predictive models","atmospheric techniques;clouds;extrapolation;geophysical image processing;image sequences;learning (artificial intelligence);remote sensing;weather forecasting","video prediction methods;short-term memory networks;generative adversarial networks;optical flow-based extrapolation methods;spatial domain;temporal domain;publicly available global-scale dataset;high-resolution cloud types;high temporal granularity;CloudCast;forecasting clouds;weather forecasting systems;incorrect cloud forecasts;Earth climate system;high-resolution datasets;historical observations;satellite-based dataset;AD 2017 01 01 to 2018 12 31","","3","","48","CCBY","2 Mar 2021","","","IEEE","IEEE Journals"
"Soil Moisture Estimation Using Sentinel-1/-2 Imagery Coupled With CycleGAN for Time-Series Gap Filing","N. Efremova; M. E. A. Seddik; E. Erten","School of Business and Management, University of Queen Mary of London, London, U.K.; Huawei Paris Research Centre, Paris, France; Faculty of Civil Engineering, Istanbul Technical University, Istanbul, Turkey","IEEE Transactions on Geoscience and Remote Sensing","1 Mar 2022","2022","60","","1","11","Fast soil moisture content (SMC) mapping is necessary to support water resource management and to understand crop growth, quality, and yield. Therefore, earth observation (EO) plays a key role due to its ability of almost real-time monitoring of large areas at a low cost. This study aimed to explore the possibility of taking advantage of freely available Sentinel-1 (S1) and Sentinel-2 (S2) EO data for the simultaneous prediction of SMC with cycle-consistent adversarial network (CycleGAN) for time-series gap filling. The proposed methodology, first, learns latent low-dimensional representation of the satellite images, then learns a simple machine learning (ML) model on top of these representations. To evaluate the methodology, a series of vineyards, located in South Australia ’s Eden valley are chosen. Specifically, we presented an efficient framework for extracting latent features from S1 and S2 imagery. We showed how one could use S1 to S2 feature translation based on CycleGAN using S1 and S2 time series when there are missing images acquired over an area of interest. The resulting data in our study is then used to fill gaps in time-series data. We used the resulting latent representations to predict SMC with various ML tools. In the experiments, CycleGAN and the autoencoders were trained with data randomly chosen around the site of interest, so we could augment the existing dataset. The best performance was demonstrated with random forest (RF) algorithm, whereas linear regression model demonstrated significant overfitting. The experiments demonstrate that the proposed methodology outperforms the compared state-of-the-art methods if there are missing optical and synthetic-aperture radar (SAR) images.","1558-0644","","10.1109/TGRS.2021.3134127","Space Research and Innovation Network for Technology (SPRINT)(grant numbers:1243832); Research Fund of the Istanbul Technical University(grant numbers:MGA-2021-43018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643416","Agriculture;generative adversarial networks (GANs);machine learning (ML);Sentinel-1;Sentinel-2;soil moisture (SM);unsupervised domain adaptation","Soil moisture;Feature extraction;Satellites;Data models;Predictive models;Training;Soil measurements","crops;feature extraction;geophysical image processing;hydrological techniques;image representation;learning (artificial intelligence);moisture;radar imaging;regression analysis;remote sensing by radar;soil;synthetic aperture radar;time series","SMC;cycle-consistent adversarial network;CycleGAN;time-series gap filling;low-dimensional representation;simple machine learning model;extracting latent features;S2 feature translation;S2 time series;time-series data;resulting latent representations;soil moisture estimation;time-series gap filing;fast soil moisture content mapping;water resource management;real-time monitoring","","2","","45","CCBY","10 Dec 2021","","","IEEE","IEEE Journals"
"PCB Defect Detection based on Generative Adversarial Network","S. You","State Key Laboratory on Integrated Optoelectronics, Institute of Semiconductors, Chinese Academy of Sciences, Beijing, China","2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)","21 Feb 2022","2022","","","557","560","This paper proposes a PCB defect detection scheme based on the generative confrontation network, which can be applied to the automatic detection system of PCB vision inspection (vision inspection). We use the edge-enhanced super-resolution GAN (EESRGAN) applied in the field of remote sensing to enhance the PCB images and complete the super-resolution detection of the reconstructed picture. And use the PCB pictures of different preprocessing models in an end-to-end manner to compare the recognition of PCB defects after training. Experiments on the PCB data set show that the PCB pictures after sliding cutting are input into the result of EESRGAN training, which can relatively accurately identify the 6 types of defects contained in the data set. Our results show the effectiveness of our data processing methods.","","978-1-6654-0886-8","10.1109/ICCECE54139.2022.9712737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712737","PCB defect detection;EESRGAN;Generative Adversarial Network","Training;Fault diagnosis;Machine vision;Image edge detection;Superresolution;Inspection;Generative adversarial networks","automatic optical inspection;computer vision;electronic engineering computing;fault diagnosis;image enhancement;image reconstruction;image resolution;inspection;neural nets;printed circuits","generative adversarial network;PCB defect detection;generative confrontation network;automatic detection system;PCB vision inspection;edge-enhanced super-resolution GAN;remote sensing;PCB images;super-resolution detection;reconstructed picture;PCB pictures;preprocessing models;PCB data;EESRGAN training","","2","","12","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"Experimental Study on Generative Adversarial Network for Precipitation Nowcasting","C. Luo; X. Li; Y. Ye; S. Feng; M. K. Ng","Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Mathematics, The University of Hong Kong, Hong Kong, China","IEEE Transactions on Geoscience and Remote Sensing","9 Jun 2022","2022","60","","1","20","Precipitation nowcasting is an important task, which can be used in numerous applications. The key challenge of the task lies in radar echo map prediction. Previous studies leverage the convolutional recurrent neural network (ConvRNN) to address the problem. However, the approaches are built upon mean square losses, and the results tend to have inaccurate appearances, shapes, and positions for predictions. To alleviate this problem, we explore the idea of adversarial regularization and systematically compare four types of generative adversarial networks (GANs), which are the combinations of GAN/Wasserstein GAN (WGAN) and its multiscale version. Extensive experiments on a real-world radar dataset and four typical meteorological examples are conducted. The results validate the effectiveness of adversarial regularization. The developed models show superior performances over the existing prediction approaches in the majority of circumstances. Moreover, we find that the WGAN regularization often delivers better results than the GAN regularization due to its robustness, and the multiscale WGAN, in general, performs the best among all the methods. To reproduce the results, we release the source code at https://github.com/luochuyao/MultiScaleGAN and the test system at http://39.97.217.145:80/.","1558-0644","","10.1109/TGRS.2022.3177625","Shenzhen Science and Technology Program(grant numbers:JCYJ20180507183823045,JCYJ20200109113014456,JCYJ20210324120208022); Hong Kong Research Grant Council(grant numbers:GRF 12300218,GRF 12300519,GRF 17201020,GRF 17300021,GRF C1013-21GF,GRF C7004-21GF); Joint NSFC-RGC(grant numbers:N-HKU76921); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780397","Adversarial regularization;deep learning;multiscale;precipitation nowcasting","Radar;Generative adversarial networks;Predictive models;Generators;Laser radar;Deep learning;Spaceborne radar","geophysics computing;meteorological radar;neural nets;recurrent neural nets;statistical analysis;weather forecasting","previous studies leverage;convolutional recurrent neural network;mean square losses;inaccurate appearances;adversarial regularization;generative adversarial network;GANs;real-world radar dataset;existing prediction approaches;WGAN regularization;GAN regularization;multiscale WGAN;radar echo map prediction","","1","","34","IEEE","23 May 2022","","","IEEE","IEEE Journals"
"Generative And Encoded Anomaly Detectors","T. H. Emerson; J. A. Edelberg; T. Doster; N. Merrill; C. C. Olson","Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW, Washington, DC, USA; Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW, Washington, DC, USA; Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW, Washington, DC, USA; Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW, Washington, DC, USA; Applied Optics Branch US Naval Research Laboratory, 4555 Overlook Ave SW, Washington, DC, USA","2019 10th Workshop on Hyperspectral Imaging and Signal Processing: Evolution in Remote Sensing (WHISPERS)","5 Dec 2019","2019","","","1","5","We present two fully unsupervised deep learning approaches for hyperspectral anomaly detection. In one approach we formulate the anomaly detection problem as an adversarial game where a generator network learns the distribution of the hyperspectral background pixels comprising a single hyperspectral image and the output of the corresponding discriminator network yields a detection statistic. The other approach formulates the detection statistic as the error between an input hyperspectral pixel and the reconstruction of that pixel by an autoencoder network trained on the image. Both methods leverage a sub-sampling scheme that allows for unsupervised training and testing on the same data set. Our approaches are validated on a four-class synthetic hyperspectral data set and compared to a statistical approach (RX) and a geometric approach (skelton kernel principal component analysis). The proposed Generative Anomaly Detector algorithm achieves top performance on the data set while the autoencoder detection scheme also demonstrates performance gains relative to the comparison algorithms. Benefits and drawbacks of the approaches are discussed and highlight the many potential directions for future work.","2158-6276","978-1-7281-5294-3","10.1109/WHISPERS.2019.8920850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920850","Anomaly Detection;Generative Adversarial Networks;Autoencoder;Hyperspectral Imaging","Hyperspectral imaging;Detectors;Anomaly detection;Training;Gallium nitride;Machine learning;Image reconstruction","hyperspectral imaging;image classification;object detection;principal component analysis;unsupervised learning","autoencoder detection scheme;fully unsupervised deep learning approaches;hyperspectral anomaly detection;anomaly detection problem;adversarial game;generator network;hyperspectral background pixels;single hyperspectral image;corresponding discriminator network;detection statistic;hyperspectral pixel;autoencoder network;sub-sampling scheme;four-class synthetic hyperspectral data;statistical approach;geometric approach;skelton kernel principal component analysis;generative anomaly detector algorithm","","4","","30","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Detection of Synthesized Satellite Images Using Deep Neural Networks","W. -H. Liao; Y. -S. Chang; Y. -C. Wu","Dept. of Computer Science, National Chengchi University, Taipei, Taiwan; Dept. of Computer Science, National Chengchi University, Taipei, Taiwan; Artificial Intelligence and E-learning Center, National Chengchi University, Taipei, Taiwan","2023 17th International Conference on Ubiquitous Information Management and Communication (IMCOM)","8 Feb 2023","2023","","","1","5","The technology of generative adversarial networks (GAN) is constantly evolving, and synthesized images can no longer be accurately distinguished by the human eyes alone. GAN has been applied to the analysis of satellite images, mostly for the purpose of data augmentation. Recently, however, we have seen a twist in its usage. In information warfare, GAN has been used to create fake satellite images or modify the image content by putting fake bridges, buildings and clouds to mislead or conceal important intelligence. To address the increasing counterfeit cases in satellite images, the goal of this research is to develop algorithms that can classify fake remote sensing images robustly and efficiently. There exist many techniques to synthesize or manipulate the content of satellite images. In this paper, we focus on the case when the entire image is forged. Three satellite image synthesis methods, including ProGAN, cGAN and CycleGAN will be investigated. The effect of image pre-processing such as histogram equalization and bilateral filter will also be evaluated. Experiments show that satellite images generated by different GANs can be easily identified by individually trained models. The performance degraded when model trained with one type of GAN samples is employed to determine the originality of images synthesized with other types of GANs. Additionally, when histogram equalization is applied to the images, the detection model fails to distinguish its authenticity. A four-class universal classification model is proposed to address this issue. An overall accuracy of over 99% has been achieved even when pre-processing has been applied.","","978-1-6654-5348-6","10.1109/IMCOM56909.2023.10035570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035570","Satellite Imagery;Generative Adversarial Network;Image Forgery Detection","Histograms;Satellites;Image synthesis;Neural networks;Interference;Generative adversarial networks;Forgery","","","","","","12","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"Regularization of Building Boundaries in Satellite Images Using Adversarial and Regularized Losses","S. Zorzi; F. Fraundorfer","Institute of Computer Graphics and Vision, Graz University of Technology; Institute of Computer Graphics and Vision, Graz University of Technology","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5140","5143","In this paper we present a method for building boundary refinement and regularization in satellite images using a fully convolutional neural network trained with a combination of adversarial and regularized losses. Compared to a pure Mask R-CNN model, the overall algorithm can achieve equivalent performance in terms of accuracy and completeness. However, unlike Mask R-CNN that produces irregular footprints, our framework generates regularized and visually pleasing building boundaries which are beneficial in many applications.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900337","Generative adversarial networks;building segmentation;boundary refinement;satellite images.","Buildings;Image reconstruction;Image segmentation;Training;Satellites;Erbium;Pipelines","convolutional neural nets;geophysical image processing","satellite images;fully convolutional neural network;regularized losses;mask R-CNN model;building boundaries;adversarial losses;building boundary refinement","","11","","7","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Generating 3d Point Clouds from a Single SAR Image Using 3D Reconstruction Network","L. Peng; X. Qiu; C. Ding; W. Tie","Suzhou Institution, Institute of Electronics; Suzhou Institution, Institute of Electronics; Chinese Academy of Sciences, Institute of Electronics; Suzhou Institution, Institute of Electronics","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","3685","3688","Obtaining the three-dimensional data of the target is very useful for the interpretation and application of the SAR target. This paper proposes a deep learning framework to recover the three-dimensional structure of the target from a single SAR image, which is expressed in the form of 3D point cloud. Due to the small data set of SAR images, the network is combined by two parts. First, the two-dimensional image in the optical perspective is predicted from the SAR target image, and then the 3D points of the target is reconstructed based on the pre-trained 3D reconstruction network model from the optical images. The experiment is based on the MSTAR datasets. The results confirm the effectiveness of the three-dimensional reconstruction method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900449","Deep learning;3D reconstruction;generative adversarial networks;synthetic aperture radar (SAR)","Three-dimensional displays;Radar polarimetry;Optical imaging;Synthetic aperture radar;Deep learning;Solid modeling;Optical fiber networks","image reconstruction;learning (artificial intelligence);radar computing;radar imaging;synthetic aperture radar","three-dimensional reconstruction method;single SAR image;three-dimensional data;deep learning framework;three-dimensional structure;3D point cloud;two-dimensional image;SAR target image;pre-trained 3D reconstruction network model;optical images;MSTAR datasets","","2","","13","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"A Deceptive Jamming Template Synthesis Method for SAR Using Generative Adversarial Nets","W. Fan; F. Zhou; T. Tian","Key Laboratory of Electronic Information Counter Measure and Simulation Technology, Xidian University, Ministry of Education, Xi'an, China; Key Laboratory of Electronic Information Counter Measure and Simulation Technology, Xidian University, Ministry of Education, Xi'an, China; Key Laboratory of Electronic Information Counter Measure and Simulation Technology, Xidian University, Ministry of Education, Xi'an, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6926","6929","In this paper, a deceptive jamming template generative adversarial network (DJTGAN) is proposed, which can intelligently generate high-fidelity deceptive jamming template matched with the practical SAR scenario. The DJTGAN consists of a deceptive jamming template generative network and a discriminative network. The generative network combines low-frequency content and high-frequency details of the target, and the discriminative network adopts PatchGAN architecture to capture local texture statistics to improve the fidelity of the deceptive jamming template. The MSTAR dataset is utilized to verify the effectiveness of the proposed DJTGAN. Moreover, the strip SAR deceptive jamming experiment based on the deceptive jamming templates generated by DJTGAN is done to further validate the effectiveness of the DJTGAN.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323221","European Space Agency; National Natural Science Foundation of China(grant numbers:61471284,61522114,61631019); National Excellent Doctoral Dissertation of PR China(grant numbers:201448); Fundamental Research Funds for the Central Universities(grant numbers:KJXX1601,7214534206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323221","Generative adversarial networks (GANs);SAR image generation;Deceptive jamming;synthetic aperture radar (SAR)","Jamming;Radar polarimetry;Synthetic aperture radar;Feature extraction;Strips;Gallium nitride;Azimuth","image texture;jamming;radar imaging;synthetic aperture radar","DJTGAN;high-fidelity deceptive jamming template;practical SAR scenario;deceptive jamming template generative network;discriminative network;high-frequency details;strip SAR deceptive jamming experiment;deceptive jamming template synthesis method;deceptive jamming template generative adversarial network","","1","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Gated Generative Adversarial Imputation Approach for Signalized Road Networks","T. Zhang; J. Wang; J. Liu","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Intelligent Transportation Systems","10 Aug 2022","2022","23","8","12144","12160","Missing data imputation is an essential component of a robust traffic surveillance system. Despite the progress of data imputation technologies in intelligent transportation systems (ITS), only limited efforts have been devoted to tackling data missing issues on signalized roads, which present distinct missing patterns due to the effect of traffic signal timing on traffic flow. Unlike most studies that try to impute missing data at the granularity of road segment and aggregated time intervals, we impute missing data for individual traffic lanes at the granularity of signal cycle. We develop a data-driven fine-grained imputation approach based on a novel gated attentional generative adversarial network (GaGAN), which is highly responsive to the dynamic traffic environments of signalized road networks. The advantage of the network lies in that it can automatically learn inter-lane spatio-temporal correlations during each signal cycle. To model the spatial correlations, we jointly leverage spatial attention mechanisms and graph convolutional operations to quantify inter-lane influences within each signal cycle. To model the temporal correlations, we propose to integrate self-attention mechanisms into gated recurrent units (termed as SA-GRU). Two sub-discriminators are developed to model lane-level complete and missing data distributions, respectively, with the goal to improve the consistency between imputed data and overall data distribution. Experimental results demonstrate that the proposed approach is superior to other state-of-the-art methods, achieving robust performance over different data missing types.","1558-0016","","10.1109/TITS.2021.3110268","National Key Research and Development Program of China (International Scientific and Technological Cooperation Program)(grant numbers:2019YFE0106500); Natural Science Foundation of China(grant numbers:41871308); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9543471","Attention;generative adversarial network;imputation;signalized road;spatio-temporal correlation","Data models;Roads;Correlation;Generative adversarial networks;Generators;Logic gates;Analytical models","learning (artificial intelligence);road traffic;road vehicles;traffic engineering computing","gated generative adversarial imputation approach;signalized road networks;missing data imputation;robust traffic surveillance system;data imputation technologies;intelligent transportation systems;tackling data;signalized roads;distinct missing patterns;traffic signal;traffic flow;road segment;aggregated time intervals;individual traffic lanes;signal cycle;data-driven fine-grained imputation approach;attentional generative adversarial network;dynamic traffic environments;inter-lane spatio-temporal correlations;leverage spatial attention mechanisms;imputed data;data distribution","","","","60","IEEE","21 Sep 2021","","","IEEE","IEEE Journals"
"Image Translation Between Sar and Optical Imagery with Generative Adversarial Nets","K. Enomoto; K. Sakurada; W. Wang; N. Kawaguchi; M. Matsuoka; R. Nakamura",Nagoya University; National Institute of Advanced Industrial Science and Technology; National Institute of Advanced Industrial Science and Technology; Nagoya University; Tokyo Institute of Technology; National Institute of Advanced Industrial Science and Technology,"IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1752","1755","In this paper, we propose a method for the translation from Synthetic Aperture Radar (SAR) to optical images using conditional Generative Adversarial Networks (cGANs). Satellite images have been widely utilized for various purposes, such as natural environment monitoring (pollution, forest or rivers), transportation improvement and prompt emergency response to disasters. However, the obscurity caused by clouds leads to unstable monitoring of the ground situation while using the optical camera. Images captured by a longer wavelength are introduced to reduce the effects of clouds. In particular, SAR images are known to be nearly unaffected by clouds and are often used for stably observing the ground situation. On the other hand, SAR images have lower spatial resolution and visibility than optical images. Therefore, we propose a deep neural network that generates optical images from SAR images. Finally, we confirm the feasibility of the proposed network on a dataset consisting of optical images and the corresponding SAR images.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518719","Satellite imagery;SAR;Deep learning;GANs","Adaptive optics;Optical imaging;Synthetic aperture radar;Optical polarization;Optical computing;Optical sensors;Image color analysis","image resolution;neural nets;radar imaging;synthetic aperture radar","SAR images;conditional generative adversarial networks;optical camera;satellite images;optical imagery;image translation;optical images","","21","","14","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Residual Learning of Cycle-GAN for Seismic Data Denoising","W. Li; J. Wang","University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Science, Institute of Earth Science, Beijing, China","IEEE Access","20 Jan 2021","2021","9","","11585","11597","Random noise attenuation has always been an indispensable step in the seismic exploration workflow. The quality of the results directly affects the results of subsequent inversion and migration imaging. This paper proposes a cycle-GAN denoising framework based on the data augmentation strategy. We introduced residual learning into the cycle-GAN to improve the training efficiency of the network. We proposed a method for generating labeled datasets directly from unlabeled real noisy data. Then we significantly improve the diversity of the training samples through an augmentation strategy. Through RCGAN, we can realize intelligent seismic data denoising work, which dramatically reduces the manual selection and intervention of denoising parameters. Finally, numerical experiments prove that our method has a remarkably good random noise suppression ability and a minimally damaging effect on useful seismic signals. The experiment tests on synthetic and real data also show the effectiveness and superiority of the proposed method RCGAN compared to the state-of-the-art denoising methods.","2169-3536","","10.1109/ACCESS.2021.3049479","National Natural Science Foundation of China(grant numbers:U20B200166); Major State Research Development Program of China(grant numbers:2016YFC0601101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316158","Geophysical data;geophysics computing;generative adversarial networks;noise reduction;residual learning","Noise reduction;Training;Neural networks;Generators;Generative adversarial networks;Signal to noise ratio;Testing","geophysical signal processing;geophysical techniques;learning (artificial intelligence);neural nets;random noise;seismology;signal denoising","residual learning;seismic data denoising;random noise attenuation;seismic exploration workflow;data augmentation strategy;intelligent seismic data denoising;cycle-GAN denoising framework;RCGAN","","9","","47","CCBY","6 Jan 2021","","","IEEE","IEEE Journals"
"Semisupervised Change Detection Based on Bihierarchical Feature Aggregation and Extraction Network","M. Zhang; T. Gao; M. Gong; S. Zhu; Y. Wu; H. Li","School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","","2023","PP","99","1","15","With the rapid development of remote sensing (RS) technology, high-resolution RS image change detection (CD) has been widely used in many applications. Pixel-based CD techniques are maneuverable and widely used, but vulnerable to noise interference. Object-based CD techniques can effectively utilize the abundant spectrum, texture, shape, and spatial information but easy-to-ignore details of RS images. How to combine the advantages of pixel-based methods and object-based methods remains a challenging problem. Besides, although supervised methods have the capability to learn from data, the true labels representing changed information of RS images are often hard to obtain. To address these issues, this article proposes a novel semisupervised CD framework for high-resolution RS images, which employs small amounts of true labeled data and a lot of unlabeled data to train the CD network. A bihierarchical feature aggregation and extraction network (BFAEN) is designed to achieve the pixelwise together with objectwise feature concatenation feature representation for the comprehensive utilization of the two-level features. In order to alleviate the coarseness and insufficiency of labeled samples, a confident learning algorithm is used to eliminate noisy labels and a novel loss function is designed for training the model using true-and pseudo-labels in a semisupervised fashion. Experimental results on real datasets demonstrate the effectiveness and superiority of the proposed method.","2162-2388","","10.1109/TNNLS.2023.3242075","National Natural Science Foundation of China(grant numbers:62036006,61906147); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10043634","Change detection (CD);confident learning (CL);convolutional neural network (CNN);feature concatenation;remote sensing (RS);semisupervised learning","Feature extraction;Convolutional neural networks;Training;Image segmentation;Remote sensing;Object recognition;Generative adversarial networks","","","","","","","IEEE","13 Feb 2023","","","IEEE","IEEE Early Access Articles"
"JMnet: Joint Metric Neural Network for Hyperspectral Unmixing","A. Min; Z. Guo; H. Li; J. Peng","School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China; School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China; School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China; Hubei Key Laboratory of Applied Mathematics, Faculty of Mathematics and Statistics, Hubei University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","15 Dec 2021","2022","60","","1","12","Hyperspectral unmixing is a significant task in remote sensing image analysis. Existing learning-based methods for hyperspectral unmixing generally are in the form of an autoencoder and take geometric distances, such as spectral angle distance (SAD) as loss functions. These methods ignored the distribution similarity between the observation and the reconstruction, which might help improve the unmixing performance. Besides, the autoencoder is trained by directly comparing the difference between the observation and the reconstruction, and the difference between their features has been neglected. Based on the above considerations, we propose a joint metric neural network for hyperspectral unmixing, by introducing the Wasserstein distance and feature matching as regularization terms and SAD as the underlying loss. The proposed neural network consists of two parts: an autoencoder is used for endmember extraction and abundance estimation, while a discriminator is used to compute the Wasserstein distance. The Wasserstein distance can stably provide useful gradient information that promotes the autoencoder to reach a solution with better unmixing performance. The feature matching is adapted to an intermediate layer of the discriminator for enforcing the features of the observation and the reconstruction to be equal, which can lead to further improvement of the unmixing performance. The model analysis and the regularization parameter analysis are conducted to demonstrate the effectiveness of our method. Experimental results on four real-world hyperspectral data sets show that our method outperforms the state-of-the-art methods, especially in terms of abundance estimation.","1558-0644","","10.1109/TGRS.2021.3069476","National Natural Science Foundation of China(grant numbers:61877021,11771130,61871177); National Key Research and Development Program of China(grant numbers:2020YFA0714200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404359","Feature matching;hyperspectral unmixing;neural network;Wasserstein distance","Hyperspectral imaging;Neural networks;Measurement;Generative adversarial networks;Estimation;Task analysis;Analytical models","","","","7","","41","IEEE","14 Apr 2021","","","IEEE","IEEE Journals"
"Spectral–Spatial Generative Adversarial Network for Super-Resolution Land Cover Mapping With Multispectral Remotely Sensed Imagery","C. Shang; S. Jiang; F. Ling; X. Li; Y. Zhou; Y. Du","School of Geosciences, Yangtze University, Wuhan, China; School of Geosciences, Yangtze University, Wuhan, China; Key laboratory of Monitoring and Estimate for Environment and Disaster of Hubei province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key laboratory of Monitoring and Estimate for Environment and Disaster of Hubei province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key laboratory of Monitoring and Estimate for Environment and Disaster of Hubei province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key laboratory of Monitoring and Estimate for Environment and Disaster of Hubei province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Dec 2022","2023","16","","522","537","Super-resolution mapping (SRM) can effectively predict the spatial distribution of land cover classes within mixed pixels at a higher spatial resolution than the original remotely sensed imagery. The uncertainty of land cover fraction errors within mixed pixels is one of the most important factors affecting SRM accuracy. Studies have shown that SRM methods using deep learning techniques have significantly improved land cover mapping accuracy but have not coped well with spectral–spatial errors. This study proposes an end-to-end SRM model using a spectral–spatial generative adversarial network (SGS) with the direct input of multispectral remotely sensed imagery, which deals with spectral–spatial error. The proposed SGS comprises the following three parts: first, cube-based convolution for spectral unmixing is adopted to generate land cover fraction images. Second, a residual-in-residual dense block fully and jointly considers spectral and spatial information and reduces spectral errors. Third, a relativistic average GAN is designed as a backbone to further improve the super-resolution performance and reduce spectral–spatial errors. SGS was tested in one synthetic and two realistic experiments with multi/hyperspectral remotely sensed imagery as the input, comparing the results with those of hard classification and several classic SRM methods. The results showed that SGS performed well at reducing land cover fraction errors, reconstructing spatial details, removing unpleasant and unrealistic land cover artifacts, and eliminating false recognition.","2151-1535","","10.1109/JSTARS.2022.3228741","Natural Science Foundation of Hubei Province(grant numbers:2022CFB689); National Natural Science Foundation of China(grant numbers:U22A20567); National Natural Science Foundation of China(grant numbers:62071457); Key Scientific Research Projects of Water Conservancy in Hubei Province, China(grant numbers:HBSLKY202103); Application Foundation Frontier Project of Wuhan(grant numbers:2020020601012283); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982425","Deep learning (DL);generative adversarial network (GAN);land cover fractions;spectral–spatial errors;super-resolution mapping (SRM)","Generative adversarial networks;Superresolution;Spatial resolution;Layout;Remote sensing;Graphical models;Distribution functions","geophysical image processing;image classification;image reconstruction;image resolution;land cover;terrain mapping","classic SRM methods;end-to-end SRM model;higher spatial resolution;land cover classes;land cover fraction errors;land cover fraction images;land cover mapping accuracy;mixed pixels;multispectral remotely sensed imagery;original remotely sensed imagery;SGS;spatial distribution;spatial information;spectral errors;spectral information;spectral unmixing;spectral-spatial error;spectral-spatial generative adversarial network;SRM accuracy;super-resolution land cover mapping;super-resolution mapping;super-resolution performance;unpleasant land cover artifacts;unrealistic land cover artifacts","","","","61","CCBY","12 Dec 2022","","","IEEE","IEEE Journals"
"Multiscale Generative Adversarial Network Based on Wavelet Feature Learning for SAR-to-Optical Image Translation","H. Li; C. Gu; D. Wu; G. Cheng; L. Guo; H. Liu","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Cybersecurity, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","25 Oct 2022","2022","60","","1","15","The synthetic aperture radar (SAR) system is a kind of active remote sensing, which can be carried on a variety of flight platforms and can observe the Earth under all-day and all-weather conditions, so it has a wide range of applications. However, the interpretation of SAR images is quite challenging and not suitable for nonexperts. In order to enhance the visual effect of SAR images, this article proposes a multiscale generative adversarial network based on wavelet feature learning (WFLM-GAN) to implement the translation from SAR images to optical images; the translated images not only retain the key content of SAR images but also have the style of optical images. The main advantages of this method over the previous SAR-to-optical image translation (S2OIT) methods are given as follows. First, the generator does not learn the mapping from SAR images to optical images directly but learns the mapping from SAR images to wavelet features and then reconstructs the gray-scale images to optimize the content, increasing the mapping relationships and helping to learn more effective features. Second, a multiscale coloring network based on detail learning and style learning is designed to further translate the gray-scale images into optical images, which makes the generated images have an excellent visual effect with details closer to real images. Extensive experiments on SAR image datasets in different regions and seasons demonstrate the superior performance of WFLM-GAN over the baseline algorithms in terms of structural similarity (SSIM), the peak signal-to-noise ratio (PSNR), the Frechet inception distance (FID), and the kernel inception distance (KID). Comprehensive ablation studies are also carried out to isolate the validity of each proposed component. Our codes will be available at https://github.com/G2022G/WFLM-GAN.","1558-0644","","10.1109/TGRS.2022.3211415","Key Project of the National Natural Science Foundation of China(grant numbers:61936007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9912365","Generative adversarial network (GAN);multiscale;synthetic aperture radar (SAR)-to-optical image translation (S2OIT);wavelet feature","Radar polarimetry;Optical imaging;Task analysis;Optical sensors;Generators;Visual effects;Generative adversarial networks","image colour analysis;radar imaging;synthetic aperture radar","optical images;SAR-to-optical image translation;SAR images;gray-scale images;SAR image datasets;multiscale generative adversarial network;wavelet feature learning;synthetic aperture radar system;translated images","","","","50","IEEE","5 Oct 2022","","","IEEE","IEEE Journals"
"Study on Weather Radar Echo Data Generation Based on DCGAN","H. Wang; M. Gao; S. Hu; Z. Sun; Z. Xu","College of Electronic Engineering, Chengdu University of Information Technology, Chengdu, China; College of Electronic Engineering, Chengdu University of Information Technology, Chengdu, China; College of Electronic Engineering, Chengdu University of Information Technology, Chengdu, China; Beijing Metstar Radar Company Ltd., Beijing, China; Second Research Institute of CAAC, Chengdu, China","IEEE Access","23 Sep 2019","2019","7","","131978","131985","Doppler weather radar can detect the changes in precipitation clouds for short-term forecasting. In the process of development of Doppler weather radar and weather identification algorithms, some typical Doppler weather radar base data corresponding to different weather phenomena are necessary for signal processing unit test and algorithm verification. However, the existing real weather radar base data with high quality can't meet the requirement in amount. In this paper, an algorithm based on Deep Convolutional Generative Adversarial Networks (DCGAN) to generate typical weather radar base data is proposed. And in the test signal simulation step, the power spectrum algorithm is improved. The results show that the data produced by the DCGAN have the same characteristics with the real weather radar base data without obvious non-meteorological noise. Moreover, the improved power spectrum algorithm performs better in terms of accuracy rate of the simulation echo signal.","2169-3536","","10.1109/ACCESS.2019.2940561","National Natural Science Foundation of China(grant numbers:U1733103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8830418","Doppler weather radar base data simulation;baseband weather radar echo signal;DCGAN;power spectrum simulation","Meteorology;Meteorological radar;Doppler radar;Data models;Signal processing algorithms;Generative adversarial networks","atmospheric techniques;Doppler radar;echo;meteorological radar;radar computing","improved power spectrum algorithm;typical weather radar base data;Deep Convolutional Generative Adversarial Networks;different weather phenomena;typical Doppler weather radar base data;weather identification algorithms;DCGAN;weather radar echo data generation","","3","","22","CCBY","10 Sep 2019","","","IEEE","IEEE Journals"
"Cross-Domain Association Mining Based Generative Adversarial Network for Pansharpening","L. He; W. Zhang; J. Shi; F. Li","Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China; Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China; School of Automation Science and Engineering, Xi'an Jiaotong University, Xi'an, China; Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Sep 2022","2022","15","","7770","7783","Multispectral (MS) pansharpening can improve the spatial resolution of MS images, which plays an increasingly important role in agriculture and environmental monitoring. Existing neural network-based methods tend to focus on global features of images, without considering the inherent relationships between similar substances in MS images. However, there is a high probability that different substances at the junction mix with each other, which leads to spectral distortion in the final pansharpened image. In this article, we propose a cross-domain association mining-based generative adversarial network for pansharpening, which consists of a spectral fidelity generator and dual discriminators. In our spectral fidelity generator, the cross-region similarity attention module is designed to establish dependencies between similar substances at different positions in the image, thereby leveraging the similar spectral features to generate pansharpened images with better spectral preservation. To mine the potential relationship between the MS image domain and the panchromatic image domain, we pretrain a spatial information extraction network. The network is then transferred to the dual-discriminator architecture to obtain the spatial information of the pansharpened images more accurately and prevent the loss of spatial details. The experimental results show that our method outperforms several state-of-the-art pansharpening methods in both quantitative and qualitative evaluations.","2151-1535","","10.1109/JSTARS.2022.3204824","National Natural Science Foundation of China(grant numbers:U1903213); Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology(grant numbers:2021SYS-04); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0966); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880485","Deep learning;dual discriminators;image association;multispectral (MS) pansharpening","Pansharpening;Generators;Feature extraction;Spatial resolution;Junctions;Generative adversarial networks;Superresolution","data mining;geophysical image processing;geophysical signal processing;image fusion;image resolution;neural nets;remote sensing","spectral fidelity generator;cross-region similarity attention module;similar substances;similar spectral features;pansharpened images;spectral preservation;MS image domain;panchromatic image domain;spatial information extraction network;state-of-the-art pansharpening methods;multispectral pansharpening;spatial resolution;MS images;neural network-based methods;inherent relationships;different substances;spectral distortion;final pansharpened image;cross-domain association mining-based generative adversarial network","","","","57","CCBY","7 Sep 2022","","","IEEE","IEEE Journals"
"HSI-IPNet: Hyperspectral Imagery Inpainting by Deep Learning With Adaptive Spectral Extraction","R. Wong; Z. Zhang; Y. Wang; F. Chen; D. Zeng","Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute of Advanced Communication and Data Science, Shanghai University, Shanghai, China; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute of Advanced Communication and Data Science, Shanghai University, Shanghai, China; Key Laboratory of Infrared Detecting and Imaging Technology, Chinese Academy of Sciences, Shanghai, China; Key Laboratory of Intelligent Infrared Perception, Chinese Academy of Sciences, Shanghai, China; Key Laboratory of Specialty Fiber Optics and Optical Access Networks, Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication, Shanghai Institute of Advanced Communication and Data Science, Shanghai University, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Aug 2020","2020","13","","4369","4380","Feature representation is the key to the hyperspectral images (HSI) inpainting. Existing works mainly focus on using spectral and temporal auxiliary images to inpainting the corrupted region, which were proved to be low robust for all bands missing and high requirements for image acquisition. In this work, we propose an end-to-end inpainting framework for HSI based on convolutional neural networks, which does not require auxiliary images and makes full use of both spectral characteristics and spatial information. For spectral characteristics, a channel attention mechanism is proposed to reduce the redundancy of hyperspectral channels and model the correlation between channels. For spatial information, a local discriminative network is able to cope with the structural continuity of the corrupted regions, and a gradient consistency loss function is proposed to maintain the texture consistency of HSIs. Experimental results in the Airborne Visual Infrared Imaging Spectrometer Indians Pines public dataset and Feicheng Hyperspectral datasets show that our proposed method can provide competitive results compared with state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2020.3012443","National Natural Science Foundation of China(grant numbers:61572307); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151178","Channel attention mechanism (CAM);global and local discriminators;gradient consistency (GC);hyperspectral remote sensing;image inpainting","Hyperspectral imaging;Generative adversarial networks;Gallium nitride;Training;Image reconstruction","feature extraction;hyperspectral imaging;image representation;image texture;learning (artificial intelligence);neural nets","hyperspectral imagery inpainting;deep learning;adaptive spectral extraction;feature representation;temporal auxiliary images;image acquisition;convolutional neural networks;spatial information;channel attention mechanism;hyperspectral channels;HSI-IPNet;airborne visual infrared imaging spectrometer Indians pines;Feicheng hyperspectral datasets","","9","","61","CCBY","28 Jul 2020","","","IEEE","IEEE Journals"
"GAN-Based LUCC Prediction via the Combination of Prior City Planning Information and Land-Use Probability","S. Sun; L. Mu; R. Feng; L. Wang; J. He","College of Marine Science and Technology, China University of Geosciences, Wuhan, China; Southern Marine Science and Engineering Guangdong Laboratory (Guangzhou), Guangzhou, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; College of Resource Environment and Tourism, Capital Normal University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Oct 2021","2021","14","","10189","10198","Currently, the world is in a period of urbanization that will accelerate the processes of land-use cover and ecological change. Thus, establishing a land-use and land-cover change (LUCC) prediction and simulation model is of great significance for understanding the process of urban change and assessing its ecological impact. In previous studies, LUCC prediction models have been mainly based on cellular automata structures that calculate a future state pixel by pixel through transition rules. Because these transition rules are usually based on the global state and each pixel is calculated according to these fixed rules, the results of these methods have room for improvement in terms of generating details and heterogeneity. In this article, a generative adversarial network (GAN)-based LUCC prediction model using multiscale local spatial information is proposed. The model is based on a pix2pix GAN and an attention structure that predicts future land use through multiscale local spatial information. To validate our model, Shenzhen, a region that is experiencing rapid urbanization, was chosen as the source of the experimental data. The results indicate that the proposed method achieved the highest accuracy in both short-time interval and long-time interval scenarios. In addition, the results of the proposed method were also closest to the ground truth from the perspective of the landscape pattern.","2151-1535","","10.1109/JSTARS.2021.3106481","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B1111020005); National Natural Science Foundation of China(grant numbers:U2006210); Shenzhen Fundamental Research Program(grant numbers:JCYJ20200109110220482); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520268","Deep learning;generative adversarial network (GAN);LUCC simulation;remote sensing;smart city","Predictive models;Data models;Generative adversarial networks;Biological system modeling;Generators;Deep learning;Urban areas","cellular automata;ecology;geographic information systems;land use planning;neural nets;probability","land-cover change prediction;state pixel;pix2pix GAN;multiscale local spatial information;generative adversarial network-based LUCC prediction model;transition rules;cellular automata structures;ecological impact;urban change;simulation model;ecological change;land-use cover;land-use probability;prior city planning information;GAN-based LUCC prediction","","3","","49","CCBY","20 Aug 2021","","","IEEE","IEEE Journals"
"Localization in Aerial Imagery with Grid Maps using LocGAN","H. Hu; J. Zhu; S. Wirges; M. Lauer","Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany","2019 IEEE Intelligent Transportation Systems Conference (ITSC)","28 Nov 2019","2019","","","2860","2865","In this work, we present LocGAN, our localization approach based on a geo-referenced aerial imagery and LiDAR grid maps. Currently, most self-localization approaches relate the current sensor observations to a map generated from previously acquired data. Unfortunately, this data is not always available and the generated maps are usually sensor setup specific. Global Navigation Satellite Systems (GNSS) can overcome this problem. However, they are not always reliable especially in urban areas due to multi-path and shadowing effects. Since aerial imagery is usually available, we can use it as prior information. To match aerial images with grid maps, we use conditional Generative Adversarial Networks (cGANs) which transform aerial images to the grid map domain. The transformation between the predicted and measured grid map is estimated using a localization network (LocNet). Given the geo-referenced aerial image transformation the vehicle pose can be estimated. Evaluations performed on the data recorded in region Karlsruhe, Germany show that our LocGAN approach provides reliable global localization results.","","978-1-5386-7024-8","10.1109/ITSC.2019.8917236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917236","","Generators;Global navigation satellite system;Principal component analysis;Training;Laser radar;Reliability;Transforms","distance measurement;geophysical image processing;image fusion;mobile robots;optical radar;remote sensing;remotely operated vehicles","self-localization approaches;current sensor observations;generated maps;Global Navigation Satellite Systems;shadowing effects;grid map domain;conditional generative adversarial networks;predicted measured grid map;localization network;geo-referenced aerial image transformation;LocGAN approach;reliable global localization results;localization approach;geo-referenced aerial imagery;LiDAR grid maps","","1","","22","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Physics-Based GAN With Iterative Refinement Unit for Hyperspectral and Multispectral Image Fusion","J. Xiao; J. Li; Q. Yuan; M. Jiang; L. Zhang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Jul 2021","2021","14","","6827","6841","Hyperspectral image (HSI) fusion can effectively improve the spatial resolution of HSIs by integrating high-resolution multispectral images (MSIs). Considering the spatial and spectral degradation relationship between a fused image and input images, a physics-based GAN is proposed to fuse HSI and MSI. A physical model estimating degradation of image is introduced in the generator and in the discriminators. For the generator, a set of recursive modules including a physical degradation model and a multiscale residual channel attention fusion module integrate the spectral-spatial difference information between input images and estimated degradation images to restore the details of the fused image. Subsequently, the residual spatial attention fusion module is used to combine the results of all recursions to obtain the final reconstructed result. As for the discriminators, three networks with the final fused image, estimated LR HSI and estimated MSI as inputs share the same architecture. Finally, the loss function that contains adversarial losses and L1 losses of the fused image and estimated degradation images is used to optimize network parameters. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2021.3075727","National Natural Science Foundation of China(grant numbers:62071341,41922008,61971319); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435191","Attention module;GAN;hyperspectral image (HSI);physics model","Degradation;Mathematical model;Generators;Feature extraction;Physics;Training;Generative adversarial networks","geophysical image processing;hyperspectral imaging;image fusion;image restoration;iterative methods;neural nets","spatial degradation relationship;spectral degradation relationship;physics-based GAN;physical degradation model;multiscale residual channel attention fusion module;spectral-spatial difference information;residual spatial attention fusion module;iterative refinement unit;hyperspectral image fusion;high-resolution multispectral imaging;LR HSI estimation;degradation image estimation","","6","","64","CCBY","19 May 2021","","","IEEE","IEEE Journals"
"Nonlinear Multi-scale Super-resolution Using Deep Learning","K. Tran; A. Panahi; A. Adiga; W. Sakla; H. Krim","Department of Computer Science, North Carolina State University, Raleigh, NC; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC; Computational Engineering Division, Lawrence Livermore National Laboratory, Livermore, CA; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","3182","3186","We propose a deep learning architecture capable of performing up to 8× single image super-resolution. Our architecture incorporates an adversarial component from the super-resolution generative adversarial networks (SRGANs) and a multi-scale learning component from the multiple scale super-resolution network (MSSRNet), which only together can recover smaller structures inherent in satellite images. To further enhance our performance, we integrate progressive growing and training to our network. This, aided by feed forwarding connections in the network to move along and enrich information from previous inputs, produces super-resolved images at scaling factors of 2, 4, and 8. To ensure and enhance the stability of GANs, we employ Wasserstein GANs (WGANs) during training. Experimentally, we find that our architecture can recover small objects in satellite images during super-resolution whereas previous methods cannot.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682354","super-resolution;remote sensing data;GANs;dilated convolutions","Spatial resolution;Satellites;Training;Signal resolution;Convolution;Gallium nitride","geophysical image processing;image resolution;learning (artificial intelligence);neural nets","deep learning architecture;multiscale learning component;satellite images;single image superresolution;nonlinear multiscale superresolution;superresolution generative adversarial networks;multiple scale superresolution network;Wasserstein GANs","","2","","25","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"Unsupervised Feature Extraction in Hyperspectral Images Based on Wasserstein Generative Adversarial Network","M. Zhang; M. Gong; Y. Mao; J. Li; Y. Wu","School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; School of Electronic Engineering, Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Xi’an Key Laboratory of Big Data and Intelligent Vision, School of Computer Science and Technology, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","23 Apr 2019","2019","57","5","2669","2688","Feature extraction (FE) is a crucial research area in hyperspectral image (HSI) processing. Recently, due to the powerful ability of deep learning (DL) to extract spatial and spectral features, DL-based FE methods have shown great potentials for HSI processing. However, most of the DL-based FE methods are supervised, and the training of them suffers from the absence of labeled samples in HSIs severely. The training issue of supervised DL-based FE methods limits their application on HSI processing. To address this issue, in this paper, a novel modified generative adversarial network (GAN) is proposed to train a DL-based feature extractor without supervision. The designed GAN consists of two components, which are a generator and a discriminator. The generator can focus on the learning of real probability distributions of data sets and the discriminator can extract spatial-spectral features with superior invariance effectively. In order to learn upsampling and downsampling strategies adaptively during FE, the proposed generator and discriminator are designed based on a fully deconvolutional subnetwork and a fully convolutional subnetwork, respectively. Moreover, a novel min-max cost function is designed for training the proposed GAN in an end-to-end fashion without supervision, by utilizing the zero-sum game relationship between the generator and discriminator. Besides, the proposed modified GAN replaces the original Jensen-Shannon divergence with the Wasserstein distance, aiming to mitigate the unstability and difficulty of the training of GAN frameworks. Experimental results on three real data sets validate the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2018.2876123","National Natural Science Foundation of China(grant numbers:61772393); National Key Research and Development Program of China(grant numbers:2017YFB0802200); Key Research and Development Program of Shaanxi Province(grant numbers:2018ZDXM-GY-045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527649","Convolutional neural network (CNN);feature extraction (FE);generative adversarial network (GAN);hyperspectral images (HSIs)","Feature extraction;Iron;Training;Gallium nitride;Generative adversarial networks;Generators;Probability distribution","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;minimax techniques;probability;supervised learning;unsupervised learning","HSI processing;fully deconvolutional subnetwork;fully convolutional subnetwork;GAN frameworks;unsupervised feature extraction;Wasserstein generative adversarial network;hyperspectral image processing;deep learning;supervised DL-based FE methods;probability distributions;min-max cost function;Jensen-Shannon divergence","","80","","91","IEEE","8 Nov 2018","","","IEEE","IEEE Journals"
"Caps-TripleGAN: GAN-Assisted CapsNet for Hyperspectral Image Classification","X. Wang; K. Tan; Q. Du; Y. Chen; P. Du","Key Laboratory for Land Environment and Disaster Monitoring of NASG, China University of Mining and Technology, Xuzhou, China; Key Laboratory of Geographic Information Science, Ministry of Education, East China Normal University, Shanghai, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Key Laboratory for Land Environment and Disaster Monitoring of NASG, China University of Mining and Technology, Xuzhou, China; Key Laboratory for Satellite Mapping Technology and Applications of NASG, Nanjing University, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","7232","7245","The increase in the spectral and spatial information of hyperspectral imagery poses challenges in classification due to the fact that spectral bands are highly correlated, training samples may be limited, and high resolution may increase intraclass difference and interclass similarity. In this paper, in order to better handle these problems, a Caps-TripleGAN framework is proposed by exploring the 1-D structure triple generative adversarial network (TripleGAN) for sample generation and integrating CapsNet for hyperspectral image classification. Moreover, spatial information is utilized to verify the learning capacity and discriminative ability of the Caps-TripleGAN framework. The experimental results obtained with three real hyperspectral data sets confirm that the proposed method outperforms most of the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2019.2912468","National Natural Science Foundation of China(grant numbers:41871337,41471356); Xuzhou Scientific Funds(grant numbers:KC16SS092); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710617","CapsNet;hyperspectral image classification;triple generative adversarial network (TripleGAN)","Hyperspectral imaging;Feature extraction;Gallium nitride;Generative adversarial networks;Hidden Markov models","geophysical image processing;hyperspectral imaging;image classification;neural nets","hyperspectral data sets;GAN-assisted CapsNet;hyperspectral image classification;1-D structure triple generative adversarial network;Caps-TripleGAN","","77","","51","IEEE","9 May 2019","","","IEEE","IEEE Journals"
"Discriminative Reconstruction Constrained Generative Adversarial Network for Hyperspectral Anomaly Detection","T. Jiang; Y. Li; W. Xie; Q. Du","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, USA","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2020","2020","58","7","4666","4679","The rich and distinguishable spectral information in hyperspectral images (HSIs) makes it possible to capture anomalous samples [i.e., anomaly detection (AD)] that deviate from background samples. However, hyperspectral anomaly detection (HAD) faces various challenges due to high dimensionality, redundant information, and unlabeled and limited samples. To address these problems, this article proposes an unsupervised discriminative reconstruction constrained generative adversarial network for HAD (HADGAN). Our solution is mainly based on the assumption that the number of normal samples is much larger than the number of abnormal ones. The key contribution of this article is to learn a discriminative background reconstruction with anomaly targets being suppressed, which produces the initial detection image (i.e., the residual image between the original image and reconstructed image) with anomaly targets being highlighted and background samples being suppressed. To accomplish this goal, first, by using an autoencoder (AE) network and an adversarial latent discriminator, the latent feature layer learns normal background distribution and AE learns a background reconstruction as much as possible. Second, consistency enhanced representation and shrink constraints are added to the latent feature layer to ensure that anomaly samples are projected to similar positions as normal samples in the latent feature layer. Third, using an adversarial image feature corrector in the input space can guarantee the reliability of the generated samples. Finally, an energy-based spatial and distance-based spectral joint anomaly detector is applied in the residual map to generate the final detection map. Experiments conducted on several data sets over different scenes demonstrate its state-of-the-art performance.","1558-0644","","10.1109/TGRS.2020.2965961","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Young Talent Fund of the University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972475","Anomaly detection (AD);autoencoder (AE);generative adversarial network (GAN);hyperspectral image (HSI);latent constraint;spatial–spectral detector","Image reconstruction;Hyperspectral imaging;Feature extraction;Detectors;Generative adversarial networks;Anomaly detection","feature extraction;geophysical image processing;hyperspectral imaging;image reconstruction;neural nets;object detection;unsupervised learning","discriminative reconstruction constrained generative adversarial network;hyperspectral anomaly detection;hyperspectral images;unsupervised discriminative reconstruction;discriminative background reconstruction;residual image;image reconstruction;autoencoder network;adversarial latent discriminator;latent feature layer;normal background distribution;adversarial image feature corrector;distance-based spectral joint anomaly detector;AE;energy-based spatial anomaly detector;HADGAN","","71","","58","IEEE","28 Jan 2020","","","IEEE","IEEE Journals"
"Data-Driven Seismic Waveform Inversion: A Study on the Robustness and Generalization","Z. Zhang; Y. Lin","Earth and Environmental Sciences, Los Alamos National Laboratory, Los Alamos, USA; Earth and Environmental Sciences, Los Alamos National Laboratory, Los Alamos, USA","IEEE Transactions on Geoscience and Remote Sensing","24 Sep 2020","2020","58","10","6900","6913","Full-waveform inversion is an important and widely used method to reconstruct subsurface velocity images. Waveform inversion is a typical nonlinear and ill-posed inverse problem. Existing physics-driven computational methods for solving waveform inversion suffer from the cycle-skipping and local-minima issues, and do not mention that solving waveform inversion is computationally expensive. In recent years, data-driven methods become a promising way to solve the waveform-inversion problem. However, most deep-learning frameworks suffer from the generalization and overfitting issue. In this article, we developed a real-time data-driven technique and we call it VelocityGAN, to reconstruct accurately the subsurface velocities. Our VelocityGAN is built on a generative adversarial network (GAN) and trained end to end to learn a mapping function from the raw seismic waveform data to the velocity image. Different from other encoder-decoder-based data-driven seismic waveform-inversion approaches, our VelocityGAN learns regularization from data and further imposes the regularization to the generator so that inversion accuracy is improved. We further develop a transfer-learning strategy based on VelocityGAN to alleviate the generalization issue. A series of experiments is conducted on the synthetic seismic reflection data to evaluate the effectiveness, efficiency, and generalization of VelocityGAN. We not only compare it with the existing physics-driven approaches and data-driven frameworks but also conduct several transfer-learning experiments. The experimental results show that VelocityGAN achieves the state-of-the-art performance among the baselines and can improve the generalization results to some extent.","1558-0644","","10.1109/TGRS.2020.2977635","Center for Space and Earth Science at Los Alamos National Laboratory (LANL) and by the Laboratory Directed Research and Development program of LANL(grant numbers:20200061DR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9044635","Condition adversarial networks;data-driven method;full-waveform inversion (FWI);transfer learning","Inverse problems;Computational modeling;Mathematical model;Generative adversarial networks;Neural networks;Gallium nitride;Generators","decoding;encoding;geophysical signal processing;geophysical techniques;inverse problems;learning (artificial intelligence);seismic waves;seismology","existing physics-driven approaches;synthetic seismic reflection data;encoder-decoder-based data-driven seismic waveform-inversion approaches;raw seismic waveform data;VelocityGAN;real-time data-driven technique;overfitting issue;waveform-inversion problem;data-driven methods;existing physics-driven computational methods;inverse problem;subsurface velocity images;important used method;full-waveform inversion;data-driven seismic waveform inversion","","48","","58","CCBY","23 Mar 2020","","","IEEE","IEEE Journals"
"A Novel Pattern for Infrared Small Target Detection With Generative Adversarial Network","B. Zhao; C. Wang; Q. Fu; Z. Han","Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang, China","IEEE Transactions on Geoscience and Remote Sensing","21 Apr 2021","2021","59","5","4481","4492","Since existing detectors are often sensitive to the complex background, a novel detection pattern based on generative adversarial network (GAN) is proposed to focus on the essential features of infrared small target in this article. Motivated by the fact that the infrared small targets have their unique distribution characteristics, we construct a GAN model to automatically learn the features of targets and directly predict the intensity of targets. The target is recognized and reconstructed by the generator, built upon U-Net, according the data distribution. A five-layer discriminator is constructed to enhance the data-fitting ability of generator. Besides, the L2 loss is added into adversarial loss to improve the localization. In general, the detection problem is formulated as an image-to-image translation problem implemented by GAN, namely the original image is translated to a detected image with only target remained. By this way, we can achieve reasonable results with no need of specific mapping function or hand-engineering features. Extensive experiments demonstrate the outstanding performance of proposed method on various backgrounds and targets. In particular, the proposed method significantly improve intersection over union (IoU) values of the detection results than state-of-the-art methods.","1558-0644","","10.1109/TGRS.2020.3012981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165022","Generative adversarial network (GAN);image-to-image translation;infrared image;small target detection","Gallium nitride;Generators;Object detection;Feature extraction;Generative adversarial networks;Image reconstruction;Task analysis","feature extraction;geophysical image processing;infrared imaging;learning (artificial intelligence);neural nets;object detection","data-fitting ability;adversarial loss;image-to-image translation problem;hand-engineering features;generative adversarial network;complex background;detection pattern;infrared small target detection;unique distribution characteristics;GAN model;data distribution;U-Net;L2 loss;specific mapping function;intersection over union values","","48","","38","IEEE","11 Aug 2020","","","IEEE","IEEE Journals"
"Semisupervised Spectral Learning With Generative Adversarial Network for Hyperspectral Anomaly Detection","K. Jiang; W. Xie; Y. Li; J. Lei; G. He; Q. Du","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, USA; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, USA","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2020","2020","58","7","5224","5236","Limited by the anomalous spectral vectors in unlabeled hyperspectral images (HSIs), anomaly detection methods based on background distribution estimation often suffer from the contamination of anomalies, which decreases the estimation accuracy and, thus, weakens the detection performance. To address this problem, we proposed a novel semisupervised spectral learning (SSL) for the hyperspectral anomaly detection framework based on the generative adversarial network (GAN). GAN is applied and developed to estimate the background distribution in a semisupervised manner and obtain an initial spectral feature because of its strong representational capability and adversarial training advantage. In the proposed framework, an initial spatial feature is generated via morphological attribute filtering. Finally, an exponential constrained nonlinear suppression fusion technique is adopted to suppress the background and combine the complementary information in different features to obtain a fused detection map. The performance of the proposed anomaly detection technique is evaluated on a series of HSIs. Experimental results demonstrate that our method can outperform state-of-the-art anomaly detection methods.","1558-0644","","10.1109/TGRS.2020.2975295","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Young Talent Fund of the University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; Joint Fund Project of the National Natural Science Foundation of China(grant numbers:U1704130); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9013035","Anomaly detection;background distribution estimation;generative adversarial network (GAN);hyperspectral image (HSI);semisupervised learning","Anomaly detection;Hyperspectral imaging;Gallium nitride;Training;Feature extraction;Generative adversarial networks","geophysical image processing;hyperspectral imaging;neural nets;object detection;supervised learning","background distribution estimation;hyperspectral anomaly detection framework;generative adversarial network;GAN;exponential constrained nonlinear suppression fusion technique;fused detection map;HSIs;anomalous spectral vectors;unlabeled hyperspectral images;semisupervised spectral learning;morphological attribute filtering","","34","","51","IEEE","26 Feb 2020","","","IEEE","IEEE Journals"
"Generative Adversarial Minority Oversampling for Spectral–Spatial Hyperspectral Image Classification","S. K. Roy; J. M. Haut; M. E. Paoletti; S. R. Dubey; A. Plaza","Computer Science and Engineering Department, Jalpaiguri Government Engineering College, Jalpaiguri, India; Department of Communication and Control Systems, National Distance Education University, Madrid, Spain; Department of Computer Architecture, School of Computer Science and Engineering, University of Málaga, Málaga, Spain; Computer Vision Group, Indian Institute of Information Technology, Sri City, India; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","15","Recently, convolutional neural networks (CNNs) have exhibited commendable performance for hyperspectral image (HSI) classification. Generally, an important number of samples are needed for each class to properly train CNNs. However, existing HSI data sets suffer from a significant class imbalance problem, where many classes do not have enough samples to characterize the spectral information. The performance of existing CNN models is biased toward the majority classes, which possess more samples for the training. This article addresses this issue of imbalanced data in HSI classification. In particular, a new 3D-HyperGAMO model is proposed, which uses generative adversarial minority oversampling. The proposed 3D-HyperGAMO automatically generates more samples for minority classes at training time, using the existing samples of that class. The samples are generated in the form of a 3-D hyperspectral patch. A different classifier from the generator and the discriminator is used in the 3D-HyperGAMO model, which is trained using both original and generated samples to determine the classes of newly generated samples to which they actually belong. The generated data are combined classwise with the original training data set to learn the network parameters of the class. Finally, the trained 3-D classifier network validates the performance of the model using the test set. Four benchmark HSI data sets, namely, Indian Pines (IP), Kennedy Space Center (KSC), University of Pavia (UP), and Botswana (BW), have been considered in our experiments. The proposed model shows outstanding data generation ability during the training, which significantly improves the classification performance over the considered data sets. The source code is available publicly at https://github.com/mhaut/3D-HyperGAMO.","1558-0644","","10.1109/TGRS.2021.3052048","Junta de Extremadura (Decreto 14/2018, de 6 de febrero, por el que se establecen las bases reguladoras de las ayudas para la realización de actividades de investigación y desarrollo tecnológico, de divulgación y de transferencia de conocimiento por los Grupos de Investigación de Extremadura, Ref. GR18060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347550","Convolutional neural networks (CNNs);deep learning;spectral–spatial hyperspectral image (HSI) classification","Gallium nitride;Training;Generators;Generative adversarial networks;Hyperspectral imaging;Feature extraction;Electronic mail","convolutional neural nets;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);solid modelling","considered data sets;generative adversarial minority oversampling;spectral-spatial hyperspectral image classification;convolutional neural networks;CNNs;commendable performance;significant class imbalance problem;spectral information;existing CNN models;majority classes;imbalanced data;HSI classification;3D-HyperGAMO model;minority classes;training time;existing samples;3-D hyperspectral patch;original generated samples;newly generated samples;original training data;3-D classifier network;benchmark HSI data sets;outstanding data generation ability;classification performance","","21","","78","IEEE","4 Feb 2021","","","IEEE","IEEE Journals"
"CVA2E: A Conditional Variational Autoencoder With an Adversarial Training Process for Hyperspectral Imagery Classification","X. Wang; K. Tan; Q. Du; Y. Chen; P. Du","Key Laboratory for Land Environment and Disaster Monitoring of National Administration of Surveying and Geoinformation (NASG), China University of Mining and Technology, Xuzhou, China; Key Laboratory for Land Environment and Disaster Monitoring of National Administration of Surveying and Geoinformation (NASG), China University of Mining and Technology, Xuzhou, China; Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, USA; Key Laboratory for Land Environment and Disaster Monitoring of National Administration of Surveying and Geoinformation (NASG), China University of Mining and Technology, Xuzhou, China; Key Laboratory for Satellite Mapping Technology and Applications of National Administration of Surveying and Geoinformation (NASG), Nanjing University, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2020","2020","58","8","5676","5692","Deep generative models such as the generative adversarial network (GAN) and the variational autoencoder (VAE) have obtained increasing attention in a wide variety of applications. Nevertheless, the existing methods cannot fully consider the inherent features of the spectral information, which leads to the applications being of low practical performance. In this article, in order to better handle this problem, a novel generative model named the conditional variational autoencoder with an adversarial training process (CVA2E) is proposed for hyperspectral imagery classification by combining variational inference and an adversarial training process in the spectral sample generation. Moreover, two penalty terms are added to promote the diversity and optimize the spectral shape features of the generated samples. The performance on three different real hyperspectral data sets confirms the superiority of the proposed method.","1558-0644","","10.1109/TGRS.2020.2968304","National Natural Science Foundation of China(grant numbers:41871337); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8989966","Generative adversarial network (GAN);hyperspectral image (HSI) classification;variational autoencoder (VAE)","Gallium nitride;Generative adversarial networks;Training;Hyperspectral imaging;Data models;Generators","convolutional neural nets;feature extraction;hyperspectral imaging;image classification;learning (artificial intelligence)","spectral shape features;spectral sample generation;variational inference;generative model;generative adversarial network;deep generative models;hyperspectral imagery classification;adversarial training process;conditional variational autoencoder","","20","","29","IEEE","10 Feb 2020","","","IEEE","IEEE Journals"
"Denoising the Optical Fiber Seismic Data by Using Convolutional Adversarial Network Based on Loss Balance","X. Dong; Y. Li","Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2021","2021","59","12","10544","10554","Distributed optical fiber acoustic sensing (DAS) is a new and rapid-developing detection technology in seismic exploration. Unfortunately, due to the weak energy of scattered optical signals and the inferior coupling between DAS cable and receiving interface, the seismic data received by DAS are often characterized by low signal-to-noise ratio (SNR); this low SNR is likely to affect some subsequent analysis, such as inversion, imaging, and interpretation. In addition, the noise caused by the inferior coupling is a new kind of noise not presented on conventional seismic data. To enhance the SNR of DAS seismic data and suppress the DAS noise effectively, we propose a convolutional adversarial denoising network (CADN) based on the basic strategy of generative adversarial network (GAN) and the usage of a denoiser to replace the original generator in GAN. In CADN, the performance of denoiser is significantly strengthened via its own mean square error (MSE) loss and the adversarial loss between it and the discriminator. To balance the two losses and thus ensure the optimization of denoiser, we construct a novel loss function, where the optimal ratio of MSE and adversarial losses is determined by quantifying the denoising performance. Both real and synthetic examples are included to testify the denoising performance of CADN. Experimental results have demonstrated that CADN can suppress most of the DAS noise and enhance the SNR of DAS seismic data; also, it can recover the effective signals completely, even the extremely weak effective signals reflected by deep layers.","1558-0644","","10.1109/TGRS.2020.3036065","National Natural Science Foundation of China(grant numbers:41974143,41730422); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9261126","Adversarial training;convolutional adversarial denoising network (CADN);distributed optical fiber acoustic sensing (DAS);generative adversarial network (GAN);low signal-to-noise ratio (SNR)","Noise reduction;Signal to noise ratio;Noise measurement;Generative adversarial networks;Optical scattering;Optical coupling;Adaptive optics","distributed sensors;fibre optic sensors;geophysical signal processing;image denoising;mean square error methods;optical fibre polarisation;seismology;vibration measurement;wavelet transforms","optical fiber seismic data;convolutional adversarial network;loss balance;distributed optical fiber acoustic sensing;seismic exploration;scattered optical signals;inferior coupling;DAS cable;receiving interface;low signal-to-noise ratio;conventional seismic data;DAS seismic data;DAS noise;convolutional adversarial denoising network;CADN;generative adversarial network;denoiser;mean square error loss;adversarial loss;loss function;denoising performance;extremely weak effective signals","","19","","51","IEEE","16 Nov 2020","","","IEEE","IEEE Journals"
"Dual-Channel Capsule Generation Adversarial Network for Hyperspectral Image Classification","J. Wang; S. Guo; R. Huang; L. Li; X. Zhang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","16","Deep learning-based methods have demonstrated significant breakthroughs in the application of hyperspectral image (HSI) classification. However, some challenging issues still exist, such as the overfitting problem caused by the limitation of training size with high-dimensional feature and the efficiency of spectral–spatial (SS) exploitation. Therefore, to efficiently model the relative position of samples within the generative adversarial network (GAN) setting, we proposed a dual-channel SS fusion capsule generative adversarial network (DcCapsGAN) for HSI classification. Dual channels (1-D-CapsGAN and 2-D-CapsGAN) are constructed by integrating the capsule network (CapsNet) with GAN for eliminating the mode collapse and gradient disappearance problem caused by traditional GAN. Meanwhile, octave convolution and multiscale convolution are integrated into the proposed model for further reducing the parameters of the CapsNet and extracting multiscale features. To further boost the classification performance, the SS channel fusion model is constructed to composite and switch the feature information of different channels, thereby facilitating the accuracy and robustness of the whole classification performance. Three commonly used HSI data sets are utilized to investigate the performance of the proposed DcCapsGAN model, and the performance of the experiment demonstrates that the proposed model can efficiently improve the classification accuracy and performance.","1558-0644","","10.1109/TGRS.2020.3044312","State Key Program of National Natural Science of China(grant numbers:61836009); National Natural Science Foundation of China(grant numbers:61801353,61876221); Fundamental Research Funds for the Central Universities(grant numbers:JB191907); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ-065); China Postdoctoral Science Foundation Funded Project(grant numbers:2018M633474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328201","Capsule networks (CapsNets);generative adversarial network (GAN);hyperspectral image (HSI);multiscale convolution","Feature extraction;Generative adversarial networks;Training;Generators;Gallium nitride;Convolution;Hyperspectral imaging","deep learning (artificial intelligence);hyperspectral imaging;image classification;sensor fusion","dual-channel capsule generation adversarial network;hyperspectral image classification;deep learning-based methods;overfitting problem;high-dimensional feature;spectral-spatial exploitation;HSI classification;dual channels;capsule network;CapsNet;gradient disappearance problem;traditional GAN;extracting multiscale features;classification performance;SS channel fusion model;DcCapsGAN model;classification accuracy;2D-CapsGAN;1D-CapsGAN;dual-channel SS fusion capsule;generative adversarial network;mode collapse;octave convolution;multiscale convolution","","16","","61","IEEE","18 Jan 2021","","","IEEE","IEEE Journals"
"Parameter Extraction Based on Deep Neural Network for SAR Target Simulation","S. Niu; X. Qiu; B. Lei; C. Ding; K. Fu","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2020","2020","58","7","4901","4914","Synthetic aperture radar (SAR) image simulation can provide SAR target images under different scenes and imaging conditions at a low cost. These simulation images can be applied to SAR target recognition, image interpretation, 3-D reconstruction, and many other fields. With the accumulation of high-resolution SAR images of targets under different imaging conditions, the simulation process should be benefited from these real images. Accurate simulation parameters are one of the keys to obtain high-quality simulation images. However, it takes a lot of time, energy, and resources to get simulation parameters from actual target measurement or adjusting manually. It is difficult to derive the analytical form of the relation between a SAR image and its simulation parameter, so nowadays the abundant real SAR images can hardly help the SAR simulation. In this article, a framework is proposed to obtain the relationship between SAR images and simulation parameters by training the deep neural network (DNN), so as to extract the simulation parameters from the real SAR image. Two DNNs, convolutional neural network (CNN), and generative adversarial network (GAN) are used to implement this framework. By modifying the network structures and setting reasonable training data, our DNNs can learn the relationship between image and simulation parameters more effectively. Experimental results show that the DNNs can extract the simulation parameters from the real SAR image, which can further improve the similarity of the simulation image while automating the setting of simulation parameters. Compared with CNN, the simulation parameters extracted by GAN can achieve better results at multiple azimuth angles.","1558-0644","","10.1109/TGRS.2020.2968493","National Natural Science Foundation of China(grant numbers:61991421,61991420); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999587","Convolutional neural network (CNN);deep learning;generative adversarial network (GAN);synthetic aperture radar (SAR) target simulation;simulation parameter extraction","Synthetic aperture radar;Data models;Analytical models;Generative adversarial networks;Radar imaging;Solid modeling;Training","convolutional neural nets;radar imaging;synthetic aperture radar","deep neural network;SAR target simulation;synthetic aperture radar image simulation;simulation image;SAR target recognition;image interpretation;high-resolution SAR images;simulation parameter;SAR simulation;parameter extraction;generative adversarial network;GAN;DNNs;CNN","","15","","34","IEEE","14 Feb 2020","","","IEEE","IEEE Journals"
"Generative Adversarial Network for Desert Seismic Data Denoising","H. Wang; Y. Li; X. Dong","Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","7062","7075","Seismic exploration is a kind of exploration method for oil and gas resources. However, the disturbance of numerous random noise will decrease the quality and signal-to-noise ratio (SNR) of real seismic records, which brings difficulties to the following works of processing and interpretation. The seismic records of desert region pose a particular problem because of the strong energy noise and the spectrum overlapping between effective signals and random noise. Recent research works demonstrate that a convolutional neural network (CNN) can increase the SNR of seismic records. The optimization of denoising methods based on CNN is principally driven by the loss functions that largely focus on minimizing the mean-squared reconstruction error between denoising records and theoretical pure records. The denoising results estimated by the CNN model are often lacking the perfection of the signal structure. Therefore, when processing seismic records with low SNR, the denoising results often have a lack of effective signal in some traces, which leads to the poor continuity of events. In order to solve this problem, we adopt the strategy of generative adversarial network (GAN) to construct a GAN for denoising. It is divided into two parts: the generator (the denoising network based on CNN) is used to remove noise, while the discriminator is used to guide the generator to restore the structure information of effective signals. The generator and discriminator enhance the performance of each other through adversarial training, and the generator after adversarial training can greatly recover events and suppress random noise in synthetic and real desert seismic data.","1558-0644","","10.1109/TGRS.2020.3030692","National Natural Science Foundations of China(grant numbers:41730422); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246717","Adversarial training;convolutional neural network (CNN);desert seismic data;generative adversarial network (GAN);low signal-to-noise ratios (SNRs)","Noise reduction;Generators;Generative adversarial networks;Gallium nitride;Convolution;Signal to noise ratio;Training","convolutional neural nets;geophysical signal processing;geophysical techniques;mean square error methods;seismology;signal denoising;signal reconstruction;signal restoration","generative adversarial network;desert seismic data denoising;seismic exploration;numerous random noise;signal-to-noise ratio;SNR;seismic records;convolutional neural network;CNN;denoising methods;denoising records;denoising network;suppress random noise;mean-squared reconstruction error","","14","","45","IEEE","2 Nov 2020","","","IEEE","IEEE Journals"
"A Semisupervised Siamese Network for Hyperspectral Image Classification","S. Jia; S. Jiang; Z. Lin; M. Xu; W. Sun; Q. Huang; J. Zhu; X. Jia","Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; College of Architectural Engineering, Civil Engineering and Environment, Ningbo University, Ningbo, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","17","With the development of hyperspectral imaging technology, hyperspectral images (HSIs) have become important when analyzing the class of ground objects. In recent years, benefiting from the massive labeled data, deep learning has achieved a series of breakthroughs in many fields of research. However, labeling HSIs requires sufficient domain knowledge and is time-consuming and laborious. Thus, how to apply deep learning effectively to small labeled samples is an important topic of research in HSI classification. To solve this problem, we propose a semisupervised Siamese network that embeds Siamese network into a semisupervised learning scheme. It integrates an autoencoder module and a Siamese network to, respectively, investigate information in a large amount of unlabeled data and rectify it with a limited labeled sample set, which is called 3DAES. First, the autoencoder method is trained on the massive unlabeled data to learn the refinement representation, creating an unsupervised feature. Second, based on this unsupervised feature, limited labeled samples are used to train a Siamese network to rectify the unsupervised feature to improve feature separability among various classes. Furthermore, by training the Siamese network, a random sampling scheme is used to accelerate training and avoid imbalance among various sample classes. Experiments on three benchmark HSI datasets consistently demonstrate the effectiveness and robustness of the proposed 3DAES approach with limited labeled samples. For study replication, the code developed for this study is available at https://github.com/ShuGuoJ/3DAES.git.","1558-0644","","10.1109/TGRS.2021.3116138","National Natural Science Foundation of China(grant numbers:41971300,61901278,62001303); Key Project of Department of Education of Guangdong Province(grant numbers:2020ZDZX3045); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011413); China Postdoctoral Science Foundation(grant numbers:2021M692162); Shenzhen Scientific Research and Development Funding Program(grant numbers:20200803152531004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565215","Autoencoder;hyperspectral image (HSI) classification;Siamese network","Feature extraction;Deep learning;Convolutional neural networks;Training;Hyperspectral imaging;Generative adversarial networks;Support vector machines","cartography;deep learning (artificial intelligence);feature extraction;geophysical image processing;hyperspectral imaging;image classification;image representation;semi-supervised learning (artificial intelligence);unsupervised learning","deep learning;HSI;semisupervised Siamese network;semisupervised learning scheme;labeled sample set;unsupervised feature;random sampling scheme;hyperspectral image classification;autoencoder module;3DAES","","11","","80","IEEE","8 Oct 2021","","","IEEE","IEEE Journals"
"Attribute-Based Double Constraint Denoising Network for Seismic Data","S. Wang; Y. Li; N. Wu; Y. Zhao; H. Yao","Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","5304","5316","At present, most of the seismic data denoising methods based on deep learning attempt to establish a synthetic seismic data set as the network training set to train network parameters. However, the synthetic data set cannot completely reflect the structural characteristics of the field seismic data, resulting in some false seismic reflections in field denoised results. For this reason, this article proposes an attribute-based denoising algorithm for seismic data called attribute-based double constraint denoising network (Att-DCDN). This method applies encoder-decoder and attribute classifier to constitute the generative adversarial network (GAN) and attenuates seismic noise by controlling with/without target attributes (noise attribute and signal attribute). Compared with the noise-free field seismic data, attribute vectors of the field data are easier to obtain. Therefore, our training set includes not only the synthetic seismic data but also the field seismic data, so as to reduce accuracy requirement of the synthetic noise-free data. In addition, we propose a double-constraint training way to reduce the losses of effective reflections during the denoising process. Specifically, we consider both noise attenuation and signal retention, i.e., reconstruction loss and residual loss are introduced to constrain recovery of effective reflections, and attribute classification loss and adversarial loss are applied to constrain the attenuation of seismic noise. Both the experimental results of synthetic and field seismic records show that our algorithm can effectively suppress the seismic noise and recover the effective reflections almost completely, even the weak signal areas that are seriously polluted by the seismic noise.","1558-0644","","10.1109/TGRS.2020.3021492","National Natural Science Foundations of China(grant numbers:41730422); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9199416","Attribute training set;attribute-based denoising;double constraint network;seismic data","Noise reduction;Training;Noise measurement;Task analysis;Data mining;Generative adversarial networks;Gallium nitride","deep learning (artificial intelligence);geophysical signal processing;geophysical techniques;geophysics computing;interference suppression;pattern classification;seismology;signal denoising","GAN;attribute vectors;noise attenuation;signal retention;reconstruction loss;residual loss;attribute classification loss;adversarial loss;seismic noise suppression;weak signal areas;generative adversarial network;attribute-based denoising algorithm;deep learning;seismic data denoising;synthetic field seismic records;denoising process;double-constraint training way;synthetic noise-free data;network training set;field data;noise-free field seismic data;signal attribute;noise attribute;target attributes;seismic noise attenuation;attribute classifier;encoder-decoder;field denoised results;false seismic reflections;synthetic data set;network parameters;network training;synthetic seismic data;attribute-based double constraint denoising network","","9","","28","IEEE","17 Sep 2020","","","IEEE","IEEE Journals"
"Dense Point Cloud Completion Based on Generative Adversarial Network","M. Cheng; G. Li; Y. Chen; J. Chen; C. Wang; J. Li","Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart Cities, School of Informatics, Xiamen University, Xiamen, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, Canada","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","10","Point cloud completion aims to reconstruct complete point clouds from partial point clouds, which is widely used in various fields such as autonomous driving and robotics. Most existing methods are sparse point cloud completion, where the number of point clouds after completion is relatively small and the details are insufficient. This article proposes a novel end-to-end generative adversarial network-based dense point cloud completion architecture (DPCG-Net). We design two generative adversarial network (GAN)-based modules that translate point cloud completion into mapping between global feature distributions obtained by encoding partial point clouds and ground truth, respectively. The first designed generator module proposes skip connections to fully connected layer-based network for regenerating global feature and changing the global feature distribution derived from the encoder module to approximate the ground truth global feature distribution. The second proposed discriminator module divides high-dimensional global feature vectors into several smaller batches for judgment to guarantee the similarity between the regenerated global feature and the ground truth. We perform quantitative and qualitative experiments on the ShapeNet and KITTI datasets. Experiments on ShapeNet demonstrate that our model outperforms other models in cases where the lack of a large proportion of point clouds results in a large loss of spatial structure, especially when 80% of point clouds are missing. Moreover, KITTI experiments reveal that it is also valid for realistic situations. In addition, application in classification shows that the classification accuracy of point clouds completed with DPCG-Net is as high as 86.5% under the condition of 80% missing point clouds.","1558-0644","","10.1109/TGRS.2021.3105551","National Natural Science Foundation of China(grant numbers:U1605254,41871380); Natural Sciences and Engineering Research Council of Canada(grant numbers:50503-10284); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9528913","3-D point cloud;deep learning;generative adversarial network (GAN);shape completion","Three-dimensional displays;Generators;Generative adversarial networks;Shape;Decoding;Feature extraction;Solid modeling","image classification;image coding;neural nets;vectors","partial point clouds;sparse point cloud completion;global feature distribution;generator module design;layer-based network;discriminator module;regenerated global feature;dense point cloud completion;high-dimensional global feature vectors;end-to-end generative adversarial network-based dense point cloud completion architecture;DPCG-Net;KITTI datasets;ShapeNet datasets;GAN","","8","","41","IEEE","3 Sep 2021","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and Panchromatic Images Using Generative Adversarial Network and Image Segmentation","W. Dong; Y. Yang; J. Qu; W. Xie; Y. Li","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","29 Dec 2021","2022","60","","1","13","Hyperspectral (HS) image fusion aims at integrating a panchromatic (PAN) image and an HS image, featuring the fused image with the spatial quality of the former and the spectral diversity of the latter. The classic fusion algorithm generally includes three consecutive procedures that are upsampling, detail extraction, and detail injection. In this article, we propose an HS and PAN image fusion method based on generative adversarial network and local estimation of injection gain. Instead of upsampling the HS image by classical interpolation techniques, a generative adversarial super-resolution network (GASN) is designed to obtain the interpolated HS image in the fusion framework. GASN establishes a spectral-information-based discriminator to conduct adversarial learning with the generator, so as to preserve the spectral information of the low-resolution HS image. An image segmentation-based injection gain estimation (ISGE) algorithm is subsequently proposed for HS and PAN images fusion. The injection gain is estimated over image segments obtained by a binary partition tree approach to improve the fusion performance. The proposed GASN and ISGE are implemented into two credible global estimation pansharpening methods, and experimental results prove the performance improvement of the proposed method. The proposed method is also compared with existing state-of-the-art methods, and experiments on several public databases demonstrate that the proposed method is competitive or superior to the state-of-the-art fusion methods.","1558-0644","","10.1109/TGRS.2021.3078711","National Defense Pre-Research Foundation; Higher Education Discipline Innovation Project(grant numbers:B08038); National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2021JQ-194,2021JQ-197); Fundamental Research Funds for the Central Universities(grant numbers:XJS210108,XJS210104); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110856); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437429","Details injection;hyperspectral (HS) fusion;image segment;injection gains","Estimation;Spatial resolution;Pansharpening;Image segmentation;Generators;Generative adversarial networks;Feature extraction","geophysical image processing;image fusion;image representation;image resolution;image sampling;image segmentation;interpolation;learning (artificial intelligence);neural nets;spectral analysis","spectral-information-based discriminator;adversarial learning;low-resolution HS image;image segmentation-based injection gain estimation algorithm;GASN;panchromatic image;hyperspectral image fusion;interpolation techniques;generative adversarial super-resolution network;interpolated HS image;HS image upsampling;PAN images fusion;global estimation pansharpening methods","","6","","43","IEEE","20 May 2021","","","IEEE","IEEE Journals"
"Convolutional Two-Stream Generative Adversarial Network-Based Hyperspectral Feature Extraction","W. Yu; M. Zhang; Z. He; Y. Shen","Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China; School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","23 Dec 2021","2022","60","","1","10","Hyperspectral image processing is faced with difficulties considering its redundant features and complex information. Studies on hyperspectral feature extraction in the deep learning domain have become increasingly popular. The mainstream techniques fully consider the spatial information in local neighborhoods when extracting spectral features by constructing deep neural networks. Deep generative models simulate the intrinsic structure of samples by adequately training, showing their potential values for signal processing. In this article, a convolutional two-stream network (cs2GAN-FE) based on the improved Wasserstein generative adversarial network (WGAN) is proposed for unsupervised hyperspectral spatial–spectral feature extraction. The improved WGAN is composed of one generator and one discriminator; the former perceives real data distributions, and the latter determines the attribution of generated data. The designed two-stream strategy is not a simple extension of a one-stream strategy and considers both the static spectral–spatial information and the dynamic spectral reflectance variation in multiple bands. Intrinsic spatial–spectral features are extracted by the trained discriminator considering sample distributions and feature relationships. The loss function is also improved for the unique structure of cs2GAN-FE. Various state-of-the-art techniques are chosen for comparison. Experimental results show the feasibility and potential of this network. Besides, experiments with the random split and the disjointed split both show that the proposed method can outperform other comparison techniques.","1558-0644","","10.1109/TGRS.2021.3073924","National Natural Science Foundation of China(grant numbers:61876054,62033004); National Key Research and Development Program of China(grant numbers:2019YFC0117400); Fundamental Research Funds for the Central Universities of China(grant numbers:19lgzd10); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420704","Feature extraction;generative model;hyperspectral image;spectral reflectance variance;unsupervised learning","Feature extraction;Hyperspectral imaging;Generators;Generative adversarial networks;Streaming media;Shape;Data models","convolutional neural nets;deep learning (artificial intelligence);feature extraction;hyperspectral imaging;image classification","static spectral-spatial information;dynamic spectral reflectance variation;feature relationships;hyperspectral image processing;deep learning;deep neural networks;deep generative models;Wasserstein generative adversarial network;unsupervised hyperspectral spatial-spectral feature extraction;WGAN;two-stream strategy;sample distributions;convolutional two-stream generative adversarial network;hyperspectral feature extraction;cs2 GAN-FE","","6","","31","IEEE","3 May 2021","","","IEEE","IEEE Journals"
"Sparse Coding-Inspired GAN for Hyperspectral Anomaly Detection in Weakly Supervised Learning","Y. Li; T. Jiang; W. Xie; J. Lei; Q. Du","State Key Laboratory of Integrated Services Networks, Xidian University, Xi&#x2019;an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi&#x2019;an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi&#x2019;an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi&#x2019;an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","17 Jan 2022","2022","60","","1","11","Anomaly detection (AD) from hyperspectral images (HSIs) is of great importance in both space exploration and Earth observations. However, the challenges caused by insufficient datasets, no labels, and noise corruption substantially downgrade the accuracy of detection. To solve these problems, this article proposes a sparse coding (SC)-inspired generative adversarial network (GAN) for weakly supervised hyperspectral AD (HAD), named sparseHAD. It can learn a discriminative latent reconstruction with small errors for background pixels and large errors for anomalous ones. First, a background-category searching step is built to alleviate the difficulty of data annotation. Then, an SC-inspired regularized network is integrated into an end-to-end GAN to form a weakly supervised spectral mapping model consisting of two encoders, a decoder, and a discriminator. This model not only makes the network more robust and interpretable experimentally and theoretically but also develops a new SC-inspired path for HAD. Subsequently, the proposed sparseHAD detects anomalies in a latent space rather than the original space, which also contributes to its noise robustness. Quantitative assessments and experiments over real HSIs demonstrate the unique promise of the proposed sparseHAD. The code, data, and trained models are available at <uri>https://github.com/JiangThea/HAD</uri>.","1558-0644","","10.1109/TGRS.2021.3102048","National Natural Science Foundation of China(grant numbers:62071360,61801359,61571345,91538101,61501346,61502367,61701360); Young Talent fund of University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512017","Anomaly detection (AD);generative adversarial network (GAN);hyperspectral images (HSIs);sparse coding (SC);weakly supervised learning (WSL)","Image reconstruction;Detectors;Generative adversarial networks;Hyperspectral imaging;Dictionaries;Supervised learning;Convolutional neural networks","geophysical image processing;hyperspectral imaging;image coding;neural nets;security of data;supervised learning","sparse coding-inspired GAN;hyperspectral anomaly detection;weakly supervised learning;hyperspectral images;HSI;space exploration;noise corruption;sparse coding-inspired generative adversarial network;weakly supervised hyperspectral AD;discriminative latent reconstruction;background pixels;background-category searching step;data annotation;SC-inspired regularized network;end-to-end GAN;weakly supervised spectral mapping model;SC-inspired path;latent space;noise robustness;sparseHAD","","6","","60","IEEE","12 Aug 2021","","","IEEE","IEEE Journals"
"Entropy Guided Adversarial Domain Adaptation for Aerial Image Semantic Segmentation","A. Zheng; M. Wang; C. Li; J. Tang; B. Luo","Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Artificial Intelligence, Anhui University, Hefei, China; Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Artificial Intelligence, Anhui University, Hefei, China; Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","14","Recent advances on aerial image semantic segmentation mainly employ the domain adaption to transfer knowledge from the source domain to the target domain. Despite the remarkable achievement, most methods focus on the global marginal distribution alignment to reduce the domain shift between source and target domains, leading to a wrong mapping of the well-aligned features. In this article, we propose an effective unsupervised domain adaptation approach, which relies on a novel entropy guided adversarial learning algorithm, for aerial image semantic segmentation. In specific, we perform local feature alignment between domains by learning a self-adaptive weight from the target prediction probability map to measure the interdomain discrepancy. To exploit the meaningful structure information among semantic regions, we propose to utilize the graph convolutions for long-range semantic reasoning. Comprehensive experimental results on the benchmark dataset of aerial image semantic segmentation and natural scenes demonstrate the superior performance of the proposed method compared to the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2021.3113581","National Natural Science Foundation of China(grant numbers:61976002,61976003,61860206004,U20B2068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9552486","Aerial image;graph convolutional network (GCN);information entropy;semantic segmentation;unsupervised domain adaptation (UDA)","Semantics;Image segmentation;Adversarial machine learning;Entropy;Generative adversarial networks;Adaptation models;Layout","entropy;feature extraction;image classification;image segmentation;natural scenes;probability;unsupervised learning","target domains;effective unsupervised domain adaptation approach;aerial image semantic segmentation;entropy guided adversarial domain adaptation;domain adaption;source domain;target domain;domain shift","","5","","67","IEEE","29 Sep 2021","","","IEEE","IEEE Journals"
"PU-GAN: A One-Step 2-D InSAR Phase Unwrapping Based on Conditional Generative Adversarial Network","L. Zhou; H. Yu; V. Pascazio; M. Xing","School of Computer Science and Engineering, Changshu Institute of Technology, Suzhou, China; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; CNIT Consorzio Nazionale Interuniversitario per le Telecomunicazioni, Parma, Italy; Academy of Advanced Interdisciplinary Research, Xidian University, Xian, China","IEEE Transactions on Geoscience and Remote Sensing","15 Mar 2022","2022","60","","1","10","Two-dimensional phase unwrapping (PU) is a classical ill-posed problem in synthetic aperture radar interferometry (InSAR). The traditional algorithmic model-based 2-D PU methods are limited by the Itoh condition, which is from the PU researchers’ experience and has critical challenges under strong phase noises or violent phase changes. Recently, advanced learning-based 2-D PU methods could break through the limitation of the Itoh condition owing to their data-driven frameworks, offering promising results in terms of both the speed and accuracy. The one-step learning-based PU method, as one of the representatives, retrieves the unwrapped phase directly from the wrapped phase through regression. However, the main disadvantage of one-step learning-based PU is that it usually blurs the output unwrapped phase due to its  $L_{2}$  loss, that is, it cannot guarantee the congruency between the rewrapped interferometric fringes of the PU solution and the input interferogram. To solve this problem, we propose a one-step 2-D PU method based on the conditional generative adversarial network (referred to as PU-GAN), which treats 2-D PU as an image-to-image translation problem. The generator in PU-GAN can be trained to generate the unwrapped phase through minimizing a  $L_{1}$ -norm loss based on a U-Net architecture, while simultaneously the corresponding discriminator can learn an adversarial loss by a structure of Patch-GAN that tries to classify if the output unwrapped phase image is real or fake. Both a theoretical analysis and the experimental results show that the proposed method outperforms the representative algorithmic model-based and learning-based 2-D PU methods.","1558-0644","","10.1109/TGRS.2022.3145342","National Science Fund for Distinguished Young Scholars(grant numbers:61825105); National Natural Science Foundation of China(grant numbers:42071438); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9687539","Adversarial loss;conditional generative adversarial network;discriminator;generator;L₁-norm loss;phase unwrapping (PU);synthetic aperture radar interferometry (InSAR)","Generators;Generative adversarial networks;Convolution;Training;Phase noise;Loss measurement;Image segmentation","deep learning (artificial intelligence);learning (artificial intelligence);radar computing;radar imaging;radar interferometry;synthetic aperture radar","PU-GAN;conditional generative adversarial network;two-dimensional phase unwrapping;Itoh condition;strong phase noises;violent phase changes;learning-based PU method;wrapped phase;PU solution;output unwrapped phase image;representative algorithmic model-based;one-step 2D InSAR Phase Unwrapping;synthetic aperture radar interferometry","","4","","39","IEEE","20 Jan 2022","","","IEEE","IEEE Journals"
"SRDN: A Unified Super-Resolution and Motion Deblurring Network for Space Image Restoration","X. Yang; X. Wang; N. Wang; X. Gao","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Geoscience and Remote Sensing","21 Feb 2022","2022","60","","1","11","Space target super-resolution (SR) is a domain-specific single image SR problem aiming to help distinguish the satellite and spacecrafts from numerous space debris. Compared to the other object SR problem, images for space target are always in low quality with varies of degradation condition, as a result of long distance and motion blur, which significantly reduces the manual classification reliability, especially for these small targets, e.g., satellite payloads. To address this challenge, we present an end-to-end SR and deblurring network (SRDN). Concretely, focusing on the low-resolution (LR) space target images with blind motion blur, we integrate the SR and deblur function together, improving the image quality by a unified generative adversarial network (GAN)-based framework. We implement a deblur module by using contrastive learning to extract degradation feature and add symmetrical downsampling and upsampling modules to the SR network in order to restore texture information, while shortcut connections are redesigned to maintain the global similarity. Extensive experiments on the public satellite dataset, BUAA-SID-share1.5, demonstrate that our network outperforms the state-of-the-art SR and deblur methods.","1558-0644","","10.1109/TGRS.2021.3131264","National Natural Science Foundation of China(grant numbers:61976166,62036007,62176195,61922066,61876142); Key Research and Development Program of Shaanxi(grant numbers:2021GY-030); Innovation Capacity Support Plan of Shaanxi Province(grant numbers:2020KJXX-027); Fundamental Research Funds for the Central Universities(grant numbers:JB210115); Open Research Projects of Zhejiang Laboratory(grant numbers:2021KG0AB01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627923","Artificial intelligence;artificial neural networks;high-resolution (HR) imaging;image denoising","Feature extraction;Target recognition;Degradation;Image resolution;Superresolution;Generative adversarial networks;Image restoration","astronomical image processing;feature extraction;image motion analysis;image representation;image resolution;image restoration;image texture;learning (artificial intelligence);neural nets","blind motion blur;deblur function;image quality;unified generative adversarial network-based framework;deblur module;SR network;public satellite dataset;SRDN;unified super-resolution;motion deblurring network;space image restoration;space target super-resolution;domain-specific single image SR problem;space debris;object SR problem;manual classification reliability;low-resolution space target images;SR and deblurring network;symmetrical downsampling module;symmetrical upsampling module;BUAA-SID-share1.5 dataset","","3","","57","IEEE","29 Nov 2021","","","IEEE","IEEE Journals"
"Rank-Aware Generative Adversarial Network for Hyperspectral Band Selection","X. Zhang; W. Xie; Y. Li; J. Lei; Q. Du; G. Yang","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","10 Mar 2022","2022","60","","1","12","Traditional clustering-based band selection (BS) methods treat each band as individuals, and selection is conducted by enlarging the difference between clusters, which leads to the loss of band interaction and information saliency evaluation. In this article, we propose a BS method named rank-aware generative adversarial network (R-GAN) to address these problems. First, centralized reference feature extraction (FE) with GAN aids R-GAN to combine interpretability and interband relevance. Then, the reference feature is refined with the saliency estimation provided by the rank-aware strategy. According to data characteristics, there are two versions of rank computation including tensor and matrix. Finally, the structural similarity index measurement (SSIM) maps the saliency to the original data space to obtain the final BS result. Extensive comparison experiments with popular existing BS approaches on five hyperspectral images (HSIs) datasets show that the proposed R-GAN can address spectral saliency effectively and select more informative band subsets, which outperforms other competitors for both detection and classification tasks. For example, on the SD-1 dataset, the ten bands selected by R-GAN achieve 0.982 ± 0.003 with an improvement of 13.7% in the area under the curve (AUC) value of anomaly detection performance. The peaked accuracy surpasses the baseline by 0.46% for the classification on the PaviaU dataset.","1558-0644","","10.1109/TGRS.2022.3142173","National Natural Science Foundation of China(grant numbers:62121001,62071360,61801359,61571345,91538101,61501346,61502367,61701360); Young Talent Fund of University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:XJS200103); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; Fundamental Research Funds for the Central Universities; Innovation Fund of Xidian University(grant numbers:5001-20109215456); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676658","Band selection (BS);hyperspectral images (HSIs);rank-aware generative adversarial network (R-GAN);spectral saliency","Generative adversarial networks;Iron;Feature extraction;Task analysis;Redundancy;Tensors;Image reconstruction","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image representation;object detection;pattern clustering","hyperspectral images;spectral saliency;informative band subsets;rank-aware generative adversarial network;hyperspectral band selection;traditional clustering-based band selection methods;band interaction;information saliency evaluation;BS method named rank-aware;centralized reference feature extraction;GAN aids R-GAN;interband relevance;saliency estimation;rank-aware strategy;data characteristics;rank computation;structural similarity index measurement maps;original data space","","1","","47","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"Attribute-Guided Target Data Separation Network for DAS VSP Data","S. Wang; Y. Li; Y. Zhao","Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","15 Feb 2022","2022","60","","1","16","Distributed acoustic sensing (DAS) technology is a rapidly evolving fiber-optic sensing technology that has been gradually applied to vertical seismic profile (VSP) data. The DAS VSP data are often contaminated by multiple interference waves, such as random noise, coupled noise, horizontal noise, and the existing methods can only map it from noisy data to effective reflections. However, interference waves are also a relative concept, and many interference waves still have certain application values. Therefore, this article proposes an innovative algorithm called attribute-guided target data separation network (Att-TDSN), which can not only complete the conventional signal-noise separation task but also achieve noise-noise separation task. Specifically, we first propose a flexible and efficient training set, namely, multidimensional weak label training set (Mul-WLTS), which introduces attribute features as the weak labels and specifies the dimension of weak labels according to the number of target data types (effective reflections and several common interference waves). Then, we use the weak labels to guide our network to map training data to specified data types, assisting the network to focus its attention on each kind of target data. Finally, the network parameters are trained by a training mode called “one-way matching and two-way constraint.” “One-way matching” makes data separation result unique, and “two-way constraint” can improve the algorithm’s amplitude preserving ability. Experiments on synthetic and field DAS VSP data show that our network can accurately separate target data. Moreover, for the traditional denoising task, Att-TDSN also has a better denoising performance than existing methods.","1558-0644","","10.1109/TGRS.2021.3126022","National Natural Science Foundation of China(grant numbers:41974143,41730422); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9605591","Attribute-guided;distributed acoustic sensing (DAS);multidimensional weak label training set;seismic data denoising;target data separation;vertical seismic profile (VSP) data","Training;Distributed databases;Interference;Reflection;Noise reduction;Generative adversarial networks;Data mining","fibre optic sensors;geophysical signal processing;geophysical techniques;image denoising;learning (artificial intelligence);neural nets;random noise;seismic waves;seismology;signal denoising;underwater sound","effective reflections;attribute-guided target data separation network;conventional signal-noise separation task;noise-noise separation task;multidimensional weak label training set;weak labels;target data types;common interference waves;map training data;specified data types;data separation result unique;synthetic field DAS VSP data;distributed acoustic sensing technology;rapidly evolving fiber-optic sensing technology;vertical seismic profile data;multiple interference waves;random noise;coupled noise;horizontal noise;noisy data","","1","","52","IEEE","8 Nov 2021","","","IEEE","IEEE Journals"
"A Demand-Driven SAR Target Sample Generation Method for Imbalanced Data Learning","C. Cao; Z. Cui; L. Wang; J. Wang; Z. Cao; J. Yang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","1 Mar 2022","2022","60","","1","15","Since there are differences in the natural frequency of various synthetic aperture radar (SAR) target samples in reality, the problem of imbalanced data on the automatic target recognition (ATR) model has gradually appeared in recent years. The problem makes the classification boundary learned by the ATR model often fuzzy or even wrong. In this article, an SAR target sample generation method was proposed, called demand-driven generative adversarial nets (DDGANs), which provided an effective way to implement imbalanced data learning. When the imbalanced data exacerbated the deterioration of the minority category target samples distribution, the proposed method generated samples to alleviate this negative impact. The proposed method innovatively used two convolutional neural networks to form the discriminator of DDGAN. Among them, a convolutional neural network was used to determine whether the generated sample is real or fake. Moreover, another convolutional neural network can simultaneously dig out the generation demands of different categories of target samples when recognizing the generated samples. The generation demands enabled DDGAN to allocate different generation capabilities to different target samples on demand, thereby alleviating the negative impact of data imbalance. At the same time, DDGAN can autonomously learn the generation demands from imbalanced training sets. Several experimental results based on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset showed the advantages of DDGAN. Compared with existing imbalanced learning algorithms, the proposed method had obvious superiority in recognition performance and data generation efficiency.","1558-0644","","10.1109/TGRS.2021.3134674","National Natural Science Foundation of China(grant numbers:61971101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9646908","Automatic target recognition (ATR);demand-driven;generative adversarial nets (GANs);imbalanced data;synthetic aperture radar (SAR)","Target recognition;Synthetic aperture radar;Training;Generative adversarial networks;Data models;Task analysis;Generators","image recognition;learning (artificial intelligence);neural nets;object detection;pattern classification;radar imaging;radar target recognition;sampling methods;synthetic aperture radar","demand-driven SAR Target sample generation method;imbalanced data learning;synthetic aperture radar target samples;automatic target recognition model;ATR model;called demand-driven generative adversarial nets;DDGAN;minority category target samples distribution;convolutional neural network;generated sample;generation demands;different generation capabilities;different target samples;data imbalance;imbalanced training sets;recognition performance;data generation efficiency","","1","","36","IEEE","10 Dec 2021","","","IEEE","IEEE Journals"
"Underwater LiDAR Image Enhancement Using a GAN Based Machine Learning Technique","D. C. Estrada; F. R. Dalgleish; C. J. D. Ouden; B. Ramos; Y. Li; B. Ouyang","Harbor Branch Oceanic Institute, Florida Atlantic University, Fort Pierce, FL, USA; L3Harris Technologies, Inc., Palm Bay, FL, USA; Harbor Branch Oceanic Institute, Florida Atlantic University, Fort Pierce, FL, USA; Harbor Branch Oceanic Institute, Florida Atlantic University, Fort Pierce, FL, USA; Harbor Branch Oceanic Institute, Florida Atlantic University, Fort Pierce, FL, USA; Harbor Branch Oceanic Institute, Florida Atlantic University, Fort Pierce, FL, USA","IEEE Sensors Journal","28 Feb 2022","2022","22","5","4438","4451","Robust underwater imaging sensors are essential in applications such as identifying and classifying marine animals in turbid waters, typically found around coastal monitoring sites. The Unobtrusive Multi-static Serial LiDAR Imager (UMSLI) has been developed to capture images taken in degraded underwater environments and provides superior range and image contrast over conventional optical cameras. In contrast to traditional underwater LiDAR systems that employ blue-green lasers, UMLSI will not harm the vision of marine animals, which is an important aspect in such settings. As with any other underwater LiDAR sensors, improving the quality of images taken in highly turbid water is essential to the UMSLI system. This paper proposes a novel machine learning image enhancement technique based on a Generative Adversarial Network (GAN) framework. One main contribution of the method is the incorporation of a correntropy-based perceptual loss. This technique has shown to be effective in enhancing ground-based images and has been adapted for LiDAR image enhancement. Given the limitation of existing underwater LiDAR data, a method for simulating degraded data for training is also presented. This LiDAR technique was validated using LiDAR data captured by the UMSLI system within Florida Atlantic University’s Harbor Branch Oceanographic Institute (FAU-HBOI) optical test facility and at the Marine and Coastal Research Laboratory at the Pacific Northwest National Laboratory (MCRL-PNNL). This image enhancement technique can be readily extended to other underwater LiDAR systems.","1558-1748","","10.1109/JSEN.2022.3146133","Department of Energy(grant numbers:DE-EE0007828); Naval Surface Warfare Center Carderock(grant numbers:N0016720P0118); Department of Agriculture; National Institute of Food and Agriculture under the L3Harris-Florida Atlantic University (FAU) Master Research Agreement(grant numbers:2019-67022-29204,A000476667); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691346","Degraded underwater environment;generative adversarial network (GAN);LiDAR image enhancement;unobtrusive multi-static serial LiDAR imager (UMSLI)","Laser radar;Image enhancement;Generative adversarial networks;Sea measurements;Scattering;Optical imaging;Cameras","cameras;geophysical image processing;image enhancement;image sensors;learning (artificial intelligence);neural nets;oceanographic techniques;optical radar;remote sensing by laser beam;remote sensing by radar;turbidity","marine animals;turbid waters;coastal monitoring sites;Unobtrusive Multistatic Serial LiDAR Imager;capture images;degraded underwater environments;superior range;image contrast;traditional underwater LiDAR systems;underwater LiDAR sensors;highly turbid water;UMSLI system;image enhancement technique;Generative Adversarial Network framework;correntropy-based perceptual loss;ground-based images;LiDAR image enhancement;underwater LiDAR data;LiDAR technique;GAN based machine;robust underwater imaging sensors","","3","","61","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"GAN-Based Synthetic Data Augmentation for Infrared Small Target Detection","J. -H. Kim; Y. Hwang","Department of Intelligent Systems and Robotics, Chungbuk National University, Cheongju, Republic of Korea; Department of Intelligent Systems and Robotics, Chungbuk National University, Cheongju, Republic of Korea","IEEE Transactions on Geoscience and Remote Sensing","14 Jun 2022","2022","60","","1","12","Recently, convolutional neural networks (CNNs) have achieved state-of-the-art performance in infrared small target detection. However, the limited number of public training data restricts the performance improvement of CNN-based methods. To handle the scarcity of training data, we propose a method that can generate synthetic training data for infrared small target detection. We adopt the generative adversarial network framework where synthetic background images and infrared small targets are generated in two independent processes. In the first stage, we synthesize infrared images by transforming visible images into infrared ones. In the second stage, target masks are implanted on the transformed images. Then, the proposed intensity modulation network synthesizes realistic target objects that can be diversely generated from further image processing. Experimental results on the recent public dataset show that, when we train various detection networks using the dataset composed of both real and synthetic images, detection networks yield better performance than using real data only.","1558-0644","","10.1109/TGRS.2022.3179891","Grand Information Technology Research Support Program(grant numbers:IITP-2022-2020-0-01462); National Research Foundation of Korea (NRF)(grant numbers:2020R1F1A1077110); Institute of Information & Communications Technology Planning & Evaluation (IITP)(grant numbers:2021-0-02068,2020-0-01077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786867","Convolutional neural network (CNN);generative adversarial network (GAN);image-to-image translation;infrared small target;synthetic data augmentation","Training data;Object detection;Generators;Generative adversarial networks;Training;Data models;Image segmentation","convolutional neural nets;infrared imaging;object detection","GAN-based synthetic data augmentation;infrared small target detection;convolutional neural networks;public training data;CNN-based methods;synthetic training data;generative adversarial network framework;synthetic background images;target masks;realistic target objects;detection networks;synthetic images","","4","","54","IEEE","2 Jun 2022","","","IEEE","IEEE Journals"
"Reinforced Swin-Convs Transformer for Simultaneous Underwater Sensing Scene Image Enhancement and Super-resolution","T. Ren; H. Xu; G. Jiang; M. Yu; X. Zhang; B. Wang; T. Luo","School of Mathematics and Statistics, Ningbo University, Ningbo, China; School of Mathematics and Statistics, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China; School of Mathematics and Statistics, Ningbo University, Ningbo, China; School of Mathematics and Statistics, Ningbo University, Ningbo, China; Faculty of Information Science and Engineering, Ningbo University, Ningbo, China","IEEE Transactions on Geoscience and Remote Sensing","23 Sep 2022","2022","60","","1","16","Underwater image enhancement (UIE) technology aims to tackle the challenge of restoring the degraded underwater images due to light absorption and scattering. Meanwhile, the ever-increasing requirement for higher resolution images from a lower resolution in the underwater domain cannot be overlooked. To address these problems, a novel U-Net-based reinforced Swin-Convs Transformer for simultaneous enhancement and superresolution (URSCT-SESR) method is proposed. Specifically, with the deficiency of U-Net based on pure convolutions, the Swin Transformer is embedded into U-Net for improving the ability to capture the global dependence. Then, given the inadequacy of the Swin Transformer capturing the local attention, the reintroduction of convolutions may capture more local attention. Thus, an ingenious manner is presented for the fusion of convolutions and the core attention mechanism to build a reinforced Swin-Convs Transformer block (RSCTB) for capturing more local attention, which is reinforced in the channel and the spatial attention of the Swin Transformer. Finally, experimental results on available datasets demonstrate that the proposed URSCT-SESR achieves the state-of-the-art performance compared with other methods in terms of both subjective and objective evaluations. The code is publicly available at https://github.com/TingdiRen/URSCT-SESR.","1558-0644","","10.1109/TGRS.2022.3205061","Natural Science Foundation of China(grant numbers:62171243,61871247,62071266,61931022,61671412,62271276); Zhejiang Natural Science Foundation of China(grant numbers:LY19F020009,LY21F010003,LY19F010002,LQ20F010002,LY21F010014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9881581","Super-resolution (SR);Swin-Convs Transformer;U-Net;underwater image enhancement (UIE)","Transformers;Atmospheric modeling;Generative adversarial networks;Image resolution;Image enhancement;Convolutional neural networks;Superresolution","image enhancement;image resolution","simultaneous underwater sensing scene image enhancement;super-resolution;image enhancement technology;degraded underwater images;light absorption;higher resolution images;underwater domain;simultaneous enhancement;URSCT-SESR;Swin Transformer;local attention;core attention mechanism;reinforced Swin-Convs Transformer block;restoring;scattering;novel U-Net-based reinforced Swin-Convs Transformer;superresolution;(URSCT-SESR) method;convolutions;Swin Transformer capturing;embedded;reintroduction;ingenious manner;fusion;mechanism;spatial attention;datasets demonstrate;state-of-the-art;subjective;objective evaluations;U-Net;capture;lower resolution;U","","2","","69","IEEE","8 Sep 2022","","","IEEE","IEEE Journals"
"DenoiseNet: Deep Generator and Discriminator Learning Network With Self-Attention Applied to Ocean Data","M. Mao; H. Wang; P. Nie; S. Xiao; R. Wu","Department of Geophysics, College of Geophysics, Chengdu University of Technology, Chengdu, China; Department of Geophysics, College of Geophysics, Chengdu University of Technology, Chengdu, China; Department of Geophysics, College of Geophysics, Chengdu University of Technology, Chengdu, China; Department of Geophysics, College of Geophysics, Chengdu University of Technology, Chengdu, China; Department of Geophysics, College of Geophysics, Chengdu University of Technology, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","9 Nov 2022","2022","60","","1","12","Well-organized seismic signals play a significant role in the subsequent seismic data processing. The multiscale learning of characteristic signals of complex structures by deep convolutional neural networks has obvious benefits in reducing random noise in seismic data. However, deep convolutional neural networks also have shortcomings. It cannot discover effective features in seismic data structures or recover high-quality seismic signals just using convolution. Therefore, the article presents a generative adversarial network (GAN) architecture in conjunction with the U-Net network. To produce the mapping connection between clean seismic signals and noisy seismic data, the U-Net network is employed as the  $G$  network of GAN. Incorporating a self-attention mechanism to strengthen the correlation between seismic data, with the goal of improving the network’s reconstruction capacity on the continuity of seismic signals. The intelligent denoising of seismic data enabled by denoising network with self-attention GAN (DsGAN) enhances labor efficiency when compared to traditional approaches. When compared to the optimal state of current models such as denoising convolutional neural network (DnCNN), denoising network GAN (DnGAN), the peak signal-to-noise ratio (PSNR) is enhanced by 1.52 dB of the DsGAN model, according to experimental data from simulated and actual seismic data. Experiments show that the network has the ability to learn complex unknown noise, and has strong generalization and robustness.","1558-0644","","10.1109/TGRS.2022.3217402","National Natural Science Foundation of China(grant numbers:91755215); Key Research and Development Program of Sichuan Province of China(grant numbers:2020YFG0146); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930787","Deep-learning;generative adversarial network (GAN);seismic denoising;self-attention","Noise reduction;Generative adversarial networks;Generators;Convolutional neural networks;Training;Noise measurement;Image reconstruction","convolutional neural nets;deep learning (artificial intelligence);geophysical signal processing;oceanographic techniques;random noise;seismology;signal denoising","deep generator;discriminator learning network;ocean data;subsequent seismic data processing;characteristic signals;deep convolutional neural networks;seismic data structures;high-quality seismic signals;generative adversarial network architecture;U-Net network;clean seismic signals;noisy seismic data;self-attention GAN enhances;convolutional neural network;network GAN;peak signal-to-noise ratio;simulated data;actual seismic data","","1","","48","IEEE","26 Oct 2022","","","IEEE","IEEE Journals"
"Clutter Mitigation in Holographic Subsurface Radar Imaging Using Generative Adversarial Network With Attentive Subspace Projection","C. Chen; Y. Su; Z. He; T. Liu; X. Song","School of Electronic Science and Technology, National University of Defense Technology, Changsha, China; School of Electronic Science and Technology, National University of Defense Technology, Changsha, China; School of Electronic Science and Technology, National University of Defense Technology, Changsha, China; School of Electronic Science and Technology, National University of Defense Technology, Changsha, China; School of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","10 Aug 2022","2022","60","","1","14","The holographic subsurface radar (HSR) has been a promising geophysical electromagnetic technique for detecting shallowly buried targets with high lateral resolution image. However, radar images are considerably interpreted by strong reflections from rough surface and inhomogeneity in media of interest. In this article, we focus on mitigating the clutter in HSR applications using a learning-based approach, which requires neither prior information regarding the penetrable medium characteristics nor analytic framework to describe the through-medium interference. The generative adversarial network (GAN) with attentive subspace projection is developed to remove the clutter and recover the target image. The subspaces containing target response are selected with the multihead attention preliminarily. Then, the generative network will further focus on the target regions, and the discriminative network will assess the generated results locally and globally. Experiments using real data were conducted to demonstrate the effectiveness of our approach. The visual and quantitative results show that the proposed approach achieves superior performance on removing clutter in HSR images compared with the state-of-the-art clutter mitigation approaches.","1558-0644","","10.1109/TGRS.2022.3194560","National Natural Science Foundation of China(grant numbers:61901501,62001488); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844001","Clutter mitigation;generative adversarial network (GAN);holographic subsurface radar (HSR);multihead attention;subspace projection","Clutter;Radar imaging;Radar;Generative adversarial networks;Radar clutter;Radar antennas;Imaging","buried object detection;ground penetrating radar;image resolution;learning (artificial intelligence);radar clutter;radar detection;radar imaging;rough surfaces","holographic subsurface radar imaging;generative adversarial network;attentive subspace projection;promising geophysical electromagnetic technique;shallowly buried targets;high lateral resolution image;radar images;strong reflections;rough surface;HSR applications;learning-based approach;penetrable medium characteristics;through-medium interference;target image;target response;multihead attention;generative network;target regions;discriminative network;generated results;removing clutter;HSR images;state-of-the-art clutter mitigation approaches","","1","","44","IEEE","28 Jul 2022","","","IEEE","IEEE Journals"
"Semisupervised Hyperspectral Image Classification Using a Probabilistic Pseudo-Label Generation Framework","M. Seydgar; S. Rahnamayan; P. Ghamisi; A. A. Bidgoli","Department of Construction Engineering, École de Technologie Supérieure (ÉTS), University of Quebec, Montreal, Canada; Department of Electrical, Computer, and Software Engineering, Ontario Tech University, Oshawa, Canada; Institute of Advanced Research in Artificial Intelligence (IARAI), Vienna, Austria; Department of Electrical, Computer, and Software Engineering, Ontario Tech University, Oshawa, Canada","IEEE Transactions on Geoscience and Remote Sensing","15 Aug 2022","2022","60","","1","18","Deep neural networks (DNNs) show impressive performance for hyperspectral image (HSI) classification when abundant labeled samples are available. The problem is that HSI sample annotation is extremely costly and the budget for this task is usually limited. To reduce the reliance on labeled samples, deep semisupervised learning (SSL), which jointly learns from labeled and unlabeled samples, has been introduced in the literature. However, learning robust and discriminative features from unlabeled data is a challenging task due to various noise effects and ambiguity of unlabeled samples. As a result, recent advances are constrained, mainly in the pretraining or warm-up stage. In this article, we propose a deep probabilistic framework to generate reliable pseudo-labels to explicitly learn discriminative features from unlabeled samples. The generated pseudo-labels of our proposed framework can be fed to various DNNs to improve their generalization capacity. Our proposed framework takes only ten labeled samples per class to represent the label set as an uncertainty-aware distribution (We use the Gaussian distribution to represent the uncertainty of the label set in the latent space.) in the latent space. The pseudo-labels are then generated for those unlabeled samples whose feature values match the distribution with high probability. By performing extensive experiments on four publicly available datasets, we show that our framework can generate reliable pseudo-labels to significantly improve the generalization capacity of several state-of-the-art DNNs. In addition, we introduce a new DNN for HSI classification that demonstrates outstanding accuracy results in comparison with its rivals.","1558-0644","","10.1109/TGRS.2022.3195924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849704","Deep neural network (DNN);hyperspectral image (HSI) classification;probabilistic embedding;pseudo-labeling;semisupervised learning (SSL)","Uncertainty;Probabilistic logic;Feature extraction;Generative adversarial networks;Training;Task analysis;Reliability","convolutional neural nets;deep learning (artificial intelligence);feature extraction;Gaussian distribution;geophysical image processing;hyperspectral imaging;image classification;neural nets;supervised learning","hyperspectral image classification;probabilistic pseudolabel generation framework;deep neural networks;HSI sample annotation;deep semisupervised learning;discriminative features;deep probabilistic framework;HSI classification;DNN","","1","","85","IEEE","4 Aug 2022","","","IEEE","IEEE Journals"
"MSTCGAN: Multiscale Time Conditional Generative Adversarial Network for Long-Term Satellite Image Sequence Prediction","K. Dai; X. Li; Y. Ye; S. Feng; D. Qin; R. Ye","Department of Computer Science, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, Harbin Institute of Technology, Shenzhen, China; National Satellite Meteorological Center, China Meteorological Administration, Beijing, China; Department of Computer Science, Harbin Institute of Technology, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","16 Jun 2022","2022","60","","1","16","Satellite image sequence prediction is a crucial and challenging task. Previous studies leverage optical flow methods or existing deep learning methods on spatial–temporal sequence models for the task. However, they suffer from either oversimplified model assumptions or blurry predictions and sequential error accumulation issue, for a long-term forecast requirement. In this article, we propose a novel multiscale time conditional generative adversarial network (MSTCGAN). To address the sequential error accumulation issue, MSTCGAN adopts a parallel prediction framework to produce the future image sequences by a one-hot time condition input. In addition, a powerful multiscale generator is designed with the multihead axial attention, which helps to carefully preserve the fine-grained details for appearance consistency. Moreover, we develop a temporal discriminator to address the blurry issue and maintain the motion consistency in prediction. Extensive experiments have been conducted on the FengYun-4A satellite dataset, and the results demonstrate the effectiveness and superiority of the proposed method over state-of-the-art approaches.","1558-0644","","10.1109/TGRS.2022.3181279","Shenzhen Science and Technology Program(grant numbers:JCYJ20200109113014456,JCYJ20210324120208022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791392","Deep learning;generative adversarial network (GAN);satellite image sequence prediction;spatial–temporal sequence prediction","Generators;Image sequences;Satellites;Generative adversarial networks;Task analysis;Training;Predictive models","deep learning (artificial intelligence);geophysical image processing;image sequences","blurry predictions;sequential error accumulation issue;long-term forecast requirement;MSTCGAN;parallel prediction framework;image sequences;one-hot time condition input;powerful multiscale generator;satellite dataset;long-term satellite image sequence prediction;optical flow methods;deep learning;oversimplified model assumptions;multiscale time conditional generative adversarial network;spatial-temporal sequence models","","","","53","IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"Self-Supervised Learning With Prediction of Image Scale and Spectral Order for Hyperspectral Image Classification","X. Yang; W. Cao; Y. Lu; Y. Zhou","Department of Computer and Information Science, University of Macau, Macau, China; Yangtze Three Gorges Technology and Economy Development Company Ltd., Beijing, China; Department of Computer Science and Technology, Harbin Institute of Technology (Shenzhen), Shenzhen, China; Department of Computer and Information Science, University of Macau, Macau, China","IEEE Transactions on Geoscience and Remote Sensing","26 Dec 2022","2022","60","","1","15","In recent years, convolutional neural networks (CNNs) have achieved great success in hyperspectral image (HSI) classification attributed to their unparalleled capacity to extract the local information. However, to successfully learn the high-level semantic image features, they always require massive amounts of manually labeled data during the training process, which is expensive, scarce, and impractical, and severely hinders the improvement of supervised deep learning methods. To alleviate these burdens, we present self-supervised learning (SSL) methods for HSI classification by a pretraining model using extensive unlabeled data and fine-tuning the HSI target classification. In this article, we propose a new method for learning image characteristics by training a CNN to recognize the image scale (IS) that is applied to the HSIs. In addition, we propose a multipretext task (MT) method to learn stable and good feature representations combing two different pretext task methods and contrastive loss function. We evaluate the proposed methods in SSL benchmarks on four benchmark HSIs datasets. The experiment results demonstrate that the proposed methods outperform the traditional supervised deep learning methods when large amounts of unlabeled HSIs data are used. Moreover, it demonstrates that the SSL method is promising to alleviate dependence on manually labeled data of HSI classification. Finally, our research contributes to the creation and refinement of SSL methods for pretextual tasks within the HSIs community.","1558-0644","","10.1109/TGRS.2022.3225663","Science and Technology Development Fund, Macau SAR(grant numbers:0049/2022/A1); University of Macau(grant numbers:MYRG2022-00072-FST); Macao Young Scholars Program(grant numbers:AM2020012); Guangdong Shenzhen Joint Youth Fund(grant numbers:2021A151511074); NSFC Funds(grant numbers:62206073); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9966805","Hyperspectral image (HSI) classification;limited labeled samples;self-supervised learning (SSL);unsupervised learning","Task analysis;Hyperspectral imaging;Self-supervised learning;Feature extraction;Convolutional neural networks;Supervised learning;Generative adversarial networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image classification;image representation;supervised learning","benchmark HSIs datasets;CNN;contrastive loss function;convolutional neural networks;extensive unlabeled data;good feature representations;high-level semantic image features;HSI target classification;HSIs community;hyperspectral image classification;image characteristics;image scale prediction;local information extraction;manually labeled data;multipretext task method;pretraining model;self-supervised learning methods;spectral order prediction;SSL benchmarks;SSL method;stable feature representations;supervised deep learning methods;training process;unlabeled HSIs data","","","","61","IEEE","30 Nov 2022","","","IEEE","IEEE Journals"
"Deep Velocity Generator: A Plug-in Network For FWI Enhancement","Y. Wang; B. Jiang; Z. Wei; W. Lu","Department of Automation, Tsinghua University, Beijing, China; College of Mechanical and Electrical Engineering, Beijing University of Chemical Technology, Beijing, China; Sinopec Petroleum Exploration and Production Research Institute, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2023","PP","99","1","1","Known for its great potential for determining subsurface properties quantitatively, full-waveform inversion (FWI) is a hot topic in the field of exploration seismology. The success of FWI depends significantly on the accuracy of the starting model. Given that both the migration and velocity profiles originate from the same geological structure, the two should be morphologically consistent. Starting from the velocity‐reflector depth trade-off, we propose a deep learning approach with a new training paradigm for building a good starting model. A velocity model and the corresponding migration image are used to form 2-channel inputs, and the Generative Adversarial Network (GAN) is trained to minimize the difference between the output and the true velocity model. After the training, the velocity generator network becomes a plug-in component to enhance the FWI performance. The network can be well generalized to unseen data by training with only the synthetic data. We perform extensive experiments on our test dataset, the Marmousi model, the salt velocity model, and field data to demonstrate the effectiveness of our method. Besides, we briefly give an explanation of why our model produces such outputs in this manuscript, making the proposed method more controllable and credible.","1558-0644","","10.1109/TGRS.2023.3247880","National Key Research and Development Program of China(grant numbers:2018YFA0702501); National Natural Science Foundation of China(grant numbers:41674116,41974126,42004101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10050165","Full-waveform inversion (FWI);deep learning;velocity model;depth migration","Data models;Computational modeling;Training;Generators;Generative adversarial networks;Deep learning;Geology","","","","","","","IEEE","22 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Hyperspectral Image Classification With Contrastive Graph Convolutional Network","W. Yu; S. Wan; G. Li; J. Yang; C. Gong","PCA Laboratory, Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Laboratory, Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Laboratory, Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Laboratory, Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; PCA Laboratory, Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","10 Feb 2023","2023","61","","1","15","Recently, graph convolutional network (GCN) has been widely used in hyperspectral image (HSI) classification due to its satisfactory performance. However, the number of labeled pixels is very limited in HSI, and thus, the available supervision information is usually insufficient, which will inevitably degrade the representation ability of most existing GCN-based methods. To enhance the feature representation ability, in this article, a GCN model with contrastive learning is proposed to explore the supervision signals contained in both spectral information and spatial relations, which is termed contrastive GCN (ConGCN), for HSI classification. First, in order to mine sufficient supervision signals from spectral information, a semisupervised contrastive loss function is utilized to maximize the agreement between different views of the same node or the nodes from the same land cover category. Second, to extract the precious yet implicit spatial relations in HSI, a graph generative loss function is leveraged to explore supplementary supervision signals contained in the graph topology. In addition, an adaptive graph augmentation technique is designed to flexibly incorporate the spectral–spatial priors of HSI, which helps facilitate the subsequent contrastive representation learning. The extensive experimental results on six typical benchmark datasets firmly demonstrate the effectiveness of the proposed ConGCN in both qualitative and quantitative aspects.","1558-0644","","10.1109/TGRS.2023.3240721","NSF of China(grant numbers:61973162,62006119); NSF of Jiangsu Province(grant numbers:BZ2021013); NSF for Distinguished Young Scholar of Jiangsu Province(grant numbers:BK20220080); Fundamental Research Funds for the Central Universities(grant numbers:30920032202,30921013114); Chinese Association for Artificial Intelligence (CAAI)-Huawei MindSpore Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032180","Contrastive learning;graph augmentation;graph convolutional network (GCN);hyperspectral image (HSI) classification","Convolutional neural networks;Convolution;Hyperspectral imaging;Representation learning;Adaptation models;Generative adversarial networks;Training","","","","","","62","IEEE","30 Jan 2023","","","IEEE","IEEE Journals"
"Self-Supervised SAR Image Registration With SAR-Superpoint and Transformation Aggregation","B. Zou; H. Li; L. Zhang","Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2023","2023","61","","1","15","Owing to various factors, including severe speckle noise and orbit direction differences, performing multitemporal synthetic aperture radar (SAR) image registration with high accuracy and robustness may become difficult. Herein, an efficient self-supervised deep learning registration network for multitemporal SAR image registration, SAR-superpoint and transformation aggregation network (SSTA-Net), is proposed. The SSTA-Net consists of three parts: 1) the SAR-Superpoint detection network (SS-Net); 2) the transformation aggregation feature matching network (TA-Net); and 3) the unstable point removal module. Specifically, a pseudolabel generation method is adopted without additional annotations. It transfers the characteristics of real SAR data to synthetic data through a feature transition module, which can generate feature point labels for real SAR images for self-training SS-Net. Furthermore, a position–channel aggregation attention is proposed and embedded into the SS-Net to efficiently capture position and channel information and to increase the stability and accuracy of feature point identification. Finally, a unique transformation aggregation strategy is designed to improve the robustness of feature matching, and an unstable point removal module is adopted to eliminate the mismatched point pairs caused by orbit differences. Six sets of multitemporal SAR images were used to evaluate the registration performance of the SSTA-Net, and our model was also compared with the traditional and deep learning algorithms. The experimental results demonstrate that the SSTA-Net outperforms various state-of-the-art approaches for SAR image registration.","1558-0644","","10.1109/TGRS.2022.3231904","National Natural Science Foundation of China(grant numbers:62271172,61871158); Aeronautical Science Foundation of China(grant numbers:20182077008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002310","Deep learning;feature matching;feature point detection;image registration;synthetic aperture radar (SAR) image","Feature extraction;Radar polarimetry;Synthetic aperture radar;Robustness;Orbits;Image registration;Generative adversarial networks","feature extraction;geophysical image processing;geophysical techniques;image denoising;image matching;image registration;image segmentation;learning (artificial intelligence);radar imaging;synthetic aperture radar;transforms","deep learning algorithms;feature point identification;feature point labels;feature transition module;mismatched point pairs;multitemporal SAR image registration;multitemporal SAR images;multitemporal synthetic aperture radar image registration;orbit differences;orbit direction differences;position-channel aggregation attention;pseudolabel generation method;registration performance;SAR data;SAR-Superpoint detection network;self-supervised deep learning registration network;self-training SS-Net;severe speckle noise;SSTA-Net consists;SSTA-Net outperforms;supervised SAR image registration;synthetic data;traditional learning algorithms;transformation aggregation feature matching network;transformation aggregation network;unique transformation aggregation strategy;unstable point removal module","","","","38","IEEE","28 Dec 2022","","","IEEE","IEEE Journals"
"HSIGAN: A Conditional Hyperspectral Image Synthesis Method With Auxiliary Classifier","W. Liu; J. You; J. Lee","Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea; Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea; Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Apr 2021","2021","14","","3330","3344","In this article, we explore a conditional hyperspectral image (HSI) synthesis method with generative adversarial networks (GAN). A new multistage and multipole generative adversarial network, which is suitable for conditional HSI generation and classification (HSIGAN), is proposed. For HSIs synthesis, it is crucial to learn a great deal of spatial-spectral distribution features from source data. The multistage progressive training makes the generator effectively imitate the real data by fully exploiting the high-dimension learning capability of GAN models. The coarse-to-fine information extraction method helps the discriminator to understand the semantic feature better while the multiscale classification prediction presents a positive impact on results. A spectral classifier joins the adversarial network, which offers a helping hand to stabilize and optimize the model. Moreover, we apply the 3-D DropBlock layer in the generator to remove semantic information in a contiguous spatial-spectral region and avoid model collapse. Experimental results of the quantitative and qualitative evaluation show that HSIGAN could generate high-fidelity, diverse hyperspectral cubes while achieving top-ranking accuracy for supervised classification. This result is encouraging for using GANs as a data augmentation strategy in the HSI vision task.","2151-1535","","10.1109/JSTARS.2021.3063911","Natural Science Foundation of Hebei Province(grant numbers:F2020403030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369834","Classification;generative adversarial network (GAN);hyperspectral image (HSI);synthesis","Gallium nitride;Generative adversarial networks;Generators;Hyperspectral imaging;Deep learning;Training;Task analysis","feature extraction;hyperspectral imaging;image classification;image colour analysis;neural nets;supervised learning","3-D DropBlock layer;conditional HSI generation and classification;HSI vision task;data augmentation strategy;supervised classification;hyperspectral cubes;semantic information;spectral classifier;multiscale classification prediction;semantic feature;coarse-to-fine information extraction method;GAN models;high-dimension learning capability;multistage progressive training;spatial-spectral distribution features;multipole generative adversarial network;auxiliary classifier;conditional hyperspectral image synthesis method;HSIGAN","","2","","58","CCBYNCND","4 Mar 2021","","","IEEE","IEEE Journals"
"The Added Value of Cycle-GAN for Agriculture Studies","E. Şener; E. Çolak; E. Erten; G. Taşkin","Department of Geomatics Engineering, Istanbul Technical University, Istanbul, Turkey; Department of Microwave Engineering and Electromagnetic Theory, Chemnitz University of Technology, Chemnitz, Germany; Department of Geomatics Engineering, Istanbul Technical University, Istanbul, Turkey; Istanbul Technical University, Earthquake Engineering and Disaster Management Institute, Istanbul, Turkey","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","7039","7042","It is significant to monitor the phenological stages of agricultural crops with accurate and up-to-date information. In monitoring the phenological phases of some crops, optical remote sensing data offers significant spectral information and outstanding feature identification. However, a continuous time series of optical remote sensing data is difficult to obtain due to the weather dependency of optical acquisitions. In this paper, the feasibility of transfer learning between the features of Sentinel-1 and Sentinel-2 is evaluated to reduce these difficulties. A feature translation based on deep learning (DL) method, namely Cycle-Consistent Generative Adversarial Networks (cycle-GAN), was applied between Sentinel-1 and Sentinel-2 data. In order to evaluate the effect of the cycle-GAN method on crop type mapping and identification, Random Forest classification was applied to four different cases (Real SAR, Fake Optical + Real SAR, Real Optical, and Real Optical + Real SAR).","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553876","Agriculture;Sentinel-1;Sentinel-2;cycle-GAN;Consistent Adversarial Networks;Random Forest Classification","Transfer learning;Time series analysis;Crops;Optical imaging;Optical sensors;Synthetic aperture radar;Remote sensing","","","","","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A Comparative Analysis of GAN-Based Methods for SAR-to-Optical Image Translation","Y. Zhao; T. Celik; N. Liu; H. -C. Li","School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; Faculty of Engineering and Science, University of Agder, Kristiansand, Norway; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","2 Jun 2022","2022","19","","1","5","Unlike optical sensors, synthetic aperture radar (SAR) sensors acquire images of the Earth’s surface with all-weather and all-time capabilities, which is vital in a situation such as a disaster assessment. However, SAR sensors do not offer as rich visual information as optical sensors. SAR-to-optical image-to-image translation generates optical images from SAR images to benefit from what both imaging modalities have to offer. It also enables multisensor image analysis of the same scene for applications such as heterogeneous change detection. Various architectures of generative adversarial networks (GANs) have achieved remarkable image-to-image translation results in different domains. Still, their performances in SAR-to-optical image translation have not been analyzed in the remote-sensing domain. This letter compares and analyzes the state-of-the-art GAN-based translation methods with open-source implementations for SAR-to-optical image translation. The results show that GAN-based SAR-to-optical image translation methods achieve satisfactory results. However, their performances depend on the structural complexity of the observed scene and the spatial resolution of the data. We also introduce a new dataset with a higher resolution than the existing SAR-to-optical image datasets and release implementations of GAN-based methods considered in this letter to support the reproducible research in remote sensing.","1558-0571","","10.1109/LGRS.2022.3177001","Sichuan Provincial Science and Technology Projects(grant numbers:2019JDJQ0023); National Key Research and Development Program of China(grant numbers:2020YFB0505704); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779739","Generative adversarial network (GAN);image-to-image translation;multisensor images;optical;remote sensing;SAR-to-optical image translation;synthetic aperture radar (SAR)","Optical sensors;Synthetic aperture radar;Optical imaging;Training;Sensors;Generators;Image sensors","disasters;geophysical image processing;image resolution;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","GAN-based methods;optical sensors;synthetic aperture radar sensors;SAR sensors;SAR-to-optical image-to-image translation;optical images;SAR images;imaging modalities;multisensor image analysis;remarkable image-to-image translation results;state-of-the-art GAN-based translation methods;GAN-based SAR-to-optical image translation methods;SAR-to-optical image datasets","","1","","16","IEEE","23 May 2022","","","IEEE","IEEE Journals"
"Generative Adversarial Training for Weakly Supervised Cloud Matting","Z. Zou; W. Li; T. Shi; Z. Shi; J. Ye","University of Michigan, Ann Arbor; Beihang University; NetEase Fuxi AI Lab; University of Michigan, Ann Arbor; Didi Chuxing & University of Michigan, Ann Arbor","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","201","210","The detection and removal of cloud in remote sensing images are essential for earth observation applications. Most previous methods consider cloud detection as a pixel-wise semantic segmentation process (cloud v.s. background), which inevitably leads to a category-ambiguity problem when dealing with semi-transparent clouds. We re-examine the cloud detection under a totally different point of view, i.e. to formulate it as a mixed energy separation process between foreground and background images, which can be equivalently implemented under an image matting paradigm with a clear physical significance. We further propose a generative adversarial framework where the training of our model neither requires any pixel-wise ground truth reference nor any additional user interactions. Our model consists of three networks, a cloud generator G, a cloud discriminator D, and a cloud matting network F, where G and D aim to generate realistic and physically meaningful cloud images by adversarial training, and F learns to predict the cloud reflectance and attenuation. Experimental results on a global set of satellite images demonstrate that our method, without ever using any pixel-wise ground truth during training, achieves comparable and even higher accuracy over other fully supervised methods, including some recent popular cloud detectors and some well-known semantic segmentation frameworks.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9009465","","Cloud computing;Clouds;Training;Attenuation;Generators;Imaging;Generative adversarial networks","clouds;geophysical image processing;image classification;image colour analysis;image segmentation;learning (artificial intelligence);remote sensing","recent popular cloud detectors;semantic segmentation frameworks;generative adversarial training;weakly supervised cloud;remote sensing images;earth observation applications;cloud detection;pixel-wise semantic segmentation process;cloud v.s. background;category-ambiguity problem;semitransparent clouds;mixed energy separation process;image matting paradigm;clear physical significance;generative adversarial framework;pixel-wise ground truth reference;cloud generator G;cloud discriminator D;cloud matting network F;cloud images;cloud reflectance;satellite images;fully supervised methods","","8","","51","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Self-Ensembling GAN for Cross-Domain Semantic Segmentation","Y. Xu; F. He; B. Du; D. Tao; L. Zhang","Institute of Advanced Research in Artificial Intelligence (IARAI), Vienna, Austria; JD Explore Academy, JD.com Inc., Beijing, China; School of Computer Science, National Engineering Research Center for Multimedia Software, Institute of Artificial Intelligence, and Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; JD Explore Academy, JD.com Inc., Beijing, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Multimedia","","2022","PP","99","1","14","Deep neural networks (DNNs) have greatly contributed to the performance gains in semantic segmentation. Nevertheless, training DNNs generally requires large amounts of pixel-level labeled data, which is expensive and time-consuming to collect in practice. To mitigate the annotation burden, this paper proposes a self-ensembling generative adversarial network (SE-GAN) exploiting cross-domain data for semantic segmentation. In SE-GAN, a teacher network and a student network constitute a self-ensembling model for generating semantic segmentation maps, which together with a discriminator, forms a GAN. Despite its simplicity, we find SE-GAN can significantly boost the performance of adversarial training and enhance the stability of the model, the latter of which is a common barrier shared by most adversarial training-based methods. We theoretically analyze SE-GAN and provide an $\mathcal {O}(1/\sqrt{N})$ generalization bound ($N$ is the training sample size), which suggests controlling the discriminator's hypothesis complexity to enhance the generalizability. Accordingly, we choose a simple network as the discriminator. Extensive and systematic experiments in two standard settings demonstrate that the proposed method significantly outperforms current state-of-the-art approaches. The source code of our model is available online (https://github.com/YonghaoXu/SE-GAN).","1941-0077","","10.1109/TMM.2022.3229976","National Natural Science Foundation of China(grant numbers:41871243,62225113,41820104006,61871299); Major Science and Technology Innovation 2030 Key Projects, New Generation Artificial Intelligence(grant numbers:2021ZD0111700); National Key Research and Development Program of China(grant numbers:2018AAA0101100); Science and Technology Major Project of Hubei Province Next-Generation AI Technologies(grant numbers:2019AEA170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002426","Deep learning;domain adaptation;semantic segmentation;adversarial learning","Training;Semantic segmentation;Generators;Adaptation models;Visualization;Generative adversarial networks;Semantics","","","","","","","IEEE","29 Dec 2022","","","IEEE","IEEE Early Access Articles"
"Integrated GANs: Semi-Supervised SAR Target Recognition","F. Gao; Q. Liu; J. Sun; A. Hussain; H. Zhou","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Cyber and Big Data Research Laboratory, Edinburgh Napier University, Edinburgh, U.K.; Department of Informatics, University of Leicester, Leicester, U.K.","IEEE Access","23 Aug 2019","2019","7","","113999","114013","With the advantage of working in all weathers and all days, synthetic aperture radar (SAR) imaging systems have a great application value. As an efficient image generation and recognition model, generative adversarial networks (GANs) have been applied to SAR image analysis and achieved promising performance. However, the cost of labeling a large number of SAR images limits the performance of the developed approaches and aggravates the mode collapsing problem. This paper presents a novel approach namely Integrated GANs (I-GAN), which consists of a conditional GANs, an unconditional GANs and a classifier, to achieve semi-supervised generation and recognition simultaneously. The unconditional GANs assist the conditional GANs to increase the diversity of the generated images. A co-training method for the conditional GANs and the classifier is proposed to enrich the training samples. Since our model is capable of representing training images with rich characteristics, the classifier can achieve better recognition accuracy. Experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset proves that our method achieves better results in accuracy when labeled samples are insufficient, compared against other state-of-the-art techniques.","2169-3536","","10.1109/ACCESS.2019.2935167","National Natural Science Foundation of China(grant numbers:61771027,61071139,61471019,61501011,61171122); Engineering and Physical Sciences Research Council(grant numbers:EP/M026981/1); Engineering and Physical Sciences Research Council(grant numbers:EP/N011074/1); Royal Society(grant numbers:NA160342); European Union’s Horizon 2020 Research and Innovation Program under the Marie–Sklodowska–Curie(grant numbers:720325); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798625","Synthetic aperture radar (SAR);generative adversarial networks (GANs);semi-supervised learning;generation;recognition","Radar polarimetry;Image recognition;Training;Generators;Gallium nitride;Synthetic aperture radar;Feature extraction","image recognition;image representation;image sampling;neural nets;object recognition;radar imaging;radar target recognition;synthetic aperture radar","Integrated GANs;semisupervised SAR Target Recognition;synthetic aperture radar imaging systems;recognition model;generative adversarial networks;SAR image analysis;image generation;moving and stationary target acquisition and recognition dataset;MSTAR dataset","","6","","50","CCBY","14 Aug 2019","","","IEEE","IEEE Journals"
"Automatic Registration of Optical and SAR Images Via Improved Phase Congruency Model","Y. Xiang; R. Tao; F. Wang; H. You; B. Han","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Oct 2020","2020","13","","5847","5861","In this article, we propose an automatic and efficient method to solve optical and synthetic aperture radar (SAR) image registration using the improved phase congruency (PC) model. First, evenly distributed keypoints are extracted from the optical images via the block-Harris method. Complementary grid points are then selected in image regions with poor structural information and supplemented to the keypoint set. For each keypoint, a robust feature representation that captures the local spatial relationship is proposed based on the improved PC model. Specifically, we propose to use two different PC models, the classic PC and the SAR-PC, to construct features for optical and SAR images, respectively. The PC features of several directions are aggregated to construct the feature descriptors, and a similarity metric via the phase correlation of feature descriptors is obtained. The proposed similarity metric cannot only find accurate correspondence but also present efficient results without presetting the size of the search region. We compare the proposed method with two baselines and state-of-the-art (SOTA) methods, i.e., OS-SIFT, histogram of oriented PC, and channel features of oriented gradients, in various scenarios. The results show that the proposed method outperforms the baselines and shows comparable performance with SOTA methods in regions with abundant structural information and better performance in regions with less structural information. Moreover, we build a high-resolution optical and SAR image matching dataset, which consists of 10 692 nonoverlapping patch pairs of $256\times 256$ pixels and 1-m resolution. Results of two benchmarks, Siamese deep matching network, and conditional generative adversarial networks show that this dataset is practical and challenging.","2151-1535","","10.1109/JSTARS.2020.3026162","National Natural Science Foundation of China(grant numbers:61901439); Key Research Program of Frontier Sciences; Chinese Academy of Science(grant numbers:ZDBS-LY-JSC036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204802","Deep learning;high-resolution dataset;image registration;optical and SAR;phase congruency (PC)","Optical imaging;Optical sensors;Synthetic aperture radar;Adaptive optics;Image registration;Feature extraction;Remote sensing","feature extraction;image classification;image matching;image registration;image resolution;neural nets;optical images;radar imaging;synthetic aperture radar","automatic method;optical images;block-Harris method;complementary grid points;image regions;structural information;robust feature representation;local spatial relationship;SAR-PC;feature descriptors;similarity metric;phase correlation;channel features;SOTA methods;phase congruency model;high-resolution optical image;SAR image matching dataset;generative adversarial networks;Siamese deep matching network;synthetic aperture radar;SAR image registration;optical image registration","","20","","52","CCBY","23 Sep 2020","","","IEEE","IEEE Journals"
"ISAR Target Recognition Using Pix2pix Network Derived from cGAN","G. Li; Z. Sun; Y. Zhang","Electrical and Information Engineering, Harbin Institute of Technology, Harbin, China; Electrical and Information Engineering, Harbin Institute of Technology, Harbin, China; Electrical and Information Engineering, Harbin Institute of Technology, Harbin, China","2019 International Radar Conference (RADAR)","27 Apr 2020","2019","","","1","4","Inverse Synthetic Aperture Radar (ISAR) image processing has received much interest in recent years, due to its effectiveness in remote sensing and military use. Although ISAR can achieve all-time all-weather target detection, the quality of images is unstable due to many factors such as sea clutter, which will interfere with target recognition. Since, for sea objects, strong correlation exists between ISAR data and optical camera data, target information extraction accuracy and reliability can be improved by jointly processing the two types of data. In this paper, the pix2pix network derived from the conditional generative adversarial network (cGAN) is used to realize the translation of the ISAR images to the corresponding optical images. In order to remove the influence of lighting conditions on the color of the optical images, we use the grayscale images instead. We combine the generated and the ISAR images to train the CNN network for recognition. Experimental results demonstrate that the proposed method can effectively improve the recognition rate of target recognition based on the ISAR images.","2640-7736","978-1-7281-2660-9","10.1109/RADAR41533.2019.171345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9078939","ISAR target recognition;cGAN;pix2pix network","Optical imaging;Generators;Marine vehicles;Image recognition;Optical sensors;Target recognition;Generative adversarial networks","neural nets;object detection;optical images;radar computing;radar imaging;radar target recognition;synthetic aperture radar","ISAR target recognition;pix2pix network derived;all-weather target detection;ISAR data;optical camera data;target information extraction accuracy;reliability;conditional generative adversarial network;ISAR images;corresponding optical images;grayscale images;CNN network;inverse synthetic aperture radar image processing","","2","","17","IEEE","27 Apr 2020","","","IEEE","IEEE Conferences"
"Translation of Aerial Image Into Digital Map via Discriminative Segmentation and Creative Generation","Y. Fu; S. Liang; D. Chen; Z. Chen","Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Beijing Laboratory of Intelligent Information Technology, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Microsoft Research Laboratory, Redmond, WA, USA; School of Geography and Information Engineering, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","2 Feb 2022","2022","60","","1","15","Automatic translation of aerial images into digital maps is an important and challenging task which is widely used in practical applications. Most of the existing works view it either as a creative image-to-image translation problem or a discriminative semantic segmentation problem. However, we notice that human annotators need to extract and understand the information in aerial images first and then translate them to online maps in a creative way, which helps them draw accurate and visually appealing online maps. In this article, we propose an end-to-end online map generation method that combines a discriminative module with a creative module based on this observation to mimic human behavior. Specifically, we first utilize a semantic segmentation module to obtain a rough aerial map, in which each region is labeled with its category, and then further improve its quality with a creative module. To train a robust network that generalizes well to unfamiliar regions, we also collect a large aerial image dataset for online map generation (AIDOMG). AIDOMG consists of 40 087 pairs of aerial images and corresponding online maps collected from nine regions of six continents. We conduct extensive experiments to verify the superiority of the new design that combines discrimination and creativity and experimental results show that the performance of the proposed method significantly outperforms baseline methods.","1558-0644","","10.1109/TGRS.2021.3110894","National Natural Science Foundation of China(grant numbers:62171038,61827901,41871305,62088101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540226","Aerial image;generative adversarial networks (GAN);map generation;semantic segmentation","Semantics;Image segmentation;Data mining;Task analysis;Visualization;Roads;Pipelines","image colour analysis;image segmentation","creative module;semantic segmentation module;rough aerial map;aerial image dataset;aerial images;discriminative segmentation;creative generation;automatic translation;digital maps;creative image-to-image translation problem;discriminative semantic segmentation problem;end-to-end online map generation method;discriminative module;AIDOMG","","2","","68","IEEE","16 Sep 2021","","","IEEE","IEEE Journals"
"Generative Adversarial Capsule Network With ConvLSTM for Hyperspectral Image Classification","W. -Y. Wang; H. -C. Li; Y. -J. Deng; L. -Y. Shao; X. -Q. Lu; Q. Du","Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen, China; Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China.; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Geoscience and Remote Sensing Letters","24 Feb 2021","2021","18","3","523","527","Recently, deep learning has been widely applied in hyperspectral image (HSI) classification since it can extract high-level spatial–spectral features. However, deep learning methods are restricted due to the lack of sufficient annotated samples. To address this problem, this letter proposes a novel generative adversarial network (GAN) for HSI classification that can generate artificial samples for data augmentation to improve the HSI classification performance with few training samples. In the proposed network, a new discriminator is designed by exploiting capsule network (CapsNet) and convolutional long short-term memory (ConvLSTM), which extracts the low-level features and combines them together with local space sequence information to form the high-level contextual features. In addition, a structured sparse  $L_{2,1}$  constraint is imposed on sample generation to control the modes of data being generated and achieve more stable training. The experimental results on two real HSI data sets show that the proposed method can obtain better classification performance than the several state-of-the-art deep classification methods.","1558-0571","","10.1109/LGRS.2020.2976482","National Natural Science Foundation of China(grant numbers:61871335); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9032346","Capsule network (CapsNet);convolutional neural network (CNN);data augmentation;deep learning;generative adversarial network (GAN);hyperspectral image (HSI) classification","Feature extraction;Training;Gallium nitride;Generative adversarial networks;Hyperspectral imaging;Generators;Machine learning","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;learning (artificial intelligence);recurrent neural nets","sample generation;HSI data sets;state-of-the-art deep classification methods;generative adversarial capsule network;ConvLSTM;hyperspectral image classification;high-level spatial-spectral features;deep learning methods;sufficient annotated samples;artificial samples;data augmentation;HSI classification performance;training samples;short-term memory;low-level features;local space sequence information;high-level contextual features","","16","","17","IEEE","11 Mar 2020","","","IEEE","IEEE Journals"
"Unsupervised Pixel-Wise Hyperspectral Anomaly Detection via Autoencoding Adversarial Networks","S. Arisoy; N. M. Nasrabadi; K. Kayabol","Electronics Engineering Department, Gebze Technical University, Gebze, Turkey; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Electronics Engineering Department, Gebze Technical University, Gebze, Turkey","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","We propose a completely unsupervised pixel-wise anomaly detection (AD) method for hyperspectral images (HSIs). The proposed method consists of three steps called data preparation, reconstruction, and detection. In the data preparation step, we apply a background purification to train the deep network in an unsupervised manner. In the reconstruction step, we propose to use three different deep autoencoding adversarial network (AEAN) models including 1-D-AEAN, 2-D-AEAN, and 3-D-AEAN which are developed for working on spectral, spatial, and joint spectral–spatial domains, respectively. The goal of the AEAN models is to generate synthesized HSIs which are close to real ones. A reconstruction error map (REM) is calculated between the original and the synthesized image pixels. In the detection step, we propose to use a weighted RX (WRX) -based detector in which the pixel weights are obtained according to REM. We compare our proposed method with the classical Reed–Xiaoli (RX), WRX, support vector data description (SVDD)-based, collaborative representation-based detector (CRD), adaptive weight deep belief network (AW-DBN) detector, and deep autoencoder AD (DAEAD) method on real hyperspectral data sets. The experimental results show that the proposed approach outperforms other detectors in the benchmark.","1558-0571","","10.1109/LGRS.2021.3049711","Scientific and Technological Research Council of Turkey (TUBITAK)(grant numbers:118E295); Council of Higher Education (YOK) of Turkey through the Research Abroad Scholarship (YUDAB) Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328854","Adversarial learning;anomaly detection (AD);autoencoder;deep learning;hyperspectral image (HSI)","Image reconstruction;Detectors;Training;Hyperspectral imaging;Generative adversarial networks;Gallium nitride;Data models","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image reconstruction;object detection;unsupervised learning","unsupervised pixel-wise hyperspectral anomaly detection;autoencoding adversarial networks;hyperspectral images;data preparation;background purification;deep network training;1D-AEAN;2D-AEAN;3D-AEAN;joint spectral-spatial domains;reconstruction error map;REM;synthesized image pixels;pixel weights;hyperspectral data sets;deep autoencoding adversarial network;AEAN;HSIs;data reconstruction;weighted RX;data detection","","14","","21","IEEE","20 Jan 2021","","","IEEE","IEEE Journals"
"An Optimized Training Method for GAN-Based Hyperspectral Image Classification","F. Zhang; J. Bai; J. Zhang; Z. Xiao; C. Pei","School of Telecommunications Engineering, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; School of Telecommunications Engineering, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","27 Sep 2021","2021","18","10","1791","1795","This letter explores how to apply a generative adversarial network (GAN) to the classification of hyperspectral images (HSIs) to obtain a smooth training process and better classification results. To this end, the ideas of the progressive growing GAN (PG-GAN) and Wasserstein generative adversarial network gradient penalty (WGAN-GP) are combined to propose a new method for HSI classification. PG-GAN is optimized from the training process of generating adversarial networks. It gradually increases the depth of the network and the size of the input image, making the training smoother. WGAN-GP is optimized in terms of the loss function. The gradient penalty method is used to solve the problems of vanishing gradient and exploding gradient, making the training more stable. Based on the combination of the two methods, a classifier is added to the model so that it can complete the HSI classification task. The proposed method is evaluated over two publicly available hyperspectral data sets, the Indian Pines and University of Pavia data sets. The results show that the proposed method can achieve good training results with only a small amount of labeled training data.","1558-0571","","10.1109/LGRS.2020.3009017","National Natural Science Foundation of China(grant numbers:61772401); Open Research Fund of Key Laboratory of Spectral Imaging Technology, Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146557","Generative adversarial network (GAN);hyperspectral image (HSI) classification;semisupervised learning","Training;Gallium nitride;Generative adversarial networks;Hyperspectral imaging;Task analysis;Generators;Image resolution","geophysical image processing;geophysics computing;hyperspectral imaging;image classification;optimisation","WGAN-GP;PG-GAN;gradient penalty method;HSI classification task;optimized training method;hyperspectral image classification;smooth training process;GAN;Wasserstein generative adversarial network;Indian Pines;University of Pavia data sets","","11","","20","IEEE","23 Jul 2020","","","IEEE","IEEE Journals"
"An Open Set Recognition Method for SAR Targets Based on Multitask Learning","X. Ma; K. Ji; L. Zhang; S. Feng; B. Xiong; G. Kuang","State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, College of Electronic Science, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, College of Electronic Science, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, College of Electronic Science, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, College of Electronic Science, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, College of Electronic Science, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, College of Electronic Science, National University of Defense Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Most of the existing synthetic aperture radar (SAR) automatic target recognition (ATR) methods aim at the closed set situation, in which the classes of targets in the test set have appeared in the training set. However, in practice, the classifier is likely to encounter the targets from unseen categories and classify them incorrectly, which brings a huge challenge to current SAR ATR techniques. To overcome this problem, this letter proposes an open set recognition (OSR) method based on multitask learning, and the method is developed from generative adversarial network (GAN). Essentially, this method decomposes OSR into two tasks: classification and abnormal detection. The classification task is the same as that in the closed set situation, while the abnormal detection task is used to determine whether the targets belongs to the unseen categories. Correspondingly, the network structure of GAN is modified and the other full-connection network branch is added to the end of the discriminator, so it has the ability to accomplish the above two tasks. Finally, according to the results of two tasks, the OSR for SAR targets can be realized. The experimental results on moving and stationary target acquisition (MSTAR) dataset demonstrate that the proposed method has the better recall, precision,  $F1$ , and accuracy than other OSR methods.","1558-0571","","10.1109/LGRS.2021.3079418","National Natural Science Foundation of China(grant numbers:62001480); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440677","Generative adversarial network (GAN);multitask learning;open set recognition (OSR);synthetic aperture radar (SAR)","Synthetic aperture radar;Generators;Task analysis;Generative adversarial networks;Training;Target recognition;Standards","image classification;learning (artificial intelligence);neural nets;object recognition;radar imaging;radar target recognition;synthetic aperture radar","SAR targets;OSR methods;SAR ATR techniques;generative adversarial network;classification task;abnormal detection task;synthetic aperture radar automatic target recognition methods;moving and stationary target acquisition dataset;MSTAR dataset;OSR method;open set recognition method based on multitask learning","","10","","19","IEEE","25 May 2021","","","IEEE","IEEE Journals"
"An Energy-Based Generative Adversarial Forecaster for Radar Echo Map Extrapolation","P. Xie; X. Li; X. Ji; X. Chen; Y. Chen; J. Liu; Y. Ye","Department of Computer Science, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, Harbin Institute of Technology, Shenzhen, China; Shenzhen Key Laboratory of Severe Weather in South China, Shenzhen, China; Shenzhen Key Laboratory of Severe Weather in South China, Shenzhen, China; Shenzhen Key Laboratory of Severe Weather in South China, Shenzhen, China; Department of Computer Science, Harbin Institute of Technology, Shenzhen, China","IEEE Geoscience and Remote Sensing Letters","15 Dec 2021","2022","19","","1","5","Precipitation nowcasting is an important task in weather forecast. The key challenge of the task lies at radar echo map extrapolation. Recent studies show that a convolutional recurrent neural network (ConvRNN) is a promising direction to solve the problem. However, the extrapolation results of the existing ConvRNN methods tend to be blurring and unrealistic. Recent studies show that generative adversarial network (GAN) is a promising tool to address the drawback, while it suffers from the instability for training. In this letter, we build a novel ConvRNN model based on the energy-based GAN for radar echo map extrapolation. The method can alleviate the blurring and unrealistic issues and is more stable. We have conducted experiments on a real-world data set, and the results show that the proposed method outperforms several existing models, including optical flow, convolution gated recurrent unit (ConvGRU), and generative adversarial ConvGRU (GA-ConvGRU).","1558-0571","","10.1109/LGRS.2020.3023950","Shenzhen Science and Technology Program(grant numbers:JCYJ20170811160212033,JCYJ20180507183823045); Science and Technology Planning Project of Guangdong Province(grant numbers:2019B020208016,2016A020223016,2014A020218014); Guangdong Meteorological Bureau Science and Technology Project(grant numbers:GRMC2018Z06); China Meteorological Administration Forecaster Special Research Project(grant numbers:CMAYBY2019081); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204814","Deep learning;image sequence prediction;nowcasting;radar echo extrapolation","Generators;Extrapolation;Convolution;Gallium nitride;Generative adversarial networks;Radar imaging","","","","9","","19","IEEE","23 Sep 2020","","","IEEE","IEEE Journals"
"A High-Quality Multicategory SAR Images Generation Method With Multiconstraint GAN for ATR","S. Du; J. Hong; Y. Wang; Y. Qi","School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","20 Dec 2021","2022","19","","1","5","The high-quality training data sets are often insufficient in synthetic aperture radar (SAR) automatic target recognition (ATR) applications. The generative adversarial network (GAN) provides a way for SAR data augmentation. It is necessary to ensure the diversity, similarity, and correct category of the generated images so that these images can be served as the supplementary data set. In this letter, the multiconstraint GAN (MCGAN) is proposed to generate high-quality multicategory SAR images. First, an encoder is used to learn the features of the real images to enhance the similarity. Then, the encoded features are mixed with noise and category labels as the input of the generator to improve the diversity and category correctness. The generated images will be sent to a pretrained classifier to ensure the correct category. Finally, the improved Wasserstein loss with the gradient penalty is extended to the model to further improve the diversity and similarity of the generated images. The MSTAR data set is used to validate the proposed method on generation. The quality evaluation and classification tests are performed on the generated images, and the results show that the MCGAN can provide high-quality images, which could assist in achieving good classification accuracy.","1558-0571","","10.1109/LGRS.2021.3065682","National Natural Science Foundation of China(grant numbers:Y7J3210371); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387399","Automatic target recognition (ATR);generative adversarial network (GAN);synthetic aperture radar (SAR) image generation","Generators;Gallium nitride;Training;Generative adversarial networks;Synthetic aperture radar;Radar polarimetry;Target recognition","image classification;image coding;image enhancement;neural nets;object recognition;radar computing;radar imaging;synthetic aperture radar","multiconstraint GAN;MSTAR data set;ATR;high-quality training data sets;synthetic aperture radar automatic target recognition applications;generative adversarial network;SAR data augmentation;high-quality multicategory SAR image generation method;MCGAN;image classification;improved Wasserstein loss","","5","","17","IEEE","26 Mar 2021","","","IEEE","IEEE Journals"
"Seismic Impedance Inversion Using Conditional Generative Adversarial Network","D. Meng; B. Wu; Z. Wang; Z. Zhu","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory of Shale Oil and Gas Enrichment Mechanisms and Effective Development and Sinopec Key Laboratory of Seismic Elastic Wave Technology, SINOPEC Petroleum Exploration and Production Research Institute, Beijing, China and; State Key Laboratory of Shale Oil and Gas Enrichment Mechanisms and Effective Development and Sinopec Key Laboratory of Seismic Elastic Wave Technology, SINOPEC Petroleum Exploration and Production Research Institute, Beijing, China and; Hainan Institute of Zhejiang University, Sanya, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Deep-learning methods, such as convolutional neural networks (CNNs), have been successfully applied to seismic impedance inversion in recent years. Compared with traditional geophysical inversion, deep-learning inversion can give inversion results with higher resolution. In this letter, we further improve the performance of deep-learning inversion and propose a seismic impedance inversion method based on conditional generative adversarial network (cGAN). In the proposed method, a generator learns to predict seismic impedance from seismic data, and a discriminator learns to distinguish between fake and real impedance. We mix the cGAN objective with mean square error (MSE) loss to bring in more information for model training. Besides, a CNN-based seismic forward model is trained to introduce the constraint of unlabeled data in the training of cGAN. Tests on Marmousi2 model and overthrust model show that the proposed method can obtain more accurate impedance and have better robustness against random noise than CNN method.","1558-0571","","10.1109/LGRS.2021.3090108","Natural Science Basic Research Program of Shaanxi(grant numbers:2020JM-18); National Natural Science Foundation of China(grant numbers:41974137,41974122); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9485121","Conditional generative adversarial network (cGAN);convolutional neural network (CNN);deep learning;seismic impedance inversion","Impedance;Generators;Training;Data models;Generative adversarial networks;Linear programming;Convolution","geophysical signal processing;seismic waves;seismology","traditional geophysical inversion;deep-learning inversion;seismic impedance inversion method;conditional generative adversarial network;seismic data;fake impedance;real impedance;accurate impedance;deep-learning methods;convolutional neural networks;mean square error;Marmousi2 model;overthrust model","","4","","27","IEEE","15 Jul 2021","","","IEEE","IEEE Journals"
"Physical-Related Feature Extraction From Simulated SAR Image Based on the Adversarial Encoding Network for Data Augmentation","S. Du; J. Hong; Y. Wang; K. Xing; T. Qiu","School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","The synthetic aperture radar automatic target recognition (SAR ATR) application based on the deep convolutional network often faces data scarcity. SAR image simulation based on electromagnetic and geometric calculations can provide a large amount of data that contains interpretable physical features, such as shadow and contour. However, there is a big difference between the simulated SAR image and the real image, and it is difficult to directly use it for data augmentation. This letter proposes the adversarial encoding network to extract the physical-related features, which can be understood as the common features between the simulated and real data. By designing the adversarial learning between an encoder and a discriminator, the encoder can extract real features from the simulated images. The encoded features are sent to a classifier to ensure the correct category information. A decoder is used to reconstruct the encoded features into the input image so that the encoded feature retains the image information as much as possible. Ablation experiments and comparative experiments are used to verify the ability of each module and the performance of the proposed method. The results show that the proposed model can achieve 98.55% accuracy, especially when the real data are insufficient for classification, which verifies that the proposed method is effective for data augmentation.","1558-0571","","10.1109/LGRS.2021.3100642","National Science of China(grant numbers:61771453); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9510178","Automatic target recognition (ATR);data augmentation;generative adversarial network (GAN)","Feature extraction;Data models;Solid modeling;Training;Radar polarimetry;Decoding;Generative adversarial networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image coding;image reconstruction;radar computing;radar imaging;synthetic aperture radar","adversarial learning;geometric calculations;electromagnetic calculations;SAR image simulation;data scarcity;deep convolutional network;SAR ATR;synthetic aperture radar automatic target recognition application;adversarial encoding network;simulated SAR image;physical-related feature extraction;data augmentation;image information;encoded feature","","2","","18","IEEE","10 Aug 2021","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification With Adversarial Attack","C. Shi; Y. Dang; L. Fang; Z. Lv; M. Zhao","School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China; Chinese Academy of Sciences, Quanzhou Institute of Equipment Manufacturing, Haixi Institute, Quanzhou, China; School of Computer Science and Engineering, Xi’an University of Technology, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","The performance of a neural network is highly dependent on the labeled samples. However, the labeled samples are primarily clean, which prevents the network from capturing the features of the samples near the decision boundary. For hyperspectral images (HSIs), high spectral dimensions and same-spectra foreign matter lead to more boundary samples in the data. In this letter, we investigate an adversarial attack algorithm against these problems for HSIs. A modified DeepFool algorithm is implemented to generate boundary adversarial samples with minimal disturbance, and the generated boundary adversarial samples are simply added to the training set to improve the accuracy of the boundary samples in the data. Furthermore, we iteratively complete network training and boundary adversarial sample generation so that the decision boundary can be adjusted according to the real-time classification situation. Extensive experiments are carried out on the two HSI datasets, and the results demonstrate that the modified DeepFool algorithm can improve the accuracy of the decision boundary. Our findings also show that adversarial attacks are sensitive to high-dimensional and multiple-category data and are worthy of further study.","1558-0571","","10.1109/LGRS.2021.3122170","National Natural Science Foundation of China(grant numbers:61902313,61973250,42101359); Natural Science Foundation of Shaan Xi Province(grant numbers:2018JQ4009); Key Laboratory Foundation of Shaanxi Education Department(grant numbers:20JS086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585480","Adversarial attack;DeepFool;hyperspectral image (HSI) classification","Training;Generative adversarial networks;Hyperspectral imaging;Probability distribution;Prediction algorithms;Task analysis;Robustness","hyperspectral imaging;image classification;learning (artificial intelligence)","adversarial attack algorithm;HSIs;modified DeepFool algorithm;generated boundary adversarial samples;boundary adversarial sample generation;decision boundary;real-time classification situation;hyperspectral image classification;neural network;labeled samples","","2","","18","IEEE","26 Oct 2021","","","IEEE","IEEE Journals"
"Relative Attributes-Based Generative Adversarial Network for Desert Seismic Noise Suppression","H. Ma; Y. Sun; N. Wu; Y. Li","Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","Since seismic data will be interfered with by a host of complicated noise during the acquisition process, the quality of the acquired seismic data is usually poor. The overlap of signals and noise makes it difficult to extract effective signals from desert seismic records. Therefore, the suppression of seismic noise and the retention of seismic signals are key issues in seismic signal processing. In order to improve the quality of the data obtained, we propose an unsupervised relative attributes-based generative adversarial network (RAGAN), which includes a generator, a discriminator, and an attribute match-aware discriminator. By encoding the data of different attributes in seismic records, the denoising task can be regarded as the conversion process of the data corresponding to the attributes. The relative attributes obtained by the difference between the target attribute and the original attribute are used to control the attributes of the data generated by the generator, so as to achieve the purpose of noise suppression. Experimental results of both synthetic and field seismic records show that the proposed method performs better than part of conventional methods.","1558-0571","","10.1109/LGRS.2021.3135034","National Natural Science Foundation of China(grant numbers:42174153); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648530","Attribute training set;relative attributes-based denoising;seismic exploration;seismic noise suppression","Generators;Noise reduction;Training;Noise measurement;Generative adversarial networks;Transforms;Convolution","geophysical signal processing;geophysical techniques;neural nets;seismology;signal denoising;unsupervised learning","desert seismic noise suppression;complicated noise;acquisition process;acquired seismic data;desert seismic records;seismic signals;seismic signal processing;unsupervised relative attributes-based generative adversarial network;attribute match-aware discriminator;conversion process;synthetic field seismic records","","1","","15","IEEE","14 Dec 2021","","","IEEE","IEEE Journals"
"Human Motion Recognition Using Doppler Radar Based on Semi-Supervised Learning","Y. Ding; B. Jin; J. Zhang; R. Liu; Y. Zhang","School of Physics and Electronics, Central South University, Changsha, China; School of Physics and Electronics, Central South University, Changsha, China; School of Physics and Electronics, Central South University, Changsha, China; School of Physics and Electronics, Central South University, Changsha, China; School of Physics and Electronics, Central South University, Changsha, China","IEEE Geoscience and Remote Sensing Letters","19 May 2022","2022","19","","1","5","Fully supervised deep learning has achieved great success in many fields, but its performance is often hindered by the abundance and quantity of available labeled training data. In the field of radar-based human motion recognition (HMR), obtaining sufficient training data is really a challenge due to the scarcity of labeled data, which causes deep learning methods to fall into an overfitting state easily. To overcome this limitation, we propose a generative adversarial network (GAN)-based semi-supervised learning model for radar-based human motion classification, which can leverage a large amount of unlabeled data to enhance classification performance. In addition, according to the characteristics of multiclassification tasks, we improve the loss function of GAN, leading the model to utilize unlabeled data more effectively. We did comparative experiments on human motion radar data measured by the Doppler radar, and the experimental results show that the proposed model has significant advantages in classification accuracy compared with the other models.","1558-0571","","10.1109/LGRS.2022.3173951","Fundamental Research Funds for the Central Universities of Central South University(grant numbers:2021zzts0499); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9771432","Generative adversarial network (GAN);human motion classification;micro-Doppler signal classification;semi-supervised learning","Generative adversarial networks;Data models;Radar;Generators;Task analysis;Semisupervised learning;Deep learning","Doppler radar;image classification;learning (artificial intelligence);pattern classification","Doppler radar;supervised deep learning;radar-based human motion recognition;deep learning methods;generative adversarial network-based semisupervised learning model;radar-based human motion classification;unlabeled data;classification performance;human motion radar data","","","","16","IEEE","10 May 2022","","","IEEE","IEEE Journals"
"Seismic Data Reconstruction via Recurrent Residual Multiscale Inference","A. Song; C. Wang; C. Zhang; J. Zhang; D. Xiong","School of Science, Chang’an University, Xi’an, China; School of Science, Chang’an University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Geophysical Technology Research and Development Center, BGP, Hebei, China","IEEE Geoscience and Remote Sensing Letters","21 Sep 2022","2022","19","","1","5","Seismic data reconstruction is an important technology in the seismic data processing. Existing reconstruction methods have achieved promising performance for regularly/randomly missing cases. However, recovering consecutive missing data remains challenging due to the loss of large amounts of information in local regions. In this letter, we devise a novel network called recurrent residual multiscale feature inference network (RRMFI-Net), which is mainly constructed by a recurrent residual multiscale feature inference (RRMFI) module and a recurrence adjustment attention (RAA) module. The RRMFI module infers and fills the missing regions multiple times and uses the result as a clue for the next inference, which makes the result more elegant. To ensure that there is no ambiguity between the results of multiple inferences, we devise an RRA module, which is fused into the RRMFI module to obtain padding information from a long distance. Experimentally, we compare RRMFI-Net with supervised state-of-the-art methods, demonstrating that RRMFI-Net is more effective on multiple indicators. Furthermore, we conduct ablation studies discussing the impact of key network hyperparameters.","1558-0571","","10.1109/LGRS.2022.3204826","National Natural Science Foundation of China(grant numbers:12001057,61976174); Fundamental Research Funds for the Central Universities in Chang’an University(grant numbers:300102122101); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ-625); Key Research and Development of Shaanxi Province of China(grant numbers:2021NY-170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893067","Recurrence adjustment attention (RAA);recurrent residual multiscale inference;seismic data reconstruction","Convolution;Logic gates;Training;Three-dimensional displays;Inference algorithms;Generative adversarial networks;Neural networks","Bayes methods;geophysical signal processing;geophysical techniques;image reconstruction;iterative methods;medical image processing;seismology","reconstruction methods;seismic data processing;recurrent residual multiscale inference;seismic data reconstruction;RRA module;multiple inferences;missing regions multiple times;RRMFI module infers;recurrence adjustment attention module;recurrent residual multiscale feature inference module;RRMFI-Net;recurrent residual multiscale feature inference network;consecutive missing data","","","","16","IEEE","14 Sep 2022","","","IEEE","IEEE Journals"
"Multiscale Contrastive Learning Networks for Automatic Denoising of Geological Sedimentary Model Images","C. Wu; H. Zhang; B. Wang; L. Zhang; L. Wang; F. Hu","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; PetroChina Tarim Oilfield Company, Korla, China; School of Geosciences, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China","IEEE Transactions on Geoscience and Remote Sensing","27 Oct 2022","2022","60","","1","12","The well-described river courses in geological sedimentary models are essential for identifying oil and gas reservoirs. However, numerous noises are generated around the river course, including irregular noises and regions due to errors in the geological data, as well as traces left over from the printed model. The cluttered distributions of noises make the complete river course unobtainable, which interferes with reservoir prediction. To the best of our knowledge, deep-learning-based methods for automatic noise detection and removal have not been explored in the field of processing geological sedimentary model images. In this article, we present multiscale contrastive learning generative adversarial networks (CLGANs) for detecting noise in geological sedimentary model images. A multiscale contrastive training strategy is proposed to capture noise locations and river discontinuities by comparing the distributions with and without noise in image space and feature space. A paired dataset that consists of images with noise and images without noise is constructed for contrastive training. Moreover, cyclic denoising is proposed to completely denoise by constantly modifying the pixel values at the noise, which prevents missed detections by calling the model multiple times. Subsequently, pixel-level denoising is designed to initially remove a portion of the noise by contouring nonriver regions, which reduces the pressure on the cyclic denoising method. The detection results demonstrate the effectiveness of CLGAN. The denoising results with excellent river connectivity and integrity highlight the superiority of the proposed denoising methods.","1558-0644","","10.1109/TGRS.2022.3214195","Major Scientific and Technological Projects of China National Petroleum Corporation(grant numbers:ZD2019-183-001); Fundamental Research Funds for the Central Universities(grant numbers:20CX05018A); Natural Science Foundation of Shandong Province(grant numbers:ZR2020MF136); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918043","Cyclic denoising;multiscale contrastive training;paired dataset;pixel-level denoising","Noise reduction;Geology;Training;Rivers;Adaptation models;Data models;Computational modeling","hydrocarbon reservoirs;image denoising;learning (artificial intelligence);rivers;wavelet transforms","geological sedimentary models;numerous noises;irregular noises;geological data;printed model;complete river course unobtainable;deep-learning-based methods;automatic noise detection;geological sedimentary model images;multiscale contrastive learning generative adversarial networks;multiscale contrastive training strategy;noise locations;model multiple times;multiscale contrastive learning networks;well-described river courses","","","","66","IEEE","13 Oct 2022","","","IEEE","IEEE Journals"
"Road Detection Network Based on Anti-Disturbance and Variable-Scale Spatial Context Detector","Q. Ding; H. Liu; H. Luo; X. Chen","School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China","IEEE Access","23 Aug 2021","2021","9","","114640","114648","Road detection plays a critical role in the application of smart transportation. The performance of the mainstream methods such as PSPNet (Pyramid Scene Parsing Network), DeepLab V3, FCRN (Fully Convolutional Residual Networks) still suffers from uncertain disturbances of surface abrasion buildings, pedestrians, and variation of illumination like tree-shadow. The extracted features are vulnerable to extra-disturbance, and non-local spatial-context information has not been fully utilized. In this paper, a detector based on anti-disturbance and variable-scale spatial context features (AVD) is proposed: the training of the multi-layer features of the detector is always taken under the imposing of fake-feature-disturbance from an independent generator, which is trained to exacerbate the detector errors and the mistakes of feature discriminator. The detector is prepared to be immune from the fake-feature-disturbance, and the discriminator is trained to distinguish the differences between the non-interference features and disturbing features. We also designed a novel variable-scale spatial context module to enhance the richness performance of the extracted features. And a soft connection link is bridged between the low and high feature layers. The detection experiments on the Munich road dataset and urban road dataset show that AVD is better than all the mainstream above methods. Our method increases the accuracy by 3% on the Munich remote sensing dataset and 0.4% on the urban road dataset. Our code and datasets are available at https://github.com/Ding-Q/AVD for download.","2169-3536","","10.1109/ACCESS.2021.3105190","National Natural Science Foundation of China(grant numbers:62061002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9514847","Object detection;convolutional neural network;anti-disturbance;generative adversarial network;road detection;remote sensing road detection","Feature extraction;Roads;Detectors;Generators;Decoding;Semantics;Generative adversarial networks","","","","2","","18","CCBYNCND","16 Aug 2021","","","IEEE","IEEE Journals"
"Atrous cGAN for SAR to Optical Image Translation","J. Noa Turnes; J. D. B. Castro; D. L. Torres; P. J. S. Vega; R. Q. Feitosa; P. N. Happ","Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","17 Dec 2021","2022","19","","1","5","Conditional (cGAN)-based methods proposed so far for synthetic aperture radar (SAR)-to-optical image synthesis tend to produce noisy and unsharp optical outcomes. In this work, we propose the atrous-cGAN, a novel cGAN architecture that improves the SAR-to-optical image translation. The proposed generator and discriminator networks rely on atrous convolutions and incorporate an atrous spatial pyramid pooling (ASPP) module to enhance fine details in the generated optical image by exploiting spatial context at multiple scales. This letter reports experiments carried out to assess the performance of atrous-cGAN for the synthesis of Landsat-8 images from Sentinel-1A data based on three public data sets. The experimental analysis indicated that the atrous-cGAN consistently outperformed the classical pix2pix counterpart in terms of visual quality, similar to the true optical image, and as a feature learning tool for semantic segmentation.","1558-0571","","10.1109/LGRS.2020.3031199","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES); Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq); NVIDIA Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241239","Atrous spatial pyramid pooling (ASPP);generative adversarial networks;synthetic aperture radar (SAR)-optical synthesis","Optical imaging;Optical sensors;Generators;Synthetic aperture radar;Convolutional codes;Optical interferometry;Artificial satellites","geophysical image processing;image classification;image enhancement;image segmentation;learning (artificial intelligence);object detection;optical images;radar imaging;synthetic aperture radar","incorporate;atrous spatial pyramid pooling module;generated optical image;Landsat-8 images;atrous-cGAN;atrous cGAN;conditional-based methods;radar-to-optical image synthesis;noisy outcomes;unsharp optical outcomes;SAR-to-optical image translation;generator;discriminator networks;atrous convolutions;current 1.0 A","","10","","18","IEEE","27 Oct 2020","","","IEEE","IEEE Journals"
"Conditional Prior Probabilistic Generative Model With Similarity Measurement for ISAR Imaging","C. Du; P. Xie; L. Zhang; Y. Ma; L. Tian","School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, China; School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, China; School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, China; Beijing Institute of Tracking Telemenntry and Telecommunication, Beijing, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","The higher bandwidth inverse synthetic aperture radar (ISAR) can obtain the higher resolution radar images, which can provide more target information and help improve radar target detection and recognition. It is essential to study how to achieve a precise high-resolution (HR) ISAR image utilizing limited measurement echoes. The existing neural-network-based ISAR imaging methods extract features only from limited measurement echoes, and the common features in HR ISAR images are not utilized sufficiently, which limits the imaging performance improvement. Moreover, in their loss functions, there are no explicit constraints on the correct recovery of strong scattering points, which are important in reflecting the target characteristics. In this letter, we propose a conditional probabilistic generative model to achieve the HR ISAR imaging. By optimizing the well-designed Kullback–Leibler (KL) divergence between conditional prior and approximate posterior probability distribution in the loss function, the common features contained in training HR radar images can be learned, and a suitable prior probability distribution for the latent variable can be obtained. To accurately recover the positions and relative amplitudes of strong scattering points, we blend a similarity measurement that is sensitive to the large values’ locations in a vector with the adversarial loss. Both visual and numerical results of extensive experiments prove that the proposed model can obtain enhanced effectiveness and efficiency compared with some counterparts.","1558-0571","","10.1109/LGRS.2021.3073691","National Nature Sciences Foundation of China(grant numbers:61771372); Shenzhen Science and Technology Program(grant numbers:KQTD20190929172704911); Open fund of Science and Technology on Electromagnetic Scattering Key Laboratory(grant numbers:61424090112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425510","Conditional prior;generative adversarial networks (GANs);high-resolution (HR) inverse synthetic aperture radar (ISAR) imaging;probabilistic generative model;similarity measurement;variational auto-encoders (VAEs)","Radar imaging;Imaging;Scattering;Training;Radar measurements;Probabilistic logic;Loss measurement","radar detection;radar imaging;radar resolution;radar target recognition;statistical distributions;synthetic aperture radar","similarity measurement;inverse synthetic aperture radar;higher resolution radar images;target information;radar target detection;high-resolution ISAR image;measurement echoes;HR ISAR images;loss function;strong scattering points;target characteristics;conditional probabilistic generative model;HR ISAR imaging;conditional prior posterior probability distribution;approximate posterior probability distribution;neural-network-based ISAR imaging;HR radar images;Kullback-Leibler divergence","","2","","16","IEEE","7 May 2021","","","IEEE","IEEE Journals"
"A Semi-Supervised Image-to-Image Translation Framework for SAR–Optical Image Matching","W. -L. Du; Y. Zhou; H. Zhu; J. Zhao; Z. Shao; X. Tian","Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau","IEEE Geoscience and Remote Sensing Letters","1 Dec 2022","2022","19","","1","5","Synthetic aperture radar (SAR) and optical image matching aims to acquire correspondences from a certain pair of SAR and optical images. Recent advances in the image-to-image translation provided a way to simplify the SAR–optical image matching into the SAR–SAR or optical–optical image matchings. The existing image-to-image translations mainly focus on supervised or unsupervised learning. However, gathering sufficient amounts of aligned training data for supervised learning is challenging, while unsupervised learning cannot guarantee enough correct correspondences. In this work, we investigate the applicability of semi-supervised image-to-image translation for SAR–optical image matching such that both aligned and unaligned SAR–optical images could be used. To this end, we combine the benefits of both supervised and unsupervised well-known image-to-image translation methods, i.e., Pix2pix and CycleGAN, and propose a simple yet effective semi-supervised image-to-image translation framework. Through extensive experimental comparisons to the baseline methods, we verify the effectiveness of the proposed framework in both semi-supervised and fully supervised settings. Our codes are available at https://github.com/WenliangDu/Semi-I2I.","1558-0571","","10.1109/LGRS.2022.3223353","National Natural Science Foundation of China(grant numbers:62002360,62272461,62101555,61806206,62106268); Science and Technology Development Fund of Macau (Macau FDCT)(grant numbers:0038/2020/A1); Opening Fund of State Key Laboratory of Lunar and Planetary Sciences (Macau University of Science and Technology) (Macau FDCT)(grant numbers:119/2017/A3); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201346,BK20210488); “Double First-Class” Project of China University of Mining and Technology for Independent Innovation and Social Service(grant numbers:2022ZZCX06); China Postdoctoral Science Foundation(grant numbers:2022M713379); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955555","Generative adversarial networks (GANs);image matching;semi-supervised image synthesis;synthetic aperture radar (SAR)","Optical imaging;Optical sensors;Adaptive optics;Image matching;Training;Synthetic aperture radar;Optical distortion","image matching;learning (artificial intelligence);optical images;radar imaging;synthetic aperture radar;unsupervised learning","aligned SAR-optical images;fully supervised settings;image-to-image translation methods;optical image matching;optical-optical image matchings;SAR-optical image;SAR-SAR;semisupervised image-to-image translation framework;unaligned SAR-optical images","","","","14","IEEE","18 Nov 2022","","","IEEE","IEEE Journals"
"Background Learning Based on Target Suppression Constraint for Hyperspectral Target Detection","W. Xie; X. Zhang; Y. Li; K. Wang; Q. Du","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Oct 2020","2020","13","","5887","5897","Hyperspectral target detection is critical in both military and civilian applications. However, it is a challenging task due to the complexity of background and the limited samples of target in hyperspectral images (HSIs). In this article, we propose a novel background learning model, called background learning based on target suppression constraint to characterize high-dimensional spectral vectors. Considering insufficient target samples, the model is trained only on the background spectral samples to accurately learn the background distribution. Then the discrepancy between the reconstructed and original HSIs are examined to spot the targets. To obtain a background training dataset, coarse detection is carried out. However, it is quite difficult to retrieve pure background data. Thus, a target suppression constraint is imposed to reduce the impact of suspected target samples on background reconstruction. Experiments on six real HSIs demonstrate that the proposed framework significantly outperforms the current state-of-the-art detection methods and yields higher detection accuracy and lower false alarm rate.","2151-1535","","10.1109/JSTARS.2020.3024903","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Aeronautical Science Foundation of China(grant numbers:6142504190206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200776","Background learning;hyperspectral image (HSI);target detection;target suppression constraint","Object detection;Training;Hyperspectral imaging;Generative adversarial networks;Image reconstruction;Gallium nitride;Feature extraction","geophysical image processing;hyperspectral imaging;learning (artificial intelligence);object detection","background learning model;background reconstruction;suspected target samples;pure background data;background training dataset;background distribution;background spectral samples;insufficient target samples;high-dimensional spectral vectors;target suppression constraint;hyperspectral images;hyperspectral target detection","","21","","44","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"Using Conditional Generative Adversarial 3-D Convolutional Neural Network for Precise Radar Extrapolation","C. Wang; P. Wang; P. Wang; B. Xue; D. Wang","Joint Laboratory of Intelligent Identification and Nowcasting Service for Convective System, CMA Public Meteorological Service Center, Beijing, China; Joint Laboratory of Intelligent Identification and Nowcasting Service for Convective System, CMA Public Meteorological Service Center, Beijing, China; Joint Laboratory of Intelligent Identification and Nowcasting Service for Convective System, CMA Public Meteorological Service Center, Beijing, China; CMA Public Meteorological Service Center, Beijing, China; Joint Laboratory of Intelligent Identification and Nowcasting Service for Convective System, CMA Public Meteorological Service Center, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Jun 2021","2021","14","","5735","5749","Radar echo extrapolation is a basic but essential task in meteorological services. It could provide radar echo prediction results with high spatiotemporal resolution in a computationally efficient way, and effectively enhance the operational system's forecasting capability for meteorological hazards. Traditional methods perform extrapolation by estimating echo motions between contiguous radar data. This strategy is difficult to characterize complex nonlinear meteorological processes effectively, and it is difficult to benefit from large historical data. Recently, machine learning (ML) models have been used for radar echo extrapolation. These methods have effectively improved extrapolation quality in a data-driven way and from the statistical perspective. Although the ML-based methods show excellent performance, they usually produce blurry extrapolations. This leads to underestimating radar echo intensity and making echo lack small-scale details. Moreover, it makes models difficult to predict severe convective hazards. To solve this problem, a two-stage extrapolation model based on 3-D convolutional neural network and conditional generative adversarial network is proposed. These two models form the “pre-extrapolation” and “postprocessing” paradigm. The pre-extrapolation model is trained in the traditional way and performs rough extrapolation. The postprocessing model uses the pre-extrapolation result as input and is trained with the adversarial strategy. It could correct the echo intensity and increase the echo's details. In the experiment, our model could provide more precise radar echo extrapolations than other methods, especially for intense echoes and convective systems, in the data of North China from 2015 to 2016.","2151-1535","","10.1109/JSTARS.2021.3083647","China Special Fund for Meteorological Research(grant numbers:GYHY201406004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440761","Conditional generative adversarial network (CGAN);convective system;convolutional neural network (CNN);radar extrapolation","Radar;Extrapolation;Meteorological radar;Radar imaging;Training;Data models;Generative adversarial networks","convection;extrapolation;geophysics computing;learning (artificial intelligence);meteorological radar;neural nets;statistical analysis;weather forecasting","adversarial strategy;precise radar echo extrapolations;conditional generative adversarial 3-D convolutional neural network;precise radar extrapolation;meteorological services;radar echo prediction results;high spatiotemporal resolution;operational system;meteorological hazards;echo motions;contiguous radar data;complex nonlinear meteorological processes;historical data;machine learning models;radar echo extrapolation;extrapolation quality;ML-based methods;blurry extrapolations;radar echo intensity;making echo lack small-scale details;severe convective hazards;two-stage extrapolation model;conditional generative adversarial network;pre-extrapolation model;rough extrapolation;postprocessing model","","9","","70","CCBY","25 May 2021","","","IEEE","IEEE Journals"
"Guided-Pix2Pix: End-to-End Inference and Refinement Network for Image Dehazing","L. Jiao; C. Hu; L. Huo; P. Tang","Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Mar 2021","2021","14","","3052","3069","Haze removal is still an essential prerequisite for image processing and computer vision tasks, and joint inference and refinement of transmission maps remain challenging in the physical scattering model-based haze removal methods. In this article, we propose an end-to-end learnable dehazing network, which is referred to as Guided-Pix2Pix, to jointly estimate and refine the transmission map and further dehaze images by the physical scattering equation. Instead of a two-stage model of predicting and postprocessing the transmission, Guided-Pix2Pix concatenates the trainable Pix2Pix backbone and differentiable guided filter as an embedded layer, which enables generating refined transmission maps in one feed-forward step, and then it substitutes these potential refinements into the physical scattering equation to restore dehazed images. To verify that our Guided-Pix2Pix can be embedded in both training and inference, we demonstrate that the guided filter layer is differentiable and capable of propagating both features forward and gradients backward. Furthermore, explicit derivatives with respect to the input of the guided filter are given, and the relationship between our derivation and that in the guided filter is also explored. Experiments show that our network is effective and robust in image dehazing, can alleviate the halo artifacts along edges, and has great generalization capability.","2151-1535","","10.1109/JSTARS.2021.3061460","China Postdoctoral Science Foundation(grant numbers:2019M660852); CAS; National Natural Science Foundation of China(grant numbers:41971396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361069","Differentiable guided filter;end-to-end refinement of transmission map;image dehazing","Mathematical model;Scattering;Atmospheric modeling;Image color analysis;Visualization;Learning systems;Generative adversarial networks","","","","7","","52","CCBYNCND","23 Feb 2021","","","IEEE","IEEE Journals"
"Spectral–Spatial Attention Feature Extraction for Hyperspectral Image Classification Based on Generative Adversarial Network","H. Liang; W. Bao; X. Shen; X. Zhang","Key Laboratory of Images, and Graphics Intelligent Processing of State Ethnic Affairs Commission: IGIPLab, North Minzu University, Yinchuan, China; Key Laboratory of Images, and Graphics Intelligent Processing of State Ethnic Affairs Commission: IGIPLab, North Minzu University, Yinchuan, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; Key Laboratory of Images, and Graphics Intelligent Processing of State Ethnic Affairs Commission: IGIPLab, North Minzu University, Yinchuan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14 Oct 2021","2021","14","","10017","10032","Recent research shows that generative adversarial network (GAN) based deep learning derived frameworks can improve the accuracy of hyperspectral image (HSI) classification on limited labeled samples. However, several studies point out that existing GAN-based methods are heavily affected by the complexity and inefficient description issues of HSIs. The discriminator in GAN always attempts to interpret high-dimensional nonlinear spectral knowledge of HSIs, thus resulting in the Hughes phenomenon. Another critical issue is sample generation. The generator is only used as a regularizer for the discriminator, which seriously restricts the performance for classification. In this article, we propose SSAT-GAN, a semisupervised spectral–spatial attention feature extraction approach based on the GAN that feeds raw data into a deep learning framework, in an end-to-end fashion. First, the unlabeled data is added into the discriminator to alleviate the problems of training samples and supplies a reconstructed real HSI data distribution through adversarial training. Second, to enhance the description of HSIs, we build spectral–spatial attention modules (SSAT) and extend them to the discriminator and the generator to extract discriminative characteristics from abundant spatial contexts and spectral signatures. The SSAT modules learn a three-dimensional filter bank with spectral–spatial attention weights to obtain meaningful feature maps to improve the discrimination of the feature representation. In terms of the mode collapse of GANs, the mean minimization loss is employed for unsupervised learning. Experimental results from three real datasets indicate that SSAT-GAN has certain advantages over the state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2021.3115971","Natural Science Foundation of Ningxia Province of China(grant numbers:2020AAC02028); Natural Science Foundation of Ningxia Province of China(grant numbers:2021AAC03179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551774","Attention module;generative adversarial network (GAN);hyperspectral image (HSI) classification;semisupervised deep learning;spectral–spatial information","Generative adversarial networks;Training;Feature extraction;Generators;Complexity theory;Minimization;Deep learning","deep learning (artificial intelligence);feature extraction;geophysical image processing;hyperspectral imaging;image classification;image representation;unsupervised learning","feature representation;unsupervised learning;SSAT-GAN;hyperspectral image classification;generative adversarial network;GAN-based methods;sample generation;semisupervised spectral-spatial attention feature extraction approach;deep learning framework;HSI data distribution;adversarial training;spectral-spatial attention modules;SSAT modules;feature maps;high-dimensional nonlinear spectral knowledge;Hughes phenomenon","","6","","49","CCBY","28 Sep 2021","","","IEEE","IEEE Journals"
"Augmentation of Vegetation Index Curves Considering the Crop-Specific Phenological Characteristics","P. V. Arun; A. Karnieli","Swiss Institute for Dryland Environmental and Energy Research, Jacob Blaustein Institutes for Desert Research, Ben Gurion University of the Negev, Sede Boker Campus, Israel; Swiss Institute for Dryland Environmental and Energy Research, Jacob Blaustein Institutes for Desert Research, Ben Gurion University of the Negev, Sede Boker Campus, Israel","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Jan 2022","2022","15","","1235","1243","The state-of-the-art crop phenological classifiers use vegetation index (VI) time-series data and deep learning (DL) techniques. However, the scarcity of training samples limits the performance of these approaches. Unlike the conventional augmentation techniques, the data augmentation of VI curves should preserve the crop-specific phenological events. The DL-based augmentation approaches do not give good results when the training samples are limited. Also, the conventional approaches such as translation, rotation, scaling, and wrapping do not preserve the characteristic features of the index curves, thereby making them inappropriate for the VI-curve-based augmentations. This article proposes a non-DL-based data augmentation strategy that requires only a minimal number of actual training samples. In the proposed approach, the periodic phenological events and the underlying trend for each crop class are modeled to improve the augmentation. The trends of different crop classes are estimated by jointly maximizing the autocorrelation and variance, while the optimal subsequences are generalized as the phenological events. The proposed augmentation strategy of using Maximal overlap discrete wavelet transform for obtaining the surrogates that retain the crop-specific features and periodicities significantly improves the results. It may be noted that the proposed approach does not alter the wavelet coefficients that are characteristics of a given crop class. The experiments using time series VI data, covering 90 fields of wheat, and 60 fields of barley, confirm better accuracy of the proposed augmentation approaches as compared to the prominent approaches.","2151-1535","","10.1109/JSTARS.2022.3142395","Israeli French High Council for Scientific & Technological Cooperation(grant numbers:3-15832); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681245","Crop classification;deep learning;maximal overlap discrete wavelet transform;normalized differential vegetation index (NDVI);time series;VENµS","Indexes;Crops;Training;Market research;Generative adversarial networks;Vegetation mapping;Time series analysis","crops;deep learning (artificial intelligence);discrete wavelet transforms;pattern classification;time series;vegetation mapping","VI time series data;crop-specific phenological characteristics;vegetation index time-series data;deep learning;crop-specific phenological events;VI-curve-based augmentations;maximal overlap discrete wavelet transform;vegetation index curve augmentation;crop phenological classifiers;nonDL-based data augmentation strategy","","","","65","CCBYNCND","13 Jan 2022","","","IEEE","IEEE Journals"
"A Target SAR Image Expansion Method Based on Conditional Wasserstein Deep Convolutional GAN for Automatic Target Recognition","J. Qin; Z. Liu; L. Ran; R. Xie; J. Tang; Z. Guo","National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China; National Lab of Radar Signal Processing, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Sep 2022","2022","15","","7153","7170","For the automatic target recognition (ATR) based on synthetic aperture radar (SAR) images, enough training data are required to effectively characterize target features and obtain good recognition performance. However, in practical applications, it is difficult to collect sufficient training data. To tackle the limitation, a novel end-to-end expansion method, called conditional Wasserstein deep convolutional generative adversarial network with gradient penalty (CWDCGAN), is proposed to achieve SAR image expansion with specified category. To be specific, the CWDCGAN innovatively designed a generative adversarial network architecture based on convolutional and deconvolution networks to improve the quality of generated images. At the same time, conditional information is introduced to control the categories of generated images, and Wasserstein distance and gradient penalty are used to modify the loss function, which makes the network training more stable. Besides, feature extraction and classifier design in a typical ATR system often rely heavily on subjective expert knowledge, which seriously affects its generalization performance. Therefore, a joint recognition method of Resnet18 and support vector machine (Renset18-SVM) is adopted to improve the generalization capacity and the recognition performance. Experimental results with public measured data show that the CWDCGAN can generate higher quality SAR images, and by feeding expanded data to Renset18-SVM, the recognition accuracy is improved under different proportions of training samples.","2151-1535","","10.1109/JSTARS.2022.3199091","National Natural Science Foundation of China(grant numbers:62001346); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858033","Automatic target recognition (ATR);generative adversarial network (GAN);resnet18;SAR image expansion;support vector machine;synthetic aperture radar (SAR)","Radar polarimetry;Generative adversarial networks;Training;Target recognition;Feature extraction;Training data;Synthetic aperture radar","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image classification;object recognition;radar imaging;radar target recognition;support vector machines;synthetic aperture radar","target SAR image expansion method;conditional Wasserstein deep convolutional GAN;automatic target recognition;synthetic aperture radar images;target features;good recognition performance;gradient penalty;CWDCGAN;generative adversarial network architecture;convolutional deconvolution networks;conditional information;Wasserstein distance;network training;feature extraction;classifier design;typical ATR system;joint recognition method;Renset18-SVM;public measured data;higher quality SAR images;expanded data;recognition accuracy;training samples;conditional Wasserstein deep convolutional generative adversarial network;end-to-end expansion method","","","","54","CCBYNCND","16 Aug 2022","","","IEEE","IEEE Journals"
"Multi-View HRRP Generation With Aspect-Directed Attention GAN","Y. Song; Q. Zhou; W. Yang; Y. Wang; C. Hu; X. Hu","Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; Advanced Technology Research Institute, Beijing Institute of Technology, Jinan, China; Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; Beijing Institute of Technology Chongqing Innovation Center, Chongqing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Sep 2022","2022","15","","7643","7656","In radar automatic target recognition (RATR), high-resolution range profile (HRRP) has received intensive attention due to its low computational cost. As HRRP is sensitive to the aspect of the target, a training set covering sufficient aspects is essential to the success of an RATR model, which is however intractable in complex environment with noncooperative targets. In this article, an aspect-directed attention generative adversarial network is proposed to generate multiview HRRPs using real samples from few aspects. The key is that the HRRPs from the similar targets share the same aspect variation pattern. Hence, an HRRP is decomposed into its identity and aspect features via an aspect-directed disentangled representation network with self-attention modules. In the training stage, the decomposition network and the aspect variation pattern are learned from full aspect samples of cooperative targets. When generation, the desired multiview HRRPs of the noncooperative target are synthesized by its identity features extracted from few aspect samples and the learned aspect variation pattern. Three types of experiments on the simulated and measured datasets demonstrate the generation performances of our method. First, the generated HRRPs are visually compared with the truth. Second, the similarity of the scattering center power and handcrafted feature distributions are quantitatively evaluated. Finally, recognition experiments verify the feasibility of data augmentation with the generated HRRPs. Extensive results show the superior performance of our method over other state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3204439","National Key R&D Program of China(grant numbers:2018YFE0202102,2018YFE0202103); China Postdoctoral Science Foundation(grant numbers:2021M690412); Natural Science Foundation of Chongqing, China(grant numbers:cstc2020jcyj-msxmX0812); Natural Science Foundation of Shandong Province(grant numbers:ZR2021MF134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878051","Disentangled representation learning;generative adversarial network (GAN);multiview high-resolution range profile;radar automatic target recognition (RATR)","Generative adversarial networks;Feature extraction;Radar;Target recognition;Generators;Training;Task analysis","feature extraction;learning (artificial intelligence);object recognition;radar computing;radar resolution;radar target recognition","multiview HRRP generation;high-resolution range profile;low computational cost;sufficient aspects;RATR model;noncooperative target;aspect-directed attention generative adversarial network;similar targets share;aspect-directed disentangled representation network;self-attention modules;decomposition network;aspect samples;desired multiview HRRPs;learned aspect variation pattern;generation performances;generated HRRPs","","","","70","CCBY","5 Sep 2022","","","IEEE","IEEE Journals"
"SAR Target Image Generation Method Using Azimuth-Controllable Generative Adversarial Network","C. Wang; J. Pei; X. Liu; Y. Huang; D. Mao; Y. Zhang; J. Yang","Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Nov 2022","2022","15","","9381","9397","Sufficient synthetic aperture radar (SAR) target images are very important for the development of research works. However, available SAR target images are often limited in practice, which hinders the progress of SAR application. In this article, we propose an azimuth-controllable generative adversarial network to generate precise SAR target images with an intermediate azimuth between two given SAR images' azimuths. This network mainly contains three parts: 1) generator, 2) discriminator, and 3) predictor. Through the proposed specific network structure, the generator can extract and fuse the optimal target features from two input SAR target images to generate an SAR target image. Then, a similarity discriminator and an azimuth predictor are designed. The similarity discriminator can differentiate the generated SAR target images from the real SAR images to ensure the accuracy of the generated while the azimuth predictor measures the difference of azimuth between the generated and the desired to ensure the azimuth controllability of the generated. Therefore, the proposed network can generate precise SAR images, and their azimuths can be controlled well by the inputs of the deep network, which can generate the target images in different azimuths to solve the small sample problem to some degree and benefit the research works of SAR images. Extensive experimental results show the superiority of the proposed method in azimuth controllability and accuracy of SAR target image generation.","2151-1535","","10.1109/JSTARS.2022.3218369","National Natural Science Foundation of China(grant numbers:61901091,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933645","Automatic target recognition (ATR);azimuth-controllable;deep learning;generative adversarial network (GAN);synthetic aperture radar (SAR);target image generation","Synthetic aperture radar;Azimuth;Radar polarimetry;Generative adversarial networks;Generators;Image synthesis;Target recognition","control engineering computing;neural nets;radar computing;radar imaging;radar tracking;synthetic aperture radar;target tracking;telecommunication control","synthetic aperture radar target images;azimuth controllable generative adversarial network;different azimuths;azimuth controllability;generated SAR target images;azimuth predictor;similarity discriminator;input SAR target images;optimal target features;intermediate azimuth;SAR application;available SAR target images;SAR target image generation method","","","","52","CCBY","31 Oct 2022","","","IEEE","IEEE Journals"
"One-Shot HRRP Generation for Radar Target Recognition","L. Shi; Z. Liang; Y. Wen; Y. Zhuang; Y. Huang; X. Ding","School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2021","2022","19","","1","5","Insufficient data of a noncooperative target seriously affect the performance of radar automatic target recognition (RATR) using the high-resolution range profile (HRRP), especially when the noncooperative target has only one sample. To this end, we propose an unsupervised data generation method to generate noncooperative HRRP signals. We utilize the pretrained generative adversarial networks (GANs) model to learn the HRRP general probability distribution. To emphasize the representative and discriminative power of generated HRRP signals, a joint optimization method is proposed to preserve category information. Moreover, a feature diversification method is proposed to make the generated samples have sufficient aspect characteristics to further fit the probability distribution of the noncooperative target. Thus, the generated HRRP signals can effectively improve the recognition performance of noncooperative target. Extensive experiments on HRRP data sets demonstrate the superior performance of our method over other state-of-the-art methods.","1558-0571","","10.1109/LGRS.2021.3063241","National Natural Science Foundation of China(grant numbers:81671766,61971369,U19B2031,U1605252,61671309); Open Fund of Science and Technology on Automatic Target Recognition Laboratory(grant numbers:6142503190202); Fundamental Research Funds for the Central Universities(grant numbers:20720190116,20720200003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9378773","Data generation;high-resolution range profile (HRRP);one-shot;radar automatic target recognition (RATR);recognition","Target recognition;Gallium nitride;Optimized production technology;Generators;Feature extraction;Training data;Training","probability;radar computing;radar resolution;radar target recognition","radar target recognition;insufficient data;noncooperative target;radar automatic target recognition;high-resolution range profile;unsupervised data generation method;noncooperative HRRP signals;pretrained generative adversarial networks model;HRRP general probability distribution;generated HRRP signals;recognition performance;joint optimization method","","4","","29","IEEE","15 Mar 2021","","","IEEE","IEEE Journals"
"A Scientometric Visualization Analysis of Image Captioning Research From 2010 to 2020","W. Liu; H. Wu; K. Hu; Q. Luo; X. Cheng","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan, China; Key Laboratory of Advanced Process Control for Light Industry, Ministry of Education, Jiangnan University, Wuxi, China; School of Mathematics and Physics, Wuhan Institute of Technology, Wuhan, China; Faculty of Resources and Environmental Science, Hubei University, Wuhan, China","IEEE Access","1 Dec 2021","2021","9","","156799","156817","Image captioning has gradually gained attention in the field of artificial intelligence and become an interesting and challenging task for image understanding. It needs to identify important objects in images, extract attributes, tell relationships, and help the machine generate human-like descriptions. Recent works in deep neural networks have greatly improved the performance of image caption models. However, machines are still unable to imitate the way humans think, talk and communicate, so image captioning remains an ongoing task. It is thus very important to keep up with the latest research and results in the field of image captioning whereas publications on this topic are numerous. Our work aims to help researchers to have a macro-level understanding of image captioning from four aspects: spatial-temporal distribution characteristics, collaborative networks, trends in subject research, and historical evolutionary path. We employ scientometric visualization methods to achieve this goal. The results show that China has published the largest amount of publications in image captioning, but the United States has the greatest impact on research in this area. Besides, thirteen academic groups are identified in the field of image description, with institutions such as Microsoft, Google, Australian National University, and Georgia Institute of Technology being the most prominent research institutions. Meanwhile, we find that evaluation methods, datasets, novel image captioning models based on generative adversarial networks, reinforcement learning, and Transformer, as well as remote sensing image captioning, are the new research trends. Lastly, we conclude that image captioning research has gone through three major development stages from 2010 to 2020, and on this basis, we propose a more comprehensive taxonomy of image captioning.","2169-3536","","10.1109/ACCESS.2021.3129782","National Natural Science Foundation of China Program(grant numbers:41930107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623457","Image captioning;image description generation;scientometric analysis;visualization","Bibliometrics;Visualization;Indexes;Conferences;Remote sensing;Market research;Image recognition","artificial intelligence;data visualisation;deep learning (artificial intelligence);image processing;information analysis","image understanding;image caption models;image description;image captioning models;scientometric visualization analysis;artificial intelligence;deep neural networks","","","","114","CCBY","22 Nov 2021","","","IEEE","IEEE Journals"
"Unsupervised Haze Removal for Aerial Imagery Based on Asymmetric Contrastive CycleGAN","X. He; W. Ji; J. Xie","School of Basic Sciences for Aviation, Naval Aviation University, Yantai, China; School of Basic Sciences for Aviation, Naval Aviation University, Yantai, China; School of Basic Sciences for Aviation, Naval Aviation University, Yantai, China","IEEE Access","30 Jun 2022","2022","10","","67316","67328","Aerial image dehazing is an important preprocessing step, since haze extremely degrades the imaging quality and affects subsequent the applications of aerial imagery. Most current haze removal methods achieve encouraging performance by relying on paired synthetic data, while are limited to their generality and scalability in the practical tasks. To this end, this paper aims to learn an effective unsupervised dehazing model from an unpaired set of clear and hazy aerial images. Motivated by the great advantages of contrastive learning in unsupervised representation field, we first attempt to formulate a Asymmetric Contrastive CycleGAN dehazing framework (namely ACC-GAN) to maximize the mutual information between the hazy domain and the haze-free domain. In the latent representation space, the introduced contrastive constraint ensures that the restored image is pulled closer to the clear image and pushed away from the hazy image, so as to indirectly regularize the unsupervised dehazing process. Importantly, different from the standard CycleGAN, we develop an additional feature transfer network into the forward path to form the asymmetric structure of ACC-GAN, which can enhance encoded features from hazy domain to haze-free domain. During training, multi-dimension loss terms are jointly built into a loss committee for generating dehazed results with higher naturalness and better fidelity. Experimental results on synthesis and real-world datasets indicate that our method is superior to existing unsupervised dehazing approaches, and is also very competitive to other related supervised models.","2169-3536","","10.1109/ACCESS.2022.3186004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9805730","Aerial imagery;haze removal;asymmetric CycleGAN;unsupervised learning;contrastive learning","Image restoration;Generative adversarial networks;Atmospheric modeling;Task analysis;Mutual information;Image color analysis;Clouds","geophysical image processing;image colour analysis;image denoising;image enhancement;image representation;image restoration;neural nets;remote sensing;unsupervised learning","unsupervised dehazing approaches;unsupervised haze removal;aerial imagery;aerial image dehazing;paired synthetic data;unpaired set;hazy aerial images;contrastive learning;unsupervised representation field;ACC-GAN;haze-free domain;latent representation space;standard CycleGAN;additional feature transfer network;asymmetric structure;image restoration;asymmetric contrastive CycleGAN dehazing framework","","1","","48","CCBY","24 Jun 2022","","","IEEE","IEEE Journals"
"Unsupervised Remoting Sensing Super-Resolution via Migration Image Prior","J. Wang; Z. Shao; T. Lu; X. Huang; R. Zhang; Y. Wang",Wuhan University; Wuhan University; Wuhan Institute of Technology; University of Arkansas; Wuhan University; Wuhan Institute of Technology,"2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Recently, satellites with high temporal resolution have fostered wide attention in various practical applications. Due to limitations of bandwidth and hardware cost, however, the spatial resolution of such satellites is considerably low, largely limiting their potentials in scenarios that require spatially explicit information. To improve image resolution, numerous approaches based on training low-high resolution pairs have been proposed to address the super-resolution (SR) task. De-spite their success, however, low/high spatial resolution pairs are usually difficult to obtain in satellites with a high temporal resolution, making such approaches in SR impractical to use. In this paper, we proposed a new unsupervised learning framework, called ""MIP"", which achieves SR tasks without low/high resolution image pairs. First, random noise maps are fed into a designed generative adversarial network (GAN) for reconstruction. Then, the proposed method converts the reference image to latent space as the migration image prior. Finally, we update the input noise via an implicit method, and further transfer the texture and structured information from the reference image. Extensive experimental results on the Draper dataset show that MIP achieves significant improvements over state-of-the-art methods both quantitatively and qualitatively. The proposed MIP is open-sourced at https://github.com/jiaming-wang/MIP.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428093","National Natural Science Foundation of China; Wuhan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428093","Super-resolution;unsupervised learning;latent space;deep neural networks","Training;Satellites;Limiting;Superresolution;Neural networks;Generative adversarial networks;Sensors","geophysical image processing;image reconstruction;image resolution;image sampling;neural nets;remote sensing;unsupervised learning","unsupervised remoting sensing super-resolution;satellites;image resolution;unsupervised learning;reference image;migration image prior;low-high resolution pair training;Draper dataset;MIP;generative adversarial network;GAN","","1","","27","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"A Robust Deep Learning Approach for the Quantitative Characterization and Clustering of Peach Tree Crowns Based on UAV Images","J. Hu; Y. Zhang; D. Zhao; G. Yang; F. Chen; C. Zhou; W. Chen","Food Science Institute, Zhejiang Academy of Agricultural Sciences, Hangzhou, China; Institute of Soil and Fertilizer, Anhui Academy of Agricultural Sciences, Hefei, China; Food Science Institute, Zhejiang Academy of Agricultural Sciences, Hangzhou, China; Information Technology Research Center and the Intelligent Equipment Research Center, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; School of Life Sciences, Anhui University, Hefei, China; Institute of Agricultural Equipment, Zhejiang Academy of Agricultural Sciences, Hangzhou, China; Food Science Institute, Zhejiang Academy of Agricultural Sciences, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","15 Mar 2022","2022","60","","1","13","The accurate large-scale measurement of peach crowns is vital in horticultural science and the optimization of orchard management. Nowadays, numerous crown parameters (e.g., crown area, height, and volume) can be obtained via the analysis of point clouds or photographs. Current laser-based sensors provide the required reliable and accurate information; however, they are costly and time-consuming. Therefore, a simpler approach for crown measurement is required. For this purpose, this study presents a pipeline for the monitoring and clustering of 259 peach tree crowns based on unmanned aerial vehicle (UAV) images of a peach orchard in Southeast China. Considering the limitation that the original aerial image dataset contains little information, a data augmentation process is adopted, and an efficient deep learning architecture based on conditional generative adversarial networks (cGANs) was designed to extract the crown area. Then, the shape of the crown area was clustered using an edge detection process and a  $k$ -means algorithm. Finally, an ellipsoid volume method (EVM) was applied to estimate the crown volume. Five indicators—namely,  $Q_{\mathrm {seg}}$ ,  $S_{\mathrm {r}}$ , Precision, Recall, and F-measure—were employed to evaluate the crown extraction effects, and the average results for testing samples were 0.832, 0.847, 0.851, 0.828, and 0.846, respectively. Compared with other approaches—namely, fully convolutional network (FCN), U-Net, SegNet21, the excess green index (ExG), and the color index of vegetation extraction (CIVE)—the proposed cGAN model performs better, achieving an accuracy improvement of 5%–25%. For the estimation of crown volume, using measurements from a light detection and ranging (LIDAR) scanner as a reference, the correlation coefficient and relative-root-mean-square error (R-RMSE) were found to be 0.836% and 14.93%, respectively. Overall, the results demonstrate that the proposed method is feasible for measuring peach tree crowns. The wide application of such technology would facilitate applied research in plant phenotyping and precision horticulture.","1558-0644","","10.1109/TGRS.2022.3142288","National Natural Science Foundation of China(grant numbers:32000283,31901662,31901722); Beijing Natural Science Foundation(grant numbers:6182011); Beijing Academy of Agriculture and Forestry Sciences(grant numbers:KJCX20170423); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678963","Crown measurement;deep learning;shape clustering;unmanned aerial vehicle (UAV) images;volume estimation","Vegetation;Autonomous aerial vehicles;Laser radar;Remote sensing;Agriculture;Volume measurement;Monitoring","autonomous aerial vehicles;convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;horticulture;image classification;mean square error methods;pattern clustering;robot vision","peach orchard;original aerial image dataset;efficient deep learning architecture;crown area;crown volume;crown extraction effects;peach tree crowns;robust deep learning approach;accurate large-scale measurement;peach crowns;numerous crown parameters;current laser-based sensors;required reliable information;crown measurement;unmanned aerial vehicle images","","","","44","IEEE","12 Jan 2022","","","IEEE","IEEE Journals"
"MORGAN: Meta-Learning-based Few-Shot Open-Set Recognition via Generative Adversarial Network","D. Pal; S. Bose; B. Banerjee; Y. Jeppu","Honeywell Technology Solutions, India; Technical University of Munich, Germany; Indian Institute of Technology, Bombay; Honeywell Technology Solutions, India","2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","6 Feb 2023","2023","","","6284","6293","In few-shot open-set recognition (FSOSR) for hyperspectral images (HSI), one major challenge arises due to the simultaneous presence of spectrally fine-grained known classes and outliers. Prior research on generative FSOSR cannot handle such a situation due to their inability to approximate the open space prudently. To address this issue, we propose a method, Meta-learning-based Open-set Recognition via Generative Adversarial Network (MORGAN), that can learn a finer separation between the closed and the open spaces. MORGAN seeks to generate class-conditioned adversarial samples for both the closed and open spaces in the few-shot regime using two GANs by judiciously tuning noise variance while ensuring discriminability using a novel Anti-Overlap Latent (AOL) regularizer. Adversarial samples from low noise variance amplify known class data density, and we use samples from high noise variance to augment ""known-unknowns"". A first-order episodic strategy is adapted to ensure stability in the GAN training. Finally, we introduce a combination of metric losses which push these augmented ""known-unknowns"" or outliers to disperse in the open space while condensing known class distributions. Extensive experiments on four benchmark HSI datasets indicate that MORGAN achieves state-of-the-art FSOSR performance consistently.1","2642-9381","978-1-6654-9346-8","10.1109/WACV56688.2023.00623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030577","Algorithms: Machine learning architectures;formulations;and algorithms (including transfer);Agriculture;Remote Sensing","Training;Measurement;Computer vision;Image recognition;Benchmark testing;Generative adversarial networks;Feature extraction","","","","","","38","IEEE","6 Feb 2023","","","IEEE","IEEE Conferences"
"GANMcC: A Generative Adversarial Network With Multiclassification Constraints for Infrared and Visible Image Fusion","J. Ma; H. Zhang; Z. Shao; P. Liang; H. Xu","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Instrumentation and Measurement","25 Dec 2020","2021","70","","1","14","Visible images contain rich texture information, whereas infrared images have significant contrast. It is advantageous to combine these two kinds of information into a single image so that it not only has good contrast but also contains rich texture details. In general, previous fusion methods cannot achieve this goal well, where the fused results are inclined to either a visible or an infrared image. To address this challenge, a new fusion framework called generative adversarial network with multiclassification constraints (GANMcC) is proposed, which transforms image fusion into a multidistribution simultaneous estimation problem to fuse infrared and visible images in a more reasonable way. We adopt a generative adversarial network with multiclassification to estimate the distributions of visible light and infrared domains at the same time, in which the game of multiclassification discrimination will make the fused result to have these two distributions in a more balanced manner, so as to have significant contrast and rich texture details. In addition, we design a specific content loss to constrain the generator, which introduces the idea of main and auxiliary into the extraction of gradient and intensity information, which will enable the generator to extract more sufficient information from source images in a complementary manner. Extensive experiments demonstrate the advantages of our GANMcC over the state-of-the-art methods in terms of both qualitative effect and quantitative metric. Moreover, our method can achieve good fused results even the visible image is overexposed. Our code is publicly available at https://github.com/jiayi-ma/GANMcC.","1557-9662","","10.1109/TIM.2020.3038013","Natural Science Fund of Hubei Province(grant numbers:2019CFA037); National Natural Science Foundation of China(grant numbers:61773295,41890820); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274337","Deep learning;generative adversarial network (GAN);image fusion;infrared;multiclassification","Image fusion;Generative adversarial networks;Generators;Gallium nitride;Task analysis;Neural networks;Data mining","game theory;image classification;image fusion;image texture;infrared imaging;neural nets;pose estimation;transforms;unsupervised learning","GANMcC;generative adversarial network;multiclassification constraints;visible image fusion;infrared image fusion;visible light distribution;infrared domains;gradient extraction;image fusion transform","","37","","54","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"Machine-learned Regularization and Polygonization of Building Segmentation Masks","S. Zorzi; K. Bittner; F. Fraundorfer","Institute of Computer Graphics and Vision, Graz University of Technology; Remote Sensing Technology Institute, German Aerospace Center (DLR); Institute of Computer Graphics and Vision, Graz University of Technology","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","3098","3105","We propose a machine learning based approach for automatic regularization and polygonization of building segmentation masks. Taking an image as input, we first predict building segmentation maps exploiting generic fullyconvolutionalnetwork(FCN). A generativeadversarialnetwork(GAN) is then involved to perform a regularization of building boundaries to make them more realistic, i.e., having more rectilinear outlines which construct right angles if required. This is achieved through the interplay between the discriminator which gives a probability of input image being true and generator that learns from discriminator's response to create more realistic images. Finally, we train the backbone convolutionalneuralnetwork(CNN) which is adapted to predict sparse outcomes corresponding to building corners out of regularized building segmentation results. Experiments on three building segmentation datasets demonstrate that the proposed method is not only capable of obtaining accurate results, but also of producing visually pleasing building outlines parameterized as polygons.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412866","","Geometry;Image segmentation;Buildings;Neural networks;Machine learning;Generative adversarial networks;Generators","buildings (structures);convolutional neural nets;image segmentation;learning (artificial intelligence);realistic images;structural engineering computing","polygonization;building segmentation masks;segmentation maps;realistic images;regularized building segmentation;building segmentation datasets;building outlines;polygons;machine-learned regularization;machine learning;automatic regularization;convolutional neural network;CNN;generative adversarial network;GAN;fully convolutional network;FCN","","7","","36","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Insulator Defect Detection Based on Feature Fusion and Attention Mechanism","Y. Zhang; B. Wei; L. Zhao; J. Liu; Z. Hao; L. Li; X. Li","School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; Aero Photogrammetry and Remote Sensing Bureau of China Coal, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China","2022 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","23 Dec 2022","2022","","","1","6","The performance of insulator defect detection model is not satisfactory due to the small object size, imbalanced and insufficient data. In this paper, based on YOLOv5 model, we propose an insulator defect detection method incorporating feature fusion and attention mechanism. Firstly, multi-scale feature fusion is introduced to strengthen the ability to extract minute features from images. Secondly, an attention mechanism based on SE-C module is proposed to improve the detection of defective objects. In addition, K-means++ is used to customize anchor boxes to meet the actual requirements and avoid mismatches. The experimental results show that the proposed model achieves 92.4% precision on the public insulator dataset, which demonstrates the applicability of the auto-detection system for insulator defects significantly.","","978-1-6654-6972-2","10.1109/ICSPCC55723.2022.9984418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984418","insulator;defect detection;attention mechanism;object detection","Power transmission lines;Object detection;Signal processing;Insulators;Feature extraction;Generative adversarial networks;Data models","feature extraction;flaw detection;image fusion;image recognition;insulators;neural nets;power engineering computing","anchor boxes;attention mechanism;feature fusion;insulator defect detection model;K-means++;public insulator dataset;small object size;YOLOv5 model","","","","19","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Exploring the Potential of Unsupervised Image Synthesis for SAR-Optical Image Matching","W. -L. Du; Y. Zhou; J. Zhao; X. Tian; Z. Yang; F. Bian","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the People's Republic of China, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau; DFH Satellite Company Ltd., Beijing, China; DFH Satellite Company Ltd., Beijing, China","IEEE Access","18 May 2021","2021","9","","71022","71033","We consider SAR-optical image matching problems, where correspondences are acquired from a pair of SAR and optical images. Recent methods for such a problem typically simplify the SAR-optical image matching to the SAR-SAR or optical-optical image matchings using supervised-image-synthesis methods. However, training supervised-image-synthesis needs plenty of aligned SAR-optical image pairs while gathering sufficient amounts of aligned multi-modal image pairs is challenging in remote sensing. In this work, we investigate the applicability of unsupervised-image-synthesis for SAR-optical image matching such that the unaligned SAR-optical images could be used. To this end, we apply feature matching loss to a well known unsupervised-image-synthesis method, i.e., CycleGAN, to enforce the feature matching consistency. Moreover, we develop a shared-matching-strategy to improve the results of SAR-optical image matching further. Qualitative comparisons against CycleGAN, StarGAN, and DualGAN demonstrate the superiority of our approach. Quantitative results show that, compared with CycleGAN, StarGAN, and DualGAN, our method obtains at least 2.6 times more qualified SAR-optical matchings.","2169-3536","","10.1109/ACCESS.2021.3079327","National Natural Science Foundation of China(grant numbers:62002360,61806206,61772530); opening fund of State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology (Macau FDCT)(grant numbers:119/2017/A3); Science and Technology Development Fund of Macau(grant numbers:0038/2020/A1); Fundamental Research Funds for the Central Universities(grant numbers:2020ZDPY0305); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201346,BK20180639); Six Talent Peaks Project in Jiangsu Province(grant numbers:2015-DZXX-010,2018-XYDXX-044); China Postdoctoral Science Foundation(grant numbers:2020M681765); Jiangsu Province Postdoctoral Research Foundation(grant numbers:2020Z178); Xuzhou Science and Technology Program(grant numbers:KC18061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427486","Image matching;unsupervised-image-synthesis;synthetic aperture radar (SAR);generative adversarial networks (GANs)","Optical imaging;Image matching;Adaptive optics;Optical sensors;Radar polarimetry;Nonlinear optics;Image synthesis","geophysical image processing;image matching;image registration;optical images;radar imaging;remote sensing;synthetic aperture radar","unsupervised image synthesis;SAR-optical image matching problems;SAR-SAR;optical-optical image matchings;supervised-image-synthesis methods;training supervised-image-synthesis;aligned SAR-optical image pairs;aligned multimodal image pairs;unaligned SAR-optical images;known unsupervised-image-synthesis method;2.6 times more qualified SAR-optical matchings","","5","","57","CCBY","11 May 2021","","","IEEE","IEEE Journals"
"Birds of a Feather Flock Together: Category-Divergence Guidance for Domain Adaptive Segmentation","B. Yuan; D. Zhao; S. Shao; Z. Yuan; C. Wang","Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; ByteDance AI Lab, Beijing, China; ByteDance AI Lab, Beijing, China; ByteDance AI Lab, Beijing, China","IEEE Transactions on Image Processing","8 Apr 2022","2022","31","","2878","2892","Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. Present UDA models focus on alleviating the domain shift by minimizing the feature discrepancy between the source domain and the target domain but usually ignore the class confusion problem. In this work, we propose an Inter-class Separation and Intra-class Aggregation (ISIA) mechanism. It encourages the cross-domain representative consistency between the same categories and differentiation among diverse categories. In this way, the features belonging to the same categories are aligned together and the confusable categories are separated. By measuring the align complexity of each category, we design an Adaptive-weighted Instance Matching (AIM) strategy to further optimize the instance-level adaptation. Based on our proposed methods, we also raise a hierarchical unsupervised domain adaptation framework for cross-domain semantic segmentation task. Through performing the image-level, feature-level, category-level and instance-level alignment, our method achieves a stronger generalization performance of the model from the source domain to the target domain. In two typical cross-domain semantic segmentation tasks, i.e., GTA $5\rightarrow $ Cityscapes and SYNTHIA $\rightarrow $ Cityscapes, our method achieves the state-of-the-art segmentation accuracy. We also build two cross-domain semantic segmentation datasets based on the publicly available data, i.e., remote sensing building segmentation and road segmentation, for domain adaptive segmentation. Our code, models and datasets are available at https://github.com/HibiscusYB/BAFFT.","1941-0042","","10.1109/TIP.2022.3162471","National Key Research and Development Program of China(grant numbers:2019YFC1510905); Air Force Equipment Pre-Research Project(grant numbers:303020401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745831","Unsupervised domain adaptation;semantic segmentation;category divergence;inter-class separation;intra-class aggregation","Image segmentation;Semantics;Adaptation models;Feature extraction;Training;Generative adversarial networks;Task analysis","image matching;image segmentation;unsupervised learning","category-divergence guidance;domain adaptive segmentation;source domain;target domain;UDA models;domain shift;class confusion problem;cross-domain representative consistency;diverse categories;confusable categories;instance-level adaptation;hierarchical unsupervised domain adaptation framework;cross-domain semantic segmentation task;instance-level alignment;remote sensing building segmentation;road segmentation;adaptive-weighted instance matching strategy;AIM","Data Collection;Image Processing, Computer-Assisted;Semantics","2","","90","IEEE","31 Mar 2022","","","IEEE","IEEE Journals"
"Adversarial Networks for Spatial Context-Aware Spectral Image Reconstruction from RGB","A. Alvarez-Gila; J. Van De Weijer; E. Garrote","TECNALIA / CVC, Universitat Autonoma de Barcelona, Derio, Spain; CVC, Universitat Autonoma de Barcelona, Barcelona, Catalunya, ES; TECNALIA, Derio, Spain","2017 IEEE International Conference on Computer Vision Workshops (ICCVW)","22 Jan 2018","2017","","","480","490","Hyperspectral signal reconstruction aims at recovering the original spectral input that produced a certain trichromatic (RGB) response from a capturing device or observer. Given the heavily underconstrained, non-linear nature of the problem, traditional techniques leverage different statistical properties of the spectral signal in order to build informative priors from real world object reflectances for constructing such RGB to spectral signal mapping. However, most of them treat each sample independently, and thus do not benefit from the contextual information that the spatial dimensions can provide. We pose hyperspectral natural image reconstruction as an image to image mapping learning problem, and apply a conditional generative adversarial framework to help capture spatial semantics. This is the first time Convolutional Neural Networks -and, particularly, Generative Adversarial Networks- are used to solve this task. Quantitative evaluation shows a Root Mean Squared Error (RMSE) drop of 44.7% and a Relative RMSE drop of 47.0% on the ICVL natural hyperspectral image dataset.","2473-9944","978-1-5386-1034-3","10.1109/ICCVW.2017.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8265274","","Image reconstruction;Generators;Hyperspectral imaging;Semantics;Imaging","hyperspectral imaging;image colour analysis;image reconstruction;neural nets;remote sensing;spectral analysis","convolutional neural networks;object reflectance;ICVL natural hyperspectral image dataset;Generative Adversarial Networks;image mapping;hyperspectral natural image reconstruction;spectral signal mapping;capturing device;trichromatic response;hyperspectral signal reconstruction;spatial context-aware spectral image reconstruction","","43","","53","IEEE","22 Jan 2018","","","IEEE","IEEE Conferences"
"Learning Spectral Cues for Multispectral and Panchromatic Image Fusion","Y. Xing; S. Yang; Y. Zhang; Y. Zhang","Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Image Processing","8 Nov 2022","2022","31","","6964","6975","Recently, deep learning based multispectral (MS) and panchromatic (PAN) image fusion methods have been proposed, which extracted features automatically and hierarchically by a series of non-linear transformations to model the complicated imaging discrepancy. But they always pay more attention to the extraction and compensation of spatial details and use the mean squared error or mean absolute error as a loss function, regardless of the preservation of spectral information contained in multispectral images. For the sake of the improvements in both spatial and spectral resolution, this paper presents a novel fusion model that takes the spectral preservation into consideration, and learns the spectral cues from the process of generating a spectrally refined multispectral image, which is constrained by a spectral loss between the generated image and the reference image. Then these spectral cues are used to modulate the PAN features to obtain final fusion result. Experimental results on reduced-resolution and full-resolution datasets demonstrate that the proposed method can obtain a better fusion result in terms of visual inspection and evaluation indices when compared with current state-of-the-art methods.","1941-0042","","10.1109/TIP.2022.3215906","National Natural Science Foundation of China (NFSC)(grant numbers:62201467,U19B2037); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110544); Natural Science Basic Research Program of Shaanxi(grant numbers:2022JQ-686); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935814","Image fusion;pansharpening;spectral cues;spectral loss;generative adversarial networks","Feature extraction;Image fusion;Pansharpening;Task analysis;Spatial resolution;Remote sensing;Modulation","feature extraction;geophysical image processing;image fusion;image resolution;learning (artificial intelligence);sensor fusion;wavelet transforms","spectral cues;spectrally refined multispectral image;spectral loss;reference image;PAN features;final fusion result;reduced-resolution;full-resolution datasets;panchromatic image;deep learning;nonlinear transformations;complicated imaging discrepancy;compensation;spatial details;mean squared error;absolute error;spectral information;multispectral images;spatial resolution;spectral resolution;fusion model;spectral preservation","","","","58","IEEE","2 Nov 2022","","","IEEE","IEEE Journals"
"Enhancing NDVI Calculation of Low-Resolution Imagery using ESRGANs","M. M. Khaliq; R. Mumtaz","School of Electrical Engineering and Computer Science, National University of Sciences and Technology (NUST), Islamabad, Pakistan; School of Electrical Engineering and Computer Science, National University of Sciences and Technology (NUST), Islamabad, Pakistan","2022 24th International Multitopic Conference (INMIC)","13 Dec 2022","2022","","","1","6","Normalized Difference Vegetation Index (NDVI) has been one of the key scales for monitoring multiple plant parameters, but satellite imagery is never up to date, which makes it difficult to get readings for the recent situation of field crops. Doing so with Unmanned Aerial System, drone, in this case, is an intricate task, but with its advantages which include timely and effective measurements with the least errors to be fixed in post-processing of data. Before this, NDVI has been calculated using an Unmanned Aerial System, but the problem of the low resolution of the imagery always lingers. With the recent advancement of generated adversarial networks, the up-scaling of images has been made possible, which, if done with the right model, rules out the need for upgrading the camera hardware that is never cost-effective. We have come up with the solution of calculating the vegetation index of field crops by implementing Enhanced Super-Resolution Generated Adversarial Networks with drone imagery to calculate the vegetation index of crop fields. A simple near-infrared spectrum camera is usually not capable of producing a higher resolution image, by implementing the aforementioned generated adversarial network, we have been able to calculate vegetation index for a comparably much higher resolution image without upgrading with sophisticated hardware. We were able to perform the calculations for more pixels (12952) against the same area yielded an output value of 0.829 as compared to 0.828 in the case of low-resolution imagery (546416 pixels). The averaged values for red and near-infrared pixels showed changes from 32.337 to 30.264 for red, and from 189.168 to 182.1656 for near-infrared pixels. The results produced with this technique are different from those generated using original images which account for a new gateway in the calculation of the NDVI.","2049-3630","979-8-3503-9710-9","10.1109/INMIC56986.2022.9972928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9972928","Machine Learning;super-resolution;Generative Adversarial Networks;Multispectral imagery;NDVI","Satellites;Superresolution;Vegetation mapping;Crops;Cameras;Hardware;Indexes","autonomous aerial vehicles;crops;geophysical image processing;image resolution;infrared imaging;mobile robots;neural nets;remote sensing","camera hardware;crop fields;data post-processing;drone imagery;enhanced super-resolution generated adversarial networks;ESRGANs;field crops;image resolution;low-resolution imagery;NDVI calculation;near-infrared pixels;near-infrared spectrum camera;normalized difference vegetation index;plant parameter monitoring;satellite imagery;unmanned aerial system","","","","22","IEEE","13 Dec 2022","","","IEEE","IEEE Conferences"
"Synthesis of Satellite-Like Urban Images From Historical Maps Using Conditional GAN","H. J. A. Andrade; B. J. T. Fernandes","Escola Politécnica da Universidade de Pernambuco, Recife, Brazil; Escola Politécnica da Universidade de Pernambuco, Recife, Brazil","IEEE Geoscience and Remote Sensing Letters","15 Dec 2021","2022","19","","1","4","One method for encouraging the public interest in the use of historical maps as a source of reliable knowledge is to represent them in a more familiar aspect, such as the style of the current-day popular application Google Maps’ satellite view. We present a method for synthesizing satellite-images from historical maps, translating their visuals using conditional generative adversarial networks (conditional GANs). We discuss a typical representation of these dated documents to allow such translations. We observe how the semantics involved in the process influence the outcomes. Finally, we discuss the effective result of bringing the past to a familiar look for the viewer.","1558-0571","","10.1109/LGRS.2020.3023170","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), Brazil(grant numbers:001); Brazilian Agencies Fundação de Amparo à Ciência e Tecnologia de Pernambuco (FACEPE) and Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205243","Neural networks;urban areas","Satellites;Visualization;Image segmentation;Gallium nitride;Computer architecture;Google;Semantics","","","","4","","11","IEEE","24 Sep 2020","","","IEEE","IEEE Journals"
"Side-Scan Sonar Image Synthesis Based on Generative Adversarial Network for Images in Multiple Frequencies","Y. Jiang; B. Ku; W. Kim; H. Ko","School of Electrical Engineering, Korea University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea; Agency for Defense Development, Jinhae, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea","IEEE Geoscience and Remote Sensing Letters","27 Aug 2021","2021","18","9","1505","1509","The side-scan sonar (SSS) is a critical sensor device used to explore underwater environments in the deep sea. Gathering SSS data, however, is an expensive and time-consuming task because it requires sensor towing and involves complicated field operations. Recently, deep learning has been making advances rapidly in the field of computer vision. Benefiting from this development, generative adversarial networks (GANs) have been demonstrated to produce realistic synthetic data of various types, including images and acoustics signals. In this letter, we propose a GAN-based semantic image synthesis model based on GAN that can generate high-quality SSS images at a low cost in less time. We evaluate the proposed model using both shallow and deep water SSS data sets that include a diverse range of imaging conditions. such as high and low sonar operating frequencies and different landscapes. The experimental results show that the proposed method can effectively generate synthesized SSS data characterized by the shape and style of real data, thereby demonstrating its promising potential for SSS data augmentation in diverse SSS relevant machine learning tasks.","1558-0571","","10.1109/LGRS.2020.3005679","Agency for Defense Development of Korea(grant numbers:UD190005DD); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137342","Generative adversarial network (GAN);image translation;semantic image synthesis;side-scan sonar (SSS)","Image segmentation;Semantics;Generators;Image synthesis;Feature extraction;Sonar;Measurement","geophysical image processing;learning (artificial intelligence);sonar imaging","expensive time-consuming task;sensor towing;complicated field operations;deep learning;computer vision;generative adversarial network;realistic synthetic data;GAN-based semantic image synthesis model;high-quality SSS images;shallow water SSS data;deep water SSS data;imaging conditions;high sonar operating frequencies;low sonar operating frequencies;SSS data augmentation;diverse SSS relevant machine learning tasks;scan sonar image synthesis;side-scan sonar;critical sensor device;underwater environments;deep sea;gathering SSS data","","2","","16","IEEE","9 Jul 2020","","","IEEE","IEEE Journals"
"Mind the Gap: Generating Imputations for Satellite Data Collections at Myriad Spatiotemporal Scopes","P. Khandelwal; D. Rammer; S. Pallickara; S. L. Pallickara","Department of Computer Science, Colorado State University, Fort Collins, Colorado; Department of Computer Science, Colorado State University, Fort Collins, Colorado; Department of Computer Science, Colorado State University, Fort Collins, Colorado; Department of Computer Science, Colorado State University, Fort Collins, Colorado","2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)","2 Aug 2021","2021","","","92","102","Hyperspectral satellite data collections have been successfully leveraged in many domains such as meteorology, agriculture, forestry, and disaster management. There is also a collection of publicly available satellite observation networks. However, gaps in scanning frequencies and inadequate spatial resolutions limit the capabilities of geoscience applications. In this study, we target the temporal sparsity of high-resolution satellite images. In particular, we propose a novel methodology to estimate high-resolution images between scheduled scans. Our model SATnet, falls broadly within the class of Generative Adversarial Networks. SATnet allows us to generate accurate high-resolution, high-frequency satellite data at diverse spatial extents. SATnet achieves this by learning relations between a sequence of high-resolution/low-frequency satellite imageries (from Sentinel-2) and an ancillary satellite image that is high-frequency/low-resolution (from MODIS). Our benchmarks demonstrate that SATnet outperforms existing approaches such as ConvLSTMs, Dynamic Filter Network, and TrajGRU with a PSNR accuracy of 31.82. We trained and deployed SATnet over a distributed storage cluster to support the high-throughput generation of imputed satellite imagery via query evaluations. Our methodology preserves geospatial proximity and facilitates the dynamic construction of satellite imagery at a particular timestamp for arbitrary spatial scopes.","","978-1-7281-9586-5","10.1109/CCGrid51090.2021.00019","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9499692","spatial data;time series analysis;deep learning;high-resolution imaging","Satellites;Query processing;Imaging;Data collection;Spatial databases;Spatiotemporal phenomena;Spatial resolution","forestry;geophysical image processing;geophysical signal processing;geophysical techniques;image classification;image resolution;remote sensing","generating imputations;myriad spatiotemporal scopes;hyperspectral satellite data collections;disaster management;publicly available satellite observation networks;scanning frequencies;inadequate spatial resolutions;high-resolution satellite images;high-resolution images;scheduled scans;model SATnet;Generative Adversarial Networks;accurate high-resolution;high-frequency satellite data;diverse spatial extents;ancillary satellite image;Dynamic Filter Network;imputed satellite imagery;arbitrary spatial scopes","","2","","42","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"Two-Branch Generative Adversarial Network With Multiscale Connections for Hyperspectral Image Classification","D. Song; Y. Tang; B. Wang; J. Zhang; C. Yang","College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China","IEEE Access","26 Jan 2023","2023","11","","7336","7347","Hyperspectral image (HSI) classification has always drawn great attention in the field of remote sensing. Various deep learning models are in the ascendant and gradually applied to HSI classification. Nevertheless, limited-labeled and class-imbalanced datasets largely make the classifier prone to overfitting. To address the above problem, this article proposes a two-branch generative adversarial network with multiscale connections (TBGAN), which includes two generators to produce the spectral and spatial samples, respectively. Thereinto, the spectral generator is imbued with the self-attention mechanism to maximumly capture the long-term dependencies across the spectral bands. And meanwhile, an elaborated discriminator with two branches is devised in TBGAN for extracting the joint spectral-spatial features. Besides, the multiscale connections are placed between the discriminator and two generators to alleviate the instability problems caused by the inherently backward propagation of gradients in GAN. Furthermore, a feature-matching term is added to the loss function to prevent the generators from overtraining upon the current discriminator, thereby further improving the stability of the network. Experiments upon three benchmark datasets demonstrate that TBGAN achieves an extremely competitive classification accuracy and exerts lower sensitivity to the training sample size compared with several state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2022.3232152","Key Program of Joint Fund of the National Natural Science Foundation of China and Shandong Province(grant numbers:U1906217,U22A20586); Natural Science Foundation of Shandong Province(grant numbers:ZR2022MD015); National Natural Science Foundation of China(grant numbers:41701513,41772350,61371189); Key Research and Development Program of Shandong Province(grant numbers:2019GGX101033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999217","Hyperspectral image classification;generative adversarial network;multiscale connections;joint spectral-spatial features","Generators;Feature extraction;Generative adversarial networks;Training;Convolutional neural networks;Hyperspectral imaging;Task analysis","deep learning (artificial intelligence);feature extraction;geophysical image processing;hyperspectral imaging;image classification;neural nets","deep learning models;extremely competitive classification accuracy;feature-matching term;GAN;HSI classification;hyperspectral image classification;limited-labeled class-imbalanced datasets;multiscale connections;self-attention mechanism;spectral bands;spectral generator;spectral-spatial feature extraction;TBGAN;two-branch generative adversarial network","","","","53","CCBY","26 Dec 2022","","","IEEE","IEEE Journals"
"Vehicle Detection in Aerial Images Based on Lightweight Deep Convolutional Network and Generative Adversarial Network","J. Shen; N. Liu; H. Sun; H. Zhou","MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, China; MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, China; MIIT Key Laboratory of Pattern Analysis and Machine Intelligence, Nanjing, China; Department of Informatics, University of Leicester, Leicester, U.K.","IEEE Access","21 Oct 2019","2019","7","","148119","148130","Vehicle detection in aerial images is a challenging task and plays an important role in a wide range of applications. Traditional detection algorithms are based on sliding-window searching and shallow-learning-based features, which limits the ability to represent features and generates a lot of computational costs. Recently, with the successful application of convolutional neural network in computer vision, many state-of-the-art detectors have been developed based on deep CNNs. However, these CNN-based models still face some difficulties and challenges in vehicle detection in aerial images. Firstly, the CNN-based detection model requires extensive calculations during training and detection, and the accuracy of detection for small objects is not high. In addition, deep learning models often require a large amount of sample data to train a robust detection model, while the annotated data of aerial vehicles is limited. In this study, we propose a lightweight deep convolutional neural network detection model named LD-CNNs. The detection algorithm not only greatly reduces the computational costs of the model, but also significantly improves the accuracy of the detection. What's more, in order to cope with the problem of insufficient training samples, we develop a multi-condition constrained generative adversarial network named MC-GAN, which can effectively generate samples. The detection performance of the proposed model has been evaluated on the Munich public dataset and the collected dataset respectively. The results show that on the Munich dataset, the proposed method achieves 86.9% on mAP (mean average precision), F1-score is 0.875, and the detection time is 1.64s on Nvidia Titan XP. At present, these detection indicators have reached state-of-the-art level in vehicle detection of aerial images.","2169-3536","","10.1109/ACCESS.2019.2947143","National Natural Science Foundation of China(grant numbers:61375021); Northwestern Polytechnical University(grant numbers:NS2016091); Collaborative Innovation Center of Novel Software Technology and Industrialization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867888","Vehicle detection;lightweight convolutional network;generative adversarial network;aerial images","Feature extraction;Vehicle detection;Object detection;Computational modeling;Generative adversarial networks;Convolutional neural networks;Computational efficiency","computer vision;feature extraction;image classification;learning (artificial intelligence);neural nets;object detection","vehicle detection;aerial images;lightweight deep convolutional network;traditional detection algorithms;sliding-window searching;shallow-learning-based features;computational costs;CNN-based models;CNN-based detection model;deep learning models;robust detection model;aerial vehicles;lightweight deep convolutional neural network detection model;detection algorithm;multicondition constrained generative adversarial network;detection performance;detection time;detection indicators;time 1.64 s","","20","","40","CCBY","14 Oct 2019","","","IEEE","IEEE Journals"
"Semi-Supervised Learning Based on Generative Adversarial Network and Its Applied to Lithology Recognition","G. Li; Y. Qiao; Y. Zheng; Y. Li; W. Wu","Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China; Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China; Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China; Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China; Laboratory of Oil and Gas Big Data, China University of Petroleum Beijing at Kelamayi, Xinjiang, China","IEEE Access","4 Jun 2019","2019","7","","67428","67437","Lithology recognition is an essential part of reservoir parameter prediction. Compared to conventional algorithms, deep learning that needs a large amount of training data as support can extract features automatically. In the process of real data acquisition, the labeled data account for only a small portion due to high drilling cost, and it is difficult to achieve the data size required for deep learning training, resulting in a significant variance of the recognition model. In this paper, for this shortage, a semi-supervised algorithm based on generative adversarial network (GAN) with Gini-regularization is proposed, called SGAN_G, which takes borehole-side data as labeled data and seismic data as unlabeled data. First, the SGAN_G is trained by Adam (a method for stochastic optimization) algorithm and utilizes a discriminator to lithology recognition. And, we add the entropy regularization to the initial loss function which enhances the convergence speed and accuracy of the model. Eventually, we propose a novel sampling approach which employs multiple sampling points of seismic data as inputs to use the stratum information implicitly. Through the experimental comparison with a variety of supervised approaches, we can see that the SGAN_G can achieve higher prediction accuracy by using unlabeled data effectively.","2169-3536","","10.1109/ACCESS.2019.2918366","National Natural Science Foundation of China(grant numbers:60473125,61701213); Innovation Foundation of CNPC(grant numbers:05E7013); National Key Project Foundation of Science(grant numbers:G5800-08-ZS-WX); Science Foundation of China University of Petroleum-Beijing At Karamay(grant numbers:RCYJ2016B-03-001); Cooperative Education Project of Nation Education Ministry(grant numbers:201702098015); Natural Science Foundation of Fujian Province(grant numbers:2018J01545,2018J01546,2019J01748); Research Funds for the Educational Department of Fujian Province(grant numbers:JA15300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8720241","Entropy regularization;generative adversarial network;lithology recognition;semi-supervised learning","Entropy;Data models;Training;Generative adversarial networks;Semisupervised learning;Generators;Petroleum","data acquisition;entropy;environmental science computing;learning (artificial intelligence);reservoirs","generative adversarial network;lithology recognition;reservoir parameter prediction;supervised approaches;SGAN_G;unlabeled data;seismic data;borehole-side data;semisupervised algorithm;recognition model;deep learning training;data size;high drilling cost;labeled data account;data acquisition;training data;conventional algorithms","","17","","30","OAPA","22 May 2019","","","IEEE","IEEE Journals"
"Synthetic IR Image Refinement Using Adversarial Learning With Bidirectional Mappings","R. Zhang; C. Mu; M. Xu; L. Xu; Q. Shi; J. Wang","School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; GBDTC, University of Technology Sydney, Ultimo, NSW, Australia; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; GBDTC, University of Technology Sydney, Ultimo, NSW, Australia; Beijing Institute of Electronic System Engineering, Beijing, China","IEEE Access","30 Oct 2019","2019","7","","153734","153750","Collecting a large dataset of real infrared (IR) images is expensive, time-consuming, and even unavailable in some specific scenarios. With recent progress in machine learning, it has become more feasible to replace real IR images with qualified synthetic IR images in learning-based IR systems. However, this alternative may fail to achieve the desired performance, due to the gap between real and synthetic IR images. Inspired by adversarial learning for image-to-image translation, we propose the Synthetic IR Refinement Generative Adversarial Network (SIR-GAN) to narrow this gap. By learning the bidirectional mappings between two unpaired domains, the realism of the simulated IR images generated from the IR Simulator are significantly improved, where the source domain contains a large number of simulated IR images, where the target domain contains a limited quantity of real IR images. Specifically, driven by the idea of transferring infrared characteristic and protect target semantic information simultaneously, we propose a SIR refinement loss to consider an infrared loss and a structure loss further to the adversarial loss and the consistency loss. To further reduce the gap, stabilize training, and avoid artefacts, we modify the proposed algorithm by developing a training strategy, adding the U-net in the generators, using the dilated convolution in the discriminators and invoking the N-Adam acts as the optimizer. Qualitative, quantitative, and ablation study experiments demonstrate the superiority of the proposed approach compared with the state-of-the-art techniques in terms of realism and fidelity. In addition, our refined IR images are evaluated in the context of a feasibility study, where the accuracy of the trained classifier is significantly improved by adding our refined data into a small real-data training set","2169-3536","","10.1109/ACCESS.2019.2947657","China Scholarship Council; Beijing Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8871125","Infrared simulation;synthetic refinement;convolutional neural networks;adversarial learning","Training;Data models;Task analysis;Semantics;Object oriented modeling;Computational modeling;Generative adversarial networks","infrared imaging;learning (artificial intelligence);pattern classification","SIR refinement loss;infrared loss;adversarial loss;refined IR images;adversarial learning;bidirectional mappings;infrared images;machine learning;qualified synthetic IR images;learning-based IR systems;image-to-image translation;Synthetic IR Refinement Generative Adversarial Network;simulated IR images;IR Simulator;synthetic IR image refinement","","7","","51","CCBY","16 Oct 2019","","","IEEE","IEEE Journals"
"An Enhanced GAN Model for Automatic Satellite-to-Map Image Conversion","Y. Zhang; Y. Yin; R. Zimmermann; G. Wang; J. Varadarajan; S. -K. Ng","School of Computing, National University of Singapore, Singapore; Institute of Data Science, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; R&D Geo, GrabTaxi Holdings, Singapore; R&D Geo, GrabTaxi Holdings, Singapore; Institute of Data Science, National University of Singapore, Singapore","IEEE Access","2 Oct 2020","2020","8","","176704","176716","Location-based service significantly relies on accurate and up-to-date maps. The conventional map generation involves labor-intensive and time-consuming manual efforts, which restricts the map-update frequency to a few years or even longer. In recent years, satellite images become more ubiquitous, and converting them to map-style images has attracted attention due to its frequent-updating and cost-effective nature. Generative adversarial network (GAN) is a promising approach for automatic satellite-to-map image conversion. However, it is still challenging to process satellite images when the underlying road structure is complex and irregular, or when some objects are visually indistinguishable due to obstruction or bad weather. To address these issues, we propose an enhanced GAN model to generate improved quality map images by bringing in the external geographic data as implicit guidance. The textual geographic data is converted to an image so that it can work collaboratively and seamlessly with the satellite image during the conversion. A high-level semantic regulation is also introduced to further reduce the noisy patterns generated during the translation, which occur frequently for the regions with sparse geographic data. The proposed method is versatile to various backbone GAN structures with a 20% performance improvement on three popular metrics (Inception Score, Frechet Inception Distance Score and SSIM score). Our proposed Semantic-regulated Geographic GAN (SG-GAN) is anticipated to reduce the manual identification efforts in broad geospatial applications.","2169-3536","","10.1109/ACCESS.2020.3025008","Singapore Ministry of Education Academic Research Fund Tier 2(grant numbers:MOE2018-T2-1-103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200630","Geographic information;location-based service;image to image translation;GAN","Gallium nitride;Generative adversarial networks;Satellites;Roads;Global Positioning System;Semantics;Urban areas","cartography;geographic information systems;image processing;location based services;mobile computing","semantic-regulated geographic GAN;enhanced GAN model;automatic satellite-to-map image conversion;map-update frequency;satellite image;map-style images;generative adversarial network;improved quality map images;backbone GAN structures;performance improvement;inception score;Frechet inception distance score;SSIM score;SG-GAN;road structure","","5","","40","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"Adversarial Mobility Learning for Human Trajectory Classification","Q. Gao; F. Zhang; F. Yao; A. Li; L. Mei; F. Zhou","Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, China; Network and Data Security Key Laboratory of Sichuan Province, University of Electronic Science and Technology of China, Chengdu, China; Sichuan Dahui Big Data Service Company Ltd., China Energy Investment Corporation, Beijing, China; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Technology, Southwest Minzu University, Chengdu, China; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Access","3 Feb 2020","2020","8","","20563","20576","Understanding human mobility is one of the important but challenging tasks in Location-based Social Networks (LBSN). Recently, a user mobility mining task called Trajectory User Linking (TUL) has become an essential and popular topic, aiming at identifying user identities through exploiting their mobility patterns. Existing methods mainly focus on learning sequential mobility patterns by capturing long-short term dependencies among historical check-ins. However, users have personalized moving preferences, which have not been considered in previous work. Besides, how to leverage the prior knowledge behind human mobility needs to be further investigated. In this work, we present a novel semi-supervised method, called AdattTUL, to make adversarial mobility learning for human trajectory classification, which is an end-to-end framework modeling human moving patterns. AdattTUL integrates multiple human preferences of check-in behaviors and involves an attention mechanism to dynamically capture the complex relationships of user check-ins from trajectory data. In addition, AdattTUL leverages an adversarial network to help in regularizing the data distribution of human trajectories. Extensive experiments conducted on real-world LBSN datasets show that AdattTUL significantly improves the TUL performance.","2169-3536","","10.1109/ACCESS.2020.2968935","National Natural Science Foundation of China(grant numbers:61602097,61802033,61472064); China Scholarship Council(grant numbers:201906070095); Department of Science and Technology of Sichuan Province(grant numbers:2018GZ0087,2019YJ0543); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967063","Trajectory user linking;adversarial model;autoencoder;attention mechanism;human mobility","Trajectory;Data models;Gallium nitride;Generative adversarial networks;Deep learning;Recurrent neural networks;Computer architecture","data mining;learning (artificial intelligence);location based services;mobile computing;pattern classification;social networking (online)","AdattTUL;adversarial network;trajectory data;check-in behaviors;human moving patterns;end-to-end framework;semisupervised method;long-short term dependencies;sequential mobility patterns;user identities;Trajectory User Linking;user mobility mining task;location-based social networks;human mobility;human trajectory classification;adversarial mobility learning","","5","","48","CCBY","23 Jan 2020","","","IEEE","IEEE Journals"
"Energy-Efficient Reconstruction Method for Transmission Lines Galloping With Conditional Generative Adversarial Network","D. Wu; H. Cao; D. Li; S. Yang","Chongqing Key Laboratory of Space Information Network and Intelligent Information Fusion, Chongqing University, Chongqing, China; State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China; Chongqing Key Laboratory of Space Information Network and Intelligent Information Fusion, Chongqing University, Chongqing, China; State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China","IEEE Access","28 Jan 2020","2020","8","","17310","17319","Conductor galloping seriously threatens the safe operation of power systems and may lead to various damages such as wire fractures or tower collapses and large-scale grid breakdowns. Real-time galloping data are important in the mechanism and effect analysis of conductor dancing prevention; moreover, they are critical for verifying anti-galloping designs and developing galloping prevention plans. However, owing to the limitations of using sensors on cables, obtaining complete galloping data is an ill-posed and challenging problem. In this study, a novel curve reconstruction method using a conditional generative adversarial network (GAN), CR-CGAN, is proposed for fully synthesizing transmission line galloping curves. We use the modeling capabilities of the recently introduced GAN by imposing additional constraints to achieve full reconstruction of the galloping curves. Moreover, we introduce a novel design in the generator-discriminator pair for improved results and a new refined loss function to enhance details. The generator uses an autoencoder with skip connections, and the inception module is used to capture different scales of spatiotemporal correlation. The discriminator is designed to use global information to determine the reliability and smoothness of the reconstructed curve. The refined loss function is aimed at reducing artifacts introduced by the GAN and ensures better reconstruction quality. A single-degree-of-freedom model is constructed to verify the effectiveness and feasibility of the proposed method. Simulation results demonstrate that the proposed method can accurately reconstruct galloping curves with limited use of sensors, thus meeting the energy efficiency demands of the monitoring system.","2169-3536","","10.1109/ACCESS.2020.2966739","National Natural Science Foundation of China(grant numbers:51877015,61571069); National Natural Science Foundation of China(grant numbers:U1831117); Fundamental Research Funds for the Central Universities(grant numbers:2019CDJGFWDZ001,2019CDXYTX0023); Natural Science Foundation of Chongqing(grant numbers:cstc2018jscx-msyb1002); Open Fund of Guizhou Provincial Key Laboratory of Radio Astronomy and Data Processing(grant numbers:KF201815); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8960387","Galloping curve;conditional generative adversarial network;energy-efficient;transmission line","Power transmission lines;Sensors;Monitoring;Generative adversarial networks;Generators;Conductors;Gallium nitride","curve fitting;electric breakdown;energy conservation;neural nets;overhead line conductors;overhead line mechanical characteristics;power engineering computing;power grids","energy-efficient reconstruction method;transmission lines galloping;conditional generative adversarial network;conductor galloping;power systems;wire fractures;tower collapses;large-scale grid breakdowns;real-time galloping data;conductor dancing prevention;anti-galloping designs;complete galloping data;ill-posed problem;curve reconstruction method;transmission line galloping curves;GAN;generator-discriminator pair;refined loss function;reconstructed curve;reconstruction quality;energy efficiency demands;galloping prevention plans","","4","","38","CCBY","15 Jan 2020","","","IEEE","IEEE Journals"
"Data Augmentation for Imbalanced HRRP Recognition Using Deep Convolutional Generative Adversarial Network","Y. Song; Y. Li; Y. Wang; C. Hu","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing, China; Chongqing Innovation Center, Beijing Institute of Technology, Chongqing, China; Chongqing Innovation Center, Beijing Institute of Technology, Chongqing, China; Radar Research Laboratory, School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IEEE Access","18 Nov 2020","2020","8","","201686","201695","In radar high-resolution range profile (HRRP) recognition, the recognition accuracy will decline when the training samples in some classes (majority classes) greatly outnumbers other classes (minority classes). To alleviate the above imbalanced problem, an HRRP data augmentation framework is proposed. A one-dimensional (1-D) deep convolutional generative adversarial network (DCGAN) is developed to generate artificial HRRPs. The fidelity of the generated HRRPs is evaluated subjectively in the raw data domain and quantitatively by the similarity in the feature domain. The experimental results show that the generated data are similar to the true HRRPs and demonstrate that the proposed framework outperforms the state-of-the-art oversampling methods when handling the imbalanced problem.","2169-3536","","10.1109/ACCESS.2020.3032580","National Key Research and Development Program of China(grant numbers:2018YFE0202101,2018YFE0202102); National Natural Science Foundation of China(grant numbers:61701026,31727901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234497","High resolution range profile (HRRP);imbalanced problem;data augmentation;1-D deep convolutional generative adversarial network (DCGAN)","Generative adversarial networks;Gallium nitride;Generators;Feature extraction;Scattering;Training;Radar","convolutional neural nets;feature extraction;learning (artificial intelligence);radar computing;radar resolution;radar target recognition;sampling methods","HRRP data augmentation framework;imbalanced HRRP recognition;radar high-resolution range profile recognition;DCGAN;one-dimensional deep convolutional generative adversarial network;feature domain;oversampling methods","","4","","50","CCBY","21 Oct 2020","","","IEEE","IEEE Journals"
"Small Target Detection Based on Squared Cross Entropy and Dense Feature Pyramid Networks","Y. Zhang; G. Chen; Z. Cai","School of Computer Science, China University of Geosciences, Wuhan, China; Network and Information Center, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China","IEEE Access","13 Apr 2021","2021","9","","55179","55190","At present, the research hotspot of small target detection mainly focuses on the methods based on deep learning, and such algorithms still have some problems that need to be solved, such as the foreground-background class imbalance and the poor performance of multi-scale detection. For the foreground-background class imbalance, the Squared Cross Entropy (SCE) loss function is proposed here to help solve the problem. Meanwhile, as Feature Pyramid Networks (FPN) is a powerful means to deal with multi-scale detection problems, a new Dense FPN structure is designed based on FPN. The Dense FPN removes the up-sampling process in FPN, and after each feature extraction layer, a continuous convolutional layer with a decreasing number of layers is added. According to the experimental results, Dense FPN outperform the original FPN on various evaluation indicators like  $AP_{S}$ ,  $AP_{M}$  and  $AP_{L}$ , showing the excellent performance of the Dense FPN in dealing with multi-scale detection problems.","2169-3536","","10.1109/ACCESS.2021.3070991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9395110","Feature Pyramid Networks;loss function;target detection;object detection","Feature extraction;Object detection;Training;Neural networks;Generative adversarial networks;Semantics;Generators","convolutional neural nets;deep learning (artificial intelligence);entropy;feature extraction;object detection;sampling methods","multiscale detection problems;small target detection;dense feature pyramid networks;deep learning;foreground-background class imbalance;feature extraction layer;squared cross entropy loss function;dense FPN structure;SCE loss function;up-sampling process;continuous convolutional layer","","3","","25","CCBYNCND","5 Apr 2021","","","IEEE","IEEE Journals"
"Prediction of Typhoon Track and Intensity Using a Generative Adversarial Network With Observational and Meteorological Data","M. Rüttgers; S. Jeon; S. Lee; D. You","Jülich Aachen Research Alliance - Center for Simulation and Data Science (JARA-CSD), Aachen, Germany; Department of Mechanical Engineering, Pohang University of Science and Technology (POSTECH), Pohang, Republic of Korea; Department of Mechanical Engineering, Inha University, Incheon, Republic of Korea; Department of Mechanical Engineering, Pohang University of Science and Technology (POSTECH), Pohang, Republic of Korea","IEEE Access","10 May 2022","2022","10","","48434","48446","To save lives and reduce damage from the destructive impacts of a typhoon, an accurate and fast forecast method is highly demanded. Particularly, predictions for short lead times, known as nowcasting, rely on fast forecasts allowing immediate emergency plannings in the affected areas. In this paper, we propose a generative adversarial network that operates on a single graphics processing unit, to predict both the track and intensity of typhoons for short lead times within fractions of a second. To investigate the effects of meteorological variables on typhoon forecasts, we conducted a parameter study for 6-h track predictions. The results of the study indicate that learning velocity, temperature, pressure, and humidity along with satellite images have positive effects on prediction accuracy. To address the limited access to observational data and facilitate predictions for 12-h intervals, we replaced satellite images with reanalysis data of the total cloud cover and vorticity fields. This replacement led to an increase in data from 76 to 757 typhoons, and it reduced the error of the 6-h track forecasts by 23.5%. The best combination of the parameter study yields track predictions in intervals of 6 and 12 h with the corresponding averaged absolute errors of 44.5 and 68.7 km. Typhoon intensities are predicted by extracting information from generated velocity fields with averaged hit rates of 87.3% and 83.2% for 6- and 12-h interval forecasts, respectively. For typhoons after 1994, tracks and intensities for 12-h intervals are compared to forecasts from the Joint Typhoon Warning Center and Regional Specialized Meteorological Center Tokyo.","2169-3536","","10.1109/ACCESS.2022.3172301","Samsung Research Funding Center of Samsung Electronics; National Research Foundation of Korea (NRF)(grant numbers:NRF-2019K1A3A1A74107685,NRF-2021R1A2C2092146); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9766340","Typhoon track prediction;typhoon intensity prediction;deep learning;nowcasting","Tropical cyclones;Satellites;Generative adversarial networks;Weather forecasting;Generators;Predictive models;Training","atmospheric humidity;atmospheric pressure;atmospheric temperature;clouds;geophysics computing;neural nets;storms;vortices;weather forecasting","Meteorological data;destructive impacts;accurate forecast method;fast forecast method;short lead times;immediate emergency plannings;generative adversarial network;single graphics processing unit;meteorological variables;typhoon forecasts;6-h track predictions;satellite images;prediction accuracy;observational data;reanalysis data;total cloud cover;vorticity fields;track forecasts;parameter study yields;Typhoon intensities;velocity fields;12-h interval forecasts;Joint Typhoon Warning Center;Regional Specialized Meteorological Center Tokyo;Typhoon track;averaged absolute errors","","3","","42","CCBYNCND","3 May 2022","","","IEEE","IEEE Journals"
"Augmenting Seismic Data Using Generative Adversarial Network for Low-Cost MEMS Sensors","A. Wu; J. Shin; J. -K. Ahn; Y. -W. Kwon","School of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; School of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea; Korea Meteorological Administration, Seoul, South Korea; School of Computer Science and Engineering, Kyungpook National University, Daegu, South Korea","IEEE Access","24 Dec 2021","2021","9","","167140","167153","The performance of a deep learning (DL) model depends on sufficient training datasets and its algorithmic structure. Even though seismological research using low-cost micro-electro-mechanical systems (MEMS) sensor received much attention recently, because of the lack of data recorded by such MEMS sensors whose data are usually polluted by different types of noise. Therefore, increasing seismic datasets is required by intelligently generating seismic data through data-augmentation techniques. However, it is difficult to characterize and measure the evolution process of seismic sequences, making the feature extraction and data generation of seismic sequences still a significant challenge. By combining the framework of Generative Adversarial Network (GAN) with long short-term memory (LSTM), attention mechanism and neural network (NN), a novel deep generation model (DGM) named EQGAN is developed to overcome the challenges, which can automatically capture the different time histories and dimension characteristics of seismic sequences, meanwhile stably generating high-quality seismic data. The reality of generated data is qualitatively clarified through the analysis of frequency domain and data autocorrelation distribution. Based on the High-throughput Screening (HTS) Theory, the quantitative evaluation index of statistical metrics is designed, and the generation performance of different machine learning models (standard GAN, LSTM, NN) is compared to prove the stability and effectiveness of EQGAN. The experimental results denote that the EQGAN has excellent stability and performance (up to 81%, much higher than that of other generation models), which provides a suitable data expansion approach for the field of seismological research.","2169-3536","","10.1109/ACCESS.2021.3132901","Development of Earthquake, Tsunami, Volcano Monitoring and Prediction Technology(grant numbers:KMA-135002988); BK21 FOUR project (AI-driven Convergence Software Education Research Program); Ministry of Education, School of Computer Science and Engineering, Kyungpook National University, Korea(grant numbers:4199990214394); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9638494","Deep learning;generative adversarial network;data augmentation;Wasserstein distance","Hidden Markov models;Generative adversarial networks;Earthquakes;Data models;Feature extraction;Training;Deep learning","data handling;deep learning (artificial intelligence);feature extraction;geophysical signal processing;least squares approximations;micromechanical devices;microsensors;recurrent neural nets;seismology","neural network;deep generation model named EQGAN;time histories;seismic sequences;high-quality seismic data;frequency domain;data autocorrelation distribution;generation performance;generation models;suitable data expansion approach;seismological research;generative adversarial network;low-cost MEMS sensors;deep learning model;sufficient training datasets;low-cost microelectro-mechanical systems sensor;seismic datasets;data-augmentation techniques;feature extraction;data generation;long short-term memory attention mechanism;DGM;EQGAN;high-throughput screening;HTS","","1","","52","CCBY","6 Dec 2021","","","IEEE","IEEE Journals"
"GAN-Based Satellite Imaging: A Survey on Techniques and Applications","H. Mansourifar; A. Moskovitz; B. Klingensmith; D. Mintas; S. J. Simske","Department of Systems Engineering, Colorado State University, Fort Collins, CO, USA; VISIMO LLC, Coraopolis, PA, USA; VISIMO LLC, Coraopolis, PA, USA; VISIMO LLC, Coraopolis, PA, USA; Department of Systems Engineering, Colorado State University, Fort Collins, CO, USA","IEEE Access","15 Nov 2022","2022","10","","118123","118140","Satellite image analysis is widely used in many real-time applications, from agriculture to the military. Due to the wide range of Generative Adversarial Network (GAN) applications in multiple areas of satellite imaging, a comprehensive review is required in this area. This paper takes the first step in this direction by categorizing the GAN-based satellite imaging research using seven considerations. We discuss not only the challenges but also future research trends and directions. Among the major findings, we have observed increasing componentization and modularization of GANs to be used as elements of larger systems. In addition to the GAN types used exclusively in each application, we demonstrate the deep neural network architectures used as the generator structure. Eventually, we summarize the results and evaluate the significant impact of GANs on improving performance compared to traditional approaches.","2169-3536","","10.1109/ACCESS.2022.3221123","U.S. Air Force under Topic Number AFX20C-TCS01 Phase II, Proposal(grant numbers:F2-14785-CGAN ISALD); Conditional Generative Adversarial Network for Infinite Synthetic Aerial Landscape Data(grant numbers:FA864921P1505); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944659","Generative adversarial network;geo-localization;image to image translation;road extraction;satellite imaging","Satellites;Imaging;Road traffic;Satellite broadcasting;Generative adversarial networks;Task analysis;Data mining","agriculture;geophysical image processing;image enhancement;neural net architecture;neural nets","challenges but also future research trends;componentization;deep neural network architectures;GAN types;GAN-based satellite imaging;Generative Adversarial Network applications;generator structure;multiple areas;real-time applications;satellite image analysis","","","","95","CCBY","9 Nov 2022","","","IEEE","IEEE Journals"
"Logging Data Completion Based on an MC-GAN-BiLSTM Model","L. Guo; L. Renze; L. Xingyu; T. Juanjuan; C. Lei; Z. Yang","State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, School of Earth Sciences and Technology, Southwest Petroleum University, Chengdu, China; State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, School of Earth Sciences and Technology, Southwest Petroleum University, Chengdu, China; State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, School of Earth Sciences and Technology, Southwest Petroleum University, Chengdu, China; State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, School of Earth Sciences and Technology, Southwest Petroleum University, Chengdu, China; State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, School of Earth Sciences and Technology, Southwest Petroleum University, Chengdu, China; School of Earth Sciences and Technology, Southwest Petroleum University, Chengdu, China","IEEE Access","6 Jan 2022","2022","10","","1810","1822","Due to environmental interference and operational errors, problems such as incomplete and random missing logging data have occurred during the geophysical logging data collection process. Since it is difficult to establish a geophysical model based on logging data and geological information, the data complementation effect of conventional methods is not very satisfactory. In this paper, we propose an MC-GAN-BiLSTM model based on spatiotemporal sequence prediction. In the model, we adopt a generative adversarial network (GAN) as a network framework, and a long short-term memory (LSTM) neural network and a bi-directional long short-term memory (Bi-LSTM) as the basic modules. We use the LSTM instead of a fully-connected layer in the GAN to extract the potential information in the logging data depth domain. We complete the logging data missing values through an encoding-decoding structure that includes the Bi-LSTM. In addition, the generator module also uses multiscale convolution to fully extract the logging data features. We use logging data random missing values and consecutive missing values to simulate a field data acquisition environment and threshold control to simulate a laboratory processing environment for experiments. The experimental results show that the coefficient of determination (R2) of the GAN-LSTM model reaches 0.906 when 30% of random logging data are missing and 0.851 when 30% of consecutive logging data are missing. The effect of the model proposed in this paper is significantly higher than the commonly used random forest (RF), sequence to sequence (seq2seq) and generative adversarial interpolation network (GAIN) models.","2169-3536","","10.1109/ACCESS.2021.3138194","Deep Earthquake Special Project of National Key Research and Development Program(grant numbers:2016YFC0601100); Sichuan Province Science and Technology Support Program(grant numbers:2019CXRC0027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9662304","Logging data;missing value;GAN;LSTM;GAIN;GAN-LSTM","Data models;Neural networks;Logic gates;Generative adversarial networks;Support vector machines;Recurrent neural networks;Predictive models","data acquisition;geophysics computing;learning (artificial intelligence);recurrent neural nets;well logging","MC-GAN-BiLSTM model;incomplete missing logging data;random missing logging data;geophysical logging data collection process;geophysical model;generative adversarial network;long short-term memory neural network;logging data depth domain;Bi-LSTM;random missing values;consecutive missing values;field data acquisition environment;threshold control;GAN-LSTM model;random logging data;consecutive logging data;generative adversarial interpolation network models;logging data completion;spatiotemporal sequence prediction;bi-directional long short-term memory;logging data missing values;encoding-decoding structure;multiscale convolution;logging data feature extraction","","","","34","CCBY","23 Dec 2021","","","IEEE","IEEE Journals"
"Self-Supervised Ground-Roll Noise Attenuation Using Self-Labeling and Paired Data Synthesis","D. A. B. Oliveira; D. G. Semin; S. Zaytsev","IBM Research, Rio de Janeiro, Brazil; Gazprom Neft, Saint Petersburg, Russian Federation; Gazprom Neft, Saint Petersburg, Russian Federation","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","7147","7159","Seismic exploration is a complex process that depends on different sources of information. An essential one is seismic imaging, and much of its interpretation performance relies on high-quality processing, which is currently still very dependent on prone-to-error human mediation. Automation of such processing steps is necessary to reduce the amount of time to treat seismic data—usually months—and improve the outcome overall quality by reducing the inherent subjectivity in the process. One of the most critical steps in seismic processing is noise suppression, and ground roll is one of the most challenging and everyday noises observed in seismic prestack data. In this article, we propose a self-supervised two-step approach to attenuate ground-roll noise in seismic prestack images. First, we detect ground-roll-affected area using convolutional neural networks, and then, we filter ground-roll noise in the detected area using conditional generative adversarial networks (cGANs). For each of these steps, we propose to build paired noisy/noise-free training sets with no supervision or reference data, hence creating a self-supervised pipeline for filtering ground-roll noise. Our two-stage approach enables noise suppression in the affected area while preserving the signal in unaffected areas. In addition, we propose to refactor conventional qualitative metrics in the industry into quantitative scores disregarding any reference data to evaluate ground-roll suppression for different geologies and report reliable results compared with expert filtering.","1558-0644","","10.1109/TGRS.2020.3029914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9235523","Deep learning;geophysical image processing;noise attenuation;self-supervised learning","Training;Attenuation;Noise reduction;Pipelines;Noise measurement;Data models;Geology","filtering theory;geophysical signal processing;geophysical techniques;image classification;image denoising;neural nets;seismic waves;seismology","self-supervised ground-roll noise attenuation;seismic exploration;complex process;seismic imaging;high-quality processing;prone-to-error human mediation;seismic data;seismic processing;noise suppression;ground roll;seismic prestack data;seismic prestack images;ground-roll-affected area;reference data;ground-roll suppression","","5","","25","IEEE","22 Oct 2020","","","IEEE","IEEE Journals"
"Semi-Supervised Deep Transfer Learning-Based on Adversarial Feature Learning for Label Limited SAR Target Recognition","W. Zhang; Y. Zhu; Q. Fu","National Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, National University of Defense Technology, Changsha, China","IEEE Access","25 Oct 2019","2019","7","","152412","152420","The data-driven convolutional neural networks (CNNs) have achieved great progress in Synthetic Aperture Radar automatic target recognition (SAR-ATR) after being trained in a large scale of labeled samples. However, the insufficiency of labeled SAR data always leads to over-fitting, causing significant performance degradation. To solve the mentioned problem, a semi-supervised transfer learning method based on generative adversarial networks (GANs) is presented in the present paper. The discriminator of GAN with an encoder and a discriminative layer is redesigned to make it capable of learning the feature representation of input data with unsupervised settings. Instead of training a deep neural network with the insufficient labeled data set, we first train a GAN with varieties of unlabeled samples to learn generic features of SAR images. Subsequently, the learned parameters are readopted to initialize the target network to transfer the generic knowledge to specific SAR target recognition task. Lastly, the target network is fine-tuned using both the labeled and unlabeled training samples by a semi-supervised loss function. We evaluate the proposed method on the MSTAR and OpenSARShip data set with 80%, 60%, 40%, and 20% of the training set labeled, respectively. The results suggest that the proposed method achieves up to 23.58% accuracy enhancement over the random-initialized model.","2169-3536","","10.1109/ACCESS.2019.2948404","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877822","SAR-ATR;convolutional neural networks (CNNs);semi-supervised transfer learning;adversarial feature learning","Training;Target recognition;Task analysis;Radar polarimetry;Gallium nitride;Feature extraction;Semisupervised learning","convolutional neural nets;image coding;image recognition;object recognition;radar computing;radar imaging;supervised learning;synthetic aperture radar","semisupervised transfer learning method;generative adversarial networks;GAN;feature representation;unsupervised learning;deep neural network;SAR imaging;labeled training samples;unlabeled training samples;semisupervised loss function;adversarial feature learning;label limited SAR target recognition;data-driven convolutional neural networks;synthetic aperture radar automatic target recognition;SAR-ATR;semisupervised deep transfer learning;CNN;MSTAR;OpenSARShip data set;random-initialized model","","22","","25","CCBY","21 Oct 2019","","","IEEE","IEEE Journals"
"A comparison of graph-based semi-supervised learning for data augmentation","W. D. G. d. Oliveira; O. A. B. Penatti; L. Berton","Institute of Science and Technology, Federal University of São Paulo, São José dos Campos, Brazil; Samsung R&D Institute, Campinas, Brazil; Institute of Science and Technology, Federal University of São Paulo, São José dos Campos, Brazil","2020 33rd SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)","25 Nov 2020","2020","","","264","271","In supervised learning, the algorithm accuracy usually improves with the size of the labeled dataset used for training the classifier. However, in many real-life scenarios, obtaining enough labeled data is costly or even not possible. In many circumstances, Data Augmentation (DA) techniques are usually employed, generating more labeled data for training machine learning algorithms. The common DA techniques are applied to already labeled data, generating simple variations of this data. For example, for image classification, image samples are rotated, cropped, flipped or other operators to generate variations of input image samples, and keeping their original labels. Other options are using Neural Networks algorithms that create new synthetic data or to employ Semi-supervised Learning (SSL) that label existing unlabeled data. In this paper, we perform a comparison among graph-based semi-supervised learning (GSSL) algorithms to augment the labeled dataset. The main advantage of using GSSL is that we can increase the training set by adding non-annotated images to the training set, therefore, we can benefit from the huge amount of unlabeled data available. Experiments are performed on five datasets for recognition of handwritten digits and letters (MNIST and EMINIST), animals (Dogs vs Cats), clothes (MNIST-Fashion) and remote sensing images (Brazilian Coffee Scenes), in which we compare different possibilities for DA, including the GSSL, Generative Adversarial Networks (GANs) and traditional Image Transformations (IT) applied on input labeled data. We also evaluated the impact of such techniques on different convolutional neural networks (CNN). Results indicate that, although all DA techniques performed well, GSSL was more robust to different image properties, presenting less accuracy variation across datasets.","2377-5416","978-1-7281-9274-1","10.1109/SIBGRAPI51738.2020.00043","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9265996","Image classification, data augmentation, image transformation, GANs, semi supervised learning, machine learning","Training;Gallium nitride;Semisupervised learning;Drones;Remote sensing;Prediction algorithms;Neural networks","graph theory;image classification;image sampling;learning (artificial intelligence);neural nets","unlabeled data;GSSL;input labeled data;supervised learning;algorithm accuracy;labeled dataset;Data Augmentation techniques;image classification;input image samples;original labels;neural networks algorithms;synthetic data;graph-based semisupervised learning algorithms;adding nonannotated images","","1","","32","IEEE","25 Nov 2020","","","IEEE","IEEE Conferences"
"Unsupervised Domain Adaptation for SAR Target Detection","Y. Shi; L. Du; Y. Guo","National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13 Jul 2021","2021","14","","6372","6385","Recent years have witnessed great progress in synthetic aperture radar (SAR) target detection methods based on deep learning. However, these methods generally assume the training data and test data obey the same distribution, which does not always hold when the radar parameters, imaging algorithm, viewpoints, scenes, etc., change in practice. When such a distribution mismatch occurs, it will cause a significant performance drop. Domain adaptation methods provide an effective way to address this problem by transferring knowledge from the source domain (training data) to the target domain (test data). In this article, we proposed an unsupervised faster R-CNN SAR target detection framework based on domain adaptation, which can improve SAR target detection performance in the unlabeled target domain by borrowing the knowledge of the labeled source domain. Our approach is composed of the following three stages: pixel-domain adaptation (PDA), multilevel feature domain adaptation (MFDA), and iterative pseudolabeling (IPL). By generating transition domain using generative adversarial networks, the PDA stage can reduce the appearance differences of SAR images. At the MFDA stage, the detector can not only learn the domain-invariant global features and instance-level regional features via multilevel adversarial learning in the common feature space but also reweight the low-level global features according to their relative importance to the target domain. At the IPL stage, we design an iterative pseudo labeling strategy that can select pseudo-labels on instance level and image level to encourage the detector to learn more discriminative features of the target domain directly. We evaluate our method using miniSAR and FARADSAR datasets. The experimental results demonstrate the effectiveness of the proposed unsupervised domain adaptation target detection approach.","2151-1535","","10.1109/JSTARS.2021.3089238","National Natural Science Foundation of China(grant numbers:61771362); Higher Education Discipline Innovation Project(grant numbers:B18039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9454400","Adversarial learning;iterative pseudo labeling (IPL);synthetic aperture radar (SAR);target detection;unsupervised domain adaptation","Object detection;Synthetic aperture radar;Radar polarimetry;Feature extraction;Handheld computers;Labeling;Detectors","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);object detection;radar imaging;synthetic aperture radar;unsupervised learning","deep learning;training data;test data;radar parameters;distribution mismatch;significant performance drop;domain adaptation methods;unsupervised faster R-CNN SAR target detection framework;SAR target detection performance;unlabeled target domain;labeled source domain;multilevel feature domain adaptation;transition domain;SAR images;domain-invariant global features;instance-level regional features;low-level global features;instance level;image level;unsupervised domain adaptation target detection approach;synthetic aperture radar;detection methods","","12","","33","CCBY","14 Jun 2021","","","IEEE","IEEE Journals"
"Research and Application of Deep Learning in Image Recognition","Y. Li","Wuhan University, Wuhan, China","2022 IEEE 2nd International Conference on Power, Electronics and Computer Applications (ICPECA)","1 Mar 2022","2022","","","994","999","Deep learning is a technical tool with broad application prospects and has an important role in the field of image recognition. In view of the theoretical value and practical significance of image recognition technology in promoting the development of computer vision and artificial intelligence, this paper will review and study the application of deep learning in image recognition. This paper first outlines the development of icon recognition technology, and then introduces three main learning models in deep learning: convolutional neural networks, recurrent neural networks, and generative adversarial networks, and provides a comparative analysis of these three learning models. Finally, the research results of deep learning image recognition application fields, such as face recognition, medical image recognition, and remote sensing image classification, are analyzed and discussed. This paper also analyze the development trend of deep learning in the field of image recognition, and conclude that the future development direction is the effective recognition of video images and the theoretical strengthening of models.","","978-1-6654-4276-3","10.1109/ICPECA53709.2022.9718847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718847","image recognition;deep learning;application","Deep learning;Training;Analytical models;Image recognition;Recurrent neural networks;Computational modeling;Training data","","","","7","","35","IEEE","1 Mar 2022","","","IEEE","IEEE Conferences"
