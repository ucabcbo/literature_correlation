"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Remote Sensing Image Synthesis via Graphical Generative Adversarial Networks","G. Wang; G. Dong; H. Li; L. Han; X. Tao; P. Ren","College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","10027","10030","We explore the use of graphical generative adversarial networks (Graphical-GAN) for synthesizing remote sensing images. The model is probabilistic graphical based generative adversarial networks (GAN). It pairs a generative network G with a recognition network R. Both of them are adversarially trained with a discriminative network D. Particularly, R is employed to infer the underlying causal relationships among both observed and latent variables from real remote sensing images. The advantages of the Graphical-GAN for synthesizing multiple categories of remote sensing images are two fold. Firstly, it considers the underlying causal relationships and captures the true data distribution of remote sensing images. Secondly, the adversarial learning generates synthetic sensing images that are similar to real ones with slight differences. Our remote sensing image synthesis scheme paves a promising way for remote sensing dataset augmentation, which is an effective means of improving the accuracy of learning models. Experimental results with high Inception Scores (IS) validate the effectiveness of the Graphical-GAN for remote sensing image synthesis.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898915","Remote Sensing Image Synthesis;Graphical Generative Adversarial Networks","Remote sensing;Image synthesis;Training;Generative adversarial networks;Bayes methods;Neural networks;Probabilistic logic","inference mechanisms;learning (artificial intelligence);remote sensing","graphical generative adversarial networks;Graphical-GAN;remote sensing images;generative network;recognition network;causal relationships;synthetic sensing images;remote sensing image synthesis scheme paves;remote sensing dataset augmentation;discriminative network","","5","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Skip Attention GAN for Remote Sensing Image Synthesis","K. Deng; K. Zhang; P. Yao; S. Cheng; P. He","Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","2305","2309","High-quality remote sensing images are difficult to obtain due to limited conditions and high cost for data acquisition. With the development of machine vision and deep learning, some image generation methods (e.g., GANs) are introduced into this field, but it’s still hard to generate images with good texture details and structural dependencies. We establish Skip Attention Mechanism to deal with this problem, which learns dependencies between local points on low-resolution feature maps, and then upsample the attention map and combine it with high-resolution feature maps. With this method, long-range dependencies learned from low-resolution are used for generating remote sensing images with more structural details. We name this method as Skip Attention GAN, which is the first method applying cross-scale attention mechanism for unsupervised remote sensing image generation. Experiments show that our method outperforms previous methods under several metrics. Visual and ablation results of attention layers show that Skip Attention has learned long-distance structural dependencies between similar targets.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414701","Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414701","Attention Mechanism;Remote Sensing Image Synthesis;Generative Adversarial Network","Measurement;Deep learning;Visualization;Image synthesis;Machine vision;Generative adversarial networks;Sensors","computer vision;data acquisition;edge detection;geophysical image processing;image classification;image resolution;image texture;learning (artificial intelligence);remote sensing;robot vision","high-quality remote sensing images;data acquisition;machine vision;deep learning;image generation methods;GANs;good texture details;Skip Attention Mechanism;low-resolution feature maps;attention map;high-resolution feature maps;long-range dependencies;Skip Attention GAN;cross-scale attention mechanism;unsupervised remote sensing image generation;attention layers;long-distance structural dependencies;remote sensing image synthesis","","1","","24","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"PCA-based Wavelet Remote Sensing Image Synthesis Simulation Method","G. Li; H. Yang; J. Wang; Y. Li; C. Zhang; H. Xie; B. Feng","Graduate School, Space Engineering University, Beijing, China; School of Space Information, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China; Graduate School, Space Engineering University, Beijing, China","2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","5 Apr 2021","2021","","","1042","1046","The traditional image mosaic method directly replaces the remote sensing target image into the corresponding area of the background image, but ""splicing traces"" are prone to appear. This paper proposes a method of fusion of remote sensing background and target images under wavelet multi-frequency based on principal component analysis. First, the remote sensing background and target image are decomposed by wavelet, and then the fusion weight of the image wavelet low-frequency coefficient is calculated by the principal component analysis method, and the high-frequency coefficient is fused by the absolute maximum method, and finally the wavelet fusion coefficient is inversely transformed to realize the image synthesis simulation. The simulation results show that the fusion image can not only highlight the target features, but also has no stitching traces.","2689-6621","978-1-7281-8028-1","10.1109/IAEAC50856.2021.9390650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390650","image geometric transformation;image fusion;principal component analysis;wavelet transform","Wavelet transforms;Analytical models;Image synthesis;Splicing;Wavelet analysis;Remote sensing;Principal component analysis","image fusion;image segmentation;inverse transforms;principal component analysis;remote sensing;wavelet transforms","absolute maximum method;wavelet fusion coefficient;fusion image;PCA-based wavelet remote sensing image synthesis simulation method;traditional image mosaic method;remote sensing target image;background image;remote sensing background;wavelet multifrequency;image wavelet low-frequency coefficient;principal component analysis method","","1","","6","IEEE","5 Apr 2021","","","IEEE","IEEE Conferences"
"Towards Generating Remote Sensing Images of the Far Past","M. B. Bejiga; F. Melgani","Department of Information Engineering and Computer science, University of Trento, Trento, Italy; Department of Information Engineering and Computer science, University of Trento, Trento, Italy","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9502","9505","Text-to-image synthesis is a research topic that has not yet been addressed by the remote sensing community. It consists in learning a mapping from text description to image pixels. In this paper, we propose to address this topic for the very first time. More specifically, our objective is to convert ancient text descriptions of geographic areas written by past explorers into an equivalent remote sensing image. To this effect, we rely on generative adversarial networks (GANs) to learn the mapping. GANs aim to represent the distribution of a dataset using weights of a deep neural network, which are trained as an adversarial competition between two networks. We collected ancient texts dating back to 7 BC to train our network and obtained interesting results, which form the basis to highlight future research directions to advance this new topic.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899834","GANs;text-to-image synthesis;remote sensing","Remote sensing;Generators;Generative adversarial networks;Training;Gallium nitride;Cost function;Mathematical model","geophysical image processing;learning (artificial intelligence);neural nets;remote sensing;text analysis;text detection","text-to-image synthesis;text description;image pixels;generative adversarial networks;GAN;deep neural network;remote sensing image generation","","","","18","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Improving Text Encoding for Retro-Remote Sensing","M. B. Bejiga; G. Hoxha; F. Melgani","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","24 Mar 2021","2021","18","4","622","626","A recent work on retro-remote sensing (converting ancient text descriptions into images) was proposed using a multilabel encoding scheme in which an input text description is represented by a binary vector indicating the presence or absence of specific objects. However, this kind of encoding disregards information such as object attributes and spatial relationship between multiple objects in a description, resulting in images that do not semantically (fully) conform to the input description. In this letter, we propose an improved text-encoding mechanism that takes into account different levels of information available from an input text. The encoded text is then used as conditional information to guide the image synthesis process using generative adversarial networks (GANs). Besides, we present a modified GAN architecture intending to improve the semantic content of the generated images. Both the qualitative and quantitative results obtained indicate that the proposed method is particularly promising.","1558-0571","","10.1109/LGRS.2020.2983851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066830","Generative adversarial networks (GANs);multimodal learning;retro-remote sensing;text-to-image synthesis","Gallium nitride;Training;Generators;Generative adversarial networks;Context modeling;Encoding;Sensors","encoding;image coding;neural net architecture;remote sensing;text analysis","input text description;binary vector;object attributes;spatial relationship;encoded text;conditional information;image synthesis;text encoding;retro-remote sensing;ancient text;text-encoding mechanism;multilabel encoding;generative adversarial networks;GAN architecture","","4","","20","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"Hyperspectral Remote Sensing Image Synthesis Based on Implicit Neural Spectral Mixing Models","L. Liu; Z. Zou; Z. Shi","Shanghai Artificial Intelligence Laboratory, Shanghai, China; Department of Guidance, Navigation and Control, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, the Beijing Key Laboratory of Digital Media, and the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","6 Jan 2023","2023","61","","1","14","Hyperspectral image (HSI) synthesis, as an emerging research topic, is of great value in overcoming sensor limitations and achieving low-cost acquisition of high-resolution remote sensing HSIs. However, the linear spectral mixing model used in recent studies oversimplifies the real-world hyperspectral imaging process, making it difficult to effectively model the imaging noise and multiple reflections of the object spectrum. As a prerequisite for hyperspectral data synthesis, accurate modeling of nonlinear spectral mixtures has long been a challenge. Considering the above difficulties, we propose a novel method for modeling nonlinear spectral mixtures based on implicit neural representations (INRs) in this article. The proposed method learns from INR and adaptively implements different mixture models for each pixel according to their spectral signature and surrounding environment. Based on the above neural mixing model, we also propose a new method for HSI synthesis. Given an RGB image as input, our method can generate an accurate and physically meaningful HSI. As a set of by-products, our method can also generate subpixel-level spectral abundance as well as the solar atmosphere signature. The whole framework is trained end-to-end in a self-supervised manner. We constructed a new dataset for HSI synthesis based on a wide range of Airborne Visible Infrared Imaging Spectrometer (AVIRIS) data. Our method achieves a mean peak signal-to-noise ratio (MPSNR) of 52.36 dB and outperforms other state-of-the-art hyperspectral synthesis methods. Finally, our method shows great benefits to downstream data-driven applications. With the HSIs and abundance directly generated from low-cost RGB images, the proposed method improves the accuracy of HSI classification tasks by a large margin, particularly for those with limited training samples.","1558-0644","","10.1109/TGRS.2022.3232705","National Natural Science Foundation of China(grant numbers:62125102); National Key Research and Development Program of China (Titled “Brain-Inspired General Vision Models and Applications”); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10000408","Adaptive spectral mixture model;hyperspectral image (HSI) synthesis;implicit neural representation (INR);remote sensing","Hyperspectral imaging;Image synthesis;Mixture models;Reflection;Atmospheric modeling;Image reconstruction;Data models","geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image processing;image sensors;infrared imaging;learning (artificial intelligence);object detection;remote sensing","accurate HSI;achieving low-cost acquisition;Airborne Visible Infrared Imaging Spectrometer data;different mixture models;emerging research topic;high-resolution remote sensing HSIs;HSI classification tasks;HSI synthesis;hyperspectral data synthesis;hyperspectral image synthesis;hyperspectral remote sensing image synthesis;imaging noise;implicit neural representations;implicit neural spectral mixing models;linear spectral mixing model;mean peak signal-to-noise ratio;neural mixing model;nonlinear spectral mixtures;physically meaningful HSI;real-world hyperspectral imaging process;RGB image;sensor limitations;solar atmosphere signature;spectral signature;state-of-the-art hyperspectral synthesis methods;subpixel-level spectral abundance","","","","90","IEEE","28 Dec 2022","","","IEEE","IEEE Journals"
"Side-Scan Sonar Image Synthesis Based on Generative Adversarial Network for Images in Multiple Frequencies","Y. Jiang; B. Ku; W. Kim; H. Ko","School of Electrical Engineering, Korea University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea; Agency for Defense Development, Jinhae, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea","IEEE Geoscience and Remote Sensing Letters","27 Aug 2021","2021","18","9","1505","1509","The side-scan sonar (SSS) is a critical sensor device used to explore underwater environments in the deep sea. Gathering SSS data, however, is an expensive and time-consuming task because it requires sensor towing and involves complicated field operations. Recently, deep learning has been making advances rapidly in the field of computer vision. Benefiting from this development, generative adversarial networks (GANs) have been demonstrated to produce realistic synthetic data of various types, including images and acoustics signals. In this letter, we propose a GAN-based semantic image synthesis model based on GAN that can generate high-quality SSS images at a low cost in less time. We evaluate the proposed model using both shallow and deep water SSS data sets that include a diverse range of imaging conditions. such as high and low sonar operating frequencies and different landscapes. The experimental results show that the proposed method can effectively generate synthesized SSS data characterized by the shape and style of real data, thereby demonstrating its promising potential for SSS data augmentation in diverse SSS relevant machine learning tasks.","1558-0571","","10.1109/LGRS.2020.3005679","Agency for Defense Development of Korea(grant numbers:UD190005DD); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137342","Generative adversarial network (GAN);image translation;semantic image synthesis;side-scan sonar (SSS)","Image segmentation;Semantics;Generators;Image synthesis;Feature extraction;Sonar;Measurement","geophysical image processing;learning (artificial intelligence);sonar imaging","expensive time-consuming task;sensor towing;complicated field operations;deep learning;computer vision;generative adversarial network;realistic synthetic data;GAN-based semantic image synthesis model;high-quality SSS images;shallow water SSS data;deep water SSS data;imaging conditions;high sonar operating frequencies;low sonar operating frequencies;SSS data augmentation;diverse SSS relevant machine learning tasks;scan sonar image synthesis;side-scan sonar;critical sensor device;underwater environments;deep sea;gathering SSS data","","2","","16","IEEE","9 Jul 2020","","","IEEE","IEEE Journals"
"Physics Guided Remote Sensing Image Synthesis Network for Ship Detection","W. Zhang; R. Zhang; G. Wang; W. Li; X. Liu; Y. Yang; D. Hu","Beijing Institute of Space Mechanics and Electricity, Department of Imaging Technology, Beijing, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Beijing Institute of Space Mechanics and Electricity, Department of Imaging Technology, Beijing, China; Beijing Institute of Space Mechanics and Electricity, Department of Imaging Technology, Beijing, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Beijing Institute of Space Mechanics and Electricity, Department of Imaging Technology, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","","2023","PP","99","1","1","Automatic detection and localization of objects in remote sensing images are of great significance for remote sensing systems. Existing frameworks usually train an object detection network using collected remote-sensing images. However, these models usually perform poorly due to the lack of large-scale training datasets, which is often the case for special remote sensing scenarios, e.g., the detection of ships in the open sea. Although image synthesis is a common strategy to alleviate the issue of data insufficiency, the trained model still performs poorly when being tested on real-world scenes. Aimed at this, a novel sensor-related image synthesis framework, dubbed as RS-ISP, is developed to address the lack of on-orbit remote sensing images. Specifically, our RS-ISP introduces two novel designs to ensure the distribution consistency between the generated images and the real images. 1) The first is a novel pipeline for modeling the physical process of noise production during image capture using specific sensors. 2) The second is the design of a detection-oriented image harmonization model. Similar to the existing design, our model first produces coarse synthetic images by copy-paste operation, on which the proposed harmonization process is used to reduce the variation of the pasted foreground and background. By incorporating these two designs into a unified framework, our RS-ISP is designed and used to produce large-scale synthetic images used to train the object detection model for detecting ships in remote sensing images. Comparative experiments demonstrated that RS-ISP increased the AP@.50 from 0.148 to 0.498 for the ship detection task. Code will be publicly available.","1558-0644","","10.1109/TGRS.2023.3248106","China Academy of Space Technology; National Natural Science Foundation of China(grant numbers:62102069,62220106008,U20B2063); Sichuan Science and Technology Program(grant numbers:2022YFG0032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10050530","Remote Sensing;Ship Detection;Image Synthesis;Image Harmonization;Physical Noise Simulation","Remote sensing;Image synthesis;Image sensors;Data models;Synthetic data;Marine vehicles;Task analysis","","","","","","","IEEE","23 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Retro-Remote Sensing With Doc2Vec Encoding","M. B. Bejiga; G. Hoxha; F. Melgani","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","2 Jun 2020","2020","","","89","92","In this work, we attempt to address the issue of developing a sophisticated text encoder for retro-remote sensing application. The encoder converts ancient landscape descriptions into a fixed-size vector that, adequately, represents the available information. This vector is then used as a conditioning data to a Generative adversarial network (GAN) that synthesizes the equivalent image. We propose using a pre-trained Doc2Vec encoder for text encoding and train a Wasserstein GAN (a variant of GAN) to convert landscape descriptions written by travelers and geographers into the equivalent image. Qualitative and quantitative analysis of the generated images signify usefulness of the proposed method.","","978-1-7281-2190-1","10.1109/M2GARSS47143.2020.9105139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105139","Deep learning;Generative adversarial networks;Retro-remote sensing;Text embedding;Text-to-image synthesis","Knowledge engineering;Image coding;Statistical analysis;Semantics;Geoscience and remote sensing;Machine learning;Generative adversarial networks","geophysical image processing;image coding;neural nets;remote sensing;text analysis;vectors","ancient landscape descriptions;fixed-size vector;text encoding;Wasserstein GAN;text encoder;retro-remote sensing;Doc2Vec encoder;generative adversarial network;Doc2Vec encoding","","2","","11","IEEE","2 Jun 2020","","","IEEE","IEEE Conferences"
"Cloud Removal of Satellite Images Using Convolutional Neural Network With Reliable Cloudy Image Synthesis Model","K. -Y. Lee; J. -Y. Sim","School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea; School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","3581","3585","Cloudy pixels in satellite images degrade the visibility of captured surface structure. We propose a novel cloudy image synthesis model and develop a cloud removal algorithm using convolutional neural network. We extract the cloud masks from real cloudy satellite images and from real sky images with clouds. Then we investigate the characteristics of real cloudy images and devise a reliable cloudy image synthesis model which considers the background surface color, misalignement of channel images, and blur in clouds. We train a hierarchical cloud removal network using the synthetic cloudy images. Experimental results demonstrate that the proposed algorithm removes the clouds from cloudy satellite images faithfully and outperforms the existing methods.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803666","Cloud removal;satellite image;convolutional neural network","Clouds;Cloud computing;Satellites;Image color analysis;Remote sensing;Image synthesis;Surface structures","clouds;geophysical image processing;image classification;image colour analysis;image denoising;image resolution;infrared imaging;neural nets;remote sensing","convolutional neural network;reliable cloudy image synthesis model;cloudy pixels;novel cloudy image synthesis model;cloud removal algorithm;cloud masks;cloudy satellite images;sky images;channel images;hierarchical cloud removal network;synthetic cloudy images","","8","","14","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Change Detection for High Resolution Remote Sensing Image Based on Co-saliency Strategy","Q. Guo; J. Zhang","School of Electronics and Information Engineering, Harbin Institute of Technology, China; School of Electronics and Information Engineering, Harbin Institute of Technology, China","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)","14 Oct 2019","2019","","","1","4","Change detection for remote sensing image is of great significance to a diverse range of applications. From the point of object-based method, this paper provides a change detection algorithm based on co-saliency strategy for multitemporal high resolution images. Firstly, the final difference image fused by difference feature and log difference feature, is generated, and feature image including spatial and contextual information is obtained by Gabor wavelet transform. Secondly, co-saliency strategy is performed via cluster-based method, combining the final difference image with feature difference image at each temporal data, and highlighting the common regions as the changed directly. Finally, actual change map is extracted by fuzzy local information C-means clustering algorithm (FLICM) and decision-voted method. The experiments show the method proposed in this paper has a superior performance in change detection for high resolution remote sensing images.","","978-1-7281-4615-7","10.1109/Multi-Temp.2019.8866911","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866911","Gabor wavelet transform;co-saliency strategy;change detection;high resolution image","Feature extraction;Remote sensing;Image resolution;Wavelet transforms;Change detection algorithms;Image synthesis","fuzzy set theory;geophysical image processing;image resolution;pattern clustering;remote sensing;wavelet transforms","high resolution remote sensing image;co-saliency strategy;object-based method;change detection algorithm;multitemporal high resolution images;cluster-based method;feature difference image;actual change map;decision-voted method;log difference feature;Gabor wavelet transform;fuzzy local information C-means clustering algorithm","","","","10","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Remote Sensing Data Augmentation Through Adversarial Training","N. Lv; H. Ma; C. Chen; Q. Pei; Y. Zhou; F. Xiao; J. Li","The State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, China; The State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, China; The State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, China; The State Key Laboratory of Integrated Service Networks, Xidian University, Xi'an, China; The Ministry of water resources of China, Beijing, China; The Ministry of water resources of China, Beijing, China; The Ministry of water resources of China, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2511","2514","In this paper, a Generative Adversarial Network(GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangsu province in China, i.e., D-sGAN(Deeply-supervised GAN). At First, to modulate the layer activations, a down-sampling scheme is designed based on the segmentation map. Then, the architecture of the generator is UNet++ with the proposed down-sampling module. Next, the generator of this net is deeply supervised by the discriminator using deep Convolutional Neural Network(CNN). This paper further proved that the proposed down-sampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with a better Fully Convolutional Networks Score(FCN-Score) compared to the GoGAN, SimGAN and CycleGAN models.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324263","National Key Research and Development Program of China(grant numbers:2018YFE0126000); National Natural Science Foundation of China(grant numbers:61571338,U1636209,61672131); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324263","data augmentation;GAN;deep supervision;down-sampling","Generators;Semantics;Remote sensing;Image segmentation;Gallium nitride;Training;Image synthesis","image classification;image segmentation;learning (artificial intelligence);neural nets;remote sensing","down-sampling module;remote sensing images;segmentation network;Fully Convolutional Networks Score;sensing data augmentation;Adversarial training;Jiangsu province;Deeply-supervised GAN;layer activations;down-sampling scheme;segmentation map","","","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Physics-Informed Hyperspectral Remote Sensing Image Synthesis With Deep Conditional Generative Adversarial Networks","L. Liu; W. Li; Z. Shi; Z. Zou","State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; Department of Guidance, Navigation and Control, School of Astronautics, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","18 May 2022","2022","60","","1","15","High-resolution hyperspectral remote sensing images are of great significance to agricultural, urban, and military applications. However, collecting and labeling hyperspectral images are time-consuming, expensive, and usually heavily rely on domain knowledge. In this article, we propose a new method for generating high-resolution hyperspectral images and subpixel ground-truth annotations from RGB images. Given a single high-resolution RGB image as its conditional input, unlike previous methods that directly predict spectral reflectance and ignores the physics behind it, we consider both imaging mechanism and spectral mixing, introduce a deep generative network that first recovers the spectral abundance for each pixel, and then generate the final spectral data cube with the standard USGS spectral library. In this way, our method not only synthesizes high-quality spectral data existing in the real world but also generates subpixel-level spectral abundance with well-defined spectral reflectance characteristics. We also introduce a spatial discriminative network and a spectral discriminative network to improve the fidelity of the synthetic output from both spatial and spectral perspectives. The whole framework can be trained end-to-end in an adversarial training paradigm. We refer to our method as “Physics-informed Deep Adversarial Spectral Synthesis (PDASS).” On the IEEE grss_dfc_2018 dataset, our method achieves an MPSNR of 47.56 on spectral reconstruction accuracy and outperforms other state-of-the-art methods. As latent variables, the generated spectral abundance and the atmospheric absorption coefficients of sunlight also suggest the effectiveness of our method.","1558-0644","","10.1109/TGRS.2022.3173532","National Natural Science Foundation of China(grant numbers:62125102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770778","Generation adversarial networks (GANs);hyperspectral image;imaging model;remote sensing;spectral super-resolution (SSR)","Hyperspectral imaging;Superresolution;Atmospheric modeling;Image reconstruction;Absorption;Spatial resolution;Libraries","geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image colour analysis;image reconstruction;image resolution;remote sensing;spectral analysis","Physics-informed hyperspectral remote sensing image synthesis;deep conditional generative adversarial networks;high-resolution hyperspectral remote sensing images;agricultural applications;urban, applications;military applications;high-resolution hyperspectral images;subpixel ground-truth annotations;RGB images;single high-resolution RGB image;conditional input;imaging mechanism;spectral mixing;deep generative network;final spectral data cube;standard USGS spectral library;high-quality spectral data;subpixel-level spectral abundance;well-defined spectral reflectance characteristics;spatial discriminative network;spectral discriminative network;spatial perspectives;spectral perspectives;adversarial training paradigm;Physics-informed Deep Adversarial Spectral Synthesis;spectral reconstruction accuracy;generated spectral abundance","","1","","79","IEEE","9 May 2022","","","IEEE","IEEE Journals"
"Exploring the Potential of Unsupervised Image Synthesis for SAR-Optical Image Matching","W. -L. Du; Y. Zhou; J. Zhao; X. Tian; Z. Yang; F. Bian","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the People's Republic of China, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau; DFH Satellite Company Ltd., Beijing, China; DFH Satellite Company Ltd., Beijing, China","IEEE Access","18 May 2021","2021","9","","71022","71033","We consider SAR-optical image matching problems, where correspondences are acquired from a pair of SAR and optical images. Recent methods for such a problem typically simplify the SAR-optical image matching to the SAR-SAR or optical-optical image matchings using supervised-image-synthesis methods. However, training supervised-image-synthesis needs plenty of aligned SAR-optical image pairs while gathering sufficient amounts of aligned multi-modal image pairs is challenging in remote sensing. In this work, we investigate the applicability of unsupervised-image-synthesis for SAR-optical image matching such that the unaligned SAR-optical images could be used. To this end, we apply feature matching loss to a well known unsupervised-image-synthesis method, i.e., CycleGAN, to enforce the feature matching consistency. Moreover, we develop a shared-matching-strategy to improve the results of SAR-optical image matching further. Qualitative comparisons against CycleGAN, StarGAN, and DualGAN demonstrate the superiority of our approach. Quantitative results show that, compared with CycleGAN, StarGAN, and DualGAN, our method obtains at least 2.6 times more qualified SAR-optical matchings.","2169-3536","","10.1109/ACCESS.2021.3079327","National Natural Science Foundation of China(grant numbers:62002360,61806206,61772530); opening fund of State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology (Macau FDCT)(grant numbers:119/2017/A3); Science and Technology Development Fund of Macau(grant numbers:0038/2020/A1); Fundamental Research Funds for the Central Universities(grant numbers:2020ZDPY0305); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201346,BK20180639); Six Talent Peaks Project in Jiangsu Province(grant numbers:2015-DZXX-010,2018-XYDXX-044); China Postdoctoral Science Foundation(grant numbers:2020M681765); Jiangsu Province Postdoctoral Research Foundation(grant numbers:2020Z178); Xuzhou Science and Technology Program(grant numbers:KC18061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427486","Image matching;unsupervised-image-synthesis;synthetic aperture radar (SAR);generative adversarial networks (GANs)","Optical imaging;Image matching;Adaptive optics;Optical sensors;Radar polarimetry;Nonlinear optics;Image synthesis","geophysical image processing;image matching;image registration;optical images;radar imaging;remote sensing;synthetic aperture radar","unsupervised image synthesis;SAR-optical image matching problems;SAR-SAR;optical-optical image matchings;supervised-image-synthesis methods;training supervised-image-synthesis;aligned SAR-optical image pairs;aligned multimodal image pairs;unaligned SAR-optical images;known unsupervised-image-synthesis method;2.6 times more qualified SAR-optical matchings","","5","","57","CCBY","11 May 2021","","","IEEE","IEEE Journals"
"DETGAN: GAN for Arbitrary-oriented Object Detection in Remote Sensing Images","S. Cheng; P. Yao; K. Deng; L. Fu","Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China","2022 Asia Conference on Algorithms, Computing and Machine Learning (CACML)","19 Aug 2022","2022","","","337","341","Object detection in remote sensing images has be-come a research focus in recent years with the development of deep learning. However, due to objective reasons such as weather, cost, etc., we can hardly obtain abundant high-quality remote sensing images, especially for specific targets, which severely limits the training of the object detector, leading to poor detection performance. Thus for the first time, this paper introduces the Generative Adversarial Networks(GANs) for arbitrary-oriented object detection in remote sensing images, by augmenting the dataset to improve the performance of detectors. We construct DETGAN with two-layer self-attention modules to capture long-distance dependence for high-quality image generation. To solve the mismatch between generated slices and the samples for detectors, we propose the GAN-to-Detection transfer strategy, in which the slices are inserted into a background with the same size as the samples for detectors and then added to the training set. Experiments show that the performance of ship detectors is successfully improved with the transfer strategy, and demonstrate that GAN is an effective way to alleviate the problem of data insufficiency in remote sensing image object detection.","","978-1-6654-8290-5","10.1109/CACML55074.2022.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852535","Generative Adversarial Network;remote sensing;arbitrary-oriented object detection;transfer strategy","Training;Machine learning algorithms;Image synthesis;Detectors;Object detection;Generative adversarial networks;Sensors","geophysical image processing;learning (artificial intelligence);object detection;remote sensing;ships","GAN-to-Detection transfer strategy;remote sensing image object detection;arbitrary-oriented object detection;objective reasons;high-quality remote sensing images;object detector;poor detection performance;high-quality image generation","","","","25","IEEE","19 Aug 2022","","","IEEE","IEEE Conferences"
"Retro-Remote Sensing: Generating Images From Ancient Texts","M. B. Bejiga; F. Melgani; A. Vascotto","Department of Computer Science and Information Engineering, University of Trento, Trento, Italy; Department of Computer Science and Information Engineering, University of Trento, Trento, Italy; Department of Computer Science and Information Engineering, University of Trento, Trento, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Mar 2019","2019","12","3","950","960","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area.","2151-1535","","10.1109/JSTARS.2019.2895693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660422","Convolutional neural networks (CNN);deep learning;generative adversarial networks (GAN);multimodal learning;remote sensing;text-to-image synthesis","Remote sensing;Gallium nitride;Earth;Sensors;Generators;Satellites;Technological innovation","convolutional neural nets;geophysical image processing;image retrieval;learning (artificial intelligence);remote sensing;statistical analysis;text analysis","retro-remote sensing;ancient texts;data modality;multimodal learning models;text-to-image synthesis;ancient text descriptions;generative adversarial networks;GANs;deep neural network;satellite images;image generation;statistical properties;image captioning;information extraction","","11","","34","IEEE","5 Mar 2019","","","IEEE","IEEE Journals"
"Physical-aware Radar Image Synthesis with Projective Network","Q. Song; F. Xu; X. X. Zhu","Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany; Key Laboratory for Information Science of Electromagnetic Waves (MoE), Fudan University, Shanghai, China; Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany","2021 XXXIVth General Assembly and Scientific Symposium of the International Union of Radio Science (URSI GASS)","14 Oct 2021","2021","","","1","4","This paper proposed a new network module named as projection network, which explicitly combined radar's projection process with trainable network. It assumes that each 2D radar cross section (RCS) map is a projection of a 3D RCS map. And it models the projection mechanism as a differentiable layer, so that it can be integrated with other neural network layers, such as convolutional and pooling layers. The proposed model is consistent with radar projection process, hence effects such as layover is considered. It is designed and used specifically for radar applications. This paper applied the proposed network on radar image synthesis, and the simulation results showed great potential of projective network.","2642-4339","978-9-4639-6-8027","10.23919/URSIGASS51995.2021.9560559","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560559","","Graphics;Solid modeling;Radar cross-sections;Three-dimensional displays;Image synthesis;Simulation;Neural networks","neural nets;radar cross-sections;radar imaging;synthetic aperture radar","projection network;trainable network;2D radar cross section map;3D RCS map;projection mechanism;neural network layers;radar projection process;radar applications;projective network;physical-aware radar image synthesis;network module","","","","9","","14 Oct 2021","","","IEEE","IEEE Conferences"
"Potential of the Reverse Synthesis Method for the High-Quality SAR Image Synthesis","E. Shiro","Independent researcher, Toronto, Canada","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8905","8908","A potential of a new Reverse synthesis method proposed at IGARSS 2017 for the high-quality Synthetic Aperture Radar (SAR) image synthesis is presented. Images produced by the method are compared with the best existing approaches for the speckle noise reduction. Further capabilities for the image quality improvement like side lobe, range and azimuth reduction, contrast improvement, autofocusing and target detectability improvement are considered. The novel approach allows both: to produce high quality and high-resolution images from existing SAR raw data and to create new high-quality systems with reduced demands to the on-board equipment.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518457","high resolution;image quality;image synthesis;SAR;speckle noise;synthetic aperture imaging;synthetic aperture radar","Speckle;Image resolution;Synthetic aperture radar;Buildings;Automobiles;Image generation;Image edge detection","image denoising;radar imaging;speckle;synthetic aperture radar","high-quality synthetic aperture radar image synthesis;side lobe;range-and-azimuth reduction;high-quality systems;SAR raw data;high-resolution images;target detectability improvement;autofocusing;contrast improvement;image quality improvement;speckle noise reduction;IGARSS 2017;high-quality SAR image synthesis;reverse synthesis method","","","","5","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Recovering Thin Cloud Covered Regions in Gf Satellite Images Based on Cloudy Image Arithmetic +","Z. Xu; K. Wu; P. Ren","College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1800","1803","We propose the Cloudy Image Arithmetic + (CIA +) for training dataset construction of thin cloud removal, which addresses the deficiency of Cloud Image Arithmetic (CIA) that cloud shadows cannot be simulated. CIA + is able to synthesize cloudy images with cloud shadows, and as in nature, the angle and intensity of the cloud shadows vary depending on the irradiation angle and cloud thickness, which achieves state-of-the-art cloudy image synthesis. The first thin cloud removal dataset on GF satellite (TCR-GF) constructed with CIA + is released to supplement public data for cloud removal. Meanwhile, we propose the Dual-attention MSGAN to remove thin clouds. The network is capable to focus on thin cloud covered regions for the coordinate attention module encodes both channel relationship and long-range dependencies with precise position information. Several qualitative and quantitative experiments validate that the Dual-attention performs excellently on our TCR-GF dataset.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884528","Cloudy Image Arithmetic + (CIA +);Cloudy Shadows Synthesis;Thin Cloud Removal;Coordinate Attention","Training;Radiation effects;Satellites;Image synthesis;Clouds;Geoscience and remote sensing;Arithmetic","clouds;geophysical image processing;remote sensing","thin cloud covered regions;gf satellite images;Cloudy Image Arithmetic;CIA;training dataset construction;Cloud Image Arithmetic;cloud shadows;cloudy images;state-of-the-art cloudy image synthesis;cloud removal dataset;TCR-GF dataset","","1","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Remote Sensing Data Augmentation Through Adversarial Training","N. Lv; H. Ma; C. Chen; Q. Pei; Y. Zhou; F. Xiao; J. Li","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Ministry of Water Resources of China, Beijing, China; Ministry of Water Resources of China, Beijing, China; Ministry of Water Resources of China, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Sep 2021","2021","14","","9318","9333","The lack of remote sensing images and poor quality limit the performance improvement of follow-up research such as remote sensing interpretation. In this article, a generative adversarial network (GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangxi and Anhui Provinces in China, i.e., deeply supervised GAN (D-sGAN). D-sGAN can generate high-quality images that are rich in changes, greatly shorten the generation time, and provide data support for applications such as semantic interpretation of remote sensing images. First, to modulate the layer activations, a downsampling scheme is designed based on the segmentation map. Then, the architecture of the generator is Unet++ with the proposed downsampling module. Next, the generator of this net is deeply supervised by the discriminator using deep convolutional neural network. This article further proved that the proposed downsampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with the faster generation speed compared to the CoGAN, SimGAN, and CycleGAN models. Furthermore, the remote sensing data generated by the model helped the interpretation network to increase the accuracy by 9%, meeting actual generation requirements.","2151-1535","","10.1109/JSTARS.2021.3110842","National Key R&D Program of China(grant numbers:2020YFB1807500); National Natural Science Foundation of China(grant numbers:62072360,62001357,61672131,61901367); Key Research and Development Plan of Shaanxi province(grant numbers:2021ZDLGY02-09,2020JQ-844); Key Laboratory of Embedded System and Service Computing(grant numbers:Tongji University,ESSCKF2019-05); Ministry of Education, Xi'an Science and Technology Plan(grant numbers:20RGZN0005); Xi'an Key Laboratory of Mobile Edge Computing and Security(grant numbers:201805052-ZD3CG36); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9531488","Data augmentation;deep supervision;downsampling;GAN","Remote sensing;Generative adversarial networks;Generators;Semantics;Training;Task analysis;Image synthesis","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image segmentation;learning (artificial intelligence);remote sensing","remote sensing images;remote sensing data;sensing data augmentation;remote sensing interpretation;generative adversarial network;high-quality images;downsampling module;adversarial training;Jiangxi provinces;Anhui provinces;China;deeply supervised GAN;D-sGAN;Unet++;deep convolutional neural network;CoGAN;SimGAN;CycleGAN models;interpretation network;generation requirements","","15","","47","CCBY","8 Sep 2021","","","IEEE","IEEE Journals"
"Unsupervised Change Detection Based on Weighted Change Vector Analysis and Improved Markov Random Field for High Spatial Resolution Imagery","H. Fang; P. Du; X. Wang; C. Lin; P. Tang","Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, the Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing University, Nanjing, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, the Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing University, Nanjing, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, the Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing University, Nanjing, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, the Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing University, Nanjing, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, the Key Laboratory for Land Satellite Remote Sensing Applications of Ministry of Natural Resources, School of Geography and Ocean Science, Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing University, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","24 Dec 2021","2022","19","","1","5","Change detection is a research hotspot in the remote sensing field. In this letter, an unsupervised change detection method was proposed by optimizing two critical steps, i.e., the generation and analysis of difference image. First, the difference vectors of features are calculated using the simple differencing method. Some changed and unchanged pixels are generated by the majority voting on the results produced by clustering the difference vectors and then are used for the weight calculation of difference vectors. The weights are calculated by means of F-Score and considered in the weighted change vector analysis to produce a discriminative difference image. Finally, the change map is obtained by the improved Markov random field which takes the difference in the neighborhood pixel values into account. Experimental results on three data sets demonstrated that the proposed method outperformed six unsupervised change detection methods in terms of overall accuracy.","1558-0571","","10.1109/LGRS.2021.3059461","Natural Science Foundation of China(grant numbers:41631176); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9365702","Change detection;change vector analysis (CVA);difference image;Markov random field (MRF);remote sensing","Spatial resolution;Remote sensing;Feature extraction;Standards;Satellites;Training;Image synthesis","","","","3","","20","IEEE","1 Mar 2021","","","IEEE","IEEE Journals"
"Satellite image formation and preprocessing methods","T. Kuchkorov; S. Urmanov; M. Kuvvatova; I. Anvarov","Computer Engineering Tashkent University of Information Technologies named after Muhammad al-Khwarizmi, Tashkent, Uzbekistan; Computer Engineering Tashkent University of Information Technologies named after Muhammad al-Khwarizmi, Tashkent, Uzbekistan; Computer Engineering Nukus branch of Tashkent University of Information Technologies named after Muhammad al-Khwarizmi, Nukus, Uzbekistan; Remote sensing Electronics Mikroelektronikaplus, Tashkent, Uzbekistan","2020 International Conference on Information Science and Communications Technologies (ICISCT)","19 Feb 2021","2020","","","1","4","This paper gives a conceptual and practical introduction to the field of satellite image formation and preprocessing. In particular, the process of satellite image generation is discussed going through each step of its lifecycle. A special explanatory emphasis is given to the process of remote sensing and further mapping and fusion of satellite imagery within acquired segments of electromagnetic spectrum. Moreover, methods of satellite imagery retrieval have been practically demonstrated. The satellite imagery data has been extracted from Sentinel 2 dataset and analyzed with Python language image processing libraries.","","978-1-7281-9969-6","10.1109/ICISCT50599.2020.9351456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351456","Computer vision;satellite image;image formation;image preprocessing;NDVI;Sentinel 2;satellite;imagery;python;sentinel;remote sensing","Information science;Satellites;Image synthesis;Wavelength measurement;Libraries;Sensors;Remote sensing","artificial satellites;geophysical image processing;image processing;Python;remote sensing","conceptual introduction;practical introduction;satellite image formation;satellite image generation;special explanatory emphasis;satellite imagery retrieval;satellite imagery data;preprocessing methods;Sentinel 2 dataset;Python language image processing libraries","","3","","5","IEEE","19 Feb 2021","","","IEEE","IEEE Conferences"
"A new method of high resolution SAR image synthesis reducing speckle noise","E. Shiro","Toronto, Canada","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5374","5377","A new “reverse synthesis” method for the speckle noise reduction conducted concurrently with the SAR image synthesis is proposed. The method keeps high resolution for high-quality pixels and averages by neighbors the pixels affected by the speckle noise. The method is based on analysis of the quality of each pixel to resemble the “corner reflector” reflection property. The approach was tested with the raw echo data from the Soviet Almaz-1A SAR satellite.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128218","SAR;speckle noise","Image resolution;Speckle;Earth;Image generation;Reflection;Buildings;Noise reduction","geophysical image processing;image denoising;image resolution;remote sensing by radar;synthetic aperture radar","corner reflector reflection property;Soviet Almaz-1A SAR satellite;neighbors the pixels;high-quality pixels;speckle noise reduction;reverse synthesis method;high resolution SAR image synthesis","","3","","1","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"HDR Synthesis Technology for Spaceborne CMOS Cameras Based on Virtual Digital TDI","B. Zhan; F. Li; M. Lu","Qian Xuesen Laboratory of Space Technology, China Association for Science and Technology, Beijing, China; Qian Xuesen Laboratory of Space Technology, China Association for Science and Technology, Beijing, China; Qian Xuesen Laboratory of Space Technology, China Association for Science and Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Jul 2020","2020","13","","3824","3833","Due to the fact that the traditional high dynamic range (HDR) imaging methods cannot be used for satellites, finding a way to generate HDR remote sensing images from the satellites has long been explored in the field of remote sensing imaging. In this article, a systematic method of synthesizing the HDR remote sensing images based on a new registration algorithm and virtual digital delay integration (TDI) technology is proposed. First, a series of original images are generated by the fast continuous shooting method through a push-broom spaceborne camera. Then, a new registration algorithm with high accuracy and high robustness is proposed in this article, which is used for image registration. Finally, an HDR multiframe image synthesis algorithm is used to generate high-quality and high signal-to-noise-ratio HDR images. This technology greatly improves the image information acquisition capabilities of digital TDI area scan cameras.","2151-1535","","10.1109/JSTARS.2020.3005667","National Key Research and Development Projects(grant numbers:2016YFB0501301); National Natural Science Foundation of China(grant numbers:61773383); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9128021","High-resolution imaging;image processing;image registration algorithms;optical imaging","Remote sensing;Cameras;Image synthesis;Satellites;Image registration;CMOS technology","cameras;CMOS image sensors;geophysical image processing;geophysical techniques;image registration;remote sensing","spaceborne CMOS cameras;HDR synthesis technology;digital TDI area scan cameras;image information acquisition capabilities;HDR multiframe image synthesis algorithm;image registration;push-broom spaceborne camera;fast continuous shooting method;original images;virtual digital delay integration technology;registration algorithm;systematic method;remote sensing imaging;HDR remote sensing images;satellites;traditional high dynamic range imaging methods","","3","","31","CCBY","29 Jun 2020","","","IEEE","IEEE Journals"
"Image Generation Based on Texture Guided VAE-AGAN for Regions of Interest Detection in Remote Sensing Images","L. Zhang; Y. Liu","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","2310","2314","Deep learning has shown great strength in regions of interest (ROIs) detection for remote sensing images (RSIs). However, for most of RSIs, the unbalanced distribution of positive and negative samples greatly limits the performance of the deep learning-based methods. To cope with this issue, we propose a novel method based on texture guided variational autoencoder-attention wise generative adversarial network (VAE-AGAN) to augment the training data for ROI detection. First, to generate realistic texture details of RSIs, we propose a texture guidance block to embed texture prior information into encoder and decoder networks. Second, we introduce the channel and spatial-wise attention layers in the discriminator construct to adaptively recalibrate the varying importance of different channels and spatial regions of input RSIs. Finally, we apply the RSI dataset balanced by our proposal to the weakly supervised ROI detection method. Experimental results demonstrate that the proposal can not only improve the performance of ROI detection, but also outperform other competing augmentation methods.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413823","National Natural Science Foundation of China; Beijing Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413823","Image generation;texture guidance;attention;generative adversarial networks","Learning systems;Deep learning;Image synthesis;Training data;Signal processing;Generative adversarial networks;Decoding","deep learning (artificial intelligence);image texture;remote sensing","remote sensing images;unbalanced distribution;positive samples;negative samples;deep learning;texture guidance block;spatial-wise attention layers;RSI;weakly supervised ROI detection method;image generation;texture guided VAE-AGAN;texture guided variational autoencoder;attention wise generative adversarial network","","1","","23","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Change Detection of Tidal Flat Images based on Siamese Network","Z. Mei; W. Zheng; Y. Fan; W. Hu","School of Data Science Engineering, East China Normal University, Shanghai, China; Information Technology Services, East China Normal University, Shanghai, China; School of Data Science Engineering, East China Normal University, Shanghai, China; School of Data Science Engineering, East China Normal University, Shanghai, China","2022 4th International Conference on Advances in Computer Technology, Information Science and Communications (CTISC)","15 Aug 2022","2022","","","1","7","In recent times, deep learning-based method have attained great success in the problem of change detection in tidal flats’s satellite remote sensing images. However, the observation of changes requires processing multiple time series images, which leads to excessive computation of deep learning models. In addition, remote sensing images often have problems such as missing pixels, which challenges the receptive field of the model. In this paper, we advance a innovative method of MSIC mosaic synthesis, which is used to synthesize the multi-series remote sensing satellite images into the highest and lowest tide level maps. The MSIC mosaic synthesis greatly reduces the computational complexity of the model. In the meantime, a model based on the Siamese network structure is also proposed, which takes bi-temporal images as input, and improves the model effect by fusing the feature pyramid module. We use the self-made landsat8 satellite remote sensing images as data to conduct experiments, and achieve a dice coefficient value of 72.8%.Experimental results show that our model obtain remarkable performance, and exceeds several state-of-the-art methods.","","978-1-6654-5872-6","10.1109/CTISC54888.2022.9849717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849717","remote sensing images;Siamese network;MSIC mosaic synthesis;feature pyramid","Learning systems;Image segmentation;Information science;Satellites;Image synthesis;Fuses;Computational modeling","geophysical image processing;image classification;image fusion;image representation;image resolution;learning (artificial intelligence);remote sensing;time series","highest tide level maps;lowest tide level maps;MSIC mosaic synthesis;computational complexity;Siamese network structure;bi-temporal images;model effect;landsat8 satellite remote;change detection;tidal flat images;deep learning-based method;tidal flats;multiple time series images;excessive computation;deep learning models;remote sensing images;receptive field;innovative method;multiseries remote sensing satellite images","","","","25","IEEE","15 Aug 2022","","","IEEE","IEEE Conferences"
"Synthetic Aperture Radar Image Synthesis by Using Generative Adversarial Nets","J. Guo; B. Lei; C. Ding; Y. Zhang","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","22 Jun 2017","2017","14","7","1111","1115","Synthetic aperture radar (SAR) image simulators based on computer-aided drawing models play an important role in SAR applications, such as automatic target recognition and image interpretation. However, the accuracy of such simulators is due to geometric error and simplification in the electromagnetic calculation. In this letter, an end-to-end model was developed that could directly synthesize the desired images from the known image database. The model was based on generative adversarial nets (GANs), and its feasibility was validated by comparisons with real images and ray-tracing results. As a further step, the samples were synthesized at angles outside of the data set. However, the training process of GAN models was difficult, especially for SAR images which are usually affected by noise interference. The major failure modes were analyzed in experiments, and a clutter normalization method was proposed to ameliorate them. The results showed that the method improved the speed of convergence up to 10 times. The quality of the synthesized images was also improved.","1558-0571","","10.1109/LGRS.2017.2699196","National Natural Science Foundation of China(grant numbers:61331017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927706","Generative adversarial nets (GANs);SAR image simulator;SAR interpretation;synthetic aperture radar (SAR)","Training;Synthetic aperture radar;Clutter;Gallium nitride;Solid modeling;Ray tracing;Biological system modeling","learning (artificial intelligence);radar clutter;radar computing;radar imaging;synthetic aperture radar","synthetic aperture radar image synthesis;generative adversarial nets;SAR image simulators;computer-aided drawing models;geometric error;electromagnetic calculation;end-to-end model;image database;GAN model training process;noise interference;failure modes;clutter normalization method;synthesized image quality","","63","1","10","IEEE","15 May 2017","","","IEEE","IEEE Journals"
"Subpixel Change Detection Based on Improved Abundance Values for Remote Sensing Images","Z. Li; W. Shi; C. Zhang; J. Geng; J. Huang; Z. Ye","College of Civil Engineering, Hefei University of Technology, Hefei, China; Department of Land Surveying and Geo-informatics, The Hong Kong Polytechnic University, Hong Kong; College of Civil Engineering, Hefei University of Technology, Hefei, China; College of Civil Engineering, Hefei University of Technology, Hefei, China; College of Civil Engineering, Hefei University of Technology, Hefei, China; College of Civil Engineering, Hefei University of Technology, Hefei, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","30 Nov 2022","2022","15","","10073","10086","To achieve land cover change detection (LCCD) with both fine spatial and temporal resolutions from remote sensing images, subpixel mapping-based approaches have been widely studied in recent years. The fine spatial but coarse temporal resolution image and the coarse spatial but fine temporal image are used to accomplish LCCD by combining their advantages. However, the performance of subpixel mapping is easily affected by the accuracy of spectral unmixing, thereby reducing the reliability of LCCD. In this article, a novel subpixel change detection scheme based on improved abundance values is proposed to tackle the aforementioned problem, in which the spatial distribution of fine spatial resolution image is borrowed to promote the accuracy of spectral unmixing. First, the coarse spatial resolution image is used to generate the original abundance image by the spectral unmixing method. Second, the spatial distribution information of the fine spatial resolution image is incorporated into the original abundance image to obtain improved abundance values. Third, the fine spatial resolution subpixel map can be generated by the subpixel mapping method using the improved abundance values. At last, the fine resolution change map can be obtained by comparing the subpixel map with the fine spatial resolution image. Experiments are conducted on a simulated dataset based on Landsat-7 images and two real datasets based on Landsat-8 and MODIS images. The results of the real datasets showed that the proposed method can effectively improve the performance of LCCD with an overall accuracy of approximately 1.26% and 0.79% to the existing methods.","2151-1535","","10.1109/JSTARS.2022.3224077","Natural Science Foundation of Anhui Province, China(grant numbers:2208085QD105); National Natural Science Foundation of China(grant numbers:42171453); Fundamental Research Funds for the Central Universities(grant numbers:JZ2021HGTA0167,JZ2021HGTB0088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9961878","Change detection;remote sensing image;spatial and temporal resolutions;subpixel mapping","Earth;Artificial satellites;Graphical models;Image synthesis;Sensors;Reliability;Spatial resolution","geophysical image processing;image classification;image resolution;remote sensing;terrain mapping","coarse spatial but fine temporal image;coarse spatial resolution image;fine resolution change map;fine spatial but coarse temporal resolution image;fine spatial resolution image;fine spatial resolution subpixel map;fine spatial resolutions;improved abundance values;land cover change detection;Landsat-7 images;LCCD;mapping-based approaches;novel subpixel change detection scheme;original abundance image;remote sensing images;spatial distribution information;subpixel mapping method;temporal resolutions","","","","45","CCBY","23 Nov 2022","","","IEEE","IEEE Journals"
"Hyperspectral Image Generation From Rgb Images With Semantic and Spatial Distribution Consistency","L. Liu; Z. Shi; Y. Zao; H. Chen","Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","1804","1807","Generating hyperspectral images (HSI) from RGB imagery can obtain HSI with both high spatial and spectral resolution, which overcomes the limitations of imaging hardware conditions. Many HSI generation methods target learning a 3-n mapping from RGB to HSI, lacking concern of the spectral categories and spatial distribution. In this paper, we propose an HSI generation method preserving the band structure similarity and semantic information. We design an MLP based classifier and trained it on many spectra of known semantic categories. Then we use it to map the spectra to semantic space and constrain the distance between the embedding of generated spectra and that of the real ones. Meanwhile, a structure similarity loss is added to constrain the spatial information. Experiment results verified the superiority of the proposed method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884531","National Natural Science Foundation of China(grant numbers:62125102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884531","Hyperspectral Image;Sepctral Super-resolution;Image generation;HSI classification","Graphical models;Image synthesis;Semantics;Superresolution;Imaging;Geoscience and remote sensing;Hardware","feature extraction;geophysical image processing;geophysical techniques;image classification;image colour analysis;image processing;image sensors;learning (artificial intelligence);multilayer perceptrons;pattern classification;remote sensing","hyperspectral image generation;rgb images;spatial distribution consistency;hyperspectral images;RGB imagery;high spatial resolution;spectral resolution;HSI generation method;3-n mapping;spectral categories;band structure similarity;semantic information;known semantic categories;semantic space;generated spectra;spatial information","","","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Synthesizing Optical and SAR Imagery From Land Cover Maps and Auxiliary Raster Data","G. Baier; A. Deschemps; M. Schmitt; N. Yokoya","RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; SERPICO Team of Inria, Bretagne-Atlantique, Rennes, France; Department of Geoinformatics, Munich University of Applied Sciences, Munich, Germany; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan","IEEE Transactions on Geoscience and Remote Sensing","20 Dec 2021","2022","60","","1","12","We synthesize both optical RGB and synthetic aperture radar (SAR) remote sensing images from land cover maps and auxiliary raster data using generative adversarial networks (GANs). In remote sensing, many types of data, such as digital elevation models (DEMs) or precipitation maps, are often not reflected in land cover maps but still influence image content or structure. Including such data in the synthesis process increases the quality of the generated images and exerts more control on their characteristics. Spatially adaptive normalization layers fuse both inputs and are applied to a full-blown generator architecture consisting of encoder and decoder to take full advantage of the information content in the auxiliary raster data. Our method successfully synthesizes medium (10 m) and high (1 m) resolution images when trained with the corresponding data set. We show the advantage of data fusion of land cover maps and auxiliary information using mean intersection over unions (mIoUs), pixel accuracy, and Fréchet inception distances (FIDs) using pretrained U-Net segmentation models. Handpicked images exemplify how fusing information avoids ambiguities in the synthesized images. By slightly editing the input, our method can be used to synthesize realistic changes, i.e., raising the water levels. The source code is available at https://github.com/gbaier/rs_img_synth, and we published the newly created high-resolution data set at https://ieee-dataport.org/open-access/geonrw.","1558-0644","","10.1109/TGRS.2021.3068532","Japan Society for the Promotion of Science through KAKENHI(grant numbers:18K18067,20K19834); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406194","Deep learning;generative adversarial network (GAN);image synthesis;synthetic aperture radar (SAR)","Generators;Semantics;Remote sensing;Image synthesis;Radar polarimetry;Image segmentation;Training","geophysical image processing;image segmentation;land cover;synthetic aperture radar;terrain mapping","land cover maps;auxiliary raster data;synthetic aperture radar remote sensing images;precipitation maps;image content;high resolution images;data fusion;synthesized images;high-resolution data;optical Imagery;SAR Imagery;optical RGB;generative adversarial networks;size 1.0 m;size 10.0 m","","7","","49","IEEE","16 Apr 2021","","","IEEE","IEEE Journals"
"Self-Attention Generative Adversarial Networks for Times Series VHR Multispectral Image Generation","F. Chaabane; S. Réjichi; F. Tupin","COSIM laboratory, SUP'COM, Carthage University, Tunisia; COSIM laboratory, SUP'COM, Carthage University, Tunisia; Department of Image and Signal Processing, Telecom ParisTech, France","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4644","4647","Recently classical deep learning approaches are commonly used to perform spatial and temporal classification especially for Very High Resolution (VHR) images. They learn from existing low resolution or undersized datasets because of the availability and prices of VHR remote sensing images. Thus, they have witnessed a conspicuous success because it is quite challenging to classify high-dimensional multispectral time series data with few labeled samples. It is also difficult to simulate high quality samples having the same features as the real ones. It goes without saying that the introduction of GANs (Generative Adversarial Network) models as an unsupervised learning method, has allowed the extraction of accurate representations of the data via latent codes and backpropagation techniques. However, it is difficult to acquire high-quality samples with unwanted noises and uncontrolled divergences. To generate high-quality multispectral time series samples, a Self-Attention Generative Adversarial Network (SAGAN) is proposed in this work. SAGAN allows attention-driven, long-range dependency modeling for VHR Multispectral time series image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations which improves training dynamics. The proposed SAGAN performs better than traditional GANs, boosting the best inception score. The main contribution of this work is the use of one of the new generation of learning techniques, SAGAN, for Times Series VHR Multispectral Image Generation. SAGAN has been recently used only for single image generation.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553597","Self-Attention Generative Adversarial Networks;Multispectral VHR time series;deep learning techniques;etc.","Training;Image synthesis;Time series analysis;Generative adversarial networks;Feature extraction;Spatial resolution;Task analysis","backpropagation;feature extraction;geophysical image processing;geophysical signal processing;image classification;image resolution;learning (artificial intelligence);remote sensing;time series;unsupervised learning","GANs models;Generative Adversarial Network;unsupervised learning method;high-quality samples;high-quality multispectral time series samples;SAGAN;VHR Multispectral time series image generation tasks;high-resolution details;lower-resolution feature maps;Times Series VHR Multispectral Image Generation;single image generation;Generative Adversarial networks;classical deep learning approaches;High Resolution images;VHR remote sensing images;high-dimensional multispectral time series data;high quality samples","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Deep Learning-Based SAR Interferogram Synthesis from Raster and Land Cover Data","P. Sibler; F. Sica; M. Schmitt","Hensoldt Sensors GmbH, Immenstaad, Germany; Department of Aerospace Engineering, University of the Bundeswehr Munich, Germany; Department of Aerospace Engineering, University of the Bundeswehr Munich, Germany","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5236","5239","Image-to-image translation between different imaging modalities in Earth observation has become a widely utilized application area of deep learning. However, most of the translation is performed on real-valued data, to some extent neglecting the opportunities of complex-valued SAR data for interferometric methods. In this work, we propose a multi-task deep learning approach for simulating complex-valued InSAR data based on splitting the overall task into multi-modal image-toimage translation sub-tasks. Instead of synthesizing complex-valued SAR data directly, magnitudes, phase values and coherence magnitudes are simulated in parallel and combined to full complex-valued information afterward. With experiments on a Sentinel-1 interferogram, conditioned by DEM and land cover data, we demonstrate the feasibility of the approach.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884964","image synthesis;deep learning;CNN;GAN;autoencoder;multi-task;complex-valued;SAR;SAR interferometry;coherence estimation","Deep learning;Earth;Imaging;Geoscience and remote sensing;Estimation;Coherence;Multitasking","learning (artificial intelligence);radar imaging;radar interferometry;remote sensing by radar;synthetic aperture radar","land cover data;SAR interferogram synthesis;image-to-image translation;widely utilized application area;real-valued data;complex-valued SAR data;multitask deep learning approach;complex-valued InSAR data;multimodal image-toimage translation;phase values","","","","13","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"High Resolution SAR Image Synthesis with Hierarchical Generative Adversarial Networks","H. Huang; F. Zhang; Y. Zhou; Q. Yin; W. Hu","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2782","2785","Generative adversarial network (GAN) is an artificial neural network based on unsupervised learning method. Due to its powerful model representation capabilities, GAN has been introduced to synthesize synthetic aperture radar (SAR) image data, for the real sample is difficult to acquire. Large-scale, high-resolution SAR images play an important role in promoting SAR applications, such as automatic target recognition and image interpretation. However, on account of the difficult training problem of GAN network, especially for SAR images with speckle noise, it is difficult to obtain high-resolution SAR images by simply transfer the net from optical image. Recent studies in other image fields have shown that hierarchical structure is an effective and useful way to decompose a generation task into several smaller subtasks. How to obtain more high-resolution SAR images from limited original samples through GAN is the target of our research. Therefore, in this paper, we introduce a hierarchical GAN network model to generate SAR images, through the multi-stage network, gradually improve the quality of the generated image, and finally obtain high-resolution images. The type and aspect of generated images are determined by the input of condition vectors in the last two stages. In addition, we introduce the triple loss, in which the background loss is used to imitating background clutter noise of SAR image, the condition loss is to make the generated images' type and aspect become controllable, and the global loss for getting higher image generation quality. The generated images show high similarity with the real samples.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900494","Generative adversarial network(GAN);synthetic aperture radar (SAR);SAR simulator;automatic target recognition (ATR);triple loss","Generative adversarial networks;Radar polarimetry;Gallium nitride;Image resolution;Synthetic aperture radar;Solid modeling;Data models","image resolution;neural nets;radar imaging;synthetic aperture radar;unsupervised learning","high resolution SAR image synthesis;hierarchical generative adversarial networks;generative adversarial network;artificial neural network;synthetic aperture radar image data;high-resolution SAR images;SAR applications;automatic target recognition;image interpretation;optical image;image fields;hierarchical GAN network model;high-resolution images;generated images;image generation quality;unsupervised learning method","","10","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"High-reflectivity objects distributed optical satellite image fusion based on NDVI classification","Z. Zexing; X. Qizhi; W. Haibo; Y. Wenyong","Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; China Centre for Resources satellite Data and Application, Beijing, China; China Centre for Resources satellite Data and Application, Beijing, China","2017 2nd International Conference on Frontiers of Sensors Technologies (ICFST)","18 Dec 2017","2017","","","231","235","Ratioing method is one type of the most famous fusion methods in remote sensing image fusion domain. Generally, the ratioing method synthesizes a low-resolution panchromatic (Pan) image by adaptive weighted summation of a multispectral (MS) image. Consequently, the accuracy of the weights for low-resolution Pan image synthesis is of great importance. However, in most cases, the optical satellite images contain lots of high-reflectivity objects, such as clouds covered regions, and high-reflectivity buildings. These objects are saturate due to their strong reflectance. The distortion of saturated objects results in the failure of weights calculation, so that causes the color distortion of fused images. To solve the problem, this paper proposes a high-reflectivity objects distributed optical satellite image fusion method based on NDVI classification. First, the NDVI index is employed to classify the pixels of a MS image into high-reflectivity group and normal group, then the pixels in normal group is used to calculate the weighted coefficients, finally the fused image is obtained by ratioing transform. Experimental results on a large number of test images show that the proposed method has good performance on reducing color distortion.","","978-1-5090-4860-1","10.1109/ICFST.2017.8210509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8210509","Image fusion;Pan-sharpening;Remote sensing image;NDVI index","Remote sensing;Optical distortion;Optical sensors;Image color analysis;Image resolution;Optical imaging;Distortion","geophysical image processing;image classification;image colour analysis;image fusion;image resolution;sensor fusion;vegetation;vegetation mapping","high-reflectivity objects;NDVI classification;ratioing method;famous fusion methods;remote sensing image fusion domain;low-resolution panchromatic image;adaptive weighted summation;multispectral image;low-resolution Pan image synthesis;optical satellite images;high-reflectivity buildings;strong reflectance;saturated objects results;weights calculation;color distortion;fused image;optical satellite image fusion method;MS image;high-reflectivity group;test images;ratioing transform;NDVI index","","","","20","IEEE","18 Dec 2017","","","IEEE","IEEE Conferences"
"Classification and Generation of Earth Observation Images Using a Joint Energy-Based Model","J. Castillo-Navarro; B. Le Saux; A. Boulch; S. Lefèvre","Université Bretagne Sud, IRISA UMR 6074, Vannes, France; European Space Agency, ESRIN Φ-lab, Frascati (Rome), Italy; valeo.ai, Paris, France; Université Bretagne Sud, IRISA UMR 6074, Vannes, France","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2098","2101","Deep learning has changed unbelievably the processing of Earth Observation tasks such as land cover mapping or image registration. Yet, today new models are needed to push further the revolution and enable new possibilities. We propose a new framework for generative modelling of Earth Observation images. It learns an energy-based model to estimate the underlying distribution of the data while jointly training a deep neural network for classification. On the varied image types of the EuroSAT benchmark, we show this model obtains classification results on par with state-of-the-art and moreover allows us to tackle a wide range of high-potential applications: image synthesis, out-of-distribution testing for domain adaptation, and image completion or denoising.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553440","CNES; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553440","Deep Learning;Energy-based Models;Generative Models;Earth Observation","Earth;Deep learning;Training;Adaptation models;Image registration;Image synthesis;Noise reduction","deep learning (artificial intelligence);geophysical image processing;image classification;image denoising;image registration;terrain mapping","Earth observation images;joint energy-based model;deep learning;Earth observation tasks;land cover mapping;generative modelling;deep neural network;image synthesis;image completion;EuroSAT benchmark;image denoising;image types","","1","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Shadow Removal of Hyperspectral Remote Sensing Images With Multiexposure Fusion","P. Duan; S. Hu; X. Kang; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; School of Robotics, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","21 Sep 2022","2022","60","","1","11","Shadow removal is a challenging problem in hyperspectral remote sensing images due to its spatial-variant properties and diverse patterns. In this work, a shadow removal framework with multiexposure fusion is proposed for hyperspectral remote sensing images, which consists of three major steps. First, a color space conversion method is exploited to detect the shadow regions. Second, the principle of the intrinsic decomposition model is utilized to generate a set of differently exposed hyperspectral images (HSIs), i.e., multiexposure images. Third, the generated multiexposure images and the original HSIs are fused together with a two-stage image fusion method so as to remove the shadows in hyperspectral remote sensing images effectively. Experiments performed on three real hyperspectral datasets confirm that the performance of the proposed method outperforms other state-of-the-art shadow removal approaches.","1558-0644","","10.1109/TGRS.2022.3203808","National Key Research and Development Program of China(grant numbers:2021YFA0715203); Major Program of the National Natural Science Foundation of China(grant numbers:61890962); National Natural Science Foundation of China(grant numbers:61871179); Scientific Research Project of Hunan Education Department(grant numbers:19B105); National Science Foundation of Hunan Province(grant numbers:2019JJ50036,2020GK2038); Hunan Provincial Natural Science Foundation for Distinguished Young Scholars(grant numbers:2021JJ022); Huxiang Young Talents Science and Technology Innovation Program(grant numbers:2020RC3013); Changsha Natural Science Foundation(grant numbers:kq2202171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874905","Color space conversion;intrinsic image decomposition;multiexposure fusion;shadow removal;two-stage image fusion","Hyperspectral imaging;Image color analysis;Lighting;Image restoration;Image edge detection;Image synthesis;Image fusion","","","","2","","53","IEEE","2 Sep 2022","","","IEEE","IEEE Journals"
"ISAR Images Generation Via Generative Adversarial Networks","R. -Y. Zhou; Z. -L. Yang; F. Wang","Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5267","5270","One of the challenges faced by current intelligent target recognition tasks is the lack of samples, especially in the Inverse Synthetic Aperture Radar (ISAR) images understanding. In this paper, we proposed an ISAR objects generative network to generate multi-aspect ISAR images. A simulated ISAR dataset of six types of aircrafts is produced via, using bidirectional analytic ray tracing (BART) method. Then, the proposed generative network is trained with the simulated ISAR dataset. We evaluated the performance of the proposed network using structural similarity (SSIM). The experimental results show that the generated targets are very close to the real ISAR samples, and the SSIM between generated and real ISAR images of aircrafts is larger than 0.7.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553814","National Natural Science Foundation of China(grant numbers:61901122); Natural Science Foundation of Shanghai(grant numbers:20ZR1406300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553814","Inverse Synthetic Aperture Radar (ISAR);Automatic Target Recognition (ATR);Generative Adversarial Nets (GANs)","Training;Target recognition;Image synthesis;Geoscience and remote sensing;Ray tracing;Generative adversarial networks;Aircraft","","","","1","","12","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"High Efficiency Weather Radar Mosaic Image Generation Framework","J. Tang; C. J. Matyas",Citadel LLC; University of Florida,"2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","367","369","Multi-radar product mosaic is a kind of common product in many applications and is a pre-requisites many weather-radar-related machine learning research. Two highly paralleled mosaic image generation frameworks are proposed. The first one is based on the MapReduce paradigm. In the map stage, each gate is mapped from radar coordinates to a radar-centric grid space; in the group stage, gates contribution to grid cells are collected; in the reduce stage, the final values are derived. The same logic applies again to combine radar-centric grids into the final domain. Another framework is replacing original reduce stage with RayTracing technology which allows generating products at multiple resolutions in one batch.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554081","Weather Radar;MapReduce;CUDA;RayTracing","Radar remote sensing;Meteorological radar;Image resolution;Image synthesis;Spaceborne radar;Machine learning;Radar imaging","data handling;image segmentation;learning (artificial intelligence);meteorological radar;parallel processing;radar signal processing","high efficiency weather radar mosaic image generation framework;multiradar product mosaic;common product;pre-requisites;weather-radar-related;highly paralleled mosaic image generation frameworks;map stage;radar coordinates;radar-centric grid space;group stage;reduce stage;radar-centric grids","","","","12","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multi-Category SAR Images Generation Based on Improved Generative Adversarial Network","S. Du; J. Hong; Y. Wang; K. Xing; T. Qiu","National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4260","4263","The generative adversarial network (GAN) provides a different way for SAR data augmentation. The traditional GAN model is mainly based on the Jensen-Shannon (JS) divergence or Wasserstein distance. The former faces mode collapse, while the latter is not suitable for multi-category image generation. In this paper, an improved model based on WGAN-GP is proposed. An encoder is used to learn the features of real samples as the input of the generator to control training to a certain extent and make the generated image quality better. In addition, a pre-trained classifier is introduced as the constraint of the generator to ensure the generated images have the correct category information. MSTAR dataset is used to verify the generation capability of the proposed model. The results show that the proposed model has the stable generation capability to provide high-quality SAR images as a supplementary training dataset, which could assist in achieving good classification accuracy.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554120","P-band;calibration;BIOMASS;parabolic antenna;target of opportunity","Training;Image quality;Image synthesis;Geoscience and remote sensing;Generative adversarial networks;Generators;Radar polarimetry","image classification;learning (artificial intelligence);maximum likelihood estimation;pattern classification;radar imaging;synthetic aperture radar","SAR data augmentation;traditional GAN model;faces mode collapse;multicategory image generation;generated image quality;pre-trained classifier;correct category information;stable generation capability;high-quality SAR images;supplementary training dataset;multicategory SAR images generation;improved generative adversarial network","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"A New Geostationary Satellite-Based Snow Cover Recognition Method for FY-4A AGRI","H. Qiao; P. Zhang; Z. Li; C. Liu","University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","18 Nov 2021","2021","14","","11372","11385","Snow cover is an important component of the cryosphere. Clouds have a large influence on optical remote sensing satellites when recognizing snow cover. Geostationary satellites, due to their high-frequency observations over coverage areas, can effectively compensate for the drawback of snow cover recognition from polar orbit optical satellites under cloud-covered conditions. However, past geostationary satellites have relatively few band settings to produce sensitive factors for snow cover recognition. The FY-4A Advanced Geostationary Radiation Imager (AGRI) satellite has the advantage of high temporal resolution with a wealth of bands, which highlights its potential in reducing the impact of clouds and accurately obtaining snow cover information. Based on the advantages of FY-4A AGRI data and the flow characteristics of clouds, we developed an improved maximum brightness temperature image synthesis algorithm, which can greatly reduce the probability of cloud and snow cover misclassification. Combining the features of FY-4A AGRI data, we reorganized the snow cover recognition factor and developed a new snow cover recognition method. The results show that the proposed method can reduce cloud cover by 57.172% compared with MOD10A1 data. After evaluating the proposed method using meteorological ground observation datasets and MOD10A1 data, we found that the overall accuracy of the proposed method can reach 94.11% and 98.55%, respectively, and the F-score (FS) can reach 73.05% and 85.40%, respectively.","2151-1535","","10.1109/JSTARS.2021.3125015","National Key R&D Program of China(grant numbers:2018YFA0605403); Science and Technology Basic Resource Investigation Program of China(grant numbers:2017FY100502); Key Deployment Program of AIRCAS(grant numbers:Y950930Z2F); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9600871","China;Fengyun-4A (FY-4A) advanced geostationary radiation imager (AGRI);geostationary satellite;snow cover","Snow;Clouds;Remote sensing;Optical sensors;Optical imaging;Geostationary satellites;Reflectivity","artificial satellites;clouds;hydrological techniques;remote sensing;snow","FY-4A;optical remote sensing;recognizing snow cover;geostationary satellites;high-frequency observations;polar orbit optical satellites;cloud-covered conditions;Advanced Geostationary Radiation Imager satellite;snow cover information;AGRI data;improved maximum brightness temperature image synthesis algorithm;snow cover recognition factor;geostationary satellite-based snow cover recognition method","","3","","79","CCBY","4 Nov 2021","","","IEEE","IEEE Journals"
"Controlled Multi-modal Image Generation for Plant Growth Modeling","M. Miranda; L. Drees; R. Roscher","German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","5118","5124","Predicting plant development is an important task in precision farming and an essential metric for decision-making by researchers and farmers. In this work, we propose a novel generative modeling technique for plant growth prediction based on conditional generative adversarial networks. We formulate plant growth as an image-to-image translation task and predict the appearance of a plant growth stage as a function of its previous stage. We take into account that plant growth is inherently multi-modal, depending on numerous and highly variable environmental factors, and thus a single input belongs to a distribution of potential outputs. We encode the ambiguity in an interpretable and low-dimensional latent vector space representing the various factors of variation that are influencing plant growth. We use a novel encoder-based data fusion technique and combine information contained in remote sensing imagery of different cropping systems with data containing the factors of variation to adequately model plant growth. This offers several advantages over existing methods: (1) we show that we can model a distribution of potential appearances and simultaneously outperform existing methods in providing more realistic predictions, (2) the complexity of plant growth is more adequately captured, as various factors influencing plant growth can be included, (3) predictions are controllable by being conditioned by an interpretable latent vector representing the factors of variation along with an input image of a previous growth stage.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956115","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956115","","Measurement;Image synthesis;Ecosystems;Predictive models;Generative adversarial networks;Pattern recognition;Task analysis","agriculture;crops;encoding;environmental factors;geophysical image processing;image fusion;regression analysis;remote sensing;sensor fusion;vectors","controlled multimodal image generation;image-to-image translation task;plant growth modeling;plant growth prediction;plant growth stage;predicting plant development;previous growth stage","","","","36","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"Detection of Transmission Towers and Insulators in Remote Sensing Images with Deep Learning","T. Wang; R. Wei; L. Wang; L. Zhu; E. Zhou; S. Liu; H. Yang; S. Wang","Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Guangdong Key Laboratory of Electric Power Equipment Reliability, Electric Power Research Institute of Guangdong Power Grid Co., Ltd., Guangzhou, China; Tianjin Zhongwei Aerospace Data System Technology Co.,Ltd., Tianjin, China; Tianjin Zhongwei Aerospace Data System Technology Co.,Ltd., Tianjin, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","3298","3303","Intelligent inspections of high-voltage electronic power grids with high-altitude aerial or satellite remote sensing images (RSIs) have attracted more and more attention. The detection of transmission tower and insulator in smart electronic power is of great importance. Traditional image recognition based methods have been proven to be difficult to complete this task effectively and efficiently, lots of deep learning based methods have been adopted due to their promising performance. However, collecting a large number of labeled aerial/satellite imaging data for deep learning requires a lot of manpower/financial costs and the deep models trained on small size of samples are often easy to over-fit. For this reason, it has practical significance to study the automatic detection of towers and insulators in the case of small samples. Aiming at the problem of object detection under small samples, a deep learning framework for simultaneous towers and insulators detection in RSIs based on Faster-RCNN and neural style image synthesis is proposed. Firstly, to alleviate the small sample size problem, a sample generation method based on neural style transfer and alpha channel image fusion techniques is proposed, which randomly combines the foreground towers and background images to expand the training data set. Secondly, upon the expanded training data, an object detection model for towers and insulators based on Faster-RCNN is further trained. Experiments show that the object detection model trained with the extended training data has better generalization performance and can better suppress false alarms.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728166","China Southern Power Grid; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728166","power grids;tower detection;insulator detection;object detection;neural style transfer","Deep learning;Satellites;Poles and towers;Training data;Object detection;Insulators;Data models","feature extraction;geophysical image processing;image fusion;image recognition;learning (artificial intelligence);neural nets;object detection;poles and towers;remote sensing","high-voltage electronic power grids;high-altitude aerial;RSIs;transmission tower;insulator;smart electronic power;traditional image recognition based methods;deep learning based methods;deep models;automatic detection;insulators;deep learning framework;simultaneous towers;Faster-RCNN;neural style image synthesis;sample size problem;sample generation method;neural style transfer;alpha channel image fusion techniques;foreground towers;background images;training data set;expanded training data;object detection model;extended training data;transmission towers;remote sensing images;intelligent inspections","","","","24","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"An Enhanced Probabilistic Posterior Sampling Approach for Synthesizing SAR Imagery With Sea Ice and Oil Spills","L. Xu; A. Wong; D. A. Clausi","School of Land Science and Technology, China University of Geosciences, Beijing, China; Systems Design Engineering Department, University of Waterloo, Waterloo, ON, Canada; Systems Design Engineering Department, University of Waterloo, Waterloo, ON, Canada","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","2","188","192","Although the synthesis of the synthetic aperture radar (SAR) imagery with both sea ice and oil spills can significantly benefit in improving the consistency and comprehensiveness of testing and evaluating algorithms that are designed for mapping cold ocean regions, creating such imagery is difficult due to the heterogeneity and complexity of the source images. This letter presents an enhanced region-based probabilistic posterior sampling approach to effectively synthesize SAR imagery with different ocean features. In the proposed approach, instead of relying entirely on the SAR intensity values, the posterior sampling is performed based on a number of quantitative factors, such as intensity, label field, and the prior class probability of sampling candidates, constituting a complete probabilistic framework that addresses key aspects in the synthesis of SAR imagery from heterogeneous sources. The experiments demonstrate that the proposed approach can better address the difficulties caused by the heterogeneity in the source images compared with the existing state-of-the-art ice synthesis method, and it will improve the consistency, comprehensiveness, and fairness of the evaluation of the remote sensing classification and segmentation algorithms.","1558-0571","","10.1109/LGRS.2016.2633572","Natural Sciences and Engineering Research Council of Canada; ArcticNet; National Natural Science Foundation of China(grant numbers:41501410); Canada Research Chairs Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797259","Image synthesis;oil spills;sea ice;statistical texture modeling","Synthetic aperture radar;Oils;Sea ice;Probabilistic logic;Image segmentation;Image generation","image sampling;marine pollution;oil pollution;pollution measurement;radar imaging;sea ice;synthetic aperture radar","enhanced probabilistic posterior sampling;SAR imagery synthesis;sea ice;oil spills;synthetic aperture radar;ocean features;source images","","3","","13","IEEE","23 Dec 2016","","","IEEE","IEEE Journals"
"HSIGAN: A Conditional Hyperspectral Image Synthesis Method With Auxiliary Classifier","W. Liu; J. You; J. Lee","Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea; Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea; Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Apr 2021","2021","14","","3330","3344","In this article, we explore a conditional hyperspectral image (HSI) synthesis method with generative adversarial networks (GAN). A new multistage and multipole generative adversarial network, which is suitable for conditional HSI generation and classification (HSIGAN), is proposed. For HSIs synthesis, it is crucial to learn a great deal of spatial-spectral distribution features from source data. The multistage progressive training makes the generator effectively imitate the real data by fully exploiting the high-dimension learning capability of GAN models. The coarse-to-fine information extraction method helps the discriminator to understand the semantic feature better while the multiscale classification prediction presents a positive impact on results. A spectral classifier joins the adversarial network, which offers a helping hand to stabilize and optimize the model. Moreover, we apply the 3-D DropBlock layer in the generator to remove semantic information in a contiguous spatial-spectral region and avoid model collapse. Experimental results of the quantitative and qualitative evaluation show that HSIGAN could generate high-fidelity, diverse hyperspectral cubes while achieving top-ranking accuracy for supervised classification. This result is encouraging for using GANs as a data augmentation strategy in the HSI vision task.","2151-1535","","10.1109/JSTARS.2021.3063911","Natural Science Foundation of Hebei Province(grant numbers:F2020403030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369834","Classification;generative adversarial network (GAN);hyperspectral image (HSI);synthesis","Gallium nitride;Generative adversarial networks;Generators;Hyperspectral imaging;Deep learning;Training;Task analysis","feature extraction;hyperspectral imaging;image classification;image colour analysis;neural nets;supervised learning","3-D DropBlock layer;conditional HSI generation and classification;HSI vision task;data augmentation strategy;supervised classification;hyperspectral cubes;semantic information;spectral classifier;multiscale classification prediction;semantic feature;coarse-to-fine information extraction method;GAN models;high-dimension learning capability;multistage progressive training;spatial-spectral distribution features;multipole generative adversarial network;auxiliary classifier;conditional hyperspectral image synthesis method;HSIGAN","","2","","58","CCBYNCND","4 Mar 2021","","","IEEE","IEEE Journals"
"SPI(Spherical Point Indicator) : Point Feature Image Generation for Point-wise Classification and Dynamic Object Removal","C. Bae; Y. -C. Lee; W. Yu; S. Lee","Department of Mechanical Engineering, Kongju National University, Cheonan, Republic of Korea; Artificial Intelligence Laboratory, ETRI, Daejeon, Rep. of Korea; Artificial Intelligence Laboratory, ETRI, Daejeon, Rep. of Korea; Division of Mechanical & Automotive Engineering, Kongju National University, Cheonan, Republic of Korea","2022 19th International Conference on Ubiquitous Robots (UR)","14 Jul 2022","2022","","","206","212","3D point cloud map is generated by accumulating LiDAR sensor data scanned at various locations and times. During scanning, dynamic objects are scanned in different poses depending on location and time, which degrades the quality of the map and negatively affects the localization. In order to improve the quality of the 3D point cloud map and for long-term management, effective to create a 3D point cloud map using only static objects by removing objects including dynamic possibilities. In this paper, we propose a Spherical Point Indicator (SPI) that can classify each point and remove dynamic probability objects through a method of generating feature images from individual points of a three-dimensional point cloud in scan units collected from a LiDAR sensor. SPI generates a unique feature image of each point by using the distribution information and intensity information of neighboring points centered on each point. The generated images are used as inputs to the CNN network and classified. The SPI feature image generation is applicable to all 3D point clouds, and all the scanned points as a result of classification are individually classified according to categories. Our approach can classify all included 3D point clouds using only one scan, and only static objects can be detected by filtering the dynamic possibilities object categories from the classified results.","","978-1-6654-8253-0","10.1109/UR55393.2022.9826243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826243","","Point cloud compression;Three-dimensional displays;Laser radar;Image synthesis;Shape;Robot kinematics;Robot sensing systems","feature extraction;geophysical image processing;image classification;image segmentation;image texture;object detection;optical radar;probability;remote sensing by laser beam;remote sensing by radar","dynamic possibilities;dynamic object removal;accumulating LiDAR sensor data;dynamic objects;long-term management;static objects;dynamic probability objects;feature images;individual points;three-dimensional point cloud;scan units;unique feature image;distribution information;intensity information;SPI feature image generation;scanned points;point-wise classification;point feature image generation","","","","15","IEEE","14 Jul 2022","","","IEEE","IEEE Conferences"
"Semi-Supervised SAR ATR via Conditional Generative Adversarial Network with Multi-Discriminator","X. Liu; Y. Huang; C. Wang; J. Pei; W. Huo; Y. Zhang; J. Yang","School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2361","2364","Convolutional neural networks (CNN) show superior potential in synthetic aperture radar automatic target recognition (SAR ATR). However, due to the difficulty of obtaining SAR images and the scarcity of labeled SAR images, supervised learning has poor performance in this area and is not widely applicable. To address this problem, a semi-supervised conditional generative adversarial network with a multi-discriminator (SCGAN-MD) is proposed in this paper. In our method, a conditional generative adversarial network (CGAN) is adopted with two discriminators for training the generated images and predicting the labels for unlabeled samples. Compared with other semi-supervised learning-based methods, our proposed method has more accurate image generation capability and can achieve improved recognition accuracy of SAR ATR. Experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) database indicate that the proposed method can effectively improve the recognition accuracy and robustness of the network with a small number of labeled samples.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554365","National Natural Science Foundation of China(grant numbers:61901091,61901090,61671117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554365","SAR ATR;semi-supervised;conditional generative adversarial network","Training;Image recognition;Target recognition;Image synthesis;Supervised learning;Generative adversarial networks;Feature extraction","image recognition;neural nets;radar computing;radar imaging;radar target recognition;supervised learning;synthetic aperture radar","recognition accuracy;semisupervised SAR ATR;conditional generative adversarial network;multidiscriminator;convolutional neural networks;synthetic aperture radar automatic target recognition;labeled SAR images;supervised learning;semisupervised learning-based methods;image generation capability;CNN;SCGAN-MD;CGAN;moving and stationary target acquisition and recognition database;MSTAR database","","3","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"K-Means Clustering Guided Generative Adversarial Networks for SAR-Optical Image Matching","W. -L. Du; Y. Zhou; J. Zhao; X. Tian","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the People’s Republic of China, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau","IEEE Access","11 Dec 2020","2020","8","","217554","217572","Synthetic Aperture Radar and optical (SAR-optical) image matching is a technique of finding correspondences between SAR and optical images. SAR-optical image matching can be simplified to single-mode image matching through image synthesis. However, the existing SAR-optical image synthesis methods are unable to provide qualified images for SAR-optical image matching. In this work, we present a K-means Clustering Guide Generative Adversarial Networks (KCG-GAN) to improve the image quality of synthesizing by constraining spatial information synthesis. KCG-GAN uses k-means segmentations as one of the image generator's inputs and introduces feature matching loss, segmentation loss, and L1 loss to the objective function. Meanwhile, to provide repeatable k-means segmentations, we develop a straightforward 1D k-means algorithm. We compare KCG-GAN with a leading image synthesis method-pix2pixHD. Qualitative results illustrate that KCG-GAN preserves more spatial structures than pix2pixHD. Quantitative results show that, compared with pix2pixHD, images synthesized by KCG-GAN are more similar to original optical images, and SAR-optical image matching based on KCG-GAN obtains at most 3.15 times more qualified matchings. Robustness tests demonstrate that SAR-optical image matching based on KCG-GAN is robust to rotation and scale changing. We also test three SIFT-like algorithms on matching original SAR-optical image pairs and matching KCG-GAN synthesized optical-optical image pairs. Experimental results show that our KCG-GAN significantly improves the performances of the three algorithms on SAR-optical image matching.","2169-3536","","10.1109/ACCESS.2020.3042213","National Natural Science Foundation of China(grant numbers:61572505,62002360,61806206,61772530,U1610124); Six Talent Peaks Project in Jiangsu Province(grant numbers:2015-DZXX-010); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180639,BK20171192); China Postdoctoral Science Foundation(grant numbers:2018M642359); Science and Technology Development Fund of Macau(grant numbers:0038/2020/A1); Science and Technology Development Fund, Macu SAR under the open project of State Key Laboratory of Lunar and Planetary Science; Fundamental Research Funds for the Central Universities(grant numbers:2020ZDPY0305); Xuzhou Science and Technology Program(grant numbers:KC18061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9279214","Image matching;image synthesis;synthetic aperture radar (SAR);generative adversarial networks (GANs)","Optical imaging;Optical sensors;Nonlinear optics;Image segmentation;Image matching;Optical distortion;Adaptive optics","image matching;image segmentation;neural nets;optical information processing;pattern clustering;radar computing;radar imaging;synthetic aperture radar;transforms","KCG-GAN;image synthesis method;SAR-optical image matching;optical-optical image pairs;SAR-optical image synthesis methods;k-means clustering guide generative adversarial networks;synthetic aperture radar;constraining spatial information synthesis;k-means segmentations;1D k-means algorithm;pix2pixHD;SIFT-like algorithms","","7","","69","CCBY","3 Dec 2020","","","IEEE","IEEE Journals"
"LDGAN: A Synthetic Aperture Radar Image Generation Method for Automatic Target Recognition","C. Cao; Z. Cao; Z. Cui","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Center for Information Geoscience, UESTC, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2020","2020","58","5","3495","3508","Under the framework of a supervised learning-based automatic target recognition (ATR) approach, recognition performance is primarily dependent on the amount of training samples. However, shortage in training samples is a consistent issue for ATR. In this article, we propose a new image to image generation method, called label-directed generative adversarial networks (LDGANs), which will provide labeled samples to be used for recognition model training. We define an entirely new loss function for the LDGAN, which utilizes the Wasserstein distance to replace the original distance measurement of the conventional generative adversarial networks (GANs), thus efficiently avoiding the collapse mode problem. The label information is also added to the loss function of the LDGAN to avoid generating a large number of unlabeled target images. More importantly, the proposed method also makes corresponding changes to the network architecture regarding the new GANs. At the same time, the detailed algorithm about the LDGAN is also introduced in this article to deal with the issue that characteristically GANs are not easy to train. Based on comparisons with other directed generation methods, the experimental results show comparative results of several types of generated images in statistical features, gradient features, classic features of synthetic aperture radar (SAR) targets and the independence from the real image. While demonstrating that the images generated by the LDGAN produced better results using the assumptions of independent and identical distribution, the experiment also explores the performance of the generated image in the ATR. A comparison of these experimental results demonstrates a better way to use the generated image for ATR. The experimental results also prove that the proposed method does have the ability to supplement information for ATR when the training sample information is insufficient.","1558-0644","","10.1109/TGRS.2019.2957453","National Natural Science Foundation of China(grant numbers:61801098); Fundamental Research Funds for the Central Universities(grant numbers:2672018ZYGX2018J013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938701","Automatic target recognition (ATR);information supplement;label-directed generative adversarial network (LDGAN);synthetic aperture radar (SAR) image","Gallium nitride;Radar polarimetry;Image recognition;Synthetic aperture radar;Target recognition;Training;Image synthesis","feature extraction;learning (artificial intelligence);neural nets;radar computing;radar imaging;radar target recognition;synthetic aperture radar","label-directed generative adversarial networks;supervised learning-based automatic target recognition approach;synthetic aperture radar image generation method;training sample information;synthetic aperture radar targets;directed generation methods;unlabeled target images;label information;GAN;conventional generative adversarial networks;loss function;recognition model training;labeled samples;LDGAN;training samples;recognition performance;ATR","","22","","32","IEEE","23 Dec 2019","","","IEEE","IEEE Journals"
"Detection of Synthesized Satellite Images Using Deep Neural Networks","W. -H. Liao; Y. -S. Chang; Y. -C. Wu","Dept. of Computer Science, National Chengchi University, Taipei, Taiwan; Dept. of Computer Science, National Chengchi University, Taipei, Taiwan; Artificial Intelligence and E-learning Center, National Chengchi University, Taipei, Taiwan","2023 17th International Conference on Ubiquitous Information Management and Communication (IMCOM)","8 Feb 2023","2023","","","1","5","The technology of generative adversarial networks (GAN) is constantly evolving, and synthesized images can no longer be accurately distinguished by the human eyes alone. GAN has been applied to the analysis of satellite images, mostly for the purpose of data augmentation. Recently, however, we have seen a twist in its usage. In information warfare, GAN has been used to create fake satellite images or modify the image content by putting fake bridges, buildings and clouds to mislead or conceal important intelligence. To address the increasing counterfeit cases in satellite images, the goal of this research is to develop algorithms that can classify fake remote sensing images robustly and efficiently. There exist many techniques to synthesize or manipulate the content of satellite images. In this paper, we focus on the case when the entire image is forged. Three satellite image synthesis methods, including ProGAN, cGAN and CycleGAN will be investigated. The effect of image pre-processing such as histogram equalization and bilateral filter will also be evaluated. Experiments show that satellite images generated by different GANs can be easily identified by individually trained models. The performance degraded when model trained with one type of GAN samples is employed to determine the originality of images synthesized with other types of GANs. Additionally, when histogram equalization is applied to the images, the detection model fails to distinguish its authenticity. A four-class universal classification model is proposed to address this issue. An overall accuracy of over 99% has been achieved even when pre-processing has been applied.","","978-1-6654-5348-6","10.1109/IMCOM56909.2023.10035570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035570","Satellite Imagery;Generative Adversarial Network;Image Forgery Detection","Histograms;Satellites;Image synthesis;Neural networks;Interference;Generative adversarial networks;Forgery","","","","","","12","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"Texture Feature Extraction Methods: A Survey","A. Humeau-Heurtier","Laboratoire Angevin de Recherche en Ingénierie des Systèmes, University of Angers, Angers, France","IEEE Access","22 Jan 2019","2019","7","","8975","9000","Texture analysis is used in a very broad range of fields and applications, from texture classification (e.g., for remote sensing) to segmentation (e.g., in biomedical imaging), passing through image synthesis or pattern recognition (e.g., for image inpainting). For each of these image processing procedures, first, it is necessary to extract—from raw images—meaningful features that describe the texture properties. Various feature extraction methods have been proposed in the last decades. Each of them has its advantages and limitations: performances of some of them are not modified by translation, rotation, affine, and perspective transform; others have a low computational complexity; others, again, are easy to implement; and so on. This paper provides a comprehensive survey of the texture feature extraction methods. The latter are categorized into seven classes: statistical approaches, structural approaches, transform-based approaches, model-based approaches, graph-based approaches, learning-based approaches, and entropy-based approaches. For each method in these seven classes, we present the concept, the advantages, and the drawbacks and give examples of application. This survey allows us to identify two classes of methods that, particularly, deserve attention in the future, as their performances seem interesting, but their thorough study is not performed yet.","2169-3536","","10.1109/ACCESS.2018.2890743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600329","Classification;feature extraction;image processing;image synthesis;segmentation;shape from texture;texture","Feature extraction;Histograms;Symmetric matrices;Entropy;Image segmentation;Transforms;Remote sensing","","","","172","","260","OAPA","3 Jan 2019","","","IEEE","IEEE Journals"
"A Semi-Supervised Image-to-Image Translation Framework for SAR–Optical Image Matching","W. -L. Du; Y. Zhou; H. Zhu; J. Zhao; Z. Shao; X. Tian","Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau","IEEE Geoscience and Remote Sensing Letters","1 Dec 2022","2022","19","","1","5","Synthetic aperture radar (SAR) and optical image matching aims to acquire correspondences from a certain pair of SAR and optical images. Recent advances in the image-to-image translation provided a way to simplify the SAR–optical image matching into the SAR–SAR or optical–optical image matchings. The existing image-to-image translations mainly focus on supervised or unsupervised learning. However, gathering sufficient amounts of aligned training data for supervised learning is challenging, while unsupervised learning cannot guarantee enough correct correspondences. In this work, we investigate the applicability of semi-supervised image-to-image translation for SAR–optical image matching such that both aligned and unaligned SAR–optical images could be used. To this end, we combine the benefits of both supervised and unsupervised well-known image-to-image translation methods, i.e., Pix2pix and CycleGAN, and propose a simple yet effective semi-supervised image-to-image translation framework. Through extensive experimental comparisons to the baseline methods, we verify the effectiveness of the proposed framework in both semi-supervised and fully supervised settings. Our codes are available at https://github.com/WenliangDu/Semi-I2I.","1558-0571","","10.1109/LGRS.2022.3223353","National Natural Science Foundation of China(grant numbers:62002360,62272461,62101555,61806206,62106268); Science and Technology Development Fund of Macau (Macau FDCT)(grant numbers:0038/2020/A1); Opening Fund of State Key Laboratory of Lunar and Planetary Sciences (Macau University of Science and Technology) (Macau FDCT)(grant numbers:119/2017/A3); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201346,BK20210488); “Double First-Class” Project of China University of Mining and Technology for Independent Innovation and Social Service(grant numbers:2022ZZCX06); China Postdoctoral Science Foundation(grant numbers:2022M713379); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955555","Generative adversarial networks (GANs);image matching;semi-supervised image synthesis;synthetic aperture radar (SAR)","Optical imaging;Optical sensors;Adaptive optics;Image matching;Training;Synthetic aperture radar;Optical distortion","image matching;learning (artificial intelligence);optical images;radar imaging;synthetic aperture radar;unsupervised learning","aligned SAR-optical images;fully supervised settings;image-to-image translation methods;optical image matching;optical-optical image matchings;SAR-optical image;SAR-SAR;semisupervised image-to-image translation framework;unaligned SAR-optical images","","","","14","IEEE","18 Nov 2022","","","IEEE","IEEE Journals"
"ArcSAR: Theory, Simulations, and Experimental Verification","M. Pieraccini; L. Miccinesi","Department of Information Engineering, University of Florence, Florence, Italy; Department of Information Engineering, University of Florence, Florence, Italy","IEEE Transactions on Microwave Theory and Techniques","26 Jan 2017","2017","65","1","293","301","ArcSAR is a ground-based synthetic aperture radar (GBSAR) that has recently been receiving increasing interest in the scientific literature. While the conventional GBSAR exploits the movement of an antenna along a linear rail to synthesize a large aperture, an ArcSAR exploits the spatial diversity of the data acquired by an antenna fixed to a rotating arm. The great advantage of ArcSAR is its capability to synthesize images at 360° with a constant resolution in azimuth. In this paper, the authors propose and test a new focusing algorithm that does not require to operate in the far field and neither with narrow beam antennas; moreover, it is flexible enough to focus on any plane (not necessarily on the rotation plane) as well as in the whole 3-D space. Furthermore, the authors demonstrate theoretically and experimentally that ArcSAR images can be affected by a “defocusing effect” of the targets far from the rotation plane, which has to be taken into consideration when designing such radars.","1557-9670","","10.1109/TMTT.2016.2613926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588999","Ground-based synthetic aperture radar (GBSAR);radar;remote sensing;synthetic aperture radar (SAR)","Synthetic aperture radar;Spaceborne radar;Radar antennas;Apertures;Radar imaging","radar imaging;synthetic aperture radar","ground-based synthetic aperture radar;antenna movement;linear rail;spatial diversity;image synthesis;3D space;ArcSAR images;defocusing effect","","43","","23","IEEE","12 Oct 2016","","","IEEE","IEEE Journals"
"Atrous cGAN for SAR to Optical Image Translation","J. Noa Turnes; J. D. B. Castro; D. L. Torres; P. J. S. Vega; R. Q. Feitosa; P. N. Happ","Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","17 Dec 2021","2022","19","","1","5","Conditional (cGAN)-based methods proposed so far for synthetic aperture radar (SAR)-to-optical image synthesis tend to produce noisy and unsharp optical outcomes. In this work, we propose the atrous-cGAN, a novel cGAN architecture that improves the SAR-to-optical image translation. The proposed generator and discriminator networks rely on atrous convolutions and incorporate an atrous spatial pyramid pooling (ASPP) module to enhance fine details in the generated optical image by exploiting spatial context at multiple scales. This letter reports experiments carried out to assess the performance of atrous-cGAN for the synthesis of Landsat-8 images from Sentinel-1A data based on three public data sets. The experimental analysis indicated that the atrous-cGAN consistently outperformed the classical pix2pix counterpart in terms of visual quality, similar to the true optical image, and as a feature learning tool for semantic segmentation.","1558-0571","","10.1109/LGRS.2020.3031199","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES); Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq); NVIDIA Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241239","Atrous spatial pyramid pooling (ASPP);generative adversarial networks;synthetic aperture radar (SAR)-optical synthesis","Optical imaging;Optical sensors;Generators;Synthetic aperture radar;Convolutional codes;Optical interferometry;Artificial satellites","geophysical image processing;image classification;image enhancement;image segmentation;learning (artificial intelligence);object detection;optical images;radar imaging;synthetic aperture radar","incorporate;atrous spatial pyramid pooling module;generated optical image;Landsat-8 images;atrous-cGAN;atrous cGAN;conditional-based methods;radar-to-optical image synthesis;noisy outcomes;unsharp optical outcomes;SAR-to-optical image translation;generator;discriminator networks;atrous convolutions;current 1.0 A","","10","","18","IEEE","27 Oct 2020","","","IEEE","IEEE Journals"
"Attribute-Guided Generative Adversarial Network With Improved Episode Training Strategy for Few-Shot SAR Image Generation","Y. Sun; Y. Wang; L. Hu; Y. Huang; H. Liu; S. Wang; C. Zhang","National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; Science and Technology on Electromagnetic Scattering Laboratory, Beijing Institute of Environmental Features, Beijing, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13 Feb 2023","2023","16","","1785","1801","Deep-learning-based models usually require a large amount of data for training, which guarantees the effectiveness of the trained model. Generative models are no exception, and sufficient training data are necessary for the diversity of generated images. However, for synthetic aperture radar (SAR) images, data acquisition is expensive. Therefore, SAR image generation under a few training samples is still a challenging problem to be solved. In this article, we propose an attribute-guided generative adversarial network (AGGAN) with an improved episode training strategy for few-shot SAR image generation. First, we design the AGGAN structure, and spectral normalization is used to stabilize the training in the few-shot situation. The attribute labels of AGGAN are designed to be the category and aspect angle labels, which are essential information for SAR images. Second, an improved episode training strategy is proposed according to the characteristics of the few-shot generative task, and it can improve the quality of generated images in the few-shot situation. In addition, we explore the effectiveness of the proposed method when using different auxiliary data for training and use the Moving and Stationary Target Acquisition and Recognition benchmark dataset and a simulated SAR dataset for verification. The experimental results show that AGGAN and the proposed improved episode training strategy can generate images of better quality when compared with some existing methods, which have been verified through visual observation, image similarity measures, and recognition experiments. When applying the generated images to the 5-shot SAR image recognition problem, the average recognition accuracy can be improved by at least 4$\%$.","2151-1535","","10.1109/JSTARS.2023.3239633","National Natural Science Foundation of China(grant numbers:61671354); National Radar Signal Processing Laboratory(grant numbers:KGJ202206); Higher Education Discipline Innovation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025584","Few-shot image generation;generative adversarial network (GAN);meta-learning;synthetic aperture radar (SAR);transfer learning","Training;Radar polarimetry;Transfer learning;Task analysis;Image synthesis;Image recognition;Data models","","","","","","67","CCBYNCND","24 Jan 2023","","","IEEE","IEEE Journals"
"SAR Target Image Generation Method Using Azimuth-Controllable Generative Adversarial Network","C. Wang; J. Pei; X. Liu; Y. Huang; D. Mao; Y. Zhang; J. Yang","Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Nov 2022","2022","15","","9381","9397","Sufficient synthetic aperture radar (SAR) target images are very important for the development of research works. However, available SAR target images are often limited in practice, which hinders the progress of SAR application. In this article, we propose an azimuth-controllable generative adversarial network to generate precise SAR target images with an intermediate azimuth between two given SAR images' azimuths. This network mainly contains three parts: 1) generator, 2) discriminator, and 3) predictor. Through the proposed specific network structure, the generator can extract and fuse the optimal target features from two input SAR target images to generate an SAR target image. Then, a similarity discriminator and an azimuth predictor are designed. The similarity discriminator can differentiate the generated SAR target images from the real SAR images to ensure the accuracy of the generated while the azimuth predictor measures the difference of azimuth between the generated and the desired to ensure the azimuth controllability of the generated. Therefore, the proposed network can generate precise SAR images, and their azimuths can be controlled well by the inputs of the deep network, which can generate the target images in different azimuths to solve the small sample problem to some degree and benefit the research works of SAR images. Extensive experimental results show the superiority of the proposed method in azimuth controllability and accuracy of SAR target image generation.","2151-1535","","10.1109/JSTARS.2022.3218369","National Natural Science Foundation of China(grant numbers:61901091,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933645","Automatic target recognition (ATR);azimuth-controllable;deep learning;generative adversarial network (GAN);synthetic aperture radar (SAR);target image generation","Synthetic aperture radar;Azimuth;Radar polarimetry;Generative adversarial networks;Generators;Image synthesis;Target recognition","control engineering computing;neural nets;radar computing;radar imaging;radar tracking;synthetic aperture radar;target tracking;telecommunication control","synthetic aperture radar target images;azimuth controllable generative adversarial network;different azimuths;azimuth controllability;generated SAR target images;azimuth predictor;similarity discriminator;input SAR target images;optimal target features;intermediate azimuth;SAR application;available SAR target images;SAR target image generation method","","","","52","CCBY","31 Oct 2022","","","IEEE","IEEE Journals"
"Spatially-varying Warping for Panoramic Image Stitching","J. Xiong; F. Li; F. Long; Y. Xu; S. Wang; J. Xu; Q. Ling","State Key Laboratory of Media Convergence Production Technology and Systems, Xinhua News Agency, Beijing, China; Department of Automation, University of Science and Technology of China, Hefei, China; Chinaso Inc., Beijing, China; Hefei Softec Auto Electronic Co. LTD, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; State Key Laboratory of Media Convergence Production Technology and Systems, Xinhua News Agency, Beijing, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","575","580","Panoramic image stitching is widely implemented to many application scenarios, such as satellite remote sensing detection, street view mapping and medical image generation. In this paper, we study two processes that have great impact on the visual effects of the stitched images and are the most concerned by researchers, namely image registration and stitching seam removal. In contrast to the traditional panoramic image stitching algorithms that use only one global projection transformation matrix for image alignment, this paper proposes a spatially-varying deformation method that uses multiple local transformation matrices, including the APAP (As-Projective-As-Possible) method that can effectively reduce the image alignment error and geometric transformation distortion, the SPHP (Shape-Preserving Half-Projective) method that can combine good alignment accuracy and reduced projection distortion, and the AANAP (Adaptive As-Natural-As-Possible) method that can improve the perspective of non-overlapping regions of the stitched image and good accuracy. The image stitching performance can be well improved.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10032845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032845","Panoramic image stitching;Image registration;Seam removal;Spatially-varying warping","Image registration;Satellites;Image synthesis;Visual effects;Distortion;Software;Image stitching","","","","","","14","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Laser Range Data Denoising via Adaptive and Robust Dictionary Learning","Z. Gao; Q. Li; R. Zhai; F. Lin","Temasek Laboratories, National University of Singapore, Singapore; Shenzhen Key Laboratory of Spatial Smart Sensing and Services, Shenzhen University, Shenzhen, China; Department of Computer Science, School of Informatics, Huazhong Agricultural University, Wuhan, China; Temasek Laboratories, National University of Singapore, Singapore","IEEE Geoscience and Remote Sensing Letters","15 Jun 2015","2015","12","8","1750","1754","Sparse representation (SR) is making significant impact in the computer vision and signal processing communities due to its stunning performance in a variety of applications for images, e.g., denoising, restoration, and synthesis. We propose an adaptive and robust SR algorithm that exploits the characteristics of typical laser range data, i.e., the availability of both range and reflectance data, to realize range data denoising. Specifically, our method estimates the informative level (IL) of each patch according to the variation in both range and reflectance modalities, followed by adaptive dictionary training that assigns dynamic sparsity weights to the patches with different ILs. Furthermore, the l1-norm-based representation fidelity measure is applied to make our method robust to outliers, which are common in laser range measurements. Extensive experiments on synthesized and actual data demonstrate that our method works effectively, resulting in superior performance both visually and quantitatively.","1558-0571","","10.1109/LGRS.2015.2424405","Theory and Methods of Digital Conservation for Cultural Heritage(grant numbers:2012CB725300); National Natural Science Fund of China(grant numbers:41101409); National Basic Research Program of China(grant numbers:2012CB725301); National Surveying and Mapping Geographic Information Public Welfare Industry Special Funding Scientific Research Projects(grant numbers:210600001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7101808","Denoising;dictionary learning;laser range data;sparse representation (SR);Denoising;dictionary learning;laser range data;sparse representation (SR)","Noise reduction;Dictionaries;Robustness;Image restoration;Adaptation models;Image edge detection;Estimation","computerised instrumentation;image denoising;image representation;measurement by laser beam","IL estimation;reflectance modality;£1-norm-based representation;laser range measurement;informative level estimation;robust SR algorithm;image synthesis;image restoration;image denoising;signal processing;computer vision;sparse representation;robust dictionary learning;laser range data denoising","","3","","26","IEEE","5 May 2015","","","IEEE","IEEE Journals"
"Augmenting Data Using Gaussian Mixture Embedding For Improving Land Cover Segmentation","D. A. B. Oliveira","IBM Research Rua Tutoia, 1157, Sao Paulo, Brasil","2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference (LAGIRS)","12 Aug 2020","2020","","","333","338","The use of convolutional neural networks improved greatly data synthesis in the last years and have been widely used for data augmentation in scenarios where very imbalanced data is observed, such as land cover segmentation. Balancing the proportion of classes for training segmentation models can be very challenging considering that samples where all classes are reasonably represented might constitute a small portion of a training set, and techniques for augmenting this small amount of data such as rotation, scaling and translation might be not sufficient for efficient training. In this context, this paper proposes a methodology to perform data augmentation from few samples to improve the performance of CNN-based land cover semantic segmentation. First, we estimate the latent data representation of selected training samples by means of a mixture of Gaussians, using an encoder-decoder CNN. Then, we change the latent embedding used to generate the mixture parameters, at random and in training time, to generate new mixture models slightly different from the original. Finally, we compute the displacement maps between the original and the modified mixture models, and use them to elastically deform the original images, creating new realistic samples out of the original ones. Our disentangled approach allows the spatial modification of displacement maps to preserve objects where deformation is undesired, like buildings and cars, where geometry is highly discriminant. With this simple pipeline, we managed to augment samples in training time, and improve the overall performance of two basal semantic segmentation CNN architectures for land cover semantic segmentation.","","978-1-7281-4350-7","10.1109/LAGIRS48042.2020.9165670","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165670","Land Cover Segmentation;Image Synthesis;Latent Data Representation;Gaussian Mixture Models.","Training;Semantics;Image segmentation;Data models;Mixture models;Databases;Computer architecture","convolutional neural nets;data structures;Gaussian processes;geophysical image processing;image classification;image segmentation;land cover;learning (artificial intelligence);mixture models;principal component analysis","augmented data;Gaussian mixture;land cover segmentation;convolutional neural networks;data synthesis;data augmentation;imbalanced data;training segmentation models;training set;CNN-based land cover semantic segmentation;latent data representation;encoder-decoder CNN;latent embedding;mixture parameters;training time;displacement maps;modified mixture models;basal semantic segmentation CNN architectures","","","","19","IEEE","12 Aug 2020","","","IEEE","IEEE Conferences"
"Multilevel Strong Auxiliary Network for Enhancing Feature Representation to Protect Secret Images","F. Chen; Q. Xing; C. Fan","Air Defense and Anti-Missile College, Air Force Engineering University, Xi'an, China; Air Defense and Anti-Missile College, Air Force Engineering University, Xi'an, China; Air Defense and Anti-Missile College, Air Force Engineering University, Xi'an, China","IEEE Transactions on Industrial Informatics","18 Apr 2022","2022","18","7","4577","4586","Image data play an important role in the network information, however, some images containing sensitive or confidential information are easy to attract the attention of malicious attackers. On the basis of deep learning and data hiding technology, in this article, a novel hiding–revealing network is designed to protect these secret images. The sender uses the hiding network to conceal the secret image into an ordinary cover image and the receiver uses the revealing network to recover the secret image. Symmetrical shortcut connection is designed to improve both the hiding and the revealing performances without adding any parameters. Consider that some secret images may have complex spatial features, a multilevel strong auxiliary module is designed to enhance feature representation and boost the restoration quality of the secret image. Then, a lifeline is proposed to transform the image hiding task into a residual identity mapping, which reduces the difficulty of network learning and obviously improves the hiding performance. In addition, a mixed loss function is designed to further improve the perceptual quality of both the hidden image and the revealed image, which further completely eliminates the secret content in the residual image and ensures the hiding security. Experimental results demonstrate that compared with the state-of-the-art methods, our proposed method achieves the best performance in both hidden image synthesis and secret image restoration.","1941-0050","","10.1109/TII.2021.3123233","National Natural Science Foundation of China(grant numbers:72071209,72001214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591276","Data hiding technology;deep learning;image steganography;multilevel strong auxiliary (MLSA) module;residual identity mapping","Steganography;Security;Image restoration;Feature extraction;Task analysis;Distortion;Remote sensing","data encapsulation;deep learning (artificial intelligence);image representation;image restoration;security of data","strong auxiliary network;secret image protection;image data;network information;deep learning;data hiding technology;ordinary cover image;image hiding task;revealed image;residual image;hidden image synthesis;secret image restoration","","","","30","CCBY","27 Oct 2021","","","IEEE","IEEE Journals"
"Cloud Removal in Satellite Images Using Spatiotemporal Generative Networks","V. Sarukkai; A. Jain; B. Uzkent; S. Ermon","Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University; Computer Science Department, Stanford University","2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","1785","1794","Satellite images hold great promise for continuous environmental monitoring and earth observation. Occlusions cast by clouds, however, can severely limit coverage, making ground information extraction more difficult. Existing pipelines typically perform cloud removal with simple temporal composites and hand-crafted filters. In contrast, we cast the problem of cloud removal as a conditional image synthesis challenge, and we propose a trainable spatiotemporal generator network (STGAN) to remove clouds. We train our model on a new large-scale spatiotemporal dataset that we construct, containing 97640 image pairs covering all continents. We demonstrate experimentally that the proposed STGAN model outperforms standard models and can generate realistic cloud-free images with high PSNR and SSIM values across a variety of atmospheric conditions, leading to improved performance in downstream tasks such as land cover classification.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093564","","Clouds;Satellites;Agriculture;Spatiotemporal phenomena;Generators;Feature extraction;Gallium nitride","clouds;environmental monitoring (geophysics);feature extraction;geophysical image processing;image classification;remote sensing;terrain mapping","hand-crafted filters;cloud removal;conditional image synthesis challenge;trainable spatiotemporal generator network;large-scale spatiotemporal dataset;image pairs;realistic cloud-free images;satellite images;spatiotemporal generative networks;continuous environmental monitoring;occlusions cast;ground information extraction;temporal composites","","18","","35","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Formation Flying SAR: Analysis of Imaging Performance by Array Theory","A. Renga; M. D. Graziano; A. Moccia","University of Naples Federico II, Naples, Italy; University of Naples Federico II, Naples, Italy; University of Naples Federico II, Naples, Italy","IEEE Transactions on Aerospace and Electronic Systems","8 Jun 2021","2021","57","3","1480","1497","This article analyzes the process of image synthesis for a formation flying synthetic aperture radar (FF-SAR), which is a multistatic synthetic aperture radar (SAR) based on a cluster of receiving-only satellites flying in a close formation, in the framework of the array theory. Indeed, the imaging properties of different close receivers, when analyzed as isolated items, are very similar and form the so-called common array. Moreover, the relative positions among the receivers implicitly define a physical array, referred to as spatial diversity array. FF-SAR imaging can be verified as a result of the spatial diversity array weighting the common array. Hence, different approaches to beamforming can be applied to the spatial diversity array to provide the FF-SAR with distinctive capabilities, such as coherent resolution enhancement and high-resolution wide-swath imaging. Simulation examples are discussed which confirm that array theory is a powerful tool to quickly and easily characterize FF-SAR imaging performance.","1557-9603","","10.1109/TAES.2020.3043526","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288956","Array theory;distributed arrays;formation flying SAR (FF-SAR);high-resolution wide-swath imaging;multistatic SAR;spaceborne SAR;synthetic aperture radar(sar)","Receivers;Synthetic aperture radar;Imaging;Satellites;Transmitters;Radar imaging;Spatial diversity","array signal processing;radar imaging;remote sensing by radar;synthetic aperture radar","common array;spatial diversity array;high-resolution wide-swath imaging;array theory;FF-SAR imaging performance;formation flying SAR;image synthesis;multistatic synthetic aperture radar;receiving-only satellites;imaging properties;receivers;physical array","","4","","56","CCBY","9 Dec 2020","","","IEEE","IEEE Journals"
"Extracting Camera Pose Using Single Image Super Resolution Networks","B. Koskowich; M. Starek","Conrad Blucher Institute for Surveying Science, College of Science and Engineering, Texas A&M University Corpus Christi, Corpus Christi, Texas, USA; Conrad Blucher Institute for Surveying Science, College of Science and Engineering, Texas A&M University Corpus Christi, Corpus Christi, Texas, USA","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1873","1876","This work proposes a mechanism which can be used as a basis for allowing camera POSE information to be maintained reliably during loss or interference with inertial motion unit or positioning system integration. This basis is formed by employing image synthesis networks with atypical data for the network type: inputs are normal down scaled source imagery while outputs are native resolution images composed of the contents of the same scene viewed from a fixed offset position. The goal of this application is to simulate the presence of a binary camera from monocular hardware, which makes feasible certain POSE estimation workflows which would normally require binary cameras on monocular platforms. Being able to rapidly synthesize images of additional camera positions without having to physically navigate to those positions allows for two methods to build off each other. First, knowing that the model should consistently maintain a specific POSE from the source camera allows synthetic images to be used to artificially inflate available data during structure from motion processing with confidence in the accuracy of synthetic points. It also enables the comparison of an image at an actual physical location with the synthetic one later as a measure of POSE accuracy which can be incorporated into a solution for computing POSE of the image source.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323098","","Cameras;Training;Gallium nitride;Testing;Pose estimation;Neural networks;Three-dimensional displays","","","","","","16","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Color Texture Analysis: A Survey","A. Humeau-Heurtier","Univ Angers, LARIS, SFR MATHSTIC, Angers, France","IEEE Access","17 Oct 2022","2022","10","","107993","108003","In the field of image processing, texture features and color are fundamental visual cue with complementary roles. They are used in many applications and in a large variety of areas such as quality control, content-based image retrieval, remote sensing, industrial inspection, surface inspection, object recognition, and medical image analysis. For this purpose, a large number of algorithms have been proposed for texture feature extraction. Some of them are dedicated to gray-scale images while others aim at processing both color and texture. It has been shown that, for many cases, the use of color improves the performance of gray level texture classification. This paper provides a comprehensive survey of the texture feature extraction methods that consider both texture and color information. We propose a categorization of these methods into seven classes, two of them being very recent. For each method, we present the concept, the advantages and drawbacks, and we give examples of application.","2169-3536","","10.1109/ACCESS.2022.3213439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915369","Chrominance;classification;color texture;feature extraction;image processing;image synthesis;luminance;segmentation;shape from texture;texture","Image color analysis;Histograms;Feature extraction;Colored noise;Visualization;Lighting;Color","content-based retrieval;feature extraction;image classification;image colour analysis;image retrieval;image texture;medical image processing;object recognition;quality control","complementary roles;quality control;content-based image retrieval;remote sensing;industrial inspection;surface inspection;medical image analysis;texture feature extraction;gray-scale images;gray level texture classification;color information;color texture analysis;image processing;texture features;fundamental visual cue","","1","","92","CCBY","10 Oct 2022","","","IEEE","IEEE Journals"
"Comparison on Generative Adversarial Networks –A Study","A. Sharma; N. Jindal; A. Thakur","Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India; Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India; Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India","2018 First International Conference on Secure Cyber Computing and Communication (ICSCCC)","2 May 2019","2018","","","391","396","Various new deep learning models have been invented, among which generative adversarial networks have gained exceptional prominence in last four years due to its property of image synthesis. GANs have been utilized in diverse fields ranging from conventional areas like image processing, biomedical signal processing, remote sensing, video generation to even off beat areas like sound and music generation. In this paper, we provide an overview of GANs along with its comparison with other networks, as well as different versions of Generative Adversarial Networks.","","978-1-5386-6373-8","10.1109/ICSCCC.2018.8703267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703267","Machine learning;Generative Adversarial Networks","Generative adversarial networks;Training;Generators;Gallium nitride;Loss measurement;Computational modeling","image processing;learning (artificial intelligence)","deep learning models;image synthesis;GANs;image processing;biomedical signal processing;video generation;music generation;sound generation;generative adversarial networks","","3","","22","IEEE","2 May 2019","","","IEEE","IEEE Conferences"
