"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Remote Sensing Image Fusion Using Ripplet Transform and Compressed Sensing","M. Ghahremani; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","IEEE Geoscience and Remote Sensing Letters","16 Sep 2014","2015","12","3","502","506","In this letter, we propose a novel remote sensing image fusion method based on the ripplet transform and the compressed sensing (CS) theory. The ripplet transform generalizes the curvelet transform by adding two parameters, namely, support c and degree d. These parameters provide the ripplet transform with anisotropy capability of representing singularities along arbitrarily shaped curves, and the curvelet transform is just a special case of the ripplet transform with c=1 and d=2. In the proposed method, the spatial details are first extracted from the PAN image by means of ripplets, and then, they are injected into the MS bands by the proposed injection model named CS-based injection. The aim of this model, which is based on the CS theory, is to minimize the spectral distortion in the pan-sharpened MS bands with respect to the original ones. The experimental results carried out on IKONOS and QuickBird data sets demonstrate that the proposed method provides better fused images in terms of the visual and quantitative evaluations.","1558-0571","","10.1109/LGRS.2014.2347955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6883128","Compressed sensing (CS);curvelet transform;image fusion;multiresolution analysis (MRA);ripplet transform;sparse representation;Compressed sensing (CS);curvelet transform;image fusion;multiresolution analysis (MRA);ripplet transform;sparse representation","Transforms;Remote sensing;Image fusion;Spatial resolution;Vectors;Dictionaries","compressed sensing;curvelet transforms;geophysical techniques;image fusion;remote sensing","ripplet transform;remote sensing image fusion method;compressed sensing theory;CS theory;curvelet transform;singularity anisotropy capability;shaped curve;PAN image;injection model;CS-based injection;spectral distortion minimization;pan-sharpened MS band;IKONOS data set;QuickBird data set;visual evaluation;quantitative evaluation","","82","","26","IEEE","26 Aug 2014","","","IEEE","IEEE Journals"
"A Remote Sensing Image Fusion Method Based on the Analysis Sparse Model","C. Han; H. Zhang; C. Gao; C. Jiang; N. Sang; L. Zhang","School of Automation, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan University, China; School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Automation, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan University, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","1","439","453","This paper addresses the remote sensing image fusion problem from the perspective of the analysis sparse model. As an alternative to the synthesis sparse representation approach, the analysis sparse model can yield richer feature representations and better results for image restoration. We, therefore, propose an image fusion method for remote sensing images based on the analysis sparse model. In this method, the analysis operators for the high-resolution multispectral (HR MS) image are trained band by band, directly from the source images, which can greatly improve the adaptability. During the analysis operator learning stage, the geometric analysis operator learning (GOAL) algorithm is utilized with the upsampled low-resolution MS (LR MS) image and the HR panchromatic (HR PAN) image, which does not require an external HR MS image data set. Moreover, the imagery system modulation transfer function (MTF) is considered during the LR MS imaging modeling process, which greatly extends the practical application potential of the proposed method. The simulated and real-data experimental results on IKONOS and QuickBird data sets show that the proposed method can effectively preserve the spectral information and the spatial detail of the image. The fused HR MS images produced by the proposed method are comparable and even superior to the images fused by the other state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2015.2507859","National High-tech Research and Development Program(grant numbers:2013AA12A301); National Natural Science Foundation of China(grant numbers:41571362,61201342,61433007); Key Laboratory of Agri-informatics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7373564","Analysis operator;analysis sparse model;image fusion;inverse problem;remote sensing;Analysis operator;analysis sparse model;image fusion;inverse problem;remote sensing","Image fusion;Remote sensing;Analytical models;Sparse matrices;Inverse problems;Spatial resolution","geophysical image processing;image fusion;image representation;image resolution;image restoration;image sampling;learning (artificial intelligence);optical transfer function;remote sensing","remote sensing image fusion method;analysis sparse model;synthesis sparse representation approach;image restoration;high-resolution multispectral imaging;HR MS imaging;geometric analysis operator learning algorithm;GOAL algorithm;upsampled low-resolution imaging;LR MS imaging;HR panchromatic imaging;HR PAN imaging;modulation transfer function;MTF;IKONOS data set;QuickBird data set","","30","","65","IEEE","6 Jan 2016","","","IEEE","IEEE Journals"
"Remote Sensing Image Fusion With Deep Convolutional Neural Network","Z. Shao; J. Cai","Collaborative Innovation Center for Geospatial Technology, Wuhan, China; Collaborative Innovation Center for Geospatial Technology, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Apr 2018","2018","11","5","1656","1669","Remote sensing images with different spatial and spectral resolution, such as panchromatic (PAN) images and multispectral (MS) images, can be captured by many earth-observing satellites. Normally, PAN images possess high spatial resolution but low spectral resolution, while MS images have high spectral resolution with low spatial resolution. In order to integrate spatial and spectral information contained in the PAN and MS images, image fusion techniques are commonly adopted to generate remote sensing images at both high spatial and spectral resolution. In this study, based on the deep convolutional neural network, a remote sensing image fusion method that can adequately extract spectral and spatial features from source images is proposed. The major innovation of this study is that the proposed fusion method contains a two branches network with the deeper structure which can capture salient features of the MS and PAN images separately. Besides, the residual learning is adopted in our network to thoroughly study the relationship between the high- and low-resolution MS images. The proposed method mainly consists of two procedures. First, spatial and spectral features are respectively extracted from the MS and PAN images by convolutional layers with different depth. Second, the feature fusion procedure utilizes the extracted features from the former step to yield fused images. By evaluating the performance on the QuickBird and Gaofen-1 images, our proposed method provides better results compared with other classical methods.","2151-1535","","10.1109/JSTARS.2018.2805923","National key R & D plan on strategic international scientific and technological innovation cooperation(grant numbers:2016YFE0202300); Fundamental Research Funds for the Central Universities(grant numbers:2042016kf0179,2042016kf1019); Wuhan Chen Guang Project(grant numbers:2016070204010114); Guangzhou science and technology project(grant numbers:201604020070); Special Task of Technical Innovation in Hubei Province(grant numbers:2016AAA018); National Natural Science Foundation of China(grant numbers:61671332,41771452,41771454); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8314460","Deep convolutional neural network;multispectral image;panchromatic image;remote sensing image fusion","Remote sensing;Feature extraction;Image fusion;Spatial resolution;Transforms;Multiresolution analysis","convolution;feature extraction;feedforward neural nets;geophysical image processing;image classification;image fusion;image resolution;remote sensing","deep convolutional neural network;panchromatic images;multispectral images;PAN images;high spatial resolution;low spectral resolution;MS images;high spectral resolution;low spatial resolution;spatial information;spectral information;image fusion techniques;remote sensing images;remote sensing image fusion method;spectral features;spatial features;source images;low-resolution;feature fusion procedure;fused images;spatial resolution","","202","","57","IEEE","12 Mar 2018","","","IEEE","IEEE Journals"
"A Dual-Domain Super-Resolution Image Fusion Method With SIRV and GALCA Model for PolSAR and Panchromatic Images","W. Liu; J. Yang; J. Zhao; F. Guo","School of Geography, Geomatics and Planning, Jiangsu Normal University, Xuzhou, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Jiangsu Key Laboratory of Resources and Environmental Information Engineering, School Environment and Spatial Information and China University of Mining and technology, Xuzhou, China; School of Geography, Geomatics and Planning, Jiangsu Normal University, Xuzhou, China","IEEE Transactions on Geoscience and Remote Sensing","25 Feb 2022","2022","60","","1","14","Hyperspectral/multispectral and panchromatic of optical remote sensing images are commonly used for multisensor image fusion, which has been applied in various applications of Earth observation. However, the utilization of optical remote sensing data suffers from the limitation of bad weather and cloud contamination. To address aforementioned issue and enhance spatial details of polarimetric synthetic aperture radar (PolSAR) image, a novel dual-domain super-resolution image fusion method is proposed by combining improved spherically invariant random vector (ISIRV) model with generalized adaptive linear combination approximation (GALCA) technology in this study. The proposed method decomposes the task of image fusion into polarimetric and texture domain fusion by integrating polarimetric components of PolSAR image and texture detail component of panchromatic image, which can significantly improve spatial resolutions of the PolSAR image while preserving polarimetric information. The data fusion experiment is implemented with three data sets including panchromatic images of Gaofen-1 (GF-1) and Gaofen-2 (GF-2) and the quad-pol SAR data of Gaofen-3 (GF-3) and Radarsat-2. Results show that the proposed dual-domain image fusion method provides a better performance compared with state-of-the-art multisensor fusion methods (BT, PCA, GS, indusion, and PRACS) regarding qualitative and quantitative evaluations. In addition, results of image fusion are applied to image classification over agricultural and urban areas of China, which shows that classification accuracy is significantly improved when compared with the result using only the original image.","1558-0644","","10.1109/TGRS.2021.3134099","National Natural Science Foundation of China(grant numbers:41771377,42071295,41901286,62101219); Natural Science Foundation of Jiangsu(grant numbers:BK20201026,BK20210921); Science Foundation of Jiangsu Normal University(grant numbers:19XSRX006); Science and Technology Innovation Key project of Shenzhen(grant numbers:JCYJ20200109150833977); Open Research Fund of Jiangsu Key Laboratory of Resources and Environmental Information Engineering, China University of Mining and Technology(grant numbers:JS202107,JS202108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643026","Generalized adaptive linear combination approximation (ALCA) method;improved spherically invariant random vector (SIRV) model;polarimetric domain;super-resolution image fusion;texture domain","Image fusion;Optical imaging;Optical sensors;Spatial resolution;Optical scattering;Adaptive optics;Remote sensing","agriculture;geophysical image processing;image classification;image fusion;image resolution;image sensors;radar imaging;radar polarimetry;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","state-of-the-art multisensor fusion methods;image classification;panchromatic image;optical remote sensing images;multisensor image fusion;optical remote sensing data suffers;polarimetric synthetic aperture radar image;novel dual-domain super-resolution image fusion method;spherically invariant random vector model;generalized adaptive linear combination approximation;polarimetric texture domain fusion;PolSAR image;spatial resolutions;data fusion experiment;Gaofen-1;Gaofen-2;Gaofen-3;dual-domain image fusion method","","1","","46","IEEE","8 Dec 2021","","","IEEE","IEEE Journals"
"A New Saliency-Driven Fusion Method Based on Complex Wavelet Transform for Remote Sensing Images","L. Zhang; J. Zhang","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","4 Dec 2017","2017","14","12","2433","2437","In remote sensing images, demands for spectral and spatial resolution vary from region to region. Regions with abundant texture and well-defined boundaries (like residential areas and roads) need more spatial details to provide better descriptions of various ground objects while regions such as farmland and mountains are mainly discriminated by spectral characteristic. However, most existing fusion algorithms for remote sensing images execute a unified processing in the whole image, leaving those important needs out of consideration. The employment of diverse fusion strategy for regions with different needs can provide an effective solution to this problem. In this letter, we propose a new saliency-driven fusion method based on complex wavelet transform. First, an adaptive saliency detection method based on clustering and spectral dissimilarity is presented to generate saliency factor for indicating diverse needs of the two kinds of resolutions in regions. Then, we combine nonlinear intensity-hue-saturation transform with multiresolution analysis based on dual-tree complex wavelet transform in order to complement each other's advantages. Finally, saliency factor is employed to control the detail injection in the fusion, helping to satisfy different needs of different regions. Experiments reveal the validity and advantages of our proposal.","1558-0571","","10.1109/LGRS.2017.2768070","National Natural Science Foundation of China(grant numbers:41771407,61571050); Beijing Natural Science Foundation(grant numbers:4162033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8116627","Complex wavelet transform;image fusion;remote sensing;saliency detection;spectral dissimilarity","Remote sensing;Wavelet transforms;Multiresolution analysis;Spatial resolution;Image fusion;Visualization","geophysical image processing;image colour analysis;image fusion;image resolution;image texture;object detection;remote sensing;trees (mathematics);wavelet transforms","remote sensing images;spectral resolution;spatial resolution;residential areas;roads;spectral characteristic;diverse fusion strategy;saliency factor;complex wavelet transform;saliency-driven fusion;adaptive saliency detection;nonlinear intensity-hue-saturation transform;multiresolution analysis;dual-tree complex wavelet transform","","37","","19","IEEE","21 Nov 2017","","","IEEE","IEEE Journals"
"Deep SURE for Unsupervised Remote Sensing Image Fusion","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson; M. Dalla Mura","Department of Electrical and Electronic Engineering, Nha Trang University, Nha Trang, Vietnam; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavík, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavík, Iceland; Institut Universitaire de France (IUF), Paris, France","IEEE Transactions on Geoscience and Remote Sensing","8 Nov 2022","2022","60","","1","13","Image fusion is utilized in remote sensing (RS) due to the limitation of the imaging sensor and the high cost of simultaneously acquiring high spatial and spectral resolution images. Optical RS imaging systems usually provide images of high spatial resolution but low spectral resolution and vice versa. Therefore, fusing those images to obtain a fused image having both high spectral and spatial resolution is desirable in many applications. This article proposes a fusion framework using an unsupervised convolutional neural network (CNN) and Stein’s unbiased risk estimate (SURE). We derive a new loss function for a CNN that incorporates a backprojection mean square error (MSE) with SURE to estimate the projected mse between the fused image and the ground truth. The main motivation is that training a CNN with this SURE loss function is unsupervised and avoids overfitting. Experimental results for two fusion examples, multispectral and hyperspectral (MS–HS) image fusion and multispectral and multispectral (MS–MS) image fusion, show that the proposed method yields high-quality fused images and outperforms the competitive methods. Codes are available at https://github.com/hvn2/Deep-SURE-Fusion.","1558-0644","","10.1109/TGRS.2022.3215902","Icelandic Research Fund(grant numbers:207233-051); University of Iceland Doctoral Fund(grant numbers:1547-154305); ANR FuMultiSPOC(grant numbers:ANR-20-ASTR-0006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9924190","Image fusion;multispectral and hyperspectral (MS–HS) image fusion;pansharpening;remote sensing (RS);Sentinel 2 sharpening;Stein’s unbiased risk estimate (SURE);unsupervised convolutional neural networks (CNNs)","Spatial resolution;Image fusion;Convolutional neural networks;Electronics packaging;Training;Pansharpening;Optical sensors","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image fusion;image resolution;mean square error methods;remote sensing;unsupervised learning","CNN;loss function;high-quality fused images;imaging sensor;spatial resolution images;spectral resolution images;unsupervised convolutional neural network;Stein unbiased risk estimate;deep SURE;unsupervised remote sensing image fusion;optical RS imaging systems;backprojection mean square error;MSE;multispectral image fusion;hyperspectral image fusion","","","","75","IEEE","19 Oct 2022","","","IEEE","IEEE Journals"
"Airport Detection Based on a Multiscale Fusion Feature for Optical Remote Sensing Images","Z. Xiao; Y. Gong; Y. Long; D. Li; X. Wang; H. Liu","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Plateau Ecology and Agriculture, Qinghai University, Xining, China; School of Urban Design, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","25 Aug 2017","2017","14","9","1469","1473","Automatically detecting airports from remote sensing images has attracted significant attention due to its importance in both military and civilian fields. However, the diversity of illumination intensities and contextual information makes this task difficult. Moreover, auxiliary features both within and surrounding the regions of interest are usually ignored. To address these problems, we propose a novel method that uses a multiscale fusion feature to represent the complementary information of each region proposal, which is extracted by constructing a GoogleNet with a light feature module model that has an additional light fully connected layer. Then, the fusion feature is input to a support vector machine whose performance is enhanced using a hard negative mining method. Finally, a simplified localization method is applied to tackle the problem of box redundancy and to optimize the locations of airports. An experiment demonstrates that the fusion feature outperforms other features on airport detection tasks from remote sensing images containing complicated contextual information.","1558-0571","","10.1109/LGRS.2017.2712638","National Key Research and Development Program of China(grant numbers:2016YFB0502602); Natural Science Foundation of China(grant numbers:41471324); Natural Science Foundation of China(grant numbers:61363019); National Natural Science Foundation of Qinghai Province(grant numbers:2014-ZJ-718); Internally Funded Projects by LIESMARS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979532","Convolutional neural network (CNN);GoogleNet-light feature (GoogleNet-LF);hard negative mining (HNM);multiscale deep fusion feature;remote-sensing airport detection","Feature extraction;Airports;Remote sensing;Support vector machines;Image segmentation;Roads;Object detection","geophysical image processing;image fusion;remote sensing;support vector machines","airport detection;multiscale fusion feature;optical remote sensing images;military fields;civilian fields;auxiliary features;GoogleNet;light feature module model;light fully connected layer;support vector machine;hard negative mining method;simplified localization method;box redundancy","","57","","20","IEEE","13 Jul 2017","","","IEEE","IEEE Journals"
"Semisupervised Remote Sensing Image Fusion Using Multiscale Conditional Generative Adversarial Network With Siamese Structure","X. Jin; S. Huang; Q. Jiang; S. -J. Lee; L. Wu; S. Yao","Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Institute of Technology Management, National Chiao Tung University, Hsinchu, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Jul 2021","2021","14","","7066","7084","Remote sensing image fusion (RSIF) can generate an integrated image with high spatial and spectral resolution. The fused remote sensing image is conducive to applications including disaster monitoring, ecological environment investigation, and dynamic monitoring. However, most existing deep learning based RSIF methods require ground truths (or reference images) to train a model, and the acquisition of ground truths is a difficult problem. To address this, we propose a semisupervised RSIF method based on the multiscale conditional generative adversarial networks by combining the multiskip connection and pseudo-Siamese structure. This new method can simultaneously extract the features of panchromatic and multispectral images to fuse them without a ground truth; the adopted multiskip connection contributes to presenting image details. In addition, we propose a composite loss function, which combines the least squares loss, L1 loss, and peak signal-to-noise ratio loss to train the model; the composite loss function can help to retain the spatial details and spectral information of the source images. Moreover, we verify the proposed method by extensive experiments, and the results show that the new method can achieve outstanding performance without relying on the ground truth.","2151-1535","","10.1109/JSTARS.2021.3090958","National Natural Science Foundation of China(grant numbers:61863036,62002313); China Postdoctoral Science Foundation(grant numbers:2020T130564,2019M653507); Key Areas Research Program of Yunnan Province in China(grant numbers:202001BB050076); Key Laboratory in Software Engineering of Yunnan Province in China(grant numbers:2020SE408); Yunnan Provincial Postdoctoral Science Foundation; Yunnan University's Research Innovation Fund for Graduate Students(grant numbers:2020230,2020231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9461404","Conditional generative adversarial network (cGAN);deep learning (DL);image fusion;loss function;remote sensing image fusion (RSIF)","Image fusion;Spatial resolution;Remote sensing;Feature extraction;Generative adversarial networks;Sensors;Fuses","geophysical image processing;geophysical techniques;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing","semisupervised remote sensing image fusion;multiscale conditional generative adversarial network;Siamese structure;spatial resolution;spectral resolution;ecological environment investigation;dynamic monitoring;deep learning;RSIF methods;ground truth;semisupervised RSIF method;pseudoSiamese structure;panchromatic images;multispectral images;image details;composite loss function;signal-to-noise ratio loss;multiskip connection","","8","","50","CCBY","21 Jun 2021","","","IEEE","IEEE Journals"
"Remote Sensing Image Fusion Based on Nonnegative Dictionary Learning","J. Wang; Y. Jiang; Y. Qi; Y. Zhai","Guangzhou College of Technology and Business, Institute of Engineering, Guangzhou, Guangdong, China; Ideological and Political Theory Teaching Department, South China Business College, Guangdong University of Foreign Studies, Guangzhou, Guangdong, China; Faculty of Megadate and Computing, Guangdong Baiyun University, Guangzhou, Guangdong, China; College of Advanced Interdisciplinary Studies, National University of Defense Technology, Changsha, Hunan, China","IEEE Access","7 Dec 2021","2021","9","","158908","158916","For the problem of Panchromatic and multispectral remote sensing image fusion, we propose a remote sensing image fusion algorithm based on nonnegative dictionary learning. The basic idea of the algorithm is to use the panchromatic image with high spatial resolution to learn the high-low resolution dictionary pair, and to improve the fusion effect of remote sensing image by combining the nonnegativity of the image. Firstly, high resolution dictionary and low resolution dictionary are learned from high spatial resolution panchromatic image by nonnegative dictionary learning technology. Then multispectral image is sparsely represented by low resolution dictionary to obtain coefficient matrix. Finally, using coefficient matrix and high resolution dictionary, high resolution multispectral image is reconstructed. Compared with state-of-the-art methods, the proposed algorithm can get high spatial resolution and well preserve spectral information of multispectral image. Our experimental results of real QUICKBIRD and IKONOS remote sensing image fusion validate the effectiveness of the proposed algorithm.","2169-3536","","10.1109/ACCESS.2021.3131268","Major Scientific Research Projects in Guangdong Province(grant numbers:2018KTSCX331,2018KQNCX378); Ministry of Education Cooperative Education(grant numbers:201802123151,201902084029); Feature Innovation Project of Colleges and Universities in Guangdong Province(grant numbers:2020ktscx163); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627909","Image fusion;pan-sharpening;dictionary learning;sparse representation;remote sensing image","Dictionaries;Remote sensing;Machine learning;Spatial resolution;Image fusion;Sparse matrices;Matching pursuit algorithms","geophysical image processing;image fusion;image representation;image resolution;learning (artificial intelligence);remote sensing","multispectral remote sensing image fusion;remote sensing image fusion algorithm;high-low resolution dictionary pair;fusion effect;nonnegativity;high resolution dictionary;high spatial resolution panchromatic image;nonnegative dictionary learning technology;coefficient matrix;high resolution multispectral image","","2","","28","CCBY","29 Nov 2021","","","IEEE","IEEE Journals"
"Spatial Dynamic Selection Network for Remote-Sensing Image Fusion","J. Hu; P. Hu; Z. Wang; X. Kang; S. Fan; D. Mao","Key Laboratory of Electric Power Robot of Hunan Province, School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; Key Laboratory of Electric Power Robot of Hunan Province, School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; Key Laboratory of Electric Power Robot of Hunan Province, School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Key Laboratory of Electric Power Robot of Hunan Province, School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; Key Laboratory of Intelligent Live Working Technology and Equipment (Robot) of Hunan Province, Live Inspection and Intelligent Operation Technology State Grid Corporation Laboratory, State Grid Hunan Transmission Maintenance Company, Hengyang, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Nowadays, high-resolution images with rich spectral information are necessary for earth observation. Remote-sensing image fusion is an effective method to provide high-resolution multispectral images, which are obtained by fusing high-resolution panchromatic images and low-resolution multispectral images. However, existing methods mostly use the same network for image feature extraction, without considering the differences among different pixels, resulting in that the extracted features are not accurate enough. This letter proposes a spatial dynamic selection network for remote-sensing image fusion. A dynamic feature extraction module composed of multiple spatial dynamic blocks (SDBs) and cross-scale context connection blocks (CSCBs) is designed. The SDB can extract image features according to the input by different networks, and realize dynamic selection of pixel features. Since the spatial structure and spectral characteristic of each pixel are different, two complementary branches are designed in the SDB to extract different features, which improves the capability of feature extraction. Multiscale network structure is designed to obtain more abundant information and the CSCB is used to integrate the information of different scales. Experimental results on GeoEye-1 and WorldView-3 datasets demonstrate the superiority of the proposed method.","1558-0571","","10.1109/LGRS.2021.3085140","National Natural Science Foundation of China(grant numbers:61601061); Open Research Fund of Key Laboratory of Intelligent Live Working Technology and Equipment (Robot) of Hunan Province(grant numbers:2019KZD1006); Open Research Fund of Key Laboratory of Electric Power Robot of Hunan Province(grant numbers:PROF1902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451167","Convolutional neural network;cross-scale context connection;remote-sensing image fusion;spatial dynamic selection","Feature extraction;Convolution;Remote sensing;Image fusion;Logic gates;Training;Robots","feature extraction;geophysical image processing;image fusion;image resolution;remote sensing","remote-sensing image fusion;high-resolution images;high-resolution multispectral images;high-resolution panchromatic images;low-resolution multispectral images;image feature extraction;spatial dynamic selection network;dynamic feature extraction module;multiple spatial dynamic blocks;image features;pixel features;spatial structure;spectral characteristic","","1","","19","IEEE","10 Jun 2021","","","IEEE","IEEE Journals"
"Hyperspectral Image Fusion and Multitemporal Image Fusion by Joint Sparsity","H. Pan; Z. Jing; H. Leung; M. Li","School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada; School of Aeronautics and Astronautics, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2021","2021","59","9","7887","7900","Different image fusion systems have been developed to deal with the massive amounts of image data for different applications, such as remote sensing, computer vision, and environment monitoring. However, the generalizability and versatility of these fusion systems remain unknown. This article proposes an efficient regularization framework to achieve different kinds of fusion tasks accounting for the spatiospectral and spatiotemporal variabilities of the fusion process. A joint minimization functional is developed by taking an advantage of a composite regularizer for enforcing joint sparsity in the gradient domain and the frame domain. The proposed composite regularizer is composed of the Hessian Schatten-norm regularization and contourlet-based regularization terms. The resulting problems are solved by the alternating direction method of multipliers (ADMM). The effectiveness of the proposed method is validated in a variety of image fusion experiments: 1) hyperspectral (HS) and panchromatic image fusion; 2) HS and multispectral image fusion; 3) multitemporal image fusion (MIF); and 4) multi-image deblurring. Results show promising performance compared with state-of-the-art fusion methods.","1558-0644","","10.1109/TGRS.2020.3039046","National Natural Science Foundation of China(grant numbers:61603249,61673262); Key Project of Science and Technology Commission of Shanghai Municipality(grant numbers:16JC1401100); Wuhan Second Ship Design and Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284643","Alternating direction method of multipliers;image fusion;regularization framework","Computer vision;Minimization;Convex functions;Spatiotemporal phenomena;Task analysis;Image fusion;Monitoring","geophysical image processing;image fusion;image restoration;minimisation","hyperspectral image fusion;multitemporal image fusion;joint sparsity;image data;remote sensing;computer vision;environment monitoring;generalizability;fusion tasks;spatiospectral variabilities;spatiotemporal variabilities;fusion process;joint minimization functional;composite regularizer;Hessian Schatten-norm regularization;contourlet-based regularization terms;image fusion experiments;panchromatic image fusion;multispectral image fusion;multiimage deblurring","","2","","63","IEEE","7 Dec 2020","","","IEEE","IEEE Journals"
"Image Fusion of Spectrally Nonoverlapping Imagery Using SPCA and MTF-Based Filters","Y. Kim; M. Kim; J. Choi; Y. Kim","Department of Civil and Environmental Engineering, Seoul National University, Seoul, South Korea; Department of Civil and Environmental Engineering, Seoul National University, Seoul, South Korea; School of Civil Engineering, Chungbuk National University, Cheongju, South Korea; Department of Civil and Environmental Engineering, Seoul National University, Seoul, South Korea","IEEE Geoscience and Remote Sensing Letters","4 Dec 2017","2017","14","12","2295","2299","Most spaceborne sensors have an inevitable tradeoff between spatial and spectral resolutions. This is a typical ill-posed inverse problem in the field of image fusion. To solve this problem, this letter proposes an image fusion method using spatial principal component analysis and modulation transfer function-based filters. The key behind the proposed fusion method is to efficiently estimate the missing spatial details by considering the spatial structures of the low-resolution multispectral (MS) imagery. Also, this letter proposes a newly developed injection gain model to resolve the local and global dissimilarity between panchromatic and MS imageries, which could prevent over- and under-injections. Finally, spatial details, optimized to be injected into the MS images, were constructed and paired with the developed injection gain model to produce high-resolution MS images. Two data sets acquired by WorldView-2 are employed for validation. The experimental results demonstrate that the proposed fusion method generates high-quality imagery in terms of both qualitative and quantitative standards.","1558-0571","","10.1109/LGRS.2017.2762427","National Research Foundation of Korea(grant numbers:NRF-2015M1A3A3A02014673); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8091115","Image fusion;injection gain;modulation transfer function (MTF);spatial principal component analysis (SPCA);WorldView-2","Image fusion;Image edge detection;Spatial resolution;Principal component analysis;Radiometry;Modulation","geophysical image processing;image fusion;image resolution;inverse problems;optical transfer function;principal component analysis;remote sensing","spatial structures;low-resolution multispectral imagery;newly developed injection gain model;local dissimilarity;global dissimilarity;panchromatic MS imageries;under-injections;MS images;high-quality imagery;spectrally nonoverlapping imagery;spaceborne sensors;inevitable tradeoff;spatial resolutions;spectral resolutions;inverse problem;image fusion method;spatial principal component analysis;modulation transfer function;missing spatial details","","11","","18","IEEE","31 Oct 2017","","","IEEE","IEEE Journals"
"An Integrated Spatio-Spectral–Temporal Sparse Representation Method for Fusing Remote-Sensing Images With Different Resolutions","C. Zhao; X. Gao; W. J. Emery; Y. Wang; J. Li","School of Electronic Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Electronic Engineering, Xidian University, Xi’an, China; Department of Aerospace Engineering Sciences, University of Colorado at Boulder, Boulder, CO, USA; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","21 May 2018","2018","56","6","3358","3370","Different spectral, spatial, and temporal features have been widely used in the remote-sensing image analysis. The further development of multiple sensor remote-sensing technologies has made it necessary to explore new methods of remote-sensing image fusion using different optical image data sets which provide complementary image properties and a tradeoff among spatial, spectral, and temporal resolutions. However, due to problems in assessing correlations between different types of satellite data with different resolutions, a few efforts have been made to explore spatio-spectral–temporal features. For this purpose, we propose a novel sparse representation model to generate synthesized frequent high-spectral and high-spatial resolution data by blending multiple types: spatio-temporal data fusion, spectral–temporal data fusion, spatio-spectral data fusion, and spatio-spectral–temporal data fusion. The proposed method exploits high-spectral correlation across spectral domains and high self-similarity across spatial domains to learn the spatio-spectral fusion basis. Then, it associates temporal changes using a local constraint sparse representation. The integrated spatio-spectral–temporal sparse representation model based on the learned spectral–spatial and temporal change features strengthens the model’s ability to provide high-resolution data needed to address demanding work in real-world applications. Finally, the proposed method is not restricted to a certain type of data, but it can associate any type of remote-sensing data and be applied to dynamic changes in heterogeneous landscapes. The experimental results illustrate the effectiveness and efficiency of the proposed method.","1558-0644","","10.1109/TGRS.2018.2798663","National Natural Science Foundation of China(grant numbers:61432014,61772402,U1605252,61671339,61571343); National Key Research and Development Program of China(grant numbers:2016QY01W0200); Key Industrial Innovation Chain in Industrial Domain(grant numbers:2016KTZDGY04-02); National High-Level Talents Special Support Program of China(grant numbers:CS31117200001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8290581","Heterogeneous land surface monitoring;remote-sensing image fusion;spatio-spectral features;temporal features","Remote sensing;Spatial resolution;Data integration;Image fusion;MODIS;Sensors","image fusion;image representation;image resolution;remote sensing;spatiotemporal phenomena","complementary image properties;satellite data;spatio-spectral-temporal features;high-spatial resolution data;spatio-spectral-temporal data fusion;high-spectral correlation;spectral domains;local constraint sparse representation;high-resolution data;remote-sensing data;integrated spatio-spectral-temporal sparse representation method;remote-sensing image analysis;multiple sensor remote-sensing technologies;remote-sensing image fusion;optical image data sets","","21","","72","IEEE","12 Feb 2018","","","IEEE","IEEE Journals"
"Spatial–Spectral Fusion of HY-1C COCTS/CZI Data for Coastal Water Remote Sensing Using Deep Belief Network","H. Ji; L. Tian; J. Li; R. Tong; Y. Guo; Q. Zeng","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Henan Engineering Research Center of Land Consolidation and Ecological Restoration, Henan Agricultural University, Zhengzhou, China; School of Urban and Environment Science, Central China Normal University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13 Jan 2021","2021","14","","1693","1704","The remote sensing monitoring of coastal waters with dramatic changes requires images with high spatial and temporal resolutions and adequate spectral bands. However, a single sensor is limited to meet these requirements. Image fusion is, therefore, widely adopted. In this article, a deep belief network (DBN) is developed to fuse images from the Chinese ocean color and temperature scanner (1000 m, eight bands) and coastal zone imager (50 m, four bands) onboard HaiYang-1C satellite to generate 50-m, eight-band, and three-day observations for coastal waters. The DBN is compared with the existing prevailing Gram-Schmidt transformation (GS) and inversion-based fusion (IBF) algorithms over the Bohai Sea at the top-of-atmosphere reflectance and product [e.g., chlorophyll-a (chl-a)] levels. Results indicate that for the spatial aspect, DBN can avoid the block effect and maintain details. The average structural similarity index of DBN is approximately 22.08% and 3.30% better than that of GS and IBF, respectively; for the spectral aspect, the mean relative errors for eight bands of DBN range from 3.15% to 21.54%. The errors are less than 50% and 80% of those of GS, while less than 80% and 110% of those of IBF, at bands 1-6 and bands 7 and 8, respectively; for chl-a retrieval, DBN yields better results with the coefficient of determination R2 of 0.78 and root-mean-square error (RMSE) of 0.10 mg/m3 compared with those of IBF (R2 = 0.59 and RMSE = 0.16 mg/m3). DBN outperforms GS and IBF at reflectance and product levels, displaying great potential for the remote sensing monitoring of coastal waters.","2151-1535","","10.1109/JSTARS.2020.3045516","National Key R&D Program of China(grant numbers:2018YFB0504900,2018YFB0504904,2016YFC0200900); National Natural Science Foundation of China(grant numbers:42071325,41571344,41701379,41701422); LIESMARS Special Research Funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9296740","Coastal water;deep belief network (DBN);fusion;HaiYang-1C (HY-1C);remote sensing","Sea measurements;Remote sensing;Spatial resolution;Oceans;Image fusion;Image resolution;Satellites","geophysical image processing;image fusion;oceanographic techniques;remote sensing","spatial-spectral fusion;coastal water remote sensing;deep belief network;remote sensing monitoring;coastal waters;adequate spectral bands;image fusion;temperature scanner;coastal zone imager;HaiYang-1C satellite;DBN range","","4","","67","CCBYNCND","17 Dec 2020","","","IEEE","IEEE Journals"
"Improving Satellite Image Fusion via Generative Adversarial Training","X. Luo; X. Tong; Z. Hu","College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, China; College of Surveying and Geo-informatics, Tongji University, Shanghai, China; MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area & Guangdong Key Laboratory of Urban Informatics & Shenzhen Key Laboratory of Spatial Smart Sensing and Services, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6969","6982","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications.","1558-0644","","10.1109/TGRS.2020.3025821","National Natural Science Foundation of China(grant numbers:41631178); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212572","Deep learning;generative adversarial networks (GANs);Landsat 8;remote sensing image fusion;residual dense blocks;Sentinel-2","Image fusion;Satellites;Training;Spatial resolution;Remote sensing","geophysical image processing;image fusion;image resolution;optical images;remote sensing","satellite image fusion;generative adversarial training;optical images;satellite platforms;multiresolution satellite images;higher-resolution images;image quality;generative adversarial network;fusion GAN approach;remote sensing image fusion problem;adversarial training strategy;generative network;G-Net;discriminative network;original multiresolution images;residual dense blocks;FusGAN approach;Landsat Operational Land Imager multiresolution images;common resolution disparities;intersatellite images;quality improvement","","7","","74","IEEE","5 Oct 2020","","","IEEE","IEEE Journals"
"Compensation Details-Based Injection Model for Remote Sensing Image Fusion","Y. Yang; L. Wu; S. Huang; J. Sun; W. Wan; J. Wu","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; Division of Computer Science and Engineering, Chonbuk National University, Jeonju, South Korea; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Geoscience and Remote Sensing Letters","20 Apr 2018","2018","15","5","734","738","Remote sensing image fusion has a potential spectral distortion problem due to the global/local spectral and spatial correlations between panchromatic (PAN) and multispectral (MS) images. To overcome this problem, in this letter, a compensation details-based injection (CDI) fusion model is presented from a new perspective of compensatory learning. In contrast to the traditional method, the two categories of details, namely, the PAN details and the CD, are considered to compensate for the spatial and spectral differences between low-resolution MS (LRMS) and high-resolution MS images. To obtain the CD, a robust sparse representation was employed to calculate the difference between the PAN and MS images during the fusion. The CD combined with the PAN details extracted by a multiscale-guided filter are then injected into the upsampled LRMS image to achieve a fused image. Extensive experiments were undertaken on several image data sets, and the results demonstrate the effectiveness of the proposed CDI method.","1558-0571","","10.1109/LGRS.2018.2810219","National Natural Science Foundation of China(grant numbers:61662026,61462031); Natural Science Foundation of Jiangxi Province(grant numbers:20161ACB21015); Project of the Education Department of Jiangxi Province(grant numbers:KJLD14031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8318926","Compensation details (CD);detail injection scheme;remote sensing image fusion;robust sparse representation (RSR)","Frequency modulation;Image reconstruction;Remote sensing;Image fusion;Spatial resolution;Distortion;Robustness","geophysical image processing;image fusion;remote sensing","spatial correlations;MS;compensation details;injection fusion model;PAN details;CD;spatial differences;spectral differences;high-resolution;upsampled LRMS image;image data sets;remote sensing image fusion;potential spectral distortion problem","","16","","23","IEEE","16 Mar 2018","","","IEEE","IEEE Journals"
"A New Geostatistical Solution to Remote Sensing Image Downscaling","Q. Wang; W. Shi; P. M. Atkinson; E. Pardo-Igúzquiza","Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Kowloon, Hong Kong; The Hong Kong Polytechnic University, Wuhan University, Wuhan, China; Faculty of Science and Technology, University of Southampton, Southampton, U.K.; Instituto Geológico y Minero de España, Madrid, Spain","IEEE Transactions on Geoscience and Remote Sensing","29 Oct 2015","2016","54","1","386","396","The availability of the panchromatic (PAN) band in remote sensing images gives birth to so-called image fusion techniques for increasing the spatial resolution of images to that of the PAN band. The spatial resolution of such spatially sharpened images, such as for the MODIS and Landsat sensors, however, may not be sufficient to provide the required detailed land-cover/land-use information. This paper proposes an area-to-point regression kriging (ATPRK)-based geostatistical solution to increase the spatial resolution of remote sensing images beyond that of any input images, including the PAN band. The new approach is a two-stage approach, including covariate downscaling and ATPRK-based image fusion. The new approach treats the PAN band as the covariate and takes advantages of its textural information. It explicitly accounts for the size of support, spatial correlation, and the point spread function of the sensor and has the characteristic of perfect coherence with the original coarse data. Moreover, the new downscaling approach can be extended readily by incorporating other ancillary information. The proposed approach was examined using both Landsat and MODIS images. The results show that it can produce more accurate sharpened images than four benchmark approaches.","1558-0644","","10.1109/TGRS.2015.2457672","Research Grants Council of Hong Kong(grant numbers:PolyU 15223015,PolyU 5249/12E); National Natural Science Foundation of China(grant numbers:41331175); Leading Talent Project of the National Administration of Surveying(grant numbers:K.SZ.XX.VTQA); Ministry of Science and Technology of China(grant numbers:2012BAJ15B04,2012AA12A305); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181666","Area-to-point regression kriging (ATPRK);downscaling;geostatistics;image fusion;Landsat Enhanced Thematic Mapper;Moderate Resolution Imaging Spectroradiometer (MODIS);Area-to-point regression kriging (ATPRK);downscaling;geostatistics;image fusion;Landsat Enhanced Thematic Mapper;Moderate Resolution Imaging Spectroradiometer (MODIS)","Spatial resolution;Remote sensing;Earth;MODIS;Satellites;Deconvolution;Image fusion","geophysical image processing;geophysical techniques;image fusion;land cover;land use;remote sensing","geostatistical solution;remote sensing image downscaling;panchromatic band;image fusion techniques;MODIS sensor;Landsat sensor;land-cover information;land-use information;area-to-point regression kriging;ATPRK-based geostatistical solution;input images;PAN band;ATPRK-based image fusion;original coarse data;textural information;ancillary information;Landsat image;MODIS image","","34","","37","IEEE","6 Aug 2015","","","IEEE","IEEE Journals"
"Multimodal Probabilistic Latent Semantic Analysis for Sentinel-1 and Sentinel-2 Image Fusion","R. Fernandez-Beltran; J. M. Haut; M. E. Paoletti; J. Plaza; A. Plaza; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IEEE Geoscience and Remote Sensing Letters","26 Aug 2018","2018","15","9","1347","1351","Probabilistic topic models have recently shown a great potential in the remote sensing image fusion field, which is particularly helpful in land-cover categorization tasks. This letter first studies the application of probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation to remote sensing synthetic aperture radar (SAR) and multispectral imaging (MSI) unsupervised land-cover categorization. Then, a novel pLSA-based image fusion approach is presented, which pursues to uncover multimodal feature patterns from SAR and MSI data in order to effectively fuse and categorize Sentinel-1 and Sentinel-2 remotely sensed data. Experiments conducted over two different data sets reveal the advantages of the proposed approach for unsupervised land-cover categorization tasks.","1558-0571","","10.1109/LGRS.2018.2843886","Generalitat Valenciana(grant numbers:APOSTD/2017/007); Ministerio de Educación, Cultura y Deporte(grant numbers:FPU14/02012-FPU15/02090,ESP2016-79503-C2-2-P); Consejería de Educación y Empleo, Junta de Extremadura(grant numbers:GR15005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392415","Image fusion;land-cover categorization;probabilistic latent semantic analysis (pLSA);Sentinel-1;Sentinel-2","Synthetic aperture radar;Remote sensing;Data models;Image fusion;Semantics;Feature extraction;Probabilistic logic","feature extraction;geophysical image processing;geophysical techniques;image fusion;land cover;remote sensing;synthetic aperture radar","MSI data;SAR data;multispectral imaging unsupervised land-cover categorization;remote sensing synthetic aperture radar;latent Dirichlet allocation;remote sensing image fusion field;probabilistic topic models;Sentinel-2 image fusion;multimodal probabilistic latent semantic analysis;unsupervised land-cover categorization tasks;Sentinel-2 remotely sensed data;Sentinel-1;multimodal feature patterns;novel pLSA-based image fusion approach","","28","","18","IEEE","21 Jun 2018","","","IEEE","IEEE Journals"
"Remote Sensing Image Fusion Using Hierarchical Multimodal Probabilistic Latent Semantic Analysis","R. Fernandez-Beltran; J. M. Haut; M. E. Paoletti; J. Plaza; A. Plaza; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4982","4993","The generative semantic nature of probabilistic topic models has recently shown encouraging results within the remote sensing image fusion field when conducting land cover categorization. However, standard topic models have not yet been adapted to the inherent complexity of remotely sensed data, which eventually may limit their resulting performance. In this scenario, this paper presents a new topic-based image fusion framework, specially designed to fuse synthetic aperture radar (SAR) and multispectral imaging (MSI) data for unsupervised land cover categorization tasks. Specifically, we initially propose a hierarchical multi-modal probabilistic latent semantic analysis (HMpLSA) model that takes advantage of two different vocabulary modalities, as well as two different levels of topics, in order to effectively uncover intersensor semantic patterns. Then, we define an SAR and MSI data fusion framework based on HMpLSA in order to perform unsupervised land cover categorization. Our experiments, conducted using three different SAR and MSI data sets, reveal that the proposed approach is able to provide competitive advantages with respect to standard clustering methods and topic models, as well as several multimodal topic model variants available in the literature.","2151-1535","","10.1109/JSTARS.2018.2881342","Generalitat Valenciana(grant numbers:APOSTD/2017/007); panish Ministry(grant numbers:FPU14/02012-FPU15/02090,ESP2016-79503-C2-2-P,TIN2015-63646-C5-5-R); Junta de Extremadura(grant numbers:GR15005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550740","Image fusion;multispectral imaging (MSI);probabilistic latent semantic analysis (pLSA);synthetic aperture radar (SAR)","Synthetic aperture radar;Data models;Remote sensing;Semantics;Image fusion;Probabilistic logic;Adaptation models","feature extraction;geophysical image processing;image classification;image fusion;pattern clustering;probability;remote sensing;sensor fusion;synthetic aperture radar;terrain mapping","hierarchical multimodal probabilistic latent semantic analysis;generative semantic nature;probabilistic topic models;remote sensing image fusion field;standard topic models;remotely sensed data;resulting performance;topic-based image fusion framework;unsupervised land cover categorization tasks;multimodal probabilistic latent semantic analysis model;HMpLSA;intersensor semantic patterns;multimodal topic model;vocabulary modalities;land cover categorization","","45","","47","IEEE","28 Nov 2018","","","IEEE","IEEE Journals"
"An Integrated Framework for the Spatio–Temporal–Spectral Fusion of Remote Sensing Images","H. Shen; X. Meng; L. Zhang","School of Resources and Environmental Sciences, Wuhan University, Wuhan, China; School of Resources and Environmental Sciences, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","30 Sep 2016","2016","54","12","7135","7148","Remote sensing satellite sensors feature a tradeoff between the spatial, temporal, and spectral resolutions. In this paper, we propose an integrated framework for the spatio-temporal-spectral fusion of remote sensing images. There are two main advantages of the proposed integrated fusion framework: it can accomplish different kinds of fusion tasks, such as multiview spatial fusion, spatio-spectral fusion, and spatio-temporal fusion, based on a single unified model, and it can achieve the integrated fusion of multisource observations to obtain high spatio-temporal-spectral resolution images, without limitations on the number of remote sensing sensors. The proposed integrated fusion framework was comprehensively tested and verified in a variety of image fusion experiments. In the experiments, a number of different remote sensing satellites were utilized, including IKONOS, the Enhanced Thematic Mapper Plus (ETM+), the Moderate Resolution Imaging Spectroradiometer (MODIS), the Hyperspectral Digital Imagery Collection Experiment (HYDICE), and Système Pour l' Observation de la Terre-5 (SPOT-5). The experimental results confirm the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2016.2596290","National Natural Science Foundation of China(grant numbers:41271376,41422108); Cross-disciplinary Collaborative Teams Program for Science, Technology and Innovation of the Chinese Academy of Sciences; Wuhan Science and Technology Program(grant numbers:2013072304010825); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560877","Image fusion;integrated framework;remote sensing;spatial resolution;spectral resolution;temporal resolution","Remote sensing;Spatial resolution;Sensors;Image fusion;Satellites;Image sensors","geophysical image processing;image fusion;remote sensing","remote sensing images;image spatio-temporal-spectral fusion;remote sensing satellite sensors;spatial resolution;temporal resolution;spectral resolution;integrated fusion framework;spatio-spectral fusion;spatio-temporal fusion","","190","","87","IEEE","6 Sep 2016","","","IEEE","IEEE Journals"
"Remote Sensing Image Fusion Based on Adaptive IHS and Multiscale Guided Filter","Y. Yang; W. Wan; S. Huang; F. Yuan; S. Yang; Y. Que","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Access","20 May 2017","2016","4","","4573","4582","The purpose of remote sensing image fusion is to sharpen a low spatial resolution multispectral (MS) image by injecting the detail map extracted from a panchromatic (PAN) image. In this paper, a novel remote sensing image fusion method based on adaptive intensity-hue-saturation (IHS) and multiscale guided filter is presented. In the proposed method, the intensity component is obtained adaptively from the upsampled MS image at first. Different from traditional IHS-based methods, we subsequently propose a multiscale guided filter strategy to filter the PAN image to achieve more detail information. Finally, the total detail map is injected into each band of the upsampled MS image to obtain the fused image by a model-based algorithm, in which an improved injection gains approach is proposed to control the quantity of the injected detail information. Experimental results demonstrated that the proposed method can provide more spatial information and preserve more spectral information compared with several state-of-the-art fusion methods in both subjective and objective evaluations.","2169-3536","","10.1109/ACCESS.2016.2599403","National Natural Science Foundation of China(grant numbers:61262034,61462031,61662026); Natural Science Foundation of Jiangxi Province(grant numbers:20151BAB207033,20161ACB21015); Project of the Education Department of Jiangxi Province(grant numbers:KJLD14031,GJJ150461); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7539694","Image fusion;multispectral (MS) image;panchromatic (PAN) image;intensity-hue-saturation (IHS) transform;guided filter","Image fusion;Multispectral imaging;Information filters;Remote sensing;Filtering algorithms;Spatial resolution","adaptive filters;geophysical image processing;image fusion;remote sensing","multiscale guided filter strategy;intensity component;multiscale guided filter;IHS;adaptive intensity hue saturation;novel remote sensing image fusion method;PAN image;panchromatic image;detail map extraction;MS image;spatial resolution multispectral;remote sensing image fusion","","64","","28","OAPA","10 Aug 2016","","","IEEE","IEEE Journals"
"Multilevel Feature Fusion and Attention Network for High-Resolution Remote Sensing Image Semantic Labeling","Y. Zhang; J. Cheng; H. Bai; Q. Wang; X. Liang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","4 Jul 2022","2022","19","","1","5","Semantic labeling of high-resolution remote sensing images (HRRSIs) has always been an important research field in remote sensing image analysis. However, remote sensing images contain substantial low- and high-level features, which makes them quite difficult to be recognized. In this letter, we proposed a multilevel feature fusion and attention network (MFANet) to adaptively capture and fuse multilevel features in a more effective and efficient manner. Specifically, the backbone of our network is divided into two branches—the detail branch and the semantic branch (SB), where the detail branch extracts low-level features and the SB extracts high-level features. The Deep Atrous Spatial Pyramid (DASPP) module is embedded at the end of the SB to capture multiscale features as a supplement to high-level features. It is worth noting that the feature alignment and fusion (FAF) module is used to align and fuse features from different stages to enhance feature representation. Furthermore, the context attention (CA) module is employed to process feature maps from the two branches to establish contextual dependencies in the spatial dimension and channel dimension, which can help the network focus on more meaningful features. The experiments are carried out on the International Society for Photogrammetry and Remote Sensing (ISPRS) Vaihingen and Potsdam datasets, and the results show that our proposed method has achieved better performance than other state-of-art methods.","1558-0571","","10.1109/LGRS.2022.3184553","National Natural Science Foundation of China (NNSFC)(grant numbers:62071104); NNSFC and Civil Aviation Administration of China (CAAC)(grant numbers:U2133211); Sichuan Science and Technology Program(grant numbers:2020YFG0085); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9800904","Attention mechanism;feature fusion;high-resolution remote sensing images (HRRSIs);multilevel feature;semantic labeling","Semantics;Labeling;Feature extraction;Task analysis;Remote sensing;Fuses;Convolution","feature extraction;geophysical image processing;image fusion;image representation;image resolution;remote sensing","image semantic labeling;high-resolution remote sensing images;remote sensing image analysis;high-level features;detail branch;semantic branch;low-level features;Deep Atrous Spatial Pyramid module;multiscale features;feature representation;feature maps;HRRSIs;multilevel feature fusion and attention network;MFANet;feature alignment and fusion module;FAF module;context attention module;CA module;International Society for Photogrammetry and Remote Sensing;ISPRS Vaihingen and Potsdam datasets","","1","","14","IEEE","20 Jun 2022","","","IEEE","IEEE Journals"
"Fusion of MS and PAN Images Preserving Spectral Quality","H. R. Shahdoosti; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","IEEE Geoscience and Remote Sensing Letters","8 Oct 2014","2015","12","3","611","615","Image fusion aims at improving spectral information in a fused image as well as adding spatial details to it. Among the existing fusion algorithms, filter-based fusion methods are the most frequently discussed cases in recent publications due to their ability to improve spatial and spectral information of multispectral (MS) and panchromatic (PAN) images. Filter-based approaches extract spatial information from the PAN image and inject it into MS images. Designing an optimal filter that is able to extract relevant and nonredundant information from the PAN image is presented in this letter. The optimal filter coefficients extracted from statistical properties of the images are more consistent with type and texture of the remotely sensed images compared with other kernels such as wavelets. Visual and statistical assessments show that the proposed algorithm clearly improves the fusion quality in terms of correlation coefficient, relative dimensionless global error in synthesis, spectral angle mapper, universal image quality index, and quality without reference, as compared with fusion methods, including improved intensity-hue-saturation, multiscale Kalman filter, Bayesian, improved nonsubsampled contourlet transform, and sparse fusion of image.","1558-0571","","10.1109/LGRS.2014.2353135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895274","Directional filter;image fusion;optimal filter;pan-sharpening;spectral information;Directional filter;image fusion;optimal filter;pan-sharpening;spectral information","Remote sensing;Visualization;Indexes;Image fusion;Image resolution;Image color analysis;Satellites","Bayes methods;correlation methods;geophysical image processing;image fusion;Kalman filters;remote sensing","image fusion;MS images;PAN images;spectral quality;filter based fusion;multispectral images;panchromatic images;filter based approach;correlation coefficient;relative dimensionless global error;intensity hue saturation;multiscale Kalman filter;Bayesian method;improved nonsubsampled contourlet transform;sparse fusion","","65","","19","IEEE","11 Sep 2014","","","IEEE","IEEE Journals"
"A Novel Multitemporal Image-Fusion Algorithm: Method and Application to GOCI and Himawari Images for Inland Water Remote Sensing","Y. Guo; C. Huang; Y. Zhang; Y. Li; W. Chen","Henan Engineering Research Center of Land Consolidation and Ecological Restoration, Henan Agricultural University, Zhengzhou, China; School of Geography, Nanjing Normal University, Nanjing, China; Henan Engineering Research Center of Land Consolidation and Ecological Restoration, Henan Agricultural University, Zhengzhou, China; School of Tourism and Urban & Rural Planning, Zhejiang Gongshang University, Hangzhou, China; Henan Engineering Research Center of Land Consolidation and Ecological Restoration, Henan Agricultural University, Zhengzhou, China","IEEE Transactions on Geoscience and Remote Sensing","21 May 2020","2020","58","6","4018","4032","A spectral-temporal-unmixing-based multitemporal image fusion (MTIF) algorithm is proposed to fuse Geostationary Ocean Color Imager (GOCI) and Himawari images. The algorithm was applied to two data sets. The fusions are quantitatively and qualitatively compared with four widely used algorithms. The results show that the MTIF algorithm performs better using both evaluation indexes and visual comparisons. For optical complex water monitoring, the MTIF - derived chlorophyll-a concentration (${C} _{{\text {chla}}}$ ) map has better spatial detail and temporal trends compared with the other algorithms. For cloudy images, the MTIF algorithm can estimate part of the under cloud water reflectance information: when there are more than 32.6 cloud-free pixels in the current study area, the MTIF algorithm can successfully recover the under cloud information. The MTIF algorithm has great potential to advance the monitoring of optical complex inland water.","1558-0644","","10.1109/TGRS.2019.2960322","National Natural Science Foundation of China(grant numbers:41701422,41571324); Key Scientific and Technological Research Projects in Henan Province(grant numbers:182102410026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955935","Geostationary Ocean Color Imager (GOCI);Himawari;image fusion;optical complex water monitoring","Spatial resolution;Remote sensing;Image fusion;Water pollution;Image color analysis;Indexes","clouds;geophysical image processing;image fusion;remote sensing","chlorophyll-a concentration;GOCI;Geostationary Ocean Color Imager;spectral-temporal-unmixing-based multitemporal image fusion algorithm;inland water remote sensing;himawari images;optical complex inland water;cloud water reflectance information;cloudy images;optical complex water monitoring;MTIF algorithm performs","","6","","70","IEEE","10 Jan 2020","","","IEEE","IEEE Journals"
"Pansharpening Using Regression of Classified MS and Pan Images to Reduce Color Distortion","Q. Xu; Y. Zhang; B. Li; L. Ding","State Key Laboratory of Virtual Reality Technologies and Systems and the Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Geodesy and Geomatics Engineering, University of New Brunswick, Fredericton, NB, Canada; State Key Laboratory of Virtual Reality Technologies and Systems and the Beijing Key Laboratory of Digital Media, School of Computer Science and Engineering, Beihang University, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2015","12","1","28","32","The synthesis of low-resolution panchromatic (Pan) image is a critical step of ratio enhancement (RE) and component substitution (CS) pansharpening methods. The two types of methods assume a linear relation between Pan and multispectral (MS) images. However, due to the nonlinear spectral response of satellite sensors, the qualified low-resolution Pan image cannot be well approximated by a weighted summation of MS bands. Therefore, in some local areas, significant gray value difference exists between a synthetic Pan image and a high-resolution Pan image. To tackle this problem, the pixels of Pan and MS images are divided into several classes by  $k$-means algorithm, and then multiple regression is used to calculate summation weights on each group of pixels. Experimental results demonstrate that the proposed technique can provide significant improvements on reducing color distortion.","1558-0571","","10.1109/LGRS.2014.2324817","National Science Fund for Distinguished Young Scholars of China(grant numbers:61125206); Key Project of the Natural Science Foundation of China(grant numbers:61331017); Discovery Grants Program of the Natural Sciences and Engineering Research Council of Canada; Canada Research Chairs Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824749","Classification;image fusion;pansharpening;remote sensing;Classification;image fusion;pansharpening;remote sensing","Remote sensing;Image color analysis;Image fusion;Image resolution;Satellites;Indexes;Vegetation mapping","geophysical image processing;image resolution;image segmentation;land cover;regression analysis;remote sensing;terrain mapping","low-resolution panchromatic image;ratio enhancement pansharpening method;component substitution pansharpening method;multispectral images;nonlinear spectral response;satellite sensors;weighted summation;multispectral bands;local areas;gray value difference;synthetic panchromatic image;high-resolution panchromatic image;k-means algorithm;multiple regression;summation weights;color distortion;land cover","","26","","19","EU","3 Jun 2014","","","IEEE","IEEE Journals"
"Remote Sensing Image Spatiotemporal Fusion Using a Generative Adversarial Network","H. Zhang; Y. Song; C. Han; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Mechanical and Electrical Engineering, Wuhan Business University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 Apr 2021","2021","59","5","4273","4286","Due to technological limitations and budget constraints, spatiotemporal fusion is considered a promising way to deal with the tradeoff between the temporal and spatial resolutions of remote sensing images. Furthermore, the generative adversarial network (GAN) has shown its capability in a variety of applications. This article presents a remote sensing image spatiotemporal fusion method using a GAN (STFGAN), which adopts a two-stage framework with an end-to-end image fusion GAN (IFGAN) for each stage. The IFGAN contains a generator and a discriminator in competition with each other under the guidance of the optimization function. Considering the huge spatial resolution gap between the high-spatial, low-temporal (HSLT) resolution Landsat imagery and the corresponding low-spatial, high-temporal (LSHT) resolution MODIS imagery, a feature-level fusion strategy is adopted. Specifically, for the generator, we first super-resolve the MODIS images while also extracting the high-frequency features of the Landsat images. Finally, we integrate the features from the MODIS and Landsat images. STFGAN is able to learn an end-to-end mapping between the Landsat-MODIS image pairs and predicts the Landsat-like image for a prediction date by considering all the bands. STFGAN significantly improves the accuracy of phenological change and land-cover-type change prediction with the help of residual blocks and two prior Landsat-MODIS image pairs. To examine the performance of the proposed STFGAN method, experiments were conducted on three representative Landsat-MODIS data sets. The results clearly illustrate the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2020.3010530","National Natural Science Foundation of China(grant numbers:61871298); Dragon 5 proposal ID(grant numbers:58817); Foundation of Hubei Educational Committee(grant numbers:B2018280); Applied Basic Research Programs of Science and Technology Commission Foundation of Wuhan(grant numbers:2019010701011390); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9159647","Generative adversarial network (GAN);multisource satellite data;remote sensing;spatiotemporal fusion","Spatial resolution;Remote sensing;Earth;Spatiotemporal phenomena;Artificial satellites;Generators","geophysical image processing;image fusion;image resolution;land surface temperature;remote sensing","land-cover-type change prediction;prior Landsat-MODIS image pairs;STFGAN method;representative Landsat-MODIS data sets;generative adversarial network;technological limitations;budget constraints;temporal resolutions;spatial resolutions;remote sensing images;remote sensing image spatiotemporal fusion method;two-stage framework;end-to-end image fusion GAN;IFGAN;huge spatial resolution gap;high-spatial, low-temporal resolution Landsat imagery;high-temporal resolution;feature-level fusion strategy;MODIS images;high-frequency features;Landsat images;end-to-end mapping;Landsat-like image;phenological change","","47","","49","IEEE","5 Aug 2020","","","IEEE","IEEE Journals"
"Remote Sensing Image Fusion Based on Adaptively Weighted Joint Detail Injection","Y. Yang; L. Wu; S. Huang; W. Wan; Y. Que","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; Division of Computer Science and Engineering, Chonbuk National University, Jeonju, South Korea; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Access","5 Mar 2018","2018","6","","6849","6864","Remote sensing image fusion based on the detail injection scheme consists of two steps: spatial details extraction and injection. The quality of the extracted spatial details plays an important role in the success of a detail injection scheme. In this paper, a remote sensing image fusion method based on adaptively weighted joint detail injection is presented. In the proposed method, the spatial details are first extracted from the multispectral (MS) and panchromatic (PAN) images through à trous wavelet transform and multiscale guided filter. Different from the traditional detail injection scheme, the extracted details are then sparsely represented to produce the primary joint details by dictionary learning from the subimages themselves. To obtain the refined joint details information, we subsequently design an adaptive weight factor considering the correlation and difference between the previous joint details and PAN image details. Finally, the refined joint details are injected into the MS image using modulation coefficient to achieve the fused image. The proposed method has been tested on QuickBird, IKONOS, and WorldView-2 datasets and compared to several state-of-the-art fusion methods in both subjective and objective evaluations. The experimental results indicate that the proposed method is effective and robust to images from various satellites sensors.","2169-3536","","10.1109/ACCESS.2018.2791574","National Natural Science Foundation of China(grant numbers:61662026,61462031); Natural Science Foundation of Jiangxi Province(grant numbers:20161ACB21015); Project of the Education Department of Jiangxi Province(grant numbers:KJLD14031); Knowledge Innovation Fund for the Graduate Students of Jiangxi Province(grant numbers:YC2017-B068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253445","Remote sensing image fusion;detail injection scheme;multiscale decomposition;sparse representation","Remote sensing;Image fusion;Transforms;Sensors;Spatial resolution;Multiresolution analysis","feature extraction;geophysical image processing;image fusion;image resolution;remote sensing;wavelet transforms","remote sensing image fusion method;adaptively weighted joint detail injection;traditional detail injection scheme;extracted details;primary joint details;refined joint details information;adaptive weight factor;previous joint details;PAN image details;fused image;state-of-the-art fusion methods;spatial details extraction;extracted spatial details;MS image;modulation coefficient;multispectral images;panchromatic images;trous wavelet transform;multiscale guided filter;QuickBird datasets;IKONOS datasets;WorldView-2 datasets","","26","","57","OAPA","10 Jan 2018","","","IEEE","IEEE Journals"
"Multi-Source Remote Sensing Intelligent Characterization Technique-Based Disaster Regions Detection in High-Altitude Mountain Forest Areas","H. Wang; H. Cao; Y. Kai; H. Bai; X. Chen; Y. Yang; L. Xing; C. Zhou","Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University, Kunming, China; Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University, Kunming, China; Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University, Kunming, China; Network and Information Center, Yunnan Normal University, Kunming, China; Yunnan Province Center of Relief Material Reserve, Anning, China; Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University, Kunming, China; School of Information Science and Technology, Yunnan Normal University, Kunming, China; Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University, Kunming, China","IEEE Geoscience and Remote Sensing Letters","8 Jul 2022","2022","19","","1","5","Natural disasters frequently have caused a huge impact on life and property losses, in Southwest China. To provide assistance for disaster relief, areas damaged in natural disasters are quickly located by utilizing satellite remote-sensing images-based deep-learning object detection technology. However, the current detection technology, for the detection of damaged objects discretely in the disaster area, has some challenges, such as partial missing of multisource images and extremely sparse targets with weak features or occlusion at large scales. Furthermore, we propose an object detection network based on the dynamic extraction of multisource image features to solve the above problems. To train our proposed network, we collect multisource remote-sensing images before and after the disaster. Finally, it is verified that when the detection error rate is less than 5%, the accuracy of the detection model reaches more than 85%.","1558-0571","","10.1109/LGRS.2022.3185420","National Natural Science Foundation of China(grant numbers:41971392); Yunnan Ten-Thousand Talents Program; Postgraduate Research Innovation Fund Project of Yunnan Normal University(grant numbers:ysdyjs2020148); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812470","Disaster rescue;multisource image fusion;object detection;satellite remote sensing","Feature extraction;Remote sensing;Satellites;Forestry;Training;Task analysis;Object detection","deep learning (artificial intelligence);disasters;geophysical image processing;object detection;remote sensing","current detection technology;damaged objects;disaster area;multisource images;object detection network;multisource image features;detection error rate;detection model;multisource remote sensing intelligent characterization technique-based disaster regions detection;high-altitude mountain forest areas;natural disasters;property losses;disaster relief;object detection technology;satellite remote-sensing images-based deep-learning object detection technology","","1","","13","IEEE","30 Jun 2022","","","IEEE","IEEE Journals"
"Multiscale Feature Adaptive Fusion for Object Detection in Optical Remote Sensing Images","H. Lv; W. Qian; T. Chen; H. Yang; X. Zhou","School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China; Jiangsu Open Laboratory of Major Scientific Instrument and Equipment, Nanjing Normal University, Nanjing, China; Datang Jiangsu Power Generation Company Ltd., Nanjing, China; School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China; School of Electrical and Automation Engineering, Nanjing Normal University, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","10 Jun 2022","2022","19","","1","5","So far, the accuracy of object detection in natural images has been continuously improved, and at the same time, object detection has received more and more attention in the field of remote sensing. However, different from natural images, remote sensing images have a large number of multiscale objects and large, complex backgrounds. As the size of remote sensing images is generally large, small objects are easy to lose information after downsampling. To deal with these problems, we propose a multiscale feature adaptive fusion (MFAF) method. Specifically, we first use the multiscale feature integration (FI) module and the spatial attention weight (SAW) module to construct the feature fusion module to achieve the adaptive fusion of multiscale features. Then, we add a detail enhancement (DE) module at the back of the backbone to enhance the quality of features in each scale. Next, we obtain the relationship between the features of different channels by embedding a squeeze and excitation (SE) block to highlight the useful features. Finally, a cross stage partial (CSP) block is adopted to replace continuous convolutions to reduce the number of parameters and the loss of features. We implement our work in YOLOv4. In experiments, we evaluate our method on large-scale remote sensing image datasets HRRSD and DIOR, and, respectively, achieve improvements of 2.7% and 1.9% in AP50 compared with YOLOv4, successfully improving the multiscale object detection performance.","1558-0571","","10.1109/LGRS.2022.3178787","Foundation of Jiangsu Open Laboratory of Major Scientific Instrument and Equipment, Nanjing Normal University(grant numbers:20210015); National Natural Science Foundation of China(grant numbers:61304227); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9784817","Cross stage partial (CSP);multiscale feature adaptive fusion (MFAF);object detection;remote sensing images;spatial attention weight (SAW)","Remote sensing;Feature extraction;Object detection;Surface acoustic waves;Sports;Semantics;Marine vehicles","feature extraction;image enhancement;image fusion;object detection;object tracking;remote sensing","optical remote sensing images;multiscale feature integration module;spatial attention weight module;feature fusion module;object detection;multiscale feature adaptive fusion;MFAF;SAW;detail enhancement;DE;squeeze and excitation block;SE block;cross stage partial block;CSP block;YOLOv4","","","","21","IEEE","30 May 2022","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion Based on Band Simulation","X. Li; Y. Yuan; Q. Wang","Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","26 Feb 2020","2020","17","3","479","483","Hyperspectral images (HSIs) usually have a high spectral resolution but low spatial resolution due to hardware limitations, while multispectral images (MSIs) usually have a low spectral resolution but high spatial resolution. To obtain an image with a high resolution both in spectral and spatial domains, a general strategy is image fusion. A variety of methods have been proposed on this, but these methods generally cannot achieve good performance due to the incomplete overlapping wavelength of the HSI and the MSI. To solve this problem, this letter proposes a novel HSI fusion method based on band simulation. The proposed method expands MSI using spectral unmixing and acquires high-resolution images based on linear least squares. The experimental results on two hyperspectral data sets show that the proposed method outperforms the competitors, especially when the overlapping wavelength of the HSI and the MSI is small.","1558-0571","","10.1109/LGRS.2019.2926308","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1107403); National Natural Science Foundation of China(grant numbers:U1864204,61773316,61871470); National Natural Science Foundation of China(grant numbers:61632018); Natural Science Foundation of Shaanxi Province(grant numbers:2018KJXX-024); Project of Special Zone for National Defense Science and Technology Innovation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8766882","Hyperspectral image (HSI);image fusion;multispectral image (MSI);spectral unmixing","Spatial resolution;Hyperspectral imaging;Image fusion;Bayes methods","hyperspectral imaging;image fusion;image resolution","low spatial resolution;hardware limitations;multispectral images;low spectral resolution;high spatial resolution;spectral domains;spatial domains;incomplete overlapping wavelength;MSI;band simulation;spectral unmixing;high-resolution images;hyperspectral data sets;multispectral image fusion;hyperspectral images;high spectral resolution;HSI fusion method","","11","","24","IEEE","19 Jul 2019","","","IEEE","IEEE Journals"
"Encoder- and Decoder-Based Networks Using Multiscale Feature Fusion and Nonlocal Block for Remote Sensing Image Semantic Segmentation","Y. Wang; Z. Sun; W. Zhao","Institute of Electronic Information Engineering, Beihang University, Beijing, China; School of Science, Xi’an Jiaotong University, Xi’an, China; Institute of Electronic Information Engineering, Beihang University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","23 Jun 2021","2021","18","7","1159","1163","With the development of convolutional neural networks, the semantic segmentation of remote sensing images has been widely developed, but there are still some unsolved problems in this field due to the lack of multiscale information and the feature mismatch at the upsampling process. To solve these problems, we propose a network called multiscale feature fusion and alignment network (MFANet). MFANet is composed of an encoder and a decoder. The encoder contains a fully convolutional network, a multilevel feature fusion block (MLFFB), and a multiscale feature pyramid (MSFP). These subnetworks can obtain fine-grained feature maps that are full of multiscale and global features and improve segmentation results at multiple object scales. Moreover, MFANet uses a light convolution subnetwork, called decoder, to upsample the segmentation map stage by stage. Combining three scales of features, the decoder can promote the feature alignment at the upsampling stage. Along with the decoder, MFANet utilizes a multistage supervision loss to enhance the localization performance and boundary regression ability. Benefitting from the encoder and decoder structure and the innovative components inside encoder, MFANet is very powerful for the semantic segmentation of remote sensing images and can suit the complicated environment. We evaluate our MFANet on the Vaihingen and Potsdam data sets, and it outperforms the state-of-art methods both in the metric and visual effect.","1558-0571","","10.1109/LGRS.2020.2998680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119775","Channel attention block (CAB);decoder;multilevel feature fusion block (MLFFB);nonlocal block;remote sensing;semantic segmentation","Decoding;Feature extraction;Image segmentation;Convolution;Semantics;Remote sensing;Interpolation","convolutional neural nets;feature extraction;image fusion;image sampling;image segmentation;object detection;remote sensing","feature mismatch;upsampling process;MFANet;encoder-based network;fully convolutional network;multilevel feature fusion block;multiscale feature pyramid;fine-grained feature maps;light convolution subnetwork;feature alignment;remote sensing image semantic segmentation;convolutional neural networks;multiscale feature fusion and alignment network;decoder-based network","","3","","19","IEEE","17 Jun 2020","","","IEEE","IEEE Journals"
"Infrared and Visible Image Fusion Method by Using Hybrid Representation Learning","G. He; J. Ji; D. Dong; J. Wang; J. Fan","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Information Science and Technology, Northwestern University, Xi’an, China; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, USA","IEEE Geoscience and Remote Sensing Letters","30 Oct 2019","2019","16","11","1796","1800","For remote sensing image fusion, infrared and visible images have very different brightness due to their disparate imaging mechanisms, the result of which is that nontarget regions in the infrared image often affect the fusion of details in the visible image. This letter proposes a novel infrared and visible image fusion method basing hybrid representation learning by combining dictionary-learning-based joint sparse representation (JSR) and nonnegative sparse representation (NNSR). In the proposed method, different fusion strategies are adopted, respectively, for the mean image, which represents the primary energy information, and for the deaveraged image, which contains important detail features. Since the deaveraged image contains a large amount of high-frequency details information of the source image, JSR is utilized to sparsely and accurately extract the common and innovation features of the deaveraged image, thus, accurately merging high-frequency details in the deaveraged image. Then, the mean image represents low-frequency and overview features of the source image, according to NNSR, mean image is classified well-directed to different feature regions and then fused, respectively. Such proposed method, on the one hand, can eliminate the impact on fusion result suffering from very different brightness causing by different imaging mechanism between infrared and visible image; on the other hand, it can improve the readability and accuracy of the result fusion image. Experimental result shows that, compared with the classical and state-of-the-art fusion methods, the proposed method not only can accurately integrate the infrared target but also has rich background details of the visible image, and the fusion effect is superior.","1558-0571","","10.1109/LGRS.2019.2907721","National Natural Science Foundation of China(grant numbers:61402368); Aerospace Science and Technology Innovation Foundation of China(grant numbers:2017ZD53047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698319","Hybrid sparse representation;infrared and visible image;mean image and deaveraged image;remote sensing image fusion","Image fusion;Feature extraction;Silicon;Dictionaries;Brightness;Imaging;Remote sensing","geophysical image processing;image fusion;image representation;infrared imaging;learning (artificial intelligence);remote sensing","visible image;result fusion image;hybrid representation learning;remote sensing image fusion;disparate imaging mechanisms;infrared image;nonnegative sparse representation;mean image;deaveraged image;source image;imaging mechanism;NNSR;JSR;dictionary-learning-based joint sparse representation","","10","","21","IEEE","24 Apr 2019","","","IEEE","IEEE Journals"
"Remote Sensing Image Fusion Based on Fuzzy Logic and Salience Measure","Y. Yang; H. Lu; S. Huang; W. Tu","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Geoscience and Remote Sensing Letters","26 Oct 2020","2020","17","11","1943","1947","Remote sensing image fusion is to fuse low spatial resolution multispectral (MS) images with high spatial resolution panchromatic (PAN) images to get high spatial resolution multispectral images. The component substitute (CS)-based methods are popular approaches for their high efficiency and high spatial resolution. However, they may produce spectral distortion, especially when there are large radiometric differences between PAN images and MS images. For tackling this problem, a new framework based on the CS model with fuzzy logic and salience measure is proposed. In order to get details that are highly relevant to MS image, a novel fuzzy logic rule based on the global salience measure is designed to fuse the details extracted from both PAN image and MS image. Furthermore, to better preserve the edges of the fused image, a new edge-preserving algorithm is defined to fuse the edges from the PAN image and MS image according to the local salience measure. A series of experiments are conducted and analyzed on both simulated images and real images from the data sets of four satellites to illustrate the effectiveness of the proposed method. Compared with some state-of-the-art methods, our method performs the best in both objective and subjective evaluations. Besides, our method has a low computational cost and is suitable for practical application.","1558-0571","","10.1109/LGRS.2019.2956286","National Natural Science Foundation of China(grant numbers:61662026,61862030); Natural Science Foundation of Jiangxi Province(grant numbers:20182BCB22006,20181BAB202010,20192ACB20002,20192ACBL21008); Project of the Education Department of Jiangxi Province(grant numbers:YC2018-B065,YC2019-B094); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930912","Edge preserving;fuzzy logic;remote sensing image fusion;salience measure","Image edge detection;Fuzzy logic;Spatial resolution;Fuses;Correlation;Remote sensing;Distortion","fuzzy logic;geophysical image processing;image fusion;image resolution;remote sensing","remote sensing image fusion;low spatial resolution multispectral images;high spatial resolution panchromatic images;high spatial resolution multispectral images;PAN image;MS image;fuzzy logic rule;salience measure;component substitute;spectral distortion;radiometric differences","","12","","23","IEEE","11 Dec 2019","","","IEEE","IEEE Journals"
"A Novel MRF-Based Multifeature Fusion for Classification of Remote Sensing Images","Q. Lu; X. Huang; J. Li; L. Zhang","School of Electronic Information, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Geography and Planning and Guangdong Key Laboratory for Urbanization and Geo-Simulation, Sun Yat-sen University, Guangzhou, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2016","13","4","515","519","The spatial information has been proved to be effective in improving the performance of spectral-based classification. However, it is difficult to describe different image scenes by using monofeature owing to complexity of the geospatial scenes. In this letter, a novel framework is developed to combine the multiple spectral and spatial features based on the Markov random field (MRF). Specifically, the pixels in an image are separated into reliable and unreliable ones according to the decision of multifeature classifications. The labels of the reliable pixels can be conveniently determined, but the unreliable pixels are then classified by fusing the multifeature classification results and reducing the classification uncertainties based on the MRF optimization. Experiments are conducted on three multispectral high-resolution images to verify the effectiveness of the proposed method. Several state-of-the-art multifeature classification methods are also achieved for the purpose of comparison. Moreover, three classifiers (i.e., multinomial logistic regression, support vector machines, and random forest) are used to test the performance of the proposed framework. It is shown that the proposed method can effectively integrate multiple features, yield promising results, and outperform other approaches compared.","1558-0571","","10.1109/LGRS.2016.2521418","National Natural Science Foundation of China(grant numbers:91338111); China National Science Fund for Excellent Young Scholars(grant numbers:41522110); National Excellent Doctoral Dissertation of PR China(grant numbers:201348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7421951","Classification;data fusion;high resolution;Markov random field (MRF);multifeature;Classification;data fusion;high resolution;Markov random field (MRF);multifeature","Reliability;Probabilistic logic;Remote sensing;Spatial resolution;Indexes;Markov processes;Uncertainty","geophysical image processing;image classification;image fusion;Markov processes;optimisation;remote sensing","MRF-based multifeature fusion;remote sensing image classification;spectral-based classification;image scenes;geospatial scenes;multiple spectral feature;Markov random field;multifeature classification;MRF optimization;multinomial logistic regression;support vector machine;random forest","","28","","14","IEEE","29 Feb 2016","","","IEEE","IEEE Journals"
"Pan-Sharpening Based on Background Prior Saliency and Joint Sparse Detail Extraction","L. Zhang; Y. Sun","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","23 Dec 2020","2021","18","1","142","146","For remote sensing images, the spectral and spatial quality requirements vary across regions. Foreground objects need high spatial quality, while background regions require high spectral fidelity for the successive processing. Saliency analysis is an effective tool for distinguishing and achieving these requirements. Thus, we propose a pan-sharpening method based on background prior saliency and joint sparse detail extraction for remote sensing images. First, aimed at the characteristics of remote sensing image fusion, a saliency analysis method based on the foreground distribution and background prior is proposed to produce a regulation factor, which can reflect the different spatial and spectral information requirements of the foreground and background. Then, we propose a detail extraction and fusion method based on the guided filter and sparse representation. We extract spatial details not only from panchromatic (PAN) images but also from multispectral (MS) images and fuse them by maximizing the sparse coefficient strategy to reduce instabilities and dissimilarities. Finally, the regulation factor is used to regulate the detail injection in the pan-sharpening process. Our method can satisfy the various spatial and spectral resolution requirements for different regions more accurately. Compared to other state-of-art methods, both the visual and quantitative results reveal that our method has a better performance at improving the spatial quality and preserving the spectral fidelity.","1558-0571","","10.1109/LGRS.2020.2968171","National Natural Science Foundation of China(grant numbers:61571050,41771407); Beijing Natural Science Foundation(grant numbers:L182029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8985391","Background prior;pan-sharpening;remote sensing;saliency analysis;sparse representation","Dictionaries;Remote sensing;Data mining;Spatial resolution;Visualization;Fuses","geophysical image processing;hyperspectral imaging;image filtering;image fusion;image resolution;image segmentation;remote sensing","background prior saliency;joint sparse detail extraction;remote sensing images;foreground objects;background regions;pan-sharpening method;remote sensing image fusion;saliency analysis method;foreground distribution;spectral information requirements;sparse representation;panchromatic images;multispectral image fusion;sparse coefficient strategy;pan-sharpening process;spatial resolution requirement;spectral resolution requirement;guided filter","","","","19","IEEE","6 Feb 2020","","","IEEE","IEEE Journals"
"Multilayer Feature Fusion Network for Scene Classification in Remote Sensing","K. Xu; H. Huang; Y. Li; G. Shi","Key Laboratory of Optoelectronic Technology and Systems, Education Ministry of China, Chongqing University, Chongqing, China; Key Laboratory of Optoelectronic Technology and Systems, Education Ministry of China, Chongqing University, Chongqing, China; Key Laboratory of Optoelectronic Technology and Systems, Education Ministry of China, Chongqing University, Chongqing, China; Key Laboratory of Optoelectronic Technology and Systems, Education Ministry of China, Chongqing University, Chongqing, China","IEEE Geoscience and Remote Sensing Letters","27 Oct 2020","2020","17","11","1894","1898","The scene classification of high spatial resolution (HSR) images is a challenging task in the remote sensing community. How to construct a discriminative representation of the HSR scene is a key step to improve classification performance. In this letter, we propose a novel feature extraction method termed multilayer feature fusion network (MF2Net) for scene classification. At first, the transferred VGGNet-16 model is employed as a feature extractor to acquire multilayer convolutional features. Then, several layers including pooling, transformation, and fusion layers are designed to process hierarchical features in four branches, and the prediction probability can be obtained for classification. Finally, the proposed model is optimized by fine-tuning techniques, where a novel data augmentation approach is explored to improve generalization ability. As a result, MF2Net effectively applies useful information from multilayers to improve the accuracy of scene classification. The experimental results on AID and NWPU-RESISC45 data sets exhibit that the MF2Net method obtains quite competitive classification results compared with many state-of-the-art methods.","1558-0571","","10.1109/LGRS.2019.2960026","Basic and Frontier Research Programs of Chongqing(grant numbers:cstc2018jcyjAX0093); Graduate Research and Innovation Foundation of Chongqing, China(grant numbers:CYB19039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948234","Convolutional neural network (CNN);multilayer feature fusion network (MF²Net);remote sensing (RS);scene classification;transfer learning","Convolutional codes;Feature extraction;Nonhomogeneous media;Training;Encoding;Data models;Remote sensing","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image resolution;multilayer perceptrons;probability;remote sensing;transforms","multilayer feature fusion network;scene classification;high spatial resolution images;remote sensing;HSR scene;feature extraction;multilayer convolutional features;fusion layer;pooling layer;transformation layer;data augmentation;NWPU-RESISC45 dataset;AID dataset;MF2Net","","46","","33","IEEE","1 Jan 2020","","","IEEE","IEEE Journals"
"Combining Multilevel Features for Remote Sensing Image Scene Classification With Attention Model","J. Ji; T. Zhang; L. Jiang; W. Zhong; H. Xiong","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Electronic Engineering, Tsinghua University, Beijing, China.; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1647","1651","Remote sensing (RS) image scene classification is a challenging task due to its intraclass variety and the interclass similarity. Recently, many convolutional neural network (CNN)-based methods explore the network to handle this task. However, RS images usually have confusing background in addition to the relevant objects, and features only derived from the whole RS images cannot achieve satisfying results. To solve the problem, this letter proposed a method of utilizing the attention network to localize multiscale discriminative regions of the RS scene images and combining features learned from the localized regions by a classification network. Specifically, the classification network is composed of three subnetworks, which are trained by certain scaled regions separately. To learn more discriminative feature representations, feature fusion module is introduced to fuse the features of the three subnetworks in a more effective way. Experiments conducted on the AID and NWPU-RESISC45 data sets evaluate the effectiveness of the proposed method.","1558-0571","","10.1109/LGRS.2019.2949253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8889697","Discriminative region;image classification;remote sensing (RS);visual attention","Feature extraction;Task analysis;Semantics;Neurons;Remote sensing;Image recognition;Fuses","convolutional neural nets;feature extraction;geophysical image processing;image classification;image enhancement;image fusion;image representation;learning (artificial intelligence);remote sensing","intraclass variety;interclass similarity;convolutional neural network-based methods;attention network;multiscale discriminative regions;RS scene images;combining features;localized regions;classification network;discriminative feature representations;multilevel features;remote sensing image scene classification;attention model;feature fusion module;subnetworks;AID datasets;NWPU-RESISC45 data sets","","23","","27","IEEE","1 Nov 2019","","","IEEE","IEEE Journals"
"Multilayer Feature Fusion With Weight Adjustment Based on a Convolutional Neural Network for Remote Sensing Scene Classification","C. Ma; X. Mu; R. Lin; S. Wang","Xi’an Research Institute of Hi-Tech, Xi’an, China; Xi’an Research Institute of Hi-Tech, Xi’an, China; Xi’an Research Institute of Hi-Tech, Xi’an, China; Xi’an Research Institute of Hi-Tech, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","20 Jan 2021","2021","18","2","241","245","Remote sensing scene classification is still a challenging task. Extracting features effectively from restricted existing labeled data is key to scene classification. Convolutional neural networks (CNNs) are an effective method of constructing discriminating feature representation. However, CNNs usually utilize the feature map from the last layer and ignore additional layers with valuable feature information. In addition, the direct integration of multiple layers brings only a small improvement due to feature redundancy and destruction. To explore the potential information from additional layers and improve the effect of feature fusion, we propose multilayer feature fusion accesses with weight adjustment based on a CNN. We construct access to deliver additional features to one layer to achieve feature fusion and set weight factors to adjust the fusion degree to reduce feature redundancy and destruction. We perform experiments on two common data sets, which indicate improved accuracies and advantages of the extraction capability of our method.","1558-0571","","10.1109/LGRS.2020.2970810","National Natural Science Foundation of China(grant numbers:61601475); Aeronautical Science Foundation of China(grant numbers:201555U8010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999791","Convolutional neural network;feature fusion;remote sensing scene classification;weight adjustment","Feature extraction;Redundancy;Nonhomogeneous media;Fuses;Remote sensing;Semantics;Kernel","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);remote sensing","weight adjustment;fusion degree;feature redundancy;convolutional neural network;remote sensing scene classification;labeled data;CNNs;feature representation;feature map;valuable feature information;multilayer feature fusion","","10","","20","IEEE","14 Feb 2020","","","IEEE","IEEE Journals"
"A Hierarchical Context Embedding Network for Object Detection in Remote Sensing Images","K. Zhang; Y. Wu; J. Wang; Q. Wang","School of Astronautics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, Northwestern Polytechnical University, Xi’an, China; School of Astronautics, School of Artificial Intelligence, OPtics and ElectroNics (iOPEN), Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, OPtics and ElectroNics (iOPEN), Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","11 Apr 2022","2022","19","","1","5","Compared with general optical images, remote sensing images (RSIs) capture large areas from high altitudes with a bird’s eye view, which is responsible for the many categories and scale variations of objects in the images, as well as the abundant scene information. Although the complexity of the RSIs presents a significant challenge to the object detection task, the complexity presents opportunities as well. The RSIs contain plenty of object-related context information, which is valuable for boosting the object detection performance. To address the existing issue of poor context utilization in RSIs, we propose a hierarchical context embedding network (HCENet) in this letter. First, we construct a semantic feature pyramid, in which the semantic context aggregation module (SFAM) integrates the semantic contexts included in the adjacent layers of features with a novel feature fusion mechanism. Furthermore, the scene-level context embedding module (SLCEM) extracts the scene context of the overall image by a simple design and is utilized to guide feature classification. Finally, we outperform the popular object detectors on the publicly available DOTA-v1.5 dataset, achieving superior performance.","1558-0571","","10.1109/LGRS.2022.3161938","National Natural Science Foundation of China(grant numbers:61976179,U21B2041); Innovation Capability Support Program of Shaanxi(grant numbers:2021KJXX-103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740689","Object detection;remote sensing images (RSIs);scene-level context;semantic context","Feature extraction;Semantics;Convolution;Object detection;Task analysis;Remote sensing;Proposals","feature extraction;geophysical image processing;image classification;image fusion;image texture;object detection;remote sensing","object detection task;RSIs;object-related context information;object detection performance;poor context utilization;hierarchical context embedding network;semantic feature pyramid;semantic context aggregation module;semantic contexts;scene-level context embedding module;scene context;popular object detectors;remote sensing images;general optical images;capture large areas;high altitudes;bird;abundant scene information","","2","","28","IEEE","24 Mar 2022","","","IEEE","IEEE Journals"
"EFCNet: Ensemble Full Convolutional Network for Semantic Segmentation of High-Resolution Remote Sensing Images","L. Chen; X. Dou; J. Peng; W. Li; B. Sun; H. Li","School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China; Institute of Intelligent Machines, Chinese Academy of Sciences, Hefei, China; School of Geosciences and Info-Physics, Central South University, Changsha, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Convolutional neural networks (CNNs) have achieved remarkable results in semantic segmentation of high-resolution remote sensing images (HRRSIs). However, the scales and textures of HRRSIs are diverse, which makes it difficult for a fixed-layer CNN to obtain rich features. In this regard, we propose an end-to-end ensemble fully convolutional network (EFCNet), which mainly includes two modules: the adaptive fusion module (AFM) and the separable convolutional module (SCM). The AFM can fuse features of different scales based on ensemble learning, whereas the SCM can reduce the complexity of the model under multifeature fusion. In the experiment, we use UNet and PSPNet to verify the framework on the ISPRS Vaihingen and Potsdam datasets. The experimental results show that the EFCNet can effectively improve the final segmentation performance and reduce the complexity of the ensemble model.","1558-0571","","10.1109/LGRS.2021.3076093","National Natural Science Foundation of China(grant numbers:41871302,61773360,41871364); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423971","Convolutional neural network (CNN);ensemble learning;remote sensing;semantic segmentation","Convolution;Feature extraction;Kernel;Training;Remote sensing;Image segmentation;Spatial resolution","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);remote sensing","EFCNet;ensemble full convolutional network;semantic segmentation;high-resolution remote sensing images;convolutional neural networks;HRRSI;fixed-layer CNN;fully convolutional network;adaptive fusion module;AFM;separable convolutional module;ensemble learning;final segmentation performance;ensemble model","","1","","22","IEEE","5 May 2021","","","IEEE","IEEE Journals"
"Semantic Segmentation Network Using Local Relationship Upsampling for Remote Sensing Images","B. Lin; G. Yang; Q. Zhang; G. Zhang","Shanghai Key Laboratory of Multidimensional Information Processing, School of Computer Science and Technology, East China Normal University, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, School of Computer Science and Technology, East China Normal University, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, School of Computer Science and Technology, East China Normal University, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, School of Computer Science and Technology, East China Normal University, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Semantic segmentation is a fundamental task in remote sensing image processing. It provides pixel-level classification, which is important for many applications, such as building extraction and land use mapping. The development of convolutional neural network has considerably improved the performance of semantic segmentation. Most semantic segmentation networks are the encoder–decoder structure. Bilinear interpolation is an ordinary upsampling method in the decoder, but bilinear interpolation only considers its own features and inserts three times its own features. This over-simple and data-independent bilinear upsampling may lead to suboptimal results. In this work, we propose an upsampling method based on local relations to replace bilinear interpolation. Upsampling is performed by correlating the local relationship of feature maps of adjacent stages, which can better integrate local and global information. We also design a fusion module based on local similarity. Our proposed method with ResNet101 as the backbone of the segmentation network can improve the average  $F_{1}$  score and overall accuracy of the Vaihingen data set by 2.69% and 1.31%, respectively. Our proposed method also has fewer parameters and less inference time.","1558-0571","","10.1109/LGRS.2020.3047443","National Nature Science Foundation of China(grant numbers:61731009,41301472); Science and Technology Commission of Shanghai Municipality(grant numbers:19511120600,18DZ2270800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321129","Decoder;local relationship upsampling;remote sensing;semantic segmentation","Semantics;Image segmentation;Decoding;Remote sensing;Interpolation;Task analysis;Fuses","convolutional neural nets;feature extraction;image classification;image fusion;image sampling;image segmentation;interpolation;remote sensing","semantic segmentation network;local relationship upsampling;remote sensing image processing;pixel-level classification;convolutional neural network;encoder-decoder structure;bilinear interpolation;feature maps;local similarity;fusion module","","","","22","IEEE","12 Jan 2021","","","IEEE","IEEE Journals"
"Remote Sensing Image Fusion via Boundary Measured Dual-Channel PCNN in Multi-Scale Morphological Gradient Domain","W. Tan; P. Xiang; J. Zhang; H. Zhou; H. Qin","Department of Computer Science, University of Copenhagen, Copenhagen, Denmark; School of Physics and Optoelectronic Engineering, Xidian University, Xi’an, China; School of Physics and Optoelectronic Engineering, Xidian University, Xi’an, China; School of Physics and Optoelectronic Engineering, Xidian University, Xi’an, China; School of Physics and Optoelectronic Engineering, Xidian University, Xi’an, China","IEEE Access","9 Mar 2020","2020","8","","42540","42549","In this paper, a remote sensing image fusion method based on boundary measured dual-channel pulse-coupled neural network (PCNN) in multi-scale morphological gradient (MSMG) domain is proposed. Firstly, the panchromatic (PAN) image is decomposed into three parts, a small-scale image, a large-scale image, and a base image through a co-occurrence filtering (CoF)-based decomposition model. Secondly, an HSI transform is applied in the multispectral (MS) image to obtained intensity, hue and saturation components. Thirdly, a PCNN fusion strategy modulated by MSMG is used to fuse the base image and the intensity component of the MS image. Then, a fused intensity image is obtained by combining the small-scale image, large-scale image and the fused approximate image. Finally, the final fused image can be reconstructed by an inverse HSI transform. Experiments in four datasets demonstrate that the proposed method obtains the best performance in most cases.","2169-3536","","10.1109/ACCESS.2020.2977299","National Natural Science Foundation of China(grant numbers:61675160); Higher Education Discipline Innovation Project(grant numbers:B17035); China Scholarship Council(grant numbers:CSC201906960047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018222","Remote sensing image fusion;co-occurrence filtering;multi-scale morphological gradient;pulse-coupled neural network","Image fusion;Remote sensing;Transforms;Image edge detection;Image reconstruction;Spatial resolution;Approximation algorithms","geophysical signal processing;image fusion;image resolution;neural nets;remote sensing;transforms","multiscale morphological gradient domain;dual-channel pulse-coupled neural network;remote sensing image fusion method;boundary measured dual-channel PCNN;fused approximate image;fused intensity image;MS image;MSMG;PCNN fusion strategy;saturation components;hue;multispectral image;co-occurrence filtering-based decomposition model;base image;large-scale image;small-scale image;panchromatic image","","18","","30","CCBY","28 Feb 2020","","","IEEE","IEEE Journals"
"Noisy Remote Sensing Image Fusion Based on JSR","X. Ma; S. Hu; S. Liu; J. Wang; S. Xu","Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; Machine Vision Engineering Research Center of Hebei Province, Hebei University, Baoding, China; Machine Vision Engineering Research Center of Hebei Province, Hebei University, Baoding, China; Research Institute of TV and Electro-Acoustics, Beijing, China","IEEE Access","19 Feb 2020","2020","8","","31069","31082","Compressed sensing has shown great potential and power in image representation, especially in image reconstruction by sparse representation. Due to complementary information and unavoidable noise existing in synthetic aperture radar (SAR) and other source images, joint sparse representation (JSR) is developed to separate redundancy and complementary information with different properties in source images and obtain a fused image, where image de-noising is done simultaneously owing to that noise is not sparse and cannot be represented by sparse representation. As a result, one noisy remote sensing image fusion method based on JSR is presented in this paper. After obtaining redundant and complementary sub-images by JSR, an improved fusion rule based on pulse coupled neural network (PCNN) is employed to fuse complementary sparse coefficients together. At the same time, because the types of noise in SAR and other source images are different, they can be treated as the complementary information in source images and suppressed at this step. Finally, a fused image can be reconstructed by adding the redundant and fused complementary sub-images. Quantitative and qualitative experimental results show that the proposed method outperforms most of other fusion methods and it is more robust to noise, having better visual effects and values of objective evaluation metrics.","2169-3536","","10.1109/ACCESS.2020.2973435","National Natural Science Foundation of China(grant numbers:61572063,61401308); Natural Science Foundation of Hebei Province(grant numbers:F2016201142,F2019201151,F2018210148); Natural Science Foundation of Hebei Province(grant numbers:QN2016085,QN2017306); Opening Foundation of Machine vision Engineering Research Center of Hebei Province(grant numbers:2018HBMV01,2018HBMV02); Hebei University(grant numbers:2014-303,8012605); Hebei University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995486","Remote sensing image fusion;joint sparse representation;pulse coupled neural network;SAR image processing","Image fusion;Dictionaries;Noise measurement;Transforms;Remote sensing;Image reconstruction;Synthetic aperture radar","image denoising;image fusion;image reconstruction;image representation;neural nets;remote sensing","JSR;image representation;image reconstruction;complementary information;source images;joint sparse representation;fused image;image de-noising;noisy remote sensing image fusion method;complementary sub-images;complementary sparse coefficients;pulse coupled neural network;PCNN","","1","","52","CCBY","12 Feb 2020","","","IEEE","IEEE Journals"
"Multisensor Satellite Image Fusion and Networking for All-Weather Environmental Monitoring","N. -B. Chang; K. Bai; S. Imen; C. -F. Chen; W. Gao","University of Central Florida, Orlando, FL, US; East China Normal University, Shanghai, CN; University of Central Florida, Orlando, FL, US; National Central University, Chung-Li, TW; Colorado State University, Fort Collins, CO, US","IEEE Systems Journal","2 May 2018","2018","12","2","1341","1357","Given the advancements of remote sensing technology, large volumes of remotely sensed images with different spatial, temporal, and spectral resolutions are available. To better monitor and understand the changing Earth's environment, fusion of remotely sensed images with different spatial, temporal, and spectral resolutions is critical for distinctive feature retrieval, interpretation, mapping, and decision analysis. A suite of methods have been developed to fuse multisensor satellite images for different purposes in the past few decades. This paper provides a thorough review of contemporary and classic image fusion methods and presents a summary of their phenomenological applications, with challenges and perspectives, for environmental systems analysis. Cross-mission satellite image fusion, networking, and missing value pixel reconstruction for environmental monitoring are described, and their complex integration is illustrated with a case study of Lake Nicaragua that elucidates the state-of-the-art remote sensing technologies for advancing water quality management.","1937-9234","","10.1109/JSYST.2016.2565900","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7485881","Earth observation;environmental systems engineering;feature extraction;image fusion;remote sensing;satellite networking","Image fusion;Feature extraction;Remote sensing;Satellites;Spatial resolution;Data integration;Instruments","environmental monitoring (geophysics);geophysical image processing;image fusion;image resolution;lakes;remote sensing;water quality","environmental monitoring;remote sensing technology;multisensor satellite image fusion;all-weather environmental;remotely sensed images;spectral resolutions;distinctive feature retrieval;classic image fusion methods;environmental systems analysis;cross-mission satellite image fusion;spatial;Earth's environment;Lake Nicaragua;water quality management","","28","","136","IEEE","6 Jun 2016","","","IEEE","IEEE Journals"
"A New Pan-Sharpening Method With Deep Neural Networks","W. Huang; L. Xiao; Z. Wei; H. Liu; S. Tang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Science, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2015","12","5","1037","1041","A deep neural network (DNN)-based new pansharpening method for the remote sensing image fusion problem is proposed in this letter. Research on representation learning suggests that the DNN can effectively model complex relationships between variables via the composition of several levels of nonlinearity. Inspired by this observation, a modified sparse denoising autoencoder (MSDA) algorithm is proposed to train the relationship between high-resolution (HR) and low-resolution (LR) image patches, which can be represented by the DNN. The HR/LR image patches only sample from the HR/LR panchromatic (PAN) images at hand, respectively, without requiring other training images. By connecting a series of MSDAs, we obtain a stacked MSDA (S-MSDA), which can effectively pretrain the DNN. Moreover, in order to better train the DNN, the entire DNN is again trained by a back-propagation algorithm after pretraining. Finally, assuming that the relationship between HR/LR multispectral (MS) image patches is the same as that between HR/LR PAN image patches, the HR MS image will be reconstructed from the observed LR MS image using the trained DNN. Comparative experimental results with several quality assessment indexes show that the proposed method outperforms other pan-sharpening methods in terms of visual perception and numerical measures.","1558-0571","","10.1109/LGRS.2014.2376034","National Natural Science Foundation of China(grant numbers:11431015,61301215,61101194,61171165); National Scientific Equipment Developing Project of China(grant numbers:2012YQ050250); Jiangsu Provincial Postdoctoral Research Funding plan of China(grant numbers:1301025C); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018004","Deep neural networks (DNNs);multispectral (MS) image;panchromatic (PAN) image;pan-sharpening;Deep neural networks (DNNs);multispectral (MS) image;panchromatic (PAN) image;pan-sharpening","Image reconstruction;Remote sensing;Training;Spatial resolution;Image fusion;Neural networks","geophysical image processing;geophysical techniques;image coding;image denoising;image fusion;image reconstruction;image representation;image resolution;neural nets;numerical analysis;remote sensing","pan-sharpening method;deep neural network;remote sensing image fusion problem;DNN;representation learning;model complex relationships;nonlinearity;modified sparse denoising autoencoder algorithm;high-resolution image patches;low-resolution image patches;low-resolution panchromatic images;high-resolution panchromatic images;MSDA series;high-resolution multispectral image patches;low-resolution multispectral image patches;HR MS image reconstruction;quality assessment indexes;visual perception;numerical measures;back-propagation algorithm","","236","","17","IEEE","22 Jan 2015","","","IEEE","IEEE Journals"
"Double-Layer Fuzzy Fusion for Multiview Through-Wall Radar Images","X. Chen; W. Chen","Key Laboratory of Electromagnetic Space Information of the Chinese Academy of Sciences, University of Science and Technology of China, Hefei, China; Key Laboratory of Electromagnetic Space Information of the Chinese Academy of Sciences, University of Science and Technology of China, Hefei, China","IEEE Geoscience and Remote Sensing Letters","7 Aug 2015","2015","12","10","2075","2079","In this letter, a double-layer fuzzy fusion method is proposed to produce a higher quality image by combing multiple through-wall radar images of the same scene. The first layer is termed as intraimage fusion, in which the global and local degrees of membership are fused within each input image based on intraimage fuzzy rules. Then, in the second layer, interimage fusion is performed between input images using interimage fuzzy rules, and the output image is obtained. Compared with other fusion methods, the proposed method decreases the possibility of losing a weak target and suppresses the clutter by taking into account the local information while enhancing the target intensity values. The method also introduces a K-means clustering technique to automatically select the parameters of the membership functions. The performance of the proposed method is evaluated using both simulated and real experimental data.","1558-0571","","10.1109/LGRS.2015.2448051","National Natural Science Foundation of China(grant numbers:61172155,61331020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154422","Fuzzy logic;image fusion;$K$-means clustering;through-wall radar imaging (TWRI);Fuzzy logic;image fusion;$K$-means clustering;through-wall radar imaging (TWRI)","Radar imaging;Remote sensing;Image fusion;Clutter;Imaging;Radar remote sensing","fuzzy logic;image fusion;pattern clustering;remote sensing by radar","multiview through-wall radar images;double-layer fuzzy fusion method;image quality;intraimage fusion;intraimage fuzzy rule;output image;K-means clustering technique","","8","","17","IEEE","10 Jul 2015","","","IEEE","IEEE Journals"
"Unaligned Hyperspectral Image Fusion via Registration and Interpolation Modeling","J. Ying; H. -L. Shen; S. -Y. Cao","College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China; Ningbo Research Institute, Zhejiang University, Ningbo, China; College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","4 Jan 2022","2022","60","","1","14","In satellite remote sensing, the hyperspectral sensor acquires high-spectral-resolution and low-spatial-resolution hyperspectral images (HSIs). Conversely, the multispectral sensor acquires low-spectral-resolution and high-spatial-resolution multispectral images (MSIs). Thus, HSI and MSI fusion is required to promote both spatial and spectral resolutions. Currently, most algorithms are based on the assumption that the HSI and MSI are perfectly aligned. However, this is hardly achievable in real scenarios when the two sensors acquire images from different viewpoints. In this article, we propose a fusion algorithm that consists of two stages, i.e., image registration and image fusion. For image registration, we introduce the normalized edge difference (NED) for image similarity measure considering the different resolutions of the original images. For image fusion, we incorporate the interpolation process in the spatial degradation model to compensate for the interpolation error. Experimental results show that our algorithm performs better than the state of the arts for unaligned image fusion.","1558-0644","","10.1109/TGRS.2021.3081136","National Natural Science Foundation of China(grant numbers:81973751); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442904","Hyperspectral image (HSI);image fusion;image interpolation;image registration;image super-resolution;multispectral image (MSI)","Image registration;Interpolation;Image edge detection;Image fusion;Transforms;Superresolution;Matrix decomposition","geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image fusion;image registration;image resolution;image sensors;interpolation;remote sensing","unaligned hyperspectral image fusion;satellite remote sensing;hyperspectral sensor;high-spectral-resolution;low-spatial-resolution hyperspectral images;multispectral sensor;low-spectral-resolution;high-spatial-resolution multispectral images;spatial resolutions;spectral resolutions;fusion algorithm;image registration;image similarity measure;original images;spatial degradation model;unaligned image fusion","","5","","52","IEEE","27 May 2021","","","IEEE","IEEE Journals"
"A Gather-to-Guide Network for Remote Sensing Semantic Segmentation of RGB and Auxiliary Image","X. Zheng; X. Wu; L. Huan; W. He; H. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","15","Convolutional neural network (CNN)-based feature fusion of RGB and auxiliary remote sensing data is known to enable improved semantic segmentation. However, such fusion is challengeable because of the substantial variance in data characteristics and quality (e.g., data uncertainties and misalignment) between two modality data. In this article, we propose a unified gather-to-guide network (G2GNet) for remote sensing semantic segmentation of RGB and auxiliary data. The key aspect of the proposed architecture is a novel gather-to-guide module (G2GM) that consists of a feature gatherer and a feature guider. The feature gatherer generates a set of cross-modal descriptors by absorbing the complementary merits of RGB and auxiliary modality data. The feature guider calibrates the RGB feature response by using the channel-wise guide weights extracted from the cross-modal descriptors. In this way, the G2GM can perform RGB feature calibration with different modality data in a gather-to-guide fashion, thus preserving the informative features while suppressing redundant and noisy information. Extensive experiments conducted on two benchmark datasets show that the proposed G2GNet is robust to data uncertainties while also improving the semantic segmentation performance of RGB and auxiliary remote sensing data.","1558-0644","","10.1109/TGRS.2021.3103517","National Key Research and Development Program of China(grant numbers:2018Y-FB0505401); National Natural Science Foundation of China Project(grant numbers:42071370,41871361); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519842","Deep learning;remote sensing;semantic segmentation","Semantics;Image segmentation;Remote sensing;Feature extraction;Convolutional neural networks;Calibration;Task analysis","convolutional neural nets;data analysis;feature extraction;image classification;image colour analysis;image fusion;image segmentation;remote sensing","gather-to-guide network;remote sensing;auxiliary image;convolutional neural network;data characteristics;G2GNet;G2GM;feature gatherer;feature guider;cross-modal descriptors;RGB feature;channel-wise guide weights;semantic segmentation;CNN","","4","","68","IEEE","20 Aug 2021","","","IEEE","IEEE Journals"
"Superpixel-Based Graphical Model for Remote Sensing Image Mapping","G. Zhang; X. Jia; J. Hu","Remote Sensing Research Center, University of New South Wales, Canberra, BC, Australia; School of Engineering and Information Technology, National Engineering Research Center for Information Technology in Agriculture, Beijing, BC, China; School of Engineering and Information Technology, University of New South Wales, Canberra, BC, Australia","IEEE Transactions on Geoscience and Remote Sensing","11 Aug 2015","2015","53","11","5861","5871","Object-oriented remote sensing image classification is becoming more and more popular because it can integrate spatial information from neighboring regions of different shapes and sizes into the classification procedure to improve the mapping accuracy. However, object identification itself is difficult and challenging. Superpixels, which are groups of spatially connected similar pixels, have the scale between the pixel level and the object level and can be generated from oversegmentation. In this paper, we establish a new classification framework using a superpixel-based graphical model. Superpixels instead of pixels are applied as the basic unit to the graphical model to capture the contextual information and the spatial dependence between the superpixels. The advantage of this treatment is that it makes the classification less sensitive to noise and segmentation scale. The contribution of this paper is the application of a graphical model to remote sensing image semantic segmentation. It is threefold. 1) Gradient fusion is applied to multispectral images before the watershed segmentation algorithm is used for superpixel generation. 2) A probabilistic fusion method is designed to derive node potential in the superpixel-based graphical model to address the problem of insufficient training samples at the superpixel level. 3) A boundary penalty between the superpixels is introduced in the edge potential evaluation. Experiments on three real data sets were conducted. The results show that the proposed method performs better than the related state-of-the-art methods tested.","1558-0644","","10.1109/TGRS.2015.2423688","National Natural Science Foundation of China(grant numbers:41431179); Natural Science Foundation of Tianjin(grant numbers:14CYBJC41700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119598","Graphical model;remote sensing;superpixel;Graphical model;remote sensing;superpixel","Graphical models;Remote sensing;Image segmentation;Object oriented modeling;Noise;Probabilistic logic;Training","geophysical image processing;geophysical techniques;image classification;image fusion;image segmentation;remote sensing","superpixel-based graphical model;remote sensing image mapping;object-oriented remote sensing image classification;classification procedure;classification framework;contextual information;segmentation scale;remote sensing image semantic segmentation;gradient fusion;watershed segmentation algorithm;superpixel generation;probabilistic fusion method;edge potential evaluation;state-of-the-art methods","","76","","37","IEEE","8 Jun 2015","","","IEEE","IEEE Journals"
"Multimodal Classification of Remote Sensing Images: A Review and Future Directions","L. Gómez-Chova; D. Tuia; G. Moser; G. Camps-Valls","Image Processing Laboratory (IPL), Universitat de Valéncia, Valéncia, Spain; Department of Geography, University of Zürich, Zürich, Switzerland; Department of Electrical, Electronic, Telecommunications Engineering and Naval Architecture (DITEN), University of Genoa, Genova, Italy; Image Processing Laboratory (IPL), Universitat de Valéncia, Valéncia, Spain","Proceedings of the IEEE","21 Aug 2015","2015","103","9","1560","1584","Earth observation through remote sensing images allows the accurate characterization and identification of materials on the surface from space and airborne platforms. Multiple and heterogeneous image sources can be available for the same geographical region: multispectral, hyperspectral, radar, multitemporal, and multiangular images can today be acquired over a given scene. These sources can be combined/fused to improve classification of the materials on the surface. Even if this type of systems is generally accurate, the field is about to face new challenges: the upcoming constellations of satellite sensors will acquire large amounts of images of different spatial, spectral, angular, and temporal resolutions. In this scenario, multimodal image fusion stands out as the appropriate framework to address these problems. In this paper, we provide a taxonomical view of the field and review the current methodologies for multimodal classification of remote sensing images. We also highlight the most recent advances, which exploit synergies with machine learning and signal processing: sparse methods, kernel-based fusion, Markov modeling, and manifold alignment. Then, we illustrate the different approaches in seven challenging remote sensing applications: 1) multiresolution fusion for multispectral image classification; 2) image downscaling as a form of multitemporal image fusion and multidimensional interpolation among sensors of different spatial, spectral, and temporal resolutions; 3) multiangular image classification; 4) multisensor image fusion exploiting physically-based feature extractions; 5) multitemporal image classification of land covers in incomplete, inconsistent, and vague image sources; 6) spatiospectral multisensor fusion of optical and radar images for change detection; and 7) cross-sensor adaptation of classifiers. The adoption of these techniques in operational settings will help to monitor our planet from space in the very near future.","1558-2256","","10.1109/JPROC.2015.2449668","Generalitat Valenciana; Swiss National Science Foundation(grant numbers:PP00P2_150593); Spanish Ministry of Economy and Competitiveness (MINECO); Italian Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7182258","Classification;fusion;multiangular;multimodal image analysis;multisource;multitemporal;remote sensing;Classification;fusion;multiangular;multimodal image analysis;multisource;multitemporal;remote sensing","Remote sensing;Spatial resolution;Sensors;Satellites;Image fusion;Synthetic aperture radar","geophysical image processing;geophysical techniques;image classification;image fusion;remote sensing","remote sensing image;image multimodal classification;Earth observation;material identification;material characterization;space platforms;airborne platforms;heterogeneous image sources;satellite sensors;material classification;multimodal image fusion;signal processing;machine learning;sparse methods;kernel-based fusion;Markov modeling;manifold alignment;multiresolution fusion;multispectral image classification;multitemporal image fusion;multidimensional interpolation;optical images;radar images","","247","","160","IEEE","7 Aug 2015","","","IEEE","IEEE Journals"
"Bag-of-Visual-Words Scene Classifier for Remote Sensing Image Based on Region Covariance","X. Chen; G. Zhu; M. Liu","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","20 May 2022","2022","19","","1","5","Scene classification is of great significance to understand the semantics and extract the information of high spatial resolution remote sensing image. The bag-of-visual-words (BOVW) model is an effective method to understand the semantic content of images. It is widely used in scene classification of remote sensing image. The existed BOVW models usually use single feature or multiple features (such as spectrum, texture, and shape) to describe visual words. However, when considering multiple low-level feature fusion strategies, most methods only combine them by simple accumulation or concatenation which can not fully learn the relationship between different features. In this letter, a visual bag-of-words scene classifier based on regional covariance (RCOVBOVW) is proposed. This method can naturally fuse multiple related features, and the covariance calculation itself has the filtering ability, which can also reduce the dimension of the features and have high efficiency. Experiments have been conducted on two public and challenging datasets (University of California (UC) Merced and Northwestern Polytechnical University (NWPU)-RESISC45), and the results show that our proposed method outperforms the most state-of-the-art methods of remote sensing image scene classification.","1558-0571","","10.1109/LGRS.2022.3174167","National Natural Science Foundation of China(grant numbers:41871296); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9771467","Bag-of-visual-words (BOVW) model;feature fusion strategies;regional covariance;remote sensing image scene classification","Feature extraction;Remote sensing;Covariance matrices;Image color analysis;Image analysis;Semantics;Visualization","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);remote sensing","visual words;multiple low-level;bag-of-words scene classifier;regional covariance;multiple related features;covariance calculation;remote sensing image scene classification;bag-of-visual-words scene classifier;region covariance;high spatial resolution remote sensing image;bag-of-visual-words model;BOVW models","","","","24","IEEE","10 May 2022","","","IEEE","IEEE Journals"
"Remote Sensing Image Spatiotemporal Fusion via a Generative Adversarial Network With One Prior Image Pair","Y. Song; H. Zhang; H. Huang; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","17 May 2022","2022","60","","1","17","Spatiotemporal fusion (STF) is an effective solution to promote the application of remote sensing images, given that the tradeoff between the temporal resolution and the spatial resolution is ubiquitous in the production of remote sensing images. However, cloud coverage makes it difficult to obtain dense cloud-free Landsat–Moderate Resolution Imaging Spectroradiometer (MODIS) image pairs on the timeline, which limits the application of existing STF methods. Considering the lack of prior image pairs and the huge spatial resolution gap between Landsat and MODIS images, this article presents a novel remote sensing image STF method based on a generative adversarial network to handle one Landsat–MODIS prior image pair case (OPGAN), which contains a generator and a discriminator simultaneously trained in a min–max game. OPGAN is built based on the STF observation model that learns the base information from the prior Landsat image and then captures temporal change (TC) information from a difference image constructed from MODIS images collected at times 1 and 2 and sensor difference information from the difference image between Landsat and MODIS images at time 1. They are combined together to reconstruct the Landsat image at time 2 at both high spatial and high temporal resolution. Moreover, a change loss is proposed to further improve the accuracy of TC prediction. Extensive experiments on the STF dataset illustrate that the proposed OPGAN method can obtain more accurate prediction of spatial information and TCs in the case of insufficient prior information.","1558-0644","","10.1109/TGRS.2022.3171331","National Natural Science Foundation of China(grant numbers:61871298,42071322); Natural Science Foundation of Hubei Province(grant numbers:2020CFA053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765452","Change loss;generative adversarial network (GAN);one prior image pair;spatiotemporal fusion (STF)","Remote sensing;Earth;Artificial satellites;Spatial resolution;Image resolution;Generative adversarial networks;MODIS","geophysical image processing;image fusion;image resolution;neural nets;radiometry;remote sensing","STF methods;huge spatial resolution gap;MODIS images;novel remote sensing image STF method;generative adversarial network;Landsat-MODIS prior image pair case;STF observation model;prior Landsat image;temporal change information;difference image;high temporal resolution;insufficient prior information;dense cloud-free Landsat-Moderate Resolution Imaging Spectroradiometer;remote sensing image spatiotemporal fusion;OPGAN;high spatial resolution","","1","","54","IEEE","29 Apr 2022","","","IEEE","IEEE Journals"
"Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","M. Jiang; H. Shen; J. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2022","2022","60","","1","15","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal–spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.","1558-0644","","10.1109/TGRS.2022.3188998","National Natural Science Foundation of China(grant numbers:42130108,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829539","Deep residual cycle generative adversarial network (GAN);heterogeneous integrated framework;land-cover change;thick cloud cover","Generators;Remote sensing;Spatial resolution;Generative adversarial networks;Feature extraction;Image fusion;Optical sensors","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;sensor fusion;spatiotemporal phenomena","imaging degradation process;heterogeneous integrated fusion framework;complementary spatial information;temporal, information;spectral information;multisource heterogeneous observations;heterogeneous spatiospectral fusion;spatiotemporal fusion;heterogeneous spatiotemporal-spectral fusion;low-resolution imaging;cloud contamination;different remote sensing satellites;Moderate Resolution Imaging Spectroradiometer;image fusion method;spatio-temporal-spectral integrated fusion;heterogeneous remote sensing images;multisource remote sensing images;heterogeneous images;heterogeneous integrated framework;deep residual cycle generative adversarial network;forward fusion part;backward degeneration feedback part;forward part;desired fusion result","","","","54","IEEE","14 Jul 2022","","","IEEE","IEEE Journals"
"Moving Object Tracking via 3-D Total Variation in Remote-Sensing Videos","J. Wei; J. Sun; Z. Wu; J. Yang; Z. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; China Satellite Maritime Tracking and Control Department, Jiangyin, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2021","2022","19","","1","5","Tracking moving objects in remote-sensing videos is becoming increasingly important in remote-sensing analysis. This letter presents a novel object tracking method for remote-sensing videos. We start with using the traditional robust principal component analysis (RPCA) model to extract the moving object from the background. To describe the continuity of moving objects in spatial and temporal directions, we incorporate a 3-D total variation (3DTV) regularization into the RPCA model. Considering that the background is not static and the captured videos will contain noise because of the instability of the sensing camera, our proposed method introduces a certain part of the function to model the noise and capture the changes in background. Experimental results on real videos provided by 2016 IEEE GRSS Data Fusion Contest and 2020 Hyperspectral Object Tracking Challenge demonstrate the advantages of the moving object-tracking method via 3-D TV.","1558-0571","","10.1109/LGRS.2021.3077257","National Natural Science Foundation of China(grant numbers:61772274,61872185); Jiangsu Provincial Natural Science Foundation of China(grant numbers:BK20180018); Fundamental Research Funds for the Central Universities(grant numbers:30917015104,30919011103,30919011402); China Postdoctoral Science Foundation(grant numbers:2017M611814); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431588","Object tracking;remote-sensing videos;robust principal component","Videos;TV;Object tracking;Hyperspectral imaging;Sun;Analytical models;Cameras","geophysical image processing;image fusion;image motion analysis;image sequences;object detection;object tracking;principal component analysis;remote sensing;sensor fusion;video signal processing","remote-sensing videos;remote-sensing analysis;novel object tracking method;traditional robust principal component analysis model;3-D total variation regularization;3DTV;captured videos;sensing camera;2020 Hyperspectral Object Tracking Challenge;moving object-tracking method;3-D TV","","","","18","IEEE","14 May 2021","","","IEEE","IEEE Journals"
"Passive Multiarray Image Fusion for RF Tomography by Opportunistic Sources","G. Gennarelli; M. G. Amin; F. Soldovieri; R. Solimene","Institute for Electromagnetic Sensing of the Environment, Italian National Research Council (CNR), Napoli, Italy; Center for Advanced Communications, College of Engineering, Villanova University, Villanova, PA, USA; Institute for Electromagnetic Sensing of the Environment, Italian National Research Council (CNR), Napoli, Italy; Department of Information Engineering, Second University of Naples, Aversa, Italy","IEEE Geoscience and Remote Sensing Letters","8 Oct 2014","2015","12","3","641","645","The large diffusion of wireless infrastructures in public and private areas is currently stimulating research on surveillance radar systems capable of exploiting network transmissions as potential sources of opportunity. Since these sources are generally narrowband, we propose in this letter a single-frequency approach for imaging targets by using passive arrays deployed around the scattering scene. Single-frequency data allow casting the imaging as an inverse source problem, which avoids the need to retrieve information about the sources prior to imaging. The drawbacks of the highly coarse resolution and blinding effects due to the sources are overcome by employing a multiarray image fusion strategy in conjunction with a change detection scheme for imaging moving targets. The proposed approach is tested via numerical experiments based on full-wave synthetic data corresponding to an indoor scenario.","1558-0571","","10.1109/LGRS.2014.2354451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898877","Inverse source;multistatic radar;opportunistic sources;urban sensing;Inverse source;multistatic radar;opportunistic sources;urban sensing","Imaging;Radar imaging;Receivers;Scattering;Arrays;Image reconstruction;Image fusion","array signal processing;image fusion;inverse problems;passive radar;radar imaging;radar resolution;radar tracking;target tracking","indoor scenario;full-wave synthetic data;numerical experiment;moving target imaging;change detection scheme;multiarray image fusion strategy;blinding effect;highly coarse resolution;information retrieval;inverse source problem;single-frequency data;scattering scene;passive array;single-frequency approach;network transmission;surveillance radar systems;private area;public area;wireless infrastructures;opportunistic sources;RF tomography;passive multiarray image fusion","","14","","17","IEEE","15 Sep 2014","","","IEEE","IEEE Journals"
"Pansharpening for Cloud-Contaminated Very High-Resolution Remote Sensing Images","X. Meng; H. Shen; Q. Yuan; H. Li; L. Zhang; W. Sun","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; School of Resources and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resources and Environmental Sciences, Wuhan University, Wuhan, China; Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China","IEEE Transactions on Geoscience and Remote Sensing","23 Apr 2019","2019","57","5","2840","2854","The optical remote sensing images not only have to make a fundamental tradeoff between the spatial and spectral resolutions, but also are inevitable to be polluted by the clouds; however, the existing pansharpening methods mainly focus on the resolution enhancement of the optical remote sensing images without cloud contamination. How to fuse the cloud-contaminated images to achieve the joint resolution enhancement and cloud removal is a promising and challenging work. In this paper, a pansharpening method for the challenging cloud-contaminated very high-resolution remote sensing images is proposed. Furthermore, the cloud-contaminated conditions for the practical observations with all the thick clouds, the thin clouds, the haze, and the cloud shadows are comprehensively considered. In the proposed methods, a two-step fusion framework based on multisource and multitemporal observations is presented: 1) the thin clouds, the haze, and the light cloud shadows are proposed to be first jointly removed and 2) a variational-based integrated fusion model is then proposed to achieve the joint resolution enhancement and missing information reconstruction for the thick clouds and dark cloud shadows. Through the proposed fusion method, a promising cloud-free fused image with both high spatial and high spectral resolutions can be obtained. To comprehensively test and verify the proposed method, the experiments were implemented based on both the cloud-free and cloud-contaminated images, and a number of different remote sensing satellites including the IKONOS, the QuickBird, the Jilin (JL)-1, and the Deimos-2 images were utilized. The experimental results confirm the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2018.2878007","National Natural Science Foundation of China(grant numbers:41801252,61671334); Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8535032","Pansharpening;cloud contamination;integrated model;image fusion;remote sensing","Remote sensing;Spatial resolution;Radiometry;Optical imaging;Optical sensors;Satellites","clouds;geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing","pansharpening methods;remote sensing satellites;haze;variational-based integrated fusion model;fusion method;cloud-free fused image;IKONOS image;QuickBird image;Jilin-1 image;Deimos-2 image;spatial resolutions;Deimos-2 images;high spectral resolutions;dark cloud shadows;missing information reconstruction;light cloud shadows;cloud-contaminated conditions;high-resolution remote sensing images;cloud removal;joint resolution enhancement;cloud-contaminated images;optical remote sensing images","","41","","55","IEEE","14 Nov 2018","","","IEEE","IEEE Journals"
"SSAU-Net: A Spectral–Spatial Attention-Based U-Net for Hyperspectral Image Fusion","S. Liu; S. Liu; S. Zhang; B. Li; W. Hu; Y. -D. Zhang","National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; College of Electronic and Information Engineering, Hebei University, Baoding, China; College of Electronic and Information Engineering, Hebei University, Baoding, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Computing and Mathematics, University of Leicester, Leicester, U.K","IEEE Transactions on Geoscience and Remote Sensing","10 Nov 2022","2022","60","","1","16","Compared with the traditional remote sensing image, there is a large amount of spectral information in the hyperspectral image (HSI), which makes HSI better reflect the actual condition of surface features. However, due to the limitations of imaging conditions, HSI tends to have a lower spatial resolution. In order to overcome this issue, we propose a spectral–spatial attention-based U-Net named SSAU-Net for HSI and multispectral image (MSI) fusion. The SSAU-Net constructs a spectral–spatial attention module by a coordinate-attention (CA) module and an efficient pyramid split attention (EPSA) module, which can enhance the image’s spectral information and spatial information. Meanwhile, the proposed network fully extracts the shallow and deep features of the images and finally generates high-resolution (HR) HSIs. Compared with the state-of-the-art HSI-MSI fusion methods, the experimental results verify that the proposed method has a better subjective and objective fusion effect.","1558-0644","","10.1109/TGRS.2022.3217168","National Natural Science Foundation of China(grant numbers:62172139,U1936204,U1803119); National Key Research and Development Plan(grant numbers:2020AAA0106800); Natural Science Foundation of Hebei Province(grant numbers:F2022201055); Science Research Project of Hebei Province(grant numbers:BJ2020030); China Postdoctoral(grant numbers:2022M713361); Open Project Program of the National Laboratory of Pattern Recognition (NLPR)(grant numbers:202200007); Natural Science Interdisciplinary Research Program of Hebei University(grant numbers:DXK202102); Research Project of Hebei University Intelligent Financial Application Technology (HUIFAT) Research and Development Center(grant numbers:XGZJ2022022); High-Performance Computing Center of Hebei University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930807","Deep learning;hyperspectral image (HSI);image fusion;multispectral image (MSI);spectral–spatial attention","Feature extraction;Matrix decomposition;Image fusion;Hyperspectral imaging;Spatial resolution;Tensors;Image reconstruction","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;image sensors;remote sensing","coordinate-attention module;EPSA;high-resolution HSI;HSI-MSI fusion method;hyperspectral image fusion;multispectral image fusion;objective fusion effect;pyramid split attention module;spatial information;spatial resolution;spectral-spatial attention module;spectral-spatial attention-based U-Net;SSAU-Net;subjective fusion effect;surface feature;traditional remote sensing image","","1","","45","CCBYNCND","26 Oct 2022","","","IEEE","IEEE Journals"
"Three-Dimensional Change Detection in Urban Areas Based on Complementary Evidence Fusion","S. Tian; Y. Zhong; A. Ma; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China; Hubei Provincial Engineering Research Center of Natural Resources Remote Sensing Monitoring, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","13","With the acceleration of urbanization, it is essential to carry out change detection (CD) and obtain surface change information in urban areas. In the early stages, the spectral information of remote sensing images was used as a change index to capture the spectral and texture changes of ground objects in a two-dimensional plane. However, due to the dense buildings in urban areas, shadows and occlusions can be easily formed, and spectral information is sensitive to the imaging environment, such as the illumination, atmospheric conditions, and imaging angles, so that the detection results based on remote sensing images can often be incomplete. Most changes include not only 2-D plane changes but also 3-D elevation changes. Compared with spectral information, the elevation is more stable and more resistant to interference. Therefore, the fusion of remote sensing image and digital surface model (DSM) data has the potential to be used to detect the changes in urban areas. In this article, we propose a complementary evidence fusion 3-D CD framework based on the Dempster–Shafer theory (CDST). In this framework, DSM and normalized difference vegetation index (NDVI) data are combined using a complementary evidence combination rule. The DSM data can effectively overcome the impact of shadows, and the NDVI data can capture the relevant changes of height-insensitive ground objects, such as vegetation and water. When mapping the basic probability assignment (BPA) of the difference image (DI), prior knowledge is used to ensure that the BPA is not affected by the data distribution. Since DSM and remote sensing image data are heterogeneous data, there is a high degree of conflict when representing the change information characteristics of specific areas. For example, the change between grassland and road is small in elevation but significant in the spectral details, and the traditional Dempster’s combination rule no longer applies. The proposed CDST framework uses a complementary evidence combination rule, which can effectively alleviate the conflicts between the evidence sources and improve the integrity of the detected changes. The experimental results obtained on real datasets confirm that the proposed method does indeed perform well.","1558-0644","","10.1109/TGRS.2021.3101506","National Natural Science Foundation of China(grant numbers:41771385,41801267,42071350); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520772","3-D change detection (CD);Dempster–Shafer theory (DST);digital surface model (DSM);remote sensing","Remote sensing;Urban areas;Buildings;Feature extraction;Vegetation mapping;Spatial resolution;Solid modeling","feature extraction;geophysical image processing;image fusion;image texture;inference mechanisms;probability;remote sensing;spectral analysis;terrain mapping;uncertainty handling;vegetation mapping","urban areas;Dempster-Shafer theory;basic probability assignment;evidence fusion 3D CD framework;spectral changes;three-dimensional change detection;remote sensing image data;NDVI data;DSM data;evidence combination rule;normalized difference vegetation index data;two-dimensional plane;texture changes;change index;spectral information;surface change information","","1","","56","IEEE","23 Aug 2021","","","IEEE","IEEE Journals"
"SwinSTFM: Remote Sensing Spatiotemporal Fusion Using Swin Transformer","G. Chen; P. Jiao; Q. Hu; L. Xiao; Z. Ye","Beijing Academy of Blockchain and Edge Computing, Beijing, China; Beijing Academy of Blockchain and Edge Computing, Beijing, China; School of Environmental Science and Engineering, Southern University of Science and Technology, Shenzhen, China; College of Engineering, Peking University, Beijing, China; School of Geophysics and Information Technology, China University of Geosciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","24 Jun 2022","2022","60","","1","18","Remote sensing images with high temporal and spatial resolutions have broad market demands and various application scenarios. This article aims to generate high-quality remote sensing image time series for feature mining of the growth quality of traditional Chinese medicine. Spatiotemporal fusion is a flexible method that combines two types of satellite images with high temporal resolution or high spatial resolution to generate high-quality remote sensing images. In recent years, many spatiotemporal fusion algorithms have been proposed, and deep learning-based methods show extraordinary talents in this field. However, the current deep learning-based methods have three problems: 1) most algorithms do not support models with large-scale learnable parameters; 2) the model structure based on convolutional neural networks will bring the noise to the image fusion process; and 3) current deep learning-based methods ignore some excellent modules in traditional spatiotemporal fusion algorithms. For the above problems and challenges, this article creatively proposes a new algorithm based on the Swin transformer and the linear spectral mixing theory. The algorithm makes full use of the advantages of the Swin transformer in feature extraction and integrates the unmixing theories into the model based on the self-attention mechanism, which greatly improves the quality of generated images. In the experimental part, the proposed algorithm achieves state-of-the-art results on three well-known public datasets and has been proven effective and reasonable in ablation studies.","1558-0644","","10.1109/TGRS.2022.3182809","National Key Research and Development Program of China(grant numbers:2020YFC1712701); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795183","Deep learning;remote sensing;spatiotemporal fusion;Swin transformer;unmixing","Spatiotemporal phenomena;Feature extraction;Remote sensing;Spatial resolution;Transformers;Learning systems;Satellites","convolutional neural nets;data mining;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image resolution;medicine;remote sensing;time series","high quality remote sensing images;current deep learning-based methods;image fusion process;traditional spatiotemporal fusion algorithms;Swin transformer;remote sensing spatiotemporal fusion;high temporal resolutions;spatial resolutions;broad market demands;high-quality remote sensing image time series;traditional Chinese medicine;satellite images;high temporal resolution;high spatial resolution","","2","","50","IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"Remote Sensing Image Vehicle Detection Based on Pre-Training and Random-Initialized Fusion Network","H. Liu; Q. Ding; Z. Hu; X. Chen","School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2021","2022","19","","1","5","Vehicle detection based on remote sensing images is of great significance to intelligent traffic management and smart city planning. The performances of the mainstream vehicle detection methods such as Fast-RCNN, SSD, FCRN, etc. are mainly affected by their extracted features, whose richness and robustness can be enhanced by introducing pre-training models from other datasets. However, the weights of the pre-training models used in traditional methods like FCRN, only shallow-tuned from the weights of another database training, without deep optimization from random initialization. This letter proposes a new pre-training and random-initialized fusion network (PRFN): the random initialization model and the pre-training model are connected in parallel, and an encoder is used to fuse their features, thereby improving the richness of features and the robustness of the model. We evaluate four different combination patterns of the pre-training and random initialized models, and the effect of the fusion feature encoder. Experiments show the parallel concatenating pattern with the fusion feature encoder achieves the best performance than other patterns. We test on three datasets, our PRFN exceeds the traditional methods, and, respectively, 1.23%, 1.04%, 0.94% higher than the baseline. All codes are available from https://github.com/LHKRobert/PRFN.","1558-0571","","10.1109/LGRS.2021.3109637","National Natural Science Foundation of China(grant numbers:62061002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537168","Fusion feature encoder;pre-training model;spatial density map;vehicle detection","Feature extraction;Vehicle detection;Decoding;Training;Remote sensing;Task analysis;Mathematical model","feature extraction;geophysical image processing;image colour analysis;image fusion;neural nets;object detection;remote sensing;road traffic;road vehicles;traffic engineering computing","mainstream vehicle detection methods;pre-training model;database training;random-initialized fusion network;random initialization model;random initialized models;fusion feature encoder;sensing image vehicle detection;remote sensing images","","3","","20","IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"Simultaneous Road Surface and Centerline Extraction From Large-Scale Remote Sensing Images Using CNN-Based Segmentation and Tracing","Y. Wei; K. Zhang; S. Ji","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; ENSTA ParisTech, Palaiseau, France; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","24 Nov 2020","2020","58","12","8919","8931","Accurate and up-to-date road maps are of great importance in a wide range of applications. Unfortunately, automatic road extraction from high-resolution remote sensing images remains challenging due to the occlusion of trees and buildings, discriminability of roads, and complex backgrounds. To address these problems, especially road connectivity and completeness, in this article, we introduce a novel deep learning-based multistage framework to accurately extract the road surface and road centerline simultaneously. Our framework consists of three steps: boosting segmentation, multiple starting points tracing, and fusion. The initial road surface segmentation is achieved with a fully convolutional network (FCN), after which another lighter FCN is applied several times to boost the accuracy and connectivity of the initial segmentation. In the multiple starting points tracing step, the starting points are automatically generated by extracting the road intersections of the segmentation results, which then are utilized to track consecutive and complete road networks through an iterative search strategy embedded in a convolutional neural network (CNN). The fusion step aggregates the semantic and topological information of road networks by combining the segmentation and tracing results to produce the final and refined road segmentation and centerline maps. We evaluated our method utilizing three data sets covering various road situations in more than 40 cities around the world. The results demonstrate the superior performance of our proposed framework. Specifically, our method's performance exceeded the other methods by 7% and 40% for the connectivity indicator for road surface segmentation and for the completeness indicator for centerline extraction, respectively.","1558-0644","","10.1109/TGRS.2020.2991733","National Key Research and Development Program of China(grant numbers:2018YFB0505003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9094008","Convolutional neural network (CNN);remote sensing images;road extraction;segmentation;tracing","Roads;Image segmentation;Remote sensing;Boosting;Feature extraction;Surface topography;Semantics","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image resolution;image segmentation;learning (artificial intelligence);object detection;remote sensing;roads","CNN based segmentation;remote sensing image resolution;road connectivity;road centerline extraction;road surface segmentation;road intersections;convolutional neural network;fusion step;centerline maps;road surface extraction;multiple starting points tracing;deep learning;boosting segmentation;road segmentation maps","","44","","50","IEEE","14 May 2020","","","IEEE","IEEE Journals"
"Dual-Collaborative Fusion Model for Multispectral and Panchromatic Image Fusion","Y. Xing; S. Yang; Z. Feng; L. Jiao","School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","2 Dec 2021","2022","60","","1","15","The aim of multispectral (MS) and panchromatic (PAN) image fusion is to obtain an MS image that has high resolution in both spectral and spatial domains. During the fusion process, there are two important issues, i.e., spectral information preservation and spatial information enhancement. In this article, we propose a dual-collaborative fusion model that considers not only the spectral correlation collaboration but also the spatial-spectral collaboration. First, the features of PAN and MS images are extracted by a shared feature embedding network. Then, in order to enhance the spatial details, the PAN features are decomposed into four subbands, and the collaborative relationships among subbands are fully explored to refine the features. After the refinement of the subbands, the high-frequency components are directly taken as the inputs of the reconstruction network, while the low-frequency components are transformed by the guidance generation network to accomplish the spatial-spectral collaboration and also make preparations for the spectral adjustment. To explore the spectral correlation collaboration, a novel graph convolutional network is designed for the modulation of intraspectral relationships. Finally, the adjusted MS features are combined with the high-frequency components of PAN features to reconstruct the high-resolution MS image. Experimental results show that the proposed method outperforms traditional state-of-the-art pan-sharpening methods as well as the available deep learning-based ones.","1558-0644","","10.1109/TGRS.2020.3036625","National Natural Science Foundation of China(grant numbers:61771380,U1701267,61906145,U1730109,91438103); Major Research Plan in Shaanxi Province of China(grant numbers:2017ZDXM-GY-103,2017ZDCXL-GY-03-02l); Science Basis Research Program in Shaanxi Province of China(grant numbers:16JK1823,2017JM6086,2019JQ-663); China Postdoctoral Science Foundation(grant numbers:2018M643587); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9310712","Deep learning (DL);dual-collaboration;image fusion;pan-sharpening;remote sensing","Feature extraction;Image fusion;Task analysis;Collaboration;Image reconstruction;Remote sensing;Spatial resolution","geophysical image processing;geophysical signal processing;image fusion;image resolution;learning (artificial intelligence);remote sensing","shared feature embedding network;spatial details;PAN features;subbands;collaborative relationships;high-frequency components;spatial-spectral collaboration;spectral adjustment;spectral correlation collaboration;adjusted MS features;high-resolution MS image;traditional state-of-the-art pan-sharpening methods;dual-collaborative fusion model;panchromatic image;spectral domains;spatial domains;fusion process;spectral information preservation;spatial information enhancement","","7","","52","IEEE","29 Dec 2020","","","IEEE","IEEE Journals"
"Spatial and Temporal Image Fusion via Regularized Spatial Unmixing","Y. Xu; B. Huang; Y. Xu; K. Cao; C. Guo; D. Meng","Department of Geography and Resource Management, Chinese University of Hong Kong, Shatin, Hong Kong; Department of Geography and Resource Management, Chinese University of Hong Kong, Shatin, Hong Kong; School of Geographic and Oceanographic Sciences, Nanjing University, Nanjing, China; Department of Geography, National University of Singapore, Singapore; Department of Geography and Resource Management, Chinese University of Hong Kong, Shatin, Hong Kong; School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2015","12","6","1362","1366","A novel spatial and temporal data fusion model based on regularized spatial unmixing was developed to generate Landsat-like synthetic data with the fine spatial resolution of Landsat Enhanced Thematic Mapper Plus (Landsat ETM $+$) data and the high temporal resolution of Moderate Resolution Imaging Spectroradiometer (MODIS) data. The proposed approach is based on the conventional spatial unmixing technique, but modified to include prior class spectra, which are estimated from pairs of MODIS and Landsat data using the spatial and temporal adaptive reflectance data fusion model. The method requires the optimization of the following three parameters: the number of classes of Landsat data, the neighborhood size of the MODIS data for spatial unmixing, and a regularization parameter added to the cost function to reduce unmixing error. Indexes of relative dimensionless global error in synthesis (ERGAS) were used to determine the best combination of the three parameters by evaluating the quality of the fused result at both Landsat and MODIS spatial resolutions. The experimental results with observed satellite data showed that the proposed approach performs better than conventional unmixing-based fusion approaches with the same parameters.","1558-0571","","10.1109/LGRS.2015.2402644","Hong Kong Research Grant Council(grant numbers:CUHK 444612); National Natural Science Foundation of China(grant numbers:60171009,41301447); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7052314","Image fusion;Landsat;Moderate Resolution Imaging Spectroradiometer (MODIS);spatial and temporal adaptive reflectance data fusion model (STARFM);spatial unmixing;Image fusion;Landsat;Moderate Resolution Imaging Spectroradiometer (MODIS);spatial and temporal adaptive reflectance data fusion model (STARFM);spatial unmixing","Remote sensing;Satellites;MODIS;Earth;Spatial resolution;Indexes","artificial satellites;geophysical image processing;image fusion;image resolution;radiometry;spatiotemporal phenomena","regularized spatial unmixing;Landsat-like synthetic data generation;spatial resolution;Landsat Enhanced Thematic Mapper Plus;Landsat ETM+;temporal resolution;moderate resolution imaging spectroradiometer;MODIS;conventional spatial unmixing technique;spatial adaptive reflectance data fusion model;temporal adaptive reflectance data fusion model;optimization;regularization parameter;cost function;unmixing error reduction;relative dimensionless global error in synthesis;ERGAS","","34","","14","IEEE","2 Mar 2015","","","IEEE","IEEE Journals"
"Band-Independent Encoder–Decoder Network for Pan-Sharpening of Remote Sensing Images","C. Liu; Y. Zhang; S. Wang; M. Sun; Y. Ou; Y. Wan; X. Liu","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Beijing Institute of Space Mechanics and Electricity, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2020","2020","58","7","5208","5223","Pan-sharpening is a fundamental task for remote sensing image processing. It aims at creating a high-resolution multispectral (HRMS) image from a multispectral (MS) image and a panchromatic (PAN) image. In this article, a new band-independent encoder-decoder network is proposed for pan-sharpening. The network takes a single band of the MS (BMS) image, the PAN image, and the low-resolution PAN (LRPAN) image as inputs. The output of the network is the corresponding band of high-resolution MS (HRBMS) image. In this way, the network can process MS images with any number of bands. The overall structure of the network consists of two encoder-decoder modules at low-resolution and high-resolution, respectively. An auxiliary LRPAN image is used to speed up the training and improve the performance. The partly shared network and hierarchical structure for low-resolution and high-resolution enable a better fusion of features extracted from different scales. With a fast fine-tuning strategy, the trained model can be applied to images from different sensors. Experiments performed on different data sets demonstrate that the proposed method outperforms several state-of-the-art pan-sharpening methods in both visual appearance and objective indexes, and the single-band evaluation results further verify the superiority of the proposed method.","1558-0644","","10.1109/TGRS.2020.2975230","National Basic Research Program of China (973 Program)(grant numbers:2018YFB0505003); National Natural Science Foundation of China(grant numbers:41871368,41801386); National High-Score Major Special Projects(grant numbers:50-H31D01-0508-13/15); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180797); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9013047","Band-independent;deep learning;encoder–decoder;pan-sharpening","Decoding;Image resolution;Task analysis;Remote sensing;Feature extraction;Sensors;Transforms","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","feature extraction fusion;fast fine-tuning strategy;objective indexes;visual appearance;hierarchical structure;HRBMS image;HRMS image;single-band evaluation;pan-sharpening methods;partly shared network;auxiliary LRPAN image;encoder-decoder modules;high-resolution MS image;low-resolution PAN image;panchromatic image;high-resolution multispectral image;remote sensing image processing;band-independent encoder-decoder network","","10","","75","IEEE","26 Feb 2020","","","IEEE","IEEE Journals"
"Spatiotemporal Remote-Sensing Image Fusion With Patch-Group Compressed Sensing","L. Li; P. Liu; J. Wu; L. Wang; G. He","College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Aerospace Information Research Institute, Beijing, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; School of Computer Science, China University of Geosciences, Wuhan, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China","IEEE Access","26 Nov 2020","2020","8","","209199","209211","Generally, it is difficult to acquire remote sensing data whose resolution is both highly spatial and highly temporal from a single satellite. In this paper, a novel compressed sensing (CS)-based spatiotemporal data fusion (CSBS) method is proposed to synthesize such high-spatiotemporal resolution images. With CSBS, a low-spatial resolution remote senisng image is treated as a sampling of the high-spatial resolution image. The down-sampling in the spatial domain of images is modeled as a CS measurement matrix in CSBS. Moreover, continuity constraints in the temporal domain are also introduced into the CSBS object function for CS reconstruction. To better represent the intrinsic features of the data, images are segmented into many small patches and clustered into several groups via K-means. Dictionary training, measurement matrix identification, and high-resolution prediction are carried out group-by-group. Based on features learned from patch groups, the transformational relationship between spatial-temporal images having different resolutions are easily identified. Compared with previous compressed sensing and dictionary learning methods, CSBS is characterized by: (1) the patch-group stratagem in dictionary learning and measurement matrix learning; (2) the combination of continuity in temporal domain and sparsity in spatial domain. The proposed method is then comprehensively compared with different methods using land-surface reflectance data. Experiment results validate the effectiveness and advancement of CSBS for spatiotemporal data fusion.","2169-3536","","10.1109/ACCESS.2020.3011258","National Natural Science Foundation of China(grant numbers:61731022,41971397,41701468,U1711266); Institute of Remote Sensing and Digital Earth (RADI), Chinese Academy of Sciences, Director Youth Funding(grant numbers:Y6XS5900CX,Y5ZZ08101B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203884","Compressed sensing;landsat;land-surface reflectance;moderate resolution imaging spectroradiometer (MODIS);remote sensing;spatiotemporal data fusion","Spatiotemporal phenomena;Spatial resolution;Compressed sensing;Data integration;Remote sensing;Earth","compressed sensing;geophysical image processing;image fusion;image processing;image reconstruction;image resolution;learning (artificial intelligence);remote sensing;sensor fusion;spatiotemporal phenomena","group-by-group;spatial-temporal images;previous compressed sensing;patch-group stratagem;dictionary learning;measurement matrix learning;temporal domain;spatial domain;land-surface reflectance data;spatiotemporal remote-sensing image fusion;patch-group compressed sensing;remote sensing data;novel compressed sensing-based spatiotemporal data fusion method;high-spatiotemporal resolution images;low-spatial resolution remote senisng image;CS measurement matrix;CSBS object function;CS reconstruction;measurement matrix identification;high-resolution prediction","","4","","46","CCBYNCND","22 Sep 2020","","","IEEE","IEEE Journals"
"Texture-Aware Deblurring for Remote Sensing Images Using $ \ell _0$-Based Deblurring and $ \ell _2$-Based Fusion","H. Lim; S. Yu; K. Park; D. Seo; J. Paik","Department of Image, Chung-Ang University, Seoul, South Korea; Department of Image, Chung-Ang University, Seoul, South Korea; Autonomous Driving Lab, PLK Technology, 314, Seongnam-si, South Korea; Department of Satellite Data Cal/Val Team, Korea Aerospace Research Institute, Daejeon, South Korea; Department of Image, Chung-Ang University, Seoul, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Jun 2020","2020","13","","3094","3108","This article presents an image deblurring method using ℓ0-norm-based deblurring and ℓ2-norm-based texture-aware image fusion for remote sensing images. To restore the details of blurred texture, the proposed method first performs texture restoration by fusing the restored results using Richardson-Lucy deconvolution and unsharp masking. Next, we analyzed the intensity and dark channel properties of remote sensing images and perform the ℓ0-norm-based deblurring using the intensity and dark channel priors. Although the ℓ0-norm-based deblurring can provide a significantly restored result, it cannot overcome the loss of the texture region. On the other hand, the proposed ℓ2-norm-based image fusion method can preserve both sharp edges and texture details. In the experiments, we demonstrate that the proposed method can provide better restored results than existing state-of-the-art deblurring methods without oversmoothing and undesired artifact.","2151-1535","","10.1109/JSTARS.2020.2999961","Korea Aerospace Research Institute(grant numbers:NRF-2017M1A3A4A07028434); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9108529","Deblur;remote sensing;image restoration","Image restoration;Remote sensing;Deconvolution;Image edge detection;Image fusion;Histograms;Earth","deconvolution;image fusion;image restoration;image texture;remote sensing","texture-aware deblurring;remote sensing images;image deblurring method;norm-based deblurring;norm-based texture-aware image fusion;blurred texture;texture restoration;texture region;sharp edges;texture details;ℓ0-based deblurring;ℓ2-based fusion","","7","","38","CCBY","4 Jun 2020","","","IEEE","IEEE Journals"
"Fusion-Based Correlation Learning Model for Cross-Modal Remote Sensing Image Retrieval","Y. Lv; W. Xiong; X. Zhang; Y. Cui","Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China; Research Institute of Information Fusion, Naval Aviation University, Yantai, China","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","With the increasing of cross-modal data, cross-modal retrieval has attracted more attention in remote sensing (RS), since it provides a more flexible and convenient way to obtain interesting information than traditional retrieval. However, existing methods cannot fully exploit the semantic information, which only focuses on the semantic consistency, and ignore the information complementarity between different modalities. In this letter, to bridge the modality gap, we propose a novel fusion-based correlation learning model (FCLM) for image-text retrieval in RS. Specifically, a cross-modal-fusion network is designed to capture the intermodality complementary information and fused feature. The fused knowledge is furtherly transferred to supervise the learning of modality-specific network by knowledge distillation, which is helpful in improving the discriminative ability of feature representation and enhancing the intermodality semantic consistency to solve the heterogeneity gap problem. Finally, extensive experiments have been conducted on a public dataset and experimental results have shown that the FCLM method is effective in performing cross-modal retrieval and outperforms several baseline methods.","1558-0571","","10.1109/LGRS.2021.3131592","National Natural Science Foundation of China(grant numbers:61790550,61790554,91538201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628066","Correlation learning;cross-modal retrieval;multimodal fusion;text-remote sensing (RS) image matching","Feature extraction;Semantics;Image retrieval;Fuses;Correlation;Buildings;Representation learning","feature extraction;geophysical image processing;image fusion;image representation;image retrieval;information retrieval;learning (artificial intelligence);remote sensing;sensor fusion","image-text retrieval;cross-modal-fusion network;intermodality complementary information;modality-specific network;intermodality semantic consistency;cross-modal retrieval;cross-modal remote sensing image retrieval;cross-modal data;traditional retrieval;semantic information;information complementarity;modality gap;fusion-based correlation learning model","","3","","14","IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"Scale Effect on Fusing Remote Sensing and Human Sensing to Portray Urban Functions","W. Tu; Y. Zhang; Q. Li; K. Mai; J. Cao","Department of Urban Informatics, School of Architecture and Urban Planning, Research Institute of Smart Cities, Shenzhen University, Shenzhen, China; State Key Laboratory of Information Engineering of Survey, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering of Survey, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Department of Urban Informatics, School of Architecture and Urban Planning, Research Institute of Smart Cities, Shenzhen University, Shenzhen, China; Department of Urban Informatics, School of Architecture and Urban Planning, Research Institute of Smart Cities, Shenzhen University, Shenzhen, China","IEEE Geoscience and Remote Sensing Letters","23 Dec 2020","2021","18","1","38","42","The development of information and communication technologies has produced massive human sensing data sets, such as point of interest, mobile phone data, and social media data sets. These data sets provide alternative human perceptions of urban spaces; therefore, they have become effective supplements for remote sensing tasks. This letter presents an exploratory framework to examine the scale effect of fusing remote sensing and human sensing. The physical and social semantics are extracted from raw remote sensing images and human sensing data, respectively. A dynamic weighting strategy is developed to explore the fusion of remote sensing and human sensing. Taking urban function inference as an example, the scale effect is evaluated by weighting remote sensing and human sensing. The experiment demonstrates that fusing remote sensing and human sensing enables us to recognize multiple types of urban functions. Meanwhile, the results are significantly affected by the scale.","1558-0571","","10.1109/LGRS.2020.2965247","National Natural Science Foundation of China(grant numbers:71961137003); Natural Science Foundation of Guangdong Province(grant numbers:2019A1515011049); Basic Research Program of Shenzhen Science and Technology Innovation Committee(grant numbers:JCJY201803053125113883,JCYJ20170412105839839); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149681","Data fusion;human sensing;scale;urban functions","Remote sensing;Sensors;Semantics;Feature extraction;Image segmentation;Visualization;Histograms","geophysical image processing;image fusion;remote sensing;social networking (online);town and country planning","remote sensing;social media data sets;alternative human perceptions;remote sensing tasks;remote sensing images;human sensing data sets;information and communication technologies;dynamic weighting strategy","","9","","25","IEEE","27 Jul 2020","","","IEEE","IEEE Journals"
"Dual-Pathway Change Detection Network Based on the Adaptive Fusion Module","X. Jiang; S. Xiang; M. Wang; P. Tang","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Department of informatics, Technical University of Munich, Munich, Germany","IEEE Geoscience and Remote Sensing Letters","31 Dec 2021","2022","19","","1","5","In recent years, with the development of high-resolution remote sensing (RS) images and deep learning technology, high-quality source data and state-of-the-art methods have become increasingly available, and great progress has been made in change detection (CD) in RS fields. However, existing methods still suffer from weak network feature representation and poor CD performance. To address these problems, we propose a novel CD network, called dual-pathway CD network (DP-CD-Net), which can help enhance feature representation and achieve a more accurate difference map. The proposed method contains a dual-pathway feature difference network (FDN), an adaptive fusion module (AFM), and an auxiliary supervision strategy. Dual-pathway FDNs can effectively enhance feature representation by supplementing the detailed information from the encoding layers. Then, we use the AFM method to fuse the difference maps. To solve the problem of training difficulty, we use the auxiliary supervision strategy to improve the performance of DP-CD-Net. We conduct extensive experiments to validate the performance of the proposed method on the LEVIR-CD dataset. The results demonstrate that the proposed method performs better than existing methods.","1558-0571","","10.1109/LGRS.2021.3103991","National Natural Science Foundation of China(grant numbers:61825103,91838303,91738302); Key Research and Development Plan Project of Hubei Province(grant numbers:2020BIB006); Key Project of Hubei Provincial Natural Science Foundation(grant numbers:2020CFA001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527334","Adaptive fusion;auxiliary supervision;change detection (CD);deep learning;dual-pathway feature difference networks (FDNs)","Feature extraction;Decoding;Training;Encoding;Indexes;Fuses;Remote sensing","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","dual-pathway change detection network;adaptive fusion module;high-resolution remote sensing images;deep learning technology;high-quality source data;RS fields;weak network feature representation;poor CD performance;called dual-pathway CD network;DP-CD-Net;accurate difference map;dual-pathway feature difference network;auxiliary supervision strategy;dual-pathway FDNs;AFM method;difference maps;LEVIR-CD dataset","","","","15","IEEE","2 Sep 2021","","","IEEE","IEEE Journals"
"Joint Image Registration and Fusion for Panchromatic and Multispectral Images","Q. Zhang; Z. Cao; Z. Hu; Y. Jia; X. Wu","National Key Laboratory of Science and Technology on Multi-spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; Shenzhen Key Laboratory of Spatial Smart Sensing and Services, Mapping and GeoInformation, College of Civil Engineering, Shenzhen University, Shenzhen, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Division of Mathematics, Informatics and Statistics, Commonwealth Scientific and Industrial Research Organization, North Ryde, N.S.W., Australia","IEEE Geoscience and Remote Sensing Letters","16 Sep 2014","2015","12","3","467","471","Both image registration and fusion are essential steps to produce high-resolution multispectral images in remote sensing. Traditionally, they are viewed as two independent processes. As a result, the registration errors ignored in the fusion process can significantly affect the fusion quality. In this context, an iterative optimization approach, which jointly considers the registration and fusion processes, is proposed for panchromatic (PAN) and multispectral (MS) images. Given a registration method and a fusion method, the joint optimization process is described as finding the optimal registration parameters to gain the optimal fusion performance. In our approach, the downhill simplex algorithm is adopted to refine the registration parameters iteratively. Experiments on a set of PAN and MS images of ZY-3 and GeoEye-1 show that the proposed approach outperforms several competing ones in terms of registration accuracy and fusion quality.","1558-0571","","10.1109/LGRS.2014.2346398","National Natural Science Foundation of China(grant numbers:41001260); Fundamental Research Funds for the Central Universities(grant numbers:201121302020003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6880780","Image fusion;image fusion quality;image registration;iterative optimization;registration accuracy;Image fusion;image fusion quality;image registration;iterative optimization;registration accuracy","Joints;Optimization;Image registration;Remote sensing;Accuracy;Indexes;Linear programming","geophysical image processing;hyperspectral imaging;image fusion;image registration;iterative methods;optimisation;remote sensing","image registration;panchromatic images;high resolution multispectral images;remote sensing;registration errors;image fusion quality;iterative optimization approach;registration method;fusion method;optimal registration parameters;optimal fusion performance;ZY-3 images;GeoEye-1 images","","32","","22","IEEE","20 Aug 2014","","","IEEE","IEEE Journals"
"NBR-Net: A Nonrigid Bidirectional Registration Network for Multitemporal Remote Sensing Images","Y. Xu; J. Li; C. Du; H. Chen","College of Electronic Science and Technology, National University of Defense Technology, Hunan, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Hunan, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Hunan, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Hunan, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","14 Apr 2022","2022","60","","1","15","Remote sensing image registration is the basis of change detection, environmental monitoring, and image fusion. Under severe appearance differences, feature-based methods have difficulty in finding sufficient feature matches to solve the global transformation and tackling the local deformation caused by height undulations and building shadows. By contrast, nonrigid registration methods are more flexible than feature-based matching methods, while often ignoring the reversibility between images, resulting in misalignment and inconsistency. To this end, this article proposes a nonrigid bidirectional registration network (NBR-Net) to estimate the flow-based dense correspondence for remote sensing images. We first propose an external cyclic registration network to strengthen the registration reversibility and geometric consistency by registering Image A to Image B and then reversely registering back to Image A. Second, we design an internal iterative refinement strategy to optimize the rough predicted flow caused by large distortion and viewpoint difference. Extensive experiments demonstrate that our method shows a performance superior to the state-of-the-art models on the multitemporal satellite image dataset. Furthermore, we attempt to extend our method to heterogeneous remote sensing image registration, which is more common in the real world. Therefore, we test our pretrained model in a satellite and unmanned aerial vehicle (UAV) image registration task. Due to the cyclic registration mechanism and coarse-to-fine refinement strategy, the proposed approach obtains the best performance on two GPS-denied UAV image datasets. Our code will be released at https://github.com/xuyingxiao/ NBR-Net.","1558-0644","","10.1109/TGRS.2022.3162094","National Natural Science Foundation of China(grant numbers:U19A2058,61806211,41971362); Natural Science Foundation of Hunan Province China(grant numbers:2020JJ4103); Hunan Provincial Innovation Foundation For Postgraduate(grant numbers:CX20210005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740686","Cycle consistency;iterative refinement;nonrigid;remote sensing image registration;reversibility;unmanned aerial vehicle (UAV) image registration","Remote sensing;Image registration;Satellites;Bidirectional control;Sensors;Strain;Feature extraction","autonomous aerial vehicles;geophysical image processing;image fusion;image matching;image registration;iterative methods;remote sensing","nonrigid bidirectional registration network;multitemporal remote sensing images;image fusion;height undulations;feature-based matching methods;flow-based dense correspondence;external cyclic registration network;multitemporal satellite image dataset;heterogeneous remote sensing image registration;unmanned aerial vehicle image registration task;GPS-denied UAV image datasets;internal iterative refinement strategy","","2","","59","IEEE","24 Mar 2022","","","IEEE","IEEE Journals"
"A Triple-Stream Network With Cross-Stage Feature Fusion for High-Resolution Image Change Detection","Y. Zhao; P. Chen; Z. Chen; Y. Bai; Z. Zhao; X. Yang","College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Airborne Remote Sensing Center, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Airborne Remote Sensing Center, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; China Urban Development Planning and Design Consulting Company Ltd., Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","11 Jan 2023","2023","61","","1","17","Change detection (CD) based on high-resolution remote sensing images can be used to monitor land cover changes, which is an important and challenging topic in the remote sensing field. In recent years, with the development of deep learning, CD methods based on deep learning have achieved good results in the field of CD. However, most current CD methods use single- or dual-stream networks to extract change features, which is insufficient to extract and learn bitemporal change information thoroughly. This article proposes a triple-stream network (TSNet) with cross-stage feature fusion for CD in high-resolution bitemporal remote sensing images. First, to obtain highly representative deep features in the original image, we perform feature extraction on bitemporal remote sensing images and their concatenated image with a dual-stream encoder and a single-stream encoder, respectively. Then, the bitemporal multiscale features extracted by the dual-stream encoder are input into a multistage bidirectional convolutional gated recurrent unit (MSBC_GRU) feature fusion module, allowing the network to learn the change information in a cross-stage manner. In addition, we use a dual-channel attention module to fuse the features extracted by dual- and single-stream encoders, improving the network’s ability to discriminate changed features. The effectiveness of TSNet is demonstrated with three publicly available CD datasets. The extensive experimental results demonstrate that the proposed method achieves the state-of-the-art CD performance on the above three datasets.","1558-0644","","10.1109/TGRS.2022.3233849","National Natural Science Foundation of China(grant numbers:42071407); China High-Resolution Earth Observation System(grant numbers:03-Y30F03-9001-20/22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005110","Attention mechanism;change detection (CD);high-resolution images;recurrent neural network (RNN);remote sensing","Feature extraction;Remote sensing;Data mining;Classification algorithms;Task analysis;Change detection algorithms;Clustering algorithms","feature extraction;geophysical image processing;image classification;image fusion;image representation;image resolution;land cover;learning (artificial intelligence);remote sensing","bitemporal change information;bitemporal multiscale features;change features;changed features;concatenated image;cross-stage feature fusion;cross-stage manner;current CD methods;deep learning;dual-channel attention module;dual-stream encoder;dual-stream networks;feature extraction;fusion module;high-resolution bitemporal remote sensing images;high-resolution image change detection;high-resolution remote sensing images;highly representative deep features;important topic;land cover changes;multistage bidirectional convolutional gated recurrent unit;publicly available CD datasets;remote sensing field;single-stream encoder;state-of-the-art CD performance;triple-stream network","","","","90","IEEE","3 Jan 2023","","","IEEE","IEEE Journals"
"Antinoise Hyperspectral Image Fusion by Mining Tensor Low-Multilinear-Rank and Variational Properties","J. Li; X. Liu; Q. Yuan; H. Shen; L. Zhang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Resource and Environmental Science, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","25 Sep 2019","2019","57","10","7832","7848","Enhancing the spatial resolution of hyperspectral (HS) images by fusing with higher spatial resolution multispectral (MS) data is of significance for applications. However, due to the narrow bandwidth, HS images (HSIs) are vulnerable to various types of noise, such as Gaussian noise and stripes, which can severely affect the fusion performance. This paper focuses on antinoise HS and MS image fusion to enhance the spatial details and suppress the noise. By analysis of the intrinsic structure and noise properties, we formulate this problem as the minimization of an objective function. Under the optimization framework, small multilinear ranks in tensor are first used to identify the intrinsic structures of the clean HSI part. Then, considering the high spectral correlation, it is assumed that any bands can be represented by the combination of certain adjacent bands. The difference between one band and its corresponding combination can be used to preserve the spatio-spectral consistency and characterize the distribution of sparse noise (such as stripe noise), based on the variational properties along two directions. The alternating direction method of multipliers (ADMM) is applied to solve and accelerate the model optimization. Experiments with both simulated- and real-data demonstrate the effectiveness of the proposed model and its robustness to the noise, in terms of both qualitative and quantitative perspectives.","1558-0644","","10.1109/TGRS.2019.2916654","National Natural Science Foundation of China(grant numbers:41701400); Natural Science Foundation of Hubei Province(grant numbers:ZRMS2017000729); China Postdoctoral Science Foundation(grant numbers:2018T110803); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0213,531118010209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730486","Hyperspectral (HS) image;image fusion;multispectral (MS) image;variational optimization","Spatial resolution;Correlation;Sparse matrices;Image fusion;Gaussian noise;Optimization","Gaussian noise;geophysical image processing;hyperspectral imaging;image fusion;image resolution;optimisation;remote sensing;tensors","antinoise hyperspectral image fusion;mining tensor low-multilinear-rank;variational properties;hyperspectral images;higher spatial resolution multispectral data;narrow bandwidth;HS images;Gaussian noise;fusion performance;spatial details;intrinsic structure;optimization framework;multilinear ranks;clean HSI part;high spectral correlation;adjacent bands;spatio-spectral consistency;sparse noise;stripe noise","","11","","65","IEEE","4 Jun 2019","","","IEEE","IEEE Journals"
"DFAF-Net: A Dual-Frequency PolSAR Image Classification Network Based on Frequency-Aware Attention and Adaptive Feature Fusion","Y. Cao; Y. Wu; M. Li; W. Liang; X. Hu","Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","31 Mar 2022","2022","60","","1","18","Multifrequency (MF) polarization synthetic aperture radar (PolSAR) systems can obtain more abundant and continuous earth resource information than single-frequency ones and have been widely used in the remote sensing community. However, there are relatively a few researches for the fine MF PolSAR image classification, which is an important part of remote sensing image interpretation. The main focus currently is on the single-frequency part. Therefore, for dual-frequency PolSAR image classification, this article proposes the dual-frequency attention fusion network (DFAF-Net). It is based on frequency-aware attention and adaptive feature fusion to improve classification performance. First, the dual-frequency PolSAR data is input into the joint feature extraction (JFE) module to obtain the joint feature representation. Meanwhile, two frequency-aware attention block (FAB) modules with the same structure are constructed, which, respectively, generate frequency-specific attention masks based on the guidance of different frequency data. Subsequently, these masks are used to weigh the joint features to highlight the description of different frequency-aware importance. The activated frequency-aware features can fully mine and utilize the complementary information provided by different frequencies, thereby enhancing the discrimination of similar landcover categories. Finally, the adaptive feature fusion block (AFFB) module is utilized to adaptively aggregate different frequency-aware features multiple times, which can effectively eliminate information differences. The obtained fusion features are more compact within classes and separable between classes, thereby effectively improving the classification performance. Experiments on three measured spaceborne and airborne dual-frequency PolSAR datasets verify that DFAF-Net can better perceive frequency characteristics and fully mine the complementary. Therefore, the classification accuracy is effectively enhanced, and the inaccuracy of single-frequency classification can be eliminated. Meanwhile, due to the introduction of the attention module, the classification performance of the proposed DFAF-Net is more competitive than the related deep learning networks. Quantitatively, the overall accuracy of DFAF-Net on the three datasets is respectively 98.30%, 97.42%, and 99.45%, which is better than other methods.","1558-0644","","10.1109/TGRS.2022.3152854","Natural Science Foundation of China(grant numbers:62172321,61871312); Civil Space Thirteen Five Years Pre-Research Project(grant numbers:D040114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9717232","Adaptive feature fusion;complementary information;dual-frequency polarization synthetic aperture radar (PolSAR) image classification;frequency-aware attention","Feature extraction;Remote sensing;Frequency measurement;Radar polarimetry;Data mining;Visualization;Spaceborne radar","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);radar imaging;radar polarimetry;remote sensing;remote sensing by radar;synthetic aperture radar","classification performance;dual-frequency PolSAR data;joint feature extraction module;joint feature representation;frequency-aware attention block modules;frequency-specific attention masks;different frequency data;joint features;different frequency-aware importance;activated frequency-aware features;adaptive feature fusion block module;different frequency-aware features;information differences;fusion features;dual-frequency PolSAR datasets;DFAF-Net;frequency characteristics;classification accuracy;single-frequency classification;attention module;dual-frequency PolSAR image classification network;multifrequency polarization synthetic aperture radar systems;abundant earth resource information;continuous earth resource information;single-frequency ones;remote sensing community;fine MF PolSAR image classification;remote sensing image interpretation;single-frequency part;dual-frequency attention fusion network","","3","","40","IEEE","18 Feb 2022","","","IEEE","IEEE Journals"
"Adversarial Learning for Knowledge Adaptation From Multiple Remote Sensing Sources","M. M. Al Rahhal; Y. Bazi; H. Al-Hwiti; H. Alhichri; N. Alajlan","Department of Information System, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia","IEEE Geoscience and Remote Sensing Letters","21 Jul 2021","2021","18","8","1451","1455","In this work, we introduce a neural architecture to unsupervised domain from multiple source domains. This architecture uses an EfficientNet as a feature extractor coupled with a set of Softmax classifiers equal to the number of source domains followed by an opportune fusion layer. To reduce the domain discrepancy between each source and target domain, we adopt a Minmax entropy approach that is based on the idea of optimizing in an adversarial manner the conditional entropy of the target samples with respect to each source classifier and minimizes it with respect to the feature extractor. As for the fusion module, we propose a weighted average fusion layer with learnable weights for aggregating the outputs of the different Softmax classifiers. Experiments on a multisource data set composed of images acquired by manned and unmanned aerial vehicles (MAVs/UAVs) over different locations are reported and discussed.","1558-0571","","10.1109/LGRS.2020.3003566","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9129743","Adversarial learning;manned and unmanned aerial vehicles (MAVs/UAVs);Minmax entropy;multiple sources;scene classification","Feature extraction;Entropy;Prototypes;Unmanned aerial vehicles;Optimization;Remote sensing;Standards","autonomous aerial vehicles;entropy;feature extraction;image classification;image fusion;learning (artificial intelligence);neural nets;pattern classification;remote sensing;remotely operated vehicles;sensor fusion","multiple source domains;feature extractor;opportune fusion layer;domain discrepancy;target domain;Minmax entropy approach;adversarial manner;conditional entropy;target samples;source classifier;fusion module;weighted average fusion layer;different Softmax classifiers;multisource data set;adversarial learning;knowledge adaptation;multiple remote sensing sources;neural architecture;unsupervised domain","","6","","17","IEEE","30 Jun 2020","","","IEEE","IEEE Journals"
"Information Loss-Guided Multi-Resolution Image Fusion","Q. Wang; W. Shi; P. M. Atkinson","College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; Geography and Environmental Science, University of Southampton, Southampton, U.K.","IEEE Transactions on Geoscience and Remote Sensing","27 Dec 2019","2020","58","1","45","57","Spatial downscaling is an ill-posed, inverse problem, and information loss (IL) inevitably exists in the predictions produced by any downscaling technique. The recently popularized area-to-point kriging (ATPK)-based downscaling approach can account for the size of support and the point spread function (PSF) of the sensor, and moreover, it has the appealing advantage of the perfect coherence property. In this article, based on the advantages of ATPK and the conceptualization of IL, an IL-guided image fusion (ILGIF) approach is proposed. ILGIF uses the fine spatial resolution images acquired in other wavelengths to predict the IL in ATPK predictions based on the geographically weighted regression (GWR) model, which accounts for the spatial variation in land cover. ILGIF inherits all the advantages of ATPK, and its prediction has perfect coherence with the original coarse spatial resolution data which can be demonstrated mathematically. ILGIF was validated using two data sets and was shown in each case to predict downscaled images more accurately than the compared benchmark methods.","1558-0644","","10.1109/TGRS.2019.2930764","National Natural Science Foundation of China(grant numbers:41971297); Tongji University(grant numbers:0250141304,02502350047); Research Grants Council of Hong Kong(grant numbers:PolyU 15223015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842599","Downscaling;geographically weighted regression (GWR);geostatistics;image fusion;information loss (IL)","Spatial resolution;Image fusion;Remote sensing;Earth;Coherence;Predictive models","geophysical image processing;geophysical techniques;image fusion;image resolution;optical transfer function;regression analysis","fine spatial resolution images;ATPK predictions;geographically weighted regression model;spatial variation;coarse spatial resolution data;information loss-guided multiresolution image fusion;spatial downscaling;inverse problem;area-to-point kriging;point spread function;IL-guided image fusion approach;ILGIF;land cover;images downscaling","","9","","46","CCBY","17 Sep 2019","","","IEEE","IEEE Journals"
"Partial Domain Adaptation for Scene Classification From Remote Sensing Imagery","J. Zheng; Y. Zhao; W. Wu; M. Chen; W. Li; H. Fu","Department of Earth System Science, and Department of Earth System Science, Ministry of Education Key Laboratory for Earth System Modeling, Xi’an Institute of Surveying and Mapping Joint Research Center for Next-Generation Smart Mapping, Tsinghua University, Beijing, China; Department of Earth System Science, and Department of Earth System Science, Ministry of Education Key Laboratory for Earth System Modeling, Xi’an Institute of Surveying and Mapping Joint Research Center for Next-Generation Smart Mapping, Tsinghua University, Beijing, China; National Supercomputing Center, Wuxi, China; Department of Earth System Science, and Department of Earth System Science, Ministry of Education Key Laboratory for Earth System Modeling, Xi’an Institute of Surveying and Mapping Joint Research Center for Next-Generation Smart Mapping, Tsinghua University, Beijing, China; School of Geospatial Engineering and Science, Sun Yat-sen University, Zhuhai, China; National Supercomputing Center, Wuxi, China","IEEE Transactions on Geoscience and Remote Sensing","19 Jan 2023","2023","61","","1","17","Although domain adaptation approaches have been proposed to tackle cross-regional, multitemporal, and multisensor remote sensing applications since they do not require any human interpretation in the target domain, most current works assume identical label space across the source and the target domains. However, in real-world applications, we often transfer knowledge from a large-scale dataset with rich annotations to a small-scale target dataset with scarcity of labels. In most cases, the label space of the source domain is usually large enough to subsume that of the target domain, which is termed partial domain adaptation. In this article, we propose a new partial domain adaptation algorithm for remote sensing scene classification and our proposed method contains three major parts. First, we employ a progressive auxiliary domain module to alleviate the negative transfer effect caused by outlier classes. Second, we adopt an improved domain adversarial neural network (DANN) with multiweights to better encourage domain confusion. Last but not least, we design an attentive complement entropy regularization to improve the prediction confidence for samples and avoid untransferable samples (such as the samples belonging to outlier classes in the source domain) being mistakenly classified. We collect three common remote sensing datasets to evaluate our proposed method. Our method achieves an average accuracy of 79.36%, which considerably outperforms other state-of-the-art partial domain adaptation methods with an average accuracy improvement of 1.90%–12.45% and attaining a 13.67% gain compared to the straightforward deep learning model (ResNet-50). The experiment results indicate that our approach shows promising prospects for solving more general and practical domain adaptation problems where the label space of the source domain subsumes that of the target domain.","1558-0644","","10.1109/TGRS.2022.3229039","National Key Research and Development Plan of China(grant numbers:2020YFB0204800); National Natural Science Foundation of China(grant numbers:T2125006,U1839206,42201358); Jiangsu Innovation Capacity Building Program(grant numbers:BM2022028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984694","Adversarial learning;deep learning;negative transfer effect;partial domain adaptation;remote sensing;scene classification","Remote sensing;Standards;Annotations;Rivers;Airports;Adaptation models;Histograms","deep learning (artificial intelligence);entropy;geophysical image processing;image classification;image fusion;remote sensing","common remote sensing datasets;cross-regional remote sensing applications;DANN;deep learning model;domain confusion;entropy regularization;general domain adaptation problems;identical label space;improved domain adversarial neural network;multisensor remote sensing applications;multitemporal remote sensing applications;negative transfer effect;outlier classes;partial domain adaptation algorithm;partial domain adaptation methods;prediction confidence;progressive auxiliary domain module;remote sensing imagery;remote sensing scene classification;ResNet-50;small-scale target dataset;source domain subsumes;target domain","","2","","91","CCBY","14 Dec 2022","","","IEEE","IEEE Journals"
"A Flexible Reference-Insensitive Spatiotemporal Fusion Model for Remote Sensing Images Using Conditional Generative Adversarial Network","Z. Tan; M. Gao; X. Li; L. Jiang","Shaanxi Key Laboratory of Earth Surface System and Environmental Carrying Capacity, Northwest University, Xi’an, China; School of Geology Engineering and Geomatics, Chang’an University, Xi’an, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","13","Due to the tradeoff between spatial and temporal resolutions of remote sensing images, spatiotemporal fusion models were proposed to synthesize the high spatiotemporal image series. Currently, spatiotemporal fusion models usually employ one coarse-resolution image acquired on a prediction date and at least another pair of coarse–fine resolution images close to the prediction time as references to derive the fine-resolution image on the prediction date. After years of development, the model accuracy has gained a certain improvement, but nearly, all the models require at least three image inputs and rigid time constraints must be applied to the references to guarantee the fusion accuracy. However, it is not always that easy to collect adequate data pairs for fine-resolution image series simulation in practice because of the bad weather condition or the time inconsistency between the coarse–fine resolution data sources, which causes some difficulties in the actual application. This article introduces the conditional generative adversarial network (CGAN) and switchable normalization technique into the spatiotemporal fusion problem and proposes a flexible deep network named the GAN-based SpatioTemporal Fusion Model (GAN-STFM) to reduce the number of model inputs and broke the time restriction on reference image selection. The GAN-STFM just needs a coarse-resolution image on the prediction date and another fine-resolution reference image at an arbitrary time in the same area for model inputs. As far as we know, this is the first spatiotemporal fusion model that requires only two images as model inputs and puts no restriction on the acquisition time of references. Even so, the GAN-STFM performs on par or better than other classical fusion models in the experiments. With this improvement, the data preparation for spatiotemporal fusion tends to be much easier than before, showing a promising perspective for practical applications.","1558-0644","","10.1109/TGRS.2021.3050551","National Natural Science Foundation of China (NSFC)(grant numbers:42001382,41901357); Scientific Research Staring Foundation of Northwest University, China(grant numbers:363042005009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336033","Conditional generative adversarial network (CGAN);convolutional neural network (CNN);data fusion;deep learning;generative adversarial network (GAN)-based SpatioTemporal Fusion Model (GAN-STFM);remote sensing;spatiotemporal","Spatiotemporal phenomena;Gallium nitride;Data models;Remote sensing;Image resolution;Spatial resolution;Mathematical model","geophysical image processing;geophysical signal processing;image fusion;image resolution;remote sensing;sensor fusion;spatiotemporal phenomena","flexible reference-insensitive spatiotemporal Fusion Model;remote sensing images;conditional generative adversarial network;spatial resolutions;temporal resolutions;high spatiotemporal image series;coarse-resolution image;prediction date;coarse-fine resolution images;prediction time;model accuracy;image inputs;fusion accuracy;fine-resolution image series simulation;coarse-fine resolution data sources;switchable normalization technique;spatiotemporal fusion problem;GAN-based SpatioTemporal Fusion Model;GAN-STFM;model inputs;reference image selection;fine-resolution reference image;classical fusion models","","27","","47","IEEE","26 Jan 2021","","","IEEE","IEEE Journals"
"Unmixing-Based Spatiotemporal Image Fusion Accounting for Complex Land Cover Changes","X. Jiang; B. Huang","Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong SAR, China; Department of Geography and Resource Management, Institute of Space and Earth Information Science and the Shenzhen Research Institute, The Chinese University of Hong Kong, Hong Kong SAR, China","IEEE Transactions on Geoscience and Remote Sensing","19 May 2022","2022","60","","1","10","Spatiotemporal reflectance fusion has received considerable attention in recent decades. However, various challenges remain despite varying levels of success, especially regarding the recovery of spatial details with complex land cover changes. Taking the blending of Landsat and Moderate Resolution Imaging Spectroradiometer (MODIS) images as an example, this article presents a locally weighted unmixing-based spatiotemporal image fusion model (LWU-STFM) that focuses on recovering complex land cover changes. The core idea is to redefine the land use class of each pixel featuring land cover change at the prediction date. The spatial unmixing process is enhanced using a proposed geographically spectrum-weighted regression (GSWR), and then, we optimize similar neighboring pixels for the final weighted-based prediction. Experiments are conducted using semisimulated and actual time-series Landsat–MODIS datasets to demonstrate the performance of the proposed LWU-STFM compared with the classic spatial and temporal adaptive reflectance fusion model (STARFM), flexible spatiotemporal data fusion (FSDAF), two enhanced FSDAF models (SFSDAF and FSDAF 2.0), and a virtual image pair-based spatiotemporal fusion model for spatial weighting (VIPSTF-SW). The results reveal that the proposed LWU-STFM outperforms the other five models with the best quantitative accuracy. In terms of the relative dimensionless global error (ERGAS) index, the errors of Landsat-like images generated using LWU-STFM are 2.8%–63.4% lower than those of other models. From visual comparisons, LWU-STFM predictions illustrate encouraging improvements in recovering spatial details of pixels with complex land cover changes in heterogeneous landscapes and, thus, advancing applications of spatiotemporal image fusion for continuous and fine-scale land surface monitoring.","1558-0644","","10.1109/TGRS.2022.3173172","Research Grant from Shenzhen Research Institute of The Chinese University of Hong Kong, China(grant numbers:D.01.13.00801); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770044","Change detection;geographically spectrum-weighted regression (GSWR);land cover changes;locally spatial unmixing;spatiotemporal image fusion (STIF)","Remote sensing;Earth;Reflectivity;Land surface;Artificial satellites;Adaptation models;Spatiotemporal phenomena","environmental monitoring (geophysics);geophysical image processing;geophysical techniques;image fusion;image resolution;land cover;remote sensing;sensor fusion;spatiotemporal phenomena;time series","flexible spatiotemporal data fusion;virtual image pair-based spatiotemporal fusion model;LWU-STFM;spatial details;complex land cover changes;spatiotemporal image fusion accounting;spatiotemporal reflectance fusion;Moderate Resolution Imaging Spectroradiometer;locally weighted unmixing-based spatiotemporal image fusion model;land cover change;spatial unmixing process;final weighted-based prediction;temporal adaptive reflectance fusion model","","","","43","IEEE","5 May 2022","","","IEEE","IEEE Journals"
"IR-MSDNet: Infrared and Visible Image Fusion Based On Infrared Features and Multiscale Dense Network","A. Raza; J. Liu; Y. Liu; J. Liu; Z. Li; X. Chen; H. Huo; T. Fang","Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Apr 2021","2021","14","","3426","3437","Infrared (IR) and visible images are heterogeneous data, and their fusion is one of the important research contents in the remote sensing field. In the last decade, deep networks have been widely used in image fusion due to their ability to preserve high-level semantic information. However, due to the lower resolution of IR images, deep learning-based methods may not be able to retain the salient features of IR images. In this article, a novel IR and visible image fusion based on IR Features & Multiscale Dense Network (IR-MSDNet) is proposed to preserve the content and key target features from both visible and IR images in the fused image. It comprises an encoder, a multiscale decoder, a traditional processing unit, and a fused unit, and can capture incredibly rich background details in visible images and prominent target details in IR features. When the dense and multiscale features are fused, the background details are obtained by utilizing attention strategy, and then combined with complimentary edge features. While IR features are extracted by traditional quadtree decomposition and Bezier interpolation, and further intensified by refinement. Finally, both the decoded multiscale features and IR features are used to reconstruct the final fused image. Experimental evaluation with other state-of-the-art fusion methods validates the superiority of our proposed IR-MSDNet in both subjective and objective evaluation metrics. Additional objective evaluation conducted on the object detection (OD) task further verifies that the proposed IR-MSDNet has greatly enhanced the details in the fused images, which bring the best OD results.","2151-1535","","10.1109/JSTARS.2021.3065121","National Key Research and Development Program of China(grant numbers:2018YFB0505000); National Science and Technology Major Project(grant numbers:21-Y20A06-9001-17/18); National Natural Science Foundation of China(grant numbers:61221003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376604","Feature attention;image fusion;multiscale feature fusion;object detection (OD);remote sensing","Feature extraction;Image fusion;Decoding;Remote sensing;Generative adversarial networks;Transforms;Object detection","feature extraction;image fusion;infrared imaging;learning (artificial intelligence);object detection;quadtrees","Bezier interpolation;quadtree decomposition;multiscale dense network;fusion methods;fused image;multiscale features;complimentary edge features;dense features;IR features;salient features;deep learning-based methods;IR images;visible images;infrared features;visible image fusion;IR-MSDNet","","10","","42","CCBY","11 Mar 2021","","","IEEE","IEEE Journals"
"Unsupervised Classification of Multispectral Images Embedded With a Segmentation of Panchromatic Images Using Localized Clusters","T. Mao; H. Tang; W. Huang","Beijing Key Laboratory of Environmental Remote Sensing and Digital Cities, Faculty of Geographical Science, Beijing Normal University, Beijing, China; Beijing Key Laboratory of Environmental Remote Sensing and Digital Cities, Faculty of Geographical Science, Beijing Normal University, Beijing, China; Beijing Key Laboratory of Environmental Remote Sensing and Digital Cities, Faculty of Geographical Science, Beijing Normal University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","30 Oct 2019","2019","57","11","8732","8744","There are many approaches to fuse panchromatic (PAN) and multispectral (MS) images for classification, mainly including sharpening-then-classification methods, classification-then-sharpening methods, and segmentation-then-classification methods. The generalized Chinese restaurant franchise (gCRF) is a segmentation-then-classification-like method to fuse very high resolution (VHR) PAN and MS images for classification, which has the limitation the same as that of the general segmentation-then-classification methods that segmentation errors will affect the subsequent classification. The problems of gCRF are that during the segmentation step, the spatial coherence in the image plane is deficient and the global clusters without spatial position information are used for segmentation, which may lead to undersegmented and disconnected regions in the segmentation results and decrease classification accuracy. In this paper, we propose an improved model, which overcomes the problems of the gCRF during the segmentation step, to increase the classification accuracy by the following two ways: 1) building the spatial coherence in the image plane by introducing neighborhood information of superpixels to construct the subimages and 2) using localized clusters with spatial location information instead of global clusters to measure the similarity between superpixels and segments. The experimental results show that the problems of undersegmentation and disconnected segments are both alleviated, resulting in better classification results in terms of the visual and quantitative aspects.","1558-0644","","10.1109/TGRS.2019.2922672","National Key R&D Programme of China(grant numbers:2017YFB0504104); National Natural Science Foundation of China(grant numbers:41571334); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765422","Chinese restaurant franchise (CRF);image fusion;remote sensing images;unsupervised classification","Image segmentation;Spatial resolution;Remote sensing;Image color analysis;Spatial coherence;Fuses","geophysical image processing;image classification;image fusion;image resolution;image segmentation;pattern clustering","panchromatic images;localized clusters;sharpening-then-classification methods;classification-then-sharpening methods;segmentation-then-classification methods;generalized Chinese restaurant franchise;gCRF;segmentation-then-classification-like method;segmentation errors;subsequent classification;segmentation step;spatial coherence;image plane;global clusters;classification accuracy;unsupervised classification;multispectral images","","7","","33","IEEE","17 Jul 2019","","","IEEE","IEEE Journals"
"Advanced Multi-Sensor Optical Remote Sensing for Urban Land Use and Land Cover Classification: Outcome of the 2018 IEEE GRSS Data Fusion Contest","Y. Xu; B. Du; L. Zhang; D. Cerra; M. Pato; E. Carmona; S. Prasad; N. Yokoya; R. Hänsch; B. Le Saux","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; German Aerospace Center (DLR), Remote Sensing Technology Institute (MF-PBA), Weßling, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute (MF-PBA), Weßling, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute (MF-PBA), Weßling, Germany; Electrical and Computer Engineering Department, University of Houston, Houston, TX, USA; RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; Computer Vision and Remote Sensing Department, Technical University of Berlin, Berlin, Germany; DTIS, ONERA, University Paris Saclay, Palaiseau, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Jul 2019","2019","12","6","1709","1724","This paper presents the scientific outcomes of the 2018 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2018 Contest addressed the problem of urban observation and monitoring with advanced multi-source optical remote sensing (multispectral LiDAR, hyperspectral imaging, and very high-resolution imagery). The competition was based on urban land use and land cover classification, aiming to distinguish between very diverse and detailed classes of urban objects, materials, and vegetation. Besides data fusion, it also quantified the respective assets of the novel sensors used to collect the data. Participants proposed elaborate approaches rooted in remote-sensing, and also in machine learning and computer vision, to make the most of the available data. Winning approaches combine convolutional neural networks with subtle earth-observation data scientist expertise.","2151-1535","","10.1109/JSTARS.2019.2911113","National Natural Science Foundation of China(grant numbers:41431175,61822113,41871243,61471274); National Key R & D Program of China(grant numbers:2018YFA0605501); Natural Science Foundation of Hubei Province(grant numbers:2018CFA05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727489","Convolutional neural networks (CNN);deep learning;hyperspectral (HS) imaging (HSI);image analysis and data fusion;multimodal;multiresolution;multisource;multispectral light detection and ranging (LiDAR)","Remote sensing;Laser radar;Data integration;Optical sensors;Optical imaging;Training","geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;land cover;learning (artificial intelligence);neural nets;terrain mapping","urban observation;advanced multisource optical remote sensing;multispectral LiDAR;hyperspectral imaging;urban land use;land cover classification;urban objects;urban monitoring;Image Analysis and Data Fusion Technical Committee;IEEE GRSS Data Fusion Contest;IEEE GRSS Data Fusion;IEEE Geoscience and Remote Sensing Society;advanced multisensor optical remote sensing;subtle Earth-observation data scientist expertise;convolutional neural networks","","137","","46","OAPA","31 May 2019","","","IEEE","IEEE Journals"
"MSFusion: Multistage for Remote Sensing Image Spatiotemporal Fusion Based on Texture Transformer and Convolutional Neural Network","G. Yang; Y. Qian; H. Liu; B. Tang; R. Qi; Y. Lu; J. Geng","Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Signal Detection and Processing in Xinjiang Uygur Autonomous Region, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; School of Software, Xinjiang University, Urumqi, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Jun 2022","2022","15","","4653","4666","Due to the limitations of current technology and budget, a single satellite sensor cannot obtain high spatiotemporal resolution remote sensing images. Therefore, remote sensing image spatio-temporal fusion technology is considered as an effective solution and has attracted extensive attention. In the field of deep learning, due to the fixed size of the perception field of a convolutional neural network, it is impossible to model the correlation of global features, and the features extracted only through convolution operation lack the ability to capture long-distance features. At the same time, complex fusion methods cannot better integrate temporal and spatial features. In order to solve these problems, we propose a multistage remote sensing image spatio-temporal fusion model based on texture transformer and convolutional neural network. The model combines the advantages of transformer and convolutional network, uses a lightweight convolution network to extract spatial features and temporal discrepancy features, uses transformer to learn global temporal correlation, and finally, fuses temporal features with spatial features. In order to make full use of the features obtained in different stages, we design a cross-stage adaptive fusion module CSAFM. The module adopts the self-attention mechanism to adaptively integrate the features of different scales while considering the temporal and spatial characteristics. To test the robustness of the model, the experiments are carried out on three datasets of CIA, LGC, and DX. Compared with five typical spatio-temporal fusion algorithms, we obtain excellent results, which prove the superiority of MSFusion model.","2151-1535","","10.1109/JSTARS.2022.3179415","National Natural Science Foundation of China(grant numbers:61966035); National Natural Science Foundation of China(grant numbers:U1803261); Natural Science Foundation of the XinJiang Uygur Autonomous Region(grant numbers:2021D01C077); Autonomous Region Graduate Innovation Project(grant numbers:XJ2019G069,XJ2021G062,XJ2020G074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786792","Multistage feature fusion;multitemporal remote sensing data;remote sensing;self-attention;spatiotemporal fusion;transformer","Remote sensing;Feature extraction;Predictive models;Transformers;Spatial resolution;Fuses;Spatiotemporal phenomena","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image representation;image resolution;image sensors;image texture;remote sensing;spatiotemporal phenomena","temporal discrepancy features;global temporal correlation;cross-stage adaptive fusion module CSAFM;temporal characteristics;spatial characteristics;spatio-temporal fusion algorithms;MSFusion model;remote sensing image spatiotemporal fusion;texture transformer;convolutional neural network;high spatiotemporal resolution remote sensing images;remote sensing image spatio-temporal fusion technology;convolution operation;multistage remote sensing image spatio-temporal fusion model","","3","","47","CCBY","2 Jun 2022","","","IEEE","IEEE Journals"
"Multispectral and Hyperspectral Image Fusion Based on Group Spectral Embedding and Low-Rank Factorization","K. Zhang; M. Wang; S. Yang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","1 Mar 2017","2017","55","3","1363","1371","Fusing low spatial resolution hyperspectral (LRHS) images and high spatial resolution multispectral (HRMS) images to obtain high spatial resolution hyperspectral images (HRHS) has received increasing interests in recent years. In this paper, a new group spectral embedding (GSE)-based LRHS and HRMS image fusion method is proposed by exploring the multiple manifold structures of spectral bands and the low-rank structure of HRHS data. First, a low-rank factorization fusion (LRFF)-based robust recovery model is developed for HRHS images, by regarding HRMS images as the spectral degradation of HRHS images and exploring the group sparse prior of difference images. Then, an assumption that grouped spectral bands share the similar local geometry is cast on LRHS and HRHS images, to formulate a GSE regularizer in the LRFF model. Finally, an iterative optimization algorithm based on augmented Lagrangian multiplier is advanced to recover HRHS images. Experimental results on several data sets show the effectiveness of the proposed method on visual and numerical comparison.","1558-0644","","10.1109/TGRS.2016.2623626","National Basic Research Program of China (973 Program)(grant numbers:2013CB329402); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438103,91438201); Fundamental Research Funds for the Central Universities(grant numbers:BDY021429); Foreign Scholars in University Research and Teaching Programs(grant numbers:B07048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7765103","Group spectral embedding (GSE);hyperspectral (HS) image;image fusion;low-rank factorization fusion (LRFF);multispectral (MS) image","Sparse matrices;Spatial resolution;Matrix decomposition;Image fusion;Correlation;Hyperspectral imaging","hyperspectral imaging;image fusion;image resolution;iterative methods;optimisation","multispectral image fusion;hyperspectral image fusion;group spectral embedding;low spatial resolution hyperspectral images;high spatial resolution multispectral images;high spatial resolution hyperspectral images;GSE-based LRHS image fusion;GSE-based HRMS image fusion;spectral band multiple manifold structures;low-rank factorization fusion;LRFF-based robust recovery model;HRHS image spectral degradation;GSE regularizer;iterative optimization algorithm;augmented Lagrangian multiplier","","62","","40","IEEE","2 Dec 2016","","","IEEE","IEEE Journals"
"Multimodal Sensors Image Fusion for Higher Resolution Remote Sensing Pan Sharpening","Y. Liu; Q. Teng; X. He; C. Ren; H. Chen","College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China","IEEE Sensors Journal","14 Sep 2022","2022","22","18","18021","18034","Panchromatic (PAN) sensors and multispectral (MS) sensors are usually applied to remote sensing tasks. Pan sharpening is an effective image fusion method based on multimodal sensors. In pan sharpening, high-resolution (HR) PAN images from PAN sensors and corresponding low-resolution MS images from MS sensors are used to obtain the HR MS images. Normally, the ideal pan-sharpening image is a fused MS image with the same resolution as the PAN image. We hope to obtain the pan-sharpening images with higher resolution, which will not only preserve spatial structure information and spectral information from PAN images and MS images but also achieve super-resolution reconstruction. For this purpose, a novel pan-sharpening framework, called multilevel and multiscale fusion network (MLMSFN), is constructed. To our knowledge, our framework is the first work to achieve beyond the limit of existing image resolution in pan sharpening. We evaluate the effectiveness of our designed method, and the experiments testify that our method can obtain higher resolution pan-sharpening images.","1558-1748","","10.1109/JSEN.2022.3195243","National Natural Science Foundation of China(grant numbers:61871279,62211530110); Fundamental Research Funds for the Central Universities(grant numbers:2021SCU12061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851610","Multilevel and multiscale;multimodal sensors;pan sharpening;super-resolution (SR)","Image resolution;Image reconstruction;Task analysis;Remote sensing;Image sensors;Spatial resolution;Superresolution","geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","multimodal sensors image fusion;higher resolution remote sensing pan sharpening;panchromatic sensors;multispectral sensors;remote sensing tasks;effective image fusion method;high-resolution PAN images;PAN sensors;corresponding low-resolution;MS sensors;HR MS images;ideal pan-sharpening image;fused MS image;PAN image;super-resolution reconstruction;pan-sharpening framework;image resolution;higher resolution pan-sharpening images","","","","57","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"A Band Divide-and-Conquer Multispectral and Hyperspectral Image Fusion Method","W. Sun; K. Ren; X. Meng; C. Xiao; G. Yang; J. Peng","Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Hubei Key Laboratory of Applied Mathematics, Faculty of Mathematics and Statistics, Hubei University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","6 Dec 2021","2022","60","","1","13","The nonoverlapped spectrum range between low spatial resolution (LR) hyperspectral (HS) and high spatial resolution (HR) multispectral (MS) images has been a fundamental but challenging problem for MS/HS fusion. The spectrum of HS data is generally 400–2500 nm, and the spectrum of MS data is generally 400–900 nm; how to obtain the high-fidelity HR HS fused image within the whole spectrum of 400–2500 nm? In this article, we proposed a band divide-and-conquer framework (BDCF) to solve the problem, by comprehensively considering spectral fidelity, spatial enhancement, and computational efficiency. First, the spectral bands of HS were divided into overlapped and nonoverlapped bands according to the spectral response between HS and MS. Then, a novel improved component substitution (CS)-based method by combing neural network was proposed to fuse the overlapped bands of LR HS. Then, a mapping-based method with the neural network was presented to construct the complicated nonlinear relationship between overlapped and nonoverlapped bands of the original LR HS data. The trained network was mapped to the fused overlapped HR HS bands to estimate the nonoverlapped HR HS bands. Experimental results on two simulated data sets and two realistic data sets of Gaofen (GF)-5 LR HS, GF-1 MS, and Sentinel-2A MS show that the proposed BDCF has superior performance in both high spectral fidelity and sharp spatial details, and it obtained competitive fusion behaviors compared with other state-of-the-art methods. Moreover, BDCF has relatively higher computational efficiency than optimal solution-based methods and deep learning-based fusion methods.","1558-0644","","10.1109/TGRS.2020.3046321","National Natural Science Foundation of China(grant numbers:41971296,41671342,41801252,41801256,U1609203,61871177); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR19D010001,LQ18D010001); Natural Science Foundation of Ningbo City(grant numbers:2019A610098); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363338","Divide-and-conquer framework (BDCF);Gaofen-5 (GF-5);hyperspectral (HS);image fusion;multispectral (MS);neural network framework","Spatial resolution;Neural networks;Image fusion;Hyperspectral imaging;Bayes methods;Sun;Signal resolution","deep learning (artificial intelligence);divide and conquer methods;hyperspectral imaging;image fusion;image resolution;spectral analysis","hyperspectral image fusion;low spatial resolution hyperspectral;high spatial resolution multispectral images;spatial enhancement;spectral bands;neural network;mapping-based method;HR HS bands;simulated data sets;realistic data sets;Gaofen-5 LR HS;GF-1 MS;deep learning-based fusion methods;band divide-and-conquer multispectral image fusion","","20","","45","IEEE","25 Feb 2021","","","IEEE","IEEE Journals"
"CEGFNet: Common Extraction and Gate Fusion Network for Scene Parsing of Remote Sensing Images","W. Zhou; J. Jin; J. Lei; J. -N. Hwang","School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; Department of Electrical Engineering, University of Washington, Seattle, WA, USA","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","10","Scene parsing of high spatial resolution (HSR) remote sensing images has achieved notable progress in recent years by the adoption of convolutional neural networks. However, for scene parsing of multimodal remote sensing images, effectively integrating complementary information remains challenging. For instance, the decrease in feature map resolution through a neural network causes loss of spatial information, likely leading to blurred object boundaries and misclassification of small objects. In addition, object scales on a remote sensing image vary substantially, undermining the parsing performance. To solve these problems, we propose an end-to-end common extraction and gate fusion network (CEGFNet) to capture both high-level semantic features and low-level spatial details for scene parsing of remote sensing images. Specifically, we introduce a gate fusion module to extract complementary features from spectral data and digital surface model data. A gate mechanism removes redundant features in the data stream and extracts complementary features that improve multimodal feature fusion. In addition, a global context module and a multilayer aggregation decoder handle scale variations between objects and the loss of spatial details due to downsampling, respectively. The proposed CEGFNet was quantitatively evaluated on benchmark scene parsing datasets containing HSR remote sensing images, and it achieved state-of-the-art performance.","1558-0644","","10.1109/TGRS.2021.3109626","National Natural Science Foundation of China(grant numbers:61502429,61972357); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY18F020012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9538389","Gate fusion module (GFM);high spatial resolution (HSR) remote sensing image;multimodal fusion;scene parsing","Feature extraction;Remote sensing;Semantics;Logic gates;Decoding;Data mining;Spatial resolution","convolutional neural nets;expectation-maximisation algorithm;feature extraction;geophysical image processing;image classification;image fusion;image resolution;image texture;remote sensing","data stream;multimodal feature fusion;CEGFNet;benchmark scene parsing datasets;HSR remote sensing images;gate fusion network;remote sensing image;high spatial resolution;convolutional neural networks;multimodal remote sensing images;feature map resolution;end-to-end common extraction;high-level semantic features;gate fusion module","","3","","40","IEEE","15 Sep 2021","","","IEEE","IEEE Journals"
"Geological Remote Sensing Interpretation Using Deep Learning Feature and an Adaptive Multisource Data Fusion Network","W. Han; J. Li; S. Wang; X. Zhang; Y. Dong; R. Fan; X. Zhang; L. Wang","School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science, the Hubei Key Laboratory of Intelligent Geo-Information Processing, and the Key Laboratory of Geological Survey and Evaluation of the Ministry of Education, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","22 Jun 2022","2022","60","","1","14","Geological remote sensing interpretation can extract elements of interest from multiple types of images, which is vital in geological survey and mapping, especially in inaccessible regions. However, due to numerous classes, high interclass similarities, complex distributions, and sample imbalances of geological elements, the interpretation results of machine learning (ML)-based methods are understandably worse than manual visual interpretation. In addition, scholars in remote sensing have mainly carried out their works to interpret a single geological element category, such as mineral, lithological, soil, and structure. The interpretation of multiple geological elements is missing, which is more in line with the open world. To improve the interpretation results of ML-based methods and reduce the labor cost in geological survey and mapping, we propose a deep learning (DL)-feature-based adaptive multisource data fusion network (AMSDFNet) for the efficient interpretation of multiple geological remote sensing elements. The AMSDFNet has two branches for learning valuable spatial and spectral information from two kinds of data sources, in which the atrous spatial pyramid pooling (ASPP) operation and an attention block are applied to adaptively extract and fuse multiscale informative features. A hard example mining algorithm was also added to select important training examples to address sample imbalance. A large-scale region in western China with sufficient geological elements was set as the research area. The proposed model improved the two critical metrics by about 2% in the experiment section. As far as we know, this research work is the first time DL features and multisource remote sensing images have been utilized to simultaneously interpret geological elements of lithology, soil, surface water, and glaciers. The extensive experimental results demonstrated the superiority of DL features and our model in geological remote sensing interpretation.","1558-0644","","10.1109/TGRS.2022.3183080","National Natural Science Foundation of China(grant numbers:U21A2013,41925007); Fundamental Research Funds for the Central Universities, China University of Geosciences, Wuhan(grant numbers:162301212697); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795323","Deep learning;geological remote sensing;multisource data fusion;semantic segmentation","Geology;Remote sensing;Feature extraction;Soil;Earth;Minerals;Data mining","data mining;feature extraction;geology;geophysical image processing;image classification;image fusion;learning (artificial intelligence);object detection;remote sensing;sensor fusion","geological remote sensing interpretation;deep learning feature;geological survey;interpretation results;machine learning-based methods;manual visual interpretation;single geological element category;multiple geological elements;deep learning-feature-based adaptive multisource data fusion network;efficient interpretation;multiple geological remote sensing elements;sufficient geological elements;multisource remote sensing images","","2","","47","IEEE","14 Jun 2022","","","IEEE","IEEE Journals"
"Learning Specific and General Realm Feature Representations for Image Fusion","F. Zhao; W. Zhao","School of Physics and Electronic Technology, Liaoning Normal University, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","IEEE Transactions on Multimedia","24 Aug 2021","2021","23","","2745","2756","A universal fusion framework for handling multi-realm image fusion reduces the cost of manual selection in varied applications. Addressing the generality of multiple realms and the sensitivity of specific realm, we propose a novel universal framework for multi-realm image fusion through learning realm-specific and realm-general feature representations. Shared principle network, adaptive realm feature extraction strategy and realm activation mechanism are designed for facilitating high generalization of across-realm and sensitivity of specific-realm simultaneously. In addition, we present realm-specific no-reference perceptual metric losses based on the edge details and contrast for optimizing the learning process, making the fused image exhibit more specific appearance. Moreover, we collect a new multi-realm image fusion dataset (MRIF), consisting of infrared and visual images, medical images and multispectral images, to facilitate our training and testing. Experimental results show that the fused image obtained by the proposed method achieves superior performance compared with the state-of-the-art methods on MRIF and the other three datasets including infrared and visual images, medical images and remote sensing images, respectively.","1941-0077","","10.1109/TMM.2020.3016123","National Natural Science Foundation of China(grant numbers:61801077); China Postdoctoral Science Foundation(grant numbers:2019T120206,2017M611221); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9167484","Universal image fusion framework;adaptive realm feature extraction strategy;realm activation mechanism;no-reference perceptual metric loss","Image fusion;Feature extraction;Biomedical imaging;Image edge detection;Visualization;Remote sensing;Transforms","feature extraction;image fusion;image representation;learning (artificial intelligence);remote sensing;sensor fusion","general realm feature representations;universal fusion framework;multiple realms;specific realm;realm-specific;realm-general feature representations;adaptive realm feature extraction strategy;realm activation mechanism;across-realm;specific-realm;fused image exhibit;multirealm image fusion dataset;visual images;medical images;multispectral images;infrared images;remote sensing images","","10","","64","IEEE","14 Aug 2020","","","IEEE","IEEE Journals"
"Attention-Based Multistage Fusion Network for Remote Sensing Image Pansharpening","W. Zhang; J. Li; Z. Hua","Institute of Network Technology, ICT (YANTAI), Yantai, China; School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Shandong Technology and Business University, Yantai, China","IEEE Transactions on Geoscience and Remote Sensing","27 Jan 2022","2022","60","","1","16","Pansharpening is a significant branch in the field of remote sensing image processing, the goal of which is to fuse panchromatic (PAN) and multispectral (MS) images through certain rules to generate high-resolution MS (HRMS) images. Therefore, how to improve the spatial and spectral resolutions of the fused image is the problem that we need to solve urgently. In this article, a multistage remote sensing image fusion network (MRFNet) is proposed on the basis of in-depth research and exploration on the fusion of the PAN and MS images to obtain a clear fused image that can reflect the ground features more comprehensively and completely. The proposed network consists of three stages that are connected by cross-stage fusion. The first two stages are used to extract the features of the PAN and MS images. The structure of the encoder–decoder and the channel attention module are used to extract the features of the remote sensing image in the channel domain. The third stage is the image reconstruction stage fusing the extracted features with the original image to improve the spatial and spectral resolutions of the fused result. A series of experiments are conducted on the benchmark datasets WorldView II, GF-2, and QuickBird. Qualitative analysis and quantitative comparison show the superiority of MRFNet in visual effects and the values of evaluation indicators.","1558-0644","","10.1109/TGRS.2021.3113984","National Natural Science Foundation of China(grant numbers:61772319,62002200,61976125,61976124); Shandong Natural Science Foundation of China(grant numbers:ZR2017MF049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555829","Attention;encoder–decoder;multispectral (MS) images;panchromatic (PAN) images;pansharpening;spatial resolution;spectral resolution","Pansharpening;Feature extraction;Spatial resolution;Remote sensing;Image resolution;Sensors;Image reconstruction","feature extraction;geophysical image processing;image fusion;image processing;image reconstruction;image resolution;remote sensing","multistage fusion network;remote sensing image pansharpening;remote sensing image processing;high-resolution MS images;spatial resolutions;spectral resolutions;multistage remote sensing image fusion network;clear fused image;cross-stage fusion;channel attention module;image reconstruction stage;fused result","","2","","60","IEEE","1 Oct 2021","","","IEEE","IEEE Journals"
"CCANet: Class-Constraint Coarse-to-Fine Attentional Deep Network for Subdecimeter Aerial Image Semantic Segmentation","G. Deng; Z. Wu; C. Wang; M. Xu; Y. Zhong","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","6 Dec 2021","2022","60","","1","20","Semantic segmentation is important for the understanding of subdecimeter aerial images. In recent years, deep convolutional neural networks (DCNNs) have been used widely for semantic segmentation in the field of remote sensing. However, because of the highly complex subdecimeter resolution of aerial images, inseparability often occurs among some geographic entities of interest in the spectral domain. In addition, the semantic segmentation methods based on DCNNs mostly obtain context information using extra information within the added receptive field. However, the context information obtained this way is not explicit. We propose a novel class-constraint coarse-to-fine attentional (CCA) deep network, which enables the formation of class information constraints to obtain explicit long-range context information. Further, the performance of subdecimeter aerial image semantic segmentation can be improved, particularly for fine-structured geographic entities. Based on coarse-to-fine technology, we obtained a coarse segmentation result and constructed an image class feature library. We propose the use of the attention mechanism to obtain strong class-constrained features. Consequently, pixels of different geographic entities can adaptively match the corresponding categories in the class feature library. Additionally, we employed a novel loss function, CCA-loss to realize end-to-end training. The experimental results obtained using two popular open benchmarks, International Society for Photogrammetry and Remote Sensing (ISPRS) 2-D semantic labeling Vaihingen data set and Institute of Electrical and Electronics Engineers (IEEE) Geoscience and Remote Sensing Society (GRSS) Data Fusion Contest Zeebrugge data set, validated the effectiveness and superiority of our proposed model. The proposed method achieved state-of-the-art performance on the IEEE GRSS Data Fusion Contest Zeebrugge data set.","1558-0644","","10.1109/TGRS.2021.3055950","National Key Research and Development Program of China(grant numbers:2018YFB0504800,2018YFB0504801); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9353671","Aerial imagery;context information;deep convolutional neural network (DCNN);remote sensing;semantic segmentation","Image segmentation;Semantics;Remote sensing;Libraries;Feature extraction;Automobiles;Training","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;photogrammetry;remote sensing;sensor fusion","image class feature library;strong class-constrained features;Remote Sensing Society Data Fusion Contest Zeebrugge data;class-constraint coarse-to-fine attentional deep network;subdecimeter aerial image semantic segmentation;subdecimeter aerial images;deep convolutional neural networks;highly complex subdecimeter resolution;semantic segmentation methods;class information constraints;long-range context information;fine-structured geographic entities;coarse-to-fine technology;coarse segmentation result","","8","","63","IEEE","12 Feb 2021","","","IEEE","IEEE Journals"
"Fine-Scale Urban Informal Settlements Mapping by Fusing Remote Sensing Images and Building Data via a Transformer-Based Multimodal Fusion Network","R. Fan; F. Li; W. Han; J. Yan; J. Li; L. Wang","Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Faculty of Science and Technology, University of Macau, Macau, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","22 Sep 2022","2022","60","","1","16","Urban informal settlements (UISs) are high-density population settlements with low standards of living and supply. UIS semantic segmentation, which identifies pixels corresponding to informal settlements in remote sensing images, is crucial to the estimation of poor communities, urban management, resource allocation, and future planning, particularly in megacities. However, most studies on informal settlement mapping are either based on parcels (image classification) or pixels (semantic segmentation). Few studies utilize object information to improve UIS mapping. Since informal settlements are formed by buildings (objects), utilizing object information can improve UIS semantic segmentation. Furthermore, current UIS mapping studies mainly focus on using single-modality remote sensing images, and there is a lack of related research on using multimodal data. Due to the spatial heterogeneity of informal settlements, using only a single modality of remote sensing image features limits the effectiveness and accuracy of informal settlements semantic segmentation. Aiming at achieving fine-scale UIS mapping results, this article proposes a UIS semantic segmentation method, namely UisNet, that utilizes a transformer-based block to receive multimodal data, including high-spatial-resolution remote sensing images (parcel- and pixel-level) and building polygon data (object-level) to identify UIS. The experiments were conducted in Shenzhen City, and they confirmed the superior performance of UisNet, which achieved an overall accuracy (OA) of 94.80% and a mean intersection over union (mIoU) of 85.51% in the testing set of the manually labeled UIS semantic segmentation dataset (UIS-Shenzhen dataset) and outperformed the best models on semantic segmentation tasks. Besides, we add a set of experiments on a public dataset [gaofen image dataset (GID) dataset] and compare our method with the current state-of-the-art semantic segmentation methods. Experiments show that the proposed UisNet improves mIoU by 1.64% to 7.58% compared to other methods. This work will be available at https://github.com/RunyuFan/.","1558-0644","","10.1109/TGRS.2022.3204345","National Natural Science Foundation of China(grant numbers:41925007,U1711266); Special Fund of Hubei Luojia Laboratory(grant numbers:220100035); Hubei Natural Science Foundation of China(grant numbers:2019CFA023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9877906","Deep learning;multimodal;remote sensing;semantic segmentation;urban informal settlements (UISs)","Semantics;Image segmentation;Buildings;Remote sensing;Task analysis;Urban areas;Transformers","feature extraction;geophysical image processing;image classification;image fusion;image registration;image segmentation;remote sensing;town and country planning","object information;current UIS mapping studies;single-modality remote sensing images;multimodal data;remote sensing image features;informal settlements semantic segmentation;fine-scale UIS mapping;UIS semantic segmentation method;transformer-based block;high-spatial-resolution remote sensing images;manually labeled UIS semantic segmentation dataset;UIS-Shenzhen dataset;semantic segmentation tasks;public dataset [gaofen image dataset dataset];current state-of-the-art semantic segmentation methods;fine-scale urban informal settlements;building data;transformer-based multimodal fusion network;high-density population settlements;informal settlement mapping;parcels;image classification;buildings","","1","","68","IEEE","5 Sep 2022","","","IEEE","IEEE Journals"
"Better Memorization, Better Recall: A Lifelong Learning Framework for Remote Sensing Image Scene Classification","D. Ye; J. Peng; H. Li; L. Bruzzone","School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Geoscience and Remote Sensing","8 Aug 2022","2022","60","","1","14","To infer unknown remote sensing scenarios, most existing technologies use a supervised learning paradigm to train deep neural network (DNN) models on closed datasets. This paradigm faces challenges such as highly spatiotemporal variants and ever-changing scale-heterogeneous remote sensing scenarios. Additionally, DNN models cannot scale to new scenarios. Lifelong learning is an effective solution to these problems. Current lifelong learning approaches focus on overcoming the catastrophic forgetting issue (i.e., a successive increase in heterogeneous remote sensing scenes causes models to forget historical scenes) while ignoring the knowledge recall issue (i.e., how to facilitate the learning of new scenes by recalling historical experiences), which is a significant problem. This article proposes a lifelong learning framework called asymmetric collaborative network (SCN) for lifelong remote sensing image (RSI) classification. This framework consists of two structurally distinct networks: a preserving network (Pres-Net) and a transient network (Trans-Net), which imitate the long- and short-term memory processes in the brain, respectively. Moreover, this framework is based on two synergistic knowledge transfer mechanisms: triple distillation and prior feature fusion. The triple distillation mechanism enables knowledge persistence from Trans-Net to Pres-Net to achieve better memorization; the prior feature fusion mechanism enables knowledge transfer from Pres-Net to Trans-Net to achieve better recall. Experiments on three open datasets demonstrate the effectiveness of SCN for three-, six-, and nine-task-length learning. The idea of asymmetric separation networks and the synergistic strategy proposed in this article are expected to provide new solutions to the translatability of the classification of RSIs in real-world scenarios. The source codes are available at https://github.com/GeoX-Lab/SCN.","1558-0644","","10.1109/TGRS.2022.3190392","National Natural Science Foundation of China(grant numbers:41871364,41871302); Scholarship from the China Scholarship Council(grant numbers:201703170123); High Performance Computing Center of Central South University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826803","Asymmetric collaborative network (SCN);catastrophic forgetting;knowledge recall;lifelong learning;remote sensing image (RSI) scene classification","Task analysis;Knowledge engineering;Remote sensing;Collaborative work;Data models;Sensors;Interference","deep learning (artificial intelligence);feature extraction;geophysical image processing;groupware;image classification;image fusion;knowledge management;remote sensing;supervised learning","preserving network;Pres-Net;transient network;Trans-Net;knowledge transfer;triple distillation mechanism;memorization;feature fusion;asymmetric separation networks;lifelong learning;remote sensing image scene classification;supervised learning;deep neural network models;DNN models;catastrophic forgetting;heterogeneous remote sensing;historical scenes;knowledge recall;asymmetric collaborative network;lifelong remote sensing image classification","","","","58","IEEE","12 Jul 2022","","","IEEE","IEEE Journals"
"AFNet: Adaptive Fusion Network for Remote Sensing Image Semantic Segmentation","R. Liu; L. Mi; Z. Chen","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2021","2021","59","9","7871","7886","Semantic segmentation of remote sensing images plays an important role in many applications. However, a remote sensing image typically comprises a complex and heterogenous urban landscape with objects in various sizes and materials, which causes challenges to the task. In this work, a novel adaptive fusion network (AFNet) is proposed to improve the performance of very high resolution (VHR) remote sensing image segmentation. To coherently label size-varied ground objects from different categories, we design multilevel architecture with the scale-feature attention module (SFAM). By SFAM, at the location of small objects, low-level features from the shallow layers of convolutional neural network (CNN) are enhanced, whilst for large objects, high-level features from deep layers are enhanced. Thus, the features of size-varied objects could be preserved during fusing features from different levels, which helps to label size-varied objects. As for labeling the category with high intra-class difference and varied scales, the multiscale structure with a scale-layer attention module (SLAM) is utilized to learn representative features, where an adjacent score map refinement module (ACSR) is employed as the classifier. By SLAM, when fusing multiscale features, based on the interested objects scale, feature map from appropriate scale is given greater weights. With such a scale-aware strategy, the learned features can be more representative, which is helpful to distinguish objects for semantic segmentation. Besides, the performance is further improved by introducing several nonlinear layers to the ACSR. Extensive experiments conducted on two well-known public high-resolution remote sensing image data sets show the effectiveness of our proposed model. Code and predictions are available at https://github.com/athauna/AFNet/","1558-0644","","10.1109/TGRS.2020.3034123","National Natural Science Foundation of China(grant numbers:62036005); National Key Research and Development Program of China(grant numbers:2018YFB0505500,2018YFB0505501); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9258402","Attention mechanism;convolutional neural network (CNN);semantic segmentation","Image segmentation;Simultaneous localization and mapping;Adaptive systems;Semantics;Optical imaging;Labeling;Convolutional neural networks","feature extraction;geophysical image processing;image classification;image fusion;image representation;image segmentation;learning (artificial intelligence);neural nets;object detection;object recognition;remote sensing;SLAM (robots)","AFNet;remote sensing image semantic segmentation;heterogenous urban landscape;novel adaptive fusion network;high resolution remote sensing image segmentation;label size-varied ground objects;scale-feature attention module;SFAM;low-level features;shallow layers;convolutional neural network;high-level features;deep layers;size-varied objects;fusing features;high intra-class difference;varied scales;scale-layer attention module;representative features;adjacent score map refinement module;multiscale features;interested objects scale;feature map;appropriate scale;scale-aware strategy;public high-resolution remote;image data sets","","16","","52","IEEE","12 Nov 2020","","","IEEE","IEEE Journals"
"Deep Multiscale Feedback Network for Hyperspectral Image Fusion","W. Wang; W. Zeng; Y. Huang; X. Ding","School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China; Fujian Key Laboratory of Sensing and Computing for Smart City, School of Information Science and Engineering, Xiamen University, Xiamen, China","IEEE Geoscience and Remote Sensing Letters","24 Dec 2021","2022","19","","1","5","Hyperspectral imaging is useful in many remote sensing tasks. However, it is often challenging to obtain high-resolution images in both the spatial and spectral domains due to hardware limitations. Hyperspectral image fusion (HIF) solves this problem by fusing a low spatial resolution hyperspectral image (LR-HSI) and a high spatial resolution multispectral images (HR-MSI) to obtain a high spatial resolution hyperspectral image (HR-HSI). Many methods have been proposed for HIF, but few approaches have explored the multiscale mutual dependencies between LR-HSI, HR-MSI, and HR-HSI. This kind of mutual dependencies come from the fact that LR-HSI, HR-MSI, and HR-HSI capture the same scene with different spatial or spectral resolutions. To this end, we propose a deep multiscale feedback network (DMFBN) that iteratively learns image fusion and degeneration for HIF. We further equip the network with an error feedback mechanism coupled with multiscale feature learning. Both strategies help better learn the mutual dependencies. Extensive quantitative and qualitative evaluations on two public datasets show that the proposed method performs favorably against the state-of-the-art (SOTA) methods.","1558-0571","","10.1109/LGRS.2021.3110204","National Natural Science Foundation of China(grant numbers:81671766,61571382,61571005,61172179,61103121); Science and Technology Key Project of Fujian Province, China(grant numbers:2019HZ020009); Fundamental Research Funds for the Central Universities(grant numbers:20720180059,20720160072); National Natural Science Foundation of Fujian Province, China(grant numbers:2017J01126); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547417","Back-project;deep learning;hyperspectral image (HSI);image fusion;pyramid structure","Image reconstruction;Spatial resolution;Task analysis;Hyperspectral imaging;Sparse matrices;Image fusion;Bayes methods","","","","","","34","IEEE","24 Sep 2021","","","IEEE","IEEE Journals"
"Aerial Scene Classification via Multilevel Fusion Based on Deep Convolutional Neural Networks","Y. Yu; F. Liu","Air Defense and Anti-missile College, Air Force Engineering University, Xi’an, China; Air Defense and Anti-missile College, Air Force Engineering University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","23 Jan 2018","2018","15","2","287","291","One of the challenging problems in understanding high-resolution remote sensing images is aerial scene classification. A well-designed feature extractor and classifier can improve classification accuracy. In this letter, we construct three different convolutional neural networks with different sizes of receptive field, respectively. More importantly, we further propose a multilevel fusion method, which can make judgment by incorporating different levels' information. The aerial image and two patches extracted from the image are fed to these three different networks, and then, a probability fusion model is established for final classification. The effectiveness of the proposed method is tested on a more challenging data set-AID that has 10000 high-resolution remote sensing images with 30 categories. Experimental results show that our multilevel fusion model gets a significant classification accuracy improvement over all state-of-the-art references.","1558-0571","","10.1109/LGRS.2017.2786241","National Natural Science Foundation of China(grant numbers:71701209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8248630","Aerial scene classification;convolutional neural network (CNN);multilevel fusion model;remote sensing","Feature extraction;Support vector machines;Training;Remote sensing;Analytical models;Convolutional codes;Semantics","feature extraction;feedforward neural nets;geophysical image processing;image classification;image fusion;image resolution;remote sensing","aerial scene classification;deep convolutional neural networks;well-designed feature extractor;classifier;multilevel fusion method;aerial image;probability fusion model;multilevel fusion model;high-resolution remote sensing images;data set-AID;classification accuracy improvement","","53","","33","IEEE","5 Jan 2018","","","IEEE","IEEE Journals"
"Registration of Multiresolution Remote Sensing Images Based on L2-Siamese Model","R. Fan; B. Hou; J. Liu; J. Yang; Z. Hong","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Water Resources and Hydro-Electric Engineering, Xi’an University of Technology, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Shaanxi Institute of Geological Survey, Xi’an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","237","248","The registration of multiresolution optical remote sensing images has been widely used in image fusion, change detection, and image stitching. However, traditional registration methods achieve poor accuracy in the registration of multiresolution remote sensing images. In this study, we propose a framework for generating deep features via a deep residual encoder (DRE) fused with shallow features for multiresolution remote sensing image registration. Through an L2 normalization Siamese network (L2-Siamese) based on the DRE, the multiscale loss function is used to learn the attribute characteristics and distance characteristics of two key points and obtain the trained feature extractor. Finally, the DRE is used to extract the deep features of the key points and their neighbors, which are concatenated with the shallow features into a fusion feature vector to complete the image registration. We performed comprehensive experiments on four sets of multiresolution optical remote sensing images and two sets of synthetic aperture radar images. The results demonstrate that the proposed registration model can achieve subpixel registration. The relative registration accuracy improved by 1.6%–7.5%, whereas the overall performance improved by 4.5%–14.1%.","2151-1535","","10.1109/JSTARS.2020.3038922","Key Research and Development Plan of Shaanxi Province(grant numbers:D5140200023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264687","Deep descriptors;L2-Siamese;multiresolution image registration;residual encoder;satellite remote sensing;Siamese network","Image registration;Image resolution;Feature extraction;Optical imaging;Adaptive optics;Optical sensors;Remote sensing","feature extraction;geophysical image processing;image fusion;image registration;image resolution;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","registration model;synthetic aperture radar images;fusion feature vector;trained feature extractor;L2 normalization Siamese network;image registration;shallow features;DRE;deep features;traditional registration methods;image stitching;image fusion;multiresolution optical remote sensing images;L2-Siamese Model;multiresolution remote sensing images;relative registration accuracy;subpixel registration","","11","","41","CCBY","19 Nov 2020","","","IEEE","IEEE Journals"
"Multiscale Semantic Fusion-Guided Fractal Convolutional Object Detection Network for Optical Remote Sensing Imagery","T. Zhang; Y. Zhuang; G. Wang; S. Dong; H. Chen; L. Li","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Engineering Center of Digital Audio and Video, Communication University of China, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; School of Electronic Engineering and Computer Science, Peking University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","20","Optical remote sensing object detection is a challenging task, because of the complex background interference, ambiguous appearances of tiny objects, densely arranged circumstances, and multiclass object with vaster scale variances and irregular aspect ratios. The performance of object detection is seriously restricted. Thus, in this article, inspired by the anchor-free object detection framework, and aiming to solve these difficulties to improve the optical remote sensing object detection performance, a powerful one-stage detector of multiscale semantic fusion-guided fractal convolution network (MSFC-Net) is proposed. First, facing these strong-coupled semantic relations in each complex scene, a compound semantic feature fusion (CSFF) way is designed for generating an effective semantic description, which is a benefit to pixel-wise object center point interpretation. In addition, it can be easily extended into a semantic segmentation task. Second, in view of accurate multiclass pixel-wise center point predictions based on an effective compound semantic description, a novel fractal convolution (FC) regression layer is designed, which adaptively achieves the regression of multiscale bounding boxes (bboxes) with irregular aspect ratio under no priori information. Third, related to the set up FC regression layer, a specific hybrid loss is designed to make the proposed MSFC-Net converge better. Finally, the extensive experiments on challenge data sets of large-scale dataset for object detection in aerial images (DOTA) and object detection in optical remote sensing images (DIOR) datasets are carried out, and comparisons indicate that the proposed MSFC-Net can perform the remarkable performance than other state-of-the-art one-stage detectors, as it can reach 80.26% mean average precision (mAP) and 79.33% mF1 on DOTA and 70.08% mAP and 73.45% mF1 on DIOR. Then, our work is available at https://github.com/ZhAnGToNG1/MSFC-Net.","1558-0644","","10.1109/TGRS.2021.3108476","Chang Jiang Scholars Program(grant numbers:T2012122); Civil Aviation Program(grant numbers:B0201); Space Based Real-Time Processing Technology(grant numbers:2018-JCJQ-ZQ-046); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535169","Anchor-free;compound semantic description;fractal convolution (FC);multiscale feature fusion;object detection;optical remote sensing","Object detection;Detectors;Semantics;Remote sensing;Optical imaging;Optical detectors;Feature extraction","convolutional neural nets;feature extraction;fractals;geophysical image processing;image classification;image fusion;image segmentation;object detection;object recognition;regression analysis;remote sensing","multiscale semantic fusion-guided fractal convolutional object detection network;optical remote sensing imagery;multiclass object;anchor-free object detection framework;optical remote sensing object detection performance;MSFC-Net;compound semantic feature fusion way;pixel-wise object center point interpretation;semantic segmentation task;compound semantic description;fractal convolution regression layer;optical remote sensing images datasets;mean average precision;mAP;DOTA;multiscale bounding boxes;DIOR;FC regression layer;CSFF","","9","","107","IEEE","10 Sep 2021","","","IEEE","IEEE Journals"
"F-UNet++: Remote Sensing Image Fusion Based on Multipurpose Adaptive Shuffle Attention and Composite Multi-Input Reconstruction Network","X. Jin; P. Zhang; Q. Jiang; S. Miao; S. Yao; W. Zhou","Engineering Research Center of Cyberspace and the School of Software, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace and the School of Software, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace and the School of Software, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace and the School of Software, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace and the School of Software, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace and the School of Software, Yunnan University, Kunming, China","IEEE Transactions on Instrumentation and Measurement","18 Jan 2023","2023","72","","1","15","The fusion of multispectral (MS) and panchromatic (PAN) images is of great significance for the construction of high-resolution remote sensing images. Because of differences in sensors, no single MS or PAN image can express the complete information of a scene. Therefore, it is a key issue to fuse MS images containing rich spectral content and PAN images with spatial information to construct a high-resolution MS image. In this work, an adaptive shuffle attention (ASA) module and an optimized UNet++ are combined in a fusion-UNet++ (F-UNet++) framework for the problem of MS and PAN image fusion. This ASA module can focus on important information in the mixed domain and adjust the dimensions of tensors. F-UNet++ includes a multiscale feature extraction module, multiscale feature fusion module, and image reconstruction module. The multiscale feature extraction module obtains spectral and spatial information, the multiscale feature fusion module fuses spectral and spatial information, and a composite multi-input image reconstruction module (CMI-UNet++) reconstructs the final image. By combining the ASA attention module, the loss of feature information can be reduced to enhance the fidelity of the spectral and spatial information of the fused image. Experiments show that F-UNet++ is qualitatively and quantitatively superior to current image fusion methods. (The code is available at https://github.com/Josephing/F-UNet).","1557-9662","","10.1109/TIM.2022.3229725","National Natural Science Foundation of China(grant numbers:62101481,62002313,62261060,62166047,62162067,62101480); Basic Research Project of Yunnan Province(grant numbers:202201AU070033,202201AT070112,202001BB050076,202005AC160007); Major Scientific and Technological Project of Yunnan Province(grant numbers:202202AD080002); Key Laboratory in Software Engineering of Yunnan Province(grant numbers:2020SE408); Science Research Foundation Project of Yunnan Education Department(grant numbers:2021Y025,2021Y406); Research and Application of Object Detection Based on Artificial Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989428","Attention mechanism;convolutional neural networks;image fusion;multispectral (MS) images;pan sharpening;panchromatic (PAN) images","Spatial resolution;Image reconstruction;Feature extraction;Sensors;Adaptation models;Tensors;Image sensors","feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing;spectral analysis","adaptive shuffle attention module;ASA module;CMI-UNet++;composite multiinput reconstruction network;current image fusion methods;F-UNet;feature information;fused image;fusion-UNet++;high-resolution MS image;high-resolution remote sensing images;MS images;multiinput image reconstruction module;multipurpose adaptive shuffle attention;multiscale feature extraction module;multiscale feature fusion module fuses spectral;PAN;rich spectral content;spatial information;spectral information","","","","63","IEEE","15 Dec 2022","","","IEEE","IEEE Journals"
"MLR-DBPFN: A Multi-Scale Low Rank Deep Back Projection Fusion Network for Anti-Noise Hyperspectral and Multispectral Image Fusion","W. Sun; K. Ren; X. Meng; G. Yang; C. Xiao; J. Peng; J. Huang","Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; Hubei Key Laboratory of Applied Mathematics, Faculty of Mathematics and Statistics, Hubei University, Wuhan, China; Key Laboratory of Environment Remediation and Ecological Health, Ministry of Education, College of Natural Resources and Environmental Science, Zhejiang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","18 Mar 2022","2022","60","","1","14","Fusing low spatial resolution (LR) hyperspectral (HS) data and high spatial resolution (HR) multispectral (MS) data aims to obtain HR HS data. However, due to bad weather and the aging of sensor equipment, HS images usually contain a lot of noise, e.g., Gaussian noise, strip noise, and mixed noise, which would make the fused image have low quality. To solve this problem, we propose the multiscale low-rank deep back projection fusion network (MLR-DBPFN). First, HS and MS are superimposed, and multiscale spectral features of the stacked image are extracted through multiscale low-rank decomposition and convolution operation, which effectively removes noisy spectral features. Second, the upsampling and downsampling network mechanisms are used to extract the multiscale spatial features from each layer of spectral features. Finally, the multiscale spectral features and multiscale spatial features are combined for network training, and the weight of the noisy spectrum features is reduced through the network feedback mechanism, which suppresses the noisy spectrum and improves the noisy HS fusion performance. Experimental results on datasets of different noise demonstrate that MLR-DBPFN has superior spatial and spectral fidelity, comparative fusion quality, and robust antinoise performance compared with state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3146296","National Natural Science Foundation of China(grant numbers:42122009,42171351,41971296,61871177,42171326,41801256,41801252); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR19D010001,LQ18D010001,LY22F010014); Hubei Provincial Natural Science Foundation of China(grant numbers:2021CFA087); Public Projects of Ningbo City(grant numbers:2021S089); Fundamental Research Funds for the Provincial Universities of Zhejiang(grant numbers:SJLZ2022002); Science and Technology Project for Department of Natural Resources of Zhejiang Province(grant numbers:2021-30,2021-31); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691361","Antinoise;deep learning;hyperspectral (HS);image fusion;multiscale low rank (MLR);multispectral (MS)","Feature extraction;Noise measurement;Image fusion;Spatial resolution;Data integration;Noise reduction;Matrix decomposition","Gaussian noise;geophysical image processing;hyperspectral imaging;image fusion;image resolution;spectral analysis;wavelet transforms","stacked image;low-rank decomposition;convolution operation;noisy spectral features;upsampling network mechanisms;downsampling network mechanisms;multiscale spatial features;multiscale spectral features;network training;noisy spectrum features;network feedback mechanism;MLR-DBPFN;spatial fidelity;spectral fidelity;comparative fusion quality;multiscale low rank deep back projection fusion network;multispectral image;low spatial resolution hyperspectral data;high spatial resolution multispectral data;HR HS data;HS images;Gaussian noise;strip noise;mixed noise;fused image;low-rank deep back projection fusion network;anti-noise hyperspectral image;HS fusion performance","","8","","45","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"An Efficient Cross-Modality Self-Calibrated Network for Hyperspectral and Multispectral Image Fusion","H. Wu; J. Gui; Y. Xu; Z. Wu; Y. Y. Tang; Z. Wei","School of Computer Science, Nanjing Audit University, Nanjing, China; School of Cyber Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Zhuhai UM Science and Technology Research Institute, Faculty of Science and Technology, University of Macau, Macau, China; School of Science, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","20 Dec 2022","2022","60","","1","12","Recently, deep convolutional neural network (CNN)-based hyperspectral and multispectral image fusion methods have shown significant performance. Nevertheless, the rich spatial and spectral details of hyperspectral images (HSIs) have not been fully explored, leaving room for further improve the representation ability of the model. In this article, we propose an efficient cross-modality self-calibrated network (CMSCN) for hyperspectral and multispectral image fusion. Specifically, we use a cross-modality nonlocal module (NL) to fuse a high-resolution multispectral image (HR-MSI) and a low-resolution hyperspectral image (LR-HSI) to get an enhanced LR-HSI. In addition, a novel cross-scale self-calibrated convolution structure is proposed to explore and exploit multiscale and hierarchical spatial-spectral features, which can improve the learning ability of the model. The introduced efficient spatial-spectral attention mechanism can calibrate the feature representation at different dimensions, thereby providing more efficient and accurate information for HSI reconstruction. Extensive experimental results on various HSIs demonstrate the superiority of our method in comparison with the state-of-the-art image fusion methods.","1558-0644","","10.1109/TGRS.2022.3225577","Research Project of University Natural Science Fund of Jiangsu Province(grant numbers:22KJB520002); National Natural Science Foundation of China(grant numbers:61971223,61976117,62071233); Natural Science Foundation of Jiangsu Province(grant numbers:BK20191409,BK20211570); Postgraduate Research Practice Innovation Program of Jiangsu Province(grant numbers:KYCX22_2218); National Natural Science Foundation of China(grant numbers:62172090,62172458); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9966811","Attention mechanism;convolutional neural network (CNN);hyperspectral and multispectral image fusion;multiscale features","Hyperspectral imaging;Convolution;Superresolution;Image fusion;Spatial resolution;Convolutional neural networks;Feature extraction","calibration;convolution;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;image resolution;learning (artificial intelligence);neural nets","cross-modality nonlocal module;deep convolutional neural network-based hyperspectral;efficient cross-modality;efficient information;enhanced LR-HSI;high-resolution multispectral image;hyperspectral images;low-resolution hyperspectral image;multispectral image fusion methods;novel cross-scale self;representation ability;rich spatial;self-calibrated network;spatial-spectral attention mechanism;spatial-spectral features;spectral details;state-of-the-art image fusion methods","","","","43","IEEE","30 Nov 2022","","","IEEE","IEEE Journals"
"Spatial–Spectral-Graph-Regularized Low-Rank Tensor Decomposition for Multispectral and Hyperspectral Image Fusion","K. Zhang; M. Wang; S. Yang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi'an, China; National Key Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","10 Apr 2018","2018","11","4","1030","1040","Hyperspectral (HS) and multispectral (MS) image fusion aims at producing high-resolution HS (HRHS) images. However, the existing methods could not simultaneously consider the structures in both the spatial and spectral domains of the HS cube. In order to effectively preserve spatial–spectral structures in HRHS images, we propose a new low-resolution HS (LRHS) and high-resolution MS (HRMS) image fusion method based on spatial–spectral-graph-regularized low-rank tensor decomposition (SSGLRTD) in this paper. First, we reformulate the image fusion problem as a low-rank tensor decomposition model to utilize the low-rank property in the HS image. Then, two graphs are constructed in spatial and spectral domains, respectively. One of them is derived from the HRMS image for the spatial consistency, and the other is inferred from the LRHS image for spectral smoothness. Finally, the SSGLRTD fusion framework is established by combining all regularizers. With these two graphs, the spatial correlation and the spectral structure in the fused HRHS images are efficiently preserved. The experimental results on different datasets reveal that the proposed fusion method outperforms several existing fusion methods in terms of visual analysis and numerical comparison.","2151-1535","","10.1109/JSTARS.2017.2785411","National Basic Research Program of China (973 Program)(grant numbers:2013CB329402); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438103,91438201); Fundamental Research Funds for the Central Universities(grant numbers:BDY021429); Foreign Scholars in University Research and Teaching Programs(grant numbers:B07048); Science Basis Research Program in Shaanxi Province of China(grant numbers:16JK1823); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253497","Hyperspectral (HS) image;image fusion;low-rank tensor decomposition;multispectral (MS) image;spatial graph regularization","Tensile stress;Image fusion;Spatial resolution;Matrix decomposition;Spectral analysis;Principal component analysis;Manifolds","geophysical image processing;graph theory;hyperspectral imaging;image fusion;image resolution;tensors","multispectral image fusion;hyperspectral image fusion;high-resolution HS images;HS cube;spatial-spectral structures;image fusion problem;low-rank tensor decomposition model;low-rank property;HS image;graphs;LRHS image;HRHS image fusion;SSGLRTD fusion;low-rank tensor decomposition;spatial-spectral-graph regularization","","78","","48","IEEE","10 Jan 2018","","","IEEE","IEEE Journals"
"Similarity Weight Learning: A New Spatial and Temporal Satellite Image Fusion Framework","H. Sun; W. Xiao","School of Geosciences and Surveying Engineering, China University of Mining and Technology, Beijing, China; School of Public Affairs, Zhejiang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","14 Apr 2022","2022","60","","1","17","Spatiotemporal fusion is a topical framework for solving the mutual restricted problem between the spatial and temporal resolution of satellite images. We pioneer an approach to replace similarity measurement steps in spatiotemporal fusion algorithms with convolutional neural networks (CNNs), building a bridge between weight function-based models and the learning-based models. Specifically, we propose a nonlocal form that separates the relational computation part from the value representation part, and construct the CNN-based similarity weight learning block for learning normalized weights. The block can be inserted into spatial and temporal adaptive reflectance fusion model (STARFM) to replace the manually designed weight calculation rules common in weight function-based methods, or into the CNN model StfNet to better utilize neighboring high-resolution images. The trained model outputs a high-resolution prediction from each base date image pair. The final result is a combination of the two predictions. In this regard, we propose the standard deviation-based weights to combine two prediction results. Four experiments are performed on Landsat–Moderate-resolution Imaging Spectroradiometer (MODIS) image pairs to determine the following: 1) the performance of the model at the target training date; 2) the generalization of the model in the target training time period; and 3) the generalization of the model at different dates and different geographical locations, each considering the different cases of giving one and two pairs of known images. Experimental results demonstrate the superiority of the similarity weight learning block and standard deviation-based weights. Among them, STARFM with the similarity weight learning block exhibits strong generalization, which testifies to the practical value of our model.","1558-0644","","10.1109/TGRS.2022.3161070","National Natural Science Foundation of China(grant numbers:42071250); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739013","Attention;generalization capability;nonlocal module;satellite image fusion;similarity measure;spatiotemporal fusion","Spatial resolution;Image resolution;Spatiotemporal phenomena;Reflectivity;Biological system modeling;Remote sensing;Data models","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing;sensor fusion","similarity weight learning;new spatial satellite image fusion framework;temporal satellite image fusion framework;topical framework;mutual restricted problem;spatial resolution;temporal resolution;satellite images;similarity measurement steps;spatiotemporal fusion algorithms;convolutional neural networks;weight function-based models;learning-based models;relational computation part;value representation part;CNN-based similarity weight;normalized weights;temporal adaptive reflectance fusion model;manually designed weight calculation rules;weight function-based methods;CNN model StfNet;high-resolution images;trained model;high-resolution prediction;base date image pair;standard deviation-based weights;Landsat-Moderate-resolution Imaging Spectroradiometer image pairs;target training date;target training time period;known images","","","","48","IEEE","21 Mar 2022","","","IEEE","IEEE Journals"
"Challenges and Opportunities of Multimodality and Data Fusion in Remote Sensing","M. Dalla Mura; S. Prasad; F. Pacifici; P. Gamba; J. Chanussot; J. A. Benediktsson","Université Grenoble Alpes, GIPSA-Lab, Grenoble, France; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; DigitalGlobe Inc., CO, Boulder, CO, USA; University of Pavia, Pavia, Italy; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","Proceedings of the IEEE","20 Aug 2015","2015","103","9","1585","1601","Remote sensing is one of the most common ways to extract relevant information about Earth and our environment. Remote sensing acquisitions can be done by both active (synthetic aperture radar, LiDAR) and passive (optical and thermal range, multispectral and hyperspectral) devices. According to the sensor, a variety of information about the Earth's surface can be obtained. The data acquired by these sensors can provide information about the structure (optical, synthetic aperture radar), elevation (LiDAR), and material content (multispectral and hyperspectral) of the objects in the image. Once considered together their complementarity can be helpful for characterizing land use (urban analysis, precision agriculture), damage detection (e.g., in natural disasters such as floods, hurricanes, earthquakes, oil spills in seas), and give insights to potential exploitation of resources (oil fields, minerals). In addition, repeated acquisitions of a scene at different times allows one to monitor natural resources and environmental variables (vegetation phenology, snow cover), anthropological effects (urban sprawl, deforestation), climate changes (desertification, coastal erosion), among others. In this paper, we sketch the current opportunities and challenges related to the exploitation of multimodal data for Earth observation. This is done by leveraging the outcomes of the data fusion contests, organized by the IEEE Geoscience and Remote Sensing Society since 2006. We will report on the outcomes of these contests, presenting the multimodal sets of data made available to the community each year, the targeted applications, and an analysis of the submitted methods and results: How was multimodality considered and integrated in the processing chain? What were the improvements/new opportunities offered by the fusion? What were the objectives to be addressed and the reported solutions? And from this, what will be the next challenges?","1558-2256","","10.1109/JPROC.2015.2462751","European project(grant numbers:ERC-2012-AdG-320684-CHESS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194740","Change detection (CD);classification;data fusion (DF);pansharpening;remote sensing;Change detection (CD);classification;data fusion (DF);pansharpening;remote sensing","Remote sensing;Data integration;Spatial resolution;Laser radar;Synthetic aperture radar;Optical sensors;Multimodal sensors;Climate change","data acquisition;geophysical image processing;hyperspectral imaging;image fusion;optical radar;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar","oil field exploitation;mineral exploitation;natural resource monitoring;environmental variable;vegetation phenology;snow cover;anthropological effect;urban sprawl;deforestation;climate change;desertification;coastal erosion;multimodal data exploitation;Earth observation;IEEE Geoscience and Remote Sensing Society;method analysis;resource exploitation;oil spill;earthquake;hurricane;flood;natural disaster;damage detection;precision agriculture;urban analysis;land use characterization;optical radar;Earth surface;hyperspectral imaging;multispectral imaging;thermal range;optical range;passive devices;LiDAR;synthetic aperture radar;remote sensing acquisition;environmental information extraction;Earth information extraction;data fusion;remote sensing multimodality","","132","","90","IEEE","13 Aug 2015","","","IEEE","IEEE Journals"
"Embedded Self-Distillation in Compact Multibranch Ensemble Network for Remote Sensing Scene Classification","Q. Zhao; Y. Ma; S. Lyu; L. Chen","Department of Electronics and Information Engineering, Beihang University, Beijing, China; Department of Electronics and Information Engineering, Beihang University, Beijing, China; Department of Electronics and Information Engineering, Beihang University, Beijing, China; Department of Electronics and Information Engineering, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","15 Feb 2022","2022","60","","1","15","Remote sensing image classification task is challenging due to the characteristics of complex composition, so different geographic elements in the same image will interfere with each other, resulting in misclassification. To solve this problem, we propose a multibranch ensemble network to enhance the feature representation ability by fusing final output logits and intermediate feature maps. However, simply adding branches will increase the complexity of models and decline the inference efficiency. To reduce the complexity of multibranch network, we make multibranch share more weights and add feature augmentation modules to compensate for the lack of diversity caused by weight sharing. To improve the efficiency of inference, we embed self-distillation (SD) method to transfer knowledge from ensemble network to main branch. Through optimizing with SD, the main branch will have close performance as an ensemble network. In this way, we can cut other branches during inference. In addition, we simplify the process of SD and totally adopt two loss functions to self-distill the logits and feature maps. In this article, we design a compact multibranch ensemble network, which can be trained in an end-to-end manner. Then, we insert an SD method on output logits and feature maps. Our proposed architecture (ESD-MBENet) performs strongly on classification accuracy with compact design. Extensive experiments are applied on three benchmark remote sensing datasets, AID, NWPU-RESISC45, and UC-Merced with three classic baseline models, VGG16, ResNet50, and DenseNet121. Results prove that ESD-MBENet can achieve better accuracy than previous state-of-the-art complex deep learning models. Moreover, abundant visualization analyses make our method more convincing and interpretable.","1558-0644","","10.1109/TGRS.2021.3126770","National Natural Science Foundation of China(grant numbers:62072021); Fundamental Research Funds for the Central Universities(grant numbers:YWF-21-BJ-J-534); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606819","Multibranch ensemble network;network pruning;remote sensing scene classification;self-distillation (SD)","Remote sensing;Feature extraction;Image classification;Task analysis;Deep learning;Knowledge engineering;Sensors","data visualisation;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image representation;remote sensing","final output logit-intermediate feature map fusion;weight sharing;compact multibranch ensemble network;SD method;benchmark remote sensing datasets;complex deep learning models;embedded self-distillation;remote sensing scene classification;remote sensing image classification task;geographic elements;feature representation ability;inference efficiency;feature augmentation modules;ESD-MBENet;AID dataset;NWPU-RESISC45 dataset;UC-Merced dataset;VGG16 model;ResNet50 model;DenseNet121 model;visualization analyses","","1","","60","CCBY","8 Nov 2021","","","IEEE","IEEE Journals"
"Pan-Sharpening Framework Based on Multiscale Entropy Level Matching and Its Application","J. Tao; C. Song; D. Song; X. Wang","National Marine Environmental Monitoring Center, Dalian, China; School of Computer and Information Technology, Liaoning Normal University, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; School of Geography and the School of Computer and Information Technology, Liaoning Normal University, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","19 Aug 2022","2022","60","","1","21","Current remote sensing hardware technology is not yet able to acquire multiband remote sensing images with both high spatial and spectral resolution. As an important tool to compensate for the lack of spatial information acquisition of multispectral (MS) images, pan-sharpening has been an important and continuously active research area in remote sensing image processing. Although many methods have emerged, the problem of how to obtain high spatial resolution while effectively maintaining the spectral information of MS images has not been well solved. Many aspects still need further research. In this article, we first investigate the essential properties and rationality of two common framework types in the multiresolution analysis (MRA) sharpening method of pan-sharpening from the source perspective—the identical-resolution framework (IRF) derived from the generalized fusion application and the different-resolution framework (DRF) exclusive to the sharpening application, and show that the core difference between the two frameworks lies in the different ideas of utilizing the multiscale transformation, i.e., they tend to expand the scale space and model the spatially blurred degradation relationship between the sources, respectively. Both of them have their own advantages and disadvantages in handling detailed information, and neither of them can effectively deal with the “detail exclusivity” problem. Based on this, the idea of “entropy level matching” (ELM) of pan-sharpening is presented, and a comprehensive framework that can combine the advantages of the two types of frameworks is constructed, namely, the multiscale ELM framework. Furthermore, as an application of this framework, we propose a sharpening method shearlet transform-based entropy matching (STEM) built on the nonsubsampled shearlet as a multiscale transformation method. According to the difference in detail injection mode in it, it can be further divided into two sharpening methods based on additive mode and substitutive mode. The comparison experiments with 11 popular methods show that the proposed two sharpening methods can effectively improve the spatial resolution of MS images while keeping the spectral information well, and the comprehensive performance advantage is obvious. The source code of the proposed method can be downloaded from https://github.com/JZ-Tao/STEM/.","1558-0644","","10.1109/TGRS.2022.3198097","National Natural Science Foundation of China(grant numbers:41971388); Innovation Team Support Program of Liaoning Higher Education Department(grant numbers:LT2017013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856682","Image fusion;multiresolution analysis (MRA);multiscale entropy level matching (ELM);pan-sharpening","Image fusion;Remote sensing;Entropy;Spatial resolution;Hafnium;Additives;Wavelet transforms","edge detection;entropy;filtering theory;geophysical image processing;image denoising;image enhancement;image fusion;image processing;image resolution;remote sensing;transforms;wavelet transforms","pan-sharpening framework;multiscale entropy level matching;current remote sensing hardware technology;multiband remote sensing images;high spatial resolution;spectral resolution;spatial information acquisition;multispectral images;important research area;continuously active research area;remote sensing image processing;spectral information;MS images;common framework types;multiresolution analysis sharpening method;identical-resolution framework;generalized fusion application;different-resolution framework;sharpening application;core difference;different ideas;spatially blurred degradation relationship;detail exclusivity problem;multiscale ELM framework;sharpening method shearlet;multiscale transformation method;sharpening methods;11 popular methods","","1","","83","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion via Nonlocal Low-Rank Tensor Decomposition and Spectral Unmixing","K. Wang; Y. Wang; X. -L. Zhao; J. C. -W. Chan; Z. Xu; D. Meng","Center for Intelligent Decision-making and Machine Learning, School of Management, Xi’an Jiaotong University, Xi’an, China; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Department of Electronics and Informatics, Vrije Universiteit Brussel, Brussels, Belgium; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Macau Institute of Systems Engineering, Macau University of Science and Technology, Taipa, Macau","IEEE Transactions on Geoscience and Remote Sensing","26 Oct 2020","2020","58","11","7654","7671","Hyperspectral (HS) imaging has shown its superiority in many real applications. However, it is usually difficult to obtain high-resolution (HR) HS images through existing imaging techniques due to the hardware limitations. To improve the spatial resolution of HS images, this article proposes an effective HS-multispectral (HS-MS) image fusion method by combining the ideas of nonlocal low-rank tensor modeling and spectral unmixing. To be more precise, instead of unfolding the HS image into a matrix as done in the literature, we directly represent it as a tensor, then a designed nonlocal Tucker decomposition is used to model its underlying spatial-spectral correlation and the spatial self-similarity. The MS image serves mainly as a data constraint to maintain spatial consistency. To further reduce the spectral distortions in spatial enhancement, endmembers, and abundances from the spectral are used for spectral regularization. An efficient algorithm based on the alternating direction method of multipliers (ADMM) is developed to solve the resulting model. Extensive experiments on four HS image data sets demonstrate the superiority of the proposed method over several state-of-the-art HS-MS image fusion methods.","1558-0644","","10.1109/TGRS.2020.2983063","National Key Research and Development Program of China(grant numbers:2018YFB1402600); National Natural Science Foundation of China(grant numbers:11971374,11501440,61603292,91846110,61876203); Fundamental Research Funds for the Central Universities(grant numbers:xjj2018085); MoE-CMCC Artifical Intelligence Project(grant numbers:MCM20190701); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070159","Hyperspectral (HS) image;image fusion;nonlocal tensor decomposition;spatial enhancement;spectral unmixing","Tensile stress;Image fusion;Sparse matrices;Spatial resolution;Correlation;Imaging;Machine learning","hyperspectral imaging;image enhancement;image fusion;image resolution;image segmentation;matrix decomposition;tensors","image segmentation;alternating direction method of multipliers;multispectral image fusion;nonlocal Tucker decomposition;spatial spectral correlation;HS-MS image fusion;spatial enhancement;spectral distortions;spatial resolution;hyperspectral image resolution;spectral unmixing;low rank tensor decomposition","","17","","81","IEEE","17 Apr 2020","","","IEEE","IEEE Journals"
"UMAG-Net: A New Unsupervised Multiattention-Guided Network for Hyperspectral and Multispectral Image Fusion","S. Liu; S. Miao; J. Su; B. Li; W. Hu; Y. -D. Zhang","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Medical Engineering of Hebei Province, College of Electronic and Information Engineering, Hebei University, Baoding, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Department of Informatics, University of Leicester, Leicester, U.K.","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Aug 2021","2021","14","","7373","7385","To reconstruct images with high spatial resolution and high spectral resolution, one of the most common methods is to fuse a low-resolution hyperspectral image (HSI) with a high-resolution (HR) multispectral image (MSI) of the same scene. Deep learning has been widely applied in the field of HSI-MSI fusion, which is limited with hardware. In order to break the limits, we construct an unsupervised multiattention-guided network named UMAG-Net without training data to better accomplish HSI-MSI fusion. UMAG-Net first extracts deep multiscale features of MSI by using a multiattention encoding network. Then, a loss function containing a pair of HSI and MSI is used to iteratively update parameters of UMAG-Net and learn prior knowledge of the fused image. Finally, a multiscale feature-guided network is constructed to generate an HR-HSI. The experimental results show the visual and quantitative superiority of the proposed method compared to other methods.","2151-1535","","10.1109/JSTARS.2021.3097178","Natural Science Foundation of Hebei Province(grant numbers:F2020201025,F2019201151,F2018210148); Science Research Project of Hebei Province(grant numbers:BJ2020030,QN2017306); National Natural Science Foundation of China(grant numbers:61572063,62172003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484840","Deep learning;hyperspectral images (HSIs);image fusion;multispectral images (MSIs)","Tensors;Image fusion;Hyperspectral imaging;Spatial resolution;Feature extraction;Image reconstruction;Dictionaries","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;iterative methods;remote sensing","hyperspectral image fusion;multispectral image fusion;high spatial resolution;high spectral resolution;low-resolution hyperspectral image;high-resolution multispectral image;deep learning;HSI-MSI fusion;multiattention encoding network;image fusion;multiscale feature-guided network;UMAG-Net;unsupervised multiattention-guided network;image reconstruction;iterative method","","11","","51","CCBYNCND","14 Jul 2021","","","IEEE","IEEE Journals"
"Super-Resolution for Hyperspectral and Multispectral Image Fusion Accounting for Seasonal Spectral Variability","R. A. Borsoi; T. Imbiriba; J. C. M. Bermudez","Lagrange Laboratory, Université Côte d’Azur, Nice, France; Department of Electrical Engineering, Federal University of Santa Catarina (DEE-UFSC), Florianópolis, Brazil; Department of Electrical Engineering, Federal University of Santa Catarina (DEE-UFSC), Florianópolis, Brazil","IEEE Transactions on Image Processing","12 Sep 2019","2020","29","","116","127","Image fusion combines data from different heterogeneous sources to obtain more precise information about an underlying scene. Hyperspectral-multispectral (HS-MS) image fusion is currently attracting great interest in remote sensing since it allows the generation of high spatial resolution HS images and circumventing the main limitation of this imaging modality. Existing HS-MS fusion algorithms, however, neglect the spectral variability often existing between images acquired at different time instants. This time difference causes variations in spectral signatures of the underlying constituent materials due to the different acquisition and seasonal conditions. This paper introduces a novel HS-MS image fusion strategy that combines an unmixing-based formulation with an explicit parametric model for typical spectral variability between the two images. Simulations with synthetic and real data show that the proposed strategy leads to a significant performance improvement under spectral variability and state-of-the-art performance otherwise.","1941-0042","","10.1109/TIP.2019.2928895","National Council for Scientific and Technological Development (CNPq)(grant numbers:304250/2017-1,409044/2018-0,141271/2017-5,204991/2018-8); Conselho Nacional de Desenvolvimento Científico e Tecnológico(grant numbers:PNPD/1811213); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8768351","Hyperspectral data;multispectral data;endmember variability;seasonal variability;super-resolution;image fusion","Image fusion;Spatial resolution;Sensors;Hyperspectral sensors;Image sensors;Atmospheric modeling","geophysical image processing;hyperspectral imaging;image fusion;image resolution;remote sensing","super-resolution;seasonal spectral variability;hyperspectral-multispectral image fusion;high spatial resolution HS images;imaging modality;HS-MS fusion algorithms;time difference;spectral signatures;different acquisition;seasonal conditions;novel HS-MS image fusion strategy;typical spectral variability;heterogeneous sources;time instants;constituent materials;remote sensing;unmixing-based formulation;explicit parametric model","","57","","45","IEEE","22 Jul 2019","","","IEEE","IEEE Journals"
"StfNet: A Two-Stream Convolutional Neural Network for Spatiotemporal Image Fusion","X. Liu; C. Deng; J. Chanussot; D. Hong; B. Zhao","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, School of Informationand Electronics, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, School of Informationand Electronics, Beijing Institute of Technology, Beijing, China; Univ. Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Grenoble, France; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, School of Informationand Electronics, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","6552","6564","Spatiotemporal image fusion is considered as a promising way to provide Earth observations with both high spatial resolution and frequent coverage, and recently, learning-based solutions have been receiving broad attention. However, these algorithms treating spatiotemporal fusion as a single image super-resolution problem, generally suffers from the significant spatial information loss in coarse images, due to the large upscaling factors in real applications. To address this issue, in this paper, we exploit temporal information in fine image sequences and solve the spatiotemporal fusion problem with a two-stream convolutional neural network called StfNet. The novelty of this paper is twofold. First, considering the temporal dependence among image sequences, we incorporate the fine image acquired at the neighboring date to super-resolve the coarse image at the prediction date. In this way, our network predicts a fine image not only from the structural similarity between coarse and fine image pairs but also by exploiting abundant texture information in the available neighboring fine images. Second, instead of estimating each output fine image independently, we consider the temporal relations among time-series images and formulate a temporal constraint. This temporal constraint aiming to guarantee the uniqueness of the fusion result and encourages temporal consistent predictions in learning and thus leads to more realistic final results. We evaluate the performance of the StfNet using two actual data sets of Landsat-Moderate Resolution Imaging Spectroradiometer (MODIS) acquisitions, and both visual and quantitative evaluations demonstrate that our algorithm achieves state-of-the-art performance.","1558-0644","","10.1109/TGRS.2019.2907310","National Natural Science Foundation of China(grant numbers:91438203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693668","Convolutional neural network;spatiotemporal image fusion;super-resolution;temporal consistency;temporal dependence (TD)","Spatial resolution;Spatiotemporal phenomena;Remote sensing;Earth;Convolutional neural networks;Image fusion","convolutional neural nets;image fusion;image resolution;image sequences;image texture;learning (artificial intelligence);spatiotemporal phenomena","neighboring fine images;quantitative evaluations;visual evaluations;structural similarity;Landsat-Moderate Resolution Imaging Spectroradiometer acquisitions;temporal consistent predictions;temporal constraint;time-series images;output fine image;spatiotemporal fusion problem;fine image sequences;temporal information;coarse image;significant spatial information loss;single image super-resolution problem;high spatial resolution;spatiotemporal image fusion;two-stream convolutional neural network;StfNet","","101","","41","IEEE","17 Apr 2019","","","IEEE","IEEE Journals"
"Tensor Regression and Image Fusion-Based Change Detection Using Hyperspectral and Multispectral Images","T. Zhan; Y. Sun; Y. Tang; Y. Xu; Z. Wu","School of Information Engineering, Nanjing Audit University, Nanjing, China; School of Information Engineering, Nanjing Audit University, Nanjing, China; School of Information Engineering, Nanjing Audit University, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China; Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","8 Oct 2021","2021","14","","9794","9802","Change detection is a popular topic in remote sensing that is generally constrained to two remote sensing images captured at two different times. However, the optimal type of remote sensing image for change detection tasks has not yet been determined. The use of only hyperspectral images (HSIs) with low spatial resolution or multispectral images (MSIs) with low spectral resolution cannot obtain satisfactory change detection results. In this article, we propose the fusion of simultaneously captured low spatial resolution HSIs and low spectral resolution MSIs with the use of a tensor regression-based method to detect change regions from the fused images at two different time points. In this method, nonlocal couple tensor CP decomposition is initially applied to fuse the HSIs and MSIs. A difference image is then obtained by subtracting the fused images at two different time points. Thereafter, the tensors are extracted from the difference image and the tensor regression-based method is used to classify the difference image and detect the final change results. Experimental results from three real datasets suggest that the proposed method substantially outperforms the existing state-of-the-art change detection methods as well as any change detection methods using single-source images.","2151-1535","","10.1109/JSTARS.2021.3115345","National Natural Science Foundation of China(grant numbers:61976117,62071233); Natural Science Foundation of Jiangsu Province(grant numbers:BK20191409); Key Projects of University Natural Science Fund of Jiangsu Province(grant numbers:19KJA360001); Qinglan Project” of Jiangsu Universities; Fundamental Research Funds for the Central Universities(grant numbers:30917015104,30919011103,30919011402); Collaborative Innovation Center of Audit Information Engineering and Technology(grant numbers:18CICA09); Young Teacher Research and Cultivation Project; Nanjing Audit University(grant numbers:18QNPY015); Postgraduate Research & Practice Innovation Program of Jiangsu Province(grant numbers:KYCX21_1944); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547758","Change detection;hyperspectral images (HSIs);image fusion;multispectral images (MSIs);tensor regression","Tensors;Spatial resolution;Image resolution;Hyperspectral imaging;Image fusion;Dictionaries;Task analysis","geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;regression analysis;remote sensing","single-source images;nonlocal couple tensor CP decomposition;tensor regression;hyperspectral images;remote sensing image;multispectral images;image fusion-based change detection","","2","","65","CCBY","24 Sep 2021","","","IEEE","IEEE Journals"
"Distortion Robust Relative Radiometric Normalization of Multitemporal and Multisensor Remote Sensing Images Using Image Features","A. Moghimi; A. Sarmadian; A. Mohammadzadeh; T. Celik; M. Amani; H. Kusetogullari","Department of Photogrammetry and Remote Sensing, Geomatics Engineering Faculty, K. N. Toosi University of Technology, Tehran, Iran; Department of Photogrammetry and Remote Sensing, Geomatics Engineering Faculty, K. N. Toosi University of Technology, Tehran, Iran; Department of Photogrammetry and Remote Sensing, Geomatics Engineering Faculty, K. N. Toosi University of Technology, Tehran, Iran; School of Electrical and Information Engineering, Wits Institute of Data Science, University of the Witwatersrand, Johannesburg, South Africa; Wood Environment and Infrastructure Solutions, Aberdeen, Canada; Department of Computer Science, Blekinge Institute of Technology, Karlskrona, Sweden","IEEE Transactions on Geoscience and Remote Sensing","8 Dec 2021","2022","60","","1","20","In this article, we propose a novel framework to radiometrically correct unregistered multisensor image pairs based on the extracted feature points with the KAZE detector and the conditional probability (CP) process in the linear model fitting. In this method, the scale, rotation, and illumination invariant radiometric control set samples (SRII-RCSS) are first extracted by the blockwise KAZE strategy. They are then distributed uniformly over both textured and texture-less land use/land cover (LULC) using grid interpolation and a set of nearest-neighbors. Subsequently, SRII-RCSS are scored by a similarity measure, and the histogram of the scores is then used to refine SRII-RCSS. The normalized subject image is produced by adjusting the subject image to the reference image using the CP-based linear regression (CPLR) based on the optimal SRII-RCSS. The registered normalized image is finally generated by registration of the normalized subject image to the reference image through a two-pass registration method, namely affine-B-spline and, then, it is enhanced by updating the normalization coefficient of CPLR based on the SRII-RCSS. In this study, eight multitemporal data sets acquired by inter/intra satellite sensors were used in tests to comprehensively assess the efficiency of the proposed method. Experimental results show that the proposed method outperforms the existing state-of-the-art relative radiometric normalization (RRN) methods both qualitatively and quantitatively, indicating its capability for RRN of unregistered multisensor image pairs.","1558-0644","","10.1109/TGRS.2021.3063151","Sichuan Provincial Science and Technology Project(grant numbers:2019JDJQ0023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9377562","Change detection;feature detection;feature matching;image matching;relative radiometric normalization;rotation invariant;scale invariant","Radiometry;Sensors;Remote sensing;Image sensors;Satellite broadcasting;Linear regression;Feature extraction","feature extraction;geophysical image processing;image fusion;image registration;interpolation;probability;radiometry;regression analysis;remote sensing;terrain mapping","illumination invariant radiometric control set samples;blockwise KAZE strategy;textured texture-less;normalized subject image;reference image;optimal SRII-RCSS;registered normalized image;two-pass registration method;normalization coefficient;multitemporal data sets;existing state-of-the-art relative radiometric normalization methods;multisensor remote sensing images;image features;radiometrically correct unregistered multisensor image pairs;extracted feature points;KAZE detector;conditional probability process;linear model;CP-based linear regression;CPLR;land use;land cover;LULC;SRII-RCSS;relative radiometric normalization;RRN methods","","17","","47","IEEE","12 Mar 2021","","","IEEE","IEEE Journals"
"A Robust Model for MODIS and Landsat Image Fusion Considering Input Noise","Z. Tan; M. Gao; J. Yuan; L. Jiang; H. Duan","College of Urban and Environmental Sciences and the Shaanxi Key Laboratory of Earth Surface System and Environmental Carrying Capacity, Northwest University, Xi’an, China; School of Geology Engineering and Geomatics, Chang’an University, Xi’an, China; College of Urban and Environmental Sciences, Northwest University, Xi’an, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Key Laboratory of Watershed Geographic Sciences, Nanjing Institute of Geography and Limnology, Chinese Academy of Sciences, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","18 Mar 2022","2022","60","","1","17","Significant progress has been made in spatiotemporal fusion for remote sensing images; however, most models require inputs to be free of clouds and without missing data, considerably confining their applications in practice. Due to recent advances in deep learning technologies, powerful modeling capabilities could be leveraged to bring potential solutions to this problem. This article proposes a novel architecture named the robust spatiotemporal fusion network (RSFN) based on the generative adversarial network and attention mechanism with dual temporal references to automatically handle input noise. The RSFN only needs one coarse-resolution image on the prediction date and two referential fine-resolution images before and after the prediction date as model inputs. Most notably, there is no special restriction attached on the data quality of referential images. The comparison with other models demonstrates the effectiveness of the RSFN model quantitatively and visually in four study areas using MODIS and Landsat images. Two main conclusions can draw from the experiments. First, the input data noise hardly affects the prediction results of the RSFN, and the RSFN can gain a comparable or even higher accuracy; conversely, the other methods only show limited resistance to input noise. Second, the RSFN with cloud-contaminated references outperforms the other models with cloud-free references after data filtering in the same study area during the same period. The satellite data quality usually varies significantly; the model robustness and fault tolerance are considered critical for actual applications. The RSFN is a simple end-to-end deep model with high accuracy and fault tolerance designed for spatiotemporal fusion with imperfect data inputs, showing promising prospects in practical applications.","1558-0644","","10.1109/TGRS.2022.3145086","National Natural Science Foundation of China(grant numbers:42101342,41971309,42001382); Third Comprehensive Scientific Expedition to Xinjiang(grant numbers:2021XJKK1403); Natural Science Special Project of Education Department of Shaanxi Province(grant numbers:21JK0928); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9687540","Data fusion;deep learning;noisy data;robust spatiotemporal fusion network (RSFN);robustness;spatiotemporal","Spatiotemporal phenomena;Remote sensing;Data models;Earth;Spatial resolution;Artificial satellites;Biological system modeling","fault tolerance;geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing","fault tolerance;simple end-to-end deep model;imperfect data inputs;robust model;MODIS;Landsat image fusion considering input noise;remote sensing images;deep learning technologies;powerful modeling capabilities;robust spatiotemporal fusion network;generative adversarial network;attention mechanism;dual temporal references;coarse-resolution image;prediction date;referential fine-resolution images;model inputs;referential images;RSFN model;Landsat images;input data noise;cloud-contaminated references;cloud-free references;data filtering;satellite data quality;model robustness","","1","","54","IEEE","20 Jan 2022","","","IEEE","IEEE Journals"
"Ensemble Multiple Kernel Active Learning For Classification of Multisource Remote Sensing Data","Y. Zhang; H. L. Yang; S. Prasad; E. Pasolli; J. Jung; M. Crawford","Hyperspectral Image Analysis Group, Electrical and Computer Engineering Department, University of Houston, Houston, TX, USA; Laboratory for Applications of Remote Sensing, within Civil Engineering, Purdue University, West Lafayette, IN, USA; Hyperspectral Image Analysis Group, Electrical and Computer Engineering Department, University of Houston, Houston, TX, USA; Laboratory for Applications of Remote Sensing, within Civil Engineering, Purdue University, West Lafayette, IN, USA; School of Science and Engineering, Texas A&M University Corpus Christi, Corpus Christi, TX, USA; Laboratory for Applications of Remote Sensing, within Civil Engineering, Purdue University, West Lafayette, IN, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","2","845","858","Incorporating disparate features from multiple sources can provide valuable diverse information for remote sensing data analysis. However, multisource remote sensing data require large quantities of labeled data to train robust supervised classifiers, which are often difficult and expensive to acquire. A mixture-of-kernel approach can facilitate the construction of an effective formulation for acquiring useful samples via active learning (AL). In this paper, we propose an ensemble multiple kernel active learning (EnsembleMKL-AL) framework that incorporates different types of features extracted from multisensor remote sensing data (hyperspectral imagery and LiDAR data) for robust classification. An ensemble of probabilistic multiple kernel classifiers is embedded into a maximum disagreement-based AL system, which adaptively optimizes the kernel for each source during the AL process. At the end of each learning step, a decision fusion strategy is implemented to make a final decision based on the probabilistic outputs. The proposed framework is tested in a multisource environment, including different types of features extracted from hyperspectral and LiDAR data. The experimental results validate the efficacy of the proposed approach. In addition, we demonstrate that using ensemble classifiers and a large number of disparate but relevant features can further improve the performance of an AL-based classification approach.","2151-1535","","10.1109/JSTARS.2014.2359136","NASA AIST(grant numbers:11-0077); Hyperspectral Image Analysis Group at the University of Houston; Laboratory for Applications of Remote Sensing at Purdue University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960863","Active learning (AL);ensemble classification;multiple kernel learning;multisource data;Active learning (AL);ensemble classification;multiple kernel learning;multisource data","Kernel;Feature extraction;Laser radar;Hyperspectral imaging;Training","decision theory;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);optical radar;probability;radar imaging;remote sensing","ensemble multiple kernel active learning;multisource remote sensing data classification;robust supervised classifier;mixture-of-kernel approach;MKL-AL;feature extraction;multisensor remote sensing data;hyperspectral imagery;LiDAR data;probabilistic multiple kernel classifier;maximum disagreement-based AL system;decision fusion strategy;probabilistic output","","61","","40","IEEE","20 Nov 2014","","","IEEE","IEEE Journals"
"Spatiotemporal Fusion of MODIS and Landsat-7 Reflectance Images via Compressed Sensing","J. Wei; L. Wang; P. Liu; X. Chen; W. Li; A. Y. Zomaya","Centre for Distributed and High Performance Computing, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; School of Computer Science, China University of Geosciences, Wuhan, China; Centre for Distributed and High Performance Computing, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia; Centre for Distributed and High Performance Computing, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Geoscience and Remote Sensing","22 Nov 2017","2017","55","12","7126","7139","The fusion of remote sensing images with different spatial and temporal resolutions is needed for diverse Earth observation applications. A small number of spatiotemporal fusion methods that use sparse representation appear to be more promising than weighted- and unmixing-based methods in reflecting abruptly changing terrestrial content. However, none of the existing dictionary-based fusion methods consider the downsampling process explicitly, which is the degradation and sparse observation from high-resolution images to the corresponding low-resolution images. In this paper, the downsampling process is described explicitly under the framework of compressed sensing for reconstruction. With the coupled dictionary to constrain the similarity of sparse coefficients, a new dictionary-based spatiotemporal fusion method is built and named compressed sensing for spatiotemporal fusion, for the spatiotemporal fusion of remote sensing images. To deal with images with a high-resolution difference, typically Landsat-7 and Moderate Resolution Imaging Spectrometer (MODIS), the proposed model is performed twice to shorten the gap between the small block size and the large resolution rate. In the experimental procedure, the near-infrared, red, and green bands of Landsat-7 and MODIS are fused with root mean square errors to check the prediction accuracy. It can be concluded from the experiment that the proposed methods can produce higher quality than five state-of-the-art methods, which prove the feasibility of incorporating the downsampling process in the spatiotemporal model under the framework of compressed sensing.","1558-0644","","10.1109/TGRS.2017.2742529","National Natural Science Foundation of China(grant numbers:41571413,41471368); China Postdoctoral Science Foundation(grant numbers:2016M591280); Special Fund for the Development Plan of the Young Teachers in the Ordinary Universities of Jiangxi Province; Open Research Fund of Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences(grant numbers:2015LDE004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036397","Compressed sensing;dictionary learning;remote sensing;sparse representation;spatiotemporal fusion","Spatiotemporal phenomena;Remote sensing;Earth;Compressed sensing;Spatial resolution;Satellites","geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing","Landsat-7 reflectance images;compressed sensing;remote sensing images;temporal resolutions;diverse Earth observation applications;spatiotemporal fusion method;sparse representation;existing dictionary-based fusion methods;downsampling process;sparse observation;high-resolution images;low-resolution images;sparse coefficients;high-resolution difference;Moderate Resolution Imaging Spectrometer;resolution rate;state-of-the-art methods;spatiotemporal model;spatial resolutions;MODIS reflectance images;weighted-based method;unmixing-based method;near-infrared band;green band;prediction accuracy","","35","","48","IEEE","13 Sep 2017","","","IEEE","IEEE Journals"
"FSL-Unet: Full-Scale Linked Unet With Spatial–Spectral Joint Perceptual Attention for Hyperspectral and Multispectral Image Fusion","X. Wang; X. Wang; K. Zhao; X. Zhao; C. Song","School of Geography and the School of Computer and Information Technology, Liaoning Normal University, Dalian, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; School of Computer and Information Technology, Liaoning Normal University, Dalian, China; School of Geography, Liaoning Normal University, Dalian, China; School of Geography and the School of Computer and Information Technology, Liaoning Normal University, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","3 Oct 2022","2022","60","","1","14","The application of hyperspectral image (HSI) is more and more extensive, but the lower spatial resolution seriously affects its application effect. Using low-resolution HSI (LR-HSI) and high-resolution (HR) multispectral image (MSI) fusion technology to achieve super-resolution reconstruction of HSI has become a mainstream method. However, most of the existing fusion methods do not make full use of the large-scale range of remote sensing images and neglect the preservation of spatial–spectral information in the fusion process. Considering that the spectral information in fused HR-HSI mainly depends on HSI, and the spatial information mainly depends on MSI, this article proposes a full-scale linked Unet with spatial–spectral joint perceptual attention (SSJPA) for hyperspectral and MSI fusion (FSL-Unet). The FSL-Unet consists of two modules. The first is the spatial–spectral attention extraction (SSAE) module, which is used to calculate the spectral attention of LR-HSI and the spatial attention of HR-MSI at different scales. The second is the full-scale link U-shaped fusion (FLUF) module, which adopts a multilevel feature extraction strategy, using denser full-scale skip connections to explore feature information in a finer-grained range, enabling the flexible combination of multiscale and multipath features. At the same time, we propose SSJPA on the encoder side of FLUF. SSJPA can make full use of the attention maps computed by the SSAE and then effectively embed spatial and spectral information into the fused image, enabling uninterrupted information transfer and aggregation. To demonstrate the effectiveness of FSL-Unet, we selected five public hyperspectral datasets for experiments. Compared with the other eight state-of-the-art fusion methods, the experimental results show that the FSL-Unet achieves competitive results. The source code for FSL-Unet can be downloaded from https://github.com/wxy11-27/FSL-Unet.","1558-0644","","10.1109/TGRS.2022.3208125","National Natural Science Foundation of China(grant numbers:41971388); Innovation Team Support Program of Liaoning Higher Education Department(grant numbers:LT2017013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9895448","Hyperspectral image (HSI);image fusion;multispectral image (MSI);perceptual attention;Unet","Spatial resolution;Hyperspectral imaging;Feature extraction;Decoding;Fuses;Tensors;Superresolution","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;remote sensing","full-scale linked Unet;spatial-spectral joint perceptual attention;hyperspectral imaging;low-resolution HSI;LR-HSI;high-resolution multispectral image fusion technology;super-resolution reconstruction;remote sensing images;spatial-spectral information;spatial information;SSJPA;MSI fusion;spatial-spectral attention extraction module;full-scale link U-shaped fusion;full-scale skip connections;attention mapping;image fusion;uninterrupted information transfer;aggregation;FSL-Unet;HR-HSI fusion process;FLUF module","","1","","45","IEEE","20 Sep 2022","","","IEEE","IEEE Journals"
"Image Fusion Using Quaternion Wavelet Transform and Multiple Features","P. Chai; X. Luo; Z. Zhang","School of Internet of Things, Jiangnan University, Wuxi, China; School of Internet of Things, Jiangnan University, Wuxi, China; College of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, China","IEEE Access","2 Jun 2017","2017","5","","6724","6734","Multi-scale-based image fusion is one of main fusion methods, in which multi-scale decomposition tool and feature extraction play very important roles. The quaternion wavelet transform (QWT) is one of the effective multi-scale decomposition tools. Therefore, this paper proposes a novel multimodal image fusion method using QWT and multiple features. First, we perform QWT on each source image to obtain low-frequency coefficients and high-frequency coefficients. Second, a weighted average fusion rule based on the phase and magnitude of low-frequency subband and spatial variance is proposed to fuse the low-frequency subbands. Next, a choose-max fusion rule based on the contrast and energy of coefficient is proposed to integrate the high-frequency subbands. Finally, the final fused image is constructed by inverse QWT. The proposed method is conducted on multi-focus images, medical images, infrared-visible images, and remote sensing images, respectively. Experimental results demonstrate the effectiveness of the proposed method.","2169-3536","","10.1109/ACCESS.2017.2685178","National Natural Science Foundation of China(grant numbers:61300151,61373055); Provincial Research(grant numbers:BK20151358,BK20151202); Ministry of Housing and Urban-rural Development of the China(grant numbers:2015-K8-035); Fundamental Research Funds for the Central Universities(grant numbers:JUSRP51618B); Equipment Development and Ministry of Education Union Fund(grant numbers:6141A02033312); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884973","Image fusion;quaternion wavelet transform;phase;magnitude;feature","Quaternions;Wavelet transforms;Feature extraction;Image fusion;Biomedical imaging;Frequency measurement","feature extraction;geophysical image processing;image fusion;inverse transforms;medical image processing;remote sensing;wavelet transforms","remote sensing images;infrared-visible images;medical images;inverse QWT;choose-max fusion rule;low-frequency subbands;weighted average fusion rule;novel multimodal image fusion method;multiscale decomposition tool;feature extraction;quaternion wavelet transform;multiscale-based image fusion","","61","","23","CCBY","22 Mar 2017","","","IEEE","IEEE Journals"
"Multiscale Block Fusion Object Detection Method for Large-Scale High-Resolution Remote Sensing Imagery","Y. Wang; Z. Dong; Y. Zhu","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Access","5 Aug 2019","2019","7","","99530","99539","Object detection in high-spatial-resolution remote sensing images (HSRIs) is an import part of the automatic extraction and understanding of image information in high-resolution earth observation systems. Regarding how to achieve optimal block object detection for large-scale HSRIs, this paper proposes a multiscale block fusion object detection method for large-scale HSRIs. First, the objects in large-scale HSRIs are detected using different block scales, and the average precision (AP) of the different object detection results is counted at different block scales. Then, according to the statistical information, the image block scales corresponding to the optimal AP value of the different objects are obtained. Finally, a soft non-maximum suppression algorithm is used to fuse the image block scale detection results corresponding to the optimal AP values of the different objects, to obtain the object detection results of the large-scale HSRIs. The experimental results confirm that the proposed method outperforms all other single-scale image block detection methods and provides acceptable object detection results in large-scale HSRIs.","2169-3536","","10.1109/ACCESS.2019.2930092","National Natural Science Foundation of China(grant numbers:41801382,91738302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8766981","Large-scale high-resolution remote sensing image;multiscale block;object detection;convolutional neural network;deep learning","Object detection;Feature extraction;Training;Remote sensing;Airplanes;Testing;Data mining","feature extraction;geophysical image processing;image fusion;image resolution;object detection;remote sensing;statistical analysis","multiscale block fusion object detection method;large-scale high-resolution remote sensing imagery;high-spatial-resolution remote sensing images;high-resolution earth observation systems;optimal block object detection;large-scale HSRIs;image block scale detection results;single-scale image block detection methods;object detection results;image information","","7","","40","CCBY","19 Jul 2019","","","IEEE","IEEE Journals"
"Counting From Sky: A Large-Scale Data Set for Remote Sensing Object Counting and a Benchmark Method","G. Gao; Q. Liu; Y. Wang","Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","21 Apr 2021","2021","59","5","3642","3655","Object counting, whose aim is to estimate the number of objects from a given image, is an important and challenging computation task. Significant efforts have been devoted to addressing this problem and achieved great progress, yet counting the number of ground objects from remote sensing images is barely studied. In this article, we are interested in counting dense objects from remote sensing images. Compared with object counting in a natural scene, this task is challenging in the following factors: large-scale variation, complex cluttered background, and orientation arbitrariness. More importantly, the scarcity of data severely limits the development of research in this field. To address these issues, we first construct a large-scale object counting data set with remote sensing images, which contains four important geographic objects: buildings, crowded ships in harbors, and large vehicles and small vehicles in parking lots. We then benchmark the data set by designing a novel neural network that can generate a density map of an input image. The proposed network consists of three parts, namely attention module, scale pyramid module, and deformable convolution module (DCM) to attack the aforementioned challenging factors. Extensive experiments are performed on the proposed data set and one crowd counting data set, which demonstrates the challenges of the proposed data set and the superiority and effectiveness of our method compared with state-of-the-art methods.","1558-0644","","10.1109/TGRS.2020.3020555","National Natural Science Foundation of China(grant numbers:41871283,U1804157,61601011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200587","Attention mechanism;deformable convolution layer;object counting;remote sensing;scale pyramid module (SPM)","Remote sensing;Task analysis;Buildings;Marine vehicles;Convolution;Neural networks;Object detection","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;natural scenes;neural nets;object detection;remote sensing;video signal processing","important computation task;challenging computation task;ground objects;remote sensing images;dense objects;large-scale variation;large-scale object;data set;important geographic objects;input image;scale pyramid module;aforementioned challenging factors;crowd counting data;large-scale data;remote sensing object counting","","18","","76","IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"Convolutional Autoencoder-Based Multispectral Image Fusion","A. Azarang; H. E. Manoochehri; N. Kehtarnavaz","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX, USA","IEEE Access","28 Mar 2019","2019","7","","35673","35683","This paper presents a deep learning-based pansharpening method for fusion of panchromatic and multispectral images in remote sensing applications. This method can be categorized as a component substitution method in which a convolutional autoencoder network is trained to generate original panchromatic images from their spatially degraded versions. Low resolution multispectral images are then fed into the trained convolutional autoencoder network to generate estimated high resolution multispectral images. The fusion is achieved by injecting the detail map of each spectral band into the corresponding estimated high resolution multispectral bands. Full reference and no-reference metrics are computed for the images of three satellite datasets. These measures are compared with the existing fusion methods whose codes are publicly available. The results obtained indicate the effectiveness of the developed deep learning-based method for multispectral image fusion.","2169-3536","","10.1109/ACCESS.2019.2905511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668404","Multispectral image fusion by convolutional autoencoder;fusion of panchromatic and multispectral images in remote sensing;convolutional autoencoder-based pansharpening","Convolution;Spatial resolution;Computer architecture;Image reconstruction;Convolutional codes;Image fusion","geophysical image processing;geophysical signal processing;image fusion;image resolution;learning (artificial intelligence);remote sensing","convolutional autoencoder-based multispectral image fusion;remote sensing applications;component substitution method;original panchromatic images;low resolution multispectral images;trained convolutional autoencoder network;estimated high resolution multispectral images;corresponding estimated high resolution multispectral bands;existing fusion methods whose codes;developed deep learning-based method","","74","","44","OAPA","17 Mar 2019","","","IEEE","IEEE Journals"
"Multi-Scale Dense Networks for Hyperspectral Remote Sensing Image Classification","C. Zhang; G. Li; S. Du","School of Civil Engineering, Hefei University of Technology, Hefei, China; School of Civil Engineering, Hefei University of Technology, Hefei, China; Institute of Remote Sensing and GIS, Peking University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","30 Oct 2019","2019","57","11","9201","9222","For hyperspectral remote sensing image (HSI) classification, the learning process of deep neural networks has been progressively advanced in depth, but the fine features are often largely lost or even disappear in the process of depth transfer. With the increase in feature aggregation and connectivity, the complexity of the network and the training parameters increases greatly, requiring more training time. This paper proposed a multi-scale dense network (MSDN) for HSI classification that made full use of different scale information in the network structure and combined scale information throughout the network. It implemented feature extraction of HSIs in two dimensions, including the features at fine and coarse levels. In the horizontal direction, it considered the deep extraction of HSI features, and the 3-D dense connection structure was used for aggregating features at different levels. In the vertical direction, scale information was considered, and three-scale feature maps at low, middle, and high levels were generated based on the first layer of the network. The MSDN used stride convolution for downsampling and combined feature information at different scale levels. The MSDN extracted features along the diagonal line. The network implemented the reconstruction of deep feature extraction and multi-scale fusion for HSI classification. The MSDN model performed well on representative HSI datasets, namely, the Indian Pines, Pavia University, Salinas, Botswana, and Kennedy Space Center datasets. It improved the training speed and accuracy for HSI classification and especially improved the convergence speed, which effectively saved computing resources and had high stability.","1558-0644","","10.1109/TGRS.2019.2925615","National Natural Science Foundation of China(grant numbers:41671456,41401451); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784389","3-D convolutional neural network (3-D CNN);3-D DenseNet;hyperspectral remote sensing image (HSI) classification;multi-scale dense network (MSDN);spectral–spatial information","Feature extraction;Training;Convolution;Remote sensing;Convergence;Data mining;Deep learning","feature extraction;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing","deep feature extraction;HSI classification;representative HSI datasets;multiscale dense network;hyperspectral remote sensing image classification;deep neural networks;feature aggregation;MSDN;deep extraction;3D dense connection structure","","95","","28","IEEE","1 Aug 2019","","","IEEE","IEEE Journals"
"Scene Classification Based on the Multifeature Fusion Probabilistic Topic Model for High Spatial Resolution Remote Sensing Imagery","Y. Zhong; Q. Zhu; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","11 Aug 2015","2015","53","11","6207","6222","Scene classification has been proved to be an effective method for high spatial resolution (HSR) remote sensing image semantic interpretation. The probabilistic topic model (PTM) has been successfully applied to natural scenes by utilizing a single feature (e.g., the spectral feature); however, it is inadequate for HSR images due to the complex structure of the land-cover classes. Although several studies have investigated techniques that combine multiple features, the different features are usually quantized after simple concatenation (CAT-PTM). Unfortunately, due to the inadequate fusion capacity of k-means clustering, the words of the visual dictionary obtained by CAT-PTM are highly correlated. In this paper, a semantic allocation level (SAL) multifeature fusion strategy based on PTM, namely, SAL-PTM (SAL-pLSA and SAL-LDA) for HSR imagery is proposed. In SAL-PTM: 1) the complementary spectral, texture, and scale-invariant-featuretransform features are effectively combined; 2) the three features are extracted and quantized separately by k-means clustering, which can provide appropriate low-level feature descriptions for the semantic representations; and 3)the latent semantic allocations of the three features are captured separately by PTM, which follows the core idea of PTM-based scene classification. The probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation (LDA) models were compared to test the effect of different PTMs for HSR imagery. A U.S. Geological Survey data set and the UC Merced data set were utilized to evaluate SAL-PTM in comparison with the conventional methods. The experimental results confirmed that SAL-PTM is superior to the single-feature methods and CAT-PTM in the scene classification of HSR imagery.","1558-0644","","10.1109/TGRS.2015.2435801","National Natural Science Foundation of China(grant numbers:41371344); Fundamental Research Funds for the Central Universities(grant numbers:2042014kf00231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119589","Fusion;high spatial resolution (HSR) imagery;latent Dirichlet allocation (LDA);multifeature;probabilistic latent semantic analysis (pLSA);probabilistic topic model (PTM);scene classification;Fusion;high spatial resolution (HSR) imagery;latent Dirichlet allocation (LDA);multifeature;probabilistic latent semantic analysis (pLSA);probabilistic topic model (PTM);scene classification","Semantics;Visualization;Resource management;Probabilistic logic;Support vector machines;Feature extraction;Probability","feature extraction;geophysical image processing;image classification;image fusion;land cover;remote sensing","scene classification;multifeature fusion probabilistic topic model;high spatial resolution remote sensing imagery;HSR remote sensing image semantic interpretation;probabilistic topic model;HSR images;land-cover classes;inadequate fusion capacity;k-means clustering;visual dictionary;semantic allocation level;SAL multifeature fusion strategy;captured scale-invariant-feature-transform features;PTM-based scene classification;probabilistic latent semantic analysis;latent Dirichlet allocation models;US Geological Survey data set;single-feature methods;HSR imagery scene classification","","191","","57","IEEE","8 Jun 2015","","","IEEE","IEEE Journals"
"Multiresolution Multimodal Sensor Fusion for Remote Sensing Data With Label Uncertainty","X. Du; A. Zare","Department of Naval Architecture and Marine Engineering, University of Michigan, Ann Arbor, USA; Department of Electrical and Computer Engineering, University of Missouri, Columbia, USA","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2020","2020","58","4","2755","2769","In remote sensing, each sensor can provide complementary or reinforcing information. It is valuable to fuse outputs from multiple sensors to boost overall performance. Previous supervised fusion methods often require accurate labels for each pixel in the training data. However, in many remote-sensing applications, pixel-level labels are difficult or infeasible to obtain. In addition, outputs from multiple sensors often have different resolutions or modalities. For example, rasterized hyperspectral imagery (HSI) presents data in a pixel grid while airborne light detection and ranging (LiDAR) generates dense 3-D point clouds. It is often difficult to directly fuse such multimodal, multiresolution data. To address these challenges, we present a novel multiple instance multiresolution fusion (MIMRF) framework that can fuse multiresolution and multimodal sensor outputs while learning from automatically generated, imprecisely labeled data. Experiments were conducted on the MUUFL Gulfport HSI and LiDAR data set and a remotely sensed soybean and weed data set. Results show improved, consistent performance on scene understanding and agricultural applications when compared to traditional fusion methods.","1558-0644","","10.1109/TGRS.2019.2955320","National Science Foundation(grant numbers:IIS-1723891); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8931670","Choquet integral (CI);hyperspectral imagery (HSI);label uncertainty;light detection and ranging (LiDAR);multimodal fusion;multiresolution;multiple instance learning (MIL);scene understanding;sensor fusion","Laser radar;Three-dimensional displays;Remote sensing;Spatial resolution;Uncertainty;Fuses","agricultural engineering;geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);object detection;optical radar;remote sensing by laser beam;remote sensing by radar","accurate labels;training data;remote-sensing applications;pixel-level labels;multiple sensors;rasterized hyperspectral imagery;3-D point clouds;multimodal data;multiresolution data;multiple instance multiresolution fusion framework;multimodal sensor outputs;imprecisely labeled data;LiDAR data;remotely sensed soybean;traditional fusion methods;multiresolution multimodal sensor fusion;remote sensing data;label uncertainty;supervised fusion methods;pixel grid while airborne light detection and ranging;dense 3D point clouds","","12","","77","IEEE","12 Dec 2019","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion via Nonlocal Low-Rank Tensor Approximation and Sparse Representation","X. Li; Y. Yuan; Q. Wang","Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","550","562","The fusion of hyperspectral (HS) and multispectral (MS) images designed to obtain high-resolution HS (HRHS) images is a very challenging work. A series of solutions has been proposed in recent years. However, the similarity in the structure of the HS image has not been fully used. In this article, we present a novel HS and MS image-fusion method based on nonlocal low-rank tensor approximation and sparse representation. Specifically, the HS image and the MS image are considered the spatially and spectrally degraded versions of the HRHS image, respectively. Then, the nonlocal low-rank constraint term is adopted in order to form the nonlocal similarity and the spatial-spectral correlation. Meanwhile, we add the sparse constraint term to describe the sparsity of abundance. Thus, the proposed fusion model is established and its optimization is solved by alternative direction method of multipliers (ADMM). The experimental results on three synthetic data sets and one real data set show the advantages of the proposed method over several state-of-the-art competitors.","1558-0644","","10.1109/TGRS.2020.2994968","National Natural Science Foundation of China(grant numbers:U1864204,U1801262,61871470,61773316); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103204","Hyperspectral (HS) image;image fusion;low-rank tensor approximation;multispectral (MS) image;sparse representation","Tensile stress;Bayes methods;Spatial resolution;Sparse matrices;Machine learning;Image fusion","approximation theory;convex programming;geophysical image processing;hyperspectral imaging;image fusion;image representation;image resolution;tensors","HRHS image;low-rank constraint term;nonlocal similarity;spatial-spectral correlation;sparse constraint term;fusion model;nonlocal low-rank tensor approximation;sparse representation;high-resolution HS images;MS image-fusion;multispectral image fusion;hyperspectral images;alternative direction method of multipliers;ADMM;synthetic data sets","","21","","38","IEEE","28 May 2020","","","IEEE","IEEE Journals"
"Remote Sensing Image Super-Resolution via Residual Aggregation and Split Attentional Fusion Network","L. Chen; H. Liu; M. Yang; Y. Qian; Z. Xiao; X. Zhong","Key Laboratory of Software Engineering, Ürümqi, China; Key Laboratory of Software Engineering, Ürümqi, China; Key Laboratory of Software Engineering, Ürümqi, China; Key Laboratory of Software Engineering, Ürümqi, China; College of Mathematics and System Sciences, Xinjiang University, Ürümqi, China; Key Laboratory of Software Engineering, Ürümqi, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Oct 2021","2021","14","","9546","9556","Remote sensing images contain various land surface scenes and different scales of ground objects, which greatly increases the difficulty of super-resolution tasks. The existing deep learning-based methods cannot solve this problem well. To achieve high-quality super-resolution of remote sensing images, a residual aggregation and split attentional fusion network (RASAF) is proposed in this article. It is mainly divided into the following three parts. First, a split attentional fusion block is proposed. It uses a basic split–fusion mechanism to achieve cross-channel feature group interaction, allowing the method to adapt to various land surface scene reconstructions. Second, to fully exploit multiscale image information, a hierarchical loss function is used. Third, residual learning is used to reduce the difficulty of training in super-resolution tasks. However, the respective residual branch features are used quite locally and fail to represent the real value. A residual aggregation mechanism is used to aggregate the local residual branch features to generate higher quality local residual branch features. The comparison of RASAF with some classical super-resolution methods using two widely used remote sensing datasets showed that the RASAF achieved better performance. And it achieves a good balance between performance and model parameter number. Meanwhile, the RASAF’s ability to support multilabel remote sensing image classification tasks demonstrates its usefulness.","2151-1535","","10.1109/JSTARS.2021.3113658","National Natural Science Foundation of China(grant numbers:61966035); National Natural Science Foundation of China(grant numbers:U1803261); Xinjiang Uygur Autonomous Region Innovation Team(grant numbers:XJEDU2017T002); Autonomous Region Graduate Innovation Project(grant numbers:XJ2020G074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9541020","Remote sensing image;residual aggregation;split attentional fusion;super-resolution (SR)","Remote sensing;Superresolution;Image reconstruction;Task analysis;Sensors;Feature extraction;Learning systems","feature extraction;geophysical image processing;image classification;image fusion;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","remote sensing image super-resolution;split attentional fusion network;remote sensing images;land surface scenes;super-resolution tasks;deep learning-based methods;high-quality super-resolution;RASAF;split attentional fusion block;basic split-fusion mechanism;cross-channel feature group interaction;land surface scene reconstructions;multiscale image information;residual learning;respective residual branch features;residual aggregation mechanism;higher quality local residual branch features;classical super-resolution methods;widely used remote sensing datasets;multilabel remote sensing image classification tasks","","7","","42","CCBY","20 Sep 2021","","","IEEE","IEEE Journals"
"Multiple Instance Choquet Integral Classifier Fusion and Regression for Remote Sensing Applications","X. Du; A. Zare","Department of Electrical and Computer Engineering, University of Missouri, Columbia, MO, USA; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA","IEEE Transactions on Geoscience and Remote Sensing","23 Apr 2019","2019","57","5","2741","2753","In classifier (or regression) fusion, the aim is to combine the outputs of several algorithms to boost overall performance. Standard supervised fusion algorithms often require accurate and precise training labels. However, accurate labels may be difficult to obtain in many remote sensing applications. This paper proposes novel classification and regression fusion models that can be trained given ambiguously and imprecisely labeled training data in which the training labels are associated with sets of data points (i.e., “bags”) instead of individual data points (i.e., “instances”) following a multiple-instance learning framework. Experiments were conducted based on the proposed algorithms on both synthetic data and applications such as target detection and crop yield prediction given remote sensing data. The proposed algorithms show effective classification and regression performance.","1558-0644","","10.1109/TGRS.2018.2876687","National Science Foundation(grant numbers:IIS-1723891); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528500","Choquet integral (CI);classifier fusion;multiple-instance learning (MIL);multiple-instance regression (MIR);remote sensing;target detection","Remote sensing;Training;Prediction algorithms;Object detection;Global Positioning System;Standards;Clustering algorithms","crops;geophysical image processing;image classification;image fusion;learning (artificial intelligence);object detection;regression analysis;remote sensing","remote sensing data;multiple instance choquet integral classifier fusion;remote sensing applications;multiple-instance learning framework;synthetic data;data points;supervised fusion algorithms;regression fusion models;crop yield prediction","","21","","58","IEEE","9 Nov 2018","","","IEEE","IEEE Journals"
"A Novel Adaptive Hybrid Fusion Network for Multiresolution Remote Sensing Images Classification","W. Ma; J. Shen; H. Zhu; J. Zhang; J. Zhao; B. Hou; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","8 Dec 2021","2022","60","","1","17","With the rapid development of earth observation technology, panchromatic (PAN) and multispectral (MS) images have also become easier to obtain. The multiresolution classification of PAN and MS images as a basic MS image analysis task has become a research hotspot. The main challenge in this field is how to process data and extract features to improve classification accuracy effectively. In this article, we design a novel adaptive hybrid fusion network (AHF-Net) for multiresolution remote sensing image classification. It includes two parts: data fusion and feature fusion. In the data fusion part, we propose an adaptive weighted intensity-hue-saturation (AWIHS) strategy, which can reduce the difference between MS and PAN images by adaptively adding each other’s unique information from the perspective of information sharing. In the feature fusion part, starting from the second-order correlation of features, we propose a correlation-based attention feature fusion (CAFF) module. It can improve the discrimination of fusion features by adaptively determining the fusion coefficient according to the importance of the input feature channel. Based on AWIHS and CAFF, inspired by the idea of feature pyramid, we combine the multilevel feature fusion and the dual-branch residual network as the backbone network of AHF-Net. By combining AWIHS and CAFF modules with the backbone network, our AHF-Net can effectively improve the classification accuracy of multiresolution remote sensing images. The effectiveness of the proposed algorithm has been verified on multiple data sets. Our code and model are available at https://github.com/1826133674/AHF-Net.","1558-0644","","10.1109/TGRS.2021.3062142","Key Scientific Technological Innovation Research Project by Ministry of Education; Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-05); National Natural Science Foundation of China(grant numbers:62006179); China Postdoctoral Science Foundation funded project(grant numbers:2019M663634,2020T130492); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9374563","Data difference reduction;deep learning (DL);feature fusion;multiresolution image classification;remote sensing","Feature extraction;Spatial resolution;Pansharpening;Data mining;Remote sensing;Fuses;Data integration","feature extraction;geophysical image processing;geophysical signal processing;image classification;image colour analysis;image fusion;image resolution;remote sensing;sensor fusion","AHF-Net;classification accuracy;novel adaptive hybrid fusion network;multiresolution remote sensing images classification;multiresolution classification;basic MS image analysis task;multiresolution remote sensing image classification;data fusion part;adaptive weighted intensity-hue-saturation strategy;feature fusion part;correlation-based attention feature fusion module;fusion features;fusion coefficient;input feature channel;feature pyramid;multilevel feature fusion;dual-branch residual network;backbone network","","5","","44","IEEE","10 Mar 2021","","","IEEE","IEEE Journals"
"Building Change Detection for VHR Remote Sensing Images via Local–Global Pyramid Network and Cross-Task Transfer Learning Strategy","T. Liu; M. Gong; D. Lu; Q. Zhang; H. Zheng; F. Jiang; M. Zhang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi’an, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","21 Feb 2022","2022","60","","1","17","Building change detection (BCD) for very-high-spatial-resolution (VHR) remote sensing images is very important and challenging in the field of remote sensing, as the building is one of the most significant and valuable man-made ground targets. This article proposes a local–global pyramid network (LGPNet) that combines a local feature pyramid module (LFPM) and a global spatial pyramid module (GSPM) for various building feature extraction. The LFPM is constructed using the convolutional kernel with three different pyramid scales, and then, the local pyramid features are obtained by adding features of each scale. In the GSPM, the global spatial pyramid features are extracted by adaptive average pooling to acquire global contextual information from different fields of view on deep features. The LFPM and the GSPM work in a parallel and complementary manner to capture discriminative features of various buildings. In addition to the LFPM and the GSPM, the proposed LGPNet also employs two general attention mechanisms, i.e., the position attention module and the channel attention module, which can select and emphasize adaptively some building features with high semantic responses. Besides, in order to mitigate the influence of other ground targets to a certain extent, a cross-task transfer learning strategy is introduced to make the LGPNet focus on the building, which significantly improves the performance of our method. Extensive experiments on two public available BCD datasets show that the proposed LGPNet can achieve significant improvement compared with eight other state-of-the-art methods. The source code and the pretrained model will be released at https://github.com/TongfeiLiu/LGPNet.","1558-0644","","10.1109/TGRS.2021.3130940","National Natural Science Foundation of China(grant numbers:62036006,61906147); Fundamental Research Funds for the Central Universities; Innovation Fund of Xidian University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627698","Attention mechanism;building change detection (BCD);pyramid network;transfer learning;very-high-spatial-resolution (VHR) remote sensing images","Buildings;Feature extraction;Remote sensing;Semantics;Image segmentation;Image color analysis;Transfer learning","feature extraction;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);object detection;remote sensing","building change detection;VHR remote sensing images;local-global pyramid network;cross-task transfer learning strategy;very-high-spatial-resolution remote sensing images;LGPNet;local feature pyramid module;LFPM;global spatial pyramid module;GSPM;building feature extraction;local pyramid features;global spatial pyramid features;global contextual information;deep features;discriminative features;position attention module;channel attention module;building features;public available BCD datasets","","4","","66","IEEE","25 Nov 2021","","","IEEE","IEEE Journals"
"A Spatial–Spectral Dual-Optimization Model-Driven Deep Network for Hyperspectral and Multispectral Image Fusion","W. Dong; T. Zhang; J. Qu; Y. Li; H. Xia","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; College of Geography and Environmental Science and the Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions, Ministry of Education, Henan University, Zhengzhou, China","IEEE Transactions on Geoscience and Remote Sensing","9 Nov 2022","2022","60","","1","16","Deep learning, especially convolutional neural networks (CNNs), has shown very promising results for multispectral (MS) and hyperspectral (HS) image fusion (MS/HS fusion) task. Most of the existing CNN methods are based on “black-box” models that are not specifically designed for MS/HS fusion, which largely ignore the priors evidently possessed by the observed HS and MS images and lack clear interpretability, leaving room for further improvement. In this article, we propose an interpretable network, named spatial–spectral dual-optimization model-driven deep network ( $\text{S}{^{\mathrm{2}}}$ DMDN), which embeds the intrinsic generation mechanism of the MS/HS fusion to the network. There are two key characteristics: 1) explicitly encode the spatial prior and spectral prior evidently possessed by the input MS and HS images in the network architecture and 2) unfold an iterative spatial–spectral dual-optimization algorithm into a model-driven deep network. The benefit is that the network has good interpretability and generalization capability, and the fused image is richer in semantics and more precise in spatial. Extensive experiments are conducted to prove the superiority of our proposed method over other state-of-the-art methods in terms of quantitative evaluation metrics and qualitative visual effects.","1558-0644","","10.1109/TGRS.2022.3217542","Higher Education Discipline Innovation Project(grant numbers:B08038); National Natural Science Foundation of China(grant numbers:62101414,62201423); Young Talent Fund of Xi’an Association for Science and Technology(grant numbers:095920221320); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2021JQ-194,2021JQ-197); Fundamental Research Funds for the Central Universities(grant numbers:XJS210108,XJS210104); China Postdoctoral Science Foundation(grant numbers:2021M702546,2021M702548); China Postdoctoral Science Special Foundation(grant numbers:2022T150508); Scientific and Technological Activities for Overseas Students of Shaanxi Province(grant numbers:2020-017); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110856); Open Fund of Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions (Henan University), Ministry of Education(grant numbers:GTYR202210); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9938429","Hyperspectral (HS) image;image fusion;model-driven deep network;spatial–spectral dual-optimization","Tensors;Task analysis;Pansharpening;Image fusion;Spatial resolution;Deep learning;Mathematical models","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;hyperspectral imaging;image fusion;optimisation","iterative spatial-spectral dual-optimization algorithm;multispectral image fusion;hyperspectral image fusion;HS images;spatial-spectral dual-optimization model-driven deep network;deep learning;convolutional neural networks;S2DMDN;MS images;network architecture","","4","","47","IEEE","3 Nov 2022","","","IEEE","IEEE Journals"
"Hyperspectral Image Stripe Removal Network With Cross-Frequency Feature Interaction","C. Wang; M. Xu; Y. Jiang; G. Deng; Z. Lu; G. Zhang; H. Cui","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","8 Mar 2022","2022","60","","1","15","Remote sensing images, especially hyperspectral images (HSIs), are extremely vulnerable to random noise and stripe noise. As a key aspect of HSI data quality improvement, stripe noise removal has always been a pervasive issue in remote sensing image processing. Convolutional neural networks have been applied for HSI data destriping. However, the existing methods lose the stripe-free component of the original image to a certain extent. These models also ignore the global spatial context of images and the correlation between spatial information and spectral information. Therefore, we propose a novel destriping convolutional network to overcome the problems with the existing methods. Octave convolution is used to extract cross-frequency features, and separate and compress the low-frequency information of the images, while dilation convolution (Dila-Conv) is used to reduce the amount of required calculation and also preserve the key image information. In addition, Dila-Conv can expand the receptive field to obtain multiscale features. Finally, a cross-channel enhanced spatial–spectral feature fusion module is used to acquire and integrate spatial context information and interchannel dependencies on a global scale as auxiliary information so that the network model can learn and pay attention to key feature information, specifically, “what to look for” and “where to look at,” which can facilitate the distinction between stripe and stripe-free components. Experimental results obtained using multiple datasets demonstrated that the proposed method can outperform the existing comparable methods and can produce satisfactory results in terms of visual effects and quantitative evaluation.","1558-0644","","10.1109/TGRS.2021.3138740","National Natural Science Foundation of China(grant numbers:41971412); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663279","Attention mechanism;convolutional neural networks (CNNs);destriping;hyperspectral images (HSIs)","Feature extraction;Noise reduction;Convolution;Data models;Matched filters;Information filters;Hyperspectral imaging","convolutional neural nets;feature extraction;hyperspectral imaging;image denoising;image fusion;learning (artificial intelligence);remote sensing","hyperspectral image stripe removal network;cross-frequency feature interaction;remote sensing images;hyperspectral images;stripe noise removal;remote sensing image processing;convolutional neural networks;HSI data destriping;spatial information;spectral information;convolutional network;octave convolution;cross-frequency features;low-frequency information;dilation convolution;Dila-Conv;key image information;cross-channel enhanced spatial-spectral feature fusion module;HSI data quality","","1","","57","IEEE","27 Dec 2021","","","IEEE","IEEE Journals"
"Light-Weight Semantic Segmentation Network for UAV Remote Sensing Images","S. Liu; J. Cheng; L. Liang; H. Bai; W. Dang","University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; Second Research Institute of Civil Aviation Administration of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Sep 2021","2021","14","","8287","8296","Semantic segmentation for unmanned aerial vehicle (UAV) remote sensing images has become one of the research focuses in the field of remote sensing at present, which could accurately analyze the ground objects and their relationships. However, conventional semantic segmentation methods based on deep learning require large-scale models that are not suitable for resource-constrained UAV remote sensing tasks. Therefore, it is important to construct a light-weight semantic segmentation method for UAV remote sensing images. With this motivation, we propose a light-weight neural network model with fewer parameters to solve the problem of semantic segmentation of UAV remote sensing images. The network adopts an encoder-decoder architecture. In the encoder, we build a light-weight convolutional neural network model with fewer channels of each layer to reduce the number of model parameters. Then, feature maps of different scales from the encoder are concatenated together after resizing to carry out the multiscale fusion. Moreover, we employ two attention modules to capture the global semantic information from the context and the correlation among channels in UAV remote sensing images. In the decoder part, the model obtains predictions of each pixel through the softmax function. We conducted experiments on the ISPRS Vaihingen dataset, UAVid dataset, and UDD6 dataset to verify the effectiveness of the light-weight network. Our method obtains quality semantic segmentation results evaluated on UAV remote sensing datasets with only 9 M parameters the model owns, which is competitive among popular methods with the same level of parameters.","2151-1535","","10.1109/JSTARS.2021.3104382","National Natural Science Foundation of China(grant numbers:62071104); Sichuan Science and Technology Program(grant numbers:2020YFG0085,2021YFG0328); Intelligent Terminal Key Laboratory of Sichuan(grant numbers:SCITLAB-0017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512294","Attention mechanism;light-weight network;remote sensing;semantic segmentation;unmanned aerial vehicle images","Remote sensing;Semantics;Image segmentation;Task analysis;Unmanned aerial vehicles;Feature extraction;Convolution","autonomous aerial vehicles;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;object detection;remote sensing;remotely operated vehicles;traffic engineering computing","light-weight semantic segmentation method;resource-constrained UAV remote sensing tasks;conventional semantic segmentation methods;light-weight semantic segmentation network;UAV remote sensing datasets;quality semantic segmentation results;light-weight network;light-weight convolutional neural network model;UAV remote sensing images;light-weight neural network model","","13","","47","CCBYNCND","12 Aug 2021","","","IEEE","IEEE Journals"
"Remote Sensing Image Sharpening by Integrating Multispectral Image Super-Resolution and Convolutional Sparse Representation Fusion","H. Wu; S. Zhao; J. Zhang; C. Lu","School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China; School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China; School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China; School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China","IEEE Access","15 Apr 2019","2019","7","","46562","46574","In remote sensing, it is quite necessary to fuse spectral information of low-resolution multispectral (LRMS) images and spatial information of panchromatic (PAN) images for obtaining high-resolution multispectral (HRMS) images. In this paper, an effective fusion method integrating multispectral (MS) image super-resolution and convolutional sparse representation (CSR) fusion is proposed to make full use of the spatial information of remote sensing images. First, for enhancing the spatial information of LRMS images with suitable sizes, a fast iterative image super-resolution algorithm based on the learned iterative shrinkage and thresholding algorithm (LISTA) is exploited in the first stage. It employs a feed-forward neural network to simplify the solution of sparse coefficients in the process of super-resolution. In the fusion stage, we propose a CSR-based image fusion framework, in which each MS super-resolution image and PAN image is decomposed into a basic layer and a detail layer, then we fuse the basic layers and the detail layers of the images, respectively. This hierarchical fusion strategy guarantees great performance in detail preservation. The experimental results on QuickBird, WorldView-2, and Landsat ETM+ datasets demonstrate that the proposed method outperforms other methods in terms of both objective evaluation and visual effect.","2169-3536","","10.1109/ACCESS.2019.2908968","National Natural Science Foundation of China(grant numbers:61772454,61811540410,61811530332); Scientific Research Fund of Hunan Provincial Education Department(grant numbers:16A008); Postgraduate Training Innovation Base Construction Project of Hunan Province(grant numbers:2017-451-30); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8681072","Remote sensing fusion;super-resolution;learned iterative shrinkage and thresholding algorithm (LISTA);convolutional sparse representation (CSR)","Spatial resolution;Transforms;Remote sensing;Iterative algorithms;Dictionaries;Image fusion","convolutional neural nets;geophysical image processing;image fusion;image representation;image resolution;image segmentation;iterative methods;learning (artificial intelligence);remote sensing","spectral information;low-resolution multispectral images;spatial information;panchromatic images;high-resolution multispectral images;convolutional sparse representation fusion;LRMS images;fast iterative image super-resolution algorithm;thresholding algorithm;CSR-based image fusion framework;hierarchical fusion strategy;multispectral image super-resolution;remote sensing image sharpening;iterative shrinkage learning;PAN images;feed-forward neural network;MS super-resolution image;QuickBird;WorldView-2;Landsat ETM+ datasets","","19","","50","OAPA","3 Apr 2019","","","IEEE","IEEE Journals"
"GA-SVM Algorithm for Improving Land-Cover Classification Using SAR and Optical Remote Sensing Data","C. Sukawattanavijit; J. Chen; H. Zhang","School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","3","284","288","Multisource remote sensing data have been widely used to improve land-cover classifications. The combination of synthetic aperture radar (SAR) and optical imagery can detect different land-cover types, and the use of genetic algorithms (GAs) and support vector machines (SVMs) can lead to improved classifications. Moreover, SVM kernel parameters and feature selection affect the classification accuracy. Thus, a GA was implemented for feature selection and parameter optimization. In this letter, a GA-SVM algorithm was proposed as a method of classifying multifrequency RADARSAT-2 (RS2) SAR images and Thaichote (THEOS) multispectral images. The results of the GA-SVM algorithm were compared with those of the grid search algorithm, a traditional method of parameter searching. The results showed that the GA-SVM algorithm outperformed the grid search approach and provided higher classification accuracy using fewer input features. The images obtained by fusing RS2 data and THEOS data provided high classification accuracy at over 95%. The results showed improved classification accuracy and demonstrated the advantages of using the GA-SVM algorithm, which provided the best accuracy using fewer features.","1558-0571","","10.1109/LGRS.2016.2628406","Geo-Informatics and Space Technology Development Agency; National Natural Science Foundation of China(grant numbers:61132006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831422","Genetic algorithms (GAs);image fusion;land-cover classification;multisource data;optical imagery;support vector machine (SVM);synthetic aperture radar (SAR)","Support vector machines;Genetic algorithms;Kernel;Synthetic aperture radar;Biological cells;Remote sensing;Adaptive optics","estimation theory;feature selection;geophysical image processing;image classification;land cover;parameter estimation;support vector machines;terrain mapping","GA-SVM algorithm;land-cover classification;optical remote sensing data;synthetic aperture radar;optical imagery;land-cover types;genetic algorithm;support vector machine;SVM kernel parameter;GA feature selection;higher classification accuracy;THEOS data;RS2 data fusion;Thaichote multispectral image;feature selection;multifrequency RADARSAT-2 SAR image classification data;grid search algorithm;traditional parameter searching method","","110","","17","IEEE","24 Jan 2017","","","IEEE","IEEE Journals"
"Cloud Removal Based on SAR-Optical Remote Sensing Data Fusion via a Two-Flow Network","R. Mao; H. Li; G. Ren; Z. Yin","School of Resource and Environmental Engineering, Wuhan University of Technology, Wuhan, China; School of Resource and Environmental Engineering, Wuhan University of Technology, Wuhan, China; School of Resource and Environmental Engineering, Wuhan University of Technology, Wuhan, China; School of Resource and Environmental Engineering, Wuhan University of Technology, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Sep 2022","2022","15","","7677","7686","Optical remote sensing imagery plays an important role in observing the Earth's surface today. However, it is not easy to obtain complete multitemporal optical remote sensing images because of the cloud cover, how reconstructing cloud-free optical images has become a big challenge task in recent years. Inspired by the remote sensing fusion methods based on the convolutional neural network model, we propose a two-flow network to remove clouds from optical images. In the proposed method, synthetic aperture radar images are used as auxiliary data to guide optical image reconstruction, which is not influenced by cloud cover. In addition, a novel loss function called content loss is introduced to improve image quality. The ablation experiment of the loss function also proves that content loss is indeed effective. To be more in line with a real situation, the network is trained, tested, and validated on the SEN12MS-CR dataset, which is a global real cloud-removal dataset. The experimental results show that the proposed method is better than other state-of-the-art methods in many indicators (RMSE, SSIM, SAM, and PSNR).","2151-1535","","10.1109/JSTARS.2022.3203508","Key Research & Developmental Program of Shandong Province(grant numbers:2019JZZY0314); National Natural Science Foundation of China(grant numbers:42071358,42171415); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9873922","Cloud removal;data fusion;deep learning;optical;remote sensing;synthetic aperture radar (SAR)","Remote sensing;Clouds;Optical imaging;Optical sensors;Convolutional neural networks;Radar polarimetry;Image reconstruction","geophysical image processing;geophysical techniques;image fusion;image reconstruction;image resolution;neural nets;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","cloud cover;novel loss function;content loss;image quality;cloud-removal dataset;cloud removal;SAR-optical remote sensing data fusion;two-flow network;optical remote sensing imagery;Earth's surface today;complete multitemporal optical remote sensing images;reconstructing cloud-free;big challenge task;remote sensing fusion methods;convolutional neural network model;optical images;synthetic aperture radar images;auxiliary data;optical image reconstruction","","1","","52","CCBY","1 Sep 2022","","","IEEE","IEEE Journals"
"Dual-Branch Multiscale Channel Fusion Unfolding Network for Optical Remote Sensing Image Super-Resolution","M. Shi; Y. Gao; L. Chen; X. Liu","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","5 Dec 2022","2022","19","","1","5","Single-image super-resolution (SR) technology is critical in remote sensing fields because it can effectively improve the details of target images. However, the application of deep learning is limited due to the lack of interpretability and the need for many parameters. This letter proposes an interpretable dual-branch multiscale channel fusion unfolding network (DMUNet) for optical remote sensing image (ORSI) SR. We design an unfolding network with double branches, each optimized with different strategies. Two branches focus on texture and edge reconstruction, respectively. This unfolding network follows the iteration process of the alternating direction method of multipliers (ADMM) and can learn the hyper-parameters adaptively. The functions of the two branches can complement each other. Further, to better fuse the feature maps of the two branches, a multiscale fusion module is proposed. This module can effectively fuse information between different branches, scales, and channels. It is noted that it only requires a little computation cost. Experiments on two public ORSI datasets demonstrate that our method can achieve significant performance in both quantitative evaluation and visual results.","1558-0571","","10.1109/LGRS.2022.3221614","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955482","Alternating direction method of multipliers (ADMM);dual-branch;optical remote sensing image (ORSI);super-resolution (SR);unfolding","Image edge detection;Image reconstruction;Remote sensing;Convolution;Tensors;Superresolution;Kernel","convex programming;geophysical image processing;image fusion;image reconstruction;image resolution;iterative methods;learning (artificial intelligence);remote sensing","ADMM;alternating direction method of multipliers;DMUNet;double branches;dual-branch multiscale channel fusion unfolding network;edge reconstruction;iteration process;multiscale fusion module;optical remote sensing image SR;optical remote sensing image super-resolution;public ORSI datasets;remote sensing fields;single-image super-resolution technology;target images;texture reconstruction","","","","18","IEEE","18 Nov 2022","","","IEEE","IEEE Journals"
"A Novel Cross-Scale Octave Network for Hyperspectral and Multispectral Image Fusion","T. Zhan; Z. Bi; H. Wu; C. Xu; Q. Du; Y. Xu; Z. Wu","Jiangsu Key Construction Laboratory of Audit Information Engineering and the School of Information Engineering, Nanjing Audit University, Nanjing, China; School of Information Engineering, Nanjing Audit University, Nanjing, China; School of Information Engineering, Nanjing Audit University, Nanjing, China; School of Information Engineering, Nanjing Audit University, Nanjing, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","26 Dec 2022","2022","60","","1","16","Recently, deep convolutional neural network-based low-resolution hyperspectral image (LR-HSI) and high-resolution multispectral image (HR-MSI) fusion methods have achieved significant performance improvement. However, the rich spatial and spectral information in HSIs is not fully explored. In this article, we propose a novel cross-scale octave network (CSONet) for hyperspectral and multispectral image fusion. Specifically, we adopt a progressive image fusion structure to effectively extract the spatial and spectral information of HR-MSI at multiple resolutions, thereby efficiently complementing LR-HSI’s information. In addition, the proposed cross-scale octave convolution module can extract rich multiscale spatial feature information and concentrate on more important spatial–spectral features at different scales with the multiscale spatial–spectral attention mechanism. Finally, a multisupervised loss function is used to improve the gradient propagation and enhance the representation ability of the network. Ablation analysis on the benchmark datasets shows the effectiveness of each component in the proposed method. Extensive experimental results on different hyperspectral images demonstrate that the proposed CSONet can achieve superior results and strong generalization ability in comparison with some state-of-the-art LR-HSI and HR-MSI fusion methods.","1558-0644","","10.1109/TGRS.2022.3229086","National Natural Science Foundation of China(grant numbers:61976117,62071233); Natural Science Foundation of Jiangsu Province(grant numbers:BK20191409,BK20211570); Key Projects of University Natural Science Fund of Jiangsu Province(grant numbers:19KJA360001); Research Project of University Natural Science Fund of Jiangsu Province(grant numbers:22KJB520002); Fundamental Research Funds for the Central Universities(grant numbers:30919011103,30919011402,30921011209); Qinglan Project; Postgraduate Research & Practice Innovation Program of Jiangsu Province(grant numbers:KYCX22_2218); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984673","Attention mechanism;convolutional neural network (CNN);cross-scale octave convolution;hyperspectral and multispectral image fusion","Spatial resolution;Hyperspectral imaging;Feature extraction;Superresolution;Data mining;Image reconstruction;Convolution","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;remote sensing","Ablation analysis;cross-scale octave convolution module;cross-scale octave network;CSONet;deep convolutional neural network-based low-resolution hyperspectral image;gradient propagation;high-resolution multispectral image fusion methods;HR-MSI fusion methods;hyperspectral image;LR-HSI information;multiscale spatial feature information;multiscale spatial-spectral attention mechanism;multisupervised loss function;progressive image fusion structure","","1","","55","IEEE","14 Dec 2022","","","IEEE","IEEE Journals"
"Hyper-Sharpening: A First Approach on SIM-GA Data","M. Selva; B. Aiazzi; F. Butera; L. Chiarantini; S. Baronti","Instituto di Fisica Applicata Nello Carrara, CNR Area della Ricerca di Firenze, Sesto Fiorentino, FI, Italy; Instituto di Fisica Applicata Nello Carrara, CNR Area della Ricerca di Firenze, Sesto Fiorentino, FI, Italy; Selex ES, Florence, Italy; Selex ES, Florence, Italy; Instituto di Fisica Applicata Nello Carrara, CNR Area della Ricerca di Firenze, Sesto Fiorentino, FI, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","6","3008","3024","This paper aims at defining a new paradigm (hypersharpening) in remote sensing image fusion. In fact, due to the development of new instruments, thinking only in terms of pansharpening is reductive. Even though some expressions as hyperspectral (HS) pansharpening already exist, there is not a suitable definition when multispectral/hyperspectral data are used as source to extract spatial details. After defining the hypersharpening framework, we draw the readers' attention to its peculiar characteristics, by proposing and evaluating two hypersharpening methods. Experiments are carried out on the data produced by the updated version of SIM-GA imager, designed by Selex ES, which is composed by a panchromatic camera and two spectrometers in the VNIR and SWIR spectral ranges, respectively. Owing to the different resolution factors among panchromatic, VNIR and SWIR data sets, we can apply hypersharpening to fuse SWIR data to VNIR resolution. Comparisons of hypersharpening with “traditional” pansharpening show hypersharpening is more effective.","2151-1535","","10.1109/JSTARS.2015.2440092","Agenzia Spaziale Italiana (ASI)(grant numbers:CNR-IFAC/SOASAR/111213); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7134741","Dimensionality reduction;hypersharpening;hyperspectral (HS);image fusion;pansharpening;remote sensing;Dimensionality reduction;hypersharpening;hyperspectral (HS);image fusion;pansharpening;remote sensing","Spatial resolution;Noise;Instruments;Hyperspectral sensors;Correlation","image fusion;image processing;remote sensing;sensor fusion","hyper-sharpening paradigm;SIM-GA data first approach;remote sensing image fusion;instrument development;reductive pansharpening;hyperspectral pansharpening;HS pansharpening;multispectral-hyperspectral data;hypersharpening framework;hypersharpening method evaluation;updated SIM-GA imager version;Selex ES;panchromatic camera;spectrometer;SWIR spectral range;VNIR spectral range;panchromatic resolution factor;VNIR resolution factor;SWIR resolution factor;SWIR data hypersharpening;VNIR resolution;traditional pansharpening","","136","","61","IEEE","25 Jun 2015","","","IEEE","IEEE Journals"
"Remote-Sensing Image Usability Assessment Based on ResNet by Combining Edge and Texture Maps","L. Xu; Q. Chen","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Jul 2019","2019","12","6","1825","1834","Authentic remote-sensing images suffer non-uniform complex distortions during acquisition, transmission, and storage. Clouds, light, and exposure also affect local quality. This paper constructs a usability-based subjective remote-sensing image dataset and gives a definition of usability for images with non-uniform distortion, where the image usability is determined by the weighted quality of image's blocks. It is difficult to extract the handcraft features from remote-sensing images with complex mixture distortion. Recently, convolutional neural network (CNN) has been introduced into blind quality assessment for images with uniform distortion, which includes feature learning and regression in one processing. In this paper, we first describe and systematically analyze the usability of remote-sensing images in detail. Then, we propose a remote-sensing image usability assessment (RSIUA) method based on a residual network by combining edge and texture maps. The score of remote-sensing image usability was obtained with the weighted averaging of the quality scores of all image blocks, and the weight of each image block was determined by its quality score. We compared the proposed method with three traditional image quality assessment methods, one CNN-based method for images with simulated distortion, and one scale-invariant feature transform-based RSIUA method. The linear correlation coefficient, Spearman's rank ordered correlation coefficient, and root-mean-squared error of experiments demonstrate that our method outperforms all five competitors. The experiments also reveal that the edge and texture maps can improve the performance.","2151-1535","","10.1109/JSTARS.2019.2914715","Suzhou Industral Innovation(grant numbers:SS201759); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718395","Edge detection map;local binary pattern (LBP) texture map;remote-sensing image;residual network (ResNet);usability assessment","Remote sensing;Image edge detection;Usability;Distortion;Image quality;Feature extraction;Image resolution","convolutional neural nets;distortion;feature extraction;geophysical image processing;image fusion;image processing;image texture;learning (artificial intelligence);mean square error methods;object detection;remote sensing;transforms","quality score;image block;traditional image quality assessment methods;CNN-based method;texture maps;authentic remote-sensing images;nonuniform complex distortions;local quality;usability-based;remote-sensing image dataset;nonuniform distortion;weighted quality;complex mixture distortion;blind quality assessment;uniform distortion;remote-sensing image usability assessment method","","9","","43","IEEE","20 May 2019","","","IEEE","IEEE Journals"
"Multisource Remote Sensing Data Classification Based on Convolutional Neural Network","X. Xu; W. Li; Q. Ran; Q. Du; L. Gao; B. Zhang","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2018","2018","56","2","937","949","As a list of remotely sensed data sources is available, how to efficiently exploit useful information from multisource data for better Earth observation becomes an interesting but challenging problem. In this paper, the classification fusion of hyperspectral imagery (HSI) and data from other multiple sensors, such as light detection and ranging (LiDAR) data, is investigated with the state-of-the-art deep learning, named the two-branch convolution neural network (CNN). More specific, a two-tunnel CNN framework is first developed to extract spectral-spatial features from HSI; besides, the CNN with cascade block is designed for feature extraction from LiDAR or high-resolution visual image. In the feature fusion stage, the spatial and spectral features of HSI are first integrated in a dual-tunnel branch, and then combined with other data features extracted from a cascade network. Experimental results based on several multisource data demonstrate the proposed two-branch CNN that can achieve more excellent classification performance than some existing methods.","1558-0644","","10.1109/TGRS.2017.2756851","National Natural Science Foundation of China(grant numbers:NSFC-91638201,61571033); Higher Education and High-Quality and World-Class Universities(grant numbers:PY201619); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8068943","Convolutional neural network (CNN);data fusion;deep learning;feature extraction;hyperspectral imagery (HSI)","Feature extraction;Laser radar;Remote sensing;Convolution;Data mining;Support vector machines;Neural networks","convolution;feature extraction;feedforward neural nets;geophysical image processing;hyperspectral imaging;image fusion;image resolution;optical radar;remote sensing","light detection and ranging data;high-resolution visual image;feature extraction;spectral-spatial features;Earth observation;remotely sensed data sources;convolutional neural network;multisource remote sensing data classification","","269","","60","IEEE","16 Oct 2017","","","IEEE","IEEE Journals"
"Denoising-Based Multiscale Feature Fusion for Remote Sensing Image Captioning","W. Huang; Q. Wang; X. Li","Center for Optical Imagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; Center for Optical Imagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; Center for Optical Imagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","24 Feb 2021","2021","18","3","436","440","With the benefits from deep learning technology, generating captions for remote sensing images has become achievable, and great progress has been made in this field in the recent years. However, a large-scale variation of remote sensing images, which would lead to errors or omissions in feature extraction, still limits the further improvement of caption quality. To address this problem, we propose a denoising-based multi-scale feature fusion (DMSFF) mechanism for remote sensing image captioning in this letter. The proposed DMSFF mechanism aggregates multiscale features with the denoising operation at the stage of visual feature extraction. It can help the encoder-decoder framework, which is widely used in image captioning, to obtain the denoising multiscale feature representation. In experiments, we apply the proposed DMSFF in the encoder-decoder framework and perform the comparative experiments on two public remote sensing image captioning data sets including UC Merced (UCM)-captions and Sydney-captions. The experimental results demonstrate the effectiveness of our method.","1558-0571","","10.1109/LGRS.2020.2980933","National Natural Science Foundation of China(grant numbers:U1864204,61773316,U1801262,61871470,61761130079); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9057472","Deep learning;encoder–decoder;feature fusion;image captioning;multiscale;remote sensing","Feature extraction;Remote sensing;Sensors;Noise reduction;Visualization;Atmospheric modeling;Nonhomogeneous media","feature extraction;geophysical image processing;image denoising;image fusion;image representation;learning (artificial intelligence);remote sensing;wavelet transforms","remote sensing images;caption quality;denoising-based multiscale feature fusion mechanism;DMSFF mechanism aggregates multiscale features;denoising operation;visual feature extraction;encoder-decoder framework;denoising multiscale feature representation;public remote sensing image captioning data;UC Merced-captions;Sydney-captions;deep learning technology","","28","","24","IEEE","6 Apr 2020","","","IEEE","IEEE Journals"
"Boundary Extraction Constrained Siamese Network for Remote Sensing Image Change Detection","J. Lei; Y. Gu; W. Xie; Y. Li; Q. Du","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","28 Apr 2022","2022","60","","1","13","Change detection (CD) is crucial to the understanding of relationships and interactions among multitemporal high-resolution remote sensing (RS) images. However, various inherent attributes of images have different impacts on CD judgment. How to effectively use helpful information to improve the performance of CD is still a challenge. In this article, we present a boundary extraction constrained Siamese network (BESNet) to dig out the efficacy of boundary information. BESNet is a joint learning network in which a novel multiscale boundary extraction (MSBE) module is embedded. In this way, traditional and deep learning techniques are leveraged to learn together to maximize their respective strengths through cooperation. In particular, a new boundary extraction constrained (BEC) loss function combined with a contractive loss function is used to optimize the BESNet. Considering the interaction between various extracted features, a channel-shuffle fusion strategy is developed to exploit their complementary advantages between features. Our experiments show that the proposed BESNet can significantly improve the CD performance and generate more complete and clearer object boundaries. Experiments conducted on two real datasets over different scenes demonstrate its state-of-the-art performance.","1558-0644","","10.1109/TGRS.2022.3165851","National Natural Science Foundation of China(grant numbers:62071360,61571345,91538101,61501346,61502367,61701360); Young Talent Fund of University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:XJS200103); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751745","Boundary extractor;change detection (CD);channel shuffle;high-resolution remote sensing (RS) image;Siamese network","Feature extraction;Image segmentation;Task analysis;Remote sensing;Deep learning;Semantics;Neural networks","feature extraction;geophysical image processing;image classification;image fusion;image processing;image representation;image segmentation;learning (artificial intelligence);object detection;remote sensing","novel multiscale boundary extraction module;traditional learning techniques;deep learning techniques;contractive loss function;BESNet;CD performance;complete object boundaries;clearer object boundaries;boundary extraction constrained Siamese network;remote sensing image change detection;high-resolution remote sensing images;CD judgment;helpful information;boundary information;joint learning network","","2","","45","IEEE","8 Apr 2022","","","IEEE","IEEE Journals"
"Attention-Based Tri-UNet for Remote Sensing Image Pan-Sharpening","W. Zhang; J. Li; Z. Hua","School of Information and Electronic Engineering, Shandong Technology and Business University, Yantai, China; School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Shandong Technology and Business University, Yantai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14 Apr 2021","2021","14","","3719","3732","Pan-sharpening of remote sensing images is a signifi-cant method for integrating remote sensing information in the fieldof computer vision, where complementary and redundant informa-tion between multispectral (MS) images and panchromatic (PAN)images is used to generate high-resolution MS (HRMS) images.Inspired by the remarkable achievements of convolutional neuralnetworks in a variety of computer-vision tasks, we incorporatedomain-specific knowledge to design our attention-based trian-gle UNet (Tri-UNet) architecture to generate high-quality HRMSimages. The attention-based Tri-UNet is mainly divided into thefollowing three modules: 1) feature extraction; 2) feature fusion;and 3) image reconstruction. In the feature extraction step, the fea-ture extraction module simultaneously extracts spectral and spatialinformation from the MS and PAN images. The feature maps arethen fused in the feature fusion module, which makes the finalfeatureimagecontainrichspectralandspatialinformation.Finally,the image reconstruction module generates a high-resolution MSimage that uses the fused image as input. The attention mechanismis introduced into the image reconstruction module to make thenetwork focus more on key information in the feature image. Theexperimental results demonstrate that the proposed method cangenerate high-quality HRMS images. A quantitative comparisonand qualitative analysis of the experimental results indicate thatour method is superior to the existing methods.","2151-1535","","10.1109/JSTARS.2021.3068274","National Natural Science Foundation of China(grant numbers:61772319,62002200,61976125,61976124); Shandong Natural Science Foundation of China(grant numbers:ZR2017MF049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383843","Attention mechanism;convolutional neural networks (CNNs);multispectral (MS) images;pan-sharpening;panchromatic (PAN) images","Remote sensing;Spatial resolution;Feature extraction;Image reconstruction;Image fusion;Sensors;Superresolution","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","convolutional neural network;attention-based Tri-UNet architecture;high-resolution MS imaging;high-quality HRMS imaging;image fusion module;image reconstruction module;feature extraction module;panchromatic imaging;multispectral imaging;computer vision;remote sensing image pan-sharpening","","11","","55","CCBYNCND","23 Mar 2021","","","IEEE","IEEE Journals"
"FDFNet: A Fusion Network for Generating High-Resolution Fully PolSAR Images","L. Lin; H. Shen; J. Li; Q. Yuan","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology and the School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology and the School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","7 Jan 2022","2022","19","","1","5","Deep learning shows potential superiority in the image fusion field. To solve the problem of the spatial resolution degradation of polarimetric synthetic aperture radar (PolSAR) images caused by system limitation, we propose a fully PolSAR images and DualSAR images fusion network (FDFNet). We use low resolution (LR)-PolSAR super-resolution (LPSR) and modified cross attention mechanism (MCroAM) to perform data fusion on LR-PolSAR and high resolution (HR)-dual-polarization synthetic aperture radar (DualSAR) and design a polarimetric decomposition attention module to introduce the polarimetric parameters of LR-PolSAR images to maintain polarimetric information. Besides, we use the differential information between LR-PolSAR and HR-DualSAR to guide spatial resolution reconstruction. The loss function based on the  $L_{1} $  norm is used to constrain the network training process. The experimental results show the superiority of the proposed method over the existing methods in visual and quantitative evaluation. In addition, polarimetric decomposition experiments verify the effectiveness of the proposed method to maintain polarimetric information.","1558-0571","","10.1109/LGRS.2021.3127958","National Natural Science Foundation of China(grant numbers:61671334,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9614198","Differential information;dual-polarization synthetic aperture radar (DualSAR);fully-polarimetric synthetic aperture radar (PolSAR);fusion;polarimetric decomposition","Feature extraction;Spatial resolution;Scattering;Image reconstruction;Superresolution;Image fusion;Handheld computers","geophysical image processing;image fusion;image reconstruction;image resolution;learning (artificial intelligence);radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","generating high-resolution fully PolSAR;image fusion field;spatial resolution degradation;polarimetric synthetic aperture radar images;fully PolSAR images;LR-PolSAR images;polarimetric information;DualSAR images fusion network;low resolution-PolSAR super-resolution","","","","8","IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"Spectral Image Fusion From Compressive Measurements Using Spectral Unmixing and a Sparse Representation of Abundance Maps","E. Vargas; H. Arguello; J. -Y. Tourneret","Department of Electrical Engineering, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia; IRIT/INP-ENSEEIHT/TéSA, University of Toulouse, Toulouse, France","IEEE Transactions on Geoscience and Remote Sensing","24 Jun 2019","2019","57","7","5043","5053","In the past years, one common way of enhancing the spatial resolution of a hyperspectral (HS) image has been to fuse it with complementary information coming from multispectral (MS) or panchromatic images. This paper proposes a new method for reconstructing a high-spatial, high-spectral image from measurements acquired after compressed sensing by multiple sensors of different spectral ranges and spatial resolutions, with specific attention to HS and MS compressed images. To solve this problem, we introduce a fusion model based on the linear spectral unmixing model classically used for HS images and investigate an optimization algorithm based on a block coordinate descent strategy. The nonnegative and sum-to-one constraints resulting from the intrinsic physical properties of abundances as well as a total variation penalization are used to regularize this ill-posed inverse problem. Simulation results conducted on realistic compressed HS and MS images show that the proposed algorithm can provide fusion results that are very close to those obtained with uncompressed images, with the advantage of using a significantly reduced number of measurements.","1558-0644","","10.1109/TGRS.2019.2895822","Universidad Industrial de Santander(grant numbers:2436,ECOS Nord); intercambio de investigadores Colombia-Francia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8657365","Compressive sampling;data fusion;remote sensing;spectral imaging","Image coding;Spatial resolution;Sensors;Imaging;Image fusion;Fuses","compressed sensing;geophysical image processing;image fusion;image reconstruction;image resolution;inverse problems;optimisation;remote sensing;sensor fusion","spatial resolution;hyperspectral image;complementary information;MS;panchromatic images;high-spectral image;compressed sensing;multiple sensors;fusion model;linear spectral unmixing model;HS images;nonnegative sum-to-one constraints;fusion results;uncompressed images;spectral image fusion;compressive measurements;sparse representation;abundance maps;spectral ranges;spatial resolutions","","14","","45","IEEE","1 Mar 2019","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion Based on Local Low Rank and Coupled Spectral Unmixing","Y. Zhou; L. Feng; C. Hou; S. -Y. Kung","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Geoscience and Remote Sensing","25 Sep 2017","2017","55","10","5997","6009","Hyperspectral images (HSIs) usually have high spectral and low spatial resolution. Conversely, multispectral images (MSIs) usually have low spectral and high spatial resolution. The fusion of HSI and MSI aims to create spectral images with high spectral and spatial resolution. In this paper, we propose a fusion algorithm by combining linear spectral unmixing with the local low-rank property. By taking advantage of the local low-rank property, we first partition the corresponding spectral image into patches. For each patch pair, we cast the fusion problem as a coupled spectral unmixing problem that extracts the abundance and the endmembers of MSI and HSI, respectively. It then updates the abundance and the endmember through an alternating update algorithm. In fact, the convergence of the alternative update algorithm can be mathematically and empirically supported. We also propose a multiscale postprocessing procedure to combine fusion results obtained under different patch sizes. In experiments on three data sets, the proposed fusion algorithms outperformed state-of-the-art fusion algorithms in both spatial and spectral domains.","1558-0644","","10.1109/TGRS.2017.2718728","National Natural Science Foundation of China(grant numbers:61571326,61471262,61520106002); National Natural Science Foundation of Tianjin(grant numbers:16JCQNJC00900); Brandeis Program of the Defense Advanced Research Project Agency and Space and Naval Warfare System Center Pacific(grant numbers:66001-15-C-4068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004518","Hyperspectral images (HSIs);image fusion;local low rank;multispectral images (MSIs);spectral unmixing","Spatial resolution;Algorithm design and analysis;Image reconstruction;Image fusion;Sensors;Hyperspectral imaging","convergence;hyperspectral imaging;image fusion;image resolution;spectral analysis","hyperspectral image fusion;multispectral image fusion;coupled spectral unmixing;spectral resolution;spatial resolution;fusion algorithm;linear spectral unmixing;local low-rank property;convergence;update algorithm;multiscale postprocessing procedure;patch size","","42","","39","IEEE","8 Aug 2017","","","IEEE","IEEE Journals"
"YOLOrs: Object Detection in Multimodal Remote Sensing Imagery","M. Sharma; M. Dhanaraj; S. Karnam; D. G. Chachlakis; R. Ptucha; P. P. Markopoulos; E. Saber","Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Jan 2021","2021","14","","1497","1508","Deep-learning object detection methods that are designed for computer vision applications tend to underperform when applied to remote sensing data. This is because contrary to computer vision, in remote sensing, training data are harder to collect and targets can be very small, occupying only a few pixels in the entire image, and exhibit arbitrary perspective transformations. Detection performance can improve by fusing data from multiple remote sensing modalities, including red, green, blue, infrared, hyperspectral, multispectral, synthetic aperture radar, and light detection and ranging, to name a few. In this article, we propose YOLOrs: a new convolutional neural network, specifically designed for real-time object detection in multimodal remote sensing imagery. YOLOrs can detect objects at multiple scales, with smaller receptive fields to account for small targets, as well as predict target orientations. In addition, YOLOrs introduces a novel mid-level fusion architecture that renders it applicable to multimodal aerial imagery. Our experimental studies compare YOLOrs with contemporary alternatives and corroborate its merits.","2151-1535","","10.1109/JSTARS.2020.3041316","National Geospatial-Intelligence Agency(grant numbers:# HM0476-19-1-2014); National Science Foundation(grant numbers:OAC-1 808 582); Air Force Office of Scientific Research(grant numbers:FA9550-20-1-0039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273212","Aerial imagery;fusion;multimodal;object detection;remote sensing (RS)","Feature extraction;Remote sensing;Object detection;Computer architecture;Real-time systems;Head;Computer vision","computer vision;convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image fusion;object detection;remote sensing","YOLOrs;real-time object detection;multimodal remote sensing imagery;multimodal aerial imagery;deep-learning object detection;computer vision;remote sensing data;training data;arbitrary perspective transformations;detection performance;remote sensing modalities;hyperspectral aperture radar;multispectral aperture radar;synthetic aperture radar;light detection;mid-level fusion architecture","","10","","50","CCBY","30 Nov 2020","","","IEEE","IEEE Journals"
"Model-Based Fusion of Multi- and Hyperspectral Images Using PCA and Wavelets","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson; J. A. Benediktsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","7 Jan 2015","2015","53","5","2652","2663","In remote sensing, due to cost and complexity issues, multispectral (MS) and hyperspectral (HS) sensors have significantly lower spatial resolution than panchromatic (PAN) images. Recently, the problem of fusing coregistered MS and HS images has gained some attention. In this paper, we propose a novel method for fusion of MS/HS and PAN images and of MS and HS images. MS and, more so, HS images contain spectral redundancy, which makes the dimensionality reduction of the data via principal component (PC) analysis very effective. The fusion is performed in the lower dimensional PC subspace; thus, we only need to estimate the first few PCs, instead of every spectral reflectance band, and without compromising the spectral and spatial quality. The benefits of the approach are substantially lower computational requirements and very high tolerance to noise in the observed data. Examples are presented using WorldView 2 data and a simulated data set based on a real HS image, with and without added noise.","1558-0644","","10.1109/TGRS.2014.2363477","Doctoral Grants of the University of Iceland Research Fund; University of Iceland Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6951484","Image fusion;maximum a posteriori probability (MAP);principal component analysis (PCA);wavelets;Image fusion;maximum a posteriori probability (MAP);principal component analysis (PCA);wavelets","Loading;Principal component analysis;Spatial resolution;Noise;Hyperspectral sensors;Wavelet transforms","data reduction;geophysical image processing;hyperspectral imaging;image fusion;principal component analysis;remote sensing;spectral analysis;wavelet transforms","model-based fusion;multispectral image sensor;hyperspectral image sensor;PCA;wavelets;remote sensing;coregistered MS image fusion;coregistered HS image fusion;PAN image fusion;spectral redundancy;dimensionality reduction;principal component analysis;spectral reflectance band;spectral quality;spatial quality;WorldView 2 data","","109","","38","IEEE","10 Nov 2014","","","IEEE","IEEE Journals"
"Improved Faster R-CNN With Multiscale Feature Fusion and Homography Augmentation for Vehicle Detection in Remote Sensing Images","H. Ji; Z. Gao; T. Mei; Y. Li","Electronic and Information School, Wuhan University, Wuhan, China; Temasek Laboratories, National University of Singapore; Electronic and Information School, Wuhan University, Wuhan, China; Department of Electrical and Computer Engineering, University of Macau, Macau, China","IEEE Geoscience and Remote Sensing Letters","30 Oct 2019","2019","16","11","1761","1765","Vehicle detection in remote sensing images has attracted remarkable attention for its important role in a variety of applications in traffic, security, and military fields. Motivated by the stunning success of region convolutional neural network (R-CNN) techniques, which have achieved the state-of-the-art performance in object detection task on benchmark data sets, we propose to improve the Faster R-CNN method with better feature extraction, multiscale feature fusion, and homography data augmentation to realize vehicle detection in remote sensing images. Extensive experiments on representative remote sensing data sets related to vehicle detection demonstrate that our method achieves better performance than the state-of-the-art approaches. The source code will be made available (after the review process).","1558-0571","","10.1109/LGRS.2019.2909541","Wuhan Institute Key Project(grant numbers:1WHS20171003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8694857","Data augmentation;faster region convolutional neural network (R-CNN);feature fusion;remote sensing images;vehicle detection","Feature extraction;Remote sensing;Vehicle detection;Proposals;Task analysis;Benchmark testing;Object detection","convolutional neural nets;feature extraction;geophysical image processing;image fusion;object detection;remote sensing;road vehicles;traffic engineering computing","multiscale feature fusion;homography data augmentation;vehicle detection;remote sensing images;representative remote sensing data;region convolutional neural network techniques;object detection task;faster R-CNN method;feature extraction","","25","","26","IEEE","22 Apr 2019","","","IEEE","IEEE Journals"
"Target-Guided Feature Super-Resolution for Vehicle Detection in Remote Sensing Images","J. Li; Z. Zhang; Y. Tian; Y. Xu; Y. Wen; S. Wang","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Key Laboratory of Aerospace Information Applications of CETC, CETC 54th Research Institute, Shijiazhuang, China; Key Laboratory of Aerospace Information Applications of CETC, CETC 54th Research Institute, Shijiazhuang, China","IEEE Geoscience and Remote Sensing Letters","4 Jan 2022","2022","19","","1","5","Vehicle detection in remote sensing images remains a challenge because most vehicles are small and cover only a relatively small area due to the low ground sample distance. Although image super-resolution can improve small object detection performance as a preprocessing step, methods for improving the quality of the entire image tend to focus on the majority backgrounds that are not important for detection and involve high computational cost. Inspired by the promising feature-level super-resolution method, in this letter, we propose a novel anchor-free vehicle detection network for small vehicle detection in remote sensing images. Specifically, a target-guided feature super-resolution network is proposed to enhance the features of the potential target. Besides, we propose a novel feature fusion module to improve the feature representation of shallow layers, which accounts for small object detection. Extensive experiments on three public remote sensing detection datasets [cars overhead with context (COWC), Vehicle Detection in Aerial Imagery (VEDAI), and UCAS-AOD] amply demonstrate that our method can achieve significant performance with a mean average precision of 0.933, 0.756, and 0.961, respectively.","1558-0571","","10.1109/LGRS.2021.3112172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548683","Feature super-resolution;remote sensing images;vehicle detection","Feature extraction;Superresolution;Detectors;Vehicle detection;Remote sensing;Object detection;Semantics","feature extraction;geophysical image processing;image fusion;image representation;image resolution;object detection;remote sensing","remote sensing images;image super-resolution;object detection performance;promising feature-level super-resolution method;novel anchor-free vehicle detection network;target-guided feature super-resolution network;feature fusion module;public remote sensing detection datasets","","4","","25","IEEE","27 Sep 2021","","","IEEE","IEEE Journals"
"P3Net: Pansharpening via Pyramidal Detail Injection With Deep Physical Constraints","K. Jon; J. Liu; L. -J. Deng; W. Zhu","Faculty of Mathematics, Kim Il Sung University, Pyongyang, D.P.R. Korea; Key Laboratory for Applied Statistics of MOE, School of Mathematics and Statistics, Northeast Normal University, Changchun, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Key Laboratory for Applied Statistics of MOE, School of Mathematics and Statistics, Northeast Normal University, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","28 Oct 2022","2022","60","","1","18","Pansharpening is an image fusion process aiming to generate high-resolution multispectral (HRMS) images from a pair of low-resolution multispectral (LRMS) images and a high-resolution PAN image. It is a fundamental and significant task for the widespread use of remote sensing images. This article proposes a new residual-learning-based multispectral pansharpening network constrained by two deep physical models, collectively termed as P3Net. It mainly consists of the mainstream PDFNet and the other two auxiliary physical models, M2PNet and H2LNet. Unlike the existing methods of processing only one image scale, the proposed PDFNet fully extracts the spatial details from the multilevel image pyramid with decreasing spatial scales. Then, the spatial information is injected into the upsampled LRMS image. Since the pansharpened result should be consistent with the observed inputs under the physics models, we learn deep pansharpening physics models to reflect the inverse relationships. In detail, we propose the lightweight M2PNet and H2LNet to represent the latent nonlinear mappings from the HRMS image to the panchromatic (PAN) image and the LRMS image, respectively. The two pretrained physics models are frozen and guide the training of the PDFNet, so as to drive clear physical interpretability and further suppress the spectral and spatial distortions. The comparative experiments with the existing state-of-the-art pansharpening methods on the QuickBird, GaoFen, and WorldView test sets demonstrate the superiority of the proposed method in terms of both quantitative metrics and subjective visual effects. The codes are available at https://github.com/KSJhon/PyramidPanWithPhysics.","1558-0644","","10.1109/TGRS.2022.3214209","National Key Research and Development Program of China(grant numbers:2020YFA0714102); National Science Foundation of China(grant numbers:12171077,11771072,12071069,12271083); Science and Technology Development Plan of Jilin Province(grant numbers:20191008004TC); Fundamental Research Funds for the Central Universities(grant numbers:2412020FZ023); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0501); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918041","Image fusion;image pyramid;pansharpening;physical model;remote sensing","Pansharpening;Sensors;Convolutional neural networks;Training;Remote sensing;Physics;Image sensors","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing","P3Net;pyramidal detail injection;deep physical constraints;image fusion process;high-resolution multispectral images;low-resolution multispectral images;high-resolution PAN image;fundamental task;remote sensing images;multispectral pansharpening network;deep physical models;mainstream PDFNet;auxiliary physical models;H2LNet;image scale;spatial details;multilevel image pyramid;spatial scales;spatial information;upsampled LRMS image;pansharpened result;deep pansharpening physics models;lightweight M2PNet;HRMS image;panchromatic image;pretrained physics models;clear physical interpretability;spectral distortions;spatial distortions;existing state-of-the-art pansharpening methods","","1","","69","IEEE","13 Oct 2022","","","IEEE","IEEE Journals"
"A Fast and Accurate Basis Pursuit Denoising Algorithm With Application to Super-Resolving Tomographic SAR","Y. Shi; X. X. Zhu; W. Yin; R. Bamler","Chair of Remote Sensing Technology, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Department of Mathematics, University of California at Los Angeles, Los Angeles, CA, USA; Chair of Remote Sensing Technology, Technical University of Munich, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","26 Sep 2018","2018","56","10","6148","6158"," $L_{1}$  regularization is used for finding sparse solutions to an underdetermined linear system. As sparse signals are widely expected in remote sensing, this type of regularization scheme and its extensions have been widely employed in many remote sensing problems, such as image fusion, target detection, image super-resolution, and others, and have led to promising results. However, solving such sparse reconstruction problems is computationally expensive and has limitations in its practical use. In this paper, we proposed a novel efficient algorithm for solving the complex-valued  $L_{1}$  regularized least squares problem. Taking the high-dimensional tomographic synthetic aperture radar (TomoSAR) as a practical example, we carried out extensive experiments, both with the simulation data and the real data, to demonstrate that the proposed approach can retain the accuracy of the second-order methods while dramatically speeding up the processing by one or two orders. Although we have chosen TomoSAR as the example, the proposed method can be generally applied to any spectral estimation problems.","1558-0644","","10.1109/TGRS.2018.2832721","H2020 European Research Council(grant numbers:ERC-2016-StG-714087); Helmholtz Association(grant numbers:VH-NG-1018); Munich Aerospace e.V. Fakultat fur Luft-und Raumfahrt; Bavaria California Technology Center through the project Large-Scale Problems in Earth Observation; Gauss Centre for Supercomputing e.V.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412239","Basis pursuit denoising (BPDN);L₁ regularization;proximal gradient (PG);second-order cone programming (SOCP);TomoSAR","Synthetic aperture radar;Image reconstruction;Image resolution;Signal to noise ratio;Estimation;Remote sensing;Sensors","image denoising;image fusion;image reconstruction;image resolution;least squares approximations;radar imaging;remote sensing by radar;synthetic aperture radar","super-resolving tomographic SAR;underdetermined linear system;sparse signals;remote sensing problems;image fusion;target detection;image super-resolution;sparse reconstruction problems;high-dimensional tomographic synthetic aperture radar;TomoSAR;simulation data;spectral estimation problems;regularized least squares problem","","21","","23","OAPA","17 Jul 2018","","","IEEE","IEEE Journals"
"Hierarchical Attention and Bilinear Fusion for Remote Sensing Image Scene Classification","D. Yu; H. Guo; Q. Xu; J. Lu; C. Zhao; Y. Lin","PLA Strategic Support Force Information Engineering University, Zheng Zhou, China; PLA Strategic Support Force Information Engineering University, Zheng Zhou, China; PLA Strategic Support Force Information Engineering University, Zheng Zhou, China; PLA Strategic Support Force Information Engineering University, Zheng Zhou, China; PLA Strategic Support Force Information Engineering University, Zheng Zhou, China; PLA Strategic Support Force Information Engineering University, Zheng Zhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Nov 2020","2020","13","","6372","6383","Remote sensing image scene classification is an important means for the understanding of remote sensing images. Convolutional neural networks (CNNs) have been successfully applied to remote sensing image scene classification and have demonstrated remarkable performance. However, with improvements in image resolution, remote sensing image categories are becoming increasingly diverse, and problems such as high intraclass diversity and high interclass similarity have arisen. The performance of ordinary CNNs at distinguishing increasingly complex remote sensing images is still limited. Therefore, we propose a feature fusion framework based on hierarchical attention and bilinear pooling called HABFNet for the scene classification of remote sensing images. First, the deep CNN ResNet50 is used to extract the deep features from different layers of the image, and these features are fused to boost their robustness and effectiveness. Second, we design an improved channel attention scheme to enhance the features from different layers. Finally, the enhanced features are cross-layer bilinearly pooled and fused, and the fused features are used for classification. Extensive experiments were conducted on three publicly available remote sensing image benchmarks. Comparisons with the state-of-the-art methods demonstrated that the proposed HABFNet achieved competitive classification performance.","2151-1535","","10.1109/JSTARS.2020.3030257","National Natural Science Foundation of China(grant numbers:41601507); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222574","Bilinear pooling;channel attention;hierarchical feature fusion;remote sensing image;scene classification","Remote sensing;Feature extraction;Visualization;Task analysis;Semantics;Object detection","convolutional neural nets;feature extraction;geophysical image processing;image classification;image enhancement;image fusion;image resolution;remote sensing","remote sensing image;remote sensing image scene classification;image resolution;bilinear fusion;convolutional neural networks;intraclass diversity;feature fusion framework;HABFNet;CNN ResNet50;channel attention scheme","","18","","66","CCBY","13 Oct 2020","","","IEEE","IEEE Journals"
"Satellite Video Super-Resolution via Multiscale Deformable Convolution Alignment and Temporal Grouping Projection","Y. Xiao; X. Su; Q. Yuan; D. Liu; H. Shen; L. Zhang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","19","As a new earth observation tool, satellite video has been widely used in remote-sensing field for dynamic analysis. Video super-resolution (VSR) technique has thus attracted increasing attention due to its improvement to spatial resolution of satellite video. However, the difficulty of remote-sensing image alignment and the low efficiency of spatial–temporal information fusion make poor generalization of the conventional VSR methods applied to satellite videos. In this article, a novel fusion strategy of temporal grouping projection and an accurate alignment module are proposed for satellite VSR. First, we propose a deformable convolution alignment module with a multiscale residual block to alleviate the alignment difficulties caused by scarce motion and various scales of moving objects in remote-sensing images. Second, a temporal grouping projection fusion strategy is proposed, which can reduce the complexity of projection and make the spatial features of reference frames play a continuous guiding role in spatial–temporal information fusion. Finally, a temporal attention module is designed to adaptively learn the different contributions of temporal information extracted from each group. Extensive experiments on Jilin-1 satellite video demonstrate that our method is superior to current state-of-the-art VSR methods.","1558-0644","","10.1109/TGRS.2021.3107352","National Natural Science Foundation of China(grant numbers:41922008,61971319); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530280","Deformable convolution;satellite video;super-resolution (SR);temporal attention (TA);temporal grouping projection","Satellites;Convolution;Optical imaging;Remote sensing;Image reconstruction;Spatial resolution;Optical sensors","feature extraction;image classification;image fusion;image resolution;remote sensing;video signal processing","satellite video super-resolution;multiscale deformable convolution alignment;earth observation tool;remote-sensing field;video super-resolution technique;spatial resolution;remote-sensing image alignment;spatial-temporal information fusion;conventional VSR methods;accurate alignment module;satellite VSR;deformable convolution alignment module;alignment difficulties;remote-sensing images;temporal grouping projection fusion strategy;temporal attention module;Jilin-1 satellite video;current state-of-the-art VSR methods","","11","","66","IEEE","6 Sep 2021","","","IEEE","IEEE Journals"
"Branch Feature Fusion Convolution Network for Remote Sensing Scene Classification","C. Shi; T. Wang; L. Wang","College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; Department of Communication Engineering, Qiqihar University, Qiqihar, China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Sep 2020","2020","13","","5194","5210","Convolutional neural networks (CNNs) have outstanding advantages in the classification of remote sensing scenes. Deep CNN models with better classification performance typically have high complexity, whereas shallow CNN models with low complexity rarely achieve good classification performance for remote sensing images with complex spatial structures. In this article, we proposed a new lightweight CNN classification method based on branch feature fusion (LCNN-BFF) for remote sensing scene classification. In contrast to a conventional single linear convolution structure, the proposed model had a bilinear feature extraction structure. The BFF method was utilized to fuse the feature information extracted from the two branches, which improved the classification accuracy. In addition, combining depthwise separable convolution and conventional convolution to extract image features greatly reduced the complexity of the model on the premise of ensuring the accuracy of classification. We tested the method on four standard datasets. The experimental results showed that, compared with recent classification methods, the number of weight parameters of the proposed method only accounted for less than 5% of the other methods; however, the classification accuracy was equivalent to or even superior to certain high-performance classification methods.","2151-1535","","10.1109/JSTARS.2020.3018307","National Natural Science Foundation of China(grant numbers:41701479,61675051); China Postdoctoral Science Foundation(grant numbers:2017M621246); Postdoctoral Science Foundation of Heilongjiang Province of China(grant numbers:LBH-Z17052); Science Foundation of Heilongjiang Province of China(grant numbers:QC2018045); Fundamental Research Funds in Heilongjiang Provincial Universities of China(grant numbers:135309342,135309456); Education Science of the 13th Five-year Plan in Heilongjiang Province(grant numbers:GJB1320385); Higher Education Teaching Reform in Heilongjiang Province(grant numbers:SJGY20190718); Innovation and Entrepreneurship Training Program for College Students(grant numbers:201910232059); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172096","Combined convolution (CConv) structures;convolutional neural network (CNN);depthwise separable convolution (DSC);feature extraction;feature fusion;remote sensing scene classification","Feature extraction;Remote sensing;Convolution;Complexity theory;Image classification;Learning systems","convolutional neural nets;feature extraction;image classification;image fusion;remote sensing","convolutional neural networks;branch feature fusion convolution network;high-performance classification methods;image features;conventional convolution;depthwise separable convolution;feature information;bilinear feature extraction structure;conventional single linear convolution structure;remote sensing scene classification;lightweight CNN classification method;complex spatial structures;remote sensing images;shallow CNN models;deep CNN models;remote sensing scenes","","24","","74","CCBY","20 Aug 2020","","","IEEE","IEEE Journals"
"DCFF-Net: A Densely Connected Feature Fusion Network for Change Detection in High-Resolution Remote Sensing Images","F. Pan; Z. Wu; Q. Liu; Y. Xu; Z. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Dec 2021","2021","14","","11974","11985","Change detection is one of the main applications of remote sensing image analysis. Due to the strong capabilities of neural networks in other fields, a growing number of research works of automatic remote sensing change detection focus on deep learning algorithms. The network architectures of change detection are mostly based on the encoder–decoder architecture. Although the encoder–decoder architecture can acquire high-level semantic information for change detection, it still exists some problems in high-resolution remote sensing images, such as the loss of high-resolution location information during the down-sampling process, insufficient high-resolution information during the up-sampling reconstruction process, and small changes are challenging to detect. To address these issues, we propose a densely connected feature fusion network (DCFF-Net) for change detection. First, we extract the multiscale raw image features by two-stream network architecture with the same weights. At the same time, bitemporal images are concatenated as one input with six channels to generate the change map by a difference extraction network based on encoder–decoder architecture. In order to better reconstruct the edge details of the change map and the changes with the small region, an attention mechanism is employed in each up-sampling process to fuse the previously extracted raw image features with difference features. The deep supervision strategy is adopted to alleviate the problem of gradient vanishing. In addition, a novel weighted loss is proposed by combining self-adjusting dice loss and binary cross-entropy loss to alleviate the data imbalance issue. We perform extensive experiments on two public change detection datasets. The visual comparison and quantitative evaluation confirm that our proposed method outperformsother state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2021.3129318","National Natural Science Foundation of China(grant numbers:61772274,62071233,61671243,61976117); Jiangsu Provincial Natural Science Foundation of China(grant numbers:BK20211570,BK20180018,BK20191409); Fundamental Research Funds for the Central Universities(grant numbers:30917015104,30919011103,30919011402,30921011209); China Postdoctoral Science Foundation(grant numbers:2017M611814,2018T110502); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622163","Change detection (CD);deep learning;feature fusion;remote sensing images","Feature extraction;Remote sensing;Semantics;Image resolution;Image segmentation;Network architecture;Data mining","entropy;feature extraction;Gaussian processes;geophysical image processing;image classification;image colour analysis;image fusion;image representation;image resolution;image segmentation;learning (artificial intelligence);neural nets;object detection;remote sensing","extracted raw image features;public change detection datasets;DCFF-Net;densely connected feature fusion network;high-resolution remote sensing images;remote sensing image analysis;automatic remote sensing change detection focus;network architectures;encoder-decoder architecture;high-level semantic information;high-resolution location information;high-resolution information;multiscale raw image features;two-stream network architecture;change map","","5","","46","CCBY","19 Nov 2021","","","IEEE","IEEE Journals"
"Clustering Feature Constraint Multiscale Attention Network for Shadow Extraction From Remote Sensing Images","Y. Xie; D. Feng; X. Shen; Y. Liu; J. Zhu; T. Hussain; S. W. Baik","Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Department of Software, Sejong University, Seoul, South Korea; Department of Software, Sejong University, Seoul, South Korea","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2022","2022","60","","1","14","Shadow extraction is an important and challenging task in remote sensing image analysis because the presence of shadows not only reduces radiation information but also affects the interpretation of remote sensing images. In this article, a clustering feature constraint multiscale attention network for shadow extraction from remote sensing images is proposed. First, in addition to the pixel-level description of the traditional neural network, our method focuses on the clustering relationships between pixel pairs to obtain the pixel group features of shadows. The feature extraction capability of the network is improved with a reweighting mechanism at the pixel level and pixel group features. Second, we employ a feature fusion algorithm by considering contextual information to improve the network’s attention toward shadow areas and enhance the nonlinear expression ability during the encoding and decoding layers. Furthermore, considering the most prominent multiscale features of shadows in remote sensing images, a deep multiscale feature aggregation structure is established to better fit the multiscale feature expression of shadows. Finally, we construct a shadow extraction dataset to verify the proposed approach. We compare our method with the results of state-of-the-art deep learning models. The results show that the intersection over union (IOU) of our method is improved by 0.85%–9.51% and that the  $F1$ -score is improved by 0.73–6.48. In addition, the test results for images with different resolutions prove that the proposed approach is more robust than the other methods.","1558-0644","","10.1109/TGRS.2022.3151901","National Natural Science Foundation of China(grant numbers:U2034202,41871289,42171397); Sichuan Youth Science and Technology Innovation Team(grant numbers:2020JDTD0003); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714371","Clustering feature constraint;contextual information;multiscale attention network;remote sensing image;shadow extraction","Feature extraction;Remote sensing;Data mining;Decoding;Semantics;Deep learning;Convolution","cartography;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image resolution;pattern clustering;remote sensing","multiscale attention network;remote sensing images;pixel-level description;neural network;feature extraction;feature fusion algorithm;deep multiscale feature aggregation structure;shadow extraction dataset;reweighting mechanism;encoding layers;decoding layers;deep learning models;intersection over union;IOU;clustering feature constraint","","4","","55","IEEE","15 Feb 2022","","","IEEE","IEEE Journals"
"Pixel-Level Prediction for Ocean Remote Sensing Image Features Fusion Based on Global and Local Semantic Relations","H. Gao; X. Xiong; L. Cao; D. Yu; G. Yang; L. Yang","First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China; First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China; Institute of Oceanographic Instrumentation, Qilu University of Technology (Shandong Academy of Sciences), Qingdao, China; Institute of Oceanographic Instrumentation, Qilu University of Technology (Shandong Academy of Sciences), Qingdao, China; First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China; Institute of Oceanographic Instrumentation, Qilu University of Technology (Shandong Academy of Sciences), Qingdao, China","IEEE Access","22 Jan 2021","2021","9","","11644","11654","With the rapid development of remote-sensing imaging technology, remote-sensing images have become increasingly diverse, and people are paying more attention to ocean remote-sensing research. Because ocean remote-sensing data are complex, and the ocean environment is diverse, results will differ, even if the same target is detected at different times in the same scene. To obtain more semantic features and better pixel-level prediction capabilities, this paper proposes a pixel-level ocean remote-sensing image algorithm (GLPO-Net) that combines local and global features. First, texture features, color features, and spatial relationship features are extracted. Second, the algorithm constructs a multiscale local cross-attention mechanism strategy to obtain feature weight information in different directions to fully mine the local features of ocean remote-sensing images. Concurrently, an algorithm constructs a multiscale global cross-attention mechanism strategy to obtain global features. Then, the fusion of global features and local features is described in each submodule to obtain more representative deep features. Finally, small-sample ocean remote-sensing is described via image pixel-level prediction. The algorithm proposed in this paper has been tested with three public ocean remote-sensing datasets. The experimental results show that the proposed GLPO-Net algorithm can learn features from small samples of ocean remote-sensing images. Compared to the prediction results of other remote-sensing image algorithms, GLPO-Net exhibits better prediction capabilities.","2169-3536","","10.1109/ACCESS.2021.3052021","National Natural Science Foundation of China(grant numbers:40406009,41376038); National Science and Technology Major Project(grant numbers:2016ZX05057015); Key Technology Research and Development Program of Shandong(grant numbers:2019GHY112017); Youth Innovation and Technology Support Program in Colleges and Universities of Shandong Province(grant numbers:2019KJN009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9326304","Ocean remote sensing;deep learning;features fusion;multi-scale convolutional","Semantics;Remote sensing;Feature extraction;Oceans;Prediction algorithms;Convolution;Sensors","feature extraction;geophysical image processing;image colour analysis;image fusion;image texture;learning (artificial intelligence);neural nets;object detection;oceanography;remote sensing","ocean remote sensing image features fusion;global semantic relation;local semantic relation;remote-sensing imaging technology;ocean remote-sensing data;pixel-level ocean remote-sensing image algorithm;texture feature extraction;color feature extraction;multiscale local cross-attention mechanism strategy;multiscale global cross-attention mechanism strategy;representative deep features;image pixel-level prediction;public ocean remote-sensing datasets;remote-sensing image algorithms;spatial relationship feature extraction;GLPO-Net;target detection","","1","","43","CCBY","18 Jan 2021","","","IEEE","IEEE Journals"
"PSRT: Pyramid Shuffle-and-Reshuffle Transformer for Multispectral and Hyperspectral Image Fusion","S. -Q. Deng; L. -J. Deng; X. Wu; R. Ran; D. Hong; G. Vivone","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; National Biodiversity Future Center, Palermo, NBFC, Italy","IEEE Transactions on Geoscience and Remote Sensing","24 Feb 2023","2023","61","","1","15","A Transformer has received a lot of attention in computer vision. Because of global self-attention, the computational complexity of Transformer is quadratic with the number of tokens, leading to limitations for practical applications. Hence, the computational complexity issue can be efficiently resolved by computing the self-attention in groups of smaller fixed-size windows. In this article, we propose a novel pyramid Shuffle-and-Reshuffle Transformer (PSRT) for the task of multispectral and hyperspectral image fusion (MHIF). Considering the strong correlation among different patches in remote sensing images and complementary information among patches with high similarity, we design Shuffle-and-Reshuffle (SaR) modules to consider the information interaction among global patches in an efficient manner. Besides, using pyramid structures based on window self-attention, the detail extraction is supported. Extensive experiments on four widely used benchmark datasets demonstrate the superiority of the proposed PSRT with a few parameters compared with several state-of-the-art approaches. The related code is available at https://github.com/Deng-shangqi/PSRThttps://github.com/Deng-shangqi/PSRT.","1558-0644","","10.1109/TGRS.2023.3244750","NSFC(grant numbers:12271083,42271350,62203089); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0501,2022NSFSC0507); National Key Research and Development Program of China(grant numbers:2020YFA0714001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044141","Image enhancement;image fusion;multispectral and hyperspectral image fusion (MHIF);pyramid structure;remote sensing;Shuffle-and-Reshuffle (SaR) Transformer","Transformers;Task analysis;Hyperspectral imaging;Feature extraction;Computer vision;Image fusion;Computational modeling","","","","","","67","IEEE","14 Feb 2023","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion Based on a Sparse Representation","Q. Wei; J. Bioucas-Dias; N. Dobigeon; J. -Y. Tourneret","IRIT and INP-ENSEEIHT, University of Toulouse, Toulouse, France; Instituto de Telecomunicações and Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal; IRIT and INP-ENSEEIHT, University of Toulouse, Toulouse, France; IRIT and INP-ENSEEIHT, University of Toulouse, Toulouse, France","IEEE Transactions on Geoscience and Remote Sensing","10 Mar 2015","2015","53","7","3658","3668","This paper presents a variational-based approach for fusing hyperspectral and multispectral images. The fusion problem is formulated as an inverse problem whose solution is the target image assumed to live in a lower dimensional subspace. A sparse regularization term is carefully designed, relying on a decomposition of the scene on a set of dictionaries. The dictionary atoms and the supports of the corresponding active coding coefficients are learned from the observed images. Then, conditionally on these dictionaries and supports, the fusion problem is solved via alternating optimization with respect to the target image (using the alternating direction method of multipliers) and the coding coefficients. Simulation results demonstrate the efficiency of the proposed algorithm when compared with state-of-the-art fusion methods.","1558-0644","","10.1109/TGRS.2014.2381272","Hypanema ANR Project(grant numbers:n°ANR-12-BS03-003); ANR-11-LABX-0040-CIMI within the program(grant numbers:ANR-11-IDEX-0002-02); Portuguese Science and Technology Foundation under Projects(grant numbers:PEst-OE/EEI/LA0008/2013,PTDC/EEI-PRO/1470/2012); Chinese Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010915","Alternating direction method of multipliers (ADMM);dictionary;hyperspectral (HS) image;image fusion;multispectral (MS) image;sparse representation;Alternating direction method of multipliers (ADMM);dictionary;hyperspectral (HS) image;image fusion;multispectral (MS) image;sparse representation","Dictionaries;Optimization;Vectors;Bayes methods;Estimation;Spatial resolution;Hyperspectral imaging","dictionaries;hyperspectral imaging;image fusion;inverse problems","hyperspectral image fusion;multispectral image fusion;sparse representation;variational-based approach;inverse problem;dictionary atoms;active coding coefficients;alternating optimization","","415","","56","IEEE","15 Jan 2015","","","IEEE","IEEE Journals"
"Adaptation and Evaluation of an Optical Flow Method Applied to Coregistration of Forest Remote Sensing Images","G. Brigot; E. Colin-Koeniguer; A. Plyer; F. Janez","DTIM, ONERA, Palaiseau, France; DTIM, ONERA, Palaiseau, France; DTIM, ONERA, Palaiseau, France; DTIM, ONERA, Palaiseau, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Aug 2016","2016","9","7","2923","2939","The coregistration of heterogeneous geospatial images is useful in various remote sensing applications. Since the number of available data increases and the resolution improves, it is interesting to have an approach as automated, fast, robust, and accurate as possible. In this paper, we present a solution based on optical-flow computation. This algorithm called GeFolki allows the registration of images in a nonparametric and dense way. GeFolki is based on a local method of optical flow derived from the Lucas-Kanade algorithm, with a multiscale implementation, and a specific filtering including rank filtering, rolling guidance filtering and local contrast inversion. The efficiency of our coregistration chain is shown on radar, LIDAR, and optical images on Remningstorp forest in Sweden. An analysis of the relevant parameters is investigated for several scenarios. Finally, we demonstrate the accuracy of our coregistration by proposing specific metrics for LIDAR/radar coregistration, and optics/radar coregistration.","2151-1535","","10.1109/JSTARS.2016.2578362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514244","Computer vision;forestry;geophysical image processing;geographic information systems;image fusion;image registration","Optical sensors;Optical imaging;Adaptive optics;Remote sensing;Laser radar;Robustness;Computer vision","forestry;geophysical image processing;image filtering;image fusion;image registration;optical radar;remote sensing by laser beam;remote sensing by radar","optical flow method;forest remote sensing image coregistration;optics-radar coregistration;LIDAR-radar coregistration;Sweden;Remningstorp forest;optical image;local contrast inversion;rolling guidance filtering;rank filtering;specific filtering;Lucas-Kanade algorithm;GeFolki algorithm;optical-flow computation;heterogeneous geospatial image coregistration","","43","","38","IEEE","15 Jul 2016","","","IEEE","IEEE Journals"
"Dual-Attention-Driven Multiscale Fusion Object Searching Network for Remote Sensing Imagery","H. Fu; Q. Li; P. Duan; J. Lin; R. Dian; S. Li; X. Kang; Z. Li","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; School of Robotics, Hunan University, Changsha, China; College of Electrical and Information Engineering and the Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; School of Robotics and the Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; College of Electrical and Information Engineering and the Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; School of Robotics, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Sep 2022","2022","15","","8131","8141","Object search is a challenging yet important task. Many efforts have been made to address this issue and achieve great progress in natural image, yet searching all the specified types of objects from remote sensing image is barely studied. In this article, we are interested in searching objects from remote sensing images. Compared to person search in natural scenes, this task is challenging in two factors: One is that remote image usually contains a large number of objects, which poses a great challenge to characterize the object features; another is that the objects in remote sensing images are dense, which easily yield erroneous localization. To address these issues, we propose a new end-to-end deep learning framework for object search in remote sensing images. First, we propose a multiscale feature aggregation module, which strengthens the representation of low-level features by fusing multilayer features. The fused features with richer details significantly improve the accuracy of object search. Second, we propose a dual-attention object enhancement module to enhance features from channel and spatial dimensions. The enhanced features significantly improve the localization accuracy for dense objects. Finally, we built two challenging datasets based on the remote sensing images, which contain complex changes in space and time. The experiments and comparisons demonstrate the state-of-the-art performance of our method on the challenging datasets.","2151-1535","","10.1109/JSTARS.2022.3207302","National Key R&D Program of China(grant numbers:2021YFA0715203); National Key Research and Development Program of China(grant numbers:2018YFB1308604); National Natural Science Foundation of China(grant numbers:U21A20518,61976086,62201205); Science and Technology Project of State Grid(grant numbers:5100-202123009A); Special Project of Foshan Science and Technology Innovation Team(grant numbers:FS0AA-KJ919-4402-0069); Changsha Natural Science Foundation of China(grant numbers:kq2202170); Changsha Natural Science Foundation(grant numbers:kq2202171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893909","Attention-based module;deep learning;object searching;remote sensing image","Search problems;Remote sensing;Feature extraction;Task analysis;Semantics;Object detection;Sensors","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);object detection;remote sensing","multiscale fusion object searching network;remote sensing imagery;object search;remote sensing image;remote image;object features;dual-attention object enhancement module;dense objects","","","","50","CCBY","16 Sep 2022","","","IEEE","IEEE Journals"
"Multicrop Fusion Strategy Based on Prototype Assignment for Remote Sensing Image Scene Classification","S. Ma; B. Hou; X. Guo; Z. Li; Z. Wu; S. Wang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, and Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, and Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, and Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, and Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, and Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, and Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education of China, and Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","10 Nov 2022","2022","60","","1","12","The gap between self-supervised visual representation learning and supervised learning is gradually closing. Self-supervised learning does not rely on a large amount of labeled data and reduces the loss of human labeled information. Compared with natural images, remote sensing images require rich samples and human annotation by experts. Moreover, many algorithms have poor interpretability and unconvincing results. Therefore, this article proposes a self-supervised method based on prototype assignment by designing a pretext task so that the network maps features to prototypes in the process of learning, swaps the code corresponding to the obtained features, combines them with another data-enhancing feature, and then optimizes the network. The prototype is introduced to explain the clustering idea embodied in the whole process. Considering the existence of the scene information-rich characteristic of remote sensing images, we introduce multiple views with different resolutions to capture more detailed information on the images. Finally, if the data enhancement method is not powerful enough, the network can easily fall into an overfitting state, which prevents the network from learning subtle differences and detailed information. To address this shortcoming, we propose a fusion strategy to flatten the decision boundary of the framework so that the model can also learn the soft similarity between sample pairs. We name the whole framework multiview fusion feature sample mining based on prototype clustering (MFPC). In extensive experiments conducted on three common remote sensing image datasets (i.e., UCMerced, AID, and NWPU45), MFPC achieves a maximum improvement of 4.3% over some existing self-supervised algorithms, indicating that it can achieve good results.","1558-0644","","10.1109/TGRS.2022.3216831","National Natural Science Foundation of China(grant numbers:62171347,61877066,61771379,62001355,62101405); Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-05,2021ZDLGY02-08); Science and Technology Program, Xi’an, China(grant numbers:XA2020-RGZNTJ-0021); Higher Education Discipline Innovation Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933708","Clustering idea;fusion strategy;interpretability;multiple views;prototype assignment;self-supervised learning","Remote sensing;Task analysis;Self-supervised learning;Prototypes;Training;Codes;Predictive models","data mining;geophysical image processing;image classification;image fusion;learning (artificial intelligence);pattern classification;pattern clustering;remote sensing;unsupervised learning","common remote sensing image datasets;data enhancement method;data-enhancing feature;existing self-supervised algorithms;fusion feature sample mining;fusion strategy;human annotation;human labeled information;natural images;network maps features;poor interpretability;prototype assignment;prototype clustering;prototypes;remote sensing image scene classification;remote sensing images;rich samples;scene information-rich characteristic;self-supervised learning;self-supervised method;self-supervised visual representation learning;unconvincing results","","4","","47","IEEE","31 Oct 2022","","","IEEE","IEEE Journals"
"Multilayer Degradation Representation-Guided Blind Super-Resolution for Remote Sensing Images","X. Kang; J. Li; P. Duan; F. Ma; S. Li","School of Robotics, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","3 Aug 2022","2022","60","","1","12","Remote sensing image super-resolution (SR) aims to boost the image resolution while recovering rich high-frequency details. Currently, most of the SR methods are based on an assumption that the degradation kernel is a specific downsampler. However, the degradation kernel is unknown and sophisticated for real remote sensing scenes, leading to a severe performance drop. To alleviate this problem, we propose a multilayer degradation representation-guided blind SR method for remote sensing images, which mainly consists of three key steps. First, an unsupervised representation learning is exploited to learn the degradation representation from low-resolution images. Then, a degradation-guided deep residual module is designed to model high-order features across different scales from the original images. Finally, a multilayer degradation-aware feature fusion mechanism is proposed to restore the finer details. Experiments on synthetic and real datasets demonstrate that the proposed method can achieve promising performance with respect to other state-of-the-art SR approaches.","1558-0644","","10.1109/TGRS.2022.3192680","National Key Research and Development Program of China(grant numbers:2021YFA0715203); Major Program of the National Natural Science Foundation of China(grant numbers:61890962); National Natural Science Foundation of China(grant numbers:61871179); Scientific Research Project of Hunan Education Department(grant numbers:19B105); National Science Foundation of Hunan Province(grant numbers:2019JJ50036,2020GK2038); Hunan Provincial Natural Science Foundation for Distinguished Young Scholars(grant numbers:2021JJ022); Huxiang Young Talents Science and Technology Innovation Program(grant numbers:2020RC3013); Changsha Natural Science Foundation(grant numbers:kq2202171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833534","Blind super-resolution (SR);degradation-guided feature extraction;multilayer feature fusion;remote sensing image;representation learning","Degradation;Feature extraction;Superresolution;Remote sensing;Kernel;Image reconstruction;Imaging","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image reconstruction;image representation;image resolution;remote sensing;unsupervised learning","multilayer degradation representation-guided blind super-resolution;remote sensing image super-resolution;SR methods;degradation kernel;unsupervised representation learning;degradation-guided deep residual module;multilayer degradation-aware feature fusion mechanism","","","","57","IEEE","20 Jul 2022","","","IEEE","IEEE Journals"
"Boosting the Accuracy of Multispectral Image Pansharpening by Learning a Deep Residual Network","Y. Wei; Q. Yuan; H. Shen; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Resource and Environmental Science, Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","25 Sep 2017","2017","14","10","1795","1799","In the field of multispectral (MS) and panchromatic image fusion (pansharpening), the impressive effectiveness of deep neural networks has recently been employed to overcome the drawbacks of the traditional linear models and boost the fusion accuracy. However, the existing methods are mainly based on simple and flat networks with relatively shallow architectures, which severely limits their performance. In this letter, the concept of residual learning is introduced to form a very deep convolutional neural network to make the full use of the high nonlinearity of the deep learning models. Through both quantitative and visual assessments on a large number of high-quality MS images from various sources, it is confirmed that the proposed model is superior to all the mainstream algorithms included in the comparison, and achieves the highest spatial-spectral unified accuracy.","1558-0571","","10.1109/LGRS.2017.2736020","National Natural Science Foundation of China(grant numbers:41401383,41422108); Fundamental Research Funds for the Central Universities(grant numbers:2042017kf0180); Natural Science Foundation of Hubei Province(grant numbers:ZRMS2016000241); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8012503","Convolutional neural network;data fusion;pansharpening;remote sensing;residual learning","Machine learning;Neural networks;Spectral analysis;Spatial resolution;Training;Feature extraction","convolution;image colour analysis;image fusion;image resolution;learning (artificial intelligence);neural nets","multispectral image pansharpening;deep residual network;panchromatic image fusion;residual learning;deep convolutional neural network;image super-resolution","","309","","24","IEEE","17 Aug 2017","","","IEEE","IEEE Journals"
"Change Detection in Heterogeneous Optical and SAR Remote Sensing Images Via Deep Homogeneous Feature Fusion","X. Jiang; G. Li; Y. Liu; X. -P. Zhang; Y. He","Institute of Information Fusion, Naval Aeronautical University, Yantai, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Institute of Information Fusion, Naval Aeronautical University, Yantai, China; Computer and Biomedical Engineering, Department of Electrical, Ryerson University, Toronto, ON, Canada; Institute of Information Fusion, Naval Aeronautical University, Yantai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 May 2020","2020","13","","1551","1566","Change detection in heterogeneous remote sensing images is crucial for disaster damage assessment. Recent methods use homogenous transformation, which transforms the heterogeneous optical and synthetic aperture radar (SAR) remote sensing images into the same feature space, to achieve change detection. Such transformations mainly operate on the low-level feature space and may corrupt the semantic content, deteriorating the performance of change detection. To solve this problem, this article presents a new homogeneous transformation model termed deep homogeneous feature fusion (DHFF) based on image style transfer (IST). Unlike the existing methods, the DHFF method segregates the semantic content and the style features in the heterogeneous images to perform homogeneous transformation. The separation of the semantic content and the style in the homogeneous transformation prevents the corruption of image semantic content, especially in the regions of change. In this way, the detection performance is improved with accurate homogeneous transformation. Furthermore, we present a new iterative IST strategy, where the cost function in each IST iteration measures and thus maximizes the feature homogeneity in additional new feature subspaces for change detection. After that, change detection is accomplished accurately on the original and the transformed images that are in the same feature space. Real remote sensing images acquired by SAR and optical satellites are utilized to evaluate the performance of the proposed method. The experiments demonstrate that the proposed DHFF method achieves significant improvement for change detection in heterogeneous optical and SAR remote sensing images in terms of both accuracy rate and Kappa index.","2151-1535","","10.1109/JSTARS.2020.2983993","National Natural Science Foundation of China(grant numbers:61790551,61925106); Civil Space Advance Research Program of China(grant numbers:D010305); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9057421","Change detection;heterogeneous;image style transfer (IST);remote sensing","Feature extraction;Synthetic aperture radar;Optical sensors;Remote sensing;Optical imaging;Semantics;Cost function","feature extraction;geophysical image processing;image fusion;object detection;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","IST iteration measures;deep homogeneous feature fusion;homogeneous transformation model;low-level feature space;homogenous transformation;heterogeneous remote sensing images;SAR remote sensing images;transformed images;change detection;feature homogeneity;image semantic content;DHFF method;image style transfer","","25","","42","CCBY","6 Apr 2020","","","IEEE","IEEE Journals"
"AttentionFGAN: Infrared and Visible Image Fusion Using Attention-Based Generative Adversarial Networks","J. Li; H. Huo; C. Li; R. Wang; Q. Feng","Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Remote Sensing Center of Public Security, People's Public Security University of China, Beijing, China","IEEE Transactions on Multimedia","26 Apr 2021","2021","23","","1383","1396","Infrared and visible image fusion aims to describe the same scene from different aspects by combining complementary information of multi-modality images. The existing Generative adversarial networks (GAN) based infrared and visible image fusion methods cannot perceive the most discriminative regions, and hence fail to highlight the typical parts existing in infrared and visible images. To this end, we integrate multi-scale attention mechanism into both generator and discriminator of GAN to fuse infrared and visible images (AttentionFGAN). The multi-scale attention mechanism aims to not only capture comprehensive spatial information to help generator focus on the foreground target information of infrared image and background detail information of visible image, but also constrain the discriminators focus more on the attention regions rather than the whole input image. The generator of AttentionFGAN consists of two multi-scale attention networks and an image fusion network. Two multi-scale attention networks capture the attention maps of infrared and visible images respectively, so that the fusion network can reconstruct the fused image by paying more attention to the typical regions of source images. Besides, two discriminators are adopted to force the fused result keep more intensity and texture information from infrared and visible image respectively. Moreover, to keep more information of attention region from source images, an attention loss function is designed. Finally, the ablation experiments illustrate the effectiveness of the key parts of our method, and extensive qualitative and quantitative experiments on three public datasets demonstrate the advantages and effectiveness of AttentionFGAN compared with the other state-of-the-art methods.","1941-0077","","10.1109/TMM.2020.2997127","Key Program of High-Resolution Earth Observation System(grant numbers:GFZX0404130307); National Natural Science Foundation of China(grant numbers:41901350); Fundamental Research Funds of People's Public Security University of China(grant numbers:2019JKF330); Fundamental Research Funds for the Central Universities(grant numbers:JZ2019HGBZ0151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103116","Attention mechanism;generative adversarial networks;infrared and visible image fusion","Image fusion;Generative adversarial networks;Feature extraction;Transforms;Generators;Gallium nitride;Fuses","image fusion;image texture;infrared imaging;neural nets;object detection","AttentionFGAN;multiscale attention networks;attention region;infrared image fusion;multiscale attention mechanism;visible image fusion;generative adversarial networks;multimodality images","","61","","47","IEEE","28 May 2020","","","IEEE","IEEE Journals"
"EUNet-CD: Efficient UNet++ for Change Detection of Very High-Resolution Remote Sensing Images","A. Raza; H. Huo; T. Fang","Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai, China; Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai, China; Key Laboratory of System Control and Information Processing, Ministry of Education of China, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","15 Feb 2022","2022","19","","1","5","Change detection (CD) is an essential remote sensing application for Earth observations widely used for several monitoring, management, and surveillance-related purposes. Generally, pixel-to-pixel prediction roles are susceptible to position details, as high-resolution bitemporal images contain abundant ground details, and so it needs more precautions to extract features. Recently, various efforts have been made for deep semantic change features, but the importance of grained features has always been ignored, leading to uncertainty of change ground entities. Besides, for an idealistic model, both accuracy and speed need focus. Thus, a novel method EUNet-CD is proposed, based on an efficient nested connection for a robust CD as a remedy for the above issues. The proposed scheme comprises: 1) an efficient convolution module (ECM)-based encoder/decoder to concise the ground entities’ details to minimize the parameters and 2) ultimate fusion strategies (based on spatial/channel attention) act as a repeater to refine the multiscale features and avoid subsampling loss. Experimental validation on different criteria shows the superiority of the proposed EUNet-CD among the state-of-the-arts (SOTA) methods. Our EUNet-CD remarkably improves the overall accuracy and minimizes the computational parameters and time floating point of operations per second (FLOPs) by 33% and 20%.","1558-0571","","10.1109/LGRS.2022.3144304","National Key Research and Development Program of China(grant numbers:2018YFB0505000); National Science and Technology Major Project(grant numbers:21-Y20A06-9001-17/18); Science Fund for Creative Research Groups of the National Natural Science Foundation of China(grant numbers:61221003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684472","Change detection (CD);deep learning;efficient UNet++;feature fusion;remote sensing (RS) images","Decoding;Convolution;Feature extraction;Semantics;Location awareness;Remote sensing;Deep learning","convolutional neural nets;feature extraction;geophysical image processing;geophysics computing;image fusion;image resolution;remote sensing","bitemporal image resolution;deep semantic change features;EUNet-CD;multiscale features;change detection;very high-resolution remote sensing images;essential remote sensing application;Earth observations;pixel-to-pixel prediction roles;UNet++;feature extraction;efficient convolution module;encoder/decoder","","6","","19","IEEE","18 Jan 2022","","","IEEE","IEEE Journals"
"Hierarchical Feature Aggregation and Self-Learning Network for Remote Sensing Image Continuous-Scale Super-Resolution","N. Ni; H. Wu; L. Zhang","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","Conducting research on remote sensing image (RSI) super-resolution (SR) is important, especially in terms of the continuous scale, which is beneficial to the application of RSI, such as RSI object detection and data fusion. Continuous-scale SR aims to use a single model to achieve SR at arbitrary (integer and noninteger) scale factors. Therefore, in this letter, we propose a hierarchical feature aggregation and self-learning network for RSI continuous-scale SR (RSI-HFAS). Our network can magnify the RSI continuously, which is beneficial for extracting the RSI multiscale features. First, we design a hierarchical feature aggregation module (HFAM) that is used for hierarchical feature extraction by placing convolutional layers on different floors and completing global feature fusion, which is crucial for achieving RSI continuous-scale SR with a single model. Second, the proposed network introduces a feedback mechanism, which can refine the hierarchical feature through feature feedback and enrich the texture parts of the RSI step by step. Finally, we design a self-learning upscaling structure to dynamically predict the number and weights of the upsampling filters, which can achieve RSI continuous-scale SR. Compared to the meta-learning based on enhanced deep SR (META-EDSR) method, our experimental results show a nearly 0.2-dB improvement on the metrics of the peak signal-to-noise ratio (PSNR).","1558-0571","","10.1109/LGRS.2021.3122985","National Natural Science Foundation of China(grant numbers:61571050,41771407); Beijing Natural Science Foundation(grant numbers:L182029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585484","Continuous-scale;feedback mechanism;hierarchical feature;remote sensing image (RSI);super-resolution (SR)","Feature extraction;Image reconstruction;Convolution;Task analysis;Superresolution;Remote sensing;Training","feature extraction;geophysical image processing;image fusion;image resolution;image sampling;image sequences;image texture;learning (artificial intelligence);object detection;remote sensing;sensor fusion","RSI continuous-scale SR;self-learning network;remote sensing image continuous-scale super-resolution;remote sensing image super-resolution;continuous scale;data fusion;RSI-HFAS;RSI continuously;RSI multiscale features;hierarchical feature aggregation module;hierarchical feature extraction;different floors;completing global feature fusion;RSI step","","2","","15","IEEE","26 Oct 2021","","","IEEE","IEEE Journals"
"Multitask Semantic Boundary Awareness Network for Remote Sensing Image Segmentation","A. Li; L. Jiao; H. Zhu; L. Li; F. Liu","Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","14","In remote sensing images, boundary information plays a crucial role in land-cover segmentation. However, it is a challenging problem that sufficiently extracts complete and sharp boundaries from complex very-high-resolution (VHR) remote sensing images. To tackle this problem, we propose a semantic boundary awareness network (SBANet). The SBANet captures refined boundary information of land covers in feature extraction and then supervises its learning with a designed boundary loss. The key of SBANet includes boundary attention module (BA-module) and adaptive weights of multitask learning (AWML). The BA-module is proposed to capture land-cover boundary information from hierarchical features aggregation in a bottom-up manner. It emphasizes useful boundary information and relieves noise information in low-level features with the guidance of high-level features. To directly learn the boundary information, AWML adds a boundary loss to the original semantic loss by an adaptive fusion manner. This multitask learning enables the semantic information and the boundary information to work collaboratively and promote each other. Note that the BA-module and AWML are plug-and-play. Experimental results demonstrate the effectiveness of the proposed SBANet on the available ISPRS 2-D semantic labeling Potsdam and Vaihingen data sets. The SBANet also achieves the state-of-the-art performance in terms of overall accuracy (OA) and mean  $F_{1}$  score (m- $F_{1}$ ).","1558-0644","","10.1109/TGRS.2021.3050885","State Key Program of National Natural Science of China(grant numbers:61836009); National Natural Science Foundation of China(grant numbers:U1701267); National Natural Science Foundation of China(grant numbers:61871310); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345482","Boundary attention;multilevel aggregation;multitask learning;remote sensing;semantic segmentation","Semantics;Feature extraction;Image segmentation;Remote sensing;Task analysis;Convolution;Spatial resolution","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;land cover;remote sensing;supervised learning","BA-module;land-cover boundary information;high-level features;original semantic loss;land-cover segmentation;feature extraction;boundary loss;multitask semantic boundary awareness network;SBANet;supervised learning;boundary attention module;adaptive weights of multitask learning;AWML;hierarchical features aggregation;low-level features;ISPRS 2-D semantic labeling Potsdam dataset;adaptive fusion;Vaihingen data set;complex very-high-resolution remote sensing image segmentation","","17","","68","IEEE","2 Feb 2021","","","IEEE","IEEE Journals"
"Convolutional Neural Networks for Multimodal Remote Sensing Data Classification","X. Wu; D. Hong; J. Chanussot","School of Information and Electronics and the Beijing Key Laboratory of Fractional Signals and Systems, Beijing Institute of Technology, Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","14 Feb 2022","2022","60","","1","10","In recent years, enormous research has been made to improve the classification performance of single-modal remote sensing (RS) data. However, with the ever-growing availability of RS data acquired from satellite or airborne platforms, simultaneous processing and analysis of multimodal RS data pose a new challenge to researchers in the RS community. To this end, we propose a deep-learning-based new framework for multimodal RS data classification, where convolutional neural networks (CNNs) are taken as a backbone with an advanced cross-channel reconstruction module, called CCR-Net. As the name suggests, CCR-Net learns more compact fusion representations of different RS data sources by the means of the reconstruction strategy across modalities that can mutually exchange information in a more effective way. Extensive experiments conducted on two multimodal RS datasets, including hyperspectral (HS) and light detection and ranging (LiDAR) data, i.e., the Houston2013 dataset, and HS and synthetic aperture radar (SAR) data, i.e., the Berlin dataset, demonstrate the effectiveness and superiority of the proposed CCR-Net in comparison with several state-of-the-art multimodal RS data classification methods. The codes will be openly and freely available at https://github.com/danfenghong/IEEE_TGRS_CCR-Net for the sake of reproducibility.","1558-0644","","10.1109/TGRS.2021.3124913","National Natural Science Foundation of China(grant numbers:62101045); China Postdoctoral Science Foundation(grant numbers:2021M690385); MIAI@Grenoble Alpes(grant numbers:ANR-19-P3IA-0003); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9598903","Classification;convolutional neural networks (CNNs);cross-channel;hyperspectral (HS);light detection and ranging (LiDAR);multimodal;reconstruction;remote sensing (RS);synthetic aperture radar (SAR)","Feature extraction;Laser radar;Synthetic aperture radar;Task analysis;Convolutional neural networks;Hyperspectral imaging;Network architecture","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image classification;image fusion;image reconstruction;image representation;optical radar;radar imaging;remote sensing;synthetic aperture radar","convolutional neural networks;airborne platforms;advanced cross-channel reconstruction module;CCR-Net learns more compact fusion representations;remote sensing data sources;multimodal remote sensing datasets;light detection and ranging data;synthetic aperture radar data;multimodal remote sensing data classification methods;single-modal remote sensing data;CNN;hyperspectral data;Houston2013 dataset;fusion representations;Berlin dataset","","39","","48","IEEE","2 Nov 2021","","","IEEE","IEEE Journals"
"GANMcC: A Generative Adversarial Network With Multiclassification Constraints for Infrared and Visible Image Fusion","J. Ma; H. Zhang; Z. Shao; P. Liang; H. Xu","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Instrumentation and Measurement","25 Dec 2020","2021","70","","1","14","Visible images contain rich texture information, whereas infrared images have significant contrast. It is advantageous to combine these two kinds of information into a single image so that it not only has good contrast but also contains rich texture details. In general, previous fusion methods cannot achieve this goal well, where the fused results are inclined to either a visible or an infrared image. To address this challenge, a new fusion framework called generative adversarial network with multiclassification constraints (GANMcC) is proposed, which transforms image fusion into a multidistribution simultaneous estimation problem to fuse infrared and visible images in a more reasonable way. We adopt a generative adversarial network with multiclassification to estimate the distributions of visible light and infrared domains at the same time, in which the game of multiclassification discrimination will make the fused result to have these two distributions in a more balanced manner, so as to have significant contrast and rich texture details. In addition, we design a specific content loss to constrain the generator, which introduces the idea of main and auxiliary into the extraction of gradient and intensity information, which will enable the generator to extract more sufficient information from source images in a complementary manner. Extensive experiments demonstrate the advantages of our GANMcC over the state-of-the-art methods in terms of both qualitative effect and quantitative metric. Moreover, our method can achieve good fused results even the visible image is overexposed. Our code is publicly available at https://github.com/jiayi-ma/GANMcC.","1557-9662","","10.1109/TIM.2020.3038013","Natural Science Fund of Hubei Province(grant numbers:2019CFA037); National Natural Science Foundation of China(grant numbers:61773295,41890820); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274337","Deep learning;generative adversarial network (GAN);image fusion;infrared;multiclassification","Image fusion;Generative adversarial networks;Generators;Gallium nitride;Task analysis;Neural networks;Data mining","game theory;image classification;image fusion;image texture;infrared imaging;neural nets;pose estimation;transforms;unsupervised learning","GANMcC;generative adversarial network;multiclassification constraints;visible image fusion;infrared image fusion;visible light distribution;infrared domains;gradient extraction;image fusion transform","","37","","54","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"A Model of Parallel Mosaicking for Massive Remote Sensing Images Based on Spark","W. Jing; S. Huo; Q. Miao; X. Chen","Northeast Forestry University, Harbin, China; Northeast Forestry University, Harbin, China; Northeast Forestry University, Harbin, China; North China University of Science and Technology, Tangshan, China","IEEE Access","26 Sep 2017","2017","5","","18229","18237","Image mosaicking is an important part of remote sensing image processing and plays a vital role in the analysis of trans-regional remote sensing images. In order to solve the problems of low utilization of nodes and frequent data I/O in traditional parallel mosaicking algorithms, we propose a parallel mosaicking algorithm based on Apache Spark. First, multi-nodes parallel computation for image overlapping region estimation are implemented in the algorithm. Then, we self-define the Resilient Distributed Data sets (RDD) for remote sensing image processing, and use the three key steps of the image mosaicking, including overlapping region estimation, image registration, and image fusion, which are as the transformation-type operators of the self-defined RDD (the self-defined RDD is what we get by extending the functionality of RDD in Spark). Finally, the parallel processing of image mosaicking is realized by calling the operators of self-defined RDD with the method of implicit conversion. Experimental results show that the parallel mosaicking algorithm of massive remote sensing image based on Spark can effectively improve the mass data image mosaicking efficiency on the basis of guaranteeing the image mosaicking effect.","2169-3536","","10.1109/ACCESS.2017.2746098","National Natural Science Foundation of China(grant numbers:31770768); Natural Science Foundation of Heilongjiang Province of China(grant numbers:ZD201403); Special Foundation for Science; Fundamental Research Funds for the Central Universities(grant numbers:2572017CB32); Open Project Program of the Hebei Key Laboratory of Data Science and Application(grant numbers:20170820001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8025338","Remote sensing image;parallel mosaicking;spark;self-defined RDD","Remote sensing;Sparks;Estimation;Parallel processing;Clustering algorithms;Algorithm design and analysis;Process control","geophysical image processing;image fusion;image registration;image segmentation;remote sensing","image registration;image fusion;parallel mosaicking algorithm;remote sensing image processing;trans-regional remote sensing images;image mosaicking;Apache Spark;image overlapping region estimation;resilient distributed data sets","","16","","26","OAPA","4 Sep 2017","","","IEEE","IEEE Journals"
"A ViT-Based Multiscale Feature Fusion Approach for Remote Sensing Image Segmentation","W. Wang; C. Tang; X. Wang; B. Zheng","College of Computer and Communication, Changsha University of Science and Technology, Changsha, China; College of Computer and Communication, Changsha University of Science and Technology, Changsha, China; College of Computer and Communication, Changsha University of Science and Technology, Changsha, China; College of Computer and Communication, Changsha University of Science and Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","8 Jul 2022","2022","19","","1","5","Semantic segmentation plays an indispensable role in automatic analysis of remote sensing image data. However, the abundant semantic information and irregular shape patterns in remote sensing images are difficult to utilize, making it hard to segment remote sensing images only using convolution and single-scale feature maps. To achieve better segmentation performance, a multiscale feature pyramid decoder (MFPD) is proposed to fuse image features extracted by vision transformer (ViT). The decoder employs a novel 2-D-to-3-D transform method to obtain multiscale feature maps that contain rich context information and fuses the multiscale feature maps by channel concatenation. Furthermore, a dimension attention module (DAM) is designed to further aggregate the context information of the extracted remote sensing image features. This approach yields superior mean intersection over union (mIoU) on the Gaofen2-CZ dataset (60.42%) and GID-5 dataset (68.21%). Experimental results indicate that the comprehensive performance of our approach exceeds the compared segmentation methods based on convolutional neural network (CNN) and ViT.","1558-0571","","10.1109/LGRS.2022.3187135","Natural Science Foundation of Hunan Province(grant numbers:2019JJ80105); Changsha City Science and Technology Plan Project(grant numbers:kq2004071); Scientific Research Project of the Hunan Provincial Department of Education(grant numbers:20C1249); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810316","Dimension attention module (DAM);remote sensing image;semantic segmentation;vision transformer (ViT)","Feature extraction;Image segmentation;Transformers;Decoding;Three-dimensional displays;Dams;Transforms","convolutional neural nets;feature extraction;geophysical image processing;image coding;image fusion;image recognition;image segmentation;remote sensing;shape recognition","vision transformer;2-D-to-3-D transform method;mean intersection over union;remote sensing image feature extraction;rich context information;multiscale feature maps;multiscale feature pyramid decoder;segmentation performance;single scale feature maps;segment remote sensing images;irregular shape patterns;semantic information;remote sensing image data;automatic analysis;semantic segmentation;remote sensing image segmentation;ViT-based multiscale feature fusion approach","","2","","17","IEEE","29 Jun 2022","","","IEEE","IEEE Journals"
"Pansharpening of Clustered MS and Pan Images Considering Mixed Pixels","H. R. Shahdoosti; N. Javaheri","Department of Electrical Engineering, Hamedan University of Technology, Hamedan, Iran; Department of Electrical Engineering, Hamedan University of Technology, Hamedan, Iran","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","6","826","830","The component substitution (CS) scheme is one of the most efficient models used by different image fusion algorithms when merging multispectral and panchromatic images, acquired with different spatial and spectral resolutions. In this letter, a new CS-based image fusion method is proposed to reduce color distortion. First, multispectral images are clustered into several classes using spectral and spatial features, and then linear regression with non-negative coefficients is used to calculate summation weights for each class of pixels. To consider mixed pixels not belonging to any distinct class, the proposed method employs the fuzzy c-means algorithm. Qualitative and quantitative results are reported for two data sets, namely, Landsat-7 Enhanded Thematic Mapper Plus and QuickBird. Visual and statistical assessments show the validity of the proposed method.","1558-0571","","10.1109/LGRS.2017.2682122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890424","Clustering;image fusion;mixed pixels;pansharpening;spatial features","Feature extraction;Linear regression;Spatial resolution;Distortion;Clustering algorithms;Remote sensing","remote sensing by radar","clustered MS;component substitution scheme;panchromatic images;multispectral images;CS-based image fusion method;c-means algorithm;Landsat-7 Enhanded Thematic Mapper Plus","","29","","24","IEEE","30 Mar 2017","","","IEEE","IEEE Journals"
"The Outcome of the 2021 IEEE GRSS Data Fusion Contest—Track MSD: Multitemporal Semantic Change Detection","Z. Li; F. Lu; H. Zhang; L. Tu; J. Li; X. Huang; C. Robinson; N. Malkin; N. Jojic; P. Ghamisi; R. Hänsch; N. Yokoya","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Microsoft AI for Good Research Lab, Redmond, USA; Mila - Université de Montréal, Montreal, QC, Canada; Microsoft Research, Redmond, WA, USA; Institute of Advanced Research in Artificial Intelligence (IARAI), Landstraβer Hauptstraβe, Vienna, Austria; German Aerospace Center (DLR), Weßling, Germany; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14 Feb 2022","2022","15","","1643","1655","We present here the scientific outcomes of the 2021 Data Fusion Contest (DFC2021) organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. DFC2021 was dedicated to research on geospatial artificial intelligence (AI) for social good with a global objective of modeling the state and changes of artificial and natural environments from multimodal and multitemporal remotely sensed data toward sustainable developments. DFC2021 included two challenge tracks: “Detection of settlements without electricity” and “Multitemporal semantic change detection.” This article mainly focuses on the outcome of the multitemporal semantic change detection track. We describe in this article the DFC2021 dataset that remains available for further evaluation of corresponding approaches and report the results of the best-performing methods during the contest.","2151-1535","","10.1109/JSTARS.2022.3144318","National Natural Science Foundation of China(grant numbers:41971295); CAS Interdisciplinary Innovation Team(grant numbers:JCTD-2019-04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690575","Convolutional neural networks;deep learning;image analysis and data fusion;land cover change detection;multimodal;random forests;weak supervision","Remote sensing;Earth;Data integration;Vegetation mapping;Predictive models;Vegetation;Artificial satellites","artificial intelligence;geophysical image processing;image fusion;object detection;remote sensing;sustainable development","multitemporal remotely sensed data;multitemporal semantic change detection;DFC2021 dataset;2021 IEEE GRSS Data Fusion Contest-track MSD;Image Analysis and Data Fusion Technical Committee;geospatial artificial intelligence;IEEE Geoscience and Remote Sensing Society;multimodal remotely sensed data;sustainable developments","","6","","41","CCBY","25 Jan 2022","","","IEEE","IEEE Journals"
"An Adaptive Two-Scale Image Fusion of Visible and Infrared Images","X. Han; T. Lv; X. Song; T. Nie; H. Liang; B. He; A. Kuijper","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun, China; Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, Germany","IEEE Access","7 May 2019","2019","7","","56341","56352","In this paper, we proposed an adaptive two-scale image fusion method using latent low-rank representation (LatLRR). First, both IR and VI images are decomposed into a two-scale representation using LatLRR to generate low-rank parts (the global structure) and saliency parts (the local structure). The algorithm denoises at the same time. Then, the guided filter is used in the saliency parts to make full use of the spatial consistency, which reduces artifacts effectively. With respect to the fusion rule of the low-rank parts, we construct adaptive weights by adopting fusion global-local-topology particle swarm optimization (FGLT-PSO) to obtain more useful information from the source images. Finally, the resulting image is reconstructed by adding the fused low-rank part and the fused saliency part. The experimental results validate that the proposed method outperforms several representative image fusion algorithms on publicly available datasets for infrared and visible image fusion in terms of subjective visual effect and objective assessment.","2169-3536","","10.1109/ACCESS.2019.2913289","Science and Technology Development Program of Jilin(grant numbers:20170204029GX); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698903","Fusion global-local-topology particle swarm optimization (FGLT-PSO);guided filter;image fusion;latent low-rank representation (LatLRR)","Image fusion;Transforms;Information filters;Filtering algorithms;Particle swarm optimization;Image reconstruction","feature extraction;image denoising;image fusion;image reconstruction;image representation;infrared imaging;particle swarm optimisation;topology","low-rank representation;LatLRR;two-scale representation;low-rank part;saliency parts;fusion rule;adaptive weights;fusion global-local-topology particle swarm optimization;source images;fused saliency part;representative image fusion algorithms;infrared image fusion;visible image fusion;adaptive two-scale image fusion;global structure","","13","","32","OAPA","25 Apr 2019","","","IEEE","IEEE Journals"
"Spatiotemporal Remote Sensing Image Fusion Using Multiscale Two-Stream Convolutional Neural Networks","Y. Chen; K. Shi; Y. Ge; Y. Zhou","College of Hydrology and Water Resources, Hohai University, Nanjing, China; College of Hydrology and Water Resources, Hohai University, Nanjing, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographical Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China; College of Hydrology and Water Resources, Hohai University, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","15 Dec 2021","2022","60","","1","12","Spatiotemporal remote sensing image fusion (STF) is a promising way to obtain remote sensing data with both fine spatial and temporal resolutions. Gradual and abrupt changes in land surface reflectance images are the main challenges in existing STF methods. Advanced deep learning techniques present powerful ability in learning image-changed information. Therefore, this article proposes a novel spatiotemporal image fusion method using multiscale two-stream convolutional neural networks (STFMCNNs). Multiscale two-stream convolutional neural networks are proposed to capture different sizes of objects in feature learning from a coarse spatial resolution image and two pairs of coarse and fine spatial resolution (FR) images at other dates. Meanwhile, temporal dependence and temporal consistency are explored as complementary information for STFMCNN. Moreover, a local fusion method is developed to characterize local variation by combining two predicted images derived from each stream. Two experiments on different real images are conducted to demonstrate the effectiveness of STFMCNN. Results show that STFMCNN outperformed three existing methods by predicting more accurate FR images with more preserved changed information.","1558-0644","","10.1109/TGRS.2021.3069116","National Key Research and Development Program of China(grant numbers:2017YFB0503501); National Natural Science Foundation of China(grant numbers:42071315); Fundamental Research Funds for the Central Universities(grant numbers:B210202008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399175","Multiscale feature;spatiotemporal image fusion;two-stream convolutional neural networks","Spatial resolution;Image resolution;Feature extraction;Convolutional neural networks;Remote sensing;Spatiotemporal phenomena;Sensors","","","","13","","41","IEEE","8 Apr 2021","","","IEEE","IEEE Journals"
"Multispectral Image Fusion Using Fractional-Order Differential and Guided Filtering","J. Li; G. Yuan; H. Fan","School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China; School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China; School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China","IEEE Photonics Journal","25 Oct 2019","2019","11","6","1","18","Remote sensing satellites can provide a large number of multispectral images. However, due to the limitations of optical sensors embedded in satellites, the spatial resolution of multispectral images is relatively low. Pansharpening aims to combine high-resolution panchromatic and multi-spectral images to generate high-resolution multi-spectral images. In this paper, we propose a pansharpening method based on a component substitution framework. We use fractional-order differential operators and guided filter to balance the spectral distortion and spatial information loss that occur when remote sensing image fusion. Fractional-order differentiation can better define the detailed map, and the guided filter can enhance the spectral information of the detailed map. Experiments show that the proposed method in this paper can better combine the spectral information and spatial information, as well as obtain satisfactory results in both subjective visual perception and objective object evaluation.","1943-0655","","10.1109/JPHOT.2019.2943489","National Science Foundation(grant numbers:EEC-0310717); National Natural Science Foundation of China(grant numbers:61772319,61976125,61976124,61602277,61773244); Shandong Natural Science Foundation of China(grant numbers:ZR2017MF049); Key Research and Development Program of Yantai City(grant numbers:2017ZH065); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8848440","Pansharpening;component substitution framework;fractional order differential operators;guided filter","Spatial resolution;Remote sensing;Distortion;Ultraviolet sources;Imaging;Satellites","geophysical image processing;image filtering;image fusion;image resolution;remote sensing","guided filter;spectral information;multispectral image fusion;remote sensing satellites;optical sensors;spatial resolution;high-resolution multispectral images;pansharpening method;component substitution framework;fractional-order differential operators;spectral distortion;spatial information loss;remote sensing image fusion;fractional-order differentiation","","2","","50","CCBY","25 Sep 2019","","","IEEE","IEEE Journals"
"Transformer-Based Regression Network for Pansharpening Remote Sensing Images","X. Su; J. Li; Z. Hua","School of Information and Electronic Engineering, Shandong Technology and Business University, Yantai, China; School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Shandong Technology and Business University, Yantai, China","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2022","2022","60","","1","23","The pansharpening entails obtaining images with uniform spectral distribution and rich spatial details by fusing multispectral images and panchromatic images, which has become a major image fusion problem in the field of remote sensing. Convolutional neural networks are widely used in image processing. We propose a transformer-based regression network (DR-NET) architecture. The first stage was feature extraction, which entailed extracting spectral information and spatial details from multispectral images and panchromatic images. The second stage was feature fusion, which entailed integrating the extracted feature information. In the third stage, image reconstruction, images with uniform distribution of spectral information, and sufficient spatial details were obtained. The fourth stage entailed optimizing the network performance and calculating the loss of shallow feature image and the image result after downsampling during image reconstruction. The performance of the DR-NET was optimized by optimizing the sum of all the loss values, which could be considered double regression. Simulated and real data experiments were conducted on the GF-2, QuickBird, and WorldView2 datasets to compare the proposed method with classical pansharpening methods. The qualitative and quantitative analyses proved that the spectral distribution of the image pansharpened using our method was uniform, the spatial details were completely retained, and the evaluation indicators were also optimal, which fully demonstrated the superior performance of the DR-NET.","1558-0644","","10.1109/TGRS.2022.3152425","National Natural Science Foundation of China(grant numbers:61772319,62002200,61976125,12001327); Shandong Natural Science Foundation of China(grant numbers:ZR2020QF012,ZR2021MF068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9715116","Convolutional neural network;multispectral image;panchromatic image;pansharpening","Feature extraction;Pansharpening;Transformers;Remote sensing;Deep learning;Image reconstruction;Convolutional neural networks","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image reconstruction;image resolution;neural nets;regression analysis;remote sensing","pansharpening remote sensing images;uniform spectral distribution;rich spatial details;multispectral images;panchromatic images;image fusion problem;convolutional neural networks;image processing;transformer-based regression network architecture;DR-NET;feature extraction;spectral information;feature fusion;extracted feature information;image reconstruction;uniform distribution;sufficient spatial details;fourth stage;network performance;shallow feature image;image result;double regression;classical pansharpening methods","","4","","56","IEEE","16 Feb 2022","","","IEEE","IEEE Journals"
"Image Fusion of Ku-Band-Based SCATSAT-1 and MODIS Data for Cloud-Free Change Detection Over Western Himalayas","S. Singh; R. K. Tiwari; V. Sood; H. S. Gusain; S. Prashar","Civil Engineering Department, Indian Institute of Technology (IIT), Ropar, Punjab, India; Civil Engineering Department, Indian Institute of Technology (IIT), Ropar, Punjab, India; Research and Development Centre, Aiotronics Automation Pvt. Ltd., Palampur, Himachal Pradesh, India; Defence Geoinformatics Research Establishment, Defence Research and Development Organization (DRDO), Manali, Himachal Pradesh, India; Electronics and Communication Engineering Department, Chitkara University School of Engineering and Technology, Chitkara University, Solan, Himachal Pradesh, India","IEEE Transactions on Geoscience and Remote Sensing","14 Feb 2022","2022","60","","1","14","Image-based fusion is a state-of-the-art process to extract vital information by combining the two or more images acquired from different satellite sensors. Recently launched (September 26, 2016) Indian Space Research Organization’s (ISRO) Ku-band (13.5 GHz)-based Scatterometer Satellite (SCATSAT-1) as an active microwave sensor can offer the day–night, all-weather monitoring services, which are not possible with the optical-based visible and infrared remote sensing satellites. Therefore, the fusion of optical and microwave data offers the cloud-free detection of Earth surface transitions and helps in emergency response to natural hazards, security, and defense. The objectives of the proposed framework are: 1) nearest neighbor-based fusion (NNF) of ISRO’s SCATSAT-1 and National Aeronautics and Space Administration’s (NASA) Moderate Resolution Imaging Spectroradiometer (MODIS) optical data; 2) generation of thematic maps using artificial neural network (ANN)-based classification of the fused data; 3) detection of spatiotemporal variations via postclassification comparison (PCC)-based change detection; 4) cross-referencing with well-defined fusion methods, i.e., Gram–Schmidt (GS), Brovey transformation (BT), and Ehlers; and 5) impact analysis of clouds on the input dataset and fusion methods. This study has been conducted over the Western Himalayas to estimate the snow cover changes under cloudy conditions with two datasets, i.e., winter and monsoon. The experimental outcomes confirm the efficacy of the proposed framework in the effective removal of clouds, generation of classified maps, and change maps. The present study includes an exhaustive list of applicative situations for cloud-free monitoring using freely and daily-based SCATSAT-1 and MODIS datasets.","1558-0644","","10.1109/TGRS.2021.3123392","Science and Engineering Research Board (SERB), Department of Science and Technology (DST), Govt. of India under Teachers Associateship for Research Excellence (TARE)(grant numbers:TAR/2019/000354); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9592791","Artificial neural network (ANN);Moderate Resolution Imaging Spectroradiometer (MODIS);nearest neighbor-based fusion (NNF);postclassification comparison (PCC);Scatterometer Satellite (SCATSAT-1)","MODIS;Optical sensors;Clouds;Sensors;Optical scattering;Optical imaging;Cloud computing","geophysical image processing;image fusion;remote sensing;remote sensing by radar;sensor fusion;snow","fused data;postclassification comparison-based change detection;well-defined fusion methods;western himalayas;snow cover changes;change maps;cloud-free monitoring;MODIS datasets;image Fusion;ku-band-based SCATSAT-1;MODIS data;cloud-free change detection;image-based fusion;state-of-the-art process;vital information;different satellite sensors;13.5 GHz;active microwave sensor;day-night;all-weather monitoring services;optical-based visible sensing satellites;infrared remote sensing satellites;optical microwave data;cloud-free detection;Earth surface transitions;ISRO's SCATSAT-1;artificial neural network-based classification;frequency 13.5 GHz","","3","","48","IEEE","28 Oct 2021","","","IEEE","IEEE Journals"
"A CBAM Based Multiscale Transformer Fusion Approach for Remote Sensing Image Change Detection","W. Wang; X. Tan; P. Zhang; X. Wang","School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China; School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China; School of Electronics and Communication Engineering, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China; School of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Aug 2022","2022","15","","6817","6825","Change detection methods play an indispensable role in remote sensing. Some change detection methods have obtained a fairly good performance by introducing attention mechanism on the basis of the convolutional neural network (CNN), but identifying intricate changes remains difficult. In response to these problems, this article proposes a new model for detecting changes in remote sensing, namely, MTCNet, which combines the advantages of multiscale transformer with the convolutional block attention module (CBAM) to improve the detection quality of different remote sensing images. On the basis of traditional convolutions, the transformer module is introduced to extract bitemporal image features by modeling contextual information. Based on the transformer module, a multiscale module is designed to form a multiscale transformer, which can obtain features at different scales in bitemporal images, thereby identifying the changes we are interested in. Based on the multiscale transformer module, the CBAM is introduced. The CBAM is split into a spatial attention module and a channel attention module, which are applied to the front and back ends of the multiscale transformer, respectively. Spatial information and channel information of feature maps are modeled separately. In this article, the validity and efficiency of the method are verified by a large number of experiments on the LEVIR-CD dataset and the WHU-CD dataset.","2151-1535","","10.1109/JSTARS.2022.3198517","Key Research and Development Projects of Hunan Province(grant numbers:2020SK2134); Natural Science Foundation of Hunan Province(grant numbers:2022JJ30625); Science and Technology Plan Project of Changsha(grant numbers:kq2004071); Scientific Research Project of Hunan Provincial Department of Education(grant numbers:20C1249); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9855775","Change detection;convolutional block attention module (CBAM);multiscale;remote sensing;transformer","Transformers;Feature extraction;Remote sensing;Context modeling;Data mining;Semantics;Decoding","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;object detection;remote sensing;transforms","spatial attention module;channel attention module;channel information;CBAM;multiscale transformer fusion approach;remote sensing image change detection;change detection methods;attention mechanism;convolutional neural network;intricate changes;convolutional block attention module;detection quality;remote sensing images;traditional convolutions;bitemporal image features;multiscale module;bitemporal images;multiscale transformer module","","3","","41","CCBYNCND","15 Aug 2022","","","IEEE","IEEE Journals"
"MATR: Multimodal Medical Image Fusion via Multiscale Adaptive Transformer","W. Tang; F. He; Y. Liu; Y. Duan","School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Image Processing","2 Aug 2022","2022","31","","5134","5149","Owing to the limitations of imaging sensors, it is challenging to obtain a medical image that simultaneously contains functional metabolic information and structural tissue details. Multimodal medical image fusion, an effective way to merge the complementary information in different modalities, has become a significant technique to facilitate clinical diagnosis and surgical navigation. With powerful feature representation ability, deep learning (DL)-based methods have improved such fusion results but still have not achieved satisfactory performance. Specifically, existing DL-based methods generally depend on convolutional operations, which can well extract local patterns but have limited capability in preserving global context information. To compensate for this defect and achieve accurate fusion, we propose a novel unsupervised method to fuse multimodal medical images via a multiscale adaptive Transformer termed MATR. In the proposed method, instead of directly employing vanilla convolution, we introduce an adaptive convolution for adaptively modulating the convolutional kernel based on the global complementary context. To further model long-range dependencies, an adaptive Transformer is employed to enhance the global semantic extraction capability. Our network architecture is designed in a multiscale fashion so that useful multimodal information can be adequately acquired from the perspective of different scales. Moreover, an objective function composed of a structural loss and a region mutual information loss is devised to construct constraints for information preservation at both the structural-level and the feature-level. Extensive experiments on a mainstream database demonstrate that the proposed method outperforms other representative and state-of-the-art methods in terms of both visual quality and quantitative evaluation. We also extend the proposed method to address other biomedical image fusion issues, and the pleasing fusion results illustrate that MATR has good generalization capability. The code of the proposed method is available at https://github.com/tthinking/MATR.","1941-0042","","10.1109/TIP.2022.3193288","National Natural Science Foundation of China(grant numbers:62072348,62176081); Science and Technology Major Project of Hubei Province (Next-Generation Artificial Intelligence (AI) Technologies)(grant numbers:2019AEA170); National Key Research and Development Program of China(grant numbers:2019YFC1509604); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844446","Image fusion;biomedical image;transformer;adaptive convolution;deep learning","Transformers;Image fusion;Single photon emission computed tomography;Magnetic resonance imaging;Transforms;Medical diagnostic imaging;Task analysis","feature extraction;image fusion;image registration;learning (artificial intelligence);medical image processing;transforms","multimodal medical image fusion;multiscale adaptive Transformer;imaging sensors;functional metabolic information;structural tissue details;complementary information;powerful feature representation ability;deep learning-based methods;convolutional operations;global context information;accurate fusion;multimodal medical images;MATR;adaptive convolution;global complementary context;global semantic extraction capability;useful multimodal information;region mutual information loss;information preservation;biomedical image fusion issues;pleasing fusion results","Algorithms;Databases, Factual;Diagnostic Imaging;Image Processing, Computer-Assisted","14","","68","IEEE","28 Jul 2022","","","IEEE","IEEE Journals"
"Low-Resolution Fully Polarimetric SAR and High-Resolution Single-Polarization SAR Image Fusion Network","L. Lin; J. Li; H. Shen; L. Zhao; Q. Yuan; X. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","8 Feb 2022","2022","60","","1","17","The data fusion technology aims to aggregate the characteristics of different data and to obtain products with multiple data advantages. To solve the problem of reduced resolution of polarimetric synthetic aperture radar (PolSAR) images due to system limitations, we propose a fully PolSAR images and single-polarization synthetic aperture radar (SinSAR) images fusion network to generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the polarimetric information of the low-resolution PolSAR (LR-PolSAR) images and the spatial information of the high-resolution single-polarization SAR (HR-SinSAR) images, we propose a fusion framework for joint LR-PolSAR images and HR-SinSAR images and design a cross-attention mechanism to extract features from the joint input data. Besides, based on the physical imaging mechanism, we designed the PolSAR polarimetric loss functions for constrained network training. The experimental results confirm the superiority of the fusion network over traditional algorithms. The average peak signal-to-noise ratio (PSNR) is increased by more than 3.6 dB, and the average mean absolute error (MAE) is reduced to less than 0.07. Experiments on polarimetric decomposition and polarimetric signature show that it maintains polarimetric information well.","1558-0644","","10.1109/TGRS.2021.3121166","National Natural Science Foundation of China(grant numbers:61671334,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583928","Cross-attention mechanism (CroAM);fully polarimetric synthetic aperture radar (PolSAR);fusion;polarimetric loss;RCNN;single-polarization SAR (SinSAR)","Feature extraction;Superresolution;Data mining;Synthetic aperture radar;Spatial resolution;Radar polarimetry;Training","correlation methods;image fusion;radar imaging;radar polarimetry;remote sensing by radar;sensor fusion;synthetic aperture radar","low-resolution fully polarimetric sar;high-resolution single-polarization sar image fusion network;data fusion technology;multiple data advantages;reduced resolution;polarimetric synthetic aperture radar images;fully PolSAR images;single-polarization synthetic aperture radar images fusion network;high-resolution PolSAR;polarimetric information;low-resolution PolSAR;high-resolution single-polarization SAR images;fusion framework;joint LR-PolSAR images;HR-SinSAR images;joint input data;physical imaging mechanism;PolSAR polarimetric loss functions;constrained network training;polarimetric decomposition;polarimetric signature show","","2","","45","IEEE","21 Oct 2021","","","IEEE","IEEE Journals"
"Multiband Image Fusion Based on Spectral Unmixing","Q. Wei; J. Bioucas-Dias; N. Dobigeon; J. -Y. Tourneret; M. Chen; S. Godsill","Signal Processing and Communications Laboratory, University of Cambridge, Cambridge, U.K.; Instituto de Telecomunicacões, Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal; Institut de Recherche en Informatique de Toulouse (IRT), The National Institute of Electrical Engineering, Electronics, Computer Science, Fluid Mechanics & Telecommunications and Network (ENSEEIHT), University of Toulouse, Toulouse, France; Institut de Recherche en Informatique de Toulouse (IRT), The National Institute of Electrical Engineering, Electronics, Computer Science, Fluid Mechanics & Telecommunications and Network (ENSEEIHT), University of Toulouse, Toulouse, France; School of Computer Engineering, DSO National Laboratories, Singapore; Signal Processing and Communications Laboratory, University of Cambridge, Cambridge, U.K.","IEEE Transactions on Geoscience and Remote Sensing","30 Sep 2016","2016","54","12","7236","7249","This paper presents a multiband image fusion algorithm based on unsupervised spectral unmixing for combining a high-spatial-low-spectral-resolution image and a low-spatial-high-spectral-resolution image. The widely used linear observation model (with additive Gaussian noise) is combined with the linear spectral mixture model to form the likelihoods of the observations. The nonnegativity and sum-to-one constraints resulting from the intrinsic physical properties of the abundances are introduced as prior information to regularize this ill-posed problem. The joint fusion and unmixing problem is then formulated as maximizing the joint posterior distribution with respect to the endmember signatures and abundance maps. This optimization problem is attacked with an alternating optimization strategy. The two resulting subproblems are convex and are solved efficiently using the alternating direction method of multipliers. Experiments are conducted for both synthetic and semi-real data. Simulation results show that the proposed unmixing-based fusion scheme improves both the abundance and endmember estimation compared with the state-of-the-art joint fusion and unmixing algorithms.","1558-0644","","10.1109/TGRS.2016.2598784","HYPANEMA ANR Project(grant numbers:ANR-12-BS03-003); Portuguese Science and Technology Foundation(grant numbers:UID/EEA/50008/2013); Thematic Trimester on Image Processing of the CIMI Labex, Toulouse, France(grant numbers:ANR-11-LABX-0040-CIMI,ANR-11-IDEX-0002-02); ERA-NET MED MapInvPlnt(grant numbers:ANR-15-NMED-0002-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7567518","Alternating direction method of multipliers;Bayesian estimation;block circulant matrix;block coordinate descent (BCD);multiband image fusion;Sylvester equation","Covariance matrices;Image fusion;Optimization;Spatial resolution;Mixture models;Sensors","Gaussian noise;geophysical image processing;image fusion;image resolution","multiband image fusion algo- rithm;unsupervised spectral unmixing;high-spatial-low-spectral-resolution image;low-spatial- high-spectral-resolution image;linear observa- tion model;additive Gaussian noise);linear spectral mixture model;intrinsic physical properties;ill-posed problem;joint fusion;joint posterior distribution;endmember signatures;abundance maps;optimization problem;alternating optimization strategy;multipliers. alternating direction method;synthetic data;semi-real data;unmixing-based fusion scheme;endmember estimation;abundance estimation","","96","","60","IEEE","14 Sep 2016","","","IEEE","IEEE Journals"
"Nonlocal Low-Rank Abundance Prior for Compressive Spectral Image Fusion","T. Gelvez; H. Arguello","Department of Electrical Engineering, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","415","425","Compressive spectral imaging (SI) (CSI) acquires few random projections of an SI reducing acquisition, storage, and, in some cases, processing costs. Then, this acquisition framework has been widely used in various tasks, such as target detection, video processing, and fusion. Particularly, compressive spectral image fusion (CSIF) aims at obtaining a high spatial-spectral resolution SI from two sets of compressed measurements: one from a hyperspectral image with a high-spectral low-spatial resolution, and one from a multispectral image with high-spatial low-spectral resolution. Most of the literature approaches include prior information, such as global low rank, smoothness, and sparsity, to solve the resulting ill-posed CSIF inverse problem. More recently, the high self-similarities exhibited by SIs have been successfully used to improve the performance of CSI inverse problems, including a nonlocal low-rank (NLLR) prior. However, to the best of our knowledge, this NLLR prior has not been implemented in the solution of the CSIF inverse problem. Therefore, this article formulates an approach that jointly includes the global low rank, the smoothness, and the NLLR priors to solve the CSIF inverse problem. The global low-rank prior is introduced with the linear mixture model that describes the SI as a linear combination of a set of few end-members to specific abundances. In this article, either the end-members are accurately estimated from the compressed measurements or initialized from a fast reconstruction of the hyperspectral image. Also, it assumes that the abundances preserve the smoothness and NLLR priors of the SI so that the fused image is obtained from the end-members and abundances that result when minimizing a cost function including the sum of two data fidelity terms and two regularizations: the smoothness and the NLLR. Simulations over three data sets show that the proposed approach increases the CSIF performance compared with literature approaches.","1558-0644","","10.1109/TGRS.2020.2993541","Minciencias through the agreement 910-2019-Program ECOS Nord-Grant(grant numbers:8598); entitled “Fusión de imágenes multitemporales ópticas y de radar de apertura sintética y su aplicación en detección de anomalías en cítricos.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9099096","Compressive spectral image fusion (CSIF);compressive spectral imaging (SI) (CSI);linear mixture model (LMM);low-rank regularization;nonlocal self-similarities","Image coding;Inverse problems;Image fusion;Mathematical model;Silicon;Spatial resolution","geophysical image processing;image fusion;image reconstruction;image resolution;inverse problems;minimisation;object detection","nonlocal low-rank abundance;compressive spectral image fusion;high spatial-spectral resolution SI;hyperspectral image;low-spatial resolution;multispectral image;low-spectral resolution;CSI inverse problems;NLLR;linear mixture model","","9","","52","IEEE","22 May 2020","","","IEEE","IEEE Journals"
"Attention_FPNet: Two-Branch Remote Sensing Image Pansharpening Network Based on Attention Feature Fusion","X. Zhong; Y. Qian; H. Liu; L. Chen; Y. Wan; L. Gao; J. Qian; J. Liu","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; College of Software, Key Laboratory of Software Engineering, Xinjiang University, and also with the Key Laboratory of signal detection and processing in Xinjiang Uygur Autonomous Region, Urumqi, China; College of Information Science and Engineering, Key Laboratory of Software Engineering, Xinjiang University, and Key Laboratory of signal detection and processing in Xinjiang Uygur Autonomous Region, Urumqi, China; College of Software, Key Laboratory of Software Engineering, Xinjiang University, and also with the Key Laboratory of signal detection and processing in Xinjiang Uygur Autonomous Region, Urumqi, China; College of Software, Key Laboratory of Software Engineering, Xinjiang University, and also with the Key Laboratory of signal detection and processing in Xinjiang Uygur Autonomous Region, Urumqi, China; College of Software, Key Laboratory of Software Engineering, Xinjiang University, and also with the Key Laboratory of signal detection and processing in Xinjiang Uygur Autonomous Region, Urumqi, China; University of Chinese Academy of Sciences, Beijing, China; TripleSAI Technology, Shenzhen, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Dec 2021","2021","14","","11879","11891","Inspired by the impressive achievements of convolutional neural networks in various computer vision tasks and the effective role of attention mechanisms, this article proposes a two-branch fusion network based on attention feature fusion (AFF) called Attention_FPNet to solve the pansharpening problem. We reconstruct the spatial information of the image in the high-pass filter domain and fully consider the spatial information in the multispectral (MS) and panchromatic (PAN) images. At the same time, the input PAN image and the upsampled MS image are directly transmitted to the reconstructed image through a long skip connection. The spectral information of the PAN and MS images is considered to improve the spectral resolution of the fused image. It also supplements the loss of spatial information that may be caused by network deepening. Moreover, an AFF method is used to replace the existing simple channel concatenation method commonly used in pansharpening, which fully considers the relationship between different feature maps and improves the fusion quality. Through experiments on image datasets acquired by the Pleiades, SPOT-6 and Gaofen-2 satellites, the results show that this method can effectively fuse PAN and MS images and generate a fused image and outperforms existing methods.","2151-1535","","10.1109/JSTARS.2021.3126645","National Natural Science Foundation of China(grant numbers:61966035); International Cooperation Project of the Science and Technology Department of the Autonomous Region(grant numbers:2020E01023); National Natural Science Foundation of China(grant numbers:U1803261); Autonomous Region Graduate Innovation Project(grant numbers:XJ2021G062,XJ2021G080); Shenzhen International S&T Cooperation Project(grant numbers:GJHZ20190821155805960); Key S&T Special Project of the Autonomous Region(grant numbers:2020A03004-4); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609690","Attention feature fusion (AFF);convolutional neural network (CNN);image fusion;pansharpening;remote sensing","Pansharpening;Remote sensing;Spatial resolution;Feature extraction;Image resolution;Image reconstruction;Convolutional neural networks","computer vision;convolutional neural nets;image fusion;image reconstruction;image resolution;remote sensing","fused image;channel concatenation;feature maps;image datasets;Attention_FPNet;two-branch remote sensing image pansharpening network;attention feature fusion;convolutional neural networks;computer vision tasks;two-branch fusion network;input PAN image;upsampled MS image;reconstructed image;high-pass filter","","7","","57","CCBY","9 Nov 2021","","","IEEE","IEEE Journals"
"FRS-Net: An Efficient Ship Detection Network for Thin-Cloud and Fog-Covered High-Resolution Optical Satellite Imagery","Z. Zhang; H. Zheng; J. Cao; X. Feng; G. Xie","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Dec 2022","2022","15","","2326","2340","According to statistics, more than 50% of optical satellite images are covered by clouds or fog. Moreover, the cloud cover rate is much higher in large bodies of water and nearby areas than inland areas due to the high amount of water evaporation and condensation. Therefore, ship detection from optical remote sensing images based on water surface analysis is more susceptible to cloud and fog interference, which affects the detection accuracy. In the time-sensitive application field of remote sensing images, due to the massive parameters in the large-scale network model, the detection speed is slow, and a lightweight detection model is commonly used. However, it is difficult for the lightweight detection model to achieve both high efficiency and accuracy for ship detection in a cloud-covered environment. To solve these problems, this manuscript proposes a lightweight algorithm called the fog remote sensing ship detection network (FRS-Net) suitable for ship detection from remote sensing images in thin-cloud and fog-covered environments. FRS-Net is developed based on the deep learning algorithm and can effectively improve ship detection accuracy under thin-cloud and fog-covered environment. First, for the allocation strategy of anchor boxes, by using the K-means clustering algorithm, FRS-Net simplifies the number of anchor boxes by utilizing the shape characteristics of the ship, which improves the time efficiency and the detection accuracy. Second, the FRS-Net network can meet the detection accuracy requirement and has a fast inference speed. The FRS-Net network is mainly composed of a backbone extraction network, a feature fusion network, and a prediction network. Experimental results on the ship detection in optical remote sensing image dataset demonstrate the detection accuracy and computational efficiency of FRS-Net. The recognition mean average precision achieved 43.20% for ship detection under thin-cloud and fog-covered environment, with an efficiency of up to 424 frames per second. FRS-Net has the potential to be applied in future scenarios such as embedded processing and onboard processing, where computing capabilities are strictly limited and the timeliness requirement is high.","2151-1535","","10.1109/JSTARS.2022.3227322","National Natural Science Foundation of China(grant numbers:61901307); Open Research Fund of State Key Laboratory of Information Engineering in Surveying; Mapping and Remote Sensing; Wuhan University(grant numbers:20E01); Scientific Research Foundation for Doctoral Program of Hubei University of Technology(grant numbers:BSQD2020054,BSQD2020055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973332","Cloud cover;deep learning;high-resolution optical satellite imagery;object detection","Marine vehicles;Remote sensing;Feature extraction;Task analysis;Object detection;Satellites;Optical sensors","deep learning (artificial intelligence);geophysical image processing;image fusion;image resolution;image sensors;optical sensors;pattern clustering;remote sensing;ships","backbone extraction network;cloud cover rate;cloud-covered environment;efficient ship detection network;feature fusion network;fog interference;fog-covered environment;fog-covered high-resolution optical satellite imagery;FRS-Net network;K-means clustering algorithm;lightweight detection model;optical fog remote sensing imaging;optical remote sensing image dataset;optical satellite imaging;ship detection accuracy;water surface analysis","","","","61","CCBYNCND","7 Dec 2022","","","IEEE","IEEE Journals"
"A Twice Optimizing Net With Matrix Decomposition for Hyperspectral and Multispectral Image Fusion","D. Shen; J. Liu; Z. Xiao; J. Yang; L. Xiao","Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; School of Computer Science and Engineering, the Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","28 Jul 2020","2020","13","","4095","4110","Fusing a low-resolution hyperspectral (LRHS) image and a high-resolution multispectral (HRMS) image to generate a high-resolution hyperspectral (HRHS) image has grown a significant and attractive application in remote sensing fields. Recently, the popularization of deep learning has injected more possibilities into the fusion work. However, there still exists a difficulty that is how to make the best of the acquired LRHS and HRMS images. In this article, we present a twice optimizing net with matrix decomposition to fulfill the fusion task, which can be roughly divided into three stages: pre-optimization, deep prior learning, post-optimization. Specifically, we first transform this fusion problem into a spectral optimization problem and a spatial optimization problem with the help of matrix decomposition. These two optimization problems can be handled sequentially by solving a linear equation, respectively, and then we can obtain the initial HRHS image by multiplying the two solutions. Next, we establish the mapping between the initial image and the reference image through an end-to-end deep residual network based on local and nonlocal connectivity. In order to get better performance, we have customized a loss function specifically for the fusion task as well. Finally, we return the predicted result again to the optimization procedure to get the final fusion image. After the evaluation on three simulated datasets and one real dataset, it illustrates that the proposed method outperforms many state-of-the-art ones.","2151-1535","","10.1109/JSTARS.2020.3009250","National Natural Science Foundation of China(grant numbers:61601201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141409","Convolutional neural network (CNN);hyperspectral image;image fusion;loss function;super resolution","Feature extraction;Optimization;Hyperspectral imaging;Image resolution;Matrix decomposition;Image fusion","geophysical image processing;image fusion;image resolution;image sensors;learning (artificial intelligence);matrix decomposition;optimisation;remote sensing","high-resolution hyperspectral image;HRMS;high-resolution multispectral image;LRHS;low-resolution hyperspectral image;final fusion image;optimization procedure;end-to-end deep residual network;reference image;initial HRHS image;optimization problems;spatial optimization problem;spectral optimization problem;post-optimization;deep prior learning;pre-optimization;matrix decomposition;twice optimizing net;deep learning;remote sensing fields","","15","","57","CCBY","15 Jul 2020","","","IEEE","IEEE Journals"
"A Multimodal Feature Selection Method for Remote Sensing Data Analysis Based on Double Graph Laplacian Diagonalization","E. Khachatrian; S. Chlaily; T. Eltoft; A. Marinoni","Department of Physics and Technology, University of Tromsø—The Arctic University of Norway, N-9037 Tromsø, Norway; Department of Physics and Technology, University of Tromsø—The Arctic University of Norway, N-9037 Tromsø, Norway; Department of Physics and Technology, University of Tromsø—The Arctic University of Norway, N-9037 Tromsø, Norway; Department of Physics and Technology, University of Tromsø—The Arctic University of Norway, N-9037 Tromsø, Norway","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Nov 2021","2021","14","","11546","11566","When dealing with multivariate remotely sensed records collected by multiple sensors, an accurate selection of information at the data, feature, or decision level is instrumental in improving the scenes’ characterization. This will also enhance the system’s efficiency and provide more details on modeling the physical phenomena occurring on the Earth’s surface. In this article, we introduce a flexible and efficient method based on graph Laplacians for information selection at different levels of data fusion. The proposed approach combines data structure and information content to address the limitations of existing graph-Laplacian-based methods in dealing with heterogeneous datasets. Moreover, it adapts the selection to each homogenous area of the considered images according to their underlying properties. Experimental tests carried out on several multivariate remote sensing datasets show the consistency of the proposed approach.","2151-1535","","10.1109/JSTARS.2021.3124308","Centre for Integrated Remote Sensing and Forecasting for Arctic Operations; Norges Forskningsråd(grant numbers:237906); Automatic Multi-sensor Remote Sensing for Sea Ice Characterization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599440","Gaussian kernel (GK);graph Laplacians;multimodal remote sensing;mutual information (MI);unsupervised information selection","Laplace equations;Dimensionality reduction;Sensor phenomena and characterization;Feature extraction;Earth;Data analysis","data analysis;data structures;feature selection;geophysical image processing;graph theory;image fusion;remote sensing","Earth's surface;information selection;data fusion;data structure;information content;multivariate remote sensing datasets;multimodal feature selection;remote sensing data analysis;double graph Laplacian diagonalization","","1","","82","CCBYNCND","2 Nov 2021","","","IEEE","IEEE Journals"
"Improving Hypersharpening for WorldView-3 Data","M. Selva; L. Santurri; S. Baronti","National Research Council, Florence Research Area, Nello Carrara Institute of Applied Physics, Sesto Fiorentino, Italy; National Research Council, Florence Research Area, Nello Carrara Institute of Applied Physics, Sesto Fiorentino, Italy; National Research Council, Florence Research Area, Nello Carrara Institute of Applied Physics, Sesto Fiorentino, Italy","IEEE Geoscience and Remote Sensing Letters","21 May 2019","2019","16","6","987","991","In this letter, hypersharpening is analyzed in depth by investigating some weaknesses present in its formulation. It is shown that the key formula of the synthesized band variant can be simplified under certain circumstances. In addition, a novel fusion schema is proposed. As a result, the gain factor adopted to weight the injected detail is computed in a different way. This schema can be applied to fuse a wide range of hyperspectral and multispectral data. In this letter, its effectiveness is demonstrated by taking into account the characteristics of WorldView-3 data.","1558-0571","","10.1109/LGRS.2018.2884087","SMART (Spettrometro Miniaturizzato Avanzato per Ricerca Tecnologica); POR-FESR 2014-2020 of the Tuscan Region; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8584412","Hypersharpening;image fusion;pansharpening;remote sensing;WorldView-3","Spatial resolution;Satellites;Computational modeling;Hyperspectral imaging;Minimization","geophysical image processing;image fusion;remote sensing;spectral analysis","hypersharpening;WorldView-3 data;synthesized band variant;fusion schema;gain factor;hyperspectral data;multispectral data","","18","","23","IEEE","20 Dec 2018","","","IEEE","IEEE Journals"
"Adaptive Multiscale Deep Fusion Residual Network for Remote Sensing Image Classification","G. Li; L. Li; H. Zhu; X. Liu; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","30 Oct 2019","2019","57","11","8506","8521","With the development of remote sensing imaging technology, remote sensing images with high-resolution and complex structure can be acquired easily. The classification of remote sensing images is always a hot and challenging problem. In order to improve the performance of remote sensing image classification, we propose an adaptive multiscale deep fusion residual network (AMDF-ResNet). The AMDF-ResNet consists of a backbone network and a fusion network. The backbone network including several residual blocks generates multiscale hierarchy features, which contain semantic information from low to high levels. In the fusion network, the adaptive feature fusion module proposed can emphasize useful information and suppress useless information by learning the weights, which represent the importance of the features. The AMDF-ResNet can make full use of the multiscale hierarchy features and the extracted feature is discriminative. In addition, we propose a samples selection method named important samples selection strategy (ISSS). Based on superpixels segmentation result, gradient information and spatial distribution are used as two references to determine the selection numbers and select samples. Compared with the random selection strategy, training samples selected by ISSS are more representative and diverse. The experimental results on four data sets demonstrate that the AMDF-ResNet and ISSS are effective.","1558-0644","","10.1109/TGRS.2019.2921342","National Natural Science Foundation of China(grant numbers:61621005); National Natural Science Foundation of China(grant numbers:U1701267); National Natural Science Foundation of China(grant numbers:91438201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746806","Deep learning (DL);feature extraction;image classification;multispectral (MS) images;remote sensing","Feature extraction;Training;Semantics;Image segmentation;Adaptive systems;Hyperspectral sensors","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;remote sensing","backbone network;fusion network;multiscale hierarchy features;adaptive feature fusion module;AMDF-ResNet;adaptive multiscale deep fusion residual network;remote sensing image classification;remote sensing imaging technology;important samples selection strategy;superpixel segmentation","","44","","54","IEEE","26 Jun 2019","","","IEEE","IEEE Journals"
"ECFNet: A Siamese Network With Fewer FPs and Fewer FNs for Change Detection of Remote-Sensing Images","S. Zhu; Y. Song; Y. Zhang; Y. Zhang","School of Software Engineering, Xi’an Jiaotong University, Xi’an, China; School of Software Engineering, Xi’an Jiaotong University, Xi’an, China; School of Software Engineering, Xi’an Jiaotong University, Xi’an, China; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","2 Feb 2023","2023","20","","1","5","High-resolution remote-sensing image change detection plays an important role in areas such as land and resources investigation, natural disaster prediction, and military strategy research. Current change detection methods often focus on extracting more discriminative features, while ignoring the information loss and imbalance problems in the process of feature fusion, which results in weakness in small change objects and edge pixels of change objects. In this letter, a simple but efficient network architecture, extraction, comparison and fusion network (ECFNet), for change detection in remote-sensing images is proposed. By constraining the number of feature channels in the fusion process by the feature comparison module (FCM), ECFNet can better utilize the fine-grained information in the multiscale feature map for result prediction, which not only improves the detection performance of small objects, but also reduces the false detection around the edge pixels of change objects. Experiments on the deeply supervised image fusion network for change detection (DSIFN-CD) test set show that ECFNet achieves state-of-the-art results with a small amount of computation.","1558-0571","","10.1109/LGRS.2023.3238553","National Natural Science Foundation of China(grant numbers:61973245); Key Research and Development Plan of Shaanxi Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023508","Change detection;remote-sensing images;Siamese network;small object detection","Feature extraction;Finite element analysis;Convolution;Remote sensing;Semantics;Network architecture;Image edge detection","","","","","","15","IEEE","20 Jan 2023","","","IEEE","IEEE Journals"
"Rotation-Insensitive and Context-Augmented Object Detection in Remote Sensing Images","K. Li; G. Cheng; S. Bu; X. You","Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Zhengzhou Institute of Surveying and Mapping, Zhengzhou, China","IEEE Transactions on Geoscience and Remote Sensing","23 Mar 2018","2018","56","4","2337","2348","Most of the existing deep-learning-based methods are difficult to effectively deal with the challenges faced for geospatial object detection such as rotation variations and appearance ambiguity. To address these problems, this paper proposes a novel deep-learning-based object detection framework including region proposal network (RPN) and local-contextual feature fusion network designed for remote sensing images. Specifically, the RPN includes additional multiangle anchors besides the conventional multiscale and multiaspect-ratio ones, and thus can deal with the multiangle and multiscale characteristics of geospatial objects. To address the appearance ambiguity problem, we propose a double-channel feature fusion network that can learn local and contextual properties along two independent pathways. The two kinds of features are later combined in the final layers of processing in order to form a powerful joint representation. Comprehensive evaluations on a publicly available ten-class object detection data set demonstrate the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2017.2778300","National Natural Science Foundation of China(grant numbers:61573284,61772425); Project of Science and Technology Innovation of Henan Province(grant numbers:142101510005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240988","Convolutional neural networks (CNNs);object detection;remote sensing images;restricted Boltzmann machine (RBM)","Object detection;Proposals;Feature extraction;Remote sensing;Geospatial analysis;Context modeling;Satellites","geophysical image processing;image fusion;image representation;learning (artificial intelligence);object detection;remote sensing","local properties;contextual properties;ten-class object detection data;context-augmented object detection;remote sensing images;geospatial object detection;rotation variations;RPN;local-contextual feature fusion network;multiangle characteristics;multiscale characteristics;appearance ambiguity problem;double-channel feature fusion network;deep-learning-based methods;multiangle anchors;rotation-insensitive object detection;conventional multiscale multiaspect-ratio;joint representation;deep-learning-based object detection framework","","254","","57","IEEE","27 Dec 2017","","","IEEE","IEEE Journals"
"Fusing Heterogeneous Data: A Case for Remote Sensing and Social Media","H. Wang; E. Skau; H. Krim; G. Cervone","Department of Computer Science, Institute for Cyber Security, The University of Texas, San Antonio, TX, USA; Los Alamos National Laboratories, Los Alamos, NM, USA; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA; Geoinformatics Department, Pennsylvania State University, University Park, PA, USA","IEEE Transactions on Geoscience and Remote Sensing","22 Nov 2018","2018","56","12","6956","6968","Data heterogeneity can pose a great challenge to process and systematically fuse low-level data from different modalities with no recourse to heuristics and manual adjustments and refinements. In this paper, a new methodology is introduced for the fusion of measured data for detecting and predicting weather-driven natural hazards. The proposed research introduces a robust theoretical and algorithmic framework for the fusion of heterogeneous data in near real time. We establish a flexible information-based fusion framework with a target optimality criterion of choice, which for illustration, is specialized to a maximum entropy principle and a least effort principle for semisupervised learning with noisy labels. We develop a methodology to account for multimodality data and a solution for addressing inherent sensor limitations. In our case study of interest, namely, that of flood density estimation, we further show that by fusing remote sensing and social media data, we can develop well founded and actionable flood maps. This capability is valuable in situations where environmental hazards, such as hurricanes or severe weather, affect very large areas. Relative to the state of the art working with such data, our proposed information-theoretic solution is principled and systematic, while offering a joint exploitation of any set of heterogeneous sensor modalities with minimally assuming priors. This flexibility is coupled with the ability to quantitatively and clearly state the fusion principles with very reasonable computational costs. The proposed method is tested and substantiated with the multimodality data of a 2013 Boulder Colorado flood event.","1558-0644","","10.1109/TGRS.2018.2846199","National Nuclear Security Administration(grant numbers:DE-NA0002576); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412269","Least effort principle;maximum entropy models;optimal transport;remote sensing;social media;volunteering labels","Social network services;Labeling;Remote sensing;Noise measurement;Satellites;Entropy;Hazards","entropy;floods;geophysical image processing;hydrological techniques;image fusion;learning (artificial intelligence);remote sensing","low-level data;robust theoretical framework;algorithmic framework;heterogeneous data;flexible information-based fusion framework;maximum entropy principle;effort principle;multimodality data;flood density estimation;remote sensing;social media data;information-theoretic solution;heterogeneous sensor modalities;fusion principles;data heterogeneity;weather-driven natural hazards","","27","","34","IEEE","17 Jul 2018","","","IEEE","IEEE Journals"
"Bidirectional Guided Attention Network for 3-D Semantic Detection of Remote Sensing Images","Z. Rao; M. He; Z. Zhu; Y. Dai; R. He","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2021","2021","59","7","6138","6153","Semantic segmentation and disparity estimation are in the research frontier of the computer vision and remote sensing (RS) fields. However, existing methods mostly deal with these two problems separately or use a combination of multiple models to solve these two tasks. Due to a lack of sufficient information sharing and fusion, they still have difficulties in coping with seasonal appearance differences in 3-D RS problems. In this article, we propose a novel multitask learning architecture that considers the bottom–up and up–bottom visual attention mechanism for 3-D semantic detection, named bidirectional guided attention network (BGA-Net). BGA-Net consists of five modules: unified backbone module (UBM), bidirectional guided attention module (BGAM), semantic segmentation module (SSM), feature matching module (FMM), and bidirectional fusion module (BFM). First, in UBM, we use a shared backbone to extract unified features and share them with three branches/modules (BGAM, SSM, and FMM). Then, SSM and FMM branches are applied to estimate segmentation and disparity maps, whereas the third branch/module (BGAM) shares the global features to guide the task-specific learning via attention mechanism. Finally, we fuse the results of the two tasks by BFM to improve the final performance. Extensive experiments demonstrate that: 1) our BGA-Net can handle the two tasks simultaneously and can be trained in an end-to-end way; 2) these modules fully take advantage of the two tasks’ information to share features and enhance the scene understanding ability, effectively against seasons change of RS images; and 3) BGA-Net has notable superiority and greater flexibility and also sets a new state of the art on the urban semantic 3-D (US3D) benchmark. Moreover, BGA-Net also provides insights into the intelligent interpretation of RS data images.","1558-0644","","10.1109/TGRS.2020.3029527","National Natural Science Foundation of China(grant numbers:61671387,61420106007,61871325,62001396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9235481","Bidirectional guided aggregation;remote sensing image;semantic segmentation;stereo matching;visual attention mechanism","Task analysis;Semantics;Image segmentation;Feature extraction;Visualization;Remote sensing;Computer architecture","computer vision;feature extraction;geophysical image processing;image classification;image fusion;image matching;image recognition;image representation;image segmentation;learning (artificial intelligence);remote sensing","urban semantic 3-D benchmark;US3D;RS data images;3-D semantic detection;disparity estimation;remote sensing fields;sufficient information sharing;3-D RS problems;up-bottom visual attention mechanism;named bidirectional guided attention network;BGA-Net consists;unified backbone module;BGAM;SSM;feature matching module;bidirectional fusion module;shared backbone;FMM branches;segmentation;disparity maps;tasks;share features;RS images","","13","","52","IEEE","22 Oct 2020","","","IEEE","IEEE Journals"
"Edge-Aware Multiscale Feature Integration Network for Salient Object Detection in Optical Remote Sensing Images","X. Zhou; K. Shen; Z. Liu; C. Gong; J. Zhang; C. Yan","School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China; PCA Laboratory, Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","13 Jan 2022","2022","60","","1","15","The optical remote sensing images (RSIs) show various spatial resolutions and cluttered background, where salient objects with different scales, types, and orientations are presented in diverse RSI scenes. Therefore, it is inappropriate to directly extend cutting-edge saliency detection methods for conventional RGB images to optical RSIs. Besides, the existing saliency models targeting RSIs often render imperfect saliency maps, where some of them are with coarse boundary details. To solve this problem, this article attempts to introduce the edge information to precisely detect salient objects in RSIs. Accordingly, we propose an edge-aware multiscale feature integration network (EMFI-Net) for salient object detection by conducting multiscale feature integration under the explicit and implicit assistance of salient edge cues. Specifically, our network contains two parts including the encoder and decoder. First, the encoder extracts multiscale deep features from three RSIs with different resolutions, where the high-level deep semantic features from three RSIs are integrated using a cascaded feature fusion module. Second, the encoder explicitly enriches the multiscale deep features by integrating the salient edge cues extracted by a salient edge extraction module. Meanwhile, we also implicitly deploy an edge-aware constraint to the supervision of the saliency map prediction by introducing a hybrid loss function. Finally, the decoder integrates the enriched multiscale deep features in a coarse-to-fine way, yielding a high-quality saliency map. The experiments conducted on two public optical RSI datasets clearly prove the effectiveness and superiority of the proposed EMFI-Net against the state-of-the-art saliency models.","1558-0644","","10.1109/TGRS.2021.3091312","National Key Research and Development Program of China(grant numbers:2020YFB1406604); National Natural Science Foundation of China(grant numbers:61901145,61973162,61931008,61671196,62071415,62001146,61701149,61801157,61971268,61901150,61972123); Fundamental Research Funds for the Central Universities(grant numbers:30920032202,30921013114); “Young Elite Scientists Sponsorship Program” by CAST(grant numbers:2018QNRC001); Hong Kong Scholars Program(grant numbers:XJ2019036); Zhejiang Province Nature Science Foundation of China(grant numbers:LR17F030006); Higher Education Discipline Innovation Project(grant numbers:D17019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474908","Multiscale deep features;optical remote sensing images;salient edge cues;salient object detection","Feature extraction;Optical imaging;Image edge detection;Optical sensors;Object detection;Image resolution;Remote sensing","edge detection;feature extraction;geophysical image processing;image colour analysis;image fusion;image segmentation;object detection;remote sensing","encoder;salient edge cues;salient edge extraction module;edge-aware constraint;saliency map prediction;enriched multiscale deep features;high-quality saliency map;public optical RSI datasets;edge-aware multiscale feature integration network;salient object detection;optical remote sensing images;salient objects;cutting-edge saliency detection methods;conventional RGB images;optical RSIs;existing saliency models;imperfect saliency maps;edge information;high-level deep semantic features;cascaded feature fusion module","","11","","74","IEEE","5 Jul 2021","","","IEEE","IEEE Journals"
"Novel Cross-Resolution Feature-Level Fusion for Joint Classification of Multispectral and Panchromatic Remote Sensing Images","S. Liu; H. Zhao; Q. Du; L. Bruzzone; A. Samat; X. Tong","College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Xinjiang Institute of Ecology and Geography, Chinese Academy of Sciences, Ürümqi, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","29 Mar 2022","2022","60","","1","14","With the increasing availability and resolution of satellite sensor data, multispectral (MS) and panchromatic (PAN) images are the most popular data that are used in remote sensing among applications. This article proposes a novel cross-resolution hidden layer feature fusion (CRHFF) approach for joint classification of multiresolution MS and PAN images. In particular, shallow spectral and spatial features at a global scale are first extracted from an MS image. Then, deep cross-resolution hidden layer features extracted from MS and PAN are fused from patches at a local scale according to an autoencoder (AE)-like deep network. Finally, the selected multiresolution hidden layer features are classified in a supervised manner. By taking advantage of integrated shallow-to-deep and global-to-local features from the high-resolution MS and PAN images, the cross-resolution latent information can be extracted and fused in order to better model imaged objects from the multimodal representation and finally increase the classification accuracy. Experimental results obtained on three real multiresolution datasets covering complex urban scenarios confirm the effectiveness of the proposed approach in terms of higher accuracy and robustness with respect to literature methods.","1558-0644","","10.1109/TGRS.2021.3127710","National Key Research and Development Program of China(grant numbers:2018YFB0505000); Natural Science Foundation of China(grant numbers:42071324,42071424); Shanghai Rising-Star Program(grant numbers:21QA1409100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612215","Classification;feature-level fusion;multiresolution images;remote sensing;shallow and deep features","Feature extraction;Satellites;Spatial resolution;Remote sensing;Data mining;Task analysis;Convolutional neural networks","feature extraction;geophysical image processing;image classification;image fusion;image resolution;remote sensing;sensor fusion","joint classification;particular features;shallow spectral features;spatial features;global scale;MS image;deep cross-resolution hidden layer features;local scale;autoencoder-like deep network;selected multiresolution hidden layer features;integrated shallow-to-deep;global-to-local features;cross-resolution latent information;classification accuracy;novel cross-resolution feature-level fusion;multispectral;panchromatic remote sensing;satellite sensor data;PAN;popular data;novel cross-resolution hidden layer feature fusion approach","","6","","43","IEEE","11 Nov 2021","","","IEEE","IEEE Journals"
"Global Visual Feature and Linguistic State Guided Attention for Remote Sensing Image Captioning","Z. Zhang; W. Zhang; M. Yan; X. Gao; K. Fu; X. Sun","Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","25 Feb 2022","2022","60","","1","16","The encoder–decoder framework is prevalent in existing remote-sensing image captioning (RSIC) models. The appearance of attention mechanisms brings significant results. However, current attention-based caption models only build up the relationships between the local features without introducing the global visual feature and removing redundant feature components. It will cause caption models to generate descriptive sentences that are weakly related to the scene of images. To solve the problems, this article proposed a global visual feature-guided attention (GVFGA) mechanism. First, GVFGA introduces the global visual feature and fuses them with local visual features to build up their relationships between them. Second, an attention gate utilizing the global visual feature is proposed in GVFGA to filter out redundant feature components in the fused image features and provide more salient image features. In addition, to relieve the hidden state’s burden, a linguistic state (LS) is proposed to specifically provide textual features, making the hidden state only guiding visual–textual attention process. What’s more, to further refine the fusion of visual features and textual features, a LS-Guided Attention (LSGA) mechanism is proposed. It can also filter out the irrelevant information in the fused visual–textual feature with the help of an attention gate. The experimental results show that this proposed image captioning model can achieve better results on three RSIC datasets, UCM-Captions, Sydney-Captions, and RSICD datasets.","1558-0644","","10.1109/TGRS.2021.3132095","China National Funds for Distinguished Young Scientists(grant numbers:61725105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9632558","Attention mechanism;image captioning;image description;remote sensing","Visualization;Remote sensing;Feature extraction;Task analysis;Linguistics;Logic gates;Airports","geophysical image processing;image fusion;image segmentation;natural language processing;neural nets;remote sensing","remote sensing image captioning;global visual feature-guided attention mechanism;image feature fusion;visual-textual attention process;GVFGA mechanism;visual-textual feature fusion;linguistic state guided attention;attention-based caption models;redundant feature component removal;LS-guided attention mechanism;LSGA mechanism;UCM-Caption dataset;Sydney-Caption dataset;RSICD dataset;encoder-decoder framework;image feature saliency","","5","","56","IEEE","1 Dec 2021","","","IEEE","IEEE Journals"
"Cosaliency Detection and Region-of-Interest Extraction via Manifold Ranking and MRF in Remote Sensing Images","L. Zhang; H. Wu","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","4 Jan 2022","2022","60","","1","17","Saliency-based region-of-interest (ROI) extraction is significant for the interpretation of remote sensing images (RSIs). Recently, cosaliency detection has shown its superiority of better extraction of common ROIs by using both intraimage and interimage cues. However, most existing methods still suffer from the complex backgrounds of RSIs, resulting in incomplete ROI extraction, many false positives, and blurred boundaries. In this article, we propose a cosaliency detection framework via manifold ranking and the Markov random field (MRF) for RSIs to address these problems. First, we design a two-stage manifold ranking schema for converting single-image saliency maps (SISMs) to multi-image saliency maps (MISMs). This step takes full advantage of the correlation between images to improve the integrity of ROIs and reduce false positives. Second, we locally fuse saliency proposals by minimizing the energy function in an MRF. The design of the energy function comprehensively considers the global and local performance of saliency proposals to assign appropriate fusion weights. Finally, we generate the ROI masks by thresholding the cosaliency maps. Our approach is evaluated on four RSI datasets and compared to the state-of-the-art methods. Experimental results demonstrate the effectiveness of our model in both cosaliency detection and ROI extraction.","1558-0644","","10.1109/TGRS.2021.3079441","Beijing Natural Science Foundation(grant numbers:L182029); National Natural Science Foundation of China(grant numbers:61571050,41771407); Beijing Normal University (BNU) Interdisciplinary Research Foundation for the First-Year Doctoral Candidates(grant numbers:BNUXKJC1926); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442923","Manifold ranking;Markov random field (MRF);region-of-interest (ROI) extraction;remote sensing;saliency detection","Saliency detection;Manifolds;Proposals;Visualization;Remote sensing;Markov processes;Computational modeling","feature extraction;geophysical image processing;image colour analysis;image fusion;image representation;image segmentation;Markov processes;medical image processing;object detection;remote sensing","remote sensing images;saliency-based region-of-interest extraction;RSIs;common ROIs;intraimage;interimage cues;incomplete ROI extraction;false positives;cosaliency detection framework;Markov random field;MRF;two-stage manifold ranking schema;single-image saliency maps;multiimage saliency maps;saliency proposals;energy function;ROI masks;cosaliency maps","","","","68","IEEE","27 May 2021","","","IEEE","IEEE Journals"
"A Collaborative Correlation-Matching Network for Multimodality Remote Sensing Image Classification","W. Ma; N. Li; H. Zhu; K. Sun; Z. Ren; X. Tang; B. Hou; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","11 May 2022","2022","60","","1","18","Recently, with the increasing availability of the high-quality panchromatic (PAN) and multispectral (MS) remote sensing (RS) images, the inherent complementarity between PAN and MS images provides a wide development prospect for the multimodality RS image classification task. However, how to cleverly relieve the modal differences and effectively integrate the single-modality PAN and MS features is still a challenge. In this article, we design a collaborative correlation-matching network (CCM-Net) for multimodality RS image classification. Concretely, we first propose a bidirectional dominant feature supervision (Bi-DFS) learning, it utilizes single-modality dominant features as supplementary supervision information to establish the joint optimization loss function, thereby adaptively narrowing the differences between modalities before the feature extraction. In the feature extraction stage, the interactive correlation feature matching (ICFM) learning, composing the spatial feature matching (Spa-FM) and spectral feature matching (Spe-FM) strategies, is proposed to establish interactive matching and enhancement between multimodality strong correlation features from the perspective of spatial and spectral, respectively, thereby effectively alleviating the semantic deviation of multimodality features. Finally, we aggregate finer multilevel multimodality features to obtain top-level features with high discrimination. The effectiveness of the proposed algorithm has been verified on multiple datasets. Our code is available at: https://github.com/Momuli/CCM-Net.git.","1558-0644","","10.1109/TGRS.2022.3170312","The Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-05); National Natural Science Foundation of China(grant numbers:62006179,62101405); The China Postdoctoral Science Foundation funded project(grant numbers:2019M663634,2020T130492); The Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); The Key Scientific Technological Innovation Research Project by Ministry of Education; The Fundamental Research Funds for the Central Universities(grant numbers:XJS201904,JB211909); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762981","Correlation-matching;deep learning;multimodality classification;panchromatic (PAN) and multispectral (MS) image;remote sensing (RS)","Feature extraction;Remote sensing;Correlation;Semantics;Spatial resolution;Task analysis;Image color analysis","feature extraction;geophysical image processing;image classification;image fusion;image matching;learning (artificial intelligence);remote sensing","collaborative correlation-matching network;multimodality remote sensing image classification;PAN;MS;multimodality RS image classification task;modal differences;bidirectional dominant feature supervision;single-modality dominant features;supplementary supervision information;joint optimization loss function;feature extraction stage;interactive correlation feature matching learning;spatial feature;spectral feature;multimodality strong correlation features;top-level features;multilevel multimodality features","","","","50","IEEE","25 Apr 2022","","","IEEE","IEEE Journals"
"MIMA: MAPPER-Induced Manifold Alignment for Semi-Supervised Fusion of Optical Image and Polarimetric SAR Data","J. Hu; D. Hong; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","30 Oct 2019","2019","57","11","9025","9040","Multi-modal data fusion has recently been shown promise in classification tasks in remote sensing. Optical data and radar data, two important yet intrinsically different data sources, are attracting more and more attention for potential data fusion. It is already widely known that a machine learning-based methodology often yields excellent performance. However, the methodology relies on a large training set, which is very expensive to achieve in remote sensing. The semi-supervised manifold alignment (SSMA), a multi-modal data fusion algorithm, has been designed to amplify the impact of an existing training set by linking labeled data to unlabeled data via unsupervised techniques. In this paper, we explore the potential of SSMA in fusing optical data and polarimetric synthetic aperture radar (SAR) data, which are multi-sensory data sources. Furthermore, we propose a MAPPER-induced manifold alignment (MIMA) for the semi-supervised fusion of multi-sensory data sources. Our proposed method unites SSMA with MAPPER, which is developed from the emerging topological data analysis (TDA) field. To the best of our knowledge, this is the first time that SSMA has been applied on fusing optical data and SAR data, and also the first time that TDA has been applied in remote sensing. The conventional SSMA derives a topological structure using k-nearest neighbor (kNN), while MIMA employs MAPPER, which considers the field knowledge and derives a novel topological structure through the spectral clustering in a data-driven fashion. The experimental results on data fusion with respect to land cover land use classification and local climate zone classification suggest superior performance of MIMA.","1558-0644","","10.1109/TGRS.2019.2924113","Deutsche Forschungsgemeinschaft(grant numbers:ZH 498/7-2); H2020 European Research Council(grant numbers:ERC-2016-StG-714087 (So2Sat)); Helmholtz-Gemeinschaft(grant numbers:VH-NG-1018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802291","Hyperspectral image;MAPPER;multi-modal data fusion;multi-sensory data fusion;multispectral image;polarimetric synthetic aperture radar (PolSAR);semi-supervised manifold alignment (SSMA);topological data analysis (TDA)","Optical imaging;Optical sensors;Synthetic aperture radar;Remote sensing;Optical distortion;Manifolds;Optical scattering","data analysis;geophysical image processing;image classification;image fusion;nearest neighbour methods;pattern clustering;radar imaging;radar polarimetry;remote sensing;synthetic aperture radar","k-nearest neighbor;unsupervised techniques;data-driven fashion;emerging topological data analysis field;multisensory data sources;polarimetric synthetic aperture radar data;unlabeled data;multimodal data fusion algorithm;SSMA;semisupervised manifold alignment;machine learning-based methodology;potential data fusion;optical data;remote sensing;polarimetric SAR data;semisupervised fusion;MAPPER-induced manifold alignment;MIMA","","43","","70","CCBY","15 Aug 2019","","","IEEE","IEEE Journals"
"Category-Wise Fusion and Enhancement Learning for Multimodal Remote Sensing Image Semantic Segmentation","A. Zheng; J. He; M. Wang; C. Li; B. Luo","Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Artificial Intelligence, Anhui University, Hefei, China; Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China; Information Materials and Intelligent Sensing Laboratory of Anhui Province, Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Artificial Intelligence, Anhui University, Hefei, China; Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Computer Science and Technology, Anhui University, Hefei, China","IEEE Transactions on Geoscience and Remote Sensing","14 Dec 2022","2022","60","","1","12","This article presents a simple yet effective method called Category-wise Fusion and Enhancement learning (CaFE), which leverages the category priors to achieve effective feature fusion and imbalance learning, for multimodal remote sensing image semantic segmentation. In particular, we disentangle the feature fusion process via the categories to achieve the category-wise fusion based on the fact that the feature fusion in the same category regions tends to have similar characteristics. The disentangled fusion would also increase the fusion capacity with a small number of parameters while reducing the dependence on large-scale training data. For the sample imbalance problem, we design a simple yet effective category-wise enhancement learning scheme. In particular, we assign the weight for each category region based on the proportion of samples in this region over the whole image. By this way, the learning algorithm would focus more on the regions with smaller proportion. Note that both category-wise feature fusion and imbalance learning are only performed in the training stage, and the segmentation efficiency is thus not affected. Experimental results on two benchmark datasets demonstrate the effectiveness of our CaFE against other state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3225843","National Natural Science Foundation of China(grant numbers:61976002,61976003,61860206004,U20B2068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968034","Category-wise enhancement learning (CEL);category-wise fusion;imbalance learning;multimodal remote sensing;semantic segmentation","Remote sensing;Feature extraction;Semantic segmentation;Semantics;Decoding;Buildings;Automobiles","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);remote sensing","category priors;category region;category-wise feature fusion;category-wise fusion;disentangled fusion;effective feature fusion;Enhancement learning;feature fusion process;fusion capacity;imbalance learning;multimodal remote sensing image semantic segmentation;simple yet effective category-wise enhancement;simple yet effective method","","6","","58","IEEE","1 Dec 2022","","","IEEE","IEEE Journals"
"ADHR-CDNet: Attentive Differential High-Resolution Change Detection Network for Remote Sensing Images","X. Zhang; M. Tian; Y. Xing; Y. Yue; Y. Li; H. Yin; R. Xia; J. Jin; Y. Zhang","School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; Yellow River Institute of Hydraulic Research, Zhengzhou, China; Yellow River Institute of Hydraulic Research, Zhengzhou, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","21 Nov 2022","2022","60","","1","13","With the development of deep learning, change detection technology has gained great progress. However, how to effectively extract multiscale substantive changed features and accurately detect small changed objects as well as the accurate details is still a challenge. To solve the problem, we propose attentive differential high-resolution change detection network (ADHR-CDNet) for remote sensing images. In ADHR-CDNet, a novel high-resolution backbone with a differential pyramid module (DPM) is proposed to extract multilevel and multiscale substantive changed features. The backbone structure with four interconnected subnetwork branches of different resolution is helpful to extract multilevel and multiscale features. DPM is capable of distinguishing between substantive changes and pseudochanges induced by illumination, shadow, seasonal variation, and so on. Then, a novel multiscale spatial feature attention module (MSSAM) is presented to effectively fuse the spatial detail information of different scale features produced by our backbone to generate finer prediction. We conduct quantitative and qualitative experiments on three public change detection datasets: the Lebedev, LEVIR building change detection dataset (LEVIR-CD), and Wuhan University (WHU) Building datasets. The proposed ADHR-CDNet reaches F1-score of 97.2% (improved 3.1%) on the Lebedev dataset, 91.4% (improved 1.6%) on the LEVIR-CD dataset, and 90.9% (improved 1.2%) on the WHU Building dataset. The experimental results demonstrate that our method performs much better than the state-of-the-art (SOTA) methods. The visualization comparison results show that our method can effectively detect small changed objects and significantly improve the details of detected changed objects. Our code is available at https://github.com/w-here/ASGO-113lab/tree/main/ADHR-CDNet.","1558-0644","","10.1109/TGRS.2022.3221492","National Natural Science Foundation of China(grant numbers:61971356,U19B2037,62201467); National Natural Science Foundation of Shaanxi Province(grant numbers:2021KWZ-03,2022JQ-686); Major Scientific and Technological Special Project of Henan Province(grant numbers:201400211000); Science Foundation for Excellent Young Scholars of Henan Province(grant numbers:212300410059); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946010","Attention module;change detection;deep learning;differential information;remote sensing","Feature extraction;Remote sensing;Image resolution;Convolutional neural networks;Buildings;Task analysis;Deep learning","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;object detection;remote sensing;video signal processing","ADHR-CDNet;change detection technology;detected changed objects;differential high-resolution change detection network;feature attention module;LEVIR-CD dataset;multilevel features;multiscale features;multiscale substantive changed features;novel high-resolution backbone;public change detection datasets;pyramid module;remote sensing images;scale features;substantive changes;WHU Building dataset;Wuhan University Building datasets","","3","","64","IEEE","10 Nov 2022","","","IEEE","IEEE Journals"
"A Degraded Reconstruction Enhancement-Based Method for Tiny Ship Detection in Remote Sensing Images With a New Large-Scale Dataset","J. Chen; K. Chen; H. Chen; Z. Zou; Z. Shi","State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; Department of Guidance, Navigation and Control, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","17 Jun 2022","2022","60","","1","14","The rapid detection of ships within the wide sea area is essential for intelligence acquisition. Most modern deep learning-based ship detection methods focus on locating ships in high-resolution (HR) remote sensing (RS) images. Seldom efforts have been made on ship detection in medium-resolution (MR) RS images. An MR image covers a much wider area than an HR one of the same size, thus facilitating quick ship detection. To this end, we propose a tiny ship detection method, namely, the degraded reconstruction enhancement network (DRENet), for MR RS images. Different from previous methods that mainly focus on feature fusion strategies to improve the expression ability of the detector, we design an additional network branch, i.e., degraded reconstruction enhancer, to learn to regress an object-aware blurred version of the input image in the training phase. Our intuition is that the proposed reconstruction branch may guide the backbone to focus more on tiny ship targets instead of the vast background. Moreover, we incorporate a CRoss-stage Multi-head Attention module in the detector to further improve the feature discrimination by leveraging the self-attention mechanism. To fill the gap of lacking a large-scale MR ship detection dataset, we introduce LEVIR-Ship, which contains 3896 GF-1/GF-6 multispectral images and over 3k tiny ship instances. Experiments on LEVIR-Ship validate the effectiveness and efficiency of the proposed method. Our method achieves 82.4 AP with 85 FPS, which outperforms many state-of-the-art ship detection methods. Our code and dataset are available at https://github.com/WindVChen/DRENet.","1558-0644","","10.1109/TGRS.2022.3180894","National Key Research and Development Program of China(grant numbers:2019YFC1510905); National Natural Science Foundation of China(grant numbers:62125102); Beijing Natural Science Foundation(grant numbers:4192034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791363","Convolutional neural network (CNN);deep learning (DL);optical image;remote sensing (RS);ship detection","Marine vehicles;Feature extraction;Detectors;Image reconstruction;Training;Remote sensing;Spatial resolution","deep learning (artificial intelligence);geophysical image processing;image fusion;image reconstruction;image resolution;object detection;remote sensing;ships","wide sea area;deep learning;ship location;high-resolution remote sensing images;medium-resolution RS images;quick ship detection;tiny ship detection method;degraded reconstruction enhancement network;MR RS images;input image;tiny ship targets;DRENet;LEVIR-Ship;feature fusion;GF-1/GF-6 multispectral images","","1","","69","IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"Pan-Sharpening Using an Efficient Bidirectional Pyramid Network","Y. Zhang; C. Liu; M. Sun; Y. Ou","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","22 Jul 2019","2019","57","8","5549","5563","Pan-sharpening is an important preprocessing step for remote sensing image processing tasks; it fuses a low-resolution multispectral image and a high-resolution (HR) panchromatic (PAN) image to reconstruct a HR multispectral (MS) image. This paper introduces a new end-to-end bidirectional pyramid network for pan-sharpening. The overall structure of the proposed network is a bidirectional pyramid, which permits the network to process MS and PAN images in two separate branches level by level. At each level of the network, spatial details extracted from the PAN image are injected into the upsampled MS image to reconstruct the pan-sharpened image from coarse resolution to fine resolution. Subpixel convolutional layers and the enhanced residual blocks are used to make the network efficient. Comparison of the results obtained with our proposed method and the results using other widely used state-of-the-art approaches confirms that our proposed method outperforms the others in visual appearance and objective indexes.","1558-0644","","10.1109/TGRS.2019.2900419","National Basic Research Program of China (973 Program)(grant numbers:2018YFB0505003); National Natural Science Foundation of China(grant numbers:41871368); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667448","Bidirectional pyramid network (BDPN);deep learning;image fusion;multilevel;pan-sharpening;remote sensing","Image reconstruction;Spatial resolution;Convolution;Sensors;Distortion;Indexes","geophysical image processing;geophysical techniques;image resolution;remote sensing","pan-sharpened image;coarse resolution;fine resolution;network efficient;efficient bidirectional pyramid network;remote sensing image processing tasks;low-resolution multispectral image;high-resolution panchromatic image;HR multispectral image;end-to-end bidirectional pyramid network;separate branches level;PAN image;upsampled MS image;preprocessing step;subpixel convolutional layers","","76","","52","IEEE","14 Mar 2019","","","IEEE","IEEE Journals"
"Multispectral and Hyperspectral Image Fusion Using a 3-D-Convolutional Neural Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Geoscience and Remote Sensing Letters","24 Apr 2017","2017","14","5","639","643","In this letter, we propose a method using a 3-D convolutional neural network to fuse together multispectral and hyperspectral (HS) images to obtain a high resolution HS image. Dimensionality reduction of the HS image is performed prior to fusion in order to significantly reduce the computational time and make the method more robust to noise. Experiments are performed on a data set simulated using a real HS image. The results obtained show that the proposed approach is very promising when compared with conventional methods. This is especially true when the HS image is corrupted by additive noise.","1558-0571","","10.1109/LGRS.2017.2668299","Icelandic Research Fund(grant numbers:174075-05); University of Iceland Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869277","Convolutional neural networks (CNNs);deep learning (DL);hyperspectral (HS);image fusion;multispectral (MS)","Spatial resolution;Neurons;Loading;Principal component analysis;Neural networks;Training","geophysical techniques;hyperspectral imaging;neural nets","multispectral image fusion;hyperspectral image fusion;3-D-convolutional neural network;high resolution image;additive noise","","201","","21","IEEE","2 Mar 2017","","","IEEE","IEEE Journals"
"PFAF-Net: Pyramid Feature Network for Multimodal Fusion","A. Raza; H. Huo; T. Fang","School of Automation, Shanghai Jiaotong University, Shanghai, China; School of Automation, Shanghai Jiaotong University, Shanghai, China; School of Automation, Shanghai Jiaotong University, Shanghai, China","IEEE Sensors Letters","4 Jan 2021","2020","4","12","1","4","Multimodal fusion is an essential research area in computer vision application. However, there are still many obstacles in the image fusion domain that cause the loss key content, due to method limitation or its least efficiency. To solve these problems, a novel method pyramid feature attention fusion strategy (PFAF-Net) based on multiscale features with core idea of different level fusion strategy is proposed. First, multiscale high-level features with different receptive fields are extracted by pyramid feature extraction module. Second, high-level and low-level features are fused with different fusion techniques, i.e., attention-based fusion strategies are adopted to efficiently fused the global and local features separately. Finally, the fused image is reconstructed with the enhanced fused features by the decoder. Thus, the proposed method not only integrates multimodal data but also has rich content details, inferring a superior image fusion. In addition, the proposed PFAF-Net has better generalization ability than existing methods on four different multimodal (i.e., multimodal medical, multiexposure, multifocus, and visual-infrared) benchmark datasets. Compared with other state-of-the-art fusion methods experimentally, the proposed method has shown comparable or better fusion performance in both subjective and objective evaluation.","2475-1472","","10.1109/LSENS.2020.3041585","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB0505000); National Science and Technology Planning Project(grant numbers:21-Y20A06-9001-17/18); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274527","Sensor systems;multimodal fusion;multiscale feature;remote sensing;sensor data fusion","Feature extraction;Image fusion;Remote sensing;Sensor systems;Multimodal sensors ;Sensor fusion;Image reconstruction","computer vision;feature extraction;image fusion","attention fusion strategy;PFAF-Net;multiscale features;pyramid feature extraction module;attention-based fusion strategies;global features;image fusion;pyramid feature network;multimodal fusion;computer vision application;method limitation;fused feature enhancement","","5","","25","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"Semisupervised Two-Level Fusion-Based Autoencoded Approach for Low-Cost Domain Adaptation of Remotely Sensed Images","S. Chakraborty; M. Roy; F. Melgani","Indian Institute of Information Technology, Guwahati, India; Indian Institute of Information Technology, Guwahati, India; University of Trento, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","24 Jun 2019","2019","16","7","1041","1045","In this letter, a low-cost semisupervised domain adaptation technique has been proposed using a two-level fusion of artificial neural networks. The proposed technique has been aimed to solve the problem of sample selection bias using a one-time collection of a few patterns from the target domain under semisupervised framework. To assess the effectiveness, experiments are conducted on the two source-target data sets acquired over India. The results are found to be encouraging.","1558-0571","","10.1109/LGRS.2019.2893647","Science and Engineering Research Board(grant numbers:ECR/2016/001227); DST for providing INSPIRE fellowship(grant numbers:IF150878); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632691","Autoencoding;domain adaptation (DA);ensemble approach;remote sensing;semisupervised learning","Neurons;Training;Remote sensing;Neural networks;Prediction algorithms;Semisupervised learning;Adaptation models","geophysical image processing;image fusion;learning (artificial intelligence);neural nets;remote sensing","target domain;semisupervised framework;remotely sensed images;low-cost semisupervised domain adaptation technique;artificial neural networks;sample selection bias;semisupervised two-level fusion-based autoencoded approach;two source-target data sets;India","","4","","11","IEEE","1 Feb 2019","","","IEEE","IEEE Journals"
"A New Variational Approach Based on Proximal Deep Injection and Gradient Intensity Similarity for Spatio-Spectral Image Fusion","Z. -C. Wu; T. -Z. Huang; L. -J. Deng; G. Vivone; J. -Q. Miao; J. -F. Hu; X. -L. Zhao","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; CNR-IMAA, National Research Council - Institute of Methodologies for Environmental Analysis, Tito Scalo, Italy; School of Computer Science, and Technology, University of Southwest Minzu of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","28 Oct 2020","2020","13","","6277","6290","Pansharpening is a very debated spatio-spectral fusion problem. It refers to the fusion of a high spatial resolution panchromatic image with a lower spatial but higher spectral resolution multispectral image in order to obtain an image with high resolution in both the domains. In this article, we propose a novel variational optimization-based (VO) approach to address this issue incorporating the outcome of a deep convolutional neural network (DCNN). This solution can take advantages of both the paradigms. On one hand, higher performance can be expected introducing machine learning (ML) methods based on the training by examples philosophy into VO approaches. On other hand, the combination of VO techniques with DCNNs can aid the generalization ability of these latter. In particular, we formulate a $\ell _2$-based proximal deep injection term to evaluate the distance between the DCNN outcome, and the desired high spatial resolution multispectral image. This represents the regularization term for our VO model. Furthermore, a new data fitting term measuring the spatial fidelity is proposed. Finally, the proposed convex VO problem is efficiently solved by exploiting the framework of the alternating direction method of multipliers (ADMM), thus guaranteeing the convergence of the algorithm. Extensive experiments both on simulated, and real datasets demonstrate that the proposed approach can outperform state-of-the-art spatio-spectral fusion methods, even showing a significant generalization ability. Please find the project page at https://liangjiandeng.github.io/Projects_Res/DMPIF_2020jstars.html.","2151-1535","","10.1109/JSTARS.2020.3030129","National Natural Science Foundation of China(grant numbers:61772003,61702083,61876203); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9220783","Deep convolutional neural networks (DCNN);dynamic gradient sparsity;gradient intensity similarity;image fusion;pansharpening;remote sensing;variational approaches","Spatial resolution;Optimization;Training;Image fusion;Convolutional neural networks;Data models","convolutional neural nets;image fusion;image resolution;learning (artificial intelligence);variational techniques","spatio-spectral fusion methods;spectral resolution multispectral image;variational optimization;high spatial resolution panchromatic image;debated spatio-spectral fusion problem;spatio-spectral image fusion;gradient intensity similarity;alternating direction method;convex VO problem;spatial fidelity;data fitting term;VO model;regularization term;high spatial resolution multispectral image;DCNN outcome;proximal deep injection term;machine learning methods;deep convolutional neural network","","16","","64","CCBY","12 Oct 2020","","","IEEE","IEEE Journals"
"Multiview Inherent Graph Hashing for Large-Scale Remote Sensing Image Retrieval","Y. Sun; W. Wu; X. Shen; Z. Cui","School of Computer and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Nov 2021","2021","14","","10705","10715","Remote sensing image retrieval (RSIR) is one of the most challenging tasks in remote sensing (RS) community. With the volume of RS images increases explosively, conventional exhaustive search is often infeasible in real applications. Recently, hashing has attracted increasing attention for RSIR due to significant advantage in terms of computation and storage. Hashing first generates a set of short compact hash codes to encode RS images, and then applies hash codes for effective RSIR. Multiview hashing usually achieves promising RSIR performance by fusing multiples kinds of RS image features. Conventional multiview hashing simply predefines graph Laplacian in each view, which cannot effectively explore underlying similarity structures among RS images. To address this issue, this article proposes a novel multiview inherent graph hashing (MvIGH) for RSIR. MvIGH captures the latent similarities among RS images, and adaptively learns weights of each view to characterize its contribution. In addition, MvIGH further minimizes the quantization errors. We develop an efficient alternating algorithm to solve the formulated optimization problem. The experiments on three public RS image datasets demonstrate the superiority of the proposed method over the existing multiview hashing methods in RSIR tasks.","2151-1535","","10.1109/JSTARS.2021.3121142","National Natural Science Foundation of China(grant numbers:62176126,61906091); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190440); Fundamental Research Funds for the Central Universities(grant numbers:30921011210); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580682","Hash learning;large-scale remote sensing (RS) image retrieval;multiview remote sensing data","Codes;Remote sensing;Quantization (signal);Hash functions;Task analysis;Binary codes;Feature extraction","feature extraction;file organisation;geophysical image processing;graph theory;image representation;image retrieval;learning (artificial intelligence);optimisation;remote sensing","large-scale remote sensing image retrieval;remote sensing community;RS images;conventional exhaustive search;short compact hash codes;effective RSIR;promising RSIR performance;RS image features;conventional multiview hashing;MvIGH;public RS image datasets;existing multiview hashing;RSIR tasks;multiview inherent graph hashing","","2","","49","CCBY","19 Oct 2021","","","IEEE","IEEE Journals"
"A Multi-Level Convolution Pyramid Semantic Fusion Framework for High-Resolution Remote Sensing Image Scene Classification and Annotation","X. Sun; Q. Zhu; Q. Qin","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Resources and Environmental Information System, Beijing, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Access","1 Feb 2021","2021","9","","18195","18208","High spatial resolution (HSR) imagery scene classification has become a hot research topic in remote sensing. Scene classification method based on the handcrafted features, such as the bag-of-visual-words (BoVW) model, describes an image by extracting local features of the scene and mapping them to the dictionary space, but usually uses a shallow structure and loses the spatial distribution characteristics of the scene. The method based on deep learning extracts hierarchical features to describe the scene, which can maintain the spatial position information well. However, deep features in different levels have scale recognition restrictions for multi-scale ground objects, and cannot understand complex scenes well. In this paper, the multi-level convolutional pyramid semantic fusion (MCPSF) framework is proposed for HSR imagery scene classification. Differing from previous scene classification methods, which integrate the feature of different levels directly, of which the fusion features have large differences in both sparsity and eigenvalue magnitude, MCPSF integrates multi-level semantic features extracted by BoVW model and convolutional neural network (CNN) model. In MCPSF, two convolution pyramid feature expression strategies are proposed to enhance the ability of capturing multi-scale land objects, i.e., local and convolutional pyramid based BoVW (LCPB) model and local and convolutional pyramid based pooling-stretched (LCPP) model. The effectiveness of the proposed method is verified on 21-class UC Merced (UCM) dataset and 30-class Aerial Image Dataset (AID). The framework was also transferred toa case study of scene annotation in Wuhan. The proposed framework significantly improves the performance when compared with other state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2021.3052977","Open Research Project of The Hubei Key Laboratory of Intelligent Geo-Information Processing(grant numbers:KLIGIP-2019A02); State Key Laboratory of Resources and Environmental Information System; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335555","High spatial resolution image;scene classification;bag of visual words;feature pyramid;multi-level;remote sensing","Feature extraction;Image analysis;Semantics;Remote sensing;Visualization;Convolution;Training","convolutional neural nets;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","multilevel convolution pyramid semantic fusion framework;high-resolution remote sensing Image scene classification;high spatial resolution imagery scene classification;handcrafted features;bag-of-visual-words model;local features;spatial distribution characteristics;deep learning;hierarchical features;spatial position information;deep features;scale recognition restrictions;multiscale ground objects;complex scenes;multilevel convolutional pyramid semantic fusion framework;MCPSF;HSR imagery scene classification;fusion features;convolution pyramid feature;multiscale land objects;local pyramid;BoVW model;30-class Aerial Image Dataset;scene annotation","","13","","48","CCBYNCND","25 Jan 2021","","","IEEE","IEEE Journals"
"Model-Based Reduced-Rank Pansharpening","F. Palsson; M. O. Ulfarsson; J. R. Sveinsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Geoscience and Remote Sensing Letters","25 Mar 2020","2020","17","4","656","660","Observation of the Earth using satellites mounted with optical sensors is an important application of remote sensing. Owing to physical constraints, multispectral (MS) sensors acquire images of lower spatial resolution than a single-band panchromatic (PAN) sensor that acquires images of the same scene. Pansharpening fuses the MS and PAN images to obtain an MS image with the same spatial resolution as the PAN image. In this letter, we propose to expand a method, initially developed for Sentinel-2 single-sensor sharpening, for pansharpening. The expanded method is based on solving a non-convex MS acquisition model using optimization methods based on cyclic decent and manifold optimization. The tuning parameters of the method are chosen using Bayesian optimization with reduced-scale evaluation. The proposed method is compared with a number of established pansharpening methods and is validated using both synthetic and real data sets.","1558-0571","","10.1109/LGRS.2019.2926681","Icelandic Research Fund(grant numbers:174075-05); Research Fund of the University of Iceland; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778736","Cyclic descent;data fusion;image fusion;pansharpening","Optimization;Optical sensors;Spatial resolution;Image sensors;Sensor fusion","Bayes methods;geophysical image processing;image fusion;image resolution;optical sensors;optimisation;remote sensing","model-based reduced-rank pansharpening;optical sensors;remote sensing;physical constraints;multispectral sensors;lower spatial resolution;single-band panchromatic sensor;pansharpening fuses;MS image;PAN image;Sentinel-2 single-sensor sharpening;expanded method;nonconvex MS acquisition model;cyclic decent optimization;manifold optimization;Bayesian optimization;reduced-scale evaluation;pansharpening methods","","25","","14","IEEE","29 Jul 2019","","","IEEE","IEEE Journals"
"Deep Fusion of Remote Sensing Data for Accurate Classification","Y. Chen; C. Li; P. Ghamisi; X. Jia; Y. Gu","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; School of Engineering and Information Technology, The University of New South Wales, Canberra, NSW, Australia; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","20 Jul 2017","2017","14","8","1253","1257","The multisensory fusion of remote sensing data has obtained a great attention in recent years. In this letter, we propose a new feature fusion framework based on deep neural networks (DNNs). The proposed framework employs deep convolutional neural networks (CNNs) to effectively extract features of multi-/hyperspectral and light detection and ranging data. Then, a fully connected DNN is designed to fuse the heterogeneous features obtained by the previous CNNs. Through the aforementioned deep networks, one can extract the discriminant and invariant features of remote sensing data, which are useful for further processing. At last, logistic regression is used to produce the final classification results. Dropout and batch normalization strategies are adopted in the deep fusion framework to further improve classification accuracy. The obtained results reveal that the proposed deep fusion model provides competitive results in terms of classification accuracy. Furthermore, the proposed deep learning idea opens a new window for future remote sensing data fusion.","1558-0571","","10.1109/LGRS.2017.2704625","State Key Laboratory of Frozen Soil Engineering(grant numbers:SKLFSE201614); Natural Science Foundation of China(grant numbers:61301206,61371180,60972144); National Science Foundation for Excellent Young Scholars(grant numbers:61522107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7940007","Convolutional neural network (CNN);data fusion;deep neural network (DNN);feature extraction (FE);hyperspectral image (HSI);light detection and ranging (LiDAR);multispectral image (MSI)","Feature extraction;Laser radar;Training;Remote sensing;Data mining;Neurons;Fuses","feature extraction;image classification;image fusion;neural nets;optical radar;regression analysis;remote sensing by laser beam","remote sensing data deep fusion;image classification;feature fusion framework;deep neural networks;convolutional neural networks;feature extraction;multihyperspectral data;light detection and ranging data;logistic regression;dropout strategy;batch normalization strategy","","109","","15","IEEE","6 Jun 2017","","","IEEE","IEEE Journals"
"Mapping of Himalaya Leucogranites Based on ASTER and Sentinel-2A Datasets Using a Hybrid Method of Metric Learning and Random Forest","Z. Wang; R. Zuo; Y. Dong","State Key Laboratory of Geological Processes and Mineral Resources, China University of Geosciences, Wuhan, China; State Key Laboratory of Geological Processes and Mineral Resources, China University of Geosciences, Wuhan, China; Institute of Geophysics and Geomatics, Hubei Subsurface Multiscale Imaging Key Laboratory, China University of Geosciences, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 May 2020","2020","13","","1925","1936","The widely distributed leucogranite belt in the Himalayan orogeny is expected to have excellent potential for developing rare metal mineralization. Finding a way to effectively map the spatial distribution of leucogranites would be a significant contribution to rare metal exploration. Research shows remote sensing technology has long been recognized the significance in geological works, which greatly promoted mineral exploration in a cost-effective manner, especially in the Himalayan orogenic belt with poor natural environment. However, several challenges still exist in relation to the limited spectral band and spatial resolution of remote sensing images, as well as the onerous data processing. In this context, this study sought to resolve these two issues by applying a hybrid approach that comprises image fusion, metric learning, and random forest methods. For the first challenge, multisource and multisensor remote sensing data were integrated to provide more comprehensive spatial texture characteristics and spectral information. To address the second challenge, this study used a hybrid method of metric learning and random forest to promote computing efficiency and classification accuracy. This process is illustrated through a case study of lithological mapping in Cuonadong dome, the northern part of the Himalayan orogeny belt. Seven target lithological units were effectively discriminated with an 85.75% overall accuracy. This provides an important scientific basis for further exploration for rare metal deposits in the Himalayan orogeny belt, and a way of thinking for detecting geological features under harsh natural conditions.","2151-1535","","10.1109/JSTARS.2020.2989509","National Natural Science Foundation of China(grant numbers:41972303,61801444); Open Research Project of the Hubei Key Laboratory of Intelligent Geo-Information Processing(grant numbers:KLIGIP-2018A01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079596","Himalaya leucogranites;lithological mapping;metric learning;random forest;remote sensing","Measurement;Remote sensing;Geology;Random forests;Belts;Spatial resolution;Metals","geochemistry;geology;geophysical image processing;geophysical prospecting;image classification;image fusion;learning (artificial intelligence);minerals;remote sensing;rocks","leucogranite belt;Himalaya leucogranites;rare metal deposits;Himalayan orogeny belt;lithological mapping;classification accuracy;computing efficiency;hybrid method;spectral information;comprehensive spatial texture characteristics;remote sensing data;random forest methods;image fusion;onerous data processing;remote sensing images;spectral band;poor natural environment;Himalayan orogenic belt;mineral exploration;remote sensing technology;rare metal exploration;rare metal mineralization;metric learning","","8","","60","CCBY","27 Apr 2020","","","IEEE","IEEE Journals"
"An Adaptive Pansharpening Method by Using Weighted Least Squares Filter","Y. Song; W. Wu; Z. Liu; X. Yang; K. Liu; W. Lu","College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; School of Engineering, The University of British Columbia, Kelowna, V1V1V7, Canada; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; School of Electrical Engineering and Information, Sichuan University, Chengdu, China; School of Software Engineering, Beijing Jiaotong University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","6 Jan 2016","2016","13","1","18","22","Multisensor image fusion or pansharpening aims to sharpen a multispectral (MS) image by integrating the detail map derived from a panchromatic (Pan) image. The intensity-hue-saturation (IHS)-based methods are well adopted in pansharpening applications. However, the pansharpened MS images by IHS-based methods usually suffer from serious spectral distortions and local artifacts due to the mismatch between the estimated detail map and its ground truth. To overcome these defects, we propose a weighted least squares (WLS)-filter-based method in this letter. Different from existing IHS-based methods, the proposed method eliminates the influence of the low-frequency components of the Pan and MS images with the WLS filter. Moreover, the derived detail map is further refined based on the spectral signatures for different bands of the MS image. We test the proposed method on various satellites data; the experimental results demonstrate that the proposed method performs well in both spectral and spatial qualities.","1558-0571","","10.1109/LGRS.2015.2492569","National Natural Science Foundation of China(grant numbers:61271330,61411140248); Science and Technology Plan of Sichuan Province(grant numbers:2014GZ0005); National Science Foundation for Post-doctoral Scientists of China(grant numbers:2014M552357); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317768","Image fusion;intensity–hue–saturation (IHS) transform;pansharpening;weighted least squares (WLS) filter;Image fusion;intensity–hue–saturation (IHS) transform;pansharpening;weighted least squares (WLS) filter","Distortion;Remote sensing;Hybrid fiber coaxial cables;Satellites;Image fusion;Spatial resolution","geophysical image processing;image fusion;least squares approximations","adaptive pansharpening method;weighted least squares filter;multisensor image fusion;multispectral image;panchromatic image;intensity-hue-saturation-based methods;pansharpened MS images;local artifacts;spectral distortions;low-frequency components;spectral signatures","","33","","23","IEEE","4 Nov 2015","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion via Graph Laplacian-Guided Coupled Tensor Decomposition","Y. Bu; Y. Zhao; J. Xue; J. C. -W. Chan; S. G. Kong; C. Yi; J. Wen; B. Wang","Research and Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen, China; Research and Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen, China; Research and Development Institute of Northwestern Polytechnical University in Shenzhen, Shenzhen, China; Department of Electronics and Informatics, Vrije Universiteit Brussel, Brussel, Belgium; Department of Computer Engineering, Sejong University, Seoul, South Korea; Department of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Natural and Applied Sciences, Northwestern Polytechnical University, Xi’an, China; Department of Automation, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","648","662","We propose a novel graph Laplacian-guided coupled tensor decomposition (gLGCTD) model for fusion of hyperspectral image (HSI) and multispectral image (MSI) for spatial and spectral resolution enhancements. The coupled Tucker decomposition is employed to capture the global interdependencies across the different modes to fully exploit the intrinsic global spatial-spectral information. To preserve local characteristics, the complementary submanifold structures embedded in high-resolution (HR)-HSI are encoded by the graph Laplacian regularizations. The global spatial-spectral information captured by the coupled Tucker decomposition and the local submanifold structures are incorporated into a unified framework. The gLGCTD fusion framework is solved by a hybrid framework between the proximal alternating optimization (PAO) and the alternating direction method of multipliers (ADMM). Experimental results on both synthetic and real data sets demonstrate that the gLGCTD fusion method is superior to state-of-the-art fusion methods with a more accurate reconstruction of the HR-HSI.","1558-0644","","10.1109/TGRS.2020.2992788","National Natural Science Foundation of China(grant numbers:61771391); Shenzhen Municipal Science and Technology Innovation Committee(grant numbers:JCYJ20170815162956949,JCYJ20180306171146740); Key R&D Plan of Shaanxi Province 2020ZDLGY07-11; Fund for Scientific Research in Flanders (Fondsvoor Wetenschappelijk Onderzoek–Vlaanderen) through Data Fusion for Image Analysis in Remote Sensing(grant numbers:G037115N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9094715","Coupled tensor decomposition;graph Laplacian;hyperspectral imaging;image fusion;manifold structure","Tensile stress;Matrix decomposition;Sparse matrices;Laplace equations;Manifolds;Hyperspectral imaging;Spatial resolution","graph theory;hyperspectral imaging;image fusion;image resolution;optimisation;tensors","gLGCTD fusion framework;gLGCTD fusion method;hyperspectral image fusion;multispectral image fusion;tensor decomposition model;spatial resolution enhancements;spectral resolution enhancements;coupled Tucker decomposition;global interdependencies;intrinsic global spatial-spectral information;local characteristics;complementary submanifold structures;high-resolution-HSI;graph Laplacian regularizations;local submanifold structures;alternating direction method of multipliers","","31","","47","IEEE","18 May 2020","","","IEEE","IEEE Journals"
"Multiscale Multiinteraction Network for Remote Sensing Image Captioning","Y. Wang; W. Zhang; Z. Zhang; X. Gao; X. Sun","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Mar 2022","2022","15","","2154","2165","Much of the recent work in remote sensing image captioning is influenced by natural image captioning. These methods tend to fix the defects of the model architecture to improve the previous work, but pay little attention to the differences between remote sensing images and natural images. By considering these differences, we propose a multiscale multiinteraction remote sensing image captioning model. As in Fig. 1(a), the targets in remote sensing images have a wide range of scales; while the natural images are generally taken close-up, resulting in a similar scale for the foreground targets. Due to the difference in shooting methods, the model pretrained on close-up natural images cannot capture multiscale remote sensing targets well. To alleviate this problem, we propose a two-stage multiscale structure for feature representation, where we first finetune the CNN backbone on remote sensing images for domain adaption, then we collect features from different stages as the multiscale feature representation. Moreover, due to the shooting distance, the height information of the target in the remote sensing image is greatly weakened, thus some objects like low plants and grasses become difficult to identify, as in Fig. 1(b). Thus, we further propose a multiinteraction feature representation module, where information flow of the same and different layers could effectively interact. By calculating the similarity score among features, we fuse features with high similarity, and increase the distance between features of different categories, thereby enhancing the distinguishability. Results on RSICD, Sydney-Captions, and UCM-Captions show a clear improvement over the compared methods.","2151-1535","","10.1109/JSTARS.2022.3153636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720234","Multiscale;multiinteraction;remote sensing image captioning","Remote sensing;Task analysis;Sensors;Feature extraction;Logic gates;Head;Automobiles","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image representation;object detection;remote sensing","natural image captioning;multiscale multiinteraction remote sensing image captioning model;foreground targets;shooting method;two-stage multiscale structure;feature representation;CNN;domain adaption;shooting distance;height information;RSICD;Sydney-Captions;UCM-Caption","","3","","48","CCBY","23 Feb 2022","","","IEEE","IEEE Journals"
"MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification","D. Lin; K. Fu; Y. Wang; G. Xu; X. Sun","Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","25 Oct 2017","2017","14","11","2092","2096","With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks. However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model G and a discriminative model D. We treat D as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. G can produce numerous images that are similar to the training data; therefore, D can learn better representations of remotely sensed images using the training data provided by G. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.","1558-0571","","10.1109/LGRS.2017.2752750","National Natural Science Foundation of China(grant numbers:41501485,61331017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8059820","Generative adversarial networks (GANs);scene classification;unsupervised representation learning","Generators;Gallium nitride;Training;Remote sensing;Feature extraction;Training data;Computational modeling","feature extraction;geophysical image processing;image classification;image fusion;image matching;image representation;remote sensing;unsupervised learning","global features;training data;unsupervised representation;remote sensing image classification;deep learning;supervised learning;convolutional networks;unsupervised model;multiple-layer feature-matching generative adversarial networks;unlabeled data;generative model G;discriminative model D;feature extractor;remote sensing data;remote sensing image databases;MARTA GAN;fusion layer","","131","","15","IEEE","5 Oct 2017","","","IEEE","IEEE Journals"
"BASNet: A Boundary-Aware Siamese Network for Accurate Remote-Sensing Change Detection","H. Wei; R. Chen; C. Yu; H. Yang; S. An","Tianjin Key Laboratory of Imaging and Sensing Microelectronic Technology, School of Microelectronics, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Imaging and Sensing Microelectronic Technology, School of Microelectronics, Tianjin University, Tianjin, China; School of Information Science and Engineering, Shandong University, Qingdao, China; Tianjin Key Laboratory of Imaging and Sensing Microelectronic Technology, School of Microelectronics, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Imaging and Sensing Microelectronic Technology, School of Microelectronics, Tianjin University, Tianjin, China","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","Change detection (CD) in remote-sensing images is one of the most crucial topics in the computer vision community. Most recent CD pipelines focus on introducing attention mechanism to enhance the discriminative ability of network, but their crude model architectures lead to inaccurate predictions and irregular boundaries. In this letter, we present a boundary-aware Siamese network (BASNet) for accurate remote-sensing CD. Based on the encoder–decoder architecture, we first propose a novel multiscale paired fusion module (MPFM) to effectively fuse the same-level feature pairs from the Siamese encoding stream. In addition, we design a location guidance module (LGM) to accurately locate the changed regions. Based on the observation that hierarchical features show different level information, we propose a multilevel feature aggregation module (MFAM) to merge the bottom-up features. Finally, we introduce a hybrid loss that fuses structural similarity (SSIM) loss and binary cross entropy (BCE) loss to focus on the structural integrity and boundary quality of changed regions. Experimental results on two public datasets demonstrate that our proposed method significantly improves the performance and outperforms the other state-of-the-art methods.","1558-0571","","10.1109/LGRS.2021.3119885","National Natural Science Foundation of China(grant numbers:61871284); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9570318","Change detection (CD);location guidance module (LGM);paired feature fusion;Siamese network","Feature extraction;Remote sensing;Task analysis;Propagation losses;Decoding;Fuses;Deep learning","computer vision;decoding;entropy;feature extraction;geophysical image processing;image classification;image fusion;image representation;image retrieval;learning (artificial intelligence);object detection;remote sensing","BASNet;boundary-aware siamese network;remote-sensing change detection;remote-sensing images;crucial topics;computer vision community;recent CD pipelines focus;crude model architectures;irregular boundaries;remote-sensing CD;encoder-decoder architecture;fusion module;same-level feature pairs;Siamese encoding stream;location guidance module;changed regions;hierarchical features;multilevel feature aggregation module;fuses structural similarity loss;structural integrity;boundary quality","","3","","17","IEEE","14 Oct 2021","","","IEEE","IEEE Journals"
"SSR-NET: Spatial–Spectral Reconstruction Network for Hyperspectral and Multispectral Image Fusion","X. Zhang; W. Huang; Q. Wang; X. Li","School of Computer Science, Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Center for OPTical IMagery Analysis and Learning (OPTIMAL), Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2021","2021","59","7","5953","5965","The fusion of a low-spatial-resolution hyperspectral image (HSI) (LR-HSI) with its corresponding high-spatial-resolution multispectral image (MSI) (HR-MSI) to reconstruct a high-spatial-resolution HSI (HR-HSI) has been a significant subject in recent years. Nevertheless, it is still difficult to achieve the cross-mode information fusion of spatial mode and spectral mode when reconstructing HR-HSI for the existing methods. In this article, based on a convolutional neural network (CNN), an interpretable spatial-spectral reconstruction network (SSR-NET) is proposed for more efficient HSI and MSI fusion. More specifically, the proposed SSR-NET is a physical straightforward model that consists of three components: 1) cross-mode message inserting (CMMI); this operation can produce the preliminary fused HR-HSI, preserving the most valuable information of LR-HSI and HR-MSI; 2) spatial reconstruction network (SpatRN); the SpatRN concentrates on reconstructing the lost spatial information of LR-HSI with the guidance of spatial edge loss ( L spat); and 3) spectral reconstruction network (SpecRN); the SpecRN pays attention to reconstruct the lost spectral information of HR-MSI under the constraint of spatial edge loss ( L spec). Comparative experiments are conducted on six HSI data sets of Urban, Pavia University (PU), Pavia Center (PC), Botswana, Indian Pines (IP), and Washington DC Mall (WDCM), and the proposed SSR-NET achieves the superior or competitive results in comparison with seven state-of-the-art methods. The code of SSR-NET is available at https://github.com/hw2hwei/SSRNET.","1558-0644","","10.1109/TGRS.2020.3018732","National Key Research and Development Program of China(grant numbers:2018YFB1107403); National Natural Science Foundation of China(grant numbers:U1864204,61773316,U1801262,61871470); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186332","Convolutional neural network (CNN);cross-mode message inserting (CMMI);hyperspectral image (HSI);image fusion;multispectral image (MSI);spatial–spectral reconstruction network (SSR-NET)","Image reconstruction;Tensile stress;Spatial resolution;Machine learning;Hyperspectral imaging;Image fusion","convolutional neural nets;hyperspectral imaging;image fusion;image reconstruction;image resolution;spectral analysis","cross-mode information fusion;spatial mode;spectral mode;convolutional neural network;SSR-NET;cross-mode message inserting;HR-HSI;LR-HSI;lost spatial information;spatial edge loss;lost spectral information;multispectral image fusion;low-spatial-resolution hyperspectral image;high-spatial-resolution multispectral image;HR-MSI;spatial-spectral reconstruction network","","41","","42","IEEE","3 Sep 2020","","","IEEE","IEEE Journals"
"Sentinel-2A Image Fusion Using a Machine Learning Approach","J. Wang; B. Huang; H. K. Zhang; P. Ma","Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Department of Geography and Resource Management, The Chinese University of Hong Kong, Hong Kong; Geospatial Sciences Center of Excellence and the Department of Geography and Geospatial Sciences, South Dakota State University, Brookings, USA; Institute of Space and Earth Information Science, The Chinese University of Hong Kong (CUHK), Hong Kong","IEEE Transactions on Geoscience and Remote Sensing","22 Nov 2019","2019","57","12","9589","9601","The multispectral instrument (MSI) carried by Sentinel-2A has 13 spectral bands with various spatial resolutions (i.e., four 10-m, six 20-m, and three 60-m bands). A wide range of applications requires a 10-m resolution for all spectral bands, including the 20- and 60-m bands. To achieve this requirement, previous studies used conventional pansharpening techniques, which require a simulated 10-m panchromatic (PAN) band from four 10-m bands [blue, green, red, and near infrared (NIR)]. The simulated PAN band may not have all the information from the original four bands and may have no spectral response function that overlaps the 20- or 60-m bands to be sharpened, which may degrade fusion quality. This paper presents a machine learning method that can directly use the information from multiple 10-m resolution bands for fusion. The method first learns the spectral relationship between the 20- or 60-m band to be sharpened and the selected 10-m bands degraded to 20 or 60 m using the support vector regression (SVR) model. The model is then applied to the selected 10-m bands to predict the 10-m-resolution version of the 20- or 60-m band. The image degradation process was tuned to closely match the Sentinel-2A MSI modulation transfer function (MTF). We applied our method to three data sets in Guangzhou, China, New South Wales, Australia, and St. Louis, USA, and achieved better fusion results than other commonly used pansharpening methods in terms of both visual and quantitative factors.","1558-0644","","10.1109/TGRS.2019.2927766","Research Grants Council, University Grants Committee(grant numbers:14652016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809414","Image fusion;Sentinel-2A;sharpening;support vector regression (SVR)","Image fusion;Spatial resolution;Machine learning;Support vector machines;Earth;Transforms;Distortion","image fusion;image resolution;learning (artificial intelligence);optical transfer function;regression analysis;support vector machines","image degradation process;Sentinel-2A;image fusion;spectral bands;spatial resolutions;panchromatic band;simulated PAN band;spectral response function;fusion quality;machine learning method;10-m resolution bands;spectral relationship;size 60.0 m;current 2.0 A","","12","","53","IEEE","21 Aug 2019","","","IEEE","IEEE Journals"
"Semantic Segmentation of Remote-Sensing Images Based on Multiscale Feature Fusion and Attention Refinement","X. He; Y. Zhou; J. Zhao; M. Zhang; R. Yao; B. Liu; H. Li","Engineering Research Center of Mine Digitization, Ministry of Education of the People’s Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the People’s Republic of China, Xuzhou, China; Disaster Intelligent Prevention and Control and Emergency Rescue Innovation Research Center, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; QianXuesen Laboratory of Space Technology, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","In recent years, the automatic extraction of remote-sensing image information has attracted full attention. However, the particularity of remote-sensing images and the scarcity of data sets with label information have brought new challenges to existing methods. Therefore, we develop a lightweight semantic segmentation network based onmultiscale feature fusion (MFF) and attention refinement (MFFANet). Our network relies on three crucial modules for improved performance. The multiscale attention refinement module strengthens the representation ability of feature maps extracted by the deep residual network. The MFF module aggregates the information carried by the high-level and low-level features while restoring the image resolution. Furthermore, the boundary enhancement module captures boundary details to solve the semantic ambiguity problem. We achieve 83.5% mean intersection over union (MIoU) on the Urban Semantic 3-D (US3D) data set and 69.3% MIoU on the Vaihingen data set with only 8.2M parameters.","1558-0571","","10.1109/LGRS.2021.3052557","National Natural Science Foundation of China(grant numbers:61806206,61772530,61773383); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180639,BK20201346,BK20171192); Six Talent Peaks Project in Jiangsu Province(grant numbers:2015-DZXX-010); China Postdoctoral Science Foundation(grant numbers:2018M642359); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340261","Attention;feature fusion;multiscale feature;remote-sensing images;semantic segmentation","Semantics;Image segmentation;Remote sensing;Feature extraction;Image resolution;Decoding;Training","feature extraction;geophysical image processing;image fusion;image resolution;image segmentation;remote sensing","remote-sensing image information;data sets;label information;lightweight semantic segmentation network;crucial modules;multiscale attention refinement module;feature maps;deep residual network;MFF module aggregates;low-level features;image resolution;boundary enhancement module captures boundary details;semantic ambiguity problem;multiscale feature fusion;automatic extraction;urban semantic 3-D data","","2","","24","IEEE","29 Jan 2021","","","IEEE","IEEE Journals"
"Multiview Graph Convolutional Hashing for Multisource Remote Sensing Image Retrieval","J. Gao; X. Shen; P. Fu; Z. Ji; T. Wang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","31 Dec 2021","2022","19","","1","5","Recently, hashing has been successfully applied for large-scale remote sensing image retrieval (LSRSIR) due to its advantage in terms of computation and storage. In LSRSIR, existing hashing methods mainly focus on single-source remotely sensed data. They cannot effectively fuse multisource remotely sensed data, which has a large potential for LSRSIR. To fulfill this gap, this letter proposes a novel deep hashing method, dubbed Multiview Graph Convolutional Hashing (MGCH) that can successfully fuse multisource remote sensing image. Since graph convolutional network (GCN) has been applied as an effective means that expresses and integrates relationships into features, MGCH applies a GCN to explore inherent structural similarity among multiview data, which will help to generate discriminative hash codes. An asymmetric scheme is developed that optimizes the proposed deep model in an end-to-end manner to improve training efficiency. We evaluate the proposed method by fusing two different kinds of RS images, i.e., multispectral (MUL) image and panchromatic (PAN) image. The experimental results on the dual-source RS image data set (DSRSID) show that the proposed MGCH outperforms state-of-the-art multiview hashing methods.","1558-0571","","10.1109/LGRS.2021.3093884","National Natural Science Foundation of China(grant numbers:61906091,61801222,062072241); Natural Science Foundation of Jiangsu Province, China (Youth Fund Project)(grant numbers:BK20190440,BK20180458); Fundamental Research Funds for the Central Universities(grant numbers:30921011210,30919011230); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9485127","Deep multiview hashing;graph convolutional network (GCN);multisource remote sensing (RS) image retrieval","Image retrieval;Convolutional codes;Remote sensing;Training;Fuses;Linear programming;Semantics","feature extraction;file organisation;geophysical image processing;image classification;image fusion;image representation;image retrieval;learning (artificial intelligence);remote sensing;unsupervised learning","dual-source RS image data;MGCH;state-of-the-art multiview;multisource remote sensing image retrieval;large-scale remote;LSRSIR;hashing methods;single-source;multisource remotely sensed data;deep hashing method;dubbed Multiview Graph Convolutional Hashing;graph convolutional network;GCN;multiview data;discriminative hash codes;RS images;multispectral image;panchromatic image","","1","","33","IEEE","15 Jul 2021","","","IEEE","IEEE Journals"
"Deep Learning for Fusion of APEX Hyperspectral and Full-Waveform LiDAR Remote Sensing Data for Tree Species Mapping","W. Liao; F. Van Coillie; L. Gao; L. Li; B. Zhang; J. Chanussot","Department of Telecommunications and Information Processing, IMEC–Ghent University, Ghent, Belgium; Department of Forest and Water Management, Ghent University, Ghent, Belgium; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; CNRS, Grenoble Images Speech Signals and Automatics Laboratory, University of Grenoble Alpes, Grenoble, France","IEEE Access","4 Dec 2018","2018","6","","68716","68729","Deep learning has been widely used to fuse multi-sensor data for classification. However, current deep learning architecture for multi-sensor data fusion might not always perform better than single data source, especially for the fusion of hyperspectral and light detection and ranging (LiDAR) remote sensing data for tree species mapping in complex, closed forest canopies. In this paper, we propose a new deep fusion framework to integrate the complementary information from hyperspectral and LiDAR data for tree species mapping. We also investigate the fusion of either “single-band"" or multi-band (i.e., fullwaveform) LiDAR with hyperspectral data for tree species mapping. Additionally, we provide a solution to estimate the crown size of tree species by the fusion of multi-sensor data. Experimental results on fusing real APEX hyperspectral and LiDAR data demonstrate the effectiveness of the proposed deep fusion framework. Compared to using only single data source or current deep fusion architecture, our proposed method yields improvements in overall and average classification accuracies ranging from 82.21% to 87.10% and 76.71% to 83.45%, respectively.","2169-3536","","10.1109/ACCESS.2018.2880083","National Natural Science Foundation of China(grant numbers:91638201); Fonds Wetenschappelijk Onderzoek(grant numbers:G037115N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8529194","Deep learning;remote sensing;data fusion;hyperspectral;LiDAR","Laser radar;Vegetation;Hyperspectral imaging;Feature extraction;Forestry","forestry;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);optical radar;remote sensing;remote sensing by laser beam;sensor fusion","APEX hyperspectral;full-waveform LiDAR remote sensing data;tree species mapping;multisensor data fusion;single data source;hyperspectral detection;light detection;deep fusion framework;hyperspectral data;LiDAR data;deep fusion architecture;deep learning architecture","","42","","58","OAPA","9 Nov 2018","","","IEEE","IEEE Journals"
"Rotation Equivariant Feature Image Pyramid Network for Object Detection in Optical Remote Sensing Imagery","P. Shamsolmoali; M. Zareapoor; J. Chanussot; H. Zhou; J. Yang","École de Technologie Supérieure, Université du Québec, Montreal, QC, Canada; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; School of Informatics, University of Leicester, Leicester, U.K; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","14","Detection of objects is extremely important in various aerial vision-based applications. Over the last few years, the methods based on convolution neural networks (CNNs) have made substantial progress. However, because of the large variety of object scales, densities, and arbitrary orientations, the current detectors struggle with the extraction of semantically strong features for small-scale objects by a predefined convolution kernel. To address this problem, we propose the rotation equivariant feature image pyramid network (REFIPN), an image pyramid network based on rotation equivariance convolution. The proposed model adopts single-shot detector in parallel with a lightweight image pyramid module (LIPM) to extract representative features and generate regions of interest in an optimization approach. The proposed network extracts feature in a wide range of scales and orientations by using novel convolution filters. These features are used to generate vector fields and determine the weight and angle of the highest-scoring orientation for all spatial locations on an image. By this approach, the performance for small-sized object detection is enhanced without sacrificing the performance for large-sized object detection. The performance of the proposed model is validated on two commonly used aerial benchmarks and the results show our proposed model can achieve state-of-the-art performance with satisfactory efficiency.","1558-0644","","10.1109/TGRS.2021.3112481","NSFC(grant numbers:61876107,U1803261); National Key Program of China(grant numbers:2019YFB1311503); Committee of Science and Technology, Shanghai(grant numbers:19510711200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547378","Feature pyramid network (FPN);object detection;remote sensing images (RSIs);rotation equivariant","Feature extraction;Object detection;Convolution;Detectors;Remote sensing;Proposals;Location awareness","computer vision;convolution;feature extraction;filtering theory;image classification;image fusion;image recognition;neural nets;object detection;object recognition;remote sensing;video signal processing","network extracts;small-sized object detection;large-sized object detection;rotation equivariant feature image pyramid network;optical remote sensing imagery;aerial vision-based applications;convolution neural networks;object scales;semantically strong features;small-scale objects;predefined convolution kernel;rotation equivariance convolution;lightweight image pyramid module;representative features","","9","","59","IEEE","24 Sep 2021","","","IEEE","IEEE Journals"
"Robust Hyperspectral Image Fusion With Simultaneous Guide Image Denoising via Constrained Convex Optimization","S. Takeyama; S. Ono","Department of Information and Communications Engineering, School of Engineering, Tokyo Institute of Technology, Tokyo, Japan; Department of Computer Science, School of Computing, Tokyo Institute of Technology, Tokyo, Japan","IEEE Transactions on Geoscience and Remote Sensing","8 Dec 2022","2022","60","","1","18","This article proposes a new high spatial resolution hyperspectral (HR-HS) image estimation method based on convex optimization. The method assumes a low spatial resolution HS (LR-HS) image and a guide image as observations, where both observations are contaminated by noise. Our method simultaneously estimates an HR-HS image and a noiseless guide image, so the method can utilize spatial information in a guide image even if it is contaminated by heavy noise. The proposed estimation problem adopts hybrid spatiospectral total variation as regularization and evaluates the edge similarity between HR-HS and guide images to effectively use a priori knowledge on an HR-HS image and spatial detail information in a guide image. To efficiently solve the problem, we apply a primal-dual splitting method. Experiments demonstrate the performance of our method and the advantage over several existing methods.","1558-0644","","10.1109/TGRS.2022.3224480","Japan Science and Technology Agency Precursory Research for Embryonic Science and Technology (JST PRESTO)(grant numbers:JPMJPR21C4); Japan Society for the Promotion of Science Grants-in-Aid for Scientific Research (JSPS KAKENHI)(grant numbers:22H03610,22H00512,21K21312,20H02145,18H05413); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9963729","Hyperspectral (HS) image fusion;multispectral (MS) image;pansharpening;primal-dual splitting method;total variation","Estimation;Spatial resolution;Pansharpening;Image edge detection;Optimization;Noise measurement;Image fusion","convex programming;hyperspectral imaging;image denoising;image fusion;image resolution","constrained convex optimization;high spatial resolution hyperspectral image estimation method;HR-HS image;low spatial resolution HS image;LR-HS image;noiseless guide image;primal-dual splitting method;robust hyperspectral image fusion;simultaneous guide image denoising;spatial detail information;spatial information","","","","52","IEEE","24 Nov 2022","","","IEEE","IEEE Journals"
"A Dual-Path Multihead Feature Enhancement Detector for Oriented Object Detection in Remote Sensing Images","Y. Li; Z. Li; F. Ye; Y. Li","Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, and the College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, and the College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, and the College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, and the College of Information and Communication Engineering, Harbin Engineering University, Harbin, China","IEEE Geoscience and Remote Sensing Letters","2 Dec 2022","2022","19","","1","5","Oriented object detection in remote sensing images (RSIs) has received more and more attention due to its broader applicability in natural scenes relative to horizontal bounding boxes (HBBs). The complex scenes and multiscale targets in RSIs make it often difficult for existing studies to extract key features of the targets effectively. At the same time, due to the problem of feature inconsistency in different layers, the direct fusion of these features is likely to cause feature conflicts, resulting in degradation of detection accuracy. To solve these problems, the dual-path multihead feature enhancement detector (DP-MHFE Det), which contains two novel architectures, is proposed in this letter. The dual-path rotation feature aggregation module (DP-RFAM) improves the feature extraction capability of the network for rotating objects through dual-path structure and deformable convolution (DCN). To use these features effectively, the multihead multilevel feature fusion enhancement network (MMFFENet) is proposed to guide the feature layers to learn and retain the key features they need autonomously, and then enhance their features according to the characteristics of different subtasks. Experiments conducted on two remote sensing datasets, DOTA and HRSC2016, show that DP-MHFE Det is faster than almost all detection methods compared to the state-of-the-art (SOTA) methods while showing strong competitiveness in accuracy.","1558-0571","","10.1109/LGRS.2022.3223907","Fundamental Research Funds for the Central Universities(grant numbers:3072022CF0806); National Defense Science and Technology Key Laboratory Project(grant numbers:J2322010); Heilongjiang Touyan Innovation Team Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956990","Dual-path;feature enhancement;multihead;oriented object detection;remote sensing image (RSI)","Feature extraction;Task analysis;Convolution;Object detection;Detectors;Training;Remote sensing","convolutional neural nets;feature extraction;geophysical image processing;image fusion;learning (artificial intelligence);natural scenes;object detection;remote sensing","DCN;deformable convolution;DP-MHFE Det;DP-RFAM;dual-path multihead feature enhancement detector;dual-path rotation feature aggregation module;dual-path structure;feature extraction;feature fusion;feature layers;HBBs;horizontal bounding boxes;MMFFENet;multihead multilevel feature fusion enhancement network;natural scenes;oriented object detection;remote sensing images;RSIs","","1","","21","IEEE","21 Nov 2022","","","IEEE","IEEE Journals"
"Aggregating Features From Dual Paths for Remote Sensing Image Scene Classification","D. Yu; Q. Xu; H. Guo; J. Lu; Y. Lin; X. Liu","Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China","IEEE Access","16 Feb 2022","2022","10","","16740","16755","Scene classification is an important and challenging task employed toward understanding remote sensing images. Convolutional neural networks have been widely applied in remote sensing scene classification in recent years, boosting classification accuracy. However, with improvements in resolution, the categories of remote sensing images have become ever more fine-grained. The high intraclass diversity and interclass similarity are the main characteristics that differentiate remote scene image classification from natural image classification. To extract discriminative representation from images, we propose an end-to-end feature fusion method that aggregates features from dual paths (AFDP). First, lightweight convolutional neural networks with fewer parameters and calculations are used to construct a feature extractor with dual branches. Then, in the feature fusion stage, a novel feature fusion method that integrates the concepts of bilinear pooling and feature connection is adopted to learn discriminative features from images. The AFDP method was evaluated on three public remote sensing image benchmarks. The experimental results indicate that the AFDP method outperforms current state-of-the-art methods, with advantages of simple form, strong versatility, fewer parameters, and less calculation.","2169-3536","","10.1109/ACCESS.2022.3147543","National Natural Science Foundation of China(grant numbers:41601507,42001338); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696349","Bilinear pooling;convolutional neural network;feature fusion;remote sensing image;scene classification","Feature extraction;Remote sensing;Image analysis;Convolutional neural networks;Image classification;Transfer learning;Sensors","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing","remote sensing images;differentiate remote scene image classification;natural image classification;end-to-end feature fusion method;aggregates features;dual paths;lightweight convolutional neural networks;feature extractor;feature fusion stage;bilinear pooling;feature connection;discriminative features;AFDP method;public remote sensing image benchmarks;remote sensing image scene classification;important task;remote sensing scene classification;classification accuracy","","1","","62","CCBY","28 Jan 2022","","","IEEE","IEEE Journals"
"A Dual-Path Fusion Network for Pan-Sharpening","J. Wang; Z. Shao; X. Huang; T. Lu; R. Zhang","State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Department of Geosciences, University of Arkansas, Fayetteville, AR, USA; School of Computer Science and Engineering, Wuhan Institute of Technology, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2022","2022","60","","1","14","Most existing deep learning-based pan-sharpening methods own several widely recognized issues, such as spectral distortion and insufficient spatial texture enhancement. To address these challenges in pan-sharpening, we propose a novel dual-path fusion network (DPFN). The proposed DPFN includes two major components: 1) the global subnetwork (GSN) and 2) the local subnetwork (LSN). In particular, GSN aims to search similar image blocks in panchromatic (PAN) space and multispectral (MS) space and exploits HR textural information from the PAN space and spectral information from the MS space for the fine representation of pan-sharpened MS features by employing a cross nonlocal block. Meanwhile, the proposed LSN based on a high-pass modification block (HMB) is designed to learn the high-pass information, aiming to enhance bandwise spatial information from MS images. HMB forces the fused image to obtain high-frequency details from PAN images. Moreover, to facilitate the generation of visually appealing pan-sharpened images, we propose a perceptual loss function and further optimize the model based on high-level features in the near-infrared space. Experiments demonstrate the superior performance of the proposed method quantitatively and qualitatively compared to existing state-of-the-art pan-sharpening methods. The source code is available at https://github.com/jiaming-wang/DPFN.","1558-0644","","10.1109/TGRS.2021.3090585","National Key Research and Development Program of China(grant numbers:2018YFB0505401); National Natural Science Foundation of China(grant numbers:41890820,41771452,41771454,62072350); Key Research and Development Program of Yunnan Province in China(grant numbers:2018IB023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467620","Image fusion;nonlocal network;pan-sharpening;residual enhancement","Distortion;Task analysis;Superresolution;Spatial resolution;Remote sensing;Sensors;Image reconstruction","deep learning (artificial intelligence);image fusion;image resolution;image texture","deep learning-based pan-sharpening methods;spectral distortion;DPFN;global subnetwork;GSN;local subnetwork;LSN;image blocks;panchromatic space;textural information;spectral information;pan-sharpened MS features;cross nonlocal block;high-pass modification block;high-pass information;bandwise spatial information;MS images;image fusion;high-level features;near-infrared space;dual-path fusion network;visually appealing PAN-sharpened images;spatial texture enhancement;HMB","","8","","60","IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"Feature-Fusion Segmentation Network for Landslide Detection Using High-Resolution Remote Sensing Images and Digital Elevation Model Data","X. Liu; Y. Peng; Z. Lu; W. Li; J. Yu; D. Ge; W. Xiang","Key Lab of Universal Wireless Communication, MoE, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Key Lab of Universal Wireless Communication, MoE, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Key Lab of Universal Wireless Communication, MoE, School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Department of Satellite Application Research, China Aero Geophysical Survey and Remote Sensing Center for Natural Resources, Beijing, China; Department of Satellite Application Research, China Aero Geophysical Survey and Remote Sensing Center for Natural Resources, Beijing, China; School of Computing, Engineering and Mathematical Sciences, La Trobe University, Melbourne, VIC, Australia","IEEE Transactions on Geoscience and Remote Sensing","11 Jan 2023","2023","61","","1","14","Landslide is one of the most dangerous and frequently occurred natural disasters. The semantic segmentation technique is efficient for wide area landslide identification from high-resolution remote sensing images (HRSIs). However, considerable challenges exist because the effects of sediments, vegetation, and human activities over long periods of time make visually blurred old landslides very challenging to detect based upon HRSIs. Moreover, for terrain features like slopes, aspect and altitude variations cannot be sufficiently extracted from 2-D HRSIs but can be from digital elevation model (DEM) data. Then, a feature-fusion based semantic segmentation network (FFS-Net) is proposed, which can extract texture and shape features from 2-D HRSIs and terrain features from DEM data before fusing these two distinct types of features in a higher feature layer. To segment landslides from background, a multiscale channel attention module is purposely designed to balance the low-level fine information and high-level semantic features. In the decoder, transposed convolution layer replaces original mathematical bilinear interpolation to better restore image resolution via learnable convolutional kernels, and both dropout and batch normalization (BN) are introduced to prevent over-fitting and accelerate the network convergence. Experimental results are presented to validate that the proposed FFS-Net can greatly improve the segmentation accuracy of visually blurred old landslides. Compared to U-Net and DeepLabV3+, FFS-Net can improve the mean intersection over union (mIoU) metric from 0.508 and 0.624 to 0.67, the F1 metric from 0.254 and 0.516 to 0.596, and the pixel accuracy (PA) metric from 0.874 and 0.906 to 0.92, respectively. For the detection of visually distinct landslides, FFS-NET also offers comparable detection performance, and the segmentation is improved for visually distinct landslides with similar color and texture to surroundings.","1558-0644","","10.1109/TGRS.2022.3233637","National Key Research and Development Program of China(grant numbers:2021YFC3000400); High Level Talent Team Project of the New Coast of Qingdao New District(grant numbers:RCTD-JC-2019-06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004996","Digital elevation model (DEM);feature fusion;high-resolution remote sensing image (HRSI);landslide detection;semantic segmentation;Siamese network","Feature extraction;Terrain factors;Semantics;Shape;Image color analysis;Data mining;Optical sensors","digital elevation models;feature extraction;geomorphology;geophysical image processing;image fusion;image resolution;image restoration;image segmentation;image texture;terrain mapping","2D HRSI;altitude variations;batch normalization;DeepLabV3+;DEM data;detection performance;digital elevation model data;feature layer;feature-fusion based semantic segmentation network;feature-fusion segmentation network;FFS-Net;FFS-NET;high-level semantic features;high-resolution remote sensing images;image resolution;landslide detection;low-level fine information;mean intersection over union;mIoU metric;network convergence;pixel accuracy metric;segment landslides;segmentation accuracy;shape features;terrain features;texture;transposed convolution layer;U-Net;visually blurred old landslides;visually distinct landslides;wide area landslide identification","","","","41","IEEE","2 Jan 2023","","","IEEE","IEEE Journals"
"Detecting Dim Small Target in Infrared Images via Subpixel Sampling Cuneate Network","X. He; Q. Ling; Y. Zhang; Z. Lin; S. Zhou","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","15 Jul 2022","2022","19","","1","5","Infrared dim small target (IDST) detection is regarded as a critical technology for the interpretation of space-based remote sensing images. In recent years, driven by deep learning technology and the surge of data, remarkable effects have been achieved for dim small target detection in infrared images. Nevertheless, the intrinsic feature scarcity and low signal-to-clutter ratio (SCR) characteristics pose tremendous challenges to deep learning-based detection methods. In this letter, we present a novel subpixel sampling cuneate network (SPSCNet) to detect dim small targets in infrared images. The overall model architecture is based on an end-to-end cuneate network with multiple groups of parallel high-to-low resolution subnetworks. Specifically, we design a multiscale feature reweighted fusion (MSFRF) module to effectively fuse multiscale feature maps which contain both low-level detail features and high-level semantics information. In addition, considering that the pooling operation may lose dim small targets with low SCR, we also exploit a subpixel sampling scheme to greatly retain the features of small targets. Moreover, to better test and verify the performance of the proposed method, we also develop an IDST dataset to conduct more comparative experiments. Extensive experiments on the single-frame infrared small target (SIRST) and IDST datasets illustrate that the proposed SPSCNet yields state-of-the-art performance in comparison with other detection algorithms.","1558-0571","","10.1109/LGRS.2022.3189225","National Natural Science Foundation of China(grant numbers:61605242,61602499,61471371); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817112","Cuneate network;dim small target detection;infrared images;multiscale feature fusion;subpixel sampling","Feature extraction;Semantics;Convolution;Training;Object detection;Task analysis;Geoscience and remote sensing","feature extraction;geophysical image processing;image fusion;image resolution;infrared imaging;learning (artificial intelligence);object detection;remote sensing;text analysis","low SCR;subpixel sampling scheme;detection algorithms;infrared images;dim small target detection;critical technology;space-based remote sensing images;deep learning technology;remarkable effects;intrinsic feature scarcity;low signal-to-clutter ratio;deep learning-based detection methods;novel subpixel sampling cuneate network;end-to-end cuneate network;high-to-low resolution subnetworks;multiscale feature reweighted fusion module;multiscale feature maps;high-level semantics information","","1","","21","IEEE","7 Jul 2022","","","IEEE","IEEE Journals"
"Iterative Classifiers Combination Model for Change Detection in Remote Sensing Imagery","R. Hedjam; M. Kalacska; M. Mignotte; H. Ziaei Nafchi; M. Cheriet","Department of Geography, McGill University, Montréal, QC, Canada; Department of Geography, McGill University, Montréal, QC, Canada; Department of Computer Science and Operations Research (DIRO), University of Montréal, Montréal, QC, Canada; École de Technologie Supérieure, Montréal, QC, Canada; École de Technologie Supérieure, Montréal, QC, Canada","IEEE Transactions on Geoscience and Remote Sensing","30 Sep 2016","2016","54","12","6997","7008","In this paper, we propose a new unsupervised change detection method designed to analyze multispectral remotely sensed image pairs. It is formulated as a segmentation problem to discriminate the changed class from the unchanged class in the difference images. The proposed method is in the category of the committee machine learning model that utilizes an ensemble of classifiers (i.e., the set of segmentation results obtained by several thresholding methods) with a dynamic structure type. More specifically, in order to obtain the final “change/no-change” output, the responses of several classifiers are combined by means of a mechanism that involves the input data (the difference image) under an iterative Bayesian-Markovian framework. The proposed method is evaluated and compared to previously published results using satellite imagery.","1558-0644","","10.1109/TGRS.2016.2593982","Natural Sciences and Engineering Research Council of Canada; Fonds de Recherche du Quebec - Nature et technologies (FRQNT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7548313","Change detection;classifiers combination;image fusion;multispectral multitemporal image analysis;remote sensing imagery","Image segmentation;Remote sensing;Estimation;Training;Synthetic aperture radar;Iterative methods;Indexes","geophysical image processing;image classification;image segmentation;remote sensing","iterative classifiers combination model;change detection;remote sensing imagery;unsupervised change detection method;multispectral remotely sensed image pairs;segmentation problem difference images;committee machine learning model;final change-no-change output;iterative Bayesian-Markovian framework;satellite imagery","","16","","51","IEEE","24 Aug 2016","","","IEEE","IEEE Journals"
"Assessment of Surface Water Quality by Using Satellite Images Fusion Based on PCA Method in the Lake Gala, Turkey","E. Batur; D. Maktav","Satellite Communication and Remote Sensing Program, Informatics Institute, İstanbul Technical University, İstanbul, Turkey; Faculty of Civil Engineering, İstanbul Technical University, İstanbul, Turkey","IEEE Transactions on Geoscience and Remote Sensing","23 Apr 2019","2019","57","5","2983","2989","Monitoring water quality with classical methods is not an easy task. Remote sensing with wide coverage and multiple temporal monitoring is the best solution for surface water quality monitoring. This paper demonstrates the determination of surface water quality parameters by using principal component analysis (PCA) data fusion and mining techniques with the aid of Landsat 8 OLI (L8 OLI), Sentinel 2A (S2A), and Göktürk-2 (GK2) satellite sensors. Chlorophyll-a, dissolved oxygen, total suspended solids, Secchi disk depth, total dissolved substance, and pH were the parameters selected for surface water quality analysis. High spectral resolution of L8 OLI/S2A images and the high spatial resolution of GK2 images were fused and analyzed by a suite of data mining models to provide more reliable images with both high spatial and temporal resolutions. Surface water quality parameters calculated by PCA-based response surface regression (RSR) method were compared with results obtained from multiple linear regression (MLR), artificial neural network (ANN), and support vector machines (SVMs) data mining methods. The performance of the data mining models derived using only multispectral band data and PCA fused data were quantified using four statistical indices; such as mean-square error (MSE), root MSE, mean absolute error, and coefficient of determination (R2). The analysis confirmed that the PCA-based RSR method is superior to MLR, ANN, and SVM data mining models to accurately estimate water quality parameters in lakes.","1558-0644","","10.1109/TGRS.2018.2879024","Research Fund of the Istanbul Technical University (Program Director: Prof. D. Maktav from the Department of Geodesy and Photogrammetry, Istanbul Technical University)(grant numbers:39809); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8541109","Göktürk-2 (GK2);image fusion;Landsat 8 OLI (L8 OLI);principal component analysis (PCA);remote sensing;Sentinel 2A (S2A);water quality","Lakes;Response surface methodology;Satellites;Sensors;Spatial resolution;Remote sensing;Principal component analysis","data mining;geophysical image processing;image fusion;lakes;mean square error methods;neural nets;principal component analysis;regression analysis;remote sensing;sensor fusion;support vector machines;water quality","satellite images fusion;PCA method;monitoring water quality;classical methods;multiple temporal monitoring;surface water quality monitoring;surface water quality parameters;mining techniques;Landsat 8 OLI;Sentinel 2A;Göktürk-2 satellite sensors;total dissolved substance;surface water quality analysis;high spectral resolution;L8 OLI/S2A;high spatial resolution;GK2 images;high spatial resolutions;temporal resolutions;PCA-based response surface regression method;artificial neural network;data mining methods;PCA-based RSR method;SVM data mining models;current 2.0 A","","35","","40","IEEE","20 Nov 2018","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion Via Self-Supervised Loss and Separable Loss","H. Gao; S. Li; R. Dian","Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; School of Robotics, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","23 Sep 2022","2022","60","","1","12","Fusion of hyperspectral images (HSIs) with low-spatial and high-spectral resolution and multispectral images (MSIs) with high-spatial and low-spectral resolution is an important method to improve spatial resolution. The existing deep-learning-based image fusion technologies usually neglect the ability of neural networks to understand differential features. In addition, the loss constraints do not stem from the physical characteristics of the hyperspectral (HS) imaging sensors. We propose the self-supervised loss and the spatially and spectrally separable loss, respectively: 1) the self-supervised loss: different from the previous way of directly stacking the upsampled HSIs and MSIs as input, we expect the potentially processed HSIs to ensure not only the integrity of HSI information but also the most reasonable balance between overall spatial and spectral features. First, the preinterpolated HSIs are decomposed into subspaces as self-supervised labels. Then, a network is designed to learn subspace information and obtain the most discriminative features and 2) the separable loss: according to the physical characteristics of HSIs, the pixel-based mean square error loss is first divided into the domain loss and spectral domain loss, and then the similarity score of the images is calculated and used to construct the weighting coefficients of the two domain losses. Finally, the separable loss is jointly expressed by the weights. Experiments on public benchmark datasets indicate that the self-supervised loss and separable loss can improve the fusion performance.","1558-0644","","10.1109/TGRS.2022.3204769","National Key Research Development Program of China(grant numbers:2021YFA0715203); Major Program of the National Natural Science Foundation of China(grant numbers:61890962); Fund of Hunan Province for Science and Technology Plan Project(grant numbers:2017RS3024); Fund of Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province(grant numbers:2018TP1013); National Natural Science Foundation of China(grant numbers:62201205); Natural Science Foundation of Changsha(grant numbers:kq2202170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893568","Discriminative samples’ generation;hyperspectral (HS) and multispectral image (MSI) fusion;self-supervised loss;separable loss","Feature extraction;Hyperspectral imaging;Image fusion;Spatial resolution;Image restoration;Principal component analysis;Optical losses","deep learning (artificial intelligence);feature extraction;hyperspectral imaging;image fusion;image resolution;image sensors;mean square error methods","deep learning;loss constraints;hyperspectral imaging sensors;self-supervised loss;spectrally separable loss;HSI;pixel-based mean square error loss;hyperspectral image fusion;high-spectral resolution;multispectral images;low-spectral resolution;spatial resolution;image fusion technologies","","1","","55","IEEE","15 Sep 2022","","","IEEE","IEEE Journals"
"A Knowledge Optimization-Driven Network With Normalizer-Free Group ResNet Prior for Remote Sensing Image Pan-Sharpening","J. He; Q. Yuan; J. Li; L. Zhang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","8 Jul 2022","2022","60","","1","16","Multispectral (MS) images play a crucial role in environmental monitoring or ecological analysis for their large scope, quick acquisition, and big data. With the rapid development of technology and increasing demand, very high-resolution MS images have attracted a lot of attention these days. However, due to sensor equipment and the imaging environment, the spatial resolution of MS images is always restricted. With the help of panchromatic images, pan-sharpening is a very important technique to enhance the spatial details of MS images. In this study, we proposed a knowledge optimization-driven pan-sharpening network with normalizer-free group ResNet prior, called PNXnet, which is unfolded from a physical knowledge optimization-driven variational model. We solved the memory overhead brought by the traditional ResNet relying on batch normalization. Results on four sensors show that high quantitative indexes and natural visual effects have verified the reliability of PNXnet. Focusing on the near-infrared (NIR) band where spatial details are hard to be injected, we compared the normalized difference vegetation index (NDVI) generated from the fused results, the estimated NDVI shows a high consistency to the ground truth with  $R^{2}$  above 0.91. Besides, we also compared the model generation. Furthermore, low model complexity and quicker computational speed make the daily application of PNXnet possible.","1558-0644","","10.1109/TGRS.2022.3186916","National Natural Science Foundation of China(grant numbers:41922008,62071341,61971319); Hubei Science Foundation for Distinguished Young Scholars(grant numbers:2020CFA051); Fundamental Research Funds for the Central Universities(grant numbers:531118010209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810012","Deep learning;knowledge unfolding;normalization;pan-sharpening;satellite imagery","Satellites;Spatial resolution;Remote sensing;Deep learning;Computational modeling;Urban areas;Training","Big Data;ecology;environmental monitoring (geophysics);geophysical image processing;image fusion;image resolution;optimisation;remote sensing;variational techniques","ecological analysis;sensor equipment;imaging environment;spatial resolution;panchromatic images;spatial details;knowledge optimization-driven pan-sharpening network;normalizer-free group ResNet prior;physical knowledge optimization-driven variational model;batch normalization;remote sensing image pan-sharpening;multispectral images;environmental monitoring;Big Data;very high-resolution MS images;PNXnet;memory overhead;natural visual effects;near-infrared band;NIR band","","","","76","IEEE","28 Jun 2022","","","IEEE","IEEE Journals"
"Deep Feature Fusion for VHR Remote Sensing Scene Classification","S. Chaib; H. Liu; Y. Gu; H. Yao","School of Computer Science, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Computer Science, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","20 Jul 2017","2017","55","8","4775","4784","The rapid development of remote sensing technology allows us to get images with high and very high resolution (VHR). VHR imagery scene classification has become an important and challenging problem. In this paper, we introduce a framework for VHR scene understanding. First, the pretrained visual geometry group network (VGG-Net) model is proposed as deep feature extractors to extract informative features from the original VHR images. Second, we select the fully connected layers constructed by VGG-Net in which each layer is regarded as separated feature descriptors. And then we combine between them to construct final representation of the VHR image scenes. Third, discriminant correlation analysis (DCA) is adopted as feature fusion strategy to further refine the original features extracting from VGG-Net, which allows a more efficient fusion approach with small cost than the traditional feature fusion strategies. We apply our approach to three challenging data sets: 1) UC MERCED data set that contains 21 different areal scene categories with submeter resolution; 2) WHU-RS data set that contains 19 challenging scene categories with various resolutions; and 3) the Aerial Image data set that has a number of 10 000 images within 30 challenging scene categories with various resolutions. The experimental results demonstrate that our proposed method outperforms the state-of-the-art approaches. Using feature fusion technique achieves a higher accuracy than solely using the raw deep features. Moreover, the proposed method based on DCA fusion produces good informative features to describe the images scene with much lower dimension.","1558-0644","","10.1109/TGRS.2017.2700322","Natural Science Foundation of China(grant numbers:61371180,61522107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934005","Discriminant correlation analysis (DCA);features fusion;scene classification;unsupervised features learning","Feature extraction;Remote sensing;Image resolution;Visualization;Correlation;Principal component analysis;Machine learning","feature extraction;geophysical image processing;image classification;image fusion;remote sensing","deep feature fusion;VHR remote sensing scene classification;remote sensing technology;very high resolution imagery scene classification;VHR imagery scene classification;visual geometry group network;VGG-Net model;feature extractor;VHR image scenes;discriminant correlation analysis;feature fusion strategy;UC MERCED dataset;Aerial Image dataset;feature fusion technique","","315","","46","IEEE","25 May 2017","","","IEEE","IEEE Journals"
"CTMFNet: CNN and Transformer Multiscale Fusion Network of Remote Sensing Urban Scene Imagery","P. Song; J. Li; Z. An; H. Fan; L. Fan","School of Information and Electronic Engineering, Institute of Network Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Institute of Network Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Institute of Network Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Institute of Network Technology, Shandong Technology and Business University, Yantai, China; School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan, China","IEEE Transactions on Geoscience and Remote Sensing","4 Jan 2023","2023","61","","1","14","Semantic segmentation of remotely sensed urban scene images is widely demanded in areas such as land cover mapping, urban change detection, and environmental protection. With the development of deep learning, methods based on convolutional neural networks (CNNs) have been dominant due to their powerful ability to represent hierarchical feature information. However, the limitations of the convolution operation itself limit the network’s ability to extract global contextual information. With the successful use of transformer in computer vision in recent years, transformer has shown great potential for modeling global contextual information. However, transformer is not sufficiently capable of capturing local detailed information. In this article, to explore the potential of the joint CNN and transformer mechanism for semantic segmentation of remotely sensed urban scenes, we propose a CNN and transformer multiscale fusion network (CTMFNet) based on encoding–decoding for urban scene understanding. To couple local–global context information more efficiently, we designed a dual backbone attention fusion module (DAFM) to couple the local and global context information of the dual-branch encoder. In addition, to bridge the semantic gap between scales, we built a multi-layer dense connectivity network (MDCN) as our decoder. The MDCN enables the full flow of semantic information between multiple scales to be fused with each other through upsampling and residual connectivity. We conducted extensive subjective and objective comparison experiments and ablation experiments on both the International Society of Photogrammetry and Remote Sensing (ISPRS) Vaihingen and ISPRS Potsdam datasets. Numerous experimental results have proven the superiority of our method compared to currently popular methods.","1558-0644","","10.1109/TGRS.2022.3232143","National Natural Science Foundation of China(grant numbers:61772319,62002200,62202268,12001327); Shandong Natural Science Foundation of China(grant numbers:ZR2021MF068,ZR2021QF134); Shandong Provincial Science and Technology Support Program of Youth Innovation Team in Colleges(grant numbers:2021KJ069,2019KJN042); Yantai Science and Technology Innovation Development Plan(grant numbers:2022JCYJ031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999245","Convolutional neural network (CNN);multiscale fusion;semantic segmentation;transformer","Feature extraction;Transformers;Semantics;Remote sensing;Decoding;Semantic segmentation;Convolutional neural networks","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image fusion;image representation;image segmentation;remote sensing","computer vision;convolutional neural networks;couple local-global context information;CTMFNet;DAFM;deep learning;dual backbone attention fusion module;global contextual information;hierarchical feature information;International Society of Photogrammetry and Remote Sensing Vaihingen;ISPRS Vaihingen;joint CNN;land cover mapping;local context information;local detailed information;MDCN;multilayer dense connectivity network;remote sensing urban scene imagery;semantic gap;semantic information;semantic segmentation;transformer mechanism;transformer multiscale fusion network;urban change detection;urban scene understanding","","","","38","IEEE","26 Dec 2022","","","IEEE","IEEE Journals"
"A Blind Full-Resolution Quality Evaluation Method for Pansharpening","X. Meng; K. Bao; J. Shu; B. Zhou; F. Shao; W. Sun; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Ningbo Institute of Surveying and Mapping & Remote Sensing, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2022","2022","60","","1","16","Pansharpening methods have been developed for nearly 40 years; however, how to quantitatively evaluate the quality of pansharpened images at full resolution (FR) is probably the most debated topic in this field due to the inherent unavailable of the real HR MS reference image. In this article, a novel blind FR quality evaluation method for pansharpening is proposed. In the proposed method, spatial and spectral features that are sensitive to spatial and spectral distortions of fused images are comprehensively considered and jointly learned based on online multivariate Gaussian (MVG) to construct the evaluation model. It directly outputs the quality of fused images, rather than the stepwise evaluation of spectral score, spatial score, and final overall quality score by the weighted combination of them, which may introduce contradictory results. First, a pristine benchmark evaluation model is established on the spatial features from the original high-spatial-resolution (HR) panchromatic (PAN) image and the spectral invariant assumption between ideal fused and original multispectral (MS) images. Second, a testing evaluation model for the fused image is founded. Finally, the quality of the fused image is measured based on the distance between the testing and benchmark models. The experimental results demonstrated the superior performance of the proposed method. Furthermore, the proposed method can be generalized to other interesting tasks, such as the nonreference evaluation for pansharpening with missing information and the nonreference evaluation for hyperspectral image fusion. The source code is available on https://github.com/yyxhpkq/MQNR.","1558-0644","","10.1109/TGRS.2021.3087708","National Natural Science Foundation of China(grant numbers:41801252,62071261); fellowship of the China Postdoctoral Science Foundation(grant numbers:2020M672490); Natural Science Foundation of Ningbo City(grant numbers:2019A610098); K. C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9461233","Full resolution (FR);image fusion;multivariate Gaussian (MVG);nonreference quality evaluation;pansharpening","Pansharpening;Spatial resolution;Indexes;Remote sensing;Protocols;Sensors;Satellites","feature extraction;Gaussian processes;geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;remote sensing","blind full-resolution quality evaluation method;HR MS reference image;spatial features;spectral features;spatial distortions;spectral distortions;pristine benchmark evaluation model;high-spatial-resolution panchromatic image;nonreference evaluation;hyperspectral image fusion;blind FR quality evaluation method;PAN image;multispectral images;pansharpening image methods;online multivariate Gaussian;MVG","","5","","55","IEEE","21 Jun 2021","","","IEEE","IEEE Journals"
"A Locally Optimized Model for Hyperspectral and Multispectral Images Fusion","K. Ren; W. Sun; X. Meng; G. Yang; J. Peng; J. Huang","Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Hubei Key Laboratory of Applied Mathematics, Faculty of Mathematics and Statistics, Hubei University, Wuhan, China; Key Laboratory of Environment Remediation and Ecological Health, Ministry of Education, College of Natural Resources and Environmental Science, Zhejiang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","28 Feb 2022","2022","60","","1","15","The maintenance of spectral variability between subclass objects and the relationship between hyperspectral (HS) bands have been a fundamental but challenging problem for fusing low spatial resolution (LR) HS and high spatial resolution (HR) multispectral (MS) images. This article presents a locally optimized image segmentation fusion (LOISF) framework for HS super-resolution reconstruction. First, LR HS and HR MS are clustered and segmented, and the label attributes of the segmented objects are identified by the prior information. Then, a novel joint fusion model for different typical ground objects is constructed based on spectral unmixing. The fusion problem is formulated mathematically as a convex optimization of a Frobenius norm, which includes spatial, spectral, and index constraints, with an alternating-directions’ optimization featuring linearization providing the solution. Experimental results demonstrate that the proposed LOISF preserves both spatial details and texture, achieving high spectral fidelity, and yielding significantly improved image quality compared to other state-of-the-art fusion methods.","1558-0644","","10.1109/TGRS.2021.3133670","National Natural Science Foundation of China(grant numbers:42122009,42171351,41971296,61871177,42171326,41801256,41801252); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR19D010001,LQ18D010001); Hubei Provincial Natural Science Foundation of China(grant numbers:2021CFA087); Public Projects of Ningbo City(grant numbers:2021S089); Fundamental Research Funds for the Provincial Universities of Zhejiang(grant numbers:SJLZ2022002); Science and Technology Project for the Department of Natural Resources of Zhejiang Province(grant numbers:2021-30,2021-31); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9641801","Hyperspectral (HS);image fusion;locally optimized image segmentation fusion (LOISF);multispectral;unmixing","Spatial resolution;Matrix decomposition;Image segmentation;Indexes;Sparse matrices;Learning systems;Image fusion","convex programming;hyperspectral imaging;image fusion;image reconstruction;image resolution;image segmentation;image texture;linearisation techniques;pattern clustering;spectral analysis","locally optimized model;spectral variability;subclass objects;hyperspectral band;low spatial resolution image;high spatial resolution multispectral images;locally optimized image segmentation fusion framework;LOISF;object segmentation;joint fusion model;ground objects;spectral unmixing;convex optimization;spatial constraints;spectral constraint;alternating-directions optimization;spatial details;image texture;spectral fidelity;image quality;index constraint;hyperspectral superresolution reconstruction;hyperspectral image;image fusion;label attributes;prior information;Frobenius norm;linearization","","4","","58","IEEE","7 Dec 2021","","","IEEE","IEEE Journals"
"Feature and Model Level Fusion of Pretrained CNN for Remote Sensing Scene Classification","P. Du; E. Li; J. Xia; A. Samat; X. Bai","Department of Geographical Information Science, Nanjing University, Nanjing, China; School of Geography, Geomatics and Planning, Jiangsu Normal University, Xuzhou, China; RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; State Key Laboratory of Desert and Oasis Ecology, CAS, Urumqi, China; Department of Geographical Information Science, Nanjing University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Sep 2019","2019","12","8","2600","2611","Convolutional neural networks (CNN) have attracted tremendous attention in the remote sensing community due to its excellent performance in different domains. Especially for remote sensing scene classification, the CNN-based methods have brought a great breakthrough. However, it is not feasible to fully design and train a new CNN model for remote sensing scene classification, as this usually requires a large number of training samples and high computational costs. To alleviate these limitations of fully training a new model, some work attempts to use the pretrained CNN models as feature extractors to build feature representation of scene images for classification and has achieved impressive results. In this scheme, how to construct feature representation of scene image via the pretrained CNN model becomes the key process. Existing studies paid a little attention to build more discriminative feature representation by exploring the potential benefits of multilayer features from a single CNN model and different feature representations from multiple CNN models. To this end, this paper presents a fusion strategy to build the feature representation of the scene images by integrating multilayer features of a single pretrained CNN model, and extends it to a framework of multiple CNN models. For these purposes, a multiscale improved Fisher kernel coding method is used to build feature representation of the scene images on convolutional layers, and a feature fusion approach based on two feature subspace learning methods [principal component analysis (PCA)/spectral regression kernel discriminant analysis and PCA/spectral regression kernel locality preserving projection] is proposed to construct final fused features for scene classification. For validation and comparison purposes, the proposed approaches are evaluated with two challenging high-resolution remote sensing datasets and shows the competitive performance compared with existing state-of-the-art baselines such as fully trained CNN models, fine tuning CNN models, and other related works.","2151-1535","","10.1109/JSTARS.2018.2878037","National Natural Science Foundation of China(grant numbers:41631176); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8537907","Convolutional neural networks (CNNs);feature fusion;multiscale improved Fisher kernel;scene classification;subspace learning","Remote sensing;Feature extraction;Kernel;Visualization;Task analysis;Semantics;Computational modeling","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);principal component analysis;regression analysis;remote sensing","multiple CNN models;scene image;feature fusion approach;feature subspace;fully trained CNN models;remote sensing scene classification;remote sensing community;CNN-based methods;feature extractors;discriminative feature representation;single pretrained CNN model;high-resolution remote sensing datasets;multilayer features;convolutional neural networks;scene image classification;scene images;multiscale improved Fisher kernel coding method;convolutional layers;principal component analysis;feature subspace learning methods;spectral regression kernel discriminant analysis;spectral regression kernel locality preserving projection","","32","","47","IEEE","16 Nov 2018","","","IEEE","IEEE Journals"
"A Crossmodal Multiscale Fusion Network for Semantic Segmentation of Remote Sensing Data","X. Ma; X. Zhang; M. -O. Pun","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; School of Mathematical Sciences, University of Science and Technology of China, Hefei, China; School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 May 2022","2022","15","","3463","3474","Driven by the rapid development of Earth observation sensors, semantic segmentation using multimodal fusion of remote sensing data has drawn substantial research attention in recent years. However, existing multimodal fusion methods based on convolutional neural networks cannot capture long-range dependencies across multiscale feature maps of remote sensing data in different modalities. To circumvent this problem, this work proposes a crossmodal multiscale fusion network (CMFNet) by exploiting the transformer architecture. In contrast to the conventional early, late, or hybrid fusion networks, the proposed CMFNet fuses information of different modalities at multiple scales using the cross-attention mechanism. More specifically, the CMFNet utilizes a novel cross-modal attention architecture to fuse multiscale convolutional feature maps of optical remote sensing images and digital surface model data through a crossmodal multiscale transformer (CMTrans) and a multiscale context augmented transformer (MCATrans). The CMTrans can effectively model long-range dependencies across multiscale feature maps derived from multimodal data, while the MCATrans can learn discriminative integrated representations for semantic segmentation. Extensive experiments on two large-scale fine-resolution remote sensing datasets, namely ISPRS Vaihingen and Potsdam, confirm the excellent performance of the proposed CMFNet as compared to other multimodal fusion methods.","2151-1535","","10.1109/JSTARS.2022.3165005","National Natural Science Foundation of China(grant numbers:41801323); China Postdoctoral Science Foundation(grant numbers:2020M682038); Shenzhen Science and Technology Innovation Committee(grant numbers:JCYJ20190813170803617); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9749821","Combined squeeze-and-excitation (CSE);cross attention;crossmodal multiscale fusion;transformer","Transformers;Remote sensing;Semantics;Image segmentation;Feature extraction;Fuses;Decoding","convolution;feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;object detection;remote sensing;sensor fusion","crossmodal multiscale fusion network;semantic segmentation;remote sensing data;substantial research attention;multimodal fusion methods;convolutional neural networks;long-range dependencies;multiscale feature maps;conventional early fusion networks;late, fusion networks;hybrid fusion networks;CMFNet fuses information;cross-attention mechanism;novel cross-modal attention architecture;multiscale convolutional feature maps;optical remote sensing images;digital surface model data;crossmodal multiscale transformer;multiscale context;multimodal data;large-scale fine-resolution remote sensing datasets","","8","","46","CCBY","5 Apr 2022","","","IEEE","IEEE Journals"
"Spatiotemporal Data Fusion Using Temporal High-Pass Modulation and Edge Primitives","J. Malleswara Rao; C. V. Rao; A. Senthil Kumar; B. Lakshmi; V. K. Dadhwal","National Remote Sensing Center, Indian Space Research Organization, Hyderabad, India; Special Products Division, Satellite Data Acquisition and Products Services Area, National Remote Sensing Center, Indian Space Research Organization, Hyderabad, India; National Remote Sensing Center, Indian Space Research Organization, Hyderabad, India; National Remote Sensing Center, Indian Space Research Organization, Hyderabad, India; National Remote Sensing Center, Indian Space Research Organization, Hyderabad, India","IEEE Transactions on Geoscience and Remote Sensing","11 Aug 2015","2015","53","11","5853","5860","Most spaceborne sensors have a tradeoff between high spatial and high temporal resolutions. This tradeoff limits the use of remote sensing data in various applications that require images in both the high spatial and high temporal resolutions. In this paper, we propose a novel technique to create a fine spatial and high temporal resolution images at a ground-based data processing system. Resourcesat-2 is one of the Indian Space Research Organization missions, and it carries the Linear Imaging Self-Scanning sensors (LISS III and LISS IV) and an Advanced Wide-Field Sensor (AWiFS) onboard. The spatial resolution of LISS III is 23.5 m, and that of AWiFS is 56 m. The temporal resolution of LISS III is 24 days, and that of AWiFS is five days. The proposed method creates a synthetic LISS-III image at 23.5-m spatial and five-day temporal resolutions. It is based on the subpixel relationship between a single AWiFS–LISS-III image pair, which is acquired before or after the prediction date. In temporal data composition, spurious spatial discontinuities are inevitable for land-cover type changes. These discontinuities were identified with temporal edge primitives and were smoothed with a spatial-profile-averaging method. A synthetic LISS-III image for time  $t_{k}$ is predicted from an AWiFS image at time  $t_{k}$ and a single AWiFS–LISS-III image pair at time  $t_{0}$, where  $t_{0}\ne t_{k}$. Experimental results demonstrated that the proposed method is superior in terms of the computational efficiency and prediction accuracy with the other existing methods.","1558-0644","","10.1109/TGRS.2015.2422712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119583","Advanced Wide-Field Sensor (AWiFS) and Linear Imaging Self-Scanning (LISS-III) sensor data;Resourcesat-2;spatiotemporal data fusion;Advanced Wide-Field Sensor (AWiFS) and Linear Imaging Self-Scanning (LISS-III) sensor data;Resourcesat-2;spatiotemporal data fusion","Spatial resolution;Image edge detection;Sensors;Remote sensing;Satellites;Earth","edge detection;geophysical image processing;geophysical techniques;image fusion;image resolution;land cover;spatiotemporal phenomena;terrain mapping","spatiotemporal data fusion;temporal high-pass modulation;spaceborne sensors;tradeoff limits;remote sensing data;temporal resolutions;high temporal resolution images;fine spatial resolution images;ground-based data processing system;Resourcesat-2;Indian Space Research Organization missions;spatial-profile-averaging method;temporal edge primitives;prediction accuracy;computational efficiency;land-cover type changes;subpixel relationship;single AWiFS-LISS-III image pair;synthetic LISS-III image;advanced wide-field sensor;AWiFS;linear imaging self-scanning sensors","","4","2","29","IEEE","8 Jun 2015","","","IEEE","IEEE Journals"
"A Spatial and Temporal Nonlocal Filter-Based Data Fusion Method","Q. Cheng; H. Liu; H. Shen; P. Wu; L. Zhang","School of Urban Design, Wuhan University, Wuhan, China; Surveying and Mapping Institute Lands and Resource Department of Guangdong Province, Guangzhou, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Resources and Environmental Engineering, Anhui University, Hefei, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","20 Jul 2017","2017","55","8","4476","4488","The tradeoff in remote sensing instruments that balances the spatial resolution and temporal frequency limits our capacity to monitor spatial and temporal dynamics effectively. The spatiotemporal data fusion technique is considered as a cost-effective way to obtain remote sensing data with both high spatial resolution and high temporal frequency, by blending observations from multiple sensors with different advantages or characteristics. In this paper, we develop the spatial and temporal nonlocal filter-based fusion model (STNLFFM) to enhance the prediction capacity and accuracy, especially for complex changed landscapes. The STNLFFM method provides a new transformation relationship between the fine-resolution reflectance images acquired from the same sensor at different dates with the help of coarse-resolution reflectance data, and makes full use of the high degree of spatiotemporal redundancy in the remote sensing image sequence to produce the final prediction. The proposed method was tested over both the Coleambally Irrigation Area study site and the Lower Gwydir Catchment study site. The results show that the proposed method can provide a more accurate and robust prediction, especially for heterogeneous landscapes and temporally dynamic areas.","1558-0644","","10.1109/TGRS.2017.2692802","National Natural Science Foundation of China(grant numbers:41422108,41601357); Natural Science Foundation of Hubei Province(grant numbers:2016CFB333); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917259","Data fusion;nonlocal;reflectance prediction;similarity information;spatiotemporal","Spatial resolution;Remote sensing;Spatiotemporal phenomena;Sensors;Monitoring;Data integration","geophysical image processing;image filtering;image fusion;remote sensing","data fusion method;remote sensing instrument;spatiotemporal data fusion technique;remote sensing data;spatiotemporal nonlocal filter-based fusion model;complex changed landscape;fine-resolution reflectance image;coarse-resolution reflectance data;remote sensing image sequence;Coleambally irrigation area;New South Wales;lower Gwydir catchment;heterogeneous landscape","","72","","42","IEEE","2 May 2017","","","IEEE","IEEE Journals"
"A Specially Optimized One-Stage Network for Object Detection in Remote Sensing Images","H. Qin; Y. Li; J. Lei; W. Xie; Z. Wang","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","24 Feb 2021","2021","18","3","401","405","With great significance in military and civilian applications, detecting indistinguishable small objects in wide-scale remote sensing images is still a challenging topic. In this letter, we propose a specially optimized one-stage network (SOON) focusing on extracting spatial information of high-resolution images by understanding and analyzing the combination of feature and semantic information of small objects. The SOON model consists of feature enhancement, multiscale detection, and feature fusion. The first part is implemented by constructing a receptive field enhancement (RFE) module and incorporating it into the network's specific parts where the information of small objects mainly exists. The second part is achieved by four detectors with different sensitivities, which access to the fused and enhanced features to enable the network to make full use of features in different scales. The third part consolidates the high-level and low-level features by adopting upsampling, concatenation, and convolution operations to build a feature pyramid structure, which explicitly yields strong feature representation and semantic information. In addition, we introduce the soft-nonmaximum suppression to preserve accurate bounding boxes in the postprocessing stage for densely arranged objects. Note that the split and merge strategy and the multiscale training strategy are employed. Extensive experiments and thorough analysis are performed on the NorthWestern Polytechnical University Very-High-Resolution (NWPU VHR)-10-v2 data set and the airplane, car and ship (ACS) data set as compared with several state-of-the-art methods. The satisfactory performance in experiments verifies the effectiveness of the design and optimization.","1558-0571","","10.1109/LGRS.2020.2975086","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9045965","Feature enhancement;feature fusion;multiscale detection;object detection;remote sensing images","Feature extraction;Detectors;Remote sensing;Object detection;Semantics;Convolution;Neural networks","edge detection;feature extraction;geophysical image processing;image classification;image fusion;image representation;image resolution;image segmentation;object detection;remote sensing","specially optimized one-stage network;object detection;military applications;civilian applications;wide-scale remote sensing images;spatial information;high-resolution images;semantic information;feature enhancement;multiscale detection;receptive field enhancement module;specific parts;fused enhanced features;low-level features;feature pyramid structure;strong feature representation;postprocessing stage;densely arranged objects;NorthWestern Polytechnical University Very-High-Resolution-10-v2 data;optimization","","12","","20","IEEE","24 Mar 2020","","","IEEE","IEEE Journals"
"A Segmentation Method for High Spatial Resolution Remote Sensing Images Based on the Fusion of Multifeatures","D. Liu; L. Han; X. Ning; Y. Zhu","Department of Information Engineering, Engineering University of People’s Armed Police, Xi’an, China; School of Geology Engineering and Geomatics, Chang’an University, Xi’an, China; School of Geology Engineering and Geomatics, Chang’an University, Xi’an, China; Department of Information Engineering, Engineering University of People’s Armed Police, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","30 Jul 2018","2018","15","8","1274","1278","A novel method based on the fusion of spectral, texture, and shape features is proposed for the segmentation of high spatial resolution remote sensing images. The method uses the region merging idea to get the final segmentation result on the basis of initial segmentation. Texture features of the regions are obtained by the nonsubsampled contourlet transform. An integrated region merging criterion is built by combining the texture, spectral, and shape features. To ensure the efficiency of the method, the region adjacency graph and the nearest neighbor graph are used to maintain the adjacency relations in the region merging stage. The global optimization strategy is adopted to realize the image segmentation gradually. Two experiments are performed to verify the effectiveness of the proposed method. One experiment validates the influence of the merging criteria with different feature combinations on the segmentation results. Another experiment compares the effect of the method with the segmentation methods embedded in ENVI and eCognition. Experimental results show that the method can make full use of the features of the images to achieve accurate and efficient segmentations.","1558-0571","","10.1109/LGRS.2018.2829807","National Natural Science Foundation of China(grant numbers:61771490); Fundamental Research Funds for the Central Universities of China(grant numbers:310850173701); Fundamental Research Funds of Engineering University of People’s Armed Police(grant numbers:WJY201607); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8372947","Remote sensing image;region adjacency graph (RAG);region merging;segmentation;texture","Merging;Image segmentation;Shape;Spatial resolution;Remote sensing;Image edge detection;Transforms","geophysical image processing;graph theory;image fusion;image segmentation;image texture;remote sensing;transforms","spectral texture;shape features;final segmentation result;initial segmentation;texture features;integrated region merging criterion;region adjacency graph;region merging stage;image segmentation;segmentation results;accurate segmentations;high-spatial resolution remote sensing images","","7","","15","IEEE","5 Jun 2018","","","IEEE","IEEE Journals"
"Multibranch Spatial-Channel Attention for Semantic Labeling of Very High-Resolution Remote Sensing Images","B. Han; J. Yin; X. Luo; X. Jia","School of Astronautics, Beihang University, Beijing, China; School of Astronautics, Beihang University, Beijing, China; School of Astronautics, Beihang University, Beijing, China; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia","IEEE Geoscience and Remote Sensing Letters","23 Nov 2021","2021","18","12","2167","2171","Very high-resolution (VHR) remote sensing images can provide fine but sometimes trivial ground object details; thus, the semantic labeling of VHR images is a challenging task. To improve the VHR labeling performance, spatial multiscale information and channel attention have been employed recently. However, the exploitation of global object features is still limited, which leads to the loss of capturing within-class variation from location to location. In this letter, we present a multibranch spatial-channel attention (MSCA) model to efficiently extract global dependency and combine it with multiscale and channel attention methods. In the spatial multiscale attention block, a multibranch feature fusion model is established to exploit the global relationship captured by self-attention and the multiscale correlation learned from dilated convolutions. To alleviate the computational cost of pixel-by-pixel self-attention operation, a spatial pyramid compressing method is also designed. In the channel attention block, average and max global pooling strategies are applied, respectively, in two channel attention branches to generalize global information from different perspectives. Those two blocks are then adaptively united by learnable weighting parameters. Experiments on two VHR image data sets demonstrate that the proposed network can yield better performance in comparison with state-of-the-art labeling methods tested.","1558-0571","","10.1109/LGRS.2020.3013253","National Natural Science Foundation of China(grant numbers:41871240); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165842","Attention mechanism;convolutional neural networks (CNNs);remote sensing (RS);semantic labeling;very high-resolution (VHR) images","Labeling;Semantics;Feature extraction;Computational modeling;Computational efficiency;Remote sensing;Spatial resolution","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);object detection;remote sensing","semantic labeling;high-resolution remote sensing images;ground object details;VHR images;VHR labeling performance;spatial multiscale information;global object features;multibranch spatial-channel attention model;global dependency;attention methods;spatial multiscale attention block;multibranch feature fusion model;global relationship;multiscale correlation;pixel-by-pixel self-attention operation;spatial pyramid compressing method;channel attention block;average pooling strategies;max global pooling strategies;channel attention branches;global information;VHR image data sets;state-of-the-art labeling methods","","4","","23","IEEE","12 Aug 2020","","","IEEE","IEEE Journals"
"Dual-Path Sparse Hierarchical Network for Semantic Segmentation of Remote Sensing Images","Y. Wang; H. Shi; S. Dong; Y. Zhuang; L. Chen","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Semantic segmentation of remote sensing images aims to label every pixel with the correct semantic category. The core challenge of the current deep convolutional network (ConvNet)-based methods lies in the difficulty of effectively aggregating high-level categorical semantics and low-level local details along the hierarchy of backbone. Most current approaches consider only fusing adjacent feature layers gradually with short-range feature connections, which lack the diversity of feature interactions, such as long-range cross-scale connections. To this end, we propose a novel dual-path sparse hierarchical network that is characterized by rich cross-scale feature interactions. Multiscale features are first sparsely grouped with a predefined interval, which is then aggregated via both long-range and short-range cross-scale connections in a hierarchical manner. Moreover, in order to further enrich the diversity of feature interactions, we also introduce another fusion path in parallel but with different sparsity for feature grouping, forming a dual-path network. In this way, our model is able to effectively aggregate multilevel features by incorporating both long-range and short-range feature interactions in both parallel and hierarchical manner. Meanwhile, the semantic and resolution gap between multilevel features can also be bridged.","1558-0571","","10.1109/LGRS.2021.3070426","China Postdoctoral Science Foundation(grant numbers:2020M670162); Chang Jiang Scholars Program(grant numbers:T2012122); Hundred Leading Talent Project of Beijing Science and Technology(grant numbers:Z141101001514005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9405375","Deep learning;remote sensing image understanding;semantic segmentation","Semantics;Image segmentation;Remote sensing;Spatial resolution;Location awareness;Feature extraction;Aggregates","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;neural nets;remote sensing","multiscale features;hierarchical manner;fusion path;feature grouping;dual-path network;multilevel features;semantic resolution gap;semantic segmentation;remote sensing images;correct semantic category;core challenge;current deep convolutional network-based methods;high-level categorical semantics;low-level local details;adjacent feature layers;long-range cross-scale connections;novel dual-path sparse hierarchical network;rich cross-scale feature interactions","","","","24","IEEE","15 Apr 2021","","","IEEE","IEEE Journals"
"A Pseudo-Siamese Deep Convolutional Neural Network for Spatiotemporal Satellite Image Fusion","W. Li; C. Yang; Y. Peng; J. Du","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Jan 2022","2022","15","","1205","1220","Due to technology and cost limitations, it is challenging to obtain high temporal and spatial resolution images from a single satellite spectrometer, which significantly limits the specific application of such remote sensing images in earth science. To solve the problem that the existing algorithms cannot effectively balance the spatial detail preservation and spectral change reconstruction, a pseudo-Siamese deep convolutional neural network (PDCNN) for spatiotemporal fusion is proposed in this article. The method proposes a pseudo-Siamese network framework model for fusion. This framework has two independent and equal feature extraction streams, but the weights are not shared. The two feature extraction streams process the image information at the previous and later moments and reconstruct the fine image of the corresponding time to fully extract the image information at different times. In the feature extraction stream, the multiscale mechanism and dilated convolution of flexible perception are designed, which can flexibly obtain feature image information and improve the model reconstruction accuracy. In addition, an attention mechanism is introduced to improve the weight of the crucial information for the remote sensing images. Adding a residual connection enhances the reuse of the initial feature information in shallow networks and reduces the loss of feature information in deep networks. Finally, the fine images obtained from the two feature extraction streams are weighted and fused to obtain the final predicted image. The subjective and objective results demonstrate that the PDCNN can effectively reconstruct the fusion image with higher quality.","2151-1535","","10.1109/JSTARS.2022.3143464","National Natural Science Foundation of China(grant numbers:61972060,U1713213,61802148,62027827); National Key Research and Development Program of China(grant numbers:2019YFE0110800); Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0025,cstc2019cxcyljrc-td0270); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684706","Convolutional neural network (CNN);multiscale mechanism;pseudo-Siamese network;spatiotemporal fusion","Feature extraction;Streaming media;Remote sensing;Spatial resolution;Image reconstruction;Convolutional neural networks;Spatiotemporal phenomena","feature extraction;geophysical image processing;image classification;image fusion;image reconstruction;image representation;image resolution;learning (artificial intelligence);neural nets;remote sensing","fusion image;pseudoSiamese deep convolutional neural network;spatiotemporal satellite image fusion;cost limitations;high temporal resolution images;spatial resolution images;single satellite spectrometer;remote sensing images;spectral change reconstruction;spatiotemporal fusion;pseudoSiamese network framework model;independent feature extraction streams;equal feature extraction streams;feature extraction stream;fine image;feature image information;model reconstruction accuracy;initial feature information;shallow networks;deep networks;final predicted image","","3","","55","CCBY","18 Jan 2022","","","IEEE","IEEE Journals"
"Rethinking Few-Shot Remote Sensing Scene Classification: A Good Embedding Is All You Need?","L. Xing; Y. Ma; W. Cao; S. Shao; W. Liu; B. Liu","Yunnan Key Laboratory of Media Convergence, Qingdao Zhongchuang Huizhi Information Technology Company Ltd., Qingdao, China; College of Oceanography and Space Informatics, First Institute of Oceanography, China University of Petroleum (East China), Ministry of Natural Resources, Qingdao, China; Chinese Academy of Sciences, Aerospace Information Research Institute, Institute of Aerospace Information Applications, University of Macau, Beijing, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China","IEEE Geoscience and Remote Sensing Letters","8 Sep 2022","2022","19","","1","5","In recent years, few-shot remote sensing scene classification (FSRSSC) has attracted more and more attention. For FSRSSC, most methods currently focus on designing a meta-learning algorithm, which obtains meta-knowledge from limited samples and then applies it to novel tasks. In this work, on one hand, we optimize the training pipeline of the feature extractor; on the other hand, we apply a novel model fusion method further to optimize the feature extractor capability of the feature extractor. We show a novel FSRSSC baseline: learning two feature representations through using two self-supervised methods on the meta-training set and then fusing the two representations into one. Then, training a linear classifier on this representation achieves state-of-the-art performance. It shows that training a good feature extractor can be more efficient than complex meta-learning algorithms for FSRSSC. We believe that our results can inspire a rethinking of FSRSSC benchmarks.","1558-0571","","10.1109/LGRS.2022.3198841","Fundamental Research Funds for the Central Universities, China University of Petroleum (East China)(grant numbers:20CX05001A); Natural Science Foundation of Shandong Province, China(grant numbers:ZR2019MF073); Major Scientific and Technological Projects of China National Petroleum Corporation(grant numbers:ZD2019-183-008); Graduate Innovation Project of China University of Petroleum (East China), Yunnan Key Laboratory of Media Convergence(grant numbers:YCX2021117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861614","Few-shot image classification;model fusion;remote sensing scene","Feature extraction;Remote sensing;Task analysis;Image analysis;Training;Semantics;Petroleum","feature extraction;geophysical image processing;image classification;image fusion;image representation;image sampling;optimisation;remote sensing;supervised learning","few-shot remote sensing scene classification;good embedding;meta-learning algorithm;training pipeline;model fusion method;feature extractor;feature representations;self-supervised methods;FSRSSC baseline","","","","26","IEEE","18 Aug 2022","","","IEEE","IEEE Journals"
"STransFuse: Fusing Swin Transformer and Convolutional Neural Network for Remote Sensing Image Semantic Segmentation","L. Gao; H. Liu; M. Yang; L. Chen; Y. Wan; Z. Xiao; Y. Qian","Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; College of Mathematics and Systems Science, Xinjiang University, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","10 Nov 2021","2021","14","","10990","11003","The applied research in remote sensing images has been pushed by convolutional neural network (CNN). Because of the fixed size of the perceptual field, CNN is unable to model global semantic relevance. Modeling global semantic information is possible with the self-attentive Transformer-based model. However, the method of patch computation used by Transformer for self-attentive computation ignores the spatial information inside each patch. To address these issues, we offer the STransFuse model as a new semantic segmentation method for remote sensing images. It is a model that combines the benefits of Transformer with CNN to improve the segmentation quality of various remote sensing images. We employ a staged model to extract coarse-grained and fine-grained feature representations at various semantic scales, unlike earlier techniques based on Transformer model fusion. In order to take full advantage of the features acquired at different stages, we designed an adaptive fusion module. This module adaptively fuses the semantic information between features at different scales employing a self-attentive mechanism. The overall accuracy (OA) of our proposed model on the Vaihingen dataset is 1.36% higher than the baseline, and 1.27% improvement in OA over baseline on the Potsdam dataset. When compared to other advanced models, the STransFuse model performs admirably.","2151-1535","","10.1109/JSTARS.2021.3119654","National Natural Science Foundation of China(grant numbers:61966035); National Natural Science Foundation of China(grant numbers:U1803261); Xinjiang Uygur Autonomous Region Innovation Team(grant numbers:XJEDU2017T002); Autonomous Region Graduate Innovation Project(grant numbers:XJ2021G062); Autonomous Region Graduate Innovation Project(grant numbers:XJ2020G074); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573374","Remote sensing;self-attention;semantic segmentation;Transformer","Remote sensing;Transformers;Semantics;Image segmentation;Computational modeling;Feature extraction;Context modeling","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image representation;image segmentation;remote sensing","CNN;patch computation;self-attentive computation;feature representation extraction;swin transformer;convolutional neural network;remote sensing image semantic segmentation;STransFuse model;transformer model fusion;self-attentive transformer-based model;Vaihingen dataset;Potsdam dataset;feature map information","","26","","50","CCBY","14 Oct 2021","","","IEEE","IEEE Journals"
"Learning Higher Quality Rotation Invariance Features for Multioriented Object Detection in Remote Sensing Images","C. Zhang; B. Xiong; X. Li; J. Zhang; G. Kuang","State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Jun 2021","2021","14","","5842","5853","Multioriented object detection, an important yet challenging task because of the bird's-eye-view perspective, complex background, and densely packed objects, is in the spotlight of detection in remote sensing images. Although existing methods have recently experienced substantial progress based on oriented head, they learn little about essential rotation invariance of the objects. In this article, a novel framework is proposed that can learn high-quality rotation invariance features of the multioriented objects by three measures. Given a remote sensing image, the multiscale semantic segmentation feature fusion module first merges the global semantic segmentation features predicted by the semantic segmentation branch and the multiscale features extracted by the backbone with FPN in order to distinguish complex background. Then, the discriminative features are used by rotation mainstream, whose structure is similar to cascade R-CNN and can extract higher quality rotation invariance features and predict more accurate location information by adaptively adjusting the distribution of the samples through progressive intersection over union thresholds. And in order to improve the performance of mainstream to predict more accurate oriented bounding box, the horizontal tributaries that can fully leverage the reciprocal relationship between the oriented detection and horizontal detection were added to the latter two stages. Extensive experiments on three public datasets for remote sensing images, i.e., Gaofen Airplane, HRSC2016, and DOTA demonstrate that without bells and whistles, the proposed method achieves superior performances compared with the existing state-of-the-art methods for multioriented detection. Moreover, our overall system achieves 59.264% mAP of airplane Detection in 2020 Gaofen challenge, ranking third in the final.","2151-1535","","10.1109/JSTARS.2021.3085665","National Natural Science Foundation of China(grant numbers:62001480); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444845","Cascade;multioriented object detection;remote sensing images;rotation invariance","Feature extraction;Object detection;Remote sensing;Proposals;Detectors;Semantics;Image segmentation","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image segmentation;object detection;remote sensing","multioriented object detection;remote sensing image;complex background;densely packed objects;high-quality rotation invariance features;global semantic segmentation features;multiscale feature extraction;discriminative features;rotation mainstream;horizontal detection;airplane detection;multiscale semantic segmentation feature fusion;bird-eye-view perspective;FPN;R-CNN;intersection over union thresholds;oriented bounding box;Gaofen Airplane;HRSC2016;DOTA","","8","","59","CCBY","1 Jun 2021","","","IEEE","IEEE Journals"
"Vessel Detection From Nighttime Remote Sensing Imagery Based on Deep Learning","J. Shao; Q. Yang; C. Luo; R. Li; Y. Zhou; F. Zhang","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Dec 2021","2021","14","","12536","12544","The continuous and rapid detection of sea vessels on a large scale is of great importance in marine traffic management, resource protection, and rights maintenance. Nighttime remote sensing can reflect human activities during the night with a wide swath and high efficiency, which is unique for vessel detection. Deep learning algorithms have already demonstrated superior performance in many fields, but it is confronted with some problems when applied to vessel detection with nighttime remote sensing imagery, including the lack of labeled dataset, the missed detection of small vessels, and false alarms of land targets. In this article, first, the nighttime remote sensing imagery was collected and the sea vessels in it were labeled. Second, to enhance the detection performance of small vessels, a modified YOLOv5 algorithm—TASFF-YOLOv5 was proposed, which was supplemented with a tiny target detection layer and a four-layer adaptively spatial feature fusion network to obtain a better feature fusion. Third, a land mask operation based on the sea–land prior database was performed to eliminate the false alarms of the land lights. The experimental results showed that the proposed TASFF-YOLOv5 could effectively improve the precision, recall, and mAP0.5 on the vessel dataset, achieving 95.2%, 93.1%, and 94.9% respectively.","2151-1535","","10.1109/JSTARS.2021.3125834","National Natural Science Foundation of China(grant numbers:62171016,61871413); Fundamental Research Funds for the Central Universities(grant numbers:buctrc202001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606559","Adaptively spatial feature fusion;nighttime remote sensing;sea–land mask;vessel detection;YOLOv5","Remote sensing;Feature extraction;Object detection;Deep learning;Synthetic aperture radar;Optical sensors;Optical imaging","feature extraction;geophysical image processing;image fusion;learning (artificial intelligence);object detection;remote sensing","missed detection;nighttime remote sensing imagery;sea vessels;detection performance;modified YOLOv5 algorithm-TASFF-YOLOv5;tiny target detection layer;vessel dataset;vessel detection;continuous detection;deep learning algorithms","","6","","38","CCBY","8 Nov 2021","","","IEEE","IEEE Journals"
"Remote Sensing Image Registration Based on Dynamic Threshold Calculation Strategy and Multiple-Feature Distance Fusion","W. Zhao; X. Ma; L. Liang; L. Liang; Y. Yang; K. Yang; S. H. Ong","School of Information Science and Technology, Yunnan Normal University, Kunming, China; School of Information Science and Technology, Yunnan Normal University, Kunming, China; School of Information Science and Technology, Yunnan Normal University, Kunming, China; School of Information Science and Technology, Yunnan Normal University, Kunming, China; School of Information Science and Technology, Yunnan Normal University, Kunming, China; School of Information Science and Technology, Yunnan Normal University, Kunming, China; NUS Graduate School for Integrative Sciences and Engineering, National University of Singapore, Singapore","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Nov 2019","2019","12","10","4049","4061","Remote sensing image registration is widely used in civilian and military applications such as target recognition, environmental transformation monitoring, and military damage assessment. The severe outliers in the extraction of feature points caused by nonrigid transformation and viewpoint changes in the process of capturing remote sensing images increase the difficulty of registration. Therefore, we present a remote sensing image registration method based on dynamic threshold calculation strategy (DTCS) and multiple-feature distance fusion. The main idea of our approach is to maximize inliers while ensuring the optimal correspondence. First, DTCS gradually screens reliable inliers to reduce the negative effect of outliers over the iterations. The multiple-feature distance fusion Gaussian mixture model is then introduced to compensate for the defect of a single feature, and DTCS acting as the prior probability combines with the deterministic annealing to achieve the optimal mapping from local to global scale. Moreover, structure constraint based on local applying force is added into the global constraint to control the alignment of feature points more accurately in the overlapping area, so as to guide the subsequent image transformation. Extensive experiments show that our method performs better in most cases compared with nine state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2019.2938622","National Natural Science Foundation of China(grant numbers:41661080,41971392); Yunnan Ten-thousand Talents Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8840913","Feature matching;image registration;multi-feature;outlier removal;remote sensing","Remote sensing;Feature extraction;Image registration;Reliability;Estimation;Euclidean distance;Periodic structures","feature extraction;Gaussian processes;image fusion;image registration;mixture models;probability;remote sensing","remote sensing image registration method;dynamic threshold calculation strategy;DTCS;multiple-feature distance fusion Gaussian mixture model;feature points extraction;image transformation;deterministic annealing","","4","","28","IEEE","17 Sep 2019","","","IEEE","IEEE Journals"
"Generalized Linear Spectral Mixing Model for Spatial–Temporal–Spectral Fusion","J. Zhou; W. Sun; X. Meng; G. Yang; K. Ren; J. Peng","Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Hubei Key Laboratory of Applied Mathematics, Faculty of Mathematics and Statistics, Hubei University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","18 Jul 2022","2022","60","","1","16","Image fusion effectively solves the trade-off among spatial resolution, temporal resolution, and spectral resolution of remote sensing sensors. However, most of existing methods focus on the fusion of two of the spatial, temporal, and spectral metrics of remote sensing images. The few spatial–temporal–spectral fusion (STSF) methods available are mainly for fusing Moderate Resolution Imaging Spectroradiometer (MODIS) and Landsat images, which are not suitable for the characteristics of the spaceborne hyperspectral images (HSIs) with low temporal resolution, such as Hyperion, ZY-1 02D, and PRISMA. For this purpose, we proposed a novel generalized linear spectral mixing model for STSF (GLMM-STSF). In the method, the GLMM is introduced into the STSF problem, and the temporal variations of images at different times are transferred to the endmember and abundance matrix variations of images for estimation. To the best of our knowledge, for the first time, the STSF task of remote sensing images is handled from the perspective of spectral unmixing. Compared with the existing STSF fusion methods, our method targets the task of fusing spaceborne HSI with low temporal and spatial resolutions with multispectral image (MSI) featured by high temporal and spatial resolutions. Taking the STSF of ZY-1 02D hyperspectral and Sentinel-2 multispectral real datasets as an example, comparisons with related state-of-the-art methods demonstrate that our proposed method achieves superior fusion performance.","1558-0644","","10.1109/TGRS.2022.3188501","National Natural Science Foundation of China(grant numbers:42122009,41971296,42171326,41801252,41801256,61871177,42171351); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR19D010001,LY22F010014,LQ18D010001); Natural Science Foundation of Hubei Province(grant numbers:2021CFA087); Public Science and Technology Plan Projects of Ningbo City(grant numbers:2021S089); Science and Technology Innovation 2025 Major Project of Ningbo City(grant numbers:2021Z107,2022Z032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815322","Hyperspectral;image fusion;multispectral;spatial resolution;spectral resolution;spectral unmixing;temporal resolution","Spatial resolution;Tensors;Task analysis;Sensors;Hyperspectral imaging;Sparse matrices;Earth","geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image resolution;radiometry;remote sensing","generalized linear spectral mixing model;image fusion;trade-off among spatial resolution;spectral resolution;remote sensing sensors;spatial metrics;temporal, metrics;spectral metrics;remote sensing images;spatial-temporal-spectral fusion methods;spaceborne hyperspectral images;low temporal resolution;GLMM-STSF;STSF problem;endmember;STSF task;spectral unmixing;existing STSF fusion methods;low temporal resolutions;spatial resolutions;multispectral image;high temporal resolutions;ZY-1 02D hyperspectral;related state-of-the-art methods;superior fusion performance","","2","","70","IEEE","5 Jul 2022","","","IEEE","IEEE Journals"
"Application of Multi-Sensor Image Fusion of Internet of Things in Image Processing","H. Li; S. Liu; Q. Duan; W. Li","School of Computer Science, Xianyang Normal University, Xianyang, China; School of Computer Science, Xianyang Normal University, Xianyang, China; School of Computer Science, Xianyang Normal University, Xianyang, China; School of Computer Science, Xianyang Normal University, Xianyang, China","IEEE Access","5 Oct 2018","2018","6","","50776","50787","The perception layer of Internet of Things (IOT) consists of various sensors. It is the source of the IOT to identify objects and collect information. Information fusion collected from multi-sensor has been widely used in various fields, such as intelligent industry, intelligent agriculture, intelligent transportation, and intelligent environmental protection. In this paper, multi-sensor image fusion, multispectral (MS) and panchromatic (PAN) images, is studied, and the fused images are used in target detection, recognition, and classification. However, traditional methods based on an injection model generally consider the MS images as a whole to compute the spectral weights. They ignore the local information of MS images and produce some spectral distortions, because for different objects, the spectral response will be different. Therefore, we propose a novel multi-sensor image fusion based on application layer of IOT (IFIOT) to preserve the spectral information of MS images. In this method, local homogeneous areas are found first by superpixel segmentation. Due to good properties of superpixel, the homogeneous areas are uniform and contain only one kind of object. Then, we estimate the spectral weights for different bands on the homogeneous area. The injection gain has an important influence on fusion results. Therefore, we adaptively compute the gain coefficients by minimizing the error between the spectral degraded MS and PAN images. Finally, after the injection of spatial details obtaining from the PAN image, fused images are produced. Experimental results reveal that the IFIOT method can give good fusion results and the spectral information is preserved well.","2169-3536","","10.1109/ACCESS.2018.2868227","National Natural Science Foundation of China(grant numbers:81473559); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2017JM6086); Science Basic Research Program in Shaanxi Province of China(grant numbers:16JK1823); Innovation Program in Shaanxi Province of China(grant numbers:2018KRM145); Xianyang Normal University(grant numbers:XSYK18012); Students’ Platform for Innovation and Entrepreneurship Training Program in Shaanxi Province of China(grant numbers:201828003); Students’ Platform for Innovation and Entrepreneurship Training Program in Xianyang Normal University of China(grant numbers:2018014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452895","Internet of Things;multisensor;image fusion;homogeneous region;adaptive gain","Image fusion;Image segmentation;Remote sensing;Internet of Things;Distortion;Spatial resolution;Sensors","distortion;image classification;image fusion;image segmentation;Internet of Things;object detection;object recognition","MS images;PAN image;spectral information;image processing;information fusion;intelligent industry;intelligent agriculture;intelligent transportation;intelligent environmental protection;local information;novel multisensor image fusion;Internet of Things;panchromatic images;target detection;target recognition;target classification;spectral distortions;spectral response;local homogeneous area;superpixel properties;spectral weight estimation;gain coefficients;error minimization;spatial detail injection;IFIoT method;superpixel segmentation","","10","","40","OAPA","31 Aug 2018","","","IEEE","IEEE Journals"
"A Phase Congruency and Local Laplacian Energy Based Multi-Modality Medical Image Fusion Method in NSCT Domain","Z. Zhu; M. Zheng; G. Qi; D. Wang; Y. Xiang","Key Laboratory of Industrial Internet of Things and Networked Control, Ministry of Education, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China; Key Laboratory of Industrial Internet of Things and Networked Control, Ministry of Education, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Mathematics and Computer and Information Science, Mansfield University of Pennsylvania, Mansfield, PA, USA; School of Communication Engineering, Chongqing University, Chongqing, Chian; Key Laboratory of Industrial Internet of Things and Networked Control, Ministry of Education, College of Automation, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Access","25 Feb 2019","2019","7","","20811","20824","Multi-modality image fusion provides more comprehensive and sophisticated information in modern medical diagnosis, remote sensing, video surveillance, and so on. This paper presents a novel multi-modality medical image fusion method based on phase congruency and local Laplacian energy. In the proposed method, the non-subsampled contourlet transform is performed on medical image pairs to decompose the source images into high-pass and low-pass subbands. The high-pass subbands are integrated by a phase congruency-based fusion rule that can enhance the detailed features of the fused image for medical diagnosis. A local Laplacian energy-based fusion rule is proposed for low-pass subbands. The local Laplacian energy consists of weighted local energy and the weighted sum of Laplacian coefficients that describe the structured information and the detailed features of source image pairs, respectively. Thus, the proposed fusion rule can simultaneously integrate two key components for the fusion of low-pass subbands. The fused high-pass and low-pass subbands are inversely transformed to obtain the fused image. In the comparative experiments, three categories of multi-modality medical image pairs are used to verify the effectiveness of the proposed method. The experiment results show that the proposed method achieves competitive performance in both the image quantity and computational costs.","2169-3536","","10.1109/ACCESS.2019.2898111","National Natural Science Foundation of China(grant numbers:61803061,61703347); Chongqing Municipal Education Commission Foundation(grant numbers:KJQN201800603); Major Science and Technology Project of Yunnan Province(grant numbers:2018ZF017); Natural Science Foundation of Chongqing(grant numbers:cstc2018jcyjAX0167); Chongqing Science and Technology Commission(grant numbers:cstc2017zdcy-zdyfX0067,cstc2017zdcy-zdyfX0055); Artificial Intelligence Technology Innovation Significant Theme Special Project of Chongqing Science and Technology Commission(grant numbers:cstc2017rgzn-zdyfX0014,cstc2017rgzn-zdyfX0035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636953","Medical image fusion;multi-modality sensor fusion;NSCT;phase congruency;Laplacian energy","Image fusion;Transforms;Laplace equations;Medical diagnostic imaging;Feature extraction;Medical diagnosis","image fusion;medical image processing;patient diagnosis;transforms","fused image;local Laplacian energy-based fusion rule;low-pass subbands;multimodality medical image pairs;image quantity;multimodality image fusion;high-pass subbands;phase congruency-based fusion rule;medical diagnosis;multi-modality medical image fusion method;NSCT domain;non-subsampled contourlet transform","","176","","52","OAPA","7 Feb 2019","","","IEEE","IEEE Journals"
"Edge-Conditioned Feature Transform Network for Hyperspectral and Multispectral Image Fusion","Y. Zheng; J. Li; Y. Li; J. Guo; X. Wu; Y. Shi; J. Chanussot","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; CAS Key Laboratory of Spectral Imaging Technology, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; LJK, Inria, CNRS, Grenoble INP, Universite Grenoble Alpes, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","15","Despite recent advances achieved by deep learning techniques in the fusion of low-spatial-resolution hyperspectral image (LR-HSI) and high-spatial-resolution multispectral image (HR-MSI), it remains a challenge to reconstruct the high-spatial-resolution HSI (HR-HSI) with more accurate spatial details and less spectral distortions, since the low-level structure information such as sharp edges tends to be weakened or lost as the network depth grows. To tackle this issue, we creatively propose an edge-conditioned feature transform network (EC-FTN) in this article, which is mainly composed of three parts, namely, feature extraction network (FEN), feature fusion and transformation network (FFTN), and image reconstruction network (IRN). First, two computationally efficient FENs with 3-D convolutions and reshaping layers are employed to extract the joint spectral-spatial features of input images. Then, the FFTN conditioned on the edge map prior can fuse and transform the features adaptively, in which a fusion node and several cascaded feature modulation modules (FMMs) equipped with feature-wise modulation layers are constructed. Specifically, the edge map is generated via transfer learning, i.e., by applying the Sobel operator to feature maps of the red-green-blue (RGB) version of HR-MSI resulting from the pretrained VGG16 model without extra training. Finally, the desired HR-HSI is recovered from the transformed features through IRN. Furthermore, we elaborately design a weighted combinatorial loss function consisting of mean absolute error, image gradient difference, and spectral angle terms to guide the training. Experiments on both ground-based and remotely sensed datasets demonstrate that our EC-FTN outperforms state-of-the-art methods in visual and quantitive evaluations, as well as in fine details reconstruction.","1558-0644","","10.1109/TGRS.2021.3108122","National Key Research and Development Program of China(grant numbers:2018AAA0102702); National Nature Science Foundation of China(grant numbers:61901343,61801359,61701360,61671383,61571345); Science and Technology on Space Intelligent Control Laboratory(grant numbers:ZDSYS-2019-03); China Postdoctoral Science Special Foundation(grant numbers:2018T111019); Open Research Fund of CAS Key Laboratory of Spectral Imaging Technology(grant numbers:LSIT201924W); Fundamental Research Funds for the Central Universities; Innovation Fund of Xidian University(grant numbers:5001-20109215456); Higher Education Discipline Innovation Project(grant numbers:B08038); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9530284","Edge prior;feature transform network;hyperspectral image (HSI);image fusion;multispectral image (MSI);transfer learning (TL)","Image edge detection;Feature extraction;Spatial resolution;Pansharpening;Image reconstruction;Convolutional neural networks;Transforms","edge detection;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image colour analysis;image fusion;image reconstruction;image representation;image resolution;image segmentation;image sensors;learning (artificial intelligence);remote sensing;sensor fusion","edge-conditioned feature transform network;multispectral image fusion;low-spatial-resolution hyperspectral image;high-spatial-resolution multispectral image;high-spatial-resolution HSI;accurate spatial details;low-level structure information;sharp edges;network depth;feature extraction network;transformation network;image reconstruction network;spectral-spatial features;input images;edge map;cascaded feature modulation modules;feature-wise modulation layers;transformed features;image gradient difference","","4","","58","IEEE","6 Sep 2021","","","IEEE","IEEE Journals"
"Dehazing for Multispectral Remote Sensing Images Based on a Convolutional Neural Network With the Residual Architecture","M. Qin; F. Xie; W. Li; Z. Shi; H. Zhang","Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Shanghai Institute of Satellite Engineering, Shanghai, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Apr 2018","2018","11","5","1645","1655","Multispectral remote sensing images are often contaminated by haze, which causes low image quality. In this paper, a novel dehazing method based on a deep convolutional neural network (CNN) with the residual structure is proposed for multispectral remote sensing images. First, multiple CNN individuals with the residual structure are connected in parallel and each individual is used to learn a regression from the hazy image to the clear image. Then, the outputs of CNN individuals are fused with weight maps to produce the final dehazing result. In the designed network, the CNN individuals, mining multiscale haze features through multiscale convolutions, are trained using different levels of haze samples to achieve different dehazing abilities. In addition, the weight maps change with the haze distribution, and the fusion of the CNN individuals is adaptive. The designed network is end-to-end, and putting a hazy image into it, the clear scene can be restored. To train the network, a wavelength-dependent haze simulation method is proposed to generate labeled data, which can synthesize hazy multispectral images highly close to real conditions. Experimental results show that the proposed method can accurately remove the haze in each band of multispectral images under different scenes.","2151-1535","","10.1109/JSTARS.2018.2812726","National Natural Science Foundation of China(grant numbers:61471016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325417","Convolutional neural network (CNN);haze removal;haze simulation;multispectral remote sensing images","Remote sensing;Atmospheric modeling;Earth;Atmospheric waves;Scattering;Cloud computing;Image restoration","feedforward neural nets;geophysical image processing;image colour analysis;image denoising;image enhancement;image filtering;image fusion;image restoration;learning (artificial intelligence);neural net architecture;remote sensing","multispectral remote sensing images;low image quality;deep convolutional neural network;residual structure;multiple CNN individuals;hazy image;clear image;wavelength-dependent haze simulation method;hazy multispectral images;dehazing abilities;weight maps;haze samples;haze distribution","","54","","31","IEEE","26 Mar 2018","","","IEEE","IEEE Journals"
"Occluded Object Detection in High-Resolution Remote Sensing Images Using Partial Configuration Object Model","S. Qiu; G. Wen; Y. Fan","ATR Key Laboratory, National University of Defense Technology, Changsha, China; ATR Key Laboratory, National University of Defense Technology, Changsha, China; ATR Key Laboratory, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 May 2017","2017","10","5","1909","1925","Deformable-part-based model (DPM) has shown great success in object detection in recent years. However, its performance will degrade on partially occluded objects and is even worse on largely occluded objects in real remote sensing applications. To address this problem, a novel partial configuration object model (PCM) is developed in this paper. Compared to conventional single-layer DPMs, an extra partial configuration layer, which is composed of partial configurations defined according to possible occlusion patterns, is introduced in PCM to block the transmission of occlusion impact. During detection, each hypothesis from a partial configuration layer will infer the entire object based on spatial interrelationship and final detection results are obtained from the fusion of these possible entire objects using a weighted continuous clustering method. As PCM makes a better compromise between the deformation modeling flexibility of small parts and the discriminative shape-capturing capability of large DPM, its performance on occluded object detection will be improved. Moreover, occlusion states of detected objects can be inferred with the intermediate results of our model. Experimental results on multiple high-resolution remote sensing image datasets demonstrate the effectiveness of the proposed model.","2151-1535","","10.1109/JSTARS.2017.2655098","Program for New Century Excellent Talents in the University of China(grant numbers:NCET-11-0866); National Science Foundation of China(grant numbers:41601487); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855669","Object detection;occlusion inference;partial configuration object model (PCM);remote sensing images","Remote sensing;Object detection;Phase change materials;Semantics;Atmospheric modeling;Deformable models;Airplanes","image fusion;object detection;remote sensing","occluded object detection;high-resolution remote sensing image;partial configuration object model;single-layer deformable-part-based model;occlusion pattern;occlusion impact;partial configuration layer;object fusion;weighted continuous clustering method","","23","","50","IEEE","14 Feb 2017","","","IEEE","IEEE Journals"
"Combining Multiple Classifiers for Domain Adaptation of Remote Sensing Image Classification","H. Wei; L. Ma; Y. Liu; Q. Du","School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; School of Mechanical Engineering and Electronic Information, China University of Geosciences, Wuhan, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Jan 2021","2021","14","","1832","1847","This article investigates the effectiveness of multiclassifier fusion technique on domain adaptation for remote sensing image classification. Since it is impossible to find a domain adaptation method that is optimal for different datasets, and it is also difficult to select the best base classifier for domain-invariant features, multiple domain adaptation fusion (MDAF) method and the multiple base classifier fusion (MBCF) method are proposed to achieve a more stable and superior classification performance. The most crucial step of the weighted fusion approach is to assign weights for classifiers. It is known that different classifiers have varied performances on different subsets of data, and therefore a samplewise adaptive weight is more desirable than a fixed one. For each sample, a desired weight should be able to characterize the reliability of a classifier, so that the advantages of different classifiers can be exploited. We propose a neighborhood consistency based adaptive weighting method, which assigns a large weight to a classifier on a sample if the prediction of the sample is consistent to the predictions of its local neighbors. Experiments with three remote sensing images demonstrate the efficiency of the proposed weighting strategy in the proposed MDAF and MBCF methods.","2151-1535","","10.1109/JSTARS.2021.3049527","National Natural Science Foundation of China(grant numbers:61771437,61102104,91442201,41772376); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314885","Classification;domain adaptation;multiclassifier fusion;neighborhood consistency;remote sensing","Remote sensing;Manifolds;Support vector machines;Image classification;Sensors;Probabilistic logic;Feature extraction","geophysical image processing;image classification;image fusion;pattern classification;remote sensing","weighted fusion approach;samplewise adaptive weight;adaptive weighting method;weighting strategy;remote sensing image classification;multiclassifier fusion technique;domain-invariant features;multiple domain adaptation fusion method;multiple base classifier fusion method;classification performance stability;MDAF;MBCF","","11","","39","CCBY","6 Jan 2021","","","IEEE","IEEE Journals"
"ESPFNet: An Edge-Aware Spatial Pyramid Fusion Network for Salient Shadow Detection in Aerial Remote Sensing Images","S. Luo; H. Li; R. Zhu; Y. Gong; H. Shen","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; KylinSoft Company, Ltd., Changsha, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 May 2021","2021","14","","4633","4646","Shadows can hinder image interpretation in aerial remote sensing images. The existing shadow detection methods focus on all shadow regions and detect the shadow regions directly, but they ignore the fact that salient shadows have a more significant effect. In this work, a novel edge-aware spatial pyramid fusion network (ESPFNet) under a multitask learning framework is proposed for salient shadow detection in aerial remote sensing images. ESPFNet has three components: a parallel spatial pyramid (PSP) structure; an edge detection module (EDM); and an edge-aware multibranch integration (EMI). The PSP structure is constructed to extract multiscale features from the input image and fuse them gradually. The EDM then integrates the shallow features and deep features to detect the shadow edges. Finally, the EMI incorporates the edge features with multibranch features, and then concatenates them with the shallow features to generate the salient shadow detection result. The experimental analyses confirm the effectiveness of the ESPFNet method in both the qualitative and quantitative performance, compared to the existing methods, with the F-score reaching 92.04% in the salient shadow test set.","2151-1535","","10.1109/JSTARS.2021.3066791","National Natural Science Foundation of China(grant numbers:41631180,41871246,41971303); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380896","Aerial remote sensing images;convolutional neural network;multitask learning;salient shadow detection","Feature extraction;Image edge detection;Remote sensing;Task analysis;Electromagnetic interference;Data mining;Buildings","edge detection;feature extraction;image fusion;remote sensing","edge features;shallow features;ESPFNet method;salient shadow test set;edge-aware spatial pyramid fusion network;aerial remote sensing images;image interpretation;parallel spatial pyramid structure;edge detection module;edge-aware multibranch integration;shadow edges;salient shadow detection","","7","","53","CCBY","17 Mar 2021","","","IEEE","IEEE Journals"
"ConDinet++: Full-Scale Fusion Network Based on Conditional Dilated Convolution to Extract Roads From Remote Sensing Images","K. Yang; J. Yi; A. Chen; J. Liu; W. Chen","Institute of Artificial Intelligence Application, Central South University of Forestry and Technology, Changsha, China; Institute of Artificial Intelligence Application, Central South University of Forestry and Technology, Changsha, China; Institute of Artificial Intelligence Application, Central South University of Forestry and Technology, Changsha, China; Institute of Artificial Intelligence Application, Central South University of Forestry and Technology, Changsha, China; Institute of Artificial Intelligence Application, Central South University of Forestry and Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","31 Dec 2021","2022","19","","1","5","Extracting roads from aerial images is an issue that has attracted much attention. Using semantic segmentation methods to extract roads often faces the problem of narrow and occluded roads. In this letter, we propose a network called ConDinet++, which improves the general codec architecture. In the encoder part, the VGG16 with pretraining parameters is utilized for the feature extraction. In the decoder part, we perform a feature fusion mechanism on the full-scale feature map. In order to improve the ability of the network to extract and integrate semantic information and further increase the receptive field, we recommend adopting the conditional dilated convolution blocks (CDBs) in the encoder, and each CDB consists of a group of cascaded conditional dilated convolutions. More importantly, the designed codec architecture can adjust the number of convolutions and the parameters of the convolution kernel according to the input data. For a slender area like a road, which occupies a small area in the picture, we use the joint loss function and introduce the joint loss of Lovasz loss and cross-entropy loss to avoid the segmentation model having a serious bias caused by highly unbalanced object sizes between roads and background. The proposed method was tested on two public datasets Massachusetts Roads Dataset and Mini DeepGlobe Road Extraction Challenge. Compared with some previous semantic segmentation networks, the proposed ConDinet++ achieved the best values of recall, F-score, and mIoU.","1558-0571","","10.1109/LGRS.2021.3093101","National Natural Science Foundation of China(grant numbers:61602528); Research Foundation for Advanced Talents of Central South University of Forestry and Technology(grant numbers:2015YJ013); Hunan Provincial Postgraduate Research and Innovation Fund(grant numbers:CX20200740); Central South Forestry University of Science and Technology Postgraduate Research and Innovation Fund(grant numbers:CX20202036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478944","Conditional convolution;remote sensing;road extraction;semantic segmentation;Unet++","Roads;Convolution;Feature extraction;Semantics;Remote sensing;Image segmentation;Kernel","codecs;convolutional neural nets;entropy;feature extraction;geophysical image processing;image fusion;image segmentation;remote sensing;roads","joint loss function;Massachusetts Roads Dataset;full-scale fusion network;road extraction;remote sensing images;aerial images;semantic segmentation methods;narrow roads;occluded roads;general codec architecture;feature extraction;feature fusion mechanism;full-scale feature map;semantic information;conditional dilated convolution blocks;cascaded conditional dilated convolutions;ConDinet++;Mini DeepGlobe Road Extraction Challenge dataset;VGG16;CDB;Lovasz loss;cross-entropy loss","","11","","24","IEEE","9 Jul 2021","","","IEEE","IEEE Journals"
"Approximate Area-to-Point Regression Kriging for Fast Hyperspectral Image Sharpening","Q. Wang; W. Shi; P. M. Atkinson; Q. Wei","Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; Hong Kong Polytechnic University, Hong Kong; Faculty of Science and Technology, Lancaster University, Lancaster, U.K.; Department of Engineering, University of Cambridge, Cambridge, U.K.","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 May 2017","2017","10","1","286","295","Area-to-point regression kriging (ATPRK) is an advanced image fusion approach in remote sensing In this paper, ATPRK is considered for sharpening hyperspectral images (HSIs), based on the availability of a fine spatial resolution panchromatic or multispectral image. ATPRK can be used straightforwardly to sharpen each coarse hyperspectral band in turn. This scheme, however, is computationally expensive due to the large number of bands in HSIs, and this problem is exacerbated for multiscene or multitemporal analysis. Thus, we extend ATPRK for fast HSI sharpening with a new approach, called approximate ATPRK (AATPRK), which transforms the original HSI to a new feature space and image fusion is performed for only the first few components before back transformation. Experiments on two HSIs show that AATPRK greatly expedites ATPRK, but inherits the advantages of ATPRK, including maintaining a very similar performance in sharpening (both ATPRK and AATPRK can produce more accurate results than seven benchmark methods) and precisely conserving the spectral properties of coarse HSIs.","2151-1535","","10.1109/JSTARS.2016.2569480","Research Grants Council of Hong Kong(grant numbers:PolyU 15223015,5249/12E); National Natural Science Foundation of China(grant numbers:41331175); Leading talent Project of National Administration of Surveying(grant numbers:K.SZ.XX.VTQA); Ministry of Science and Technology of China(grant numbers:2012BAJ15B04,2012AA12A305); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7491202","Area-to-point regression kriging (ATPRK);downscaling;geostatistics;hyperspectral image (HSI);image fusion;sharpening","Spatial resolution;Principal component analysis;Hyperspectral imaging;Image fusion;Earth","hyperspectral imaging;image fusion;regression analysis;remote sensing","area-to-point regression kriging;fast hyperspectral image sharpening;image fusion approach;remote sensing;fine spatial-resolution panchromatic image;multispectral image;multiscene analysis;multitemporal analysis;approximate ATPRK;feature space","","8","","39","IEEE","14 Jun 2016","","","IEEE","IEEE Journals"
"Multimodal Fusion Remote Sensing Image–Audio Retrieval","R. Yang; S. Wang; Y. Sun; H. Zhang; Y. Liao; Y. Gu; B. Hou; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Aug 2022","2022","15","","6220","6235","Remote sensing image–audio retrieval (RSIAR) has been an emerging research topic in recent years, and many different methods have been proposed for this topic. These RSIAR methods have achieved good retrieval results, but two problems remain: the lack of discriminability of audio modality and the existence of a heterogeneous gap between audio and image. These two problems make the cross-modal common embedding space for audio and images suboptimal, often failing to perform superior retrieval. This article proposes a novel RSIAR method named multimodal fusion remote sensing image–audio retrieval (MMFR) to address these two problems. MMFR first converts original audio input to text. Then, MMFR uses a feature fusion module to obtain a fusion representation fused with text information instead of the original sole audio representation. Fusion text information can make the pronunciation-based audio feature more semantically discriminable and convert pronunciation-based audio feature to more “high-level” fusion feature to cross the heterogeneous gap. Seven different fusion methods are tried in the feature fusion module. In addition, the triplet loss, the semantic loss, and the consistency loss are used to optimize the common retrieval space. Extensive experiments conducted on the UCM_IV, RSICD_IV, and SYDNE_IV datasets demonstrate that our MMFR method outperforms state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3194076","Key Research and Development Program of Shaanxi(grant numbers:2021ZDLGY01-06,2022ZDLGY01-12); National Key Research and Development Program of China(grant numbers:2021ZD0110404); National Natural Science Foundation of China(grant numbers:62171347); Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847567","Feature fusion;multimodal learning;remote sensing audio–image retrieval","Feature extraction;Semantics;Remote sensing;Image retrieval;Sensors;Radar polarimetry;Optimization","audio signal processing;feature extraction;image fusion;image representation;image retrieval;information retrieval;remote sensing","audio modality;audio image;novel RSIAR method;multimodal fusion remote sensing image-audio retrieval;original audio input;feature fusion module;original sole audio representation;fusion text information;convert pronunciation-based audio feature;high-level fusion feature;seven different fusion methods;common retrieval space;RSIAR methods;good retrieval results","","2","","66","CCBY","2 Aug 2022","","","IEEE","IEEE Journals"
"Spatiotemporal Satellite Image Fusion Using Deep Convolutional Neural Networks","H. Song; Q. Liu; G. Wang; R. Hang; B. Huang","Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Collaborative Innovation Center on Forecast and Evaluation of Meteorological Disasters, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Chinese University of Hong Kong, Hong Kong","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Mar 2018","2018","11","3","821","829","We propose a novel spatiotemporal fusion method based on deep convolutional neural networks (CNNs) under the application background of massive remote sensing data. In the training stage, we build two five-layer CNNs to deal with the problems of complicated correspondence and large spatial resolution gaps between MODIS and Landsat images. Specifically, we first learn a nonlinear mapping CNN between MODIS and low-spatial-resolution (LSR) Landsat images and then learn a super-resolution CNN between LSR Landsat and original Landsat images. In the prediction stage, instead of directly taking the outputs of CNNs as the fusion result, we design a fusion model consisting of high-pass modulation and a weighting strategy to make full use of the information in prior images. Specifically, we first map the input MODIS images to transitional images via the learned nonlinear mapping CNN and further improve the transitional images to LSR Landsat images via the fusion model; then, via the learned SR CNN, the LSR Landsat images are supersolved to transitional images, which are further improved to Landsat images via the fusion model. Compared with the previous learning-based fusion methods, mainly referring to the sparse-representation-based methods, our CNNs-based spatiotemporal method has the following advantages: 1) automatically extracting effective image features; 2) learning an end-to-end mapping between MODIS and LSR Landsat images; and 3) generating more favorable fusion results. To examine the performance of the proposed fusion method, we conduct experiments on two representative Landsat-MODIS datasets by comparing with the sparse-representation-based spatiotemporal fusion model. The quantitative evaluations on all possible prediction dates and the comparison of fusion results on one key date in both visual effect and quantitative evaluations demonstrate that the proposed method can generate more accurate fusion results.","2151-1535","","10.1109/JSTARS.2018.2797894","National Natural Science Foundation of China(grant numbers:41501377,61532009,91546117); Foundation of Jiangsu Province of China(grant numbers:BK20150906,15KJA520001); National Social and Scientific Fund Program(grant numbers:16ZDA047); HKRGC General Research Fund(grant numbers:14606315); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8291042","Convolutional neural network (CNN);nonlinear mapping (NLM);spatial resolution;temporal resolution","Remote sensing;Earth;Artificial satellites;Spatial resolution;MODIS;Spatiotemporal phenomena;Training","feature extraction;feedforward neural nets;geophysical image processing;image fusion;image representation;image resolution;learning (artificial intelligence);remote sensing","Landsat-MODIS datasets;end-to-end mapping learning;automatic effective image feature extraction;MODIS images;massive remote sensing data;low-spatial-resolution Landsat images;accurate fusion results;spatiotemporal fusion model;representative Landsat-MODIS datasets;spatiotemporal method;sparse-representation-based methods;learned SR CNN;LSR Landsat images;learned nonlinear mapping CNN;transitional images;prior images;super-resolution CNN;spatial resolution gaps;spatiotemporal fusion method;deep convolutional neural networks;spatiotemporal satellite image fusion","","170","","35","IEEE","13 Feb 2018","","","IEEE","IEEE Journals"
"Convolution Structure Sparse Coding for Fusion of Panchromatic and Multispectral Images","K. Zhang; M. Wang; S. Yang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jan 2019","2019","57","2","1117","1130","Recently, sparse coding-based image fusion methods have been developed extensively. Although most of them can produce competitive fusion results, three issues need to be addressed: 1) these methods divide the image into overlapped patches and process them independently, which ignore the consistency of pixels in overlapped patches; 2) the partition strategy results in the loss of spatial structures for the entire image; and 3) the correlation in the bands of multispectral (MS) image is ignored. In this paper, we propose a novel image fusion method based on convolution structure sparse coding (CSSC) to deal with these issues. First, the proposed method combines convolution sparse coding with the degradation relationship of MS and panchromatic (PAN) images to establish a restoration model. Then, CSSC is elaborated to depict the correlation in the MS bands by introducing structural sparsity. Finally, feature maps over the constructed high-spatial-resolution (HR) and low-spatial-resolution (LR) filters are computed by alternative optimization to reconstruct the fused images. Besides, a joint HR/LR filter learning framework is also described in detail to ensure consistency and compatibility of HR/LR filters. Owing to the direct convolution on the entire image, the proposed CSSC fusion method avoids the partition of the image, which can efficiently exploit the global correlation and preserve the spatial structures in the image. The experimental results on QuickBird and Geoeye-1 satellite images show that the proposed method can produce better results by visual and numerical evaluation when compared with several well-known fusion methods.","1558-0644","","10.1109/TGRS.2018.2864750","National Natural Science Foundation of China(grant numbers:91438103,61771376,61771380,91438201,U1730109,U1701267); Equipment Pre-Research Project of the 13th Five-Years Plan(grant numbers:6140137050206,414120101026,6140312010103,6141A020223,6141B06160301,6141B07090102); Major Research Plan in Shaanxi Province of China(grant numbers:2017ZDXM-GY-103,017ZDCXLGY-03-02); Foundation of the State Key Laboratory of CEMEE(grant numbers:Grant2018K0101B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457487","Convolution sparse coding (CSC);image fusion;multispectral (MS) image;panchromatic (PAN) image;structure sparsity","Image coding;Convolution;Dictionaries;Image fusion;Correlation;Image restoration;Image resolution","geophysical image processing;image coding;image fusion;image representation;image resolution;learning (artificial intelligence);remote sensing;sensor fusion","convolution structure sparse coding;image fusion method;competitive fusion results;overlapped patches;partition strategy results;spatial structures;multispectral image;convolution sparse coding;panchromatic images;MS bands;structural sparsity;high-spatial-resolution;low-spatial-resolution;fused images;direct convolution;CSSC fusion method;Geoeye-1 satellite images;joint HR-LR filter learning framework","","28","","51","IEEE","11 Sep 2018","","","IEEE","IEEE Journals"
"Parallelized Nonlinear Target Detection for Asbestos Identification in Large-Scale Remote Sensing Data","Y. Shi; J. Qu; X. Song; Y. Li; H. Song; A. Vizziello; P. Gamba","School of Automobile, Chang’an University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; School of Information Engineering, Chang’an University, Xi’an, China; Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Pavia, Italy; Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Pavia, Italy","IEEE Geoscience and Remote Sensing Letters","16 Dec 2022","2022","19","","1","5","Due to the side effects of asbestos on human health and environments, many countries have banned the use of asbestos-containing materials, but there are still illegal products with asbestos in daily life. In order to investigate the distributions of asbestos to facilitate its removal, this letter studies the feasibility of asbestos identification with hyperspectral (HS) and panchromatic (PAN) data, taking images captured by the PRISMA and ZY1E 2-D satellites over Pavia, Italy, as examples. In this work, a pansharpening method with guided filter was used to improve the HS image quality in terms of spectral fidelity and spatial details. Then, the possible location of asbestos could be obtained by a nonlinear target detector named bilinear sparse target detector (BSTD). Considering high computational cost for large-scale remote sensing data processing, we further develop BSTD to its parallelized version (denoted as PBSTD). Given the ground truth of asbestos over Pavia by the Regional Environmental Protection Agency–ARPA, Lombardia, our PBSTD and several popular methods are evaluated from both qualitative and quantitative perspectives, showing that most algorithms could correctly detect large-size asbestos roofs, and the nonlinear PBSTD and MSDinter perform better in small-size asbestos identification than other linear detectors. However, the detection accuracy on small-size asbestos is insufficient in practical applications, which indicates that there are still issues to achieve accurate small-size asbestos identification using coarse-spatial-resolution spaceborne remote sensing.","1558-0571","","10.1109/LGRS.2022.3223673","National Nature Science Foundation of China(grant numbers:62072053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9963660","Asbestos identification;nonlinear detection;parallelization;remote sensing","Hyperspectral imaging;Spatial resolution;Detectors;Pansharpening;Object detection;Satellites;Testing","asbestos;filtering theory;geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image resolution;object detection;remote sensing","accurate small-size asbestos identification;asbestos-containing materials;bilinear sparse target detector;coarse-spatial-resolution spaceborne remote sensing;hyperspectral data;large-scale remote sensing data processing;large-size asbestos roofs;nonlinear target detector;panchromatic data;pansharpening method;parallelized nonlinear target detection;sparse target detector","","","","23","IEEE","24 Nov 2022","","","IEEE","IEEE Journals"
"Remote Sensing Scene Classification Using Heterogeneous Feature Extraction and Multi-Level Fusion","X. Wang; M. Xu; X. Xiong; C. Ning","College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; School of Computer and Electronic Information, Nanjing Normal University, Nanjing, China","IEEE Access","11 Dec 2020","2020","8","","217628","217641","Understanding the scenes provided by remote sensing (RS) images has drawn increasing attention in the last decade. It is to automatically classify a RS scene image by feature extraction and label assignment. Although much effort has been dedicated to developing discriminative feature extraction as well as automatic classification techniques, it is still very challenging owing to the complex distributions of the ground objects in high spatial resolution scenes. To enhance the ability to represent the RS scenes, an integration of multiple types of features for remote sensing scenes is considered as an effective way. Nevertheless, different kinds of features possess different characteristics, and how to fuse them together is a critical problem. In this paper, to fuse three different but complementary types of features that are extracted to characterize global attributes of scenes together, a discriminant correlation analysis based feature-level fusion approach is proposed, which maximizes the correlation of corresponding features across the two feature sets and in addition de-correlates features that belong to different classes within each feature set. In addition, to further improve the scene classification performance, another kind of information fusion form, called decision-level fusion is adopted in the classification stage. In our proposed decision-level fusion technique, the final results of multiple classifiers are combined via majority voting. Extensive experiments results conducted on the well-known SIRI-WHU dataset, the WHU-RS dataset and the UC Merced Land Use dataset have demonstrated that the proposed remote sensing scene classification method based on heterogeneous feature extraction and multi-level fusion is superior to many state-of-the-art scene classification algorithms.","2169-3536","","10.1109/ACCESS.2020.3042501","Fundamental Research Funds for the Central Universities(grant numbers:B210202077); Jiangsu Innovation Program for Graduate Education(grant numbers:KYCX20_0536,KYLX15_0278); Jiangsu Province Government Scholarship for Studying Abroad, Six Talents Peak Project of Jiangsu Province(grant numbers:XYDXX-007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9281302","Remote sensing;classification;heterogeneous feature extraction;multi-level fusion","Image analysis;Correlation;Fuses;Feature extraction;Classification algorithms;Task analysis;Remote sensing","feature extraction;geophysical image processing;image classification;image fusion;remote sensing","remote sensing scenes;discriminant correlation analysis based feature-level fusion approach;corresponding features;feature sets;de-correlates features;scene classification performance;information fusion form;called decision-level fusion;classification stage;decision-level fusion technique;WHU-RS dataset;remote sensing scene classification method;heterogeneous feature extraction;multilevel fusion;scene classification algorithms;remote sensing images;RS scene image;label assignment;discriminative feature extraction;automatic classification techniques;high spatial resolution scenes;RS scenes","","10","","53","CCBY","4 Dec 2020","","","IEEE","IEEE Journals"
"Change Detection Method of High Resolution Remote Sensing Image Based on D-S Evidence Theory Feature Fusion","J. Zhao; S. Liu; J. Wan; M. Yasir; H. Li","College of Oceanography and Space Informatics, China University of Petroleum Qingdao, Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum Qingdao, Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum Qingdao, Qingdao, China; School of Geosciences, China University of Petroleum Qingdao, Qingdao, China; College of Oceanography and Space Informatics, China University of Petroleum Qingdao, Qingdao, China","IEEE Access","8 Jan 2021","2021","9","","4673","4687","Using high-resolution satellite image to detect change has been a hotspot in the field of remote sensing for a long time series. The change detection method combining feature extraction and machine learning could extract the change information effectively, but the manual sample selection is a huge workload for a wide range remote sensing images, and it is also difficult to ensure the accuracy of the pre-detection sample using a single difference image. Therefore, in this paper, a new method for change detection has been put forward based on multi-feature fusion of D-S evidence theory. In this approach, the texture difference image has calculated by structural similarity, because the difference image based on structural similarity plays a great role in change detection, which was verified in experiments. The difference images based on texture features and traditional spectral features are fused by D-S evidence theory, and texture features and spectral features have been fully utilized. Setting rules to select samples with high confidence based on pixels, and SLIC super-pixel segmentation has applied in order to improve further the credibility of the sample. Finally, the samples selected by SLIC segmentation optimization are sent to the classifier training to obtain the final result. The experimental results show that texture features play a very important role in the change detection of high-resolution remote sensing images, and D-S evidence theory could effectively fuse spectral texture features to improve the accuracy of change detection. The proposed method has high accuracy and good performance in change detection.","2169-3536","","10.1109/ACCESS.2020.3047915","National Key Research and Development Program of China(grant numbers:2017YFC145600); National Natural Science Foundation of China(grant numbers:41776182); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9310308","Change detection;D-S evidence theory;structural similarity (SSIM);multi feature fusion;remote sensing;high-resolution satellite image","Remote sensing;Feature extraction;Data mining;Support vector machines;Monitoring;Image segmentation;Satellites","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;image texture;learning (artificial intelligence);object detection;remote sensing","change detection method;high resolution remote sensing image;D-S evidence theory feature;high-resolution satellite image;feature extraction;machine learning;change information;wide range remote sensing images;pre-detection sample;single difference image;multifeature fusion;texture difference image;traditional spectral features;high-resolution remote sensing images;spectral texture features","","10","","66","CCBY","29 Dec 2020","","","IEEE","IEEE Journals"
"Exploiting Joint Sparsity for Pansharpening: The J-SparseFI Algorithm","X. X. Zhu; C. Grohnfeldt; R. Bamler","The Remote Sensing Technology Institute (IMF), Technical University of Munich, Munich, Germany; The Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Wessling, Germany; The Remote Sensing Technology Institute (IMF), Technical University of Munich, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2016","2016","54","5","2664","2681","Recently, sparse signal representation of image patches has been explored to solve the pansharpening problem. Although these proposed sparse-reconstruction-based methods lead to promising results, three issues remained unsolved: 1) high computational cost; 2) no consideration given to the possibility of mutually correlated information in different multispectral channels; and 3) requirement that the spectral responses of the panchromatic (Pan) image and the multispectral image cover the same wavelength range, which is not necessarily valid for most sensors. In this paper, we propose a sophisticated sparse image fusion algorithm, which is named “jointly sparse fusion of images” (J-SparseFI). It is based on the earlier proposed sparse fusion of images (SparseFI) algorithm and overcomes the aforementioned three drawbacks of the existing sparse image fusion algorithms. The computational problem is handled by reducing the problem size and by proposing a fully parallelizable scheme. Moreover, J-SparseFI exploits the possible signal structure correlations between multispectral channels by introducing the joint sparsity model (JSM) and sharpening the highly correlated adjacent multispectral channels together. This is done by exploiting the distributed compressive sensing theory that restricts the solution of an underdetermined system by considering an ensemble of signals being jointly sparse. J-SparseFI also offers a practical solution to overcome spectral range mismatch between the Pan and multispectral images. By means of sensor spectral response and channel mutual correlation analysis, the multispectral channels are assigned to primary groups of joint channels, secondary groups of joint channels, and individual channels. Primary groups of joint channels, individual channels, and secondary groups of joint channels are then reconstructed sequentially, by the JSM or by modified SparseFI, using a dictionary trained from the Pan image or previously reconstructed high-resolution multispectral channels. A recipe of how to choose appropriate algorithm parameters, including the most crucial regularization parameter, is provided. The algorithm is evaluated and validated using WorldView-2-like images that are simulated using very high resolution airborne HySpex hyperspectral imagery and further practically demonstrated using real WorldView-2 images. The algorithm's performance is compared with other state-of-the-art methods. Visual and quantitative analyses demonstrate the high quality of the proposed method. In particular, the analysis of the difference images suggests that J-SparseFI is superior in image resolution recovery.","1558-0644","","10.1109/TGRS.2015.2504261","Helmholtz Association(grant numbers:VH-NG-1018); Munich Aerospace e.V.—Fakultät für Luft- und Raumfahrt; Gauss Centre for Supercomputing e.V.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360927","Data fusion;joint sparsity;jointly sparse fusion of images (J-SparseFI);pansharpening;sparse fusion of images (SparseFI);Data fusion;joint sparsity;jointly sparse fusion of images (J-SparseFI);pansharpening;sparse fusion of images (SparseFI)","Sensors;Spatial resolution;Image sensors;Image fusion;Correlation","compressed sensing;geophysical techniques;remote sensing","HySpex hyperspectral imagery;WorldView-2-like images;sensor spectral response;channel mutual correlation analysis;distributed compressive sensing theory;joint sparsity model;sparse image fusion algorithm;multispectral image cover;panchromatic image;multispectral channels;mutually correlated information;sparse-reconstruction-based methods;pansharpening problem;J-SparseFI algorithm;joint sparsity","","94","","34","CCBY","18 Dec 2015","","","IEEE","IEEE Journals"
"An Integrated Approach to Registration and Fusion of Hyperspectral and Multispectral Images","Y. Zhou; A. Rangarajan; P. D. Gader","Department of Radiology and Biomedical Imaging, Yale University, New Haven, USA; Department of Computer and Information Science and Engineering, University of Florida, Gainesville, USA; Department of Computer and Information Science and Engineering, University of Florida, Gainesville, USA","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2020","2020","58","5","3020","3033","Combining a hyperspectral (HS) image and a multispectral (MS) image - an example of image fusion - can result in a spatially and spectrally high-resolution image. Despite the plethora of fusion algorithms in remote sensing, a necessary prerequisite, namely registration, is mostly ignored. This limits their application to well-registered images from the same source. In this article, we propose and validate an integrated registration and fusion approach (code available at https://github.com/zhouyuanzxcv/Hyperspectral). The registration algorithm minimizes a least-squares (LSQ) objective function with the point spread function (PSF) incorporated together with a nonrigid freeform transformation applied to the HS image and a rigid transformation applied to the MS image. It can handle images with significant scale differences and spatial distortion. The fusion algorithm takes the full high-resolution HS image as an unknown in the objective function. Assuming that the pixels lie on a low-dimensional manifold invariant to local linear transformations from spectral degradation, the fusion optimization problem leads to a closed-form solution. The method was validated on the Pavia University, Salton Sea, and the Mississippi Gulfport datasets. When the proposed registration algorithm is compared to its rigid variant and two mutual information-based methods, it has the best accuracy for both the nonrigid simulated dataset and the real dataset, with an average error less than 0.15 pixels for nonrigid distortion of maximum 1 HS pixel. When the fusion algorithm is compared with current state-of-the-art algorithms, it has the best performance on images with registration errors as well as on simulations that do not consider registration effects.","1558-0644","","10.1109/TGRS.2019.2946803","NSF IIS(grant numbers:NSF IIS 1743050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897135","Point spread function (PSF);hyperspectral (HS) image analysis;image fusion;nonrigid registration","Linear programming;Spatial resolution;Image reconstruction;Hyperspectral sensors;Distortion","geophysical image processing;image fusion;image registration;image resolution;least squares approximations;optical transfer function;optimisation;remote sensing","mutual information-based methods;PSF;LSQ;integrated registration-fusion approach;remote sensing;fusion optimization problem;local linear transformations;high-resolution HS image;MS image;nonrigid freeform transformation;point spread function;least-squares objective function;registration algorithm;hyperspectral image registration;multispectral image fusion;multispectral image registration;hyperspectral image fusion;registration errors;fusion algorithm","","22","","48","IEEE","12 Nov 2019","","","IEEE","IEEE Journals"
"Adaptive Deep Co-Occurrence Feature Learning Based on Classifier-Fusion for Remote Sensing Scene Classification","R. Tombe; S. Viriri","School of Computer Science, University of KwaZuLu-Natal, Durban, South Africa; School of Computer Science, University of KwaZuLu-Natal, Durban, South Africa","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2021","2021","14","","155","164","Remote sensing scene classification has numerous applications on land cover land use. However, classifying the scene images into their correct categories is a challenging task. This challenge is attributable to the diverse semantics of remote sensing images. This nature of remote sensing images makes the task of effective feature extraction and learning complex. Effective image feature representation is essential in image analysis and interpretation for accurate scene image classification with machine learning algorithms. The recent literature shows that convolutional neural networks are mighty in feature extraction for remote sensing scene classification. Additionally, recent literature shows that classifier-fusion attains superior results than individual classifiers. This article proposes the adaptive deep co-accordance feature learning (ADCFL). The ADCFL method utilizes a convolutional neural network to extract spatial feature information from an image in a co-occurrence manner with filters, and then this information is fed to the multigrain forest for feature learning and classification through majority votes with ensemble classifiers. An evaluation of the effectiveness of ADCFL is conducted on the public datasets Resisc45 and Ucmerced. The classification accuracy results attained by the ADCFL demonstrate that the proposed method achieves improved results.","2151-1535","","10.1109/JSTARS.2020.3044264","Inyuvesi Yakwazulu-Natali; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9291440","Adaptive deep co-occurrence learning;deep feature extraction;ensemble learning;machine learning;multigrained forests;scene classification","Remote sensing;Feature extraction;Deep learning;Forestry;Semantics;Earth;Task analysis","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);remote sensing","remote sensing images;effective feature extraction;effective image feature representation;image analysis;accurate scene image classification;convolutional neural network;remote sensing scene classification;spatial feature information;scene images;adaptive deep co-accordance feature learning;classifier-fusion;machine learning algorithms;multigrain forest;ensemble classifiers;public datasets;Resisc45;Ucmerced","","2","","57","CCBY","11 Dec 2020","","","IEEE","IEEE Journals"
"Exploiting Full-Scale Feature for Remote Sensing Object Detection Based on Refined Feature Mining and Adaptive Fusion","H. Xu; X. Wang; T. Rui; B. Duan; D. Wang","Department of Mechanical Engineering, College of Field Engineering, Army Engineering University, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University, Nanjing, China","IEEE Access","1 Nov 2021","2021","9","","145422","145434","Object detection for remote sensing images remains a challenging problem. In this paper, we proposed an effective remote sensing detection network (RS-Net) based on YOLOv4, which greatly solves the difficulties of remote sensing object detection. Center to our RS-Net is coarse-grained highlighting (CGH), fine-grained mining (FGM) and scale distillation (SD) modules. Through a fusion of original and complementary feature, CGH alleviates the dilemma of object semantic extraction caused by the variability of background information. Besides, FGM is designed to further enhance the feature extraction ability, which is implemented by prominent branch and ensemble branch in parallel. The former highlights the object semantics of being suppressed due to its high similarity with background. The latter mines object information as a whole to reduce the interference from noise. Finally, SD significantly improves the richness of small object information in shallow layer. Experiments conducted over DOTA and NWPU-VHR datasets confirm that the proposed RS-Net achieves competitive detection performance compared with state-of-the-art detectors.","2169-3536","","10.1109/ACCESS.2021.3111742","China National Key Research and Development Program(grant numbers:2016YFC0802904); National Natural Science Foundation of China(grant numbers:61671470); 62nd batch of funded projects of China Postdoctoral Science Foundation(grant numbers:2017M623423); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535136","Remote sensing object detection;YOLOv4;feature mining;adaptive fusion","Remote sensing;Feature extraction;Object detection;Neck;Detectors;Semantics;Convolution","data mining;feature extraction;geophysical image processing;image classification;image fusion;image segmentation;object detection;remote sensing","full-scale feature;remote sensing object detection;refined feature mining;remote sensing images;effective remote sensing detection network;original feature;complementary feature;object semantic extraction;feature extraction ability;object semantics;object information;RS-Net achieves competitive detection performance","","","","54","CCBY","10 Sep 2021","","","IEEE","IEEE Journals"
"CycleGAN-STF: Spatiotemporal Fusion via CycleGAN-Based Image Generation","J. Chen; L. Wang; R. Feng; P. Liu; W. Han; X. Chen","Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Aerospace Information Research Institute (AIR), Chinese Academy of Sciences (CAS), Beijing, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2021","2021","59","7","5851","5865","Due to the trade-off of temporal resolution and spatial resolution, spatiotemporal image-fusion uses existing high-spatial-low-temporal (HSLT) and high-temporal-low-spatial (HTLS) images as prior knowledge to reconstruct high-temporal-high-spatial (HTHS) images. However, some existing spatiotemporal image-fusion algorithms ignore the issue that the spatial information of HTLS images is insufficient to support the acquisition of spatial information, which leads to the unsatisfactory accuracy of the fusion result. To introduce more spatial information, the algorithm in this article uses Cycle-generative adversarial networks (GANs) to simulate the change process of two HSLT images at k-1 and k+1, and to generate some simulated images between k-1 and k+1. Then, the generated images are selected under the help of HTLS images, and the selected ones are then enhanced with wavelet transform. Finally, the image with spatial information is introduced into the Flexible Spatiotemporal DAta Fusion (FSDAF) framework to improve the performance of spatiotemporal image-fusion. Extensive experiments on two real data sets demonstrate that our proposed method outperforms current state-of-the-art spatiotemporal image-fusion methods.","1558-0644","","10.1109/TGRS.2020.3023432","National Natural Science Foundation of China(grant numbers:U1711266,41925007,41701429,41371344); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206067","Generative adversarial network (GAN);image-generation;remote sensing;spatiotemporal image-fusion;wavelet transform","Spatiotemporal phenomena;Spatial resolution;Remote sensing;Wavelet transforms;Generative adversarial networks","image enhancement;image fusion;image resolution;neural nets;wavelet transforms","cycle generative adversarial networks;HSLT images;HTLS images;temporal resolution;spatial resolution;spatiotemporal image fusion;high temporal high spatial images;CycleGAN based image generation;flexible spatiotemporal data fusion framework;high temporal low spatial images;image enhancement;wavelet transform","","19","","52","IEEE","25 Sep 2020","","","IEEE","IEEE Journals"
"Remote Sensing Change Detection via Temporal Feature Interaction and Guided Refinement","Z. Li; C. Tang; L. Wang; A. Y. Zomaya","School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science and the Key Laboratory of Geological Survey and Evaluation, Ministry of Education, China University of Geosciences, Wuhan, China; School of Computer Science and the Key Laboratory of Geological Survey and Evaluation, Ministry of Education, China University of Geosciences, Wuhan, China; School of Information Technologies, University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Geoscience and Remote Sensing","2 Sep 2022","2022","60","","1","11","Remote sensing change detection (RSCD), which identifies the changed and unchanged pixels from a registered pair of remote sensing images, has enjoyed remarkable success recently. However, locating changed objects with fine structural details is still a challenging problem in RSCD. In this article, we propose a novel RSCD network via temporal feature interaction and guided refinement (TFI-GR) to solve this issue. Specifically, unlike previous methods, which just employ one single concatenation or subtraction operation for bi-temporal feature fusion, we design a temporal feature interaction module (TFIM) to enhance interaction between bi-temporal features and capture temporal difference information at diverse feature levels. Afterward, a guided refinement modules (GRMs), which aggregates both low- and high-level temporal difference representations to polish the location information of high-level features and filter the background clutters of low-level features, is repeatedly performed. Finally, the multilevel temporal difference features are progressively fused to generate change maps for change detection. To demonstrate the effectiveness of the proposed TFI-GR, comprehensive experiments are performed on three high spatial resolution RSCD datasets. Experimental results indicate that the proposed method is superior to other state-of-the-art change detection methods. The demo code of this work is publicly available at https://github.com/guanyuezhen/TFI-GR.","1558-0644","","10.1109/TGRS.2022.3199502","National Science Foundation of China(grant numbers:41925007,62076228); Opening Fund of the Key Laboratory of Geological Survey and Evaluation of Ministry of Education(grant numbers:GLAB2020ZR18); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9863802","Change detection;feature pyramid;guided refinement module (GRM);temporal feature interaction module (TFIM)","Feature extraction;Remote sensing;Task analysis;Semantics;Geology;Fuses;Clutter","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image processing;object detection;remote sensing;video signal processing","sensing change detection;changed pixels;unchanged pixels;remote sensing images;changed objects;novel RSCD network;single concatenation;bi-temporal feature fusion;temporal feature interaction module;bi-temporal features;capture temporal difference information;diverse feature levels;guided refinement modules;high-level temporal difference representations;high-level features;low-level features;multilevel temporal difference features;change maps;high spatial resolution RSCD datasets;state-of-the-art change detection methods","","2","","49","IEEE","19 Aug 2022","","","IEEE","IEEE Journals"
"Pyramid Fully Convolutional Network for Hyperspectral and Multispectral Image Fusion","F. Zhou; R. Hang; Q. Liu; X. Yuan","Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Jun 2019","2019","12","5","1549","1558","Low spatial resolution hyperspectral (LRHS) and high spatial resolution multispectral (HRMS) image fusion has been recognized as an important technology for enhancing the spatial resolution of LRHS image. Recent advances in convolutional neural network have improved the performance of state-of-the-art fusion methods. However, it is still a challenging problem to effectively explore the spatial information of HRMS image. In this paper, we propose a pyramid fully convolutional network made up of an encoder sub-network and a pyramid fusion sub-network to address this issue. Specifically, the encoder sub-network aims to encode the LRHS image into a latent image. Then, this latent image, together with a HRMS image pyramid input, is used to progressively reconstruct the high spatial resolution hyperspectral image in a global-to-local manner. Furthermore, to sharpen the blurry predictions easily obtained by the standard l2 loss, we introduce the gradient difference loss as a regularization term. We evaluate the proposed method on three datasets acquired by three different satellite sensors. Experimental results demonstrate that the proposed method achieves better performance than several state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2019.2910990","National Natural Science Foundation of China(grant numbers:61532009,61825601,61876090); Foundation of Jiangsu Province of China(grant numbers:BK20180786,18KJB520032); Postgraduate Research & Practice Innovation Program of Jiangsu Province(grant numbers:KYCX18_1023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709694","Convolutional neural network (CNN);gradient difference loss (GDL);hyperspectral (HS) image;image fusion;multispectral (MS) image","Spatial resolution;Feature extraction;Image fusion;Hyperspectral imaging","geophysical image processing;image classification;image fusion;image resolution;neural nets","pyramid fully convolutional network;low spatial resolution hyperspectral;high spatial resolution multispectral image fusion;LRHS image;convolutional neural network;spatial information;encoder sub-network;pyramid fusion sub-network;latent image;HRMS image pyramid input;high spatial resolution hyperspectral image;blurry predictions","","47","","47","IEEE","8 May 2019","","","IEEE","IEEE Journals"
"Patch-Aware Deep Hyperspectral and Multispectral Image Fusion by Unfolding Subspace-Based Optimization Model","J. Liu; D. Shen; Z. Wu; L. Xiao; J. Sun; H. Yan","Department of Electrical Engineering, City University of Hong Kong, Hong Kong; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, China; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; Department of Electrical Engineering, City University of Hong Kong, Hong Kong","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Jan 2022","2022","15","","1024","1038","Hyperspectral and multispectral image fusion aims to fuse a low-spatial-resolution hyperspectral image (HSI) and a high-spatial-resolution multispectral image to form a high-spatial-resolution HSI. Motivated by the success of model- and deep learning-based approaches, we propose a novel patch-aware deep fusion approach for HSI by unfolding a subspace-based optimization model, where moderate-sized patches are used in both training and test phases. The goal of this approach is to make full use of the information of patch under subspace representation, restrict the scale and enhance the interpretability of the deep network, thereby improving the fusion. First, a subspace-based fusion model was built with two regularization terms to localize pixels and extract texture. Then, the subspace-based fusion model was solved by the alternating direction method of multipliers algorithm, and the model was divided into one fidelity-based problem and two regularization-based problems. Finally, a structured deep fusion network was proposed by unfolding all steps of the algorithm as network layers. Specifically, the fidelity-based problem was solved by a gradient descent algorithm and implemented by a network. The two regularization-based problems were described by proximal operators and learnt by two u-shaped architectures. Moreover, an aggregation fusion technique was proposed to improve the performance by averaging the fused images in all iterations and aggregating the overlapping patches in the test phase. Experimental results, conducted on both synthetic and real datasets, demonstrated the effectiveness of the proposed approach.","2151-1535","","10.1109/JSTARS.2022.3140211","National Natural Science Foundation of China(grant numbers:62071204,61871226,61772274); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201338,BK20180018); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); China Postdoctoral Science Foundation(grant numbers:2021M691275); Jiangsu Postdoctoral Research Foundation(grant numbers:2021K148B); Higher Education Discipline Innovation Project(grant numbers:B12018); Hong Kong Innovation and Technology Commission; Hong Kong Research Grants Council(grant numbers:11204821); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9670687","Alternating direction method of multipliers (ADMM);deep learning;hyperspectral image (HSI);image fusion;subspace;unfolding","Image fusion;Optimization;Tensors;Spatial resolution;Location awareness;Computational modeling;Pansharpening","feature extraction;geophysical image processing;gradient methods;hyperspectral imaging;image fusion;image representation;image resolution;image texture;learning (artificial intelligence);optimisation;sensor fusion","subspace-based fusion model;fidelity-based problem;regularization-based problems;structured deep fusion network;aggregation fusion technique;patch-aware deep hyperspectral;multispectral image fusion;subspace-based optimization model;low-spatial-resolution hyperspectral image;high-spatial-resolution multispectral image;high-spatial-resolution HSI;deep learning-based approaches;novel patch-aware deep fusion approach;moderate-sized patches;subspace representation;deep network","","2","","81","CCBYNCND","5 Jan 2022","","","IEEE","IEEE Journals"
"Large-Factor Super-Resolution of Remote Sensing Images With Spectra-Guided Generative Adversarial Networks","Y. Meng; W. Li; S. Lei; Z. Zou; Z. Shi","Shanghai Artificial Intelligence Laboratory, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; AVIC Chengdu Aircraft Industrial (Group) Company Ltd., Chengdu, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2022","2022","60","","1","11","Large-factor image super-resolution (SR) is a challenging task due to the high uncertainty and incompleteness of the missing details to be recovered. In remote sensing images, the subpixel spectral mixing and semantic ambiguity of ground objects make this task even more challenging. In this article, we propose a novel method for large-factor SR of remote sensing images named spectra-guided generative adversarial networks (SpecGANs). In response to the above problems, we explore whether introducing additional hyperspectral images (HSIs) to GAN as conditional input can be the key to solving the problems. Different from previous approaches that mainly focus on improving the feature representation of a single source input, we propose a dual-branch network architecture to effectively fuse low-resolution (LR) red, green, blue (RGB) images and corresponding HSIs, which fully exploit the rich hyperspectral information as conditional semantic guidance. Due to the spectral specificity of ground objects, the semantic accuracy of the generated images is guaranteed. To further improve the visual fidelity of the generated output, we also introduce the Latent Code Bank with rich visual priors under a generative adversarial training framework so that high-resolution, detailed, and realistic images can be progressively generated. Extensive experiments show the superiority of our method over the state-of-art image SR methods in terms of both quantitative evaluation metrics and visual quality. Ablation experiments also suggest the necessity of adding spectral information and the effectiveness of our designed fusion module. To our best knowledge, we are the first to achieve up to 32x SR of remote sensing images with high visual fidelity under the premise of accurate ground object semantics. Our code can be publicly available at https://github.com/YapengMeng/SpecGAN.","1558-0644","","10.1109/TGRS.2022.3222360","National Key Research and Development Program of China (Titled “Brain-inspired General Vision Models and Applications”); National Natural Science Foundation of China(grant numbers:62125102); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950553","Deep convolutional neural networks (CNNs);generative adversarial networks (GANs);hyperspectral image (HSI);remote sensing image;super-resolution (SR)","Superresolution;Hyperspectral imaging;Semantics;Task analysis;Visualization;Generative adversarial networks;Image reconstruction","geophysical image processing;image classification;image fusion;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","accurate ground object semantics;additional hyperspectral images;factor super-resolution;generative adversarial training framework;ground objects;large-factor image super-resolution;large-factor SR;realistic images;remote sensing images;spectra-guided generative adversarial networks;state-of-art image SR methods","","","","63","IEEE","14 Nov 2022","","","IEEE","IEEE Journals"
"Divide to Attend: A Multiple Receptive Field Attention Module for Object Detection in Remote Sensing Images","H. Tan; Y. Jiong; X. Wan; J. Wang","School of Information Science and Engineering, Xinjiang University, Urumqi, China; School of Software, Xinjiang University, Urumqi, China; School of Software, Xinjiang University, Urumqi, China; School of Software, Xinjiang University, Urumqi, China","IEEE Access","26 Aug 2022","2022","10","","87266","87281","The study of remote sensing image object detection has excellent research value in environmental protection and public safety. However, the performance of the detectors is unsatisfactory due to the large variability of object size and complex background noise in remote sensing images. Therefore, it is essential to improve the detection performance of the detectors. Inspired by the idea of “divide and conquer”, we proposed a Multiple Receptive Field Attention (MRFA) to solve these problems and which is a plug-and-play attention method. First, we use the method of multiple receptive field feature map generation to convert the input feature map into four feature maps with different receptive fields. In this way, the small, medium, large, and immense objects in the input feature maps are “seen” in these feature maps, respectively. Then, we used the multiple attention map fusion method to focus objects of different sizes separately, which can effectively suppress noise in the background of remote sensing images. Experiments on remote sensing object detection datasets DIOR and HRRSD demonstrate that the performance of our method is better than other state-of-the-art attention modules. At the same time, the experiments on remote sensing image semantic segmentation dataset WHDLD and classification dataset AID prove the generalization and superiority of our method.","2169-3536","","10.1109/ACCESS.2022.3199368","National Natural Science Foundation of China(grant numbers:61862060,61462079,61562086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858151","Attention mechanism;object detection;remote sensing images;image processing","Remote sensing;Object detection;Detectors;Image processing;Convolutional neural networks;Feature extraction;Sensors","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;object detection;remote sensing","Multiple Receptive Field Attention module;remote sensing images;remote sensing image object detection;object size;complex background noise;detection performance;-play attention method;multiple receptive field feature map generation;input feature map;feature maps;different receptive fields;immense objects;multiple attention map fusion method;state-of-the-art attention modules","","","","54","CCBY","17 Aug 2022","","","IEEE","IEEE Journals"
"Information Content of Very-High-Resolution SAR Images: Semantics, Geospatial Context, and Ontologies","C. O. Dumitru; S. Cui; G. Schwarz; M. Datcu","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR),, Weßling, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR),, Weßling, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR),, Weßling, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR),, Weßling, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 May 2015","2015","8","4","1635","1650","Currently, the amount of collected Earth Observation (EO) data is increasing considerably with a rate of several Terabytes of data per day. As a consequence of this increasing data volume, new concepts for exploration and information retrieval are urgently needed. To this end, we propose to explore satellite image data via an image information mining (IIM) approach in which the main steps are feature extraction, classification, semantic annotation, and interactive query processing. This leads to a new process chain and a robust taxonomy for the retrieved categories capitalizing on human interaction and judgment. We concentrated on land cover categories that can be retrieved from high-resolution synthetic aperture radar (SAR) images of the spaceborne TerraSAR-X instrument, where we annotated different urban areas all over the world and defined a taxonomy element for each prevailing surface cover category. The annotation resulted from a test dataset comprising more than 100 scenes covering diverse areas of Africa, Asia, Europe, the Middle East, and North and South America. The scenes were grouped into several collections with similar source areas and each collection was processed separately in order to discern regional characteristics. In the first processing step, each scene was tiled into patches. Then the features were extracted from each patch by a Gabor filter bank and a support vector machine with relevance feedback classifying the feature sets into user-oriented land cover categories. Finally, the categories were semantically annotated using Google Earth for ground truthing. The annotation followed a multilevel approach that allowed the fusion of information being visible on different resolution levels. The novelty of this paper lies in the fact that a semantic annotation was performed with a large number of high-resolution radar images that allowed the definition of more than 850 surface cover categories. This opens the way toward an automated identification and classification of urban areas, infrastructure (e.g., airports), geographic objects (e.g., mountains), industrial installations, military compounds, vegetation, and agriculture. Applications that may result from this work can be a semantic catalog of urban images to be used in crisis situations or after a disaster. In addition, the proposed taxonomies can become a basis for building a semantic catalog of satellite images. Finally, we defined four powerful types of high-level queries. Querying on such high levels provides new opportunities for users to search an image database for specific parameters or semantic relationships.","2151-1535","","10.1109/JSTARS.2014.2363595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960829","Annotation;classification;feature extraction;high-resolution images;indexing;ontologies;querying;semantic catalogs;synthetic aperture radar (SAR);taxonomies;TerraSAR-X;Annotation;classification;feature extraction;high-resolution images;indexing;ontologies;querying;semantic catalogs;synthetic aperture radar (SAR);taxonomies;TerraSAR-X","Synthetic aperture radar;Remote sensing;Feature extraction;Semantics;Satellites;Taxonomy;Earth","data mining;feature extraction;Gabor filters;geophysical image processing;image classification;image fusion;image resolution;image retrieval;land cover;ontologies (artificial intelligence);radar imaging;remote sensing by radar;support vector machines;synthetic aperture radar;terrain mapping;visual databases","Gabor filter bank;support vector machine;user-oriented land cover categories;Google Earth;ground truthing;high-resolution radar image fusion;surface cover categories;urban area automated identification;urban area automated classification;infrastructure;geographic objects;industrial installations;military compounds;vegetation;agriculture;semantic catalog;urban images;crisis situations;disaster;high-level queries;image database;semantic relationships;information content;very-high-resolution SAR images;geospatial context;ontologies;regional characteristics;source areas;South America;North America;Middle East;Europe;Asia;Africa;surface cover category;taxonomy element;spaceborne TerraSAR-X instrument;high-resolution synthetic aperture radar images;land cover categories;human judgment;human interaction;interactive query processing;semantic annotation;feature classification;feature extraction;image information mining approach;satellite image data;information retrieval;data volume;collected Earth Observation data","","44","","37","IEEE","20 Nov 2014","","","IEEE","IEEE Journals"
"Classification of Very High-Resolution Remote Sensing Imagery Using a Fully Convolutional Network With Global and Local Context Information Enhancements","H. Hu; Z. Li; L. Li; H. Yang; H. Zhu","School of Resource and Environment Sciences, Wuhan University, Wuhan, China; Hubei Institute of Land Surveying and Mapping, Wuhan, China; RE-Institute of Smart Perception and Intelligent Computing, Wuhan University, Wuhan, China; Institutes of Physical Science and Information Technology, Anhui University, Hefei, China; School of Resource and Environment Sciences, Wuhan University, Wuhan, China","IEEE Access","24 Jan 2020","2020","8","","14606","14619","Deep learning methods for semantic image segmentation can effectively extract geographical features from very high-resolution (VHR) remote sensing images. However, these methods experience over-segmentation in low-level features and a breakdown in the integrity of objects with fixed patch sizes due to the multi-scaled geographical features. In this study, a dual attention mechanism is introduced and embedded into densely connected convolutional networks (DenseNets) to form a dense-global-entropy network (DGEN) for the semantic segmentation of VHR remote sensing images. In the DGEN architecture, a global attention enhancement module is developed for context acquisition, and a local attention fusion module is designed for detail selection. This network presents the improved semantic segmentation performance of test ISPRS 2D datasets. The experimental results indicate an improvement in the overall accuracy (OA), F1, kappa coefficient and mean intersection over union (MIoU). Compared with the DeeplabV3+ and SegNet models, the OA improves by 2.79% and 1.19%; the mean F1 improves by 3.43% and 0.88%; the kappa coefficient improves by 4.04% and 1.82%; and the MIoU improves by 5.22% and 1.47%, respectively. The experiments showed that the dual attention mechanism presented in this study can improve segmentation and maintain object integrity during the encoding-decoding process.","2169-3536","","10.1109/ACCESS.2020.2964760","National Natural Science Foundation of China(grant numbers:41871298,41271443,41571395,41471328,41971311); National Basic Research Program of China (973 Program)(grant numbers:2016YFB0501403); National Administration of Surveying, Mapping and Geoinformation of China; Natural Science Foundation of Anhui Province; Major Program of Science and Technology of Anhui Province(grant numbers:18030801111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952711","Attention mechanism;DenseNet;semantic segmentation;very high-resolution remote sensing images","Remote sensing;Image segmentation;Semantics;Feature extraction;Deep learning;Computational modeling;Convolution","entropy;feature extraction;geophysical image processing;image classification;image fusion;image resolution;image segmentation;learning (artificial intelligence);object detection;remote sensing","object integrity;high-resolution remote sensing imagery;fully convolutional network;local context information enhancements;semantic image segmentation;high-resolution remote sensing images;low-level features;fixed patch;multiscaled geographical features;dual attention mechanism;densely connected convolutional networks;dense-global-entropy network;VHR remote sensing images;DGEN architecture;global attention enhancement module;context acquisition;local attention fusion module;improved semantic segmentation performance;test ISPRS 2D datasets;kappa coefficient","","6","","53","CCBY","8 Jan 2020","","","IEEE","IEEE Journals"
"Unsupervised Multimodal Change Detection Based on Structural Relationship Graph Representation Learning","H. Chen; N. Yokoya; C. Wu; B. Du","Graduate School of Frontier Sciences, The University of Tokyo, Chiba, Japan; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project (AIP), RIKEN, Tokyo, Japan; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Computer Science and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","26 Dec 2022","2022","60","","1","18","Unsupervised multimodal change detection is a practical and challenging topic that can play an important role in time-sensitive emergency applications. To address the challenge that multimodal remote sensing images cannot be directly compared due to their modal heterogeneity, we take advantage of two types of modality-independent structural relationships in multimodal images. In particular, we present a structural relationship graph representation learning framework for measuring the similarity of the two structural relationships. First, structural graphs are generated from preprocessed multimodal image pairs by means of an object-based image analysis approach. Then, a structural relationship graph convolutional autoencoder (SR-GCAE) is proposed to learn robust and representative features from graphs. Two loss functions aiming at reconstructing vertex information and edge information are presented to make the learned representations applicable for structural relationship similarity measurement. Subsequently, the similarity levels of two structural relationships are calculated from learned graph representations, and two difference images are generated based on the similarity levels. After obtaining the difference images, an adaptive fusion strategy is presented to fuse the two difference images. Finally, a morphological filtering-based postprocessing approach is employed to refine the detection results. Experimental results on six datasets with different modal combinations demonstrate the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2022.3229027","Japan Science and Technology Agency FOREST(grant numbers:JPMJFR206S); Japan Society for the Promotion of Science KAKENHI(grant numbers:22H03609); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984688","Change detection;graph convolutional autoencoder;graph representation learning;multimodal remote sensing images;structural relationship","Remote sensing;Feature extraction;Optical sensors;Optical imaging;Image sensors;Representation learning;Radar polarimetry","convolutional neural nets;feature extraction;geophysical image processing;graph theory;image classification;image filtering;image fusion;image registration;image segmentation;remote sensing;unsupervised learning","difference images;edge information;learned graph representations;learned representations;modal heterogeneity;modality-independent structural relationships;morphological filtering-based;multimodal remote sensing images;object-based image analysis approach;preprocessed multimodal image pairs;similarity levels;structural relationship graph convolutional autoencoder;structural relationship graph representation learning;structural relationship similarity measurement;time-sensitive emergency applications;unsupervised multimodal change detection;vertex information","","","","88","IEEE","14 Dec 2022","","","IEEE","IEEE Journals"
"A Generalized Metaphor of Chinese Restaurant Franchise to Fusing Both Panchromatic and Multispectral Images for Unsupervised Classification","T. Mao; H. Tang; J. Wu; W. Jiang; S. He; Y. Shu","Key Laboratory of Environmental Change and Natural Disaster and the State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, Beijing, China; Key Laboratory of Environmental Change and Natural Disaster and the State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, Beijing, China; Key Laboratory of Environmental Change and Natural Disaster and the State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, Beijing, China; Key Laboratory of Environmental Change and Natural Disaster and the State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, Beijing, China; Key Laboratory of Environmental Change and Natural Disaster and the State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, Beijing, China; Key Laboratory of Environmental Change and Natural Disaster and the State Key Laboratory of Earth Surface Processes and Resource Ecology, Beijing Normal University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","1 Jun 2016","2016","54","8","4594","4604","Two-step ways are often used for fusing both panchromatic (PAN) and multispectral (MS) images for classification, e.g., classifying MS images sharpened by PAN images or directly pouring fine spatial details of PAN images into a classification result of MS images. In this paper, we present a unified Bayesian framework to iteratively discovering semantic segments from PAN images and allocating cluster labels for the segments using MS images. Specifically, the probabilistic generative process of both PAN and MS images is explained with a generalized metaphor of the Chinese restaurant franchise (CRF) (gCRF), in which the two iterative random processes, i.e., table selection and dish selection, are adapted to discovering semantic segments in PAN images and inferring cluster labels for the discovered segments using MS images, respectively. Our major contributions are twofold: 1) The CRF is generalized into an image fusion framework by elegantly decomposing its two random processes, and 2) the random process of table selection in the CRF is transformed into stochastic image segmentation by enforcing spatial constraints over adjacent pixels. The qualitative analysis of experimental results shows that the gCRF can effectively utilize both the spatial details of the PAN images and the spectral information of the MS images. In terms of quantitative evaluation, the gCRF is comparable with support vector machine-based supervised classification methods.","1558-0644","","10.1109/TGRS.2016.2545927","National Natural Science Foundation of China(grant numbers:41571334); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458164","Chinese restaurant process;image fusion;object-based image analysis;unsupervised classification;Chinese restaurant process;image fusion;object-based image analysis;unsupervised classification","Image segmentation;Satellites;Semantics;Random processes;Spatial resolution;Probabilistic logic;Image fusion","geophysical image processing;geophysical techniques;image classification;image fusion;image segmentation;support vector machines","Chinese restaurant franchise generalized metaphor;panchromatic images;multispectral images;unsupervised classification;PAN images;unified Bayesian framework;probabilistic generative process;iterative random processes;semantic segments;image fusion framework;random processes;stochastic image segmentation;support vector machine-based supervised classification methods","","17","","39","IEEE","22 Apr 2016","","","IEEE","IEEE Journals"
"A Contextual Bidirectional Enhancement Method for Remote Sensing Image Object Detection","J. Zhang; C. Xie; X. Xu; Z. Shi; B. Pan","Hebei Province Key Laboratory of Big Data Calculation, Tianjin, China; Hebei Province Key Laboratory of Big Data Calculation, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China; Statistics and Data Science, Nankai University, Tianjin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Aug 2020","2020","13","","4518","4531","In remote sensing images, the backgrounds of objects include crucial contextual information that may contribute to distinguishing objects. However, there are at least two issues that should be addressed: not all the backgrounds are beneficial, and object information may be suppressed by backgrounds. To address these problems, in this article, we propose the contextual bidirectional enhancement (CBD-E) method to simultaneously remove unexpected background information and enhance objects' features. CBD-E integrates the features of different background regions sequentially in two directions. On the one hand, a gate function is used to filter out unexpected information in the background and thus improve the recall of detection. On the other hand, a spatial-group-based visual attention mechanism is adopted to enhance the features of objects to reduce the false alarm. The gate function provides an approach to selecting meaningful information in the background, while the spatial-group- based visual attention mechanism enhances the information control ability of the gate function. In the experiments, we have validated the effectiveness of both the gate function and the visual attention mechanism and further demonstrated that the proposed contextual fusion strategy performs well on two published data sets.","2151-1535","","10.1109/JSTARS.2020.3015049","National Key R&D Program of China(grant numbers:2017YFC1405605); Natural Science Foundation of Hebei Province(grant numbers:F2019202062); China Postdoctoral Science Foundation(grant numbers:2020M670631); Tianjin Science and Technology Program(grant numbers:18ZXZNGX00100,18YFCZZC00060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165075","Bidirectional fusion;context;remote sensing object detection;visual attention","Logic gates;Remote sensing;Feature extraction;Visualization;Detectors;Object detection;Proposals","feature extraction;image enhancement;image filtering;image fusion;image segmentation;object detection;remote sensing","background information;gate function;remote sensing image object detection;contextual fusion;spatial group based visual attention;contextual bidirectional enhancement;object feature enhancement","","16","","58","CCBY","11 Aug 2020","","","IEEE","IEEE Journals"
"A New Orientation Estimation Method Based on Rotation Invariant Gradient for Feature Points","W. Xu; S. Zhong; W. Zhang; J. Wang; L. Yan","National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","22 Apr 2021","2021","18","5","791","795","For many remote sensing image applications, orientation estimation is a crucial step for feature points extraction and matching, but it has attracted little attention. Due to the intensity differences between remote sensing image pairs, it is difficult to estimate the orientations of corresponding points accurately, resulting in performance degradation of feature matching. Thus, encountering the intensity differences, a robust spatial structure description for feature regions, and an effective calculation manner from description to orientation play a key role in accurate orientation estimation. To this end, in this letter, we first define a plausible orientation for feature points by the total gradient, offering an effective way to convert the gradient trend of the feature region to orientation. Therefore, we further propose a novel orientation estimation method, in which the rotation invariant gradient is introduced to improve the accuracy of gradient calculation and robustness of spatial structure description. Experimental results on multisensor remote sensing images demonstrate that our method increases the orientation estimation accuracy remarkably and outperforms other orientation estimation methods by a large margin, and effectively improves the performance of feature matching.","1558-0571","","10.1109/LGRS.2020.2985358","National Natural Science Foundation of China(grant numbers:61971460); Hubei Provincial Natural Science Foundation of China(grant numbers:2018CFA089); National Science Foundation for Young Scientists of China(grant numbers:61806081); China Postdoctoral Science Foundation(grant numbers:2018M632858); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090871","Feature point;image registration;orientation estimation;remote sensing","Feature extraction;Remote sensing;Robustness;Estimation error;Degradation;Market research","feature extraction;geophysical image processing;image fusion;image matching;remote sensing","new orientation estimation method;orientation estimation methods;orientation estimation accuracy;multisensor remote sensing images;robustness;gradient calculation;rotation invariant gradient;novel orientation estimation method;gradient trend;total gradient;plausible orientation;accurate orientation estimation;orientation play;effective calculation manner;feature region;robust spatial structure description;feature matching;performance degradation;remote sensing image pairs;intensity differences;feature points extraction;remote sensing image applications","","4","","17","IEEE","11 May 2020","","","IEEE","IEEE Journals"
"GCFnet: Global Collaborative Fusion Network for Multispectral and Panchromatic Image Classification","H. Zhao; S. Liu; Q. Du; L. Bruzzone; Y. Zheng; K. Du; X. Tong; H. Xie; X. Ma","College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; Institute of Cartography and Geographic Information System, Chinese Academy of Surveying and Mapping, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","8 Nov 2022","2022","60","","1","14","Among various multimodal remote sensing data, the pairing of multispectral (MS) and panchromatic (PAN) images is widely used in remote sensing applications. This article proposes a novel global collaborative fusion network (GCFnet) for joint classification of MS and PAN images. In particular, a global patch-free classification scheme based on an encoder-decoder deep learning (DL) network is developed to exploit context dependencies in the image. The proposed GCFnet is designed based on a novel collaborative fusion architecture, which mainly contains three parts: 1) two shallow-to-deep feature fusion branches related to individual MS and PAN images; 2) a multiscale cross-modal feature fusion branch of the two images, where an adaptive loss weighted fusion strategy is designed to calculate the total loss of two individual and the cross-modal branches; and 3) a probability weighted decision fusion strategy for the fusion of the classification results of three branches to further improve the classification performance. Experimental results obtained on three real datasets covering complex urban scenarios confirm the effectiveness of the proposed GCFnet in terms of higher accuracy and robustness compared to existing methods. By utilizing both sampled and non-sampled position data in the feature extraction process, the proposed GCFnet can achieve excellent performance even in a small sample-size case.","1558-0644","","10.1109/TGRS.2022.3215020","National Key Research and Development Program of China(grant numbers:2018YFB0505000); National Natural Science Foundation of China(grant numbers:42071324,42001387); Shanghai Rising-Star Program(grant numbers:21QA1409100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919865","Classification;deep learning (DL);feature fusion;global collaborative fusion;remote sensing","Feature extraction;Remote sensing;Collaboration;Training;Spatial resolution;Image classification;Decoding","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);object recognition;probability;remote sensing;sensor fusion","GCFnet;multispectral;panchromatic image classification;multimodal remote sensing data;MS;PAN;remote sensing applications;novel global collaborative fusion network;joint classification;global patch-free classification scheme;encoder-decoder deep learning network;novel collaborative fusion architecture;shallow-to-deep feature fusion branches;multiscale cross-modal feature fusion branch;adaptive loss weighted fusion strategy;cross-modal branches;probability weighted decision fusion strategy;classification performance;nonsampled position data","","2","","32","IEEE","14 Oct 2022","","","IEEE","IEEE Journals"
"A Spatiotemporal Fusion Based Cloud Removal Method for Remote Sensing Images With Land Cover Changes","H. Shen; J. Wu; Q. Cheng; M. Aihemaiti; C. Zhang; Z. Li","School of Resource and Environmental Sciences and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Urban Design, Wuhan University, Wuhan, China; College of Water Conservancy and Civil Engineering, Xinjiang Agricultural University, Urumqi, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Mar 2019","2019","12","3","862","874","Cloud contamination greatly limits the potential utilization of optical remote sensing images for geoscience applications. Many solutions have been developed to remove the clouds from multispectral images. Among these approaches, the temporal-based methods which borrow complementary information from multitemporal images outperform the other methods. However, the common fundamental supposition of the temporal-based methods decides that they are only suitable for scenes with phenological changes, while they perform poorly in cases with significant land cover changes. In this paper, a cloud removal procedure based on multisource data fusion is developed to overcome this limitation. On the basis of the temporal-based approaches, which employ a cloud-free image as reference, this method further introduces two auxiliary images with similar wavelengths and close acquisition dates to the reference and target (contaminated) images into the reconstruction process. The temporal variability of the land cover is captured from the two auxiliary images through a modified spatiotemporal data fusion model, and thus, the serious errors produced by the temporal-based methods can be avoided. Moreover, a residual correction strategy based on the Poisson equation is used to enhance the spectral coherence between the recovered and remaining regions. The experiments confirmed that the proposed method can perform very well for cases with significant land cover changes. Compared with some state-of-the-art approaches, it produces lower bias and more robust efficacy. In conclusion, our method will act as an important technical supplement to the current cloud removal framework, and it provides the possibility to handle scenes with significant land cover changes.","2151-1535","","10.1109/JSTARS.2019.2898348","National Natural Science Foundation of China(grant numbers:41601357,61671334); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661508","Cloud removal;land cover changes;optical remote sensing images;residual correction;spatiotemporal data fusion","Remote sensing;Data integration;Spatiotemporal phenomena;Spatial resolution;Earth;Optical sensors;Image reconstruction","clouds;geophysical image processing;image fusion;image reconstruction;land cover;Poisson equation;remote sensing;spatiotemporal phenomena","cloud contamination;optical remote sensing images;multispectral images;cloud removal procedure;multisource data fusion;cloud-free image;auxiliary images;modified spatiotemporal data fusion model;cloud removal method;land cover changes;cloud removal framework;spatiotemporal fusion;image reconstruction process;Poisson equation;spectral coherence","","34","","46","IEEE","6 Mar 2019","","","IEEE","IEEE Journals"
"High-Resolution Remote Sensing Image Retrieval Based on Classification-Similarity Networks and Double Fusion","Y. Liu; C. Chen; Z. Han; L. Ding; Y. Liu","School of Geography, South China Normal University, Guangzhou, China; School of Geography, South China Normal University, Guangzhou, China; School of Geography, South China Normal University, Guangzhou, China; School of Geography, South China Normal University, Guangzhou, China; School of Geography, South China Normal University, Guangzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Apr 2020","2020","13","","1119","1133","In high-resolution remote sensing image retrieval (HRRSIR), convolutional neural networks (CNNs) have an absolute performance advantage over the traditional hand-crafted features. However, some CNN-based HRRSIR models are classification-oriented, they pay no attention to similarity, which is critical to image retrieval; whereas others concentrate on learning similarity, failing to take full advantage of information about class labels. To address these issues, we propose a novel model called classification-similarity network (CSN), which aims for image classification and similarity prediction at the same time. In order to further improve performance, we build and train two CSNs, and two kinds of information from them, i.e., deep features and similarity scores, are consolidated to measure the final similarity between two images. Besides, the optimal fusion theorem in biometric authentication, which gives a theoretical scheme to make sure that fusion will definitely lead to a better performance, is used to conduct score fusion. Extensive experiments are carried out over publicly available datasets, demonstrating that CSNs are distinctly superior to usual CNNs and our proposed “two CSNs + feature fusion + score fusion” method outperforms the state-of-the-art models.","2151-1535","","10.1109/JSTARS.2020.2981372","National Natural Science Foundation of China(grant numbers:61673184); China Scholarship Council(grant numbers:201806755003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039566","Classification-similarity network (CSN);double fusion;feature fusion;high-resolution remote sensing image retrieval (HRRSIR);optimal fusion weights;score fusion","Face;Image retrieval;Remote sensing;Feature extraction;Euclidean distance;Predictive models","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image representation;image retrieval;learning (artificial intelligence);remote sensing","high-resolution remote sensing image retrieval;classification-similarity network;double fusion;convolutional neural networks;CNN-based HRRSIR models;image classification;similarity scores;optimal fusion theorem;score fusion method;CSN","","11","","63","CCBY","17 Mar 2020","","","IEEE","IEEE Journals"
"Evidential Fusion Based Technique for Detecting Landslide Barrier Lakes From Cloud-Covered Remote Sensing Images","X. Chen; J. Li; Y. Zhang; W. Jiang; L. Tao; W. Shen","State Key Laboratory of Earth Surface Processes and Resource Ecology and the Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Ministry of Civil Affairs/Ministry of Education of China, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Ministry of Civil Affairs/Ministry of Education of China, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Ministry of Civil Affairs/Ministry of Education of China, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Ministry of Civil Affairs/Ministry of Education of China, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Academy of Disaster Reduction and Emergency Management, Beijing Normal University, Ministry of Civil Affairs/Ministry of Education of China, Beijing, China; College of Marine Sciences, Shanghai Ocean University, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 May 2017","2017","10","5","1742","1757","Landslide barrier lakes usually form quickly after disasters and require very timely remote sensing images to monitor the land-cover change. However, cloud-free images are not always available in emergency situations. This paper provides a method to fuse multitemporal cloud-covered images for change detection, based on the evidential fusion framework. First, the frame of discernment is defined by postclassification comparison results. Second, a way of measuring the basic belief assignment (BBA) is introduced based on the confusion matrixes. Next, a simple BBA redistribution process is proposed to deal with cloud coverage problems. Then, the complementary and redundant information from the input images can be fused following the evidence combination and decision making rules in the evidential fusion framework. Finally, the land-cover change map can be derived. Thanks to the Dempster-Shafer evidence theory, the proposed method can complete the change detection process-including data fusion and cloud removal-in an integrated manner. The proposed method is applied to detect the landslide barrier lake in a real case study, using a series of cloud-covered images from the GF-1 satellite. Result comparisons show that the proposed method is more effective than some basic fusion strategies that perform change detection and cloud removal in separate steps. Then, some approaches to improve the proposed method are discussed: introducing new evidence combination rule, improving the classification accuracy, and adding new evidences. All the results indicate the potential of evidential fusion for change detection from cloud-covered images.","2151-1535","","10.1109/JSTARS.2017.2665529","National Natural Science Foundation of China(grant numbers:51379104,41571342,51579135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892890","Barrier lake;change detection;cloud;evidence theory;GF-1 satellite;image sequence analysis;remote sensing","Remote sensing;Lakes;Terrain factors;Earth;Decision making;Data integration;Satellites","disasters;geomorphology;geophysical image processing;image fusion;lakes;land cover;remote sensing","evidential fusion technique;landslide barrier lake detection;cloud-covered remote sensing image;disaster;land-cover change;cloud-free image;multitemporal cloud-covered image;basic belief assignment;BBA redistribution process;cloud coverage;Dempster-Shafer evidence theory;data fusion;cloud removal;GF-1 satellite;basic fusion strategy;classification accuracy","","7","","32","IEEE","5 Apr 2017","","","IEEE","IEEE Journals"
"Learning a Joint Embedding of Multiple Satellite Sensors: A Case Study for Lake Ice Monitoring","M. Tom; Y. Jiang; E. Baltsavias; K. Schindler","Chair of Glaciology and Geomorphodynamics, University of Zurich, Zürich, Switzerland; Chair of Photogrammetry and Remote Sensing, Swiss Federal Institute of Technology (ETH Zürich), Zürich, Switzerland; Chair of Photogrammetry and Remote Sensing, Swiss Federal Institute of Technology (ETH Zürich), Zürich, Switzerland; Chair of Photogrammetry and Remote Sensing, Swiss Federal Institute of Technology (ETH Zürich), Zürich, Switzerland","IEEE Transactions on Geoscience and Remote Sensing","21 Oct 2022","2022","60","","1","15","Fusing satellite imagery acquired with different sensors has been a long-standing challenge of Earth observation, particularly across different modalities such as optical and synthetic aperture radar (SAR) images. Here, we explore the joint analysis of imagery from different sensors in the light of representation learning: we propose to learn a joint embedding of multiple satellite sensors within a deep neural network. Our application problem is the monitoring of lake ice on Alpine lakes. To reach the temporal resolution requirement of the Swiss Global Climate Observing System (GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra moderate resolution imaging spectroradiometer (MODIS), and Suomi-NPP visible infrared imaging radiometer suite (VIIRS). The large gaps between the optical and SAR domains and between the sensor resolutions make this a challenging instance of the sensor fusion problem. Our approach can be classified as a late fusion that is learned in a data-driven manner. The proposed network architecture has separate encoding branches for each image sensor, which feed into a single latent embedding, i.e., a common feature representation shared by all inputs, such that subsequent processing steps deliver comparable output irrespective of which sort of input image was used. By fusing satellite data, we map lake ice at a temporal resolution of <1.5 days. The network produces spatially explicit lake ice maps with pixelwise accuracies >91% [respectively, mean per-class Intersection-over-Union (mIoU) scores >60%] and generalizes well across different lakes and winters. Moreover, it sets a new state-of-the-art for determining the important ice-on and ice-off dates for the target lakes, in many cases meeting the GCOS requirement.","1558-0644","","10.1109/TGRS.2022.3211184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9906117","Deep neural network;lake ice;moderate resolution imaging spectroradiometer (MODIS);satellite embedding learning;sensor fusion;Sentinel-1 SAR (S1-SAR);synthetic aperture radar (SAR);visible infrared imaging radiometer suite (VIIRS)","Lakes;Optical sensors;Spatial resolution;Ice;Satellites;Optical imaging;Integrated optics","geophysical image processing;ice;image classification;image fusion;infrared imaging;lakes;learning (artificial intelligence);radar imaging;radiometers;radiometry;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","Terra moderate resolution imaging spectroradiometer;S1-SAR;Sentinel-1 SAR;image sources;Swiss Global Climate Observing System office;temporal resolution requirement;Alpine lakes;application problem;deep neural network;representation learning;Earth observation;satellite imagery;lake ice monitoring;multiple satellite sensors;joint embedding;target lakes;important ice-on;winters;different lakes;spatially explicit lake ice maps;satellite data;input image;single latent embedding;image sensor;sensor fusion problem;sensor resolutions;optical SAR domains;Suomi-NPP visible infrared imaging radiometer suite;time 1.5 d","","1","","58","IEEE","30 Sep 2022","","","IEEE","IEEE Journals"
"The TanDEM-X DEM Mosaicking: Fusion of Multiple Acquisitions Using InSAR Quality Parameters","A. Gruber; B. Wessel; M. Martone; A. Roth","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Weßling, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Weßling, Germany; Microwaves and Radar Institute, German Aerospace Center (DLR), Weßling, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Weßling, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","3","1047","1057","Since 2010, TanDEM-X and its twin satellite TerraSAR-X fly in a close orbit formation and form a single-pass synthetic aperture radar (SAR) interferometer. The formation was established to acquire a global high-precision digital elevation model (DEM) using SAR interferometry (InSAR). In order to achieve the required height accuracy of the TanDEM-X DEM, at least two global coverages have to be acquired. However, in difficult and mountainous terrain, up to five coverages are present. Here, acquisitions from ascending and descending orbits are needed to fill gaps and to overcome geometric limitations. Therefore, a strategy to properly combine the available height estimates is mandatory. The objective of this paper is the presentation of the operational TanDEM-X DEM mosaicking approach. In general, multiple InSAR DEM heights are combined by means of a weighted average with the height error as weight. Apart from this widely used mosaicking approach, one big challenge remains with the handling of larger height discrepancies between the input data, which are mainly caused by phase unwrapping errors, but also by temporal changes between acquisitions. In the case of inconsistencies, the TanDEM-X mosaicking approach performs a grouping into height levels. A priority concept is set up to evaluate the different groups of heights considering the number of DEMs and several InSAR quality parameters: the height error, the phase unwrapping method, and the height of ambiguity. This allows the identification of the most reliable height level for mosaicking. This fusion concept is verified on different test areas affected by phase unwrapping errors in flat and mountainous terrain as well as by height discrepancies in forests. The results show that the quality of the final TanDEM-X DEM mosaic benefits a lot from this mosaicking approach.","2151-1535","","10.1109/JSTARS.2015.2421879","German Federal Ministry for Economic Affairs and Energy(grant numbers:50 EE 1035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7109106","Digital elevation models (DEMs);image fusion;interferometric synthetic aperture radar (InSAR);mosaicking;TanDEM-X;Digital elevation models (DEMs);image fusion;interferometric synthetic aperture radar (InSAR);mosaicking;TanDEM-X","Reliability;Orbits;Synthetic aperture radar;Calibration;Accuracy;Coherence;Earth","digital elevation models;geophysical image processing;image fusion;image segmentation;radar interferometry;remote sensing by radar;synthetic aperture radar;terrain mapping","InSAR quality parameters;satellite TerraSAR-X;close orbit formation;single-pass synthetic aperture radar interferometer;global high-precision digital elevation model;TanDEM-X DEM height accuracy;ascending orbit;descending orbit;geometric limitations;TanDEM-X DEM mosaicking approach;multiple InSAR DEM height levels;weighted average;height error;input data;phase unwrapping errors;phase unwrapping method;mountainous terrain;flat terrain","","51","","28","OAPA","15 May 2015","","","IEEE","IEEE Journals"
"Multisource Remote Sensing Data Classification With Graph Fusion Network","X. Du; X. Zheng; X. Lu; A. A. Doudkin","University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Spectral Imaging Technology CAS, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Key Laboratory of Spectral Imaging Technology CAS, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Department of Computers, Belarusian State University of Informatics and Radioelectronics (BSUIR), Minsk, Belarus","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2021","2021","59","12","10062","10072","The land cover classification has been an important task in remote sensing. With the development of various sensors technologies, carrying out classification work with multisource remote sensing (MSRS) data has shown an advantage over using a single type of data. Hyperspectral images (HSIs) are able to represent the spectral properties of land cover, which is quite common for land cover understanding. Light detection and ranging (LiDAR) images contain altitude information of the ground, which is greatly helpful with urban scene analysis. Current HSI and LiDAR fusion methods perform feature extraction and feature fusion separately, which cannot well exploit the correlation of data sources. In order to make full use of the correlation of multisource data, an unsupervised feature extraction-fusion network for HSI and LiDAR, which utilizes feature fusion to guide the feature extraction procedure, is proposed in this article. More specifically, the network takes multisource data as input and directly output the unified fused feature. A multimodal graph is constructed for feature fusion, and graph-based loss functions including Laplacian loss and t-distributed stochastic neighbor embedding (t-SNE) loss are utilized to constrain the feature extraction network. Experimental results on several data sets demonstrate the proposed network can achieve more excellent classification performance than some state-of-the-art methods.","1558-0644","","10.1109/TGRS.2020.3047130","Funds for International Cooperation and Exchange of the National Natural Science Foundation of China(grant numbers:62011530021); Belarusian Republican Foundation for Fundamental Research and National Natural Science Foundation of China (joint project BRFFR-NNSF)(grant numbers:F20-017); National Science Found for Distinguished Young Scholars(grant numbers:61925112); National Natural Science Foundation of China(grant numbers:61806193,61772510); Innovation Capability Support Program of Shaanxi(grant numbers:2020KJXX-091,2020TD-015); Natural Science Basic Research Program of Shaanxi(grant numbers:2019JQ-340,2019JC-23); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325097","Classification;deep learning;hyperspectral image (HSI);light detection and ranging (LiDAR);remote sensing","Feature extraction;Laser radar;Data mining;Fuses;Correlation;Task analysis;Dimensionality reduction","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;land cover;optical radar;remote sensing by laser beam;stochastic processes","feature extraction network;data sets;multisource remote sensing data classification;graph fusion network;land cover classification;classification work;hyperspectral images;land cover understanding;feature fusion;data sources;multisource data;unsupervised feature extraction-fusion network;feature extraction procedure;unified fused feature;graph-based loss;Laplacian loss;t-distributed stochastic neighbor embedding loss","","23","","52","IEEE","14 Jan 2021","","","IEEE","IEEE Journals"
"$\text{A}^{3}$Seg: An Annealing Augmented and Aligned Semantic Segmentation Network for High Spatial Resolution Remote Sensing Images","G. Xue; Y. Liu; Y. Huang; M. Li; G. Yang","School of Software, Shandong University, Jinan, China; School of Software, Shandong University, Jinan, China; School of Computer, Heze University, Heze, China; School of Software, Shandong University, Jinan, China; School of Computer, Heze University, Heze, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Jul 2022","2022","15","","5607","5622","Semantic segmentation of high spatial resolution (HSR) remote sensing images (RSIs) plays an important role in many applications. However, HSR RSIs have significantly larger spatial sizes than typical natural images, which results in fewer valuable samples when training models. In addition, fusing multiscale features is the key step in obtaining features with strong semantic and high spatial information. However, current feature fusion methods are too straightforward to address misalignment issues. To handle these two problems, we propose an annealing augmented and aligned segmentation network, named $\text{A}^{3}$Seg. Specifically, we propose an annealing online hard example mining (AOHEM) strategy to automatically select more valuable samples during the training stage. Based on AOHEM, a contextual augmentation block is proposed to extract sufficient contextual information using three different attention mechanisms in consideration of three different feature properties. Finally, we propose a novel feature alignment block to fuse features at different levels by alignment with the guidance of a salient feature. Experimental results on three different HSR RSIs datasets demonstrate that the proposed method outperforms the state-of-the-art general semantic segmentation methods with a better tradeoff between accuracy and complexity.","2151-1535","","10.1109/JSTARS.2022.3187145","National Natural Science Foundation of China(grant numbers:U1903127); Taishan Industry Leading Talents(grant numbers:tscy20200303); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810337","Aligned feature fusion;remote sensing imagery;semantic segmentation;valuable sample mining","Semantics;Feature extraction;Training;Remote sensing;Image segmentation;Task analysis;Transformers","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;remote sensing","annealing online hard example;annealing augmented aligned segmentation network;current feature fusion methods;high spatial information;strong semantic information;multiscale features;training models;fewer valuable samples;typical natural images;larger spatial sizes;high spatial resolution remote sensing images;aligned semantic segmentation network;3 Seg;state-of-the-art general semantic segmentation methods;different HSR RSIs datasets;salient feature;novel feature alignment block;different feature properties;sufficient contextual information;contextual augmentation block","","1","","51","CCBYNCND","29 Jun 2022","","","IEEE","IEEE Journals"
"Dual-Resolution and Deformable Multihead Network for Oriented Object Detection in Remote Sensing Images","D. Yu; Q. Xu; X. Liu; H. Guo; J. Lu; Y. Lin; L. Lv","Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China; Key Laboratory of Spatiotemporal Perception and Intelligent Processing, Ministry of Natural Resources, Zhengzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2023","2023","16","","930","945","Compared with general object detection, the scale variations, arbitrary orientations, and complex backgrounds of objects in remote sensing images make it more challenging to detect oriented objects. Especially for oriented objects that have large aspect ratios, it is more difficult to accurately detect their boundary. Many methods show excellent performance on oriented object detection, most of which are anchor-based algorithms. To mitigate the performance gap between anchor-free algorithms and anchor-based algorithms, this article proposes an anchor-free algorithm called dual-resolution and deformable multihead network (DDMNet) for oriented object detection. Specifically, the dual-resolution network with bilateral fusion is adopted to extract high-resolution feature maps which contain both spatial details and multiscale contextual information. Then, the deformable convolution is incorporated into the network to alleviate the misalignment problem of oriented object detection. And a dilated feature fusion module is performed on the deformable feature maps to expand their receptive fields. Finally, box boundary-aware vectors instead of the angle are leveraged to represent the oriented bounding box and the multihead network is designed to get robust predictions. DDMNet is a single-stage oriented object detection method without using anchors and exhibits promising performance on the public challenging benchmarks. DDMNet obtains 90.49%, 93.25%, and 78.66% mean average precision on the HRSC2016, FGSD2021, and DOTA datasets. In particular, DDMNet achieves 79.86% at mAP75 and 53.85% at mAP85 on the HRSC2016 dataset, respectively, outperforming the current state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3230797","National Natural Science Foundation of China(grant numbers:42001338); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9993705","Box boundary-aware vectors;deformable feature fusion;multihead network;oriented object detection;remote sensing image","Feature extraction;Object detection;Remote sensing;Inference algorithms;Proposals;Prediction algorithms;Convolution","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;object detection;remote sensing","anchor-free algorithm;DDMNet;deformable convolution;deformable multihead network;DOTA dataset;dual-resolution network;FGSD2021 dataset;general object detection;HRSC2016 dataset;remote sensing images;single-stage oriented object detection method","","","","69","CCBY","20 Dec 2022","","","IEEE","IEEE Journals"
"A Shallow-to-Deep Feature Fusion Network for VHR Remote Sensing Image Classification","S. Liu; Y. Zheng; Q. Du; L. Bruzzone; A. Samat; X. Tong; Y. Jin; C. Wang","College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Xinjiang Institute of Ecology and Geography, Chinese Academy of Sciences, Ürümqi, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","9 Jun 2022","2022","60","","1","13","With more detailed spatial information being represented in very-high-resolution (VHR) remote sensing images, stringent requirements are imposed on accurate image classification. Due to the diverse land objects with intraclass variation and interclass similarity, efficient and fine classification of VHR images especially in complex scenes are challenging. Even for some popular deep learning (DL) frameworks, geometric details of land objects may be lost in deep feature levels, so it is difficult to maintain the highly detailed spatial information (e.g., edges, small objects) only relying on the last high-level layer. Moreover, many of the newly developed DL methods require massive well-labeled samples, which inevitably deteriorates the model generalization ability under the few-shot learning. Therefore, in this article, a lightweight shallow-to-deep feature fusion network (SDF2N) is proposed for VHR image classification, where the traditional machine learning (ML) and DL schemes are integrated to learn rich and representative information to improve the classification accuracy. In particular, the shallow spectral–spatial features are first extracted and then a novel triple-stage fusion (TSF) module is designed to learn the saliency and discriminative information at different levels for classification. The TSF module includes three feature fusion stages, that is, low-level spectral–spatial feature fusion, middle-level multiscale feature fusion, and high-level multilayer feature fusion. The proposed SDF2N takes the advantage of the shallow-to-deep features, which can extract representative and complementary information from crossing layers. It is important to note that even with limited training samples, the SDF2N still can achieve satisfying classification performance. Experimental results obtained on three real VHR remote sensing datasets including two multispectral and one airborne hyperspectral images covering complex urban scenarios confirm the effectiveness of the proposed approach compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3179288","National Key Research and Development Program of China(grant numbers:2018YFB0505000,2018YFB0505400); National Natural Science Foundation of China(grant numbers:42071324,42001387); Shanghai Rising-Star Program(grant numbers:21QA1409100); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785804","Extended multiattribute profiles (EMAP);shallow-to-deep feature fusion;spectral–spatial feature extraction;squeeze-excitation (SE) attention mechanism;very-highresolution (VHR) image classification","Feature extraction;Data mining;Convolution;Spatial resolution;Biological system modeling;Training;Kernel","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image resolution;object detection;remote sensing","shallow-to-deep features;representative information;complementary information;satisfying classification performance;VHR remote sensing datasets;multispectral images;airborne hyperspectral images;shallow-to-deep feature fusion network;VHR remote sensing image classification;very-high-resolution remote sensing images;accurate image classification;diverse land objects;intraclass variation;interclass similarity;fine classification;VHR images;deep learning frameworks;geometric details;deep feature levels;highly detailed spatial information;high-level layer;few-shot learning;VHR image classification;rich information;classification accuracy;shallow spectral-spatial features;discriminative information;feature fusion stages;low-level spectral-spatial feature fusion;middle-level multiscale feature fusion;high-level multilayer;triple-stage fusion module","","4","","45","IEEE","30 May 2022","","","IEEE","IEEE Journals"
"Fusion of Infrared and Visible Images for Remote Detection of Low-Altitude Slow-Speed Small Targets","H. Sun; Q. Liu; J. Wang; J. Ren; Y. Wu; H. Zhao; H. Li","Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun, China; National Subsea Centre, Robert Gordon University, Aberdeen, U.K.; 28th Research Institute of China Electronics Technology Group, Nanjing, China; School of Computer Sciences, Guangdong Polytechnic Normal University, Guangzhou, China; School of Computer Sciences, Guangdong Polytechnic Normal University, Guangzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","18 Mar 2021","2021","14","","2971","2983","Detection of the low-altitude and slow-speed small (LSS) targets is one of the most popular research topics in remote sensing. Despite of a few existing approaches, there is still an accuracy gap for satisfying the practical needs. As the LSS targets are too small to extract useful features, deep learning based algorithms can hardly be used. To this end, we propose in this article an effective strategy for determining the region of interest, using a multiscale layered image fusion method to extract the most representative information for LSS-target detection. In addition, an improved self-balanced sensitivity segment model is proposed to detect the fused LSS target, which can further improve both the detection accuracy and the computational efficiency. We conduct extensive ablation studies to validate the efficacy of the proposed LSS-target detection method on three public datasets and three self-collected datasets. The superior performance over the state of the arts has fully demonstrated the efficacy of the proposed approach.","2151-1535","","10.1109/JSTARS.2021.3061496","Key Laboratory of Airborne Optical Imaging and Measurement; Chinese Academy of Sciences and International Cooperation Project of Changchun Institute of Optics; Fine Mechanics and Physics(grant numbers:Y9U933T190); Dazhi Scholarship of the Guangdong Polytechnic Normal University; National Natural Science Foundation of China(grant numbers:62072122); Education Department of Guangdong Province(grant numbers:2019KSYS009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9362227","Background subtraction;image fusion;low-altitude and slow-speed small (LSS) target detection;saliency detection","Object detection;Feature extraction;Image segmentation;Image fusion;Sensitivity;Meteorology;Lighting","feature extraction;geophysical image processing;image fusion;image segmentation;infrared imaging;object detection;remote sensing","infrared images;LSS-target detection;detection accuracy;fused LSS target;self-balanced sensitivity segment model;multiscale layered image fusion method;deep learning;remote sensing;low-altitude slow-speed small targets;remote detection;visible images","","13","","45","CCBY","24 Feb 2021","","","IEEE","IEEE Journals"
"Novel Adaptive Component-Substitution-Based Pan-Sharpening Using Particle Swarm Optimization","W. Wang; L. Jiao; S. Yang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2015","12","4","781","785","Component substitution (CS) technique is a famous framework for merging multispectral (MS) and panchromatic (Pan) images. The synthetic intensity component is important in the CS fusion framework. In this letter, we propose an optimization model to obtain the adaptive weights. The adaptive weights are computed by maximizing an objective function, which measures the radiometric similarity between the low-scale intensity image and the spatially degraded Pan image. Correlation coefficient, mean-structural-similarity index, and mutual information are used as the similarity criteria, respectively. A particle-swarm-optimization algorithm is adopted to solve the single objection optimization problem. The proposed CS framework is compared with popular CS-based fusion methods. Visual analysis and quality results demonstrate that the proposed adaptive CS fusion framework has superior performance.","1558-0571","","10.1109/LGRS.2014.2361834","National Basic Research Program (973 Program) of China(grant numbers:2013CB329402); National Natural Science Foundation of China(grant numbers:61072106,61173090,61072108); National Research Foundation for the Doctoral Program of Higher Education of China(grant numbers:20110203110006); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT1170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6953019","Component substitution (CS);correlation coefficient (CC);mean structural similarity (MSSIM) index;mutual information (MI);particle swarm optimization (PSO);Component substitution (CS);correlation coefficient (CC);mean structural similarity (MSSIM) index;mutual information (MI);particle swarm optimization (PSO)","Remote sensing;Indexes;Spatial resolution;Linear programming;Image fusion;Particle swarm optimization","image fusion;particle swarm optimisation;radiometry","adaptive component-substitution-based pan-sharpening imaging;multispectral imaging;MS imaging;synthetic intensity component;adaptive weight computation;objective function maximization;radiometric similarity measurement;low-scale intensity imaging;spatially degraded Pan imaging;correlation coefficient;mean-structural-similarity index;mutual information;particle-swarm-optimization algorithm;single objection optimization problem;CS-based fusion method;visual analysis","","15","","19","IEEE","11 Nov 2014","","","IEEE","IEEE Journals"
"Improvement of a Pansharpening Method Taking Into Account Haze","H. Li; L. Jing","Key Laboratory of Digital Earth Sciences, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Sciences, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Nov 2017","2017","10","11","5039","5055","Pansharpening is an important technique used to generate high-quality high-spatial-resolution multispectral (MS) bands by fusing low-spatial-resolution MS bands and a panchromatic (PAN) band obtained by the same sensor. A PAN-modulation (PM)-based pansharpening method taking account of haze, which is referred as Haze- and Ratio- based (HR) method, has been demonstrated to yield good performances, indicating that the impact of haze should be considered in PM-based methods. It is obvious that the haze values used in the HR fusion influence the spectral vectors of fused pixels, thus affect the spectral distortion of fused images. In order to reach stable and good performances of the HR method, the determination of the optimal haze values is discussed in this study. First, six approaches for haze values determination, which are variations of the histogram minimal approach and the darkest pixel approach employed by the image-based dark-object subtraction method for atmospheric correction of remote-sensed images, are compared. Then, an improved approach for haze values determination is proposed. The proposed approach is proved to be effective for improving the performance of the HR method. This is very important for the employment of the HR method in practical applications and by more researchers.","2151-1535","","10.1109/JSTARS.2017.2730221","Youth Foundation of Director of the Institution of Remote Sensing and Digital Earth; Chinese Academy of Sciences(grant numbers:Y6SJ1100CX); Key Research Program of the Chinese Academy of Sciences(grant numbers:ZDRW-ZS-2016-6-1-3); National Science and Technology Support Program of China(grant numbers:2015BAB05B05-02); Chinese Academy of Sciences(grant numbers:2015-XBQN-A-07); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8002562","Darkest pixel (DP);haze;high spatial resolution (HSR);pansharpening;remote sensing","Remote sensing;Earth;Histograms;Atmospheric modeling;Distortion;Scattering","geophysical image processing;image fusion;image resolution;remote sensing;smoke","haze values determination;HR method;panchromatic band;PAN-modulation;fused pixels;spectral distortion;fused images;optimal haze values;histogram minimal approach;pansharpening method;HR fusion;PM-based pansharpening method;image-based dark-object subtraction method;atmospheric correction;remote-sensed images","","11","","32","IEEE","4 Aug 2017","","","IEEE","IEEE Journals"
"Attention and Feature Fusion SSD for Remote Sensing Object Detection","X. Lu; J. Ji; Z. Xing; Q. Miao","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China","IEEE Transactions on Instrumentation and Measurement","12 Feb 2021","2021","70","","1","9","Object detection is a basic topic in the field of remote sensing. However, remote sensing images usually suffer in complicated backgrounds, object scale variations, and small objects, which make remote sensing object detection still difficult. Although the existing two-stage methods have higher accuracy, their detection speed is slow. While the one-stage object detection algorithms such as single-shot detector (SSD) and YOLO, although they can achieve real-time detection, they have poor detection performance, especially for small objects. In this article, in order to further improve the remote sensing object detection performance of one-stage methods, we propose an end-to-end network named attention and feature fusion SSD. First, a multilayer feature fusion structure is designed to enhance the semantic information of the shallow features. Next, a dual-path attention module is introduced to screen the feature information. This module uses spatial attention and channel attention to suppress the background noise and highlight the key feature. Then, the feature representation ability of the network is further enhanced by a multiscale receptive field module. Finally, the loss function is optimized to alleviate the imbalance between the positive and negative samples. The experimental results on the DOTA and NWPU VHR-10 data sets verify the effectiveness of our method.","1557-9662","","10.1109/TIM.2021.3052575","National Natural Science Foundation of China(grant numbers:61771386,61772396); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333604","Convolutional neural network;dual-path attention mechanism;feature fusion;image processing;object detection;remote sensing","Feature extraction;Remote sensing;Object detection;Semantics;Detectors;Convolution;Noise measurement","computer vision;feature extraction;geophysical image processing;image fusion;neural nets;object detection;remote sensing","DOTA data sets;NWPU VHR-10 data sets;loss function;multiscale receptive field module;feature representation ability;background noise;feature information;dual-path attention module;semantic information;feature fusion SSD;end-to-end network;YOLO;single-shot detector;object scale variations;real-time detection;one-stage object detection algorithms;remote sensing images;channel attention;spatial attention;multilayer feature fusion structure;remote sensing object detection performance","","13","","33","IEEE","22 Jan 2021","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and Multispectral Images Based on a Bayesian Nonparametric Approach","L. Sui; L. Li; J. Li; N. Chen; Y. Jiao","College of Geology Engineering and Geomatics, Changan University, Xian, China; JIKAN Research Institute of Engineering Investigations and Design, Co., Ltd., Xi'an, China; Departments of Geography and Environmental Management, and Systems Design Engineering, University of Waterloo, Waterloo, ON, Canada; College of Geology Engineering and Geomatics, Changan University, Xian, China; Department of Surveying and Mapping, Gansu Industry Polytechnic College, Tianshui, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Apr 2019","2019","12","4","1205","1218","This paper presents a new approach to fusion of hyperspectral and multispectral images based on Bayesian nonparametric sparse representation. The approach formulates the image fusion problem within a constrained optimization framework, while assuming that the target image lives in a lower dimensional subspace. The subspace transform matrix is determined by principal component analysis, and the sparse regularization term is designed depending on a set of dictionaries and sparse coefficients associated with the observed images. Specifically, the dictionary elements and sparse coefficients are learned by the Bayesian nonparametric approach with the beta-Bernoulli process, which establishes the probability distribution models for each latent variable and calculates the posterior distributions by Gibbs sampling. Finally, serving the obtained posterior distributions as a priori, the fusion problem is solved via an alternate optimization process, where the alternate direction method of multipliers is applied to perform the optimization with respect to the target image. The Bayesian nonparametric method is used to optimize the sparse coefficients. Exhaustive experiments using both two public datasets and one real-world dataset of remote sensing images show that the proposed approach outperforms the existing state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2019.2902847","National Natural Science Foundation of China(grant numbers:41372330,41601345,41571346,41871380,4141379); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675395","Bayesian nonparametric model;dictionary learning;hyperspectral and multispectral images;image fusion;sparse representation","Spatial resolution;Bayes methods;Dictionaries;Optimization;Sparse matrices;Image fusion","Bayes methods;Gaussian processes;geophysical image processing;image fusion;image representation;image sampling;learning (artificial intelligence);matrix algebra;nonparametric statistics;optimisation;principal component analysis;probability;remote sensing","Bayesian nonparametric approach;beta-Bernoulli process;probability distribution models;posterior distributions;alternate optimization process;target image;Bayesian nonparametric method;sparse coefficients;remote sensing images;multispectral images;Bayesian nonparametric sparse representation;image fusion problem;constrained optimization framework;lower dimensional subspace;subspace transform matrix;principal component analysis;sparse regularization term;observed images;dictionary elements;hyperspectral images","","10","","59","IEEE","27 Mar 2019","","","IEEE","IEEE Journals"
"Multimodal Remote Sensing Image Registration With Accuracy Estimation at Local and Global Scales","M. L. Uss; B. Vozel; V. V. Lukin; K. Chehdi","Department of Transmitters, Receivers and Signal Processing, National Aerospace University, Kharkov, Ukraine; IETR UMR CNRS 6164, University of Rennes 1, Lannion, France; Department of Transmitters, Receivers and Signal Processing, National Aerospace University, Kharkov, Ukraine; IETR UMR CNRS 6164, University of Rennes 1, Lannion, France","IEEE Transactions on Geoscience and Remote Sensing","27 Sep 2016","2016","54","11","6587","6605","This paper focuses on the potential accuracy of remote sensing (RS) image registration. We investigate how this accuracy can be estimated without ground truth available and used to improve registration quality of mono- and multimodal pair of images. At the local scale of image fragments, the Cramér-Rao lower bound (CRLB) on registration error is estimated for each local correspondence between coarsely registered pair of images. This CRLB is defined by local image texture and noise properties. Opposite to the standard approach, where registration accuracy is only evaluated at the output of the registration process, such valuable information is used by us as an additional input knowledge. It greatly helps in detecting and discarding outliers and refining the estimation of geometrical transformation model parameters. Based on these ideas, a new area-based registration method called registration with accuracy estimation (RAE) is proposed. In addition to its ability to automatically register very complex multimodal image pairs with high accuracy, the RAE method is able to provide registration accuracy at the global scale as a covariance matrix of estimation error of geometrical transformation model parameters or as pointwise registration standard deviation. This accuracy does not depend on any ground truth availability and characterizes each pair of registered images individually. Thus, the RAE method can identify image areas for which a predefined registration accuracy is guaranteed. This is essential for RS applications imposing strict constraints on registration accuracy such as change detection, image fusion, and disaster management. The RAE method is proved successful with reaching subpixel accuracy while registering eight complex mono-/multimodal and multitemporal image pairs including optical-to-optical, optical-to-radar, optical-to-digital elevation model (DEM) images, and DEM-to-radar cases. Other methods employed in comparisons fail to provide in a stable manner accurate results on the same test cases.","1558-0644","","10.1109/TGRS.2016.2587321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7526459","Area-based registration;Cramér–Rao lower bound (CRLB);digital elevation model (DEM) to radar image registration;multimodal/multitemporal registration;optical-to-DEM;optical-to-radar;polynomial model;registration accuracy;signal-dependent noise model;subpixel accuracy","Image registration;Optical imaging;Optical sensors;Correlation;Estimation;Adaptive optics;Optical transmitters","digital elevation models;disasters;emergency management;geophysical image processing;image fusion;image registration;remote sensing by radar","DEM-to-radar image;digital elevation model;optical-to-DEM image;optical-to-radar image;optical-to-optical image;multitemporal image pair;monomodal image pair;disaster management;image fusion;change detection;multimodal image pair;registration-with-accuracy estimation;area-based registration method;geometrical transformation model;registration process;registration accuracy;image noise property;local image texture;registration error;Cramer-Rao lower bound;registration quality;multimodal remote sensing image registration","","27","","51","IEEE","29 Jul 2016","","","IEEE","IEEE Journals"
"Structure Consistency-Based Graph for Unsupervised Change Detection With Homogeneous and Heterogeneous Remote Sensing Images","Y. Sun; L. Lei; X. Li; X. Tan; G. Kuang","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","21","Change detection (CD) of remote sensing (RS) images is one of the important problems in earth observation, which has been extensively studied in recent years. However, with the development of RS technology, the specific characteristics of remotely sensed images, including sensor characteristics, resolutions, noises, and distortions in imagery, make the CD more complex. In this article, we propose a structure consistency-based method for CD, which detects changes by comparing the structures of two images, rather than comparing the pixel values of images. Because the image structure is imaging modality-invariant and not sensitive to noise, illumination, and other interference factors, the proposed method can be applied to a variety of CD scenarios and has strong robustness. Structural comparison is realized by constructing and mapping an improved nonlocal patch-based graph (NLPG) to avoid the data leakage of two images. First, we demonstrate the effectiveness of the method in homogeneous and heterogeneous CD, which shows that the proposed method can be used as a unified CD framework. Second, we extend the method to the heterogeneous CD with multichannel synthetic aperture radar (SAR) image, which can provide a reference for future research as the heterogeneous CD with multichannel SAR is rarely studied. Third, through the decomposition and in-depth analysis of NLPG, we modify the graph construction process, structure difference calculation, and the difference image fusion to make it more robust and accurate. Experiments on six scenarios 12 data sets demonstrate the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2021.3053571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344836","Graph;heterogeneous data;nonlocal similarity;structure consistency;unsupervised change detection (CD)","Synthetic aperture radar;Radar polarimetry;Image resolution;Optical variables measurement;Optical imaging;Adaptive optics;Optical sensors","geographic information systems;geophysical image processing;image fusion;radar imaging;remote sensing;synthetic aperture radar","image structure;illumination;CD scenarios;structural comparison;improved nonlocal patch-based graph;heterogeneous CD;unified CD framework;multichannel synthetic aperture radar image;graph construction process;structure difference calculation;difference image fusion;structure consistency-based graph;unsupervised change detection;heterogeneous remote sensing images;earth observation;RS technology;remotely sensed images;sensor characteristics;structure consistency-based method","","24","","79","IEEE","2 Feb 2021","","","IEEE","IEEE Journals"
"LWCDnet: A Lightweight Network for Efficient Cloud Detection in Remote Sensing Images","C. Luo; S. Feng; X. Yang; Y. Ye; X. Li; B. Zhang; Z. Chen; Y. Quan","Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer and Information Science, University of Macau, Macau, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and the Shenzhen Key Laboratory of Internet Information Collaboration, Harbin Institute of Technology, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","20 May 2022","2022","60","","1","16","Cloud detection is the task of detecting cloud areas in remote sensing images (RSIs), and it has attracted extensive research interest. Recently, deep learning (DL)-based methods have been proposed and achieved great performance for cloud detection. However, due to the satellite’s limitation in storage and memory, existing DL approaches, which suffer from extensive computation and large model size, are almost impossible to be deployed on satellites. To fill this gap, we target at studying effective and efficient cloud detection solutions that are suitable for satellites. In this article, we develop a lightweight autoencoder-based cloud detection method, namely, lightweight encoder–decoder cloud detection network (LWCDnet). In the encoder part, the designed novel lightweight dual-branch block (LWDBB) in the backbone extracts spatial and contextual information concurrently. Moreover, a lightweight feature pyramid module (LWFPM) is proposed to capture high-level multiscale contextual information. In the decoder part, the lightweight feature fusion module (LWFFM) compensates for the missing spatial and detail information from the encoder to the high-level feature maps. We evaluate the proposed method on two public datasets: LandSat8 and Moderate-Resolution Imaging Spectroradiometer (MODIS). Extensive experiments demonstrate that the proposed LWCDnet achieves comparable accuracy as the state-of-the-art cloud detection methods and lightweight semantic segmentation algorithms. In the meantime, LWCDnet has much less computation burden with smaller model size.","1558-0644","","10.1109/TGRS.2022.3173661","Shenzhen Science and Technology Program(grant numbers:JCYJ20210324120208022,JCYJ20200109113014456); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775756","Cloud detection;deep learning (DL);LandSat8;lightweight network;Moderate-Resolution Imaging Spectroradiometer (MODIS)","Clouds;Feature extraction;Remote sensing;Semantics;Satellites;Cloud computing;Earth","atmospheric techniques;clouds;feature extraction;geophysical image processing;geophysical signal processing;image fusion;image resolution;image segmentation;learning (artificial intelligence);object detection;remote sensing","LWCDnet;lightweight network;efficient cloud detection;remote sensing images;cloud areas;extensive research interest;deep learning-based methods;satellite;extensive computation;satellites;effective cloud;lightweight autoencoder-based cloud detection method;lightweight encoder-decoder cloud detection network;designed novel lightweight dual-branch block;lightweight feature pyramid module;high-level multiscale contextual information;lightweight feature fusion module;high-level feature maps;Moderate-Resolution Imaging Spectroradiometer;state-of-the-art cloud detection methods;lightweight semantic segmentation algorithms","","2","","56","IEEE","16 May 2022","","","IEEE","IEEE Journals"
"A Pansharpening Method Based on the Sparse Representation of Injected Details","M. R. Vicinanza; R. Restaino; G. Vivone; M. Dalla Mura; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics (DIEM), University of Salerno, Salerno, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics (DIEM), University of Salerno, Salerno, Italy; North Atlantic Treaty Organization (NATO) Science and Technology Organization (STO) Centre for Maritime Research and Experimentation, La Spezia, Italy; GIPSA-Lab, Grenoble Institute of Technology, Saint-Martin-d'Hères, France; GIPSA-Lab, University of Iceland, Reykjavík, Iceland","IEEE Geoscience and Remote Sensing Letters","14 Aug 2014","2015","12","1","180","184","The application of sparse representation (SR) theory to the fusion of multispectral (MS) and panchromatic images is giving a large impulse to this topic, which is recast as a signal reconstruction problem from a reduced number of measurements. This letter presents an effective implementation of this technique, in which the application of SR is limited to the estimation of missing details that are injected in the available MS image to enhance its spatial features. We propose an algorithm exploiting the details self-similarity through the scales and compare it with classical and recent pansharpening methods, both at reduced and full resolution. Two different data sets, acquired by the WorldView-2 and IKONOS sensors, are employed for validation, achieving remarkable results in terms of spectral and spatial quality of the fused product.","1558-0571","","10.1109/LGRS.2014.2331291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6849483","Compressed sensing;data fusion;multispectral (MS) images;pansharpening;sparse representation (SR);Compressed sensing;data fusion;multispectral (MS) images;pansharpening;sparse representation (SR)","Remote sensing;Dictionaries;Sensors;Spatial resolution;Image sensors;Indexes","image fusion;image processing;signal reconstruction","injected detail sparse representation;sparse representation theory application;SR theory application;multispectral image fusion;panchromatic image fusion;MS image fusion;signal reconstruction problem recast;reduced measurement number;technique implementation;SR application;missing detail estimation;available MS image injection;spatial feature enhancement;self-similarity detail;classical pansharpening method;reduced resolution;full resolution;WorldView-2 sensor data set;IKONOS sensor data set;fused product spectral quality;fused product spatial quality","","112","","22","IEEE","8 Jul 2014","","","IEEE","IEEE Journals"
"Multi-Layers Feature Fusion of Convolutional Neural Network for Scene Classification of Remote Sensing","C. Ma; X. Mu; D. Sha","Xi’an High Technology Research Institution, Xi’an, China; Xi’an High Technology Research Institution, Xi’an, China; Department of Earth Systems and Geoinformation Sciences, George Mason University, Fairfax, VA, USA","IEEE Access","6 Sep 2019","2019","7","","121685","121694","Remote sensing scene classification is still a challenging task in remote sensing applications. How to effectively extract features from a dataset with limited scale is crucial for improvement of scene classification. Recently, convolutional neural network (CNN) performs impressively in different fields of computer vision and has been used for remote sensing. However, most works focus on the feature maps of the last convolution layer and pay little attention to the benefits of additional layers. In fact, the feature information hidden in different layers has potential for feature discrimination capacity. The most attention of this work is how to explore the potential of multiple layers from a CNN model. Therefore, this paper proposes multi-layers feature fusion based on CNN and designs a fusion module to solve relevant issues of fusion. In this module, firstly, all the feature maps are transformed to match sizes mutually due to infeasible fusion of feature maps with different scales; then, two fusion methods are introduced to integrate feature maps from different layers instead of the last convolution layer only; finally, the fusion of features are delivered to the next layer or classifier as the routine CNN does. The experimental results show that the suggested methods achieve promising performance on public datasets.","2169-3536","","10.1109/ACCESS.2019.2936215","National Natural Science Foundation of China(grant numbers:61601475); Aeronautical Science Foundation of China(grant numbers:201555U8010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805301","Multi-layer feature fusion;convolutional neural network;scene classification;remote sensing image","Feature extraction;Remote sensing;Convolution;Semantics;Encoding;Shape;Data mining","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;remote sensing","multilayers feature fusion;convolutional neural network;remote sensing applications;CNN;convolution layer;feature information;feature discrimination capacity;fusion module;remote sensing scene classification;feature extraction;computer vision","","38","","41","CCBY","19 Aug 2019","","","IEEE","IEEE Journals"
"Multiband Remote Sensing Image Pansharpening Based on Dual-Injection Model","Y. Yang; L. Wu; S. Huang; W. Wan; W. Tu; H. Lu","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 May 2020","2020","13","","1888","1904","Pansharpening exploits the high-frequency component (HFC) of panchromatic (PAN) images to restore the spatial-resolution of the corresponding multispectral (MS) image. In this article, a dual-injection model-based multiband remote sensing image pansharpening method is presented that focuses on how to correctly use the HFC to improve the MS image for obtaining a high-spatial resolution and MS image. The model is based on a two-step HFC injection algorithm with two different injection gains. In the first step, an HFC is reconstructed with sparse theory, and an injection gain based on the relationship between PAN and MS images is developed. Employing the previous injection gain and the reconstructed HFC on an upsampling MS image, an improved LRMS (ILRMS) image is then produced. In the second step, another injection gain based on the differences and similarities between the PAN and MS images is designed. With the help of this injection gain, the fusion image is achieved via the adaptive integration of the ILRMS image and the HFC from the PAN image. Experiments confirm that the proposed method is more effective than some popular widely used pansharpening methods.","2151-1535","","10.1109/JSTARS.2020.2981975","National Natural Science Foundation of China(grant numbers:61662026,61862030); Natural Science Foundation of Jiangxi Province(grant numbers:20182BCB22006,20181BAB202010,20192 ACB20002,0192ACBL21008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050909","Dual-injection model;injection gains;pansharpening;sparse representation (SR)","Hybrid fiber coaxial cables;Spatial resolution;Dictionaries;Remote sensing;Image reconstruction","geophysical image processing;image fusion;image resolution;remote sensing","sparse theory;panchromatic images;high-frequency component;PAN image;ILRMS image;fusion image;improved LRMS image;upsampling MS image;injection gain;two-step HFC injection algorithm;high-spatial resolution;dual-injection model-based multiband remote sensing image pansharpening method;multispectral image","","8","","45","CCBY","30 Mar 2020","","","IEEE","IEEE Journals"
"Multicascaded Feature Fusion-Based Deep Learning Network for Local Climate Zone Classification Based on the So2Sat LCZ42 Benchmark Dataset","W. Ji; Y. Chen; K. Li; X. Dai","State Key Laboratory of Remote Sensing Science, Faculty of Geographical Science, Beijing Normal University, Beijing, China; Beijing Key Laboratory of Environment Remote Sensing and Digital Cities, Beijing Normal University, Beijing, China; State Key Laboratory of Remote Sensing Science, Faculty of Geographical Science, Beijing Normal University, Beijing, China; State Key Laboratory of Remote Sensing Science, Faculty of Geographical Science, Beijing Normal University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Dec 2022","2023","16","","449","467","A detailed investigation of the microclimate is beneficial for optimizing the urban inner/spatial pattern to enhance thermal comfort or even reduce heatwave disasters, whereas accurately classifying local climate zones (LCZs) severely restricts analysis of thermal characterization. Generally, deep learning-based approaches are effective for adaptive LCZ mapping, yet they often have poor accuracy because inadequate cascade feature extraction patterns may not adapt to the fuzzy LCZ boundaries produced by intricate urban textures, especially when using large-scale datasets. To address these issues, we propose a novel CNN model in which we design a strategy that incorporates a triple feature fusion pattern to enhance LCZ recognition based on the So2sat LCZ 42 large-scale annotated dataset. The approach connects multilayer cascaded information to participate in category judgment, which avoids the loss of effective feature information via continuous cascade transformation as much as possible. The results show that the overall accuracy and kappa coefficient of the proposed model reach 0.70 and 0.68, respectively, manifesting an improvement of approximately 4.47% and 6.25% over advanced LCZ classification approaches. In particular, the accuracy of the proposed approach improves even further after the fusion structure or layer depth is partially removed or reduced, respectively. Finally, we discuss several items, including the effectiveness of different parameters and cascaded feature fusion patterns, the superiority of multilayer cascade feature fusion, the mapping impact of seasons and cloud cover, and even some other issues in LCZ mapping. This article will facilitate improvements in the research precision of urban thermal environments.","2151-1535","","10.1109/JSTARS.2022.3226524","National Natural Science Foundation of China(grant numbers:42171316,72171128); Beijing Laboratory of Water Resources Security; BNU Interdisciplinary Research Foundation(grant numbers:BNUXKJC2105); Open Fund of State Key Laboratory of Remote Sensing Science and Beijing Engineering Research Center(grant numbers:OF202204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9969870","Deep learning;local climate zones;land surface temperature;remote sensing;urban heat island","Feature extraction;Remote sensing;Meteorology;Convolutional neural networks;Semantic segmentation;Convolution;Urban areas","feature extraction;fuzzy set theory;image classification;image fusion;learning (artificial intelligence);neural nets;optimisation","adaptive LCZ mapping;advanced LCZ classification approaches;continuous cascade transformation;deep learning-based approaches;effective feature information;feature fusion patterns;fusion structure;fuzzy LCZ boundaries;heatwave disasters;inadequate cascade feature extraction patterns;intricate urban textures;large-scale annotated dataset;large-scale datasets;LCZ recognition;local climate zone classification;local climate zones severely;multicascaded feature fusion-based deep learning network;multilayer cascade;multilayer cascaded information;So2Sat LCZ42 Benchmark Dataset;thermal characterization;thermal comfort;triple feature fusion pattern;urban thermal environments","","1","","41","CCBY","5 Dec 2022","","","IEEE","IEEE Journals"
"A Multi-Cooperative Deep Convolutional Neural Network for Spatiotemporal Satellite Image Fusion","W. Li; C. Yang; Y. Peng; X. Zhang","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Oct 2021","2021","14","","10174","10188","Remote sensing satellite images with high temporal and high spatial resolution play a critical role in earth science applications. However, it is difficult for a single satellite to obtain such images due to technical and cost constraints. Therefore, spatiotemporal image fusion based on deep learning has received extensive attention in recent years. This article proposes a multicooperative deep convolutional neural network (MCDNet) for spatiotemporal satellite image fusion. This method is a new multinetwork model in which multiple networks work together to reconstruct the predicted image. The multinetwork model consists of a super-resolution network, a difference reconstruction network, and a collaborative training network. First, the super-resolution network uses the combination of a novel multiscale mechanism and dilated convolutions to make full use of the spectral information of the coarse image and upgrade it to a transitional image that matches the fine image. The difference reconstruction network uses structural relevance to complete the reconstruction of the fine difference image. The collaborative training network extracts the hidden information from the fine image and uses the time relevance to restrict the training of the difference reconstruction network. Finally, the fine difference image and the known fine image are combined to complete the image fusion. The new compound loss function can help multinetwork models better complete cooperative training. Through experiments on two datasets and comparison with existing fusion algorithms, the subjective and objective results prove that MCDNet can effectively reconstruct higher-quality prediction images.","2151-1535","","10.1109/JSTARS.2021.3113163","National Natural Science Foundation of China(grant numbers:61972060,U1713213,62027827); National Key R&D Program of China(grant numbers:2019YFE0110800); Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0025,cstc2019cxcyljrc-td0270); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540272","Convolutional neural network (CNN);dilated convolution;multiscale mechanism;spatiotemporal fusion","Image reconstruction;Convolutional neural networks;Spatiotemporal phenomena;Spatial resolution;Feature extraction;Satellites;Training","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image fusion;image reconstruction;image resolution;remote sensing","multicooperative deep convolutional neural network;spatiotemporal satellite image fusion;remote sensing satellite images;temporal resolution;spatial resolution;deep learning;super-resolution network;difference reconstruction network","","4","","45","CCBY","16 Sep 2021","","","IEEE","IEEE Journals"
"A Novel Utilization of Image Registration Techniques to Process Mastcam Images in Mars Rover With Applications to Image Fusion, Pixel Clustering, and Anomaly Detection","B. Ayhan; M. Dao; C. Kwan; H. -M. Chen; J. F. Bell; R. Kidd","Applied Research LLC, Rockville, MD, USA; Applied Research LLC, Rockville, MD, USA; Applied Research LLC, Rockville, MD, USA; Applied Research LLC, Rockville, MD, USA; Arizona State University, Tempe, AZ, USA; Jet Propulsion Laboratory, Pasadena, CA, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Oct 2017","2017","10","10","4553","4564","The Mars Science Laboratory is a robotic rover mission to Mars launched by NASA on November 26, 2011, which successfully landed the Curiosity rover in Gale Crater on August 6, 2012. The Curiosity rover has two mast cameras (Mastcams) that acquire stereo images at a number of different wavelengths. Each camera has nine bands of which six bands are overlapped in the two cameras. These acquired stereo band images at different wavelengths can be fused into a 12-band multispectral image cube, which could be helpful to guide the rover to interesting locations. Since the two Mastcams' fields of view are three times different from each other, in order to fuse the left- and right-camera band images to form a multispectral image cube, there is a need for a precise image alignment of the stereo images with registration errors at the subpixel level. A two-step image alignment approach with a novel utilization of existing image registration algorithms is introduced in this paper and is applied to a set of Mastcam stereo images. The effect of the two-step alignment approach using more than 100 pairs of Mastcam images, selected from over 500000 images in NASA's Planetary Data System database, clearly demonstrated that the fused images can improve pixel clustering and anomaly detection performance. In particular, registration errors in the subpixel level are observed with the applied alignment approach. Moreover, the pixel clustering and anomaly detection performance have been observed to be better when using fused images.","2151-1535","","10.1109/JSTARS.2017.2716923","NASA(grant numbers:NNX12CB05C,NNX16CP38P); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7984783","Anomaly detection;image fusion;image registration;Mars rover;multispectral image;pixel clustering;RANSAC;SIFT;SURF","Image registration;Cameras;Feature extraction;Spatial resolution;Space vehicles;Image fusion","cameras;image fusion;image registration;image resolution;Mars;pattern clustering;planetary rovers;planetary surfaces;spectral analysis;statistical analysis;stereo image processing","left- camera band images;right-camera band images;image registration algorithms;Mastcam stereo images;fused images;pixel clustering;Mars rover;image fusion;Mars Science Laboratory;robotic rover mission;Curiosity rover;mast cameras;image alignment;anomaly detection;multispectral image cube;Mastcams fields;NASA;Gale Crater;Planetary Data System","","58","","35","IEEE","19 Jul 2017","","","IEEE","IEEE Journals"
"Spatiotemporal Fusion With Only Two Remote Sensing Images as Input","J. Wu; Q. Cheng; H. Li; S. Li; X. Guan; H. Shen","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences and the Collaborative Innovation Center for Geospatial Technology, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Oct 2020","2020","13","","6206","6219","Spatiotemporal data fusion is an effective way of generating a dense time series with a high spatial resolution. Traditionally, the spatiotemporal fusion models, especially the popular ones such as the spatial and temporal adaptive reflectance fusion model, require at least three images as input, i.e., a coarse-resolution image on the target date and a pair of fine- and coarse-resolution images on the reference date. However, this cannot always be satisfied, as the high-quality coarse-resolution image on the reference date may be unavailable in some application scenarios. This led to efforts to achieve data fusion only using the other two images as input. In this article, we proposed an effective strategy that can be combined with any spatiotemporal fusion model to accomplish the fusion with simplified input. To confirm the validity of the method, we comprehensively compared the fusion performances under the two input modalities. In total, 38 tests were conducted with Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat, and Sentinel-2 land surface reflectance products. Results suggest that by applying the proposed method, the fusion performance with only two input images is comparable or even superior to that with three input images. This article challenges the stereotype that spatiotemporal data fusion strictly needs at least three input images. The proposed method extends the application scenarios of spatiotemporal fusion, and creates opportunities to fuse sensors with barely overlapping temporal coverages, such as the Landsat 8 Operational Land Imager and the Sentinel-2 MultiSpectral Instrument.","2151-1535","","10.1109/JSTARS.2020.3028116","National Key R&D Program of China(grant numbers:2018YFB2100501); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210773","Landsat 8;Moderate Resolution Imaging Spectroradiometer (MODIS);Sentinel-2;simplified input modality;spatiotemporal data fusion;two input images","Spatiotemporal phenomena;Spatial resolution;Remote sensing;Sensors;Artificial satellites;Earth;Data integration","geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion;spatiotemporal phenomena;time series","input modalities;Sentinel-2 land surface reflectance products;spatiotemporal data fusion;Landsat 8 Operational Land Imager;remote sensing images;high spatial resolution;spatial reflectance fusion model;temporal adaptive reflectance fusion model;target date;reference date;high-quality coarse-resolution image;spatiotemporal fusion model","","6","","51","CCBY","1 Oct 2020","","","IEEE","IEEE Journals"
"Soft-Then-Hard Super-Resolution Mapping Based on Pansharpening Technique for Remote Sensing Image","P. Wang; M. Dalla Mura; J. Chanussot; G. Zhang","College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Grenoble Images Parole Signals Automatics Laboratory, Grenoble Institute of Technology, Grenoble, France; Grenoble Images Parole Signals Automatics Laboratory, Grenoble Institute of Technology, Grenoble, France; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Jan 2019","2019","12","1","334","344","Super-resolution mapping (SRM) technique can explore the spatial distribution information of land cover classes in mixed pixels for multispectral image (MSI) or hyspectral image (HSI). Soft-then-hard super-resolution mapping (STHSRM) is an important type of SRM technique. STHSRM first utilizes the subpixel sharpening to produce the high-resolution fractional images with the soft attribute values for each subpixel and then allocates the hard class labels to each subpixel. However, due to the low resolution in the original image, the fractional images are difficult to pick up the full spatial-spectral information from the original image. In this paper, pansharpening technique is utilized in STHSRM (STHSRM-PAN) to produce the fractional images with more spatial-spectral information, which improves the mapping results. First, the original low-resolution MSI or HSI and a panchromatic image (PAN) are fused by pansharpening technique to produce the improved resolution image with the high spectral resolution of MSI or HSI and the high spatial resolution of PAN. The high-resolution fractional images with more spatial-spectral information are then obtained by unmixing the improved resolution image. Finally, the class labels are assigned to each subpixel according to the soft attribute values from the high-resolution fractional images. Comparing with the state-of-the-art STHSRM algorithms, the STHSRM-PAN shows the best performance with the percentage correctly classified and Kappa coefficient (Kappa) in the three experimental results.","2151-1535","","10.1109/JSTARS.2018.2885793","National Natural Science Foundation of China(grant numbers:61801211,61871218,61501233,61501228); Fundamental Research Funds for the Central Universities(grant numbers:1004-,YAH18050,3082017NP2017421); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8584054","Pansharpening;remote sensing image;soft-then-hard super-resolution mapping;spatial-spectral information","Spatial resolution;Principal component analysis;Remote sensing;Correlation;Graphical models;Distribution functions","geophysical image processing;geophysical techniques;geophysics computing;image classification;image fusion;image resolution;land cover;remote sensing;terrain mapping","soft-then-hard super-resolution mapping;pansharpening technique;remote sensing image;super-resolution mapping technique;spatial distribution information;hyspectral image;SRM technique;high-resolution fractional images;spatial-spectral information;STHSRM-PAN;low-resolution MSI;panchromatic image;high spectral resolution;high spatial resolution;HSI","","16","","53","IEEE","20 Dec 2018","","","IEEE","IEEE Journals"
"Stagewise Unsupervised Domain Adaptation With Adversarial Self-Training for Road Segmentation of Remote-Sensing Images","L. Zhang; M. Lan; J. Zhang; D. Tao","School of Computer Science, Institute of Artificial Intelligence, Wuhan University, Wuhan, China; School of Computer Science, Institute of Artificial Intelligence, Wuhan University, Wuhan, China; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; JD Explore Academy, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","13","Road segmentation from remote-sensing images is a challenging task with wide ranges of application potentials. Deep neural networks have advanced this field by leveraging the power of large-scale labeled data, which, however, are extremely expensive and time-consuming to acquire. One solution is to use cheap available data to train a model and deploy it to directly process the data from a specific application domain. Nevertheless, the well-known domain shift (DS) issue prevents the trained model from generalizing well on the target domain. In this article, we propose a novel stagewise domain adaptation model called RoadDA to address the DS issue in this field. In the first stage, RoadDA adapts the target domain features to align with the source ones via generative adversarial networks (GANs)-based interdomain adaptation. Specifically, a feature pyramid fusion module is devised to avoid information loss of long and thin roads and learn discriminative and robust features. Besides, to address the intradomain discrepancy in the target domain, in the second stage, we propose an adversarial self-training method. We generate the pseudo labels of the target domain using the trained generator and divide it to labeled easy split and unlabeled hard split based on the road confidence scores. The features of hard split are adapted to align with the easy ones using adversarial learning and the intradomain adaptation process is repeated to progressively improve the segmentation performance. Experiment results on two benchmarks demonstrate that RoadDA can efficiently reduce the domain gap and outperforms state-of-the-art methods. The code is available at https://github.com/LANMNG/RoadDA.","1558-0644","","10.1109/TGRS.2021.3104032","National Natural Science Foundation of China(grant numbers:62076188); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)(grant numbers:2019AEA170); Fundamental Research Funds for the Central Universities(grant numbers:2042021kf0196); supercomputing system in the Supercomputing Center of Wuhan University; Australian Research Council (ARC)(grant numbers:FL-170100117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516689","Remote sensing (RS);road segmentation;self-training;unsupervised domain adaptation (UDA)","Roads;Image segmentation;Adaptation models;Task analysis;Data models;Feature extraction;Predictive models","feature extraction;image fusion;image segmentation;learning (artificial intelligence);neural nets;remote sensing;road traffic;traffic engineering computing","deep neural networks;RoadDA;DS;generative adversarial networks-based interdomain adaptation;feature pyramid fusion module;road confidence scores;adversarial learning;intradomain adaptation process;road segmentation;remote-sensing images;stagewise unsupervised domain adaptation;adversarial self-training","","32","","49","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Multiple Kernel Learning for Remote Sensing Image Classification","S. Niazmardi; B. Demir; L. Bruzzone; A. Safari; S. Homayouni","School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; School of Surveying and Geospatial Engineering, College of Engineering, University of Tehran, Tehran, Iran; Department of Geography, Environmental Studies and Geomatics, University of Ottawa, Ottawa, ON, Canada","IEEE Transactions on Geoscience and Remote Sensing","27 Feb 2018","2018","56","3","1425","1443","This paper presents multiple kernel learning (MKL) in the context of remote sensing (RS) image classification problems by illustrating main characteristics of different MKL algorithms and analyzing their properties in RS domain. A categorization of different MKL algorithms is initially introduced, and some promising MKL algorithms for each category are presented. In particular, MKL algorithms presented only in machine learning are introduced in RS. Then, the investigated MKL algorithms are theoretically compared in terms of their: 1) computational complexities; 2) accuracy with different qualities of kernels; and 3) accuracy with different numbers of kernels. After the theoretical comparison, experimental analyses are carried out to compare different MKL algorithms in terms of: 1) model selection and 2) feature fusion problems. On the basis of the theoretical and experimental analyses of MKL algorithms, some guidelines for a proper selection of the MKL algorithms are derived.","1558-0644","","10.1109/TGRS.2017.2762597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101572","Feature fusion;kernel-based classification;multiple kernel learning (MKL);remote sensing (RS)","Kernel;Algorithm design and analysis;Machine learning algorithms;Optimization;Classification algorithms;Computational complexity;Support vector machines","image classification;image fusion;learning (artificial intelligence);remote sensing","multiple kernel learning;MKL algorithms;remote sensing image classification;machine learning;feature fusion","","27","","56","IEEE","9 Nov 2017","","","IEEE","IEEE Journals"
"ICIF-Net: Intra-Scale Cross-Interaction and Inter-Scale Feature Fusion Network for Bitemporal Remote Sensing Images Change Detection","Y. Feng; H. Xu; J. Jiang; H. Liu; J. Zheng","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","2 May 2022","2022","60","","1","13","Change detection (CD) of remote sensing (RS) images has enjoyed remarkable success by virtue of convolutional neural networks (CNNs) with promising discriminative capabilities. However, CNNs lack the capability of modeling long-range dependencies in bitemporal image pairs, resulting in inferior identifiability against the same semantic targets yet with varying features. The recently thriving Transformer, on the contrary, is warranted, for practice, with global receptive fields. To jointly harvest the local-global features and circumvent the misalignment issues caused by step-by-step downsampling operations in traditional backbone networks, we propose an intra-scale cross-interaction and inter-scale feature fusion network (ICIF-Net), explicitly tapping the potential of integrating CNN and Transformer. In particular, the local features and global features, respectively, extracted by CNN and Transformer, are interactively communicated at the same spatial resolution using a linearized Conv Attention module, which motivates the counterpart to glimpse the representation of another branch while preserving its own features. In addition, with the introduction of two attention-based inter-scale fusion schemes, including mask-based aggregation and spatial alignment (SA), information integration is enforced at different resolutions. Finally, the integrated features are fed into a conventional change prediction head to generate the output. Extensive experiments conducted on four CD datasets of bitemporal (RS) images demonstrate that our ICIF-Net surpasses the other state-of-the-art (SOTA) approaches.","1558-0644","","10.1109/TGRS.2022.3168331","National Key Research and Development Program of China(grant numbers:2018YFE0126100); Natural Science Foundation of China(grant numbers:61602413); Open Research Projects of Zhejiang Laboratory(grant numbers:2019KD0AD01/007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9759285","Change detection (CD);convolutional neural network (CNN);inter-scale feature fusion;intra-scale cross-interaction;remote sensing (RS);Transformer","Feature extraction;Transformers;Semantics;Convolutional neural networks;Image segmentation;Context modeling;Task analysis","feature extraction;filtering theory;geophysical image processing;image classification;image fusion;image resolution;neural nets;remote sensing","integrated features;bitemporal images;ICIF-Net surpasses;intra-scale cross-interaction;inter-scale feature fusion network;bitemporal remote sensing images change detection;convolutional neural networks;CNNs;bitemporal image pairs;recently thriving Transformer;global receptive fields;local-global features;step-by-step downsampling operations;traditional backbone networks;local features;attention-based inter-scale fusion schemes","","7","","53","IEEE","18 Apr 2022","","","IEEE","IEEE Journals"
"Semantic Joint Monocular Remote Sensing Image Digital Surface Model Reconstruction Based on Feature Multiplexing and Inpainting","J. Lu; Q. Hu","Beihang Hangzhou Innovation Institute Yuhang, Hangzhou, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","3 Jun 2022","2022","60","","1","15","Digital surface model (DSM) presents height information of the Earth’s surface and plays an important role in many remote sensing (RS) applications. Since the conventional acquisition of DSM is laborious and expensive, DSM reconstruction from monocular RS images has attracted extensive research in recent years, which is an ill-posed problem and thus rather challenging. Related works have achieved great accomplishments in this regard; however, they still face some limitations in training robustness, accuracy, and efficiency. To address the issues, a semantic joint monocular RS image DSM regression framework is proposed in this article, whose salient points include that: 1) semantic segmentation is integrated into the DSM regression task so that a shared backbone can extract complementary features from each objective to improve the performance of the individual task. Meanwhile, based on the consistency of the two training objectives, a two-stage joint loss function is introduced to improve the convergence and robustness of model training; 2) an encoding–decoding backbone is designed based on feature multiplexing, which simultaneously achieves multiscale feature fusion and information decoupling, thereby greatly reducing model parameters and improving efficiency while ensuring feature extraction effect; and 3) an iterative upsampling approach is introduced to transform the full-scale spatial features into large receptive-field and locally discriminative dynamic kernels, which are used to inpaint coarse-grained features while decoding, thus enhancing regression accuracy. Finally, experiments demonstrate the effectiveness of the proposal. It is easy to train and achieves superior or comparable accuracy compared with state-of-the-art related works while improving the efficiency by a large margin.","1558-0644","","10.1109/TGRS.2022.3176670","National Natural Science Foundation of China(grant numbers:61960206011); Beijing Natural Science Foundation(grant numbers:JQ19017); Zhejiang Provincial Natural Science Foundation(grant numbers:LD22E050004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779230","Digital surface model (DSM) reconstruction;feature inpainting upsampling (FIU);feature multiplexing;joint multitask;monocular remote sensing (RS) image","Feature extraction;Semantics;Training;Decoding;Task analysis;Predictive models;Robustness","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;image segmentation;regression analysis;remote sensing","two-stage joint loss function;feature multiplexing;multiscale feature fusion;information decoupling;feature extraction;coarse-grained features;semantic joint monocular remote sensing image digital surface model reconstruction;Earth surface;semantic segmentation;encoding-decoding backbone;dynamic kernels;feature inpainting;semantic joint monocular RS image DSM regression framework;iterative upsampling approach;locally discriminative dynamic kernels","","","","42","IEEE","20 May 2022","","","IEEE","IEEE Journals"
"Transfer Representation Learning Meets Multimodal Fusion Classification for Remote Sensing Images","M. Ma; W. Ma; L. Jiao; X. Liu; F. Liu; L. Li; S. Yang; B. Hou","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xian, China","IEEE Transactions on Geoscience and Remote Sensing","31 Oct 2022","2022","60","","1","15","To maximize the complementary advantages of synergistic multimodal, a transfer representation learning fusion network (TRLF-Net) is proposed for multisource remote sensing images collaborative classification in this article. First, with respect to the feature encoding, we design a dual-branch attention sparse transfer module (DAST-Module), which combines the spatial and channel attention (CA) masks to migrate the advantage attributes of the panchromatic (PAN) and the MS images mutually. This not only enhances their respective image advantages but also facilitates the sparse fusion of low-level features. Second, for the separation of multiscale information, a deep dual-scale decomposition module (DDSD-Module) is designed, which allows the decompose of high-frequency and low-frequency components. Then it uses the decomposed information to make the essential difference as small as possible, and the surrounding contour difference is as large as possible of the complementary multimodal image through the design of the loss function. Finally, to address the problem of large intraclass and small interclass differences, we develop a representation fusion of the global and local features’ module (RFGAL-Module). It mainly adopts global features to sort local features within classes, and then outputs them in a cascade. Thus, the characterization ability of features is improved, and the global and local features are used in a coordinated manner to accomplish the sample classification tasks. In particular, the experimental results demonstrate that TRLF-Net can obtain much improved accuracy and efficiency. The code is accessible in: https://github.com/ru-willow/SRLF-Net.","1558-0644","","10.1109/TGRS.2022.3215177","Key Scientific Technological Innovation Research Project by Ministry of Education; National Natural Science Foundation of China Innovation Research Group Fund(grant numbers:61621005); State Key Program and the Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61836009); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103,91838303); National Natural Science Foundation of China(grant numbers:U1701267,62076192,62006177,61902298,61573267,61906150,62276199); Higher Education Discipline Innovation Project; Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT 15R53); ST Innovation Project from the Chinese Ministry of Education; Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-06); National Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ-659,2022JQ-607); China Postdoctoral fund(grant numbers:2022T150506); Scientific Research Project of Education Department In Shaanxi Province of China(grant numbers:20JY023); Fundamental Research Funds for the Central Universities(grant numbers:XJS201901,XJS201903,JBF201905,JB211908); Chinese Association for Artificial Intelligence (CAAI)-Huawei MindSpore Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921217","Attention sparse transfer (AST);deep learning;dual-scale decomposition;feature representation;multiresolution classification;remote sensing","Feature extraction;Spatial resolution;Sensors;Semantics;Representation learning;Deep learning;Convolutional neural networks","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","transfer representation learning meets multimodal fusion classification;complementary advantages;fusion network;TRLF-Net;multisource remote sensing images collaborative classification;feature encoding;dual-branch attention sparse transfer module;DAST-Module;MS images;respective image advantages;sparse fusion;low-level features;multiscale information;dual-scale decomposition module;DDSD-Module;low-frequency components;decomposed information;essential difference;surrounding contour difference;complementary multimodal image;interclass differences;representation fusion;global features;local features;sample classification tasks","","","","51","IEEE","17 Oct 2022","","","IEEE","IEEE Journals"
"Assessment of Hyperspectral Sharpening Methods for the Monitoring of Natural Areas Using Multiplatform Remote Sensing Imagery","J. Marcello; E. Ibarrola-Ulzurrun; C. Gonzalo-Martín; J. Chanussot; G. Vivone","Unidad Asociada ULPGC-CSIC, Instituto de Oceanografía y Cambio Global, Universidad de Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; Department of Computer Architecture and Technology, Universidad Politécnica de Madrid, Campus de Montegancedo, Madrid, Spain; Department of Computer Architecture and Technology, Universidad Politécnica de Madrid, Campus de Montegancedo, Madrid, Spain; Grenoble Institute of Engineering, GIPSA-Lab, Université Grenoble Alpes, CNRS, Grenoble, France; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy","IEEE Transactions on Geoscience and Remote Sensing","25 Sep 2019","2019","57","10","8208","8222","The use of cutting-edge geospatial technologies to monitor ecosystems and the development of tailored tools for assessing such natural areas is a fundamental task. In this context, the growing availability of hyperspectral (HS) imagery from satellite and aerial platforms can provide valuable information for the sustainable management of ecosystems. However, in some cases, the spectral richness provided by HS sensors is at the expense of spatial quality. To alleviate this inconvenience, which can be critical to monitor some heterogeneous and mixed natural areas, a number of HS sharpening techniques have been developed to increase the spatial resolution while trying to preserve the spectral content. This image processing field has attracted the interest of the scientific community, and many research studies have been conducted to assess the performance of different HS sharpening algorithms. In the last decade, however, many comparative studies rely upon simulated data. In this work, the challenging application of sharpening methods in real situations using multiplatform or multisensor data is also addressed. Thus, experiments with real data have been conducted, in addition to a thorough assessment of HS sharpening techniques using simulated imagery in scenarios with different spatial resolution ratios and registration errors. In particular, airborne and satellite HS imageries have been pansharpened with drone, orthophotos, and satellite high spatial resolution data evaluating 11 fusion algorithms. After a comprehensive analysis, considering different visual and quantitative quality indicators, the algorithm characteristics have been summarized and the methods with higher performance and robustness have been identified.","1558-0644","","10.1109/TGRS.2019.2918932","ARTEMISAT-2 Project; Spanish AEI and FEDER funds(grant numbers:CTM2016-77733-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736496","CASI;classification;Hyperion;hyperspectral (HS);image fusion;orthophotos;pansharpening","Spatial resolution;Sensors;Satellites;Hyperspectral imaging;Monitoring","ecology;geophysical image processing;geophysics computing;image fusion;image processing;image registration;image resolution;remote sensing;sensor fusion;terrain mapping","ecosystems;spectral richness;HS sensors;spatial quality;heterogeneous areas;mixed natural areas;HS sharpening techniques;spectral content;image processing field;different HS sharpening algorithms;multiplatform data;multisensor data;simulated imagery;different spatial resolution ratios;particular HS imageries;airborne HS imageries;satellite HS imageries;satellite high spatial resolution data;different visual quality indicators;quantitative quality indicators;hyperspectral sharpening methods;multiplatform remote sensing imagery;cutting-edge geospatial technologies;tailored tools;growing availability;hyperspectral imagery;aerial platforms;sustainable management","","13","","62","IEEE","13 Jun 2019","","","IEEE","IEEE Journals"
"Panchromatic and Hyperspectral Image Fusion: Outcome of the 2022 WHISPERS Hyperspectral Pansharpening Challenge","G. Vivone; A. Garzelli; Y. Xu; W. Liao; J. Chanussot","Institute of Methodologies for Environmental Analysis, National Research Council-IMAA, Tito, Italy; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Flanders Make and Ghent University, Kortrijk, Belgium; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Dec 2022","2023","16","","166","179","This article presents the scientific outcomes of the 2022 Hyperspectral Pansharpening Challenge organized by the 12th IEEE Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (IEEE WHISPERS 2022). The 2022 Hyperspectral Pansharpening Challenge aims at fusing a panchromatic image with hyperspectral data to get a high spatial resolution hyperspectral cube with the same spatial resolution of the panchromatic image while preserving the spectral information of hyperspectral data. Four datasets acquired by the PRISMA mission owned and managed by the Italian Space Agency have been prepared for participants. They are made available for the benefit of the scientific community. Each dataset contains a panchromatic image and a hyperspectral cube with different spatial resolutions. More than 100 registrations have been received for the event. Four teams submitted their outcomes. Since no team actually outperformed the baseline provided by the organizers, the challenge was declared inconclusive and no winner was recognized.","2151-1535","","10.1109/JSTARS.2022.3220974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944883","Hyperspectral imaging;image fusion;optical imaging;PRISMA images;pansharpening;remote sensing;resolution enhancement","Pansharpening;Spatial resolution;Indexes;Visualization;Space missions;Sensor phenomena and characterization;Optical sensors","geophysical image processing;geophysical signal processing;hyperspectral imaging;image fusion;image registration;image resolution;image sensors;remote sensing;sensor fusion","2022 Hyperspectral Pansharpening Challenge;2022 WHISPERS Hyperspectral Pansharpening Challenge;high spatial resolution hyperspectral cube;hyperspectral data;Hyperspectral Image;IEEE WHISPERS 2022;panchromatic image;scientific outcomes","","1","","45","CCBY","10 Nov 2022","","","IEEE","IEEE Journals"
"Super-Resolution Reconstruction Method of Remote Sensing Image Based on Multi-Feature Fusion","Z. -X. Huang; C. -W. Jing","Zhejiang Yijia Geographic Information Technology Company, Ltd., Hangzhou, China; Hangzhou Normal University, Hangzhou, China","IEEE Access","30 Jan 2020","2020","8","","18764","18771","The acquisition of remote sensing images is affected by imaging equipment and environmental conditions. Usually on lower performance devices, the resolution of the acquired images is also low. Among many methods, the super-resolution reconstruction method based on generative adversarial networks has obvious advantages over previous network models in reconstructing image texture details. However, it is found in experiments that not all of these reconstructed textures exist in the image itself. Aiming at the problem of whether the texture details of the reconstructed image are accurate and clear, we propose a super-resolution reconstruction method combining wavelet transform and generative adversarial network. Using wavelet multi-resolution analysis, training wavelet decomposition coefficients in the generative adversarial network can effectively improve the local detail information of the reconstructed image. Experimental results show that our method can effectively reconstruct more natural image textures and make the images more visually clear. In the remote sensing image test set, the four indicators of the algorithm, peak signal to noise ratio (PSNR), structural similarity (SSIM), Feature Similarity (FSIM) and Universal Image Quality (UIQ) are slightly better than the algorithms mentioned in the article.","2169-3536","","10.1109/ACCESS.2020.2967804","Key Special Projects through the Provincial Scientific Research Institutes Program of Zhejiang Province, China(grant numbers:2014F50022); Construction of Agricultural Science Park Program of Zhejiang Province, China(grant numbers:2019E70002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963714","Remote sensing image;super-resolution;self-correlation;image texture","Image reconstruction;Wavelet transforms;Reconstruction algorithms;Remote sensing;Generative adversarial networks","image fusion;image reconstruction;image resolution;image texture;neural nets;remote sensing;wavelet transforms","wavelet multiresolution analysis;generative adversarial network;reconstructed image;natural image textures;remote sensing image test;super-resolution reconstruction method;remote sensing images;imaging equipment;image texture details;reconstructed textures;universal image quality;multi-feature fusion;environmental conditions;wavelet transform;training wavelet decomposition coefficients;peak signal to noise ratio;structural similarity;feature similarity","","11","","27","CCBY","20 Jan 2020","","","IEEE","IEEE Journals"
"Context-Based Oriented Object Detector for Small Objects in Remote Sensing Imagery","Q. Jiang; J. Dai; T. Rui; F. Shao; G. Lu; J. Wang","Department of Mechanical Engineering, College of Field Engineering, Army Engineering University of PLA, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University of PLA, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University of PLA, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University of PLA, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University of PLA, Nanjing, China; Department of Mechanical Engineering, College of Field Engineering, Army Engineering University of PLA, Nanjing, China","IEEE Access","28 Sep 2022","2022","10","","100526","100539","Object detection in remote sensing imagery is a challenging task in the field of computer vision and has high research value. To improve the classification accuracy and positioning accuracy of object detection, we propose a new multi-scale oriented object detector suitable for small objects. Firstly, the feature fusion network based on information balance (IBFF) is proposed to reduce the reuse of different layers’ features from the backbone network and reduce the interference of redundant information based on the premise that the output features have sufficient information, and retain enough shallow detail information. Secondly, to efficiently utilize deep and shallow features, enhance important features, and reduce background noise interference, different attention-based context feature fusion modules (DACFF) are designed according to the characteristics of different feature fusion stages. Finally, an improved strategy of oriented bounding box regression is proposed to obtain the oriented bounding box with a simpler and more effective strategy. The proposed method was evaluated on two public remote sensing datasets, DOTA and HRSC2016, and their mAP values are 80.96% and 95.01%, respectively, which verified the effectiveness of the proposed algorithm.","2169-3536","","10.1109/ACCESS.2022.3204622","National Natural Science Foundation of China(grant numbers:61671470); China National Key Research and Development Program(grant numbers:2016YFC0802904); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878072","Object detection;remote sensing imagery;feature fusion;attentional mechanism","Feature extraction;Object detection;Remote sensing;Detectors;Semantics;Data mining;Optical sensors;Image processing","computer vision;feature extraction;geophysical image processing;image classification;image fusion;object detection;regression analysis;remote sensing","classification accuracy;positioning accuracy;object detection;multiscale oriented object detector;feature fusion network;information balance;different layers;backbone network;redundant information;output features;sufficient information;shallow detail information;deep features;shallow features;background noise interference;different attention-based context feature fusion modules;different feature fusion stages;oriented bounding box regression;public remote sensing datasets;context-based oriented object detector;remote sensing imagery;computer vision;high research value","","","","60","CCBY","5 Sep 2022","","","IEEE","IEEE Journals"
"Enblending Mosaicked Remote Sensing Images With Spatiotemporal Fusion of Convolutional Neural Networks","J. Wei; W. Tang; C. He","Institute of Space Science and Technology, Nanchang University, Nanchang, China; Institute of Space Science and Technology, Nanchang University, Nanchang, China; Institute of Space Science and Technology, Nanchang University, Nanchang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Jun 2021","2021","14","","5891","5902","Mosaicking of remote sensing images stitches images of different moments or sensors to produce a new image under a uniform geographic coordinate system. In a mosaicking process, the critical enblending operation is divided into color balance, seamline finder, and fusion of overlapping areas, which is still challenging to maintain color consistency and data fidelity. In this article, a new mosaicking framework using spatiotemporal fusion is proposed to solve the enblending issue. Two additional low-resolution reference images are introduced for each mosaicking image. With spatiotemporal fusion methods, all mosaicking images are reconstructed to a uniform time, then the combination of overlapping areas become easy. Furthermore, a new spatiotemporal fusion method is proposed by cascading enhanced deep neural networks to fuse images quickly and effectively. In the validation procedure, the proposed method is compared with eight color harmony methods or tools by mosaicking the red, green, and blue bands of Landsat-8 images with images from the moderate-resolution imaging spectroradiometer as the reference. The digital evaluations and visual comparisons demonstrate that the newly method outweighs majority methods regarding to the radiometric, structural, and spectral fidelity, which proves the feasibility of our new enblending method.","2151-1535","","10.1109/JSTARS.2021.3082619","National Natural Science Foundation of China(grant numbers:61860130,41974195); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439158","Color harmony;convolutional neural networks (CNNs);enblend;Landsat;mosaicking;spatiotemporal fusion","Spatiotemporal phenomena;Image color analysis;Remote sensing;Earth;Radiometry;Artificial satellites;Spatial resolution","geophysical image processing;image fusion;image resolution;image segmentation;neural nets;remote sensing","mosaicking image;overlapping areas;spatiotemporal fusion method;enhanced deep neural networks;color harmony methods;Landsat-8 images;moderate-resolution imaging spectroradiometer;enblending mosaicked remote sensing images;convolutional neural networks;remote sensing images stitches images;uniform geographic coordinate system;mosaicking process;critical enblending operation;color balance;color consistency;mosaicking framework;low-resolution reference images","","4","","54","CCBY","21 May 2021","","","IEEE","IEEE Journals"
"Pan-Sharpening Based on Convolutional Neural Network by Using the Loss Function With No-Reference","Z. Xiong; Q. Guo; M. Liu; A. Li","Key Laboratory of Information Fusion Estimation and Detection, Heilongjiang University, Harbin, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Information Fusion Estimation and Detection, Heilongjiang University, Harbin, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","897","906","In order to preserve the spatial and spectral information of the original panchromatic and multispectral images, this article designs a loss function suitable for pan-sharpening and a four-layer convolutional neural network that could adequately extract spectral and spatial features from original source images. The major advantage of this study is that the designed loss function does not need the reference fused image, and then the proposed pan-sharpening method does not need to make the simulation data for training. This is the big difference from most existing pan-sharpening methods. Moreover, the loss function takes into account the characteristics of remote sensing images, including the spatial and spectral evaluation indicators. We also add the feature enhancement layer in convolutional neural network, thus, the proposed four-layer network contains feature extraction, feature enhancement, linear mapping and reconstruction. In order to evaluate the effectiveness and universality of the proposed fusion model, we selected thousands of remote sensing images that include different sensors, different times and different land-cover types to make the training dataset. By evaluating the performance on the WorldView-2, Pleiades and Gaofen-1 experimental data, the results show that the proposed method achieves optimal performance in terms of both the subjective visual effect and the object assessment. Furthermore, the codes will be available at https://github.com/Zhangxi-Xiong/pan-sharpening.","2151-1535","","10.1109/JSTARS.2020.3038057","National Natural Science Foundation of China(grant numbers:61771470,41590853); Key Research Program of Frontier Sciences; Chinese Academy of Sciences(grant numbers:QYZDY-SSWDQC026); Chinese Academy of Sciences(grant numbers:XDA19060103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9258922","Convolutional neural network;deep learning;feature enhancement;loss function;pan-sharpening;remote sensing image fusion","Feature extraction;Training data;Training;Spatial resolution;Remote sensing;Multiresolution analysis;Deep learning","feature extraction;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","original panchromatic images;multispectral images;four-layer convolutional neural network;original source images;designed loss function;reference fused image;pan-sharpening method;pan-sharpening methods;remote sensing images;spatial evaluation indicators;spectral evaluation indicators;feature enhancement layer;four-layer network;feature extraction;land-cover types","","16","","45","CCBY","13 Nov 2020","","","IEEE","IEEE Journals"
"Automatic Building Extraction via Adaptive Iterative Segmentation With LiDAR Data and High Spatial Resolution Imagery Fusion","S. Chen; W. Shi; M. Zhou; M. Zhang; P. Chen","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Kowloon, Hong Kong; State Key Laboratory of Information Engineering in Surveying Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Kowloon, Hong Kong","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 May 2020","2020","13","","2081","2095","Extracting buildings from remotely sensed data is a fundamental task in many geospatial applications. However, this task is resistant to automation due to variability in building shapes and the environmental complexity surrounding buildings. To solve this problem, this article introduces a novel automatic building extraction method that integrates LiDAR data and high spatial resolution imagery using adaptive iterative segmentation and hierarchical overlay analysis based on data fusion. An adaptive iterative segmentation method overcomes over- and undersegmentation based on the globalized probability of boundary contour detection algorithm. A data-fusion-based hierarchical overlay analysis extracts building candidate regions based on segmentation results. A morphological operation optimizes a building candidate region to obtain final building results. Experiments were conducted on the international society for photogrammetry and remote sensing (ISPRS) Vaihingen benchmark dataset. The extracted building footprints were compared with those extracted using the state-of-the-art methods. Evaluation results show that the proposed method achieved the highest area-based quality compared to results from the other tested methods on the ISPRS website. A detailed comparison with four state-of-the-art methods shows that the proposed method requiring no samples achieves competitive extraction results. Furthermore, the proposed method achieved a completeness of 94.1%, a correctness of 90.3%, and a quality of 85.5% over the whole Vaihingen dataset, indicating that the method is robust, with great potential in practical applications.","2151-1535","","10.1109/JSTARS.2020.2992298","Ministry of Science and Technology of the People's Republic of China(grant numbers:2017YFB0503604); Hong Kong Polytechnic University; Smart Cities Research Institute; State Bureau of Surveying and Mapping of the People's Republic of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9088264","Adaptive segmentation;building extraction;data fusion;high spatial resolution imagery (HSRI);LiDAR","Buildings;Feature extraction;Laser radar;Image segmentation;Data mining;Remote sensing;Image edge detection","buildings (structures);feature extraction;geophysical image processing;image fusion;image resolution;image segmentation;optical radar;photogrammetry;remote sensing","ISPRS;remote sensing;photogrammetry;morphological operation;globalized probability;high spatial resolution imagery;adaptive iterative segmentation;automatic building extraction;building shapes;geospatial applications;high spatial resolution imagery fusion;lidar data","","15","","61","CCBY","6 May 2020","","","IEEE","IEEE Journals"
"Attention-Based and Staged Iterative Networks for Pansharpening of Remote Sensing Images","X. Su; J. Li; Z. Hua","School of Information and Electronic Engineering, Shandong Technology and Business University, Yantai, China; School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Shandong Technology and Business University, Yantai, China","IEEE Transactions on Geoscience and Remote Sensing","31 Oct 2022","2022","60","","1","21","The pansharpening method combines complementary features from panchromatic (PAN) images and multispectral (MS) images to provide high-resolution MS images. Therefore, how to extract the features completely and reconstruct the image with high quality is the key link to obtain the ideal fusion image. We propose an attention-based and staged iterative network (ASIN) framework, which considers each subnetwork of the iterative network as a multistage process of pansharpening and carries out feature extraction and image reconstruction in each stage. Using the advantages of an iterative network for cross-stage depth features for the hierarchical extraction of refined features of MS images and PAN images for image reconstruction, we use the large kernel attention (LKA) module and the cascaded asymmetric coupling representation module to build the framework for feature extraction and the attention fusion module (AFM) to fuse the features of PAN and MS in the image reconstruction stage. LKA has channel and spatial adaptability, as well as strong long-range dependency establishment capability, which makes the feature extraction more complete. Asymmetric coupled representation module (ACRM) outputs refined spectral and spatial features by learning the hybrid correlation of MS and PAN images. AFM effectively utilizes the spectral and spatial features of the input, enabling the network to reduce information loss and retain important information. On the QuickBird (QB), WorldView-2 (WV2), and Gaofen-2 (GF-2) datasets, the superior performance of our method over the contrasting methods is demonstrated by quantitative comparison and qualitative analysis.","1558-0644","","10.1109/TGRS.2022.3215205","National Natural Science Foundation of China(grant numbers:61772319,62002200,62202268,62272281); Shandong Provincial Science and Technology Support Program of Youth Innovation Team in Colleges(grant numbers:2021KJ069,2019KJN042); Yantai Science and Technology Innovation Development Plan(grant numbers:2022JCYJ031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921219","Convolutional neural network (CNN);multispectral (MS) image;panchromatic (PAN) image;pansharpening","Feature extraction;Pansharpening;Image reconstruction;Remote sensing;Iterative methods;Deep learning;Spatial resolution","feature extraction;geophysical image processing;image classification;image fusion;image reconstruction;image resolution;iterative methods;remote sensing","remote sensing images;pansharpening method;complementary features;panchromatic images;multispectral images;ideal fusion image;attention-based;iterative network framework;feature extraction;cross-stage depth features;hierarchical extraction;refined features;MS images;PAN images;kernel attention module;cascaded asymmetric coupling representation module;attention fusion module;image reconstruction stage;asymmetric coupled representation module;spectral features;spatial features","","","","61","IEEE","17 Oct 2022","","","IEEE","IEEE Journals"
"HyperFusion: A Computational Approach for Hyperspectral, Multispectral, and Panchromatic Image Fusion","X. Tian; W. Zhang; Y. Chen; Z. Wang; J. Ma","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","17 Feb 2022","2022","60","","1","16","Fusing hyperspectral image (HSI) and multispectral image (MSI) of high spatial resolution is typically utilized to obtain HSIs of high spatial resolution. However, the spatial quality of most existing methods is unsatisfactory due to the limited spatial resolution of an MSI. To further improve the spatial resolution of the fused HSI while keeping the spectral information well, we propose a new computational paradigm, named HyperFusion, which simultaneously fuses HSI, MSI, and panchromatic (PAN) image. To achieve this goal, we first establish two data fidelity terms based on a physical observation that HSI and MSI can be treated as degraded versions of the fused HSI. Consequently, the spatial and spectral information from HSI and MSI can be well preserved. To efficiently transfer the spatial details of PAN into the fused HSI while keeping the spectral information well, we further construct a prior constraint from PAN based on the structural similarity. Meanwhile, we impose another low-rank prior constraint on the coefficient matrix to accurately describe the latent characteristics of the HSI with high spatial resolution. By incorporating the aforementioned data fidelity terms and prior constraints, we finally formulate the objective as an optimization problem and utilize the alternative direction multiplier method to solve it efficiently. Comprehensive experiments on simulated and real datasets are carried out to demonstrate the superiority of HyperFusion over other state of the arts in terms of visual quality and quantitative analysis. We also adopt a simulated experiment of vegetation coverage index analysis to verify the effectiveness of HyperFusion in remote sensing applications.","1558-0644","","10.1109/TGRS.2021.3128279","National Natural Science Foundation of China(grant numbers:61971315,61773295); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9615043","Alternating direction method of multipliers;image fusion;low rank;sharpening;structural similarity","Spatial resolution;Pansharpening;Tensors;Hyperspectral imaging;Transforms;Three-dimensional displays;Fuses","geophysical image processing;geophysical signal processing;hyperspectral imaging;image fusion;image resolution;image sensors;remote sensing;sensor fusion","prior constraint;PAN;high spatial resolution;HyperFusion;panchromatic image;hyperspectral image;MSI;spatial quality;fused HSI;spectral information;spatial information;spatial details","","5","","80","IEEE","15 Nov 2021","","","IEEE","IEEE Journals"
"Recurrent Multiresolution Convolutional Networks for VHR Image Classification","J. R. Bergado; C. Persello; A. Stein","Department of Earth Observation Science, Faculty of Geoinformation Science and Earth Observation, University of Twente, Enschede, The Netherlands; Department of Earth Observation Science, Faculty of Geoinformation Science and Earth Observation, University of Twente, Enschede, The Netherlands; Department of Earth Observation Science, Faculty of Geoinformation Science and Earth Observation, University of Twente, Enschede, The Netherlands","IEEE Transactions on Geoscience and Remote Sensing","25 Oct 2018","2018","56","11","6361","6374","Classification of very high-resolution (VHR) satellite images has three major challenges: 1) inherent low intraclass and high interclass spectral similarities; 2) mismatching resolution of available bands; and 3) the need to regularize noisy classification maps. Conventional methods have addressed these challenges by adopting separate stages of image fusion, feature extraction, and postclassification map regularization. These processing stages, however, are not jointly optimizing the classification task at hand. In this paper, we propose a single-stage framework embedding the processing stages in a recurrent multiresolution convolutional network trained in an end-to-end manner. The feedforward version of the network, called FuseNet, aims to match the resolution of the panchromatic and multispectral bands in a VHR image using convolutional layers with corresponding downsampling and upsampling operations. Contextual label information is incorporated into FuseNet by means of a recurrent version called ReuseNet. We compared FuseNet and ReuseNet against the use of separate processing steps for both image fusions, e.g., pansharpening and resampling through interpolation and map regularization such as conditional random fields. We carried out our experiments on a land-cover classification task using a Worldview-03 image of Quezon City, Philippines, and the International Society for Photogrammetry and Remote Sensing 2-D semantic labeling benchmark data set of Vaihingen, Germany. FuseNet and ReuseNet surpass the baseline approaches in both the quantitative and qualitative results.","1558-0644","","10.1109/TGRS.2018.2837357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388225","Convolutional networks;deep learning;land cover classification;recurrent networks;very high-resolution (VHR) image","Feature extraction;Spatial resolution;Image fusion;Task analysis;Kernel;Labeling","convolution;feature extraction;feedforward neural nets;geophysical image processing;geophysical techniques;image classification;image denoising;image fusion;image resolution;interpolation;photogrammetry;remote sensing;terrain mapping","recurrent multiresolution convolutional network;VHR image classification;high-resolution satellite images;high interclass spectral similarities;noisy classification maps;image fusion;panchromatic bands;multispectral bands;convolutional layers;ReuseNet;interpolation;land-cover classification task;Worldview-03 image;Remote Sensing 2-D semantic labeling benchmark data;FuseNet;post classification map regularization;feature extraction","","48","2","41","IEEE","19 Jun 2018","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion via Variational Tensor Subspace Decomposition","Y. Xing; Y. Zhang; S. Yang; Y. Zhang","National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","The fusion of hyperspectral image (HSI) and multispectral image (MSI) refers to enhance the spatial resolution of HSI with the help of a corresponding MSI that has a high spatial resolution to finally obtain an HSI with high resolution in both spatial and spectral domains. In this letter, we propose a variational tensor subspace decomposition-based fusion method to fully explore the differences and correlations among three modes of the HSI tensor. Experimental results on two HSI datasets show that the proposed method can achieve superior performance compared with existing state-of-the-art fusion methods with high computational efficiency.","1558-0571","","10.1109/LGRS.2021.3094558","National Natural Science Foundation of China (NFSC)(grant numbers:U1701267,U19B2037); Fundamental Research Funds for Central Universities(grant numbers:D5000210749); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9486897","Hyperspectral;image fusion;tensor decomposition;total-variation (TV)","Tensors;Spatial resolution;TV;Matrix decomposition;Correlation;Spectral analysis;Optimization","geophysical image processing;hyperspectral imaging;image fusion;image resolution;tensors","MSI;variational tensor subspace decomposition-based fusion method;HSI tensor;HSI datasets;multispectral image fusion methods;hyperspectral image fusion methods;image resolution","","3","","19","IEEE","15 Jul 2021","","","IEEE","IEEE Journals"
"An Unsupervised SAR and Optical Image Fusion Network Based on Structure-Texture Decomposition","Y. Ye; W. Liu; L. Zhou; T. Peng; Q. Xu","State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China","IEEE Geoscience and Remote Sensing Letters","16 Nov 2022","2022","19","","1","5","Although the unique advantages of optical and synthetic aperture radar (SAR) images promote their fusion, the integration of complementary features from the two types of data and their effective fusion remains a vital problem. To address that, a novel framework is designed based on the observation that the structure of SAR images and the texture of optical images look complementary. The proposed framework, named SOSTF, is an unsupervised end-to-end fusion network that aims to integrate structural features from SAR images and detailed texture features from optical images into the fusion results. The proposed method adopts the nest connect-based architecture, including an encoder network, a fusion part, and a decoder network. To maintain the structure and texture information of input images, the encoder architecture is utilized to extract multiscale features from images. Then, we use the densely connected convolutional network (DenseNet) to perform feature fusion. Finally, we reconstruct the fusion image using a decoder network. In the training stage, we introduce a structure-texture decomposition model. In addition, a novel texture-preserving and structure-enhancing loss function are designed to train the DenseNet to enhance the structure and texture features of fusion results. Qualitative and quantitative comparisons of the fusion results with nine advanced methods demonstrate that the proposed method can fuse the complementary features of SAR and optical images more effectively.","1558-0571","","10.1109/LGRS.2022.3219341","National Natural Science Foundation of China(grant numbers:41971281,61972021,42271446); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0537); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9936631","Image fusion;SOSTF;synthetic aperture radar (SAR) and optical images;unsupervised","Optical sensors;Optical imaging;Radar polarimetry;Adaptive optics;Optical filters;Optical reflection;Optical losses","feature extraction;geophysical image processing;image fusion;image texture;optical images;radar imaging;sensor fusion;synthetic aperture radar","complementary features;decoder network;densely connected convolutional network;detailed texture features;effective fusion;encoder network;feature fusion;fusion image;fusion part;fusion results;input images;multiscale features;novel texture-preserving;optical image fusion network;optical images;SAR images;structural features;structure-texture decomposition model;texture information;unsupervised end-to-end fusion network;unsupervised SAR","","","","20","IEEE","3 Nov 2022","","","IEEE","IEEE Journals"
"A Novel Unmixing-Based Hypersharpening Method via Convolutional Neural Network","X. Lu; T. Li; J. Zhang; F. Jia","School of Information Science and Technology, Donghua University, Shanghai, China; Information System Laboratory, Shanghai Institute of Satellite Engineering, Shanghai, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Information Science and Technology, Donghua University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","8 Dec 2021","2022","60","","1","14","Hypersharpening (namely, hyperspectral (HS) and multispectral (MS) image fusion) aims at enhancing the spatial resolution of HS image via an auxiliary higher resolution MS image. Currently, numerous hypersharpening methods are proposed successively, among which the unmixing-based approaches have been widely researched and demonstrated their effectiveness in the spectral fidelity aspect. However, existing unmixing-based fusion methods substantially employ mathematical techniques to solve the spectral mixture model, without taking full advantage of the collaborative spatial–spectral information that is usually helpful for abundance estimation improvement. To overcome this drawback, in this article, a novel unmixing-based HS and MS image fusion method, via a convolutional neural network (CNN), is proposed to promote spectral fidelity. The main idea of this work is to use CNN to fully explore the spatial information and the spectral information of both HS and MS images simultaneously, thereby enhancing the accuracy of estimating the abundance maps. Experiments on four simulated and real remote sensing data sets demonstrate that the proposed method is beneficial to the spectral fidelity of the fused images compared with some state-of-the-art algorithms. Meanwhile, it is also easy to implement and has a certain advantage in running time.","1558-0644","","10.1109/TGRS.2021.3063105","Natural Science Foundation of Shanghai(grant numbers:19ZR1453800); Fundamental Research Funds for the Central Universities(grant numbers:2232020D-46); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382252","Convolutional neural network (CNN);hypersharpening;hyperspectral (HS);image fusion;multispectral (MS)","Spatial resolution;Image fusion;Tensors;Mixture models;Task analysis;Image reconstruction;Collaboration","convolutional neural nets;geophysical image processing;image fusion;image resolution;remote sensing","novel unmixing-based hypersharpening method;convolutional neural network;spatial resolution;HS image;auxiliary higher resolution MS image;numerous hypersharpening methods;unmixing-based approaches;spectral fidelity aspect;unmixing-based fusion methods;spectral mixture model;collaborative spatial-spectral information;abundance estimation improvement;spatial information","","2","","49","IEEE","22 Mar 2021","","","IEEE","IEEE Journals"
"Sallfus, library for satellite images fusion on homogeneous and heterogeneous computing architectures","M. -D. Rubén Javier; V. -P. Nelson Enrique; R. -R. Andrés Ovidio",Universidad Distrital Francisco Jose de Caldas; Universidad Distrital Francisco Jose de Caldas; Universidad Distrital Francisco Jose de Caldas,"IEEE Latin America Transactions","12 Apr 2021","2020","18","12","2130","2137","Fusion of satellite images consists of improving the quality of a multispectral image by combining data from a high spatial resolution panchromatic image with a high spectral resolution multispectral image. To carry out this, different techniques are available, which perform various operations at the pixel level, which leads to generating a dependency between the computational requirement and the image size. Currently, there are some libraries that implement these fusion methods, however, none of them allow this fusion process to be carried out on heterogeneous architectures, which enable the integration of acceleration platforms that reduce execution time through massive parallelization. For this reason, this document presents a library called Sallfus, which allows executing and evaluating the quality and performance of image fusion methods such as the Brovey transform, Multiplicative method, Principal Component Analysis (PCA) and Wavelet A trous, on homogeneous and heterogeneous architectures. Likewise, an evaluation of the library is made from an analysis of execution times and image quality using mathematical-statistical indices such as the correlation coefficient (CC), BIAS coefficient and Root of the Root Mean Square Error (RMSE). The results of the library evaluation showed that the merging process with images of 8192 pixels, presents a speed-up of approximately 591x for Brovey, 309x for Multiplicative, 18x for PCA and 6x for A trous. Additionally, it was observed that the methods that presented the best performance both computationally and in the quality of the merged image were Brovey and Wavelet A trous. Availability and implementation: https://github.com/Parall-UD/sallfus.","1548-0992","","10.1109/TLA.2020.9400441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9400441","Brovey transform;Heterogenous Computing;Multiplicative transform;Principal Component Analysis;Satellite-image fusion;Wavelet À trous","Principal component analysis;Wavelet transforms;Graphics processing units;Libraries;Satellites;Image fusion;Computer architecture","correlation methods;image enhancement;image fusion;image resolution;remote sensing","spectral resolution multispectral image;spatial resolution panchromatic image;heterogeneous computing architectures;homogeneous computing architectures;satellite images fusion;library evaluation;image quality","","","","","IEEE","12 Apr 2021","","","IEEE","IEEE Journals"
"STransUNet: A Siamese TransUNet-Based Remote Sensing Image Change Detection Network","J. Yuan; L. Wang; S. Cheng","College of Information Science and Engineering, Xinjiang University, Urumqi, China; College of Information Science and Engineering, Xinjiang University, Urumqi, China; College of Information Science and Engineering, and College of Mathematics and System Science, Xinjiang University, Urumqi, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","8 Nov 2022","2022","15","","9241","9253","In modern remote sensing image change detection (CD), convolution neural network (CNN), especially U-shaped structure (UNet), has achieved great success due to their powerful discriminative ability. However, UNet-based CNN networks usually have limitations in modeling global dependencies due to the intrinsic locality of convolution operations. Transformer has recently emerged as an alternative architecture for dense prediction tasks due to the global self-attention mechanism. However, due to the limitation of hardware resources, pure Transformer methods generally lack the ability to capture global information at a low level. Based on these existing problems, we propose STransUNet, which combines Transformer and UNet architecture. STransUNet can not only capture shallow detail features at an early stage but also model global context in high-level feature. In addition, we design an efficient feature fusion module named cross-enhanced adaptive fusion (CEAF). Our model mainly consists of three parts: encoder, fusion module, and decoder. The decoder is a CNN-Transformer hybrid structure. CNN extracts multilevel feature information. Transformer encodes tokenized sequence to capture global context. CEAF module cross-enhances and adaptively fuses bitemporal features to enhance feature representation. In the decoding stage, we introduce a cascaded upsampling decoder (CUP). CUP progressively aggregates low-level CNN features and high-level Transformer features to full resolution. On four public CD datasets, our STransUNet achieves better CD results than six state-of-the-art algorithms.","2151-1535","","10.1109/JSTARS.2022.3217038","The Open Project of Key Laboratory of Applied Mathematics of Xinjiang Uygur Autonomous Region(grant numbers:2022D04028); Natural Science Foundation of Xinjiang Uygur Autonomous Region(grant numbers:2022D01C82); Tianshan Innovation Team of Xinjiang Uygur Autonomous Region(grant numbers:2020D14044); Postgraduate Scientific Research Innovation Project of Xinjiang Uygur Autonomous Region(grant numbers:XJ2022G072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928568","Change detection (CD);convolution neural net- work (CNN);remote sensing;transformer;UNet","Transformers;Convolution;Task analysis;Decoding;Feature extraction;Semantics;Context modeling","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image representation;remote sensing","modern remote sensing image change detection;convolution neural network;U-shaped structure;powerful discriminative ability;UNet-based CNN networks;global dependencies;convolution operations;dense prediction tasks;global self-attention mechanism;global information;STransUNet;UNet architecture;shallow detail features;model global context;high-level feature;efficient feature fusion module named cross-enhanced;multilevel feature information;adaptively fuses bitemporal features;feature representation;low-level CNN features;siamese TransUNet-based remote sensing image change detection network;high-level transformer features;CNN-transformer hybrid structure;pure transformer methods;CUP;CEAF module","","","","47","CCBY","25 Oct 2022","","","IEEE","IEEE Journals"
"A Multiscale Assimilation Approach to Improve Fine-Resolution Leaf Area Index Dynamics","H. Jin; A. Li; G. Yin; Z. Xiao; J. Bian; X. Nan; J. Jing","Research Center for Digital Mountain and Remote Sensing Application, Institute of Mountain Hazards and Environment, Chinese Academy of Sciences, Chengdu, China; Research Center for Digital Mountain and Remote Sensing Application, Institute of Mountain Hazards and Environment, Chinese Academy of Sciences, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; State Key Laboratory of Remote Sensing Science, College of Remote Sensing Science and Engineering, Faculty of Geographical Science, Beijing Normal University, Beijing, China; Research Center for Digital Mountain and Remote Sensing Application, Institute of Mountain Hazards and Environment, Chinese Academy of Sciences, Chengdu, China; Research Center for Digital Mountain and Remote Sensing Application, Institute of Mountain Hazards and Environment, Chinese Academy of Sciences, Chengdu, China; Sichuan Provincial Coal Design & Research Institute, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","25 Sep 2019","2019","57","10","8153","8168","Fine spatial details of vegetation growth are usually lost in leaf area index (LAI) products obtained from coarse spatial resolution satellite sensors. This may bring uncertainties in ecosystem process models, which usually require LAI products with fine spatiotemporal resolutions. Successful downscaling of LAI dynamics to fine spatial resolution is very important for meeting the demands of these models. Hence, a multiscale multisensor approach using the ensemble Kalman smoother (EnKS) technique is proposed in this paper. The LAI dynamics at a coarser spatial resolution are incorporated as prior information into the remotely sensed observations for time series LAI estimation at a finer spatial resolution. Downscaled LAI dynamics are evaluated based on spatial distribution and temporal trajectory. The results indicate the assimilated LAI to be in good agreement with the reference values at the different spatial scales. For example, the coefficient of determination (R2) between the reference values and fine-resolution LAI results retrieved by the proposed approach is 0.71 with a root-mean-square-error (RMSE) value of 0.65 on Julian day 185 at the Agro site. The method has proved to be effective for downscaling LAI dynamics, which improves the spatiotemporal patterns of fine-resolution LAI retrievals with respect to earlier methods.","1558-0644","","10.1109/TGRS.2019.2918548","National Natural Science Foundation of China(grant numbers:41631180,41671376,41301385); National Key Research and Development Program of China(grant numbers:2016YFA0600103); Sichuan Science and Technology Program(grant numbers:2019YJ0007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736988","Data assimilation;downscaling;Landsat;MODIS;multiscale","Spatial resolution;Remote sensing;Satellites;MODIS;Time series analysis;Artificial satellites","data assimilation;geophysical image processing;image fusion;Kalman filters;vegetation;vegetation mapping","leaf area index products;coarse spatial resolution satellite sensors;ecosystem process models;LAI products;fine spatiotemporal resolutions;multiscale multisensor approach;ensemble Kalman smoother technique;coarser spatial resolution;time series LAI estimation;finer spatial resolution;downscaled LAI dynamics;assimilated LAI;fine-resolution LAI results;fine-resolution LAI retrievals;multiscale assimilation approach;fine-resolution leaf area index dynamics;root-mean-square-error value;RMSE value","","9","","41","IEEE","14 Jun 2019","","","IEEE","IEEE Journals"
"An Efficient Detection of Moroccan Coastal Upwelling Based on Fusion of Chlorophyll-a and Sea Surface Temperature Images With a New Validation Index","Z. El Abidi; K. Minaoui; A. El Aouni; A. Tamim; H. Laanaya","LRIT Laboratory, CNRST (URAC29), IT Rabat Center, Faculty of Sciences, Mohammed V University in Rabat, Rabat, Morocco; LRIT Laboratory, CNRST (URAC29), IT Rabat Center, Faculty of Sciences, Mohammed V University in Rabat, Rabat, Morocco; Airsea Team, Inria Grenoble Alpes and Laboratoire Jean Kuntzmann, Grenoble, France; Department of Marine Fisheries, Higher Institute of Marine Fisheries (ISPM), Agadir, Morocco; LRIT Laboratory, CNRST (URAC29), IT Rabat Center, Faculty of Sciences, Mohammed V University in Rabat, Rabat, Morocco","IEEE Geoscience and Remote Sensing Letters","21 Jul 2021","2021","18","8","1322","1326","This research deals with the problem of identifying and extracting effectively the main Moroccan upwelling front. The proposed methodology, based on the image-fusion concept, comes to benefit from the information available in both sea-surface temperature (SST) and chlorophyll-a satellite images. Moreover, a new validation index is proposed by computing a simple gradient along the extracted upwelling limit. The developed procedure is applied over a database of 366 SST and 366 chlorophyll-a images from 2007 to 2014, covering the Moroccan Atlantic coast. The final results are validated qualitatively by an oceanographer and quantitatively by our innovative index. The findings of validation demonstrate the performance of our fusion approach.","1558-0571","","10.1109/LGRS.2020.3002473","Centre National pour la Recherche Scientifique et Technique (CNRST) through the Research Excellence Awards Program; “Projets Prioritaires de Recherche under Project PPR2-6 porté par Khalid Minaoui”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127830","Chlorophyll-a;image fusion;main Moroccan upwelling front;sea-surface temperature (SST);validation index","Ocean temperature;Indexes;Satellites;Image color analysis;Sea surface","geophysical image processing;image fusion;ocean temperature;oceanographic regions;oceanographic techniques","Moroccan coastal upwelling;new validation index;effectively the main Moroccan upwelling front;image-fusion concept;sea-surface temperature;simple gradient;extracted upwelling limit;366 SST;chlorophyll-a images;Moroccan Atlantic coast;innovative index;fusion approach","","1","","21","IEEE","29 Jun 2020","","","IEEE","IEEE Journals"
"A Synchronous Long Time-Series Completion Method Using 3-D Fully Convolutional Neural Networks","M. Peng; L. Zhang; X. Sun; Y. Cen; X. Zhao","College of resources and environment, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Remote Sensing Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; College of resources and environment, University of Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Traditional spatiotemporal fusion methods utilize remote sensing images from two or three adjacent dates to predict missing images. Thus, temporal information in longtime data sets is not fully utilized, increasing the costs of long time series construction in terms of time and labor. Here, we propose a synchronous long time-series completion method using a 3-D fully convolutional neural network (LTSC3D). This method can be used to convert long time-series high-temporal-low-spatial (HTLS)- and high-spatial-low-temporal (HSLT)-resolution remotely sensed images into 4-D data sets. From these data sets, we can then extract and fuse spatiotemporal features to produce synchronous long time-series high-temporal-high-spatial (HTHS)-resolution predictions. The model was tested using simulated and real data sets and compared with four representative traditional spatiotemporal fusion methods. The results demonstrated the high accuracy and high efficiency of our method.","1558-0571","","10.1109/LGRS.2021.3055847","National Natural Science Foundation of China(grant numbers:41830108); Xinjiang Production and Construction Group (XPCC) Major Science and Technology Projects(grant numbers:2018AA00402); XPCC Innovation Team in Key Areas(grant numbers:2018CB004); National Key Research and Development Program of China(grant numbers:2017YFE0194900); National Key Research and Development Program of China(grant numbers:2017YFC1500900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354355","3-D convolution neural network (CNN);deep learning;Landsat;long time-series completion;moderate resolution imaging spectroradiometer (MODIS);spatiotemporal data fusion","Feature extraction;Spatiotemporal phenomena;Remote sensing;Earth;Artificial satellites;Solid modeling;Data models","image fusion;image resolution;time series","synchronous long time-series completion method;3D fully convolutional neural network;high-temporal-high-spatial-resolution;high-temporal-low-spatial resolution;high-spatial-low-temporal resolution","","","","29","IEEE","15 Feb 2021","","","IEEE","IEEE Journals"
"Fusing Landsat-8, Sentinel-1, and Sentinel-2 Data for River Water Mapping Using Multidimensional Weighted Fusion Method","Q. Liu; S. Zhang; N. Wang; Y. Ming; C. Huang","College of Urban and Environmental Sciences and the Shaanxi Key Laboratory of Earth Surface System and Environmental Carrying Capacity, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China; College of Urban and Environmental Sciences and the Shaanxi Key Laboratory of Earth Surface System and Environmental Carrying Capacity, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jul 2022","2022","60","","1","12","River water extent is critical for understanding river discharge or its hydrological conditions. Although numerous methods have been proposed to map river water from either optical or synthetic aperture radar (SAR) remotely sensed images, uncertainties still exist broadly. In this study, we developed an image fusion method that integrates Landsat-8, Sentinel-1, and Sentinel-2 images simultaneously for river water mapping with two major steps. Firstly, a posterior probability support vector machine (SVM) model was adopted to generate water probability maps from each individual image; and second, a multidimensional weighted fusion method (MDWFM) was developed to fuse these probability maps. Four reaches with different characteristics were selected as case study sites. High-resolution aerial images were acquired and used as the reference to evaluate our results. We found that the fusion process not only improves the quality of river water mapping but also excludes the cloud interference. The fused river water maps become more reliable after the conflicts from difference images being solved by the proposed MDWFM method that contains a proportional conflict redistribution rule. The weighted root mean square difference was reduced to 0.066, and the area under the ROC curve reached up to 0.984. The critical success index (CSI), kappa coefficient (KC), and F-measure reached up to 0.810, 0.836, and 0.895, respectively. These stable and accurate river extent mapping results obtained through fusing multiple images with high spatial resolution (SR) (10 m) and short revisit interval (0.4–4.4 days) are of great significance for enriching the data and methodology of hydrological studies.","1558-0644","","10.1109/TGRS.2022.3187154","National Natural Science Foundation of China(grant numbers:U2243205); National Key Research and Development Program of China(grant numbers:2019YFC1510503); Shaanxi Natural Science Foundation(grant numbers:2021JM-314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810288","Image fusion;multidimensional weighted fusion;posterior probability;support vector machine (SVM);synthetic aperture radar (SAR)","Rivers;Earth;Remote sensing;Artificial satellites;Uncertainty;Indexes;Fuses","geophysical image processing;hydrological techniques;image classification;image fusion;radar imaging;remote sensing by radar;rivers;support vector machines;synthetic aperture radar","Landsat-8;Sentinel-1;Sentinel-2 data;river water mapping;multidimensional weighted fusion method;river water extent;river discharge;image fusion method;Sentinel-2 images;posterior probability support vector machine model;water probability maps;high-resolution aerial images;fused river water maps;difference images;stable river extent;synthetic aperture radar remotely sensed images;support vector machine model;cloud interference;critical success index;kappa coefficient;F-measure;ROC curve;spatial resolution;hydrological studies;redistribution rule","","","","90","IEEE","29 Jun 2022","","","IEEE","IEEE Journals"
"3M-CDNet-V2: An Efficient Medium-Weight Neural Network for Remote Sensing Image Change Detection","B. Zhao; P. Tang; X. Luo; D. Liu; H. Wang","Nanhu Laboratory, Research Center of Big Data Technology, Jiaxing, China; Nanhu Laboratory, Research Center of Big Data Technology, Jiaxing, China; Nanhu Laboratory, Research Center of Big Data Technology, Jiaxing, China; PIESAT International Information Technology Ltd., Beijing, China; Nanhu Laboratory, Research Center of Big Data Technology, Jiaxing, China","IEEE Access","30 Aug 2022","2022","10","","89581","89597","Remote sensing-based change detection (CD) is a critical technique of detecting land surface changes in earth observation. Inspired by recent success of a lightweight CD network– 3M-CDNet, we implemented 9 meaningful modifications to it, for example, the incorporation of MHSA (Multi-Head Self-Attention). This elaborately designed model is termed as 3M-CDNet-V2. Its effectiveness and advantages were demonstrated on three engineering CD datasets, and experimental results indicated that: (1) relative to other state-of-the-art algorithms, the proposed model obtained very competitive or slight better performance on both visual comparison and quantitative metrics evaluation. (2) By applying a novel transfer learning strategy, 3M-CDNet-V2 could perform well on the small dataset. (3) The incorporation of MHSA brings a substantial accuracy improvement while moderately increasing the computational complexity. (4) The late fusion framework and deep supervision contribute most to the performance gain of 3M-CDNet-V2, while the introduction of low-level features to the classifier by skip connection could guide the model to focus on detailed spatial information such as small changes, narrow shaped objects, or accurate boundaries. We hope that our 3M-CDNet-V2 model helps in improving the understanding of network architecture design for CD.","2169-3536","","10.1109/ACCESS.2022.3201129","Project of Remote Sensing Recognition and Monitoring of Hidden Geological Hazards, Sichuan, China(grant numbers:YG-2020-08); Nonprofit Research Project, Jiaxing, China(grant numbers:2022AY30001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864637","Neural network;remote sensing;medium weight;change detection","Feature extraction;Semantics;Transformers;Remote sensing;Computer architecture;Computational modeling;Neural networks;Change detection algorithms","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;object detection;pattern classification;remote sensing;terrain mapping;video signal processing","efficient medium-weight neural network;remote sensing image change detection;sensing-based change detection;land surface changes;lightweight CD network- 3M-CDNet;engineering CD datasets;3M-CDNet-V2 model","","","","47","CCBY","23 Aug 2022","","","IEEE","IEEE Journals"
"Spatiotemporal Reflectance Fusion via Tensor Sparse Representation","Y. Peng; W. Li; X. Luo; J. Du; X. Zhang; Y. Gan; X. Gao","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Institute of Meteorological Science, Chongqing, China; School of Computer Science and Cyber Engineering, Guangzhou University, Guangzhou, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Graduate School of Science and Technology, Shizuoka University, Shizuoka, Japan; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Geoscience and Remote Sensing","13 Jan 2022","2022","60","","1","18","Tradeoffs between the spatial and temporal resolutions of current satellite instruments limit our ability to conduct high-quality and continuous monitoring of the earth’s surface dynamics. Spatiotemporal image fusion has become increasingly necessary to obtain remote sensing images with high spatiotemporal resolution. However, current learning-based methods concentrate on predicting images only from spatial similarity and neglect spectral correlations of remote sensing images, leading to significant spectral information loss. In this article, we develop a novel nonlocal tensor sparse representation-based semicoupled dictionary learning approach (SCDNTSR) for spatiotemporal fusion. In the SCDNTSR method, the spectral correlation and the spatial similarity of the nonlocal similar cubes are simultaneously exploited through the tensor–tensor product-based tensor sparse representation. Furthermore, the semicoupled mapping prior knowledge of sparse coefficients across the high- and low-spatial resolution (HSR\LSR) image spaces is exploited with the coupled dictionary to constrain the similarity of sparse coefficients to improve the prediction performance. In addition, to capture additional prior spatial information, the SCDNTSR provides a new method to determine the degradation relationship between the target HSR and LSR difference images with the help of the known HSR and LSR difference images. The proposed SCDNTSR method was tested on real datasets at both the Coleambally Irrigation Area study site and the Lower Gwydir Catchment study site. Results show that the proposed method outperforms five state-of-the-art methods, especially in maintaining the spectral information, proving the feasibility of integrating the degradation relationship, spatio-spectral-nonlocal correlation, and semicoupled mapping priors of the multisource data into the proposed model.","1558-0644","","10.1109/TGRS.2021.3091157","Natural Science Foundation of China(grant numbers:61972060,U1713213,62027827,41871226); National Key Research and Development Program of China(grant numbers:2019YFE0110800); Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0025,cstc2019cxcyljrc-td0270); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9471790","Dictionary learning;nonlocal tensor sparse representation (TSR);spatial-spectral-nonlocal correlation;spatiotemporal fusion","Spatiotemporal phenomena;Remote sensing;Spatial resolution;Correlation;Tensors;Sensors;Learning systems","geophysical image processing;geophysical signal processing;geophysical techniques;image fusion;image representation;image resolution;irrigation;learning (artificial intelligence);remote sensing;spatiotemporal phenomena;tensors","spatiotemporal reflectance fusion;spatial resolutions;temporal resolutions;current satellite instruments;continuous monitoring;earth;spatiotemporal image fusion;remote sensing images;high spatiotemporal resolution;current learning-based methods;spatial similarity;neglect spectral correlations;significant spectral information loss;novel nonlocal tensor sparse representation;spatiotemporal fusion;SCDNTSR method;spectral correlation;nonlocal similar cubes;tensor-tensor product-based;semicoupled mapping prior knowledge;sparse coefficients;HSR\LSR;coupled dictionary;prediction performance;additional prior spatial information;degradation relationship;spatio-spectral-nonlocal correlation;semicoupled mapping priors","","4","","61","IEEE","2 Jul 2021","","","IEEE","IEEE Journals"
"ADMM-HFNet: A Matrix Decomposition-Based Deep Approach for Hyperspectral Image Fusion","D. Shen; J. Liu; Z. Wu; J. Yang; L. Xiao","Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","17","Hyperspectral image (HSI) fusion refers to the reconstruction of a high-resolution HSI by fusing a low-resolution HSI (LR-HSI) and a high-resolution multispectral image (HR-MSI) over the same scene. Recently, researchers have proposed many approaches to handle this issue. However, most of them assume that both the spatial and spectral degradation functions are known, which are often limited or unavailable in reality. This article presents a novel model-driven deep network based on matrix decomposition, which considers spectral correlations and reasonably embeds the well-known observation models. Specifically, the proposed method decomposes the desired HSI into spectral basis and coefficients. The spectral basis can be estimated from the LR-HSI via singular value decomposition. To learn the coefficients, a learning model is constructed by merging the observation models, matrix decomposition, and sparsity into a concise single formulation. For solving the proposed model, a deep framework is built by unrolling the alternating direction method of multipliers (ADMM), dubbed as ADMM-HFNet, where the involved parameters can be learned adaptively. It is worth noting that the spectral basis cannot fully represent the desired HSI. Therefore, another model is constructed here to supplement the approximation error, which can also be embedded in the deep network. After checking on three datasets, it is found that the proposed method stands out from advanced competing techniques in both quality measures and visual effects.","1558-0644","","10.1109/TGRS.2021.3112181","National Natural Science Foundation of China(grant numbers:62071204,61601201); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201338); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547385","Alternating direction method of multipliers (ADMM);hyperspectral image (HSI) fusion;image reconstruction;matrix decomposition;model-driven deep learning","Matrix decomposition;Correlation;Adaptation models;Spatial resolution;Hyperspectral imaging;Task analysis;Image fusion","correlation methods;geophysical image processing;hyperspectral imaging;image colour analysis;image fusion;image reconstruction;image resolution;image sensors;learning (artificial intelligence);matrix decomposition;singular value decomposition","matrix decomposition-based;hyperspectral image fusion;high-resolution HSI;low-resolution HSI;LR-HSI;high-resolution multispectral image;spatial degradation functions;spectral degradation functions;novel model-driven deep network;spectral correlations;well-known observation models;desired HSI;spectral basis;singular value decomposition;learning model;deep framework;ADMM-HFNet","","7","","65","IEEE","24 Sep 2021","","","IEEE","IEEE Journals"
"A Remote-Sensing Image Pan-Sharpening Method Based on Multi-Scale Channel Attention Residual Network","X. Li; F. Xu; X. Lyu; Y. Tong; Z. Chen; S. Li; D. Liu","College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; School of Information Engineering, Zhengzhou University, Zhengzhou, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Information Center, Yellow River Conservancy Commission, Zhengzhou, China; Information Center, Yellow River Conservancy Commission, Zhengzhou, China","IEEE Access","11 Feb 2020","2020","8","","27163","27177","Pan-sharpening is a significant task that aims to generate high spectral- and spatial- resolution remote-sensing image by fusing multi-spectral (MS) and panchromatic (PAN) image. The conventional approaches are insufficient to protect the fidelity both in spectral and spatial domains. Inspired by the robust capability and outstanding performance of convolutional neural networks (CNN) in natural image super-resolution tasks, CNN-based pan-sharpening methods are worthy of further exploration. In this paper, a novel pan-sharpening method is proposed by introducing a multi-scale channel attention residual network (MSCARN), which can represent features accurately and reconstruct a pan-sharpened image comprehensively. In MSCARN, the multi-scale feature extraction blocks comprehensively extract the coarse structures and high-frequency details. Moreover, the multi-residual architecture guarantees the consistency of feature learning procedure and accelerates convergence. Specifically, we introduce a channel attention mechanism to recalibrate the channel-wise features by considering interdependencies among channels adaptively. The extensive experiments are implemented on two real-datasets from GaoFen series satellites. And the results show that the proposed method performs better than the existing methods both in full-reference and no-reference metrics, meanwhile, the visual inspection displays in accordance with the quantitative metrics. Besides, in comparison with pan-sharpening by convolutional neural networks (PNN), the proposed method achieves faster convergence rate and lower loss.","2169-3536","","10.1109/ACCESS.2020.2971502","National Basic Research Program of China (973 Program)(grant numbers:2018YFC0407105); Technology Project of China Huaneng Group(grant numbers:MW 2017/P28); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8981952","Pan-sharpening;deep learning;multi-scale feature extraction;multi-residual learning;channel-attention mechanism;convergence acceleration","Feature extraction;Remote sensing;Spatial resolution;Computer architecture;Convolution;Convolutional neural networks","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","remote-sensing image pan-sharpening method;multiscale channel attention residual network;spectral domains;spatial domains;convolutional neural networks;natural image super-resolution tasks;CNN-based pan-sharpening methods;pan-sharpened image;multiscale feature extraction blocks;multiresidual architecture;channel attention mechanism;channel-wise features;spatial-resolution remote-sensing image","","16","","52","CCBY","4 Feb 2020","","","IEEE","IEEE Journals"
"Hyperspectral-Multispectral Image Fusion via Tensor Ring and Subspace Decompositions","H. Xu; M. Qin; S. Chen; Y. Zheng; J. Zheng","College of Computer Science, and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science, and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Sciences and Engineering, Tianjin University of Technology, Tianjin, China; Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing, China; College of Computer Science, and Technology, Zhejiang University of Technology, Hangzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 Sep 2021","2021","14","","8823","8837","Fusion from a spatially low resolution hyperspectral image (LR-HSI) and a spectrally low resolution multispectral image (MSI) to produce a high spatial-spectral HSI (HR-HSI), known as hyperspectral super resolution, has risen to a preferred topic for reinforcing the spatial-spectral resolution of HSI in recent years. In this work, we propose a new model, namely, low-rank tensor ring decomposition based on tensor nuclear norm (LRTRTNN), for HSI-MSI fusion. Specifically, for each spectrally subspace cube, similar patches are grouped to exploit both the global low-rank property of LR-HSI and the nonlocal similarity of HR-MSI. Afterward, a joint optimization of all groups via the presented LRTRTNN approximation is implemented in a unified cost function. With the introduced tensor nuclear norm (TNN) constraint, all 3D tensor ring factors are no longer unfolded to suit the matrix nuclear norm used in conventional methods, and the internal tensor structure can be naturally retained. The alternating direction method of multipliers is introduced for coefficients update. Numerical and visual experiments on real data show that our LRTRTNN method outperforms most state-of-the-art algorithms in terms of fusing performance.","2151-1535","","10.1109/JSTARS.2021.3108233","National Key R&D Program of China(grant numbers:2018YFE0126100); National Natural Science Foundation of China(grant numbers:62020106004,61602413); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY19F030016); Open Research Projects of Zhejiang Lab(grant numbers:2019KD0AD01/007); Scientific Research Fund of the National Health Commission of China(grant numbers:WKJ-ZJ-2102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9525254","Hyperspectral imaging;hyperspectral super resolution;image fusion;low-rank decomposition;multispectral image (MSI);tenson ring;tensor nuclear norm (TNN)","Tensors;Spatial resolution;Hyperspectral imaging;Three-dimensional displays;Matrix decomposition;Sparse matrices;Earth","geophysical image processing;hyperspectral imaging;image fusion;image resolution;matrix algebra;remote sensing;tensors","spectrally subspace cube;low-rank property;LR-HSI;nonlocal similarity;presented LRTRTNN approximation;introduced tensor nuclear norm constraint;3D tensor ring factors;matrix nuclear norm;internal tensor structure;LRTRTNN method;hyperspectral-multispectral image fusion;spatially low resolution hyperspectral image;spectrally low resolution multispectral image;spatial-spectral HSI;HR-HSI;hyperspectral super resolution;preferred topic;low-rank tensor ring decomposition;HSI-MSI fusion","","6","","51","CCBY","30 Aug 2021","","","IEEE","IEEE Journals"
"Multiscale Building Extraction With Refined Attention Pyramid Networks","Q. Tian; Y. Zhao; Y. Li; J. Chen; X. Chen; K. Qin","National Key Laboratory of Remote Sensing Information and Image Analysis Technology, Beijing Research Institute of Uranium Geology, Beijing, China; National Key Laboratory of Remote Sensing Information and Image Analysis Technology, Beijing Research Institute of Uranium Geology, Beijing, China; Zachry Department of Civil and Environmental Engineering, Texas A&M University, College Station, TX, USA; Iflytek Intelligent Information Technology Company, Ltd., Hefei, China; National Key Laboratory of Remote Sensing Information and Image Analysis Technology, Beijing Research Institute of Uranium Geology, Beijing, China; National Key Laboratory of Remote Sensing Information and Image Analysis Technology, Beijing Research Institute of Uranium Geology, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Automatic building extraction from high-resolution aerial and satellite images has many practical applications, such as urban planning and disaster management. However, the complex appearance and various scales of buildings in remote-sensing images bring a challenge for building extraction. In this study, we developed a novel multiscale building extraction method based on refined attention pyramid networks (RAPNets). We built an encoder–decoder structure, and combine atrous convolution, deformable convolution, attention mechanism, and pyramid pooling module to improve the performance of feature extraction in the encoding path. Moreover, the salient multiscale features were extracted by embedding the convolutional block attention module into the lateral connections. Finally, the refined feature pyramid structure was adopted in the decoding path to fuse the multiscale features to obtain the final extraction results. Experiments on two standard data sets (Inria aerial image labeling data set and xBD data set) show that our method achieves reliable results and outperforms the comparing methods.","1558-0571","","10.1109/LGRS.2021.3075436","National Natural Science Foundation of China(grant numbers:41602333); National Key Laboratories Stable Supporting Program of China(grant numbers:ZS1901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423811","Aerial image;building extraction;deep learning;refined attention pyramid networks (RAPNets);satellite image","Feature extraction;Buildings;Semantics;Data mining;Task analysis;Satellites;Data models","feature extraction;geophysical image processing;image classification;image fusion;image resolution;object detection;remote sensing","refined attention pyramid networks;automatic building extraction;high-resolution aerial;satellite images;urban planning;disaster management;remote-sensing images;multiscale building extraction method;encoder-decoder structure;atrous convolution;deformable convolution;attention mechanism;pyramid pooling module;feature extraction;salient multiscale features;convolutional block attention module;refined feature pyramid structure;final extraction results;Inria aerial image labeling data","","8","","24","IEEE","4 May 2021","","","IEEE","IEEE Journals"
"Aircraft Recognition Based on Landmark Detection in Remote Sensing Images","A. Zhao; K. Fu; S. Wang; J. Zuo; Y. Zhang; Y. Hu; H. Wang","Institute of Electronics, Chinese Academy of Sciences, Suzhou, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Northeastern University, Boston, MA, USA; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Suzhou, China; Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","20 Jul 2017","2017","14","8","1413","1417","Aircraft type recognition of remote sensing images is critical both in civil and military applications. In this letter, we propose a novel landmark-based aircraft recognition method which is highly accurate and efficient. First, we propose a new idea to address the aircraft type recognition problem by aircraft's landmark detection. Its advantages are two folds. On the one hand, it needs fewer labeled data and alleviates the work of human annotation. On the other hand, a trained model has strong expansibility because it can be used for any type of aircraft that not contained in training data set without retraining. Then, we use a variant of a convolutional neural network called vanilla network for all landmarks regression at the same time. Therefore, it can avoid bad local minimum effectively by encoding the geometric constraints among landmarks implicitly. To handle aircrafts in myriads of poses, rotation jittering is used for data augmentation in preprocessing and multicrop fusion is used in postprocessing. Thus, an 80% reduction in error rate could be reached. Finally, we use the landmark template matching to recognize the aircraft. Our method shows a competitive performance both in accuracy and efficiency.","1558-0571","","10.1109/LGRS.2017.2715858","National Natural Science Foundation of China(grant numbers:41501485); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7970161","Aircraft recognition;convolutional neural network (CNN);landmark detection","Aircraft;Training;Military aircraft;Training data;Feature extraction;Remote sensing;Neural networks","aircraft;image fusion;image matching;neural nets;object detection;remote sensing","remote sensing images;aircraft type recognition;civil applications;military applications;aircraft landmark detection;convolutional neural network;vanilla network;landmarks regression;rotation jittering;data augmentation;multicrop fusion;landmark template matching","","29","","14","IEEE","6 Jul 2017","","","IEEE","IEEE Journals"
"Fusion of WorldView-2 Stereo and Multitemporal TerraSAR-X Images for Building Height Extraction in Urban Areas","Y. Xu; P. Ma; E. Ng; H. Lin","Institute of Future Cities, The Chinese University of Hong Kong, Hong Kong, China; Institute of Space and Earth Information Science and Shenzhen Research Institute, The Chinese University of Hong Kong, Hong Kong, China; School of Architecture, the Institute of Environment, Energy and Sustainability (IEES), and the Institute of Future Cities (IOFC), The Chinese University of Hong Kong, Hong Kong, China; Institute of Space and Earth Information Science and Shenzhen Research Institute, The Chinese University of Hong Kong, Hong Kong, China","IEEE Geoscience and Remote Sensing Letters","15 Jun 2015","2015","12","8","1795","1799","We investigated the joint use of the high-resolution WorldView-2 optical satellite images and the multitemporal TerraSAR-X synthetic aperture radar (SAR) satellite images to extract building height information in high-density urban areas. The main idea of the proposed fusion approach is to take full advantage of both data sets in building height retrieval. The proposed approach includes two main stages. First, initial building height estimates are extracted from WorldView-2 stereo images and multitemporal SAR images. These initial results are then combined using a novel object-based fusion approach, in which the heights of points for the same building footprint are retrieved and integrated. Experiments on the Mong Kok area of Hong Kong showed that the proposed approach using both data sets outperforms the use of either stereo images or SAR images alone. According to the results of the proposed approach, the average absolute height retrieval error is 6.53 m, which is much lower than using stereo and SAR images (9.08 and 12.24 m, respectively). The proposed fusion approach is suitable for building height retrieval in urban areas where single satellite data have limitations.","1558-0571","","10.1109/LGRS.2015.2427738","Hong Kong Research Grants Council(grant numbers:CUHK 14408214); Innovation and Technology Support Program of HKSAR(grant numbers:ITS/075/13); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7109861","Building height;object-based fusion;synthetic aperture radar (SAR);WorldView-2;Building height;object-based fusion;synthetic aperture radar (SAR);WorldView-2","Buildings;Synthetic aperture radar;Remote sensing;Urban areas;Laser radar;Optical imaging;Optical sensors","geophysical image processing;image fusion;image resolution;image retrieval;radar imaging;spaceborne radar;stereo image processing;synthetic aperture radar","Hong Kong;Mong Kok area;object-based fusion approach;image retrieval;high-density urban area;synthetic aperture radar;high-resolution WorldView-2 optical satellite image fusion;building height information extraction;multitemporal TerraSAR-X satellite image fusion;distance 6.53 m;distance 9.08 m;distance 12.24 m","","14","1","21","IEEE","18 May 2015","","","IEEE","IEEE Journals"
"MAENet: Multiple Attention Encoder–Decoder Network for Farmland Segmentation of Remote Sensing Images","H. Huan; Y. Liu; Y. Xie; C. Wang; D. Xu; Y. Zhang","School of Artificial Intelligence, Nanjing University of Information Science and Technology, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China; School of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China; School of Artificial Intelligence, Nanjing University of Information Science and Technology, Nanjing, China; Nanjing Nanji Intelligent Agricultural Machinery Technology Research Institute, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","11 Jan 2022","2022","19","","1","5","With the rapid development of computer vision, semantic segmentation as an important part of the technology has made some achievements in different applications. However, in the farmland segmentation scenario of remote sensing images, the capability of common semantic segmentation methods in restoring the farmland edge and identifying narrow farmland ridges needs to be improved. Therefore, in this letter a semantic segmentation method–multiple attention encoder–decoder network (MAENet)–for farmland segmentation is proposed. The design of a dual-pooling efficient channel attention (DPECA) module and its embedment in the backbone to improve the efficiency of feature extraction is described; secondly, a dual-feature attention (DFA) module is proposed to extract contextual information of high-level features; finally, a global-guidance information upsample (GIU) module is added to the decoder to reduce the influence of redundant information on feature fusion. We use three self-made farmland image datasets representing UAV data to train MAENet and compare them with other methods. The results show that the performances of segmentation and generalization of MAENet are improved compared with other methods. The MIoU and Kappa coefficient in the farmland multi-classification test set can reach 93.74% and 96.74%.","1558-0571","","10.1109/LGRS.2021.3137522","National Natural Science Foundation of China(grant numbers:42176176); Guangxi Innovative Development Grand Grant; Land Observation Satellite Supporting Platform of National Civil Space Infrastructure Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658516","Attention module;farmland segmentation;feature fusion;pyramid;UAV images","Feature extraction;Image segmentation;Semantics;Data mining;Convolution;Remote sensing;Decoding","computer vision;farming;feature extraction;image fusion;image resolution;image segmentation;neural nets;remote sensing","dual-pooling efficient channel attention module;feature extraction;dual-feature attention module;global-guidance information upsample module;feature fusion;farmland image datasets;MAENet;multiple attention encoder-decoder network;remote sensing images;computer vision;farmland segmentation;semantic segmentation;farmland edge restoration;farmland ridges;DPECA","","1","","21","IEEE","22 Dec 2021","","","IEEE","IEEE Journals"
"Spectral Reconstruction Network From Multispectral Images to Hyperspectral Images: A Multitemporal Case","T. Li; T. Liu; Y. Wang; X. Li; Y. Gu","Heilongjiang Province Key Laboratory of Space-Air-Ground Integrated Intelligent Remote Sensing, Harbin, China; Heilongjiang Province Key Laboratory of Space-Air-Ground Integrated Intelligent Remote Sensing, Harbin, China; Heilongjiang Province Key Laboratory of Space-Air-Ground Integrated Intelligent Remote Sensing, Harbin, China; Heilongjiang Province Key Laboratory of Space-Air-Ground Integrated Intelligent Remote Sensing, Harbin, China; Heilongjiang Province Key Laboratory of Space-Air-Ground Integrated Intelligent Remote Sensing, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","15 Aug 2022","2022","60","","1","16","Hyperspectral (HS) satellite data have been widely applied in many fields due to its numerous bands. Along with the advantages of high spectral resolution, HS satellite data are still limited by some disadvantages of high acquisition cost, low revisiting capability, and low spatial resolution. Compared with HS satellites, multispectral (MS) satellites have a large number, large width, strong coverage, and high spatial resolution. Therefore, MS data can be used as the input to the spectral reconstruction (SR) to obtain HS data with high temporal resolution. Better HS data can be obtained by spectral reconstructing with these continuous multitemporal data than with single-temporal data. A multitemporal spectral reconstruction network (MTSRN) is proposed in this article, which is used to reconstruct HS images from multitemporal MS images. The proposed MTSRN comprises multiple single-temporal spectral reconstruction networks (STSRN) for extracting temporal features and a multitemporal fusion network (MTFN). The parallel component alternative (PA) post-processing method enhances the physical plausibility of reconstructed HS data. To demonstrate performance of the proposed method in aspects of multitemporal reconstruction, experiments are conducted on four multitemporal HS and MS satellite datasets. The experimental results prove that the proposed MTSRN obtains better SR results compared with the SR method based on single-temporal information.","1558-0644","","10.1109/TGRS.2022.3195748","National Natural Science Foundation for Outstanding Scholars(grant numbers:62025107); National Natural Science Foundation of Key International Cooperation(grant numbers:61720106002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847255","Hyperspectral (HS) data;multispectral (MS) data;multitemporal;neural networks;spectral reconstruction (SR);spectral superresolution","Hyperspectral imaging;Image reconstruction;Reconstruction algorithms;Satellites;Dictionaries;Spatial resolution;Feature extraction","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;remote sensing","multispectral images;hyperspectral images;multitemporal case;hyperspectral satellite data;high spectral resolution;HS satellite data;high acquisition cost;low revisiting capability;low spatial resolution;multispectral satellites;width coverage;high spatial resolution;MS data;high temporal resolution;HS data;spectral reconstructing;continuous multitemporal data;single-temporal data;multitemporal spectral reconstruction network;MTSRN;HS images;multitemporal MS images;single-temporal spectral reconstruction networks;temporal features;multitemporal fusion network;parallel component alternative post-processing method;reconstructed HS data;multitemporal reconstruction;single-temporal information","","","","69","IEEE","1 Aug 2022","","","IEEE","IEEE Journals"
"Shallow–Deep Convolutional Network and Spectral-Discrimination-Based Detail Injection for Multispectral Imagery Pan-Sharpening","L. Liu; J. Wang; E. Zhang; B. Li; X. Zhu; Y. Zhang; J. Peng","School of Information Science and Technology, Northwest University, Xi'an, China; School of Information Science and Technology, Northwest University, Xi'an, China; School of Information Science and Technology, Northwest University, Xi'an, China; School of Information Science and Technology, Northwest University, Xi'an, China; School of Information Science and Technology, Northwest University, Xi'an, China; School of Information Science and Technology, Northwest University, Xi'an, China; School of Information Science and Technology, Northwest University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2020","2020","13","","1772","1783","Pan-sharpening is a significant task in remote sensing image processing, which merges a high-resolution panchromatic (PAN) image and a low-resolution multispectral (MS) image to create a high-resolution MS image. In this article, we propose a novel deep-learning-based MS image pan-sharpening method that combines a shallow-deep convolutional network (SDCN) and a spectral discrimination-based detail injection (SDDI) model. SDCN consists of a shallow network and a deep network, which can capture mid-level and high-level spatial features from PAN images. SDDI, inspired by the “Amélioration de la Résolution Spatial par Injection de Structures” concept, is developed to merge the spatial details extracted by SDCN into MS images with minimal spectral distortion. SDCN and SDDI are collaboratively learned for achieving high-spatial-resolution MS image and preserving more spectral information. Both the visual assessment and the quantitative assessment results on IKONOS and QuickBird datasets confirmed that the proposed method outperforms several state-of-the-art pan-sharpening methods.","2151-1535","","10.1109/JSTARS.2020.2981695","National Key R&D Program of China(grant numbers:2017YFB1402103); Key R&D Program of Shaanxi(grant numbers:2018ZDXM-GY-186); Changjiang Scholars and Innovative Research Team in University(grant numbers:IRT_17R87); National Natural Science Foundation of China(grant numbers:61772419); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JM-454,2019JQ-294,2018JQ6092); China Postdoctoral Science Foundation(grant numbers:2018M643718); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042292","Deep learning;detail injection model;image fusion;pan-sharpening;remote sensing","Feature extraction;Spatial resolution;Data mining;Remote sensing;Deep learning;Earth","filtering theory;geophysical image processing;geophysical techniques;image enhancement;image fusion;image resolution;learning (artificial intelligence);remote sensing","pan-sharpening methods;spectral information;high-spatial-resolution MS image;minimal spectral distortion;MS images;Injection de Structures;Amélioration de la Résolution Spatial;PAN images;high-level spatial features;deep network;shallow network;SDDI;spectral discrimination-based detail injection model;SDCN;MS image pan-sharpening method;high-resolution MS image;low-resolution multispectral image;high-resolution panchromatic image;remote sensing image processing;multispectral imagery pan-sharpening;spectral-discrimination-based detail Injection;shallow-deep convolutional network","","24","","53","CCBY","19 Mar 2020","","","IEEE","IEEE Journals"
"T$^{3}$SR: Texture Transfer Transformer for Remote Sensing Image Superresolution","D. Cai; P. Zhang","School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, Guangdong, China; School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, Guangdong, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Sep 2022","2022","15","","7346","7358","Remote sensing image superresolution has made significant progress in recent years, aiming to restore natural and realistic high-resolution images from low-resolution images. However, most image superresolution remote sensing methods are improved only by deepening their network and expanding the network size, consuming substantial computing resources and imposing a bottleneck in development. Here, we propose an end-to-end image superresolution network called texture transfer transformer for remote sensing image superresolution (T$^{3}$SR). For the first time, T$^{3}$SR introduces image texture transfer into remote sensing, which achieves the most advanced results. Specifically, T$^{3}$SR divides image superresolution into two stages: texture transfer and feature fusion. First, to solve the problems of missing textures, artifacts, and blurring in a single image superresolution approach, we design a texture transfer module to serve the shallow texture transfer. Second, to further reduce the dependence of the model on the reference image, we propose a U-Transformer-based feature fusion scheme to reduce the dependence on the reference image. Finally, we conduct numerous experiments on standard public datasets to fully evaluate our approach. In addition to verifying the method's superiority based on the reference image paradigm, we also test the performance without the reference image. All results show that our method yields an abundant texture and finish with better visual results. Moreover, the best score is also obtained in the quantitative parameters of PSNR and SSIM. Compared with the best available approach, T$^{3}$SR has an improved performance by 0.79 dB and 0.33 dB in the datasets of WHU-RS19 and RSSCN7, respectively.","2151-1535","","10.1109/JSTARS.2022.3198557","Guangdong Science and Technology Department Guangdong Key Laboratory of Advanced IntelliSense Technology(grant numbers:2019B121203006); Shenzhen Science and Technology Program(grant numbers:KQTD20190929172704911); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857583","Image superresolution;reference image;self-reference;texture transfer","Superresolution;Remote sensing;Transformers;Task analysis;Correlation;Meters;Computer vision","feature extraction;geophysical image processing;image fusion;image resolution;image texture;remote sensing","end-to-end image superresolution network;texture transfer transformer;remote sensing image superresolution;image texture transfer;feature fusion;single image superresolution approach;texture transfer module;shallow texture transfer;reference image paradigm;high-resolution images;low-resolution images;T3SR","","","","51","CCBY","16 Aug 2022","","","IEEE","IEEE Journals"
"Processing of Multiresolution Thermal Hyperspectral and Digital Color Data: Outcome of the 2014 IEEE GRSS Data Fusion Contest","W. Liao; X. Huang; F. Van Coillie; S. Gautama; A. Pižurica; W. Philips; H. Liu; T. Zhu; M. Shimoni; G. Moser; D. Tuia","Ghent University-TELIN-IPI-iMinds, Ghent, Belgium; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; FORSIT Research Unit, Ghent University, Ghent, Belgium; Ghent University-TELIN-IPI-iMinds, Ghent, Belgium; Ghent University-TELIN-IPI-iMinds, Ghent, Belgium; Ghent University-TELIN-IPI-iMinds, Ghent, Belgium; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Signal and Image Centre, Department of Electrical Engineering, Royal Military Academy (SIC-RMA), Brussels, Belgium; Department of Electrical, Electronic, Telecommunications Engineering and Naval Architecture (DITEN), University of Genoa, Genoa, Italy; Department of Geography, University of Zurich, Zurich, Switzerland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","6","2984","2996","This paper reports the outcomes of the 2014 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee (IADF TC) of the IEEE Geoscience and Remote Sensing Society (IEEE GRSS). As for previous years, the IADF TC organized a data fusion contest aiming at fostering new ideas and solutions for multisource remote sensing studies. In the 2014 edition, participants considered multiresolution and multisensor fusion between optical data acquired at 20-cm resolution and long-wave (thermal) infrared hyperspectral data at 1-m resolution. The Contest was proposed as a double-track competition: one aiming at accurate landcover classification and the other seeking innovation in the fusion of thermal hyperspectral and color data. In this paper, the results obtained by the winners of both tracks are presented and discussed.","2151-1535","","10.1109/JSTARS.2015.2420582","Swiss National Science Foundation(grant numbers:150593); SBO-IWT; FWO(grant numbers:G037115N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7109121","Hyperspectral;image analysis and data fusion (IADF);landcover classification;multimodal-;multiresolution-;multisource-data fusion;thermal imaging;Hyperspectral;image analysis and data fusion (IADF);landcover classification;multimodal-;multiresolution-;multisource-data fusion;thermal imaging","Feature extraction;Image resolution;Data integration;Hyperspectral imaging;Image analysis;Vegetation mapping","geophysical image processing;hyperspectral imaging;image colour analysis;image fusion;image resolution;land cover;remote sensing","multiresolution thermal hyperspectral data processing;digital color data processing;AD 2014;IEEE GRSS data fusion contest;Image Analysis and Data Fusion Technical Committee;IEEE Geoscience and Remote Sensing Society;multisource remote sensing study;optical data multiresolution;multisensor fusion;long-wave infrared hyperspectral data;thermal infrared hyperspectral data;double-track competition;land cover classification;thermal hyperspectral fusion data;thermal hyperspectral color data","","160","","62","OAPA","15 May 2015","","","IEEE","IEEE Journals"
"Space Object Recognition With Stacking of CoAtNets Using Fusion of RGB and Depth Images","N. Aldahoul; H. A. Karim; M. A. Momo; F. I. F. Escobara; M. J. T. Tan","Computer Science Division, New York University Abu Dhabi, Abu Dhabi, United Arab Emirates; Faculty of Engineering, Multimedia University, Cyberjaya, Malaysia; Fleet Management Systems & Technologies, Esenyurt, Turkey; Department of Natural Sciences, University of Saint La Salle, Bacolod, Philippines; Department of Chemical Engineering, University of Saint La Salle, Bacolod, Philippines","IEEE Access","18 Jan 2023","2023","11","","5089","5109","Space situational awareness (SSA) system requires recognition of space objects that are varied in sizes, shapes, and types. The space images are challenging because of several factors such as illumination and noise and thus make the recognition task complex. Image fusion is an important area in image processing for various applications including RGB-D sensor fusion, remote sensing, medical diagnostics, and infrared and visible image fusion. Recently, various image fusion algorithms have been developed and they showed a superior performance to explore more information that are not available in single images. In this paper, we compared various methods of RGB and Depth image fusion for space object classification task. The experiments were carried out, and the performance was evaluated using 13 fusion performance metrics. It was found that the guided filter context enhancement (GFCE) outperformed other image fusion methods in terms of average gradient (8.2593), spatial frequency (28.4114), and entropy (6.9486). additionally, due to its ability to balance between good performance and inference speed (11.41 second), GFCE was selected for RGB and Depth image fusion stage before feature extraction and classification stage. The outcome of fusion method is fused images that were used to train a deep ensemble of CoAtNets to classify space objects into ten categories. The deep ensemble learning methods including bagging, boosting, and stacking were trained and evaluated for classification purposes. It was found that combination of fusion and stacking was able to improve classification accuracy largely compared to the baseline methods by producing an average accuracy of 89 % and average F1 score of 89 %.","2169-3536","","10.1109/ACCESS.2023.3235965","Multimedia University, Malaysian; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013363","CoAtNet;deep ensemble learning;RGB-D;image fusion;space object classification","Space vehicles;Image fusion;Task analysis;Satellites;Deep learning;Machine learning;Image recognition","deep learning (artificial intelligence);feature extraction;image classification;image colour analysis;image filtering;image fusion;object recognition","classification stage;deep ensemble learning methods;feature extraction;fusion method;guided filter context enhancement;image fusion algorithms;image fusion methods;image processing;infrared image fusion;recognition task complex;RGB-D sensor fusion;space images;space object classification task;space object recognition;space objects;space situational awareness system;time 11.41 s;visible image fusion","","1","","83","CCBY","10 Jan 2023","","","IEEE","IEEE Journals"
"MLFF-GAN: A Multilevel Feature Fusion With GAN for Spatiotemporal Remote Sensing Images","B. Song; P. Liu; J. Li; L. Wang; L. Zhang; G. He; L. Chen; J. Liu","Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Guangdong Key Laboratory for Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory for Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","30 May 2022","2022","60","","1","16","Due to the limitation of technology and budget, it is often difficult for sensors of a single remote sensing satellite to have both high-temporal and high-spatial (HTHS) resolution at the same time. In this article, we proposed a new multilevel feature fusion with generative adversarial network (MLFF-GAN) for generating fusion HTHS images. The MLFF-GAN mainly uses U-net-like architecture, and its generator is composed of three stages: feature extraction, feature fusion, and image reconstruction. In the feature extraction and reconstruction stage, the generator employs the encoding and decoding structure to extract three groups of multilevel features (MLFs), which can cope with the huge difference of resolution between high-resolution images and low-resolution images. In the feature fusion stage, adaptive instance normalization (AdaIN) block is designed to learn the global distribution relationship between multitemporal images, and an attention module (AM) is used to learn the local information weights for the change of small areas. The proposed MLFF-GAN was tested on two Landsat and Moderate-Resolution Imaging Spectroradiometer (MODIS) datasets. Some state-of-the-art algorithms are comprehensively compared with MLFF-GAN. We also carried on the ablation experiment to test the effectiveness of different submodules in the MLFF-GAN. The experiment results and ablation analysis show the better performances of the proposed method when compared with other methods. The code is available at https://github.com/songbingze/MLFF-GAN.","1558-0644","","10.1109/TGRS.2022.3169916","National Natural Science Foundation of China(grant numbers:61731022,41971397); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781347","Adaptive instance normalization (AdaIN);generative adversarial network (GAN);spatial attention mechanism;U-net","Spatial resolution;Feature extraction;Generators;Generative adversarial networks;Spatiotemporal phenomena;Image resolution;Image reconstruction","feature extraction;geophysical image processing;image coding;image fusion;image reconstruction;image resolution;neural nets;remote sensing","multitemporal images;spatiotemporal remote sensing images;single remote sensing satellite;feature extraction;image reconstruction;high-resolution images;multilevel feature fusion with generative adversarial network;HTHS images;MLFF-GAN;adaptive instance normalization block;attention module;moderate-resolution imaging spectroradiometer;high-temporal and high-spatial resolution","","3","","58","IEEE","24 May 2022","","","IEEE","IEEE Journals"
"LDP-Net: An Unsupervised Pansharpening Network Based on Learnable Degradation Processes","J. Ni; Z. Shao; Z. Zhang; M. Hou; J. Zhou; L. Fang; Y. Zhang","College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Computer Science, Sichuan University, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Jul 2022","2022","15","","5468","5479","Pansharpening in remote sensing image aims at acquiring a high-resolution multispectral (HRMS) image directly by fusing a low-resolution multispectral (LRMS) image with a panchromatic (PAN) image. The main concern is how to effectively combine the rich spectral information of LRMS image with the abundant spatial information of PAN image. Recently, many methods based on deep learning have been proposed for the pansharpening task. However, these methods usually have two main drawbacks: 1) requiring HRMS for supervised learning; and 2) simply ignoring the latent relation between the MS and PAN image and fusing them directly. To solve these problems, we propose a novel unsupervised network based on learnable degradation processes, dubbed as LDP-Net. A reblurring block and a graying block are designed to learn the corresponding degradation processes, respectively. In addition, a novel hybrid loss function is proposed to constrain both spatial and spectral consistency between the pansharpened image and the PAN and LRMS images at different resolutions. Experiments on GaoFen-2, Worldview-2, and Worldview-3 images demonstrate that our proposed LDP-Net can fuse PAN and LRMS images effectively without the help of HRMS samples, achieving promising performance in terms of both qualitative visual effects and quantitative metrics.","2151-1535","","10.1109/JSTARS.2022.3188181","Sichuan Science and Technology Program(grant numbers:2021JDJQ0024); Sichuan University “From 0 to 1” Innovative Research Program(grant numbers:2022SCUH0016); National Natural Science Fund of China(grant numbers:61922029); Science and Technology Plan Project Fund of Hunan Province(grant numbers:2019RS2016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9814841","Image fusion;pansharpening;remote sensing;unsupervised learning","Pansharpening;Spatial resolution;Transforms;Degradation;Remote sensing;Training;Supervised learning","geophysical image processing;geophysical techniques;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing","Worldview-3 images;LDP-Net;unsupervised pansharpening network;learnable degradation processes;remote sensing image;high-resolution multispectral image;HRMS;low-resolution multispectral image;panchromatic image;rich spectral information;LRMS image;abundant spatial information;PAN image;deep learning;pansharpening task;supervised learning;novel unsupervised network;corresponding degradation processes;spatial consistency;spectral consistency;pansharpened image","","1","","52","CCBY","4 Jul 2022","","","IEEE","IEEE Journals"
"On the Use of the Expanded Image in Quality Assessment of Pansharpened Images","M. Selva; L. Santurri; S. Baronti","Istituto di Fisica Applicata “Nello Carrara,” CNR Area della Ricerca di Florence, Sesto Fiorentino, Italy; Istituto di Fisica Applicata “Nello Carrara,” CNR Area della Ricerca di Florence, Sesto Fiorentino, Italy; Istituto di Fisica Applicata “Nello Carrara,” CNR Area della Ricerca di Florence, Sesto Fiorentino, Italy","IEEE Geoscience and Remote Sensing Letters","23 Feb 2018","2018","15","3","320","324","This letter discusses whether the expanded multispectral image, i.e., the original multispectral image upsampled to the panchromatic scale, can be used during the assessment of the quality of pansharpened multispectral images. By considering Wald's protocol, the authors demonstrate that the adoption of the expanded image as the reference is erroneous and brings a quality assessment of the fused images that is misleading. In addition, some recommendations about the valid role of the expanded image are provided. The discussion is supported by a quantitative analysis and visual comparisons.","1558-0571","","10.1109/LGRS.2017.2777916","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8267248","Expanded multispectral image;hypersharpening;image fusion;image quality;pansharpening;Wald’s protocol","Protocols;Indexes;Quality assessment;Visualization;Image color analysis;Spatial resolution;Weight measurement","image fusion;remote sensing","expanded multispectral image;pansharpened multispectral images;fused images;pansharpened image quality assessmen","","23","","14","IEEE","23 Jan 2018","","","IEEE","IEEE Journals"
"Second-Order Total Generalized Variation Regularization for Pansharpening","G. Khademi; H. Ghassemian","Image Processing and Information Analysis Laboratory, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Laboratory, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Pansharpening can be regarded as an inverse problem, where a high-resolution multispectral (MS) image is estimated given a low-resolution MS image and a panchromatic (Pan) image. Considering the effectiveness of total generalized variation (TGV) to regularize ill-posed inverse problems, this letter proposes a variational model in accordance with the observation model based on the satellite imaging system and the second-order TGV. Furthermore, a primal–dual algorithm is adopted to resolve the variational model by splitting it into a sequence of simpler sub-problems, which leads to a more efficient algorithm compared to the other variational methods. In addition, the exploitation of TGV in the variational model allows the presence of the very fine details of the Pan image in the sharpened MS image whereas it is beyond the capabilities of the total variation (TV)-based models. The proposed algorithm is compared with some recent classical and variational pansharpening methods by experiments on two data sets at full and reduced resolution.","1558-0571","","10.1109/LGRS.2020.3043435","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305948","Image fusion;pansharpening;primal-dual algorithm;total generalized variation (TGV)","Optimization;Image resolution;Computational modeling;Adaptation models;TV;Numerical models;Sensors","geophysical image processing;image denoising;image fusion;image reconstruction;image resolution;inverse problems;remote sensing;variational techniques","high-resolution multispectral image;low-resolution MS image;panchromatic image;inverse problem;variational model;observation model;satellite imaging system;second-order TGV;primal-dual algorithm;simpler sub-problems;variational methods;Pan image;sharpened MS image;total variation-based models;recent classical pansharpening methods;variational pansharpening methods;order total generalized variation regularization","","2","","15","IEEE","23 Dec 2020","","","IEEE","IEEE Journals"
"Open Data for Global Multimodal Land Use Classification: Outcome of the 2017 IEEE GRSS Data Fusion Contest","N. Yokoya; P. Ghamisi; J. Xia; S. Sukhanov; R. Heremans; I. Tankoyeu; B. Bechtel; B. Le Saux; G. Moser; D. Tuia","RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; Department of Signal Processing in Earth Observation, Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany; Department of Advanced Interdisciplinary Studies, The University of Tokyo, Tokyo, Japan; AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany; AGT International, Darmstadt, Germany; Center for Earth System Research and Sustainability, Universität Hamburg, Hamburg, Germany; DTIS, ONERA, Université Paris Saclay, Palaiseau, France; Department of Electrical, Electronic, Telecommunications Engineering and Naval Architecture, University of Genoa, Genoa, Italy; Laboratory of Geo-Information Science and Remote Sensing, Wageningen University & Research, Wageningen, The Netherlands","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Apr 2018","2018","11","5","1363","1377","In this paper, we present the scientific outcomes of the 2017 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2017 Contest was aimed at addressing the problem of local climate zones classification based on a multitemporal and multimodal dataset, including image (Landsat 8 and Sentinel-2) and vector data (from OpenStreetMap). The competition, based on separate geographical locations for the training and testing of the proposed solution, aimed at models that were accurate (assessed by accuracy metrics on an undisclosed reference for the test cities), general (assessed by spreading the test cities across the globe), and computationally feasible (assessed by having a test phase of limited time). The techniques proposed by the participants to the Contest spanned across a rather broad range of topics, and of mixed ideas and methodologies deriving from computer vision and machine learning but also deeply rooted in the specificities of remote sensing. In particular, rigorous atmospheric correction, the use of multidate images, and the use of ensemble methods fusing results obtained from different data sources/time instants made the difference.","2151-1535","","10.1109/JSTARS.2018.2799698","Cluster of Excellence “CliSAP”(grant numbers:EXC177); University of Hamburg; Swiss National Science Foundation(grant numbers:PP00P2-150593); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338367","Convolutional neural networks (CNNs);crowdsourcing;deep learning (DL);ensemble learning;image analysis and data fusion (IADF);multimodal;multiresolution;multisource;OpenStreetMap (OSM);random fields","Urban areas;Remote sensing;Earth;Training;Artificial satellites;Data integration;Image resolution","geophysical image processing;image classification;image fusion;land use;learning (artificial intelligence);remote sensing;terrain mapping","remote sensing;local climate zones classification;multidate images;global multimodal land use classification;geographical locations;data fusion contest;ensemble methods;OpenStreetMap;Landsat 8;Sentinel-2;computer vision;machine learning;AD 2017","","84","","62","OAPA","16 Apr 2018","","","IEEE","IEEE Journals"
"SIRF: Simultaneous Satellite Image Registration and Fusion in a Unified Framework","C. Chen; Y. Li; W. Liu; J. Huang","Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA","IEEE Transactions on Image Processing","11 Aug 2015","2015","24","11","4213","4224","In this paper, we propose a novel method for image fusion with a high-resolution panchromatic image and a low-resolution multispectral (Ms) image at the same geographical location. The fusion is formulated as a convex optimization problem which minimizes a linear combination of a least-squares fitting term and a dynamic gradient sparsity regularizer. The former is to preserve accurate spectral information of the Ms image, while the latter is to keep sharp edges of the high-resolution panchromatic image. We further propose to simultaneously register the two images during the fusing process, which is naturally achieved by virtue of the dynamic gradient sparsity property. An efficient algorithm is then devised to solve the optimization problem, accomplishing a linear computational complexity in the size of the output image in each iteration. We compare our method against six state-of-the-art image fusion methods on Ms image data sets from four satellites. Extensive experimental results demonstrate that the proposed method substantially outperforms the others in terms of both spatial and spectral qualities. We also show that our method can provide high-quality products from coarsely registered real-world IKONOS data sets. Finally, a MATLAB implementation is provided to facilitate future research.","1941-0042","","10.1109/TIP.2015.2456415","National Science Foundation(grant numbers:IIS-1423056,CMMI-1434401,CNS-1405985); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7156141","Image fusion;pan-sharpening;image registration;dynamic gradient sparsity;group sparsity;joint fusion;Image fusion;pan-sharpening;image registration;dynamic gradient sparsity;group sparsity;joint fusion","Image fusion;Distortion;Image registration;Remote sensing;Distortion measurement;Spatial resolution","geophysical image processing;image fusion;image registration;image resolution;iterative methods;least squares approximations;mathematics computing;optimisation;terrain mapping","simultaneous satellite image registration;SIRF;high-resolution panchromatic image;low-resolution multispectral image;geographical location;convex optimization problem;linear least-squares fitting combination;dynamic gradient sparsity regularizer;spectral information;Ms image;image edges;linear computational complexity;dynamic gradient sparsity property;image iteration;state-of-the-art image fusion methods;spectral qualities;spatial qualities;high-quality products;MATLAB implementation;coarsely registered real-world IKONOS data sets;future research","","100","","53","IEEE","14 Jul 2015","","","IEEE","IEEE Journals"
"Blind Correction of Local Misalignments Between Multispectral and Panchromatic Images","B. Aiazzi; L. Alparone; A. Garzelli; L. Santurri","Institute of Applied Physics, National Research Council, Sesto Fiorentino, Italy; Department of Information Engineering, University of Florence, Florence, Italy; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Institute of Applied Physics, National Research Council, Sesto Fiorentino, Italy","IEEE Geoscience and Remote Sensing Letters","26 Sep 2018","2018","15","10","1625","1629","In this letter, we propose a simple yet robust procedure to combat the residual local misalignment between the image data sets that are usually processed for pansharpening: a higher resolution panchromatic (Pan) image and a series of lower resolution multispectral (MS) bands, preliminarily interpolated to the pixel size of Pan. Unlike a conventional coregistration, which requires a preliminary orthorectification enforced by an accurate digital surface model, the proposed method automatically exploits the characteristics of the Pan image to alleviate the effects of misalignment on fusion products, whichever is the method chosen for pansharpening. More specifically, the space-varying residue of the multivariate regression between resampled MS bands and low-pass-filtered Pan image, which locally measures the extent of MS-to-Pan misalignments, is injected into the MS bands after being weighted by the projection coefficients of each band. Tests on simulated Pléiades images demonstrate that global shifts up to five pixels along each direction are perfectly compensated. Tests on a true GeoEye-1 image, whose shifts are space-varying and of unknown extent, highlight the attained improvement in spatial alignment.","1558-0571","","10.1109/LGRS.2018.2850151","Project SMART; Tuscany Region in the framework of POR-FESR 2014–2020 for the development of an advanced imaging spectrometer; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411119","Image fusion;multispectral (MS) pansharpening;parallaxes;registration;satellite remote sensing","Instruments;Spatial resolution;Multivariate regression;Optics;Weight measurement;Indexes","geophysical image processing;geophysical techniques;image fusion;image registration;image resolution;terrain mapping","image data sets;pansharpening;higher resolution panchromatic image;lower resolution multispectral bands;preliminary orthorectification;Pan image;space-varying residue;resampled MS bands;MS-to-Pan misalignments;simulated Pléiades images;GeoEye-1 image;blind correction;local misalignments;residual local misalignment;digital surface model","","12","","10","IEEE","16 Jul 2018","","","IEEE","IEEE Journals"
"Detail-Injection-Model-Inspired Deep Fusion Network for Pansharpening","Z. Xiang; L. Xiao; J. Yang; W. Liao; W. Philips","Image Processing and Interpretation (IPI) Research Group at IMEC, Ghent University, Ghent, Belgium; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Telecommunications and Information Processing (TELIN), Ghent University, Ghent, Belgium; IPI Research Group at IMEC, Ghent University, Ghent, Belgium","IEEE Transactions on Geoscience and Remote Sensing","30 Aug 2022","2022","60","","1","15","Pansharpening is an image fusion procedure, which aims to produce a high spatial resolution multispectral (MS) image by combining a low spatial resolution MS image and a high spatial resolution panchromatic image. The most popular and successful paradigm for pansharpening is the framework known as detail injection, while it cannot fully exploit complex and nonlinear complementary features of both images. In this article, we propose a detail-injection-model-inspired deep fusion network for pansharpening (DIM-FuNet). First, by treating pansharpening as a complicated and nonlinear detail learning and injection problem, we establish a unified optimizing detail-injection model with triple detail fidelity terms: 1) a band-dependent spatial detail fidelity term; 2) a local detail fidelity term; and 3) a complicated detail synthesis term. Second, the model is optimized via the iterative gradient descent and unfolded into a deep convolutional neural network. Subsequently, the unrolling network has triple branches, in which a point-wise convolutional subnetwork and a depth-wise convolutional subnetwork are corresponding to the former two detail-constrained terms, and an adaptive weighted reconstruction module with a fusion subnetwork to aggregate details of two branches and to synthesize the final complicated details. Finally, the deep unrolling network is trained in an end-to-end manner. Different from traditional deep fusion networks, the architecture design of DIM-FuNet is guided by the optimizing model and thus promotes better interpretability. Experimental results on reduced and full-resolution demonstrate the effectiveness of the proposed DIM-FuNet which achieves the best performance compared with the state-of-the-art pansharpening method.","1558-0644","","10.1109/TGRS.2022.3197438","National Natural Science Foundation of China(grant numbers:61871226,61571230,62201226); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); Fundamental Research Funds for the Central Universities(grant numbers:30918011104,30920021134); National Major Research Plan of China(grant numbers:2016YFF0103604); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852483","Algorithm unrolling;data fusion;deep learning;pansharpening;remote sensing","Pansharpening;Spatial resolution;Iterative methods;Image resolution;Optimization;Convolutional neural networks;Remote sensing","geophysical image processing;gradient methods;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","detail-injection-model-inspired deep fusion network;image fusion procedure;high spatial resolution multispectral image;low spatial resolution MS image;high spatial resolution panchromatic image;complex features;nonlinear complementary features;DIM-FuNet;treating pansharpening;nonlinear detail learning;injection problem;unified optimizing detail-injection model;triple detail fidelity terms;band-dependent spatial detail fidelity term;local detail fidelity term;complicated detail synthesis term;deep convolutional neural network;fusion subnetwork;deep unrolling network;traditional deep fusion networks;optimizing model;state-of-the-art pansharpening method","","","","41","IEEE","8 Aug 2022","","","IEEE","IEEE Journals"
"Fusion of Landsat 8 OLI and Sentinel-2 MSI Data","Q. Wang; G. A. Blackburn; A. O. Onojeghuo; J. Dash; L. Zhou; Y. Zhang; P. M. Atkinson","Lancaster Environment Centre, Lancaster University, Lancaster, U.K.; Lancaster Environment Centre, Lancaster University, Lancaster, U.K.; Department of Surveying and Geoinformatics, Nnamdi Azikiwe University, Awka, PMB, Nigeria; Geography and Environment, University of Southampton, Highfield, Southampton, U.K.; Geography and Environment, University of Southampton, Highfield, Southampton, U.K.; University of Chinese Academy of Sciences, Beijing, China; Faculty of Science and Technology, Lancaster University, Lancaster, U.K.","IEEE Transactions on Geoscience and Remote Sensing","22 Jun 2017","2017","55","7","3885","3899","Sentinel-2 is a wide-swath and fine spatial resolution satellite imaging mission designed for data continuity and enhancement of the Landsat and other missions. The Sentinel-2 data are freely available at the global scale, and have similar wavelengths and the same geographic coordinate system as the Landsat data, which provides an excellent opportunity to fuse these two types of satellite sensor data together. In this paper, a new approach is presented for the fusion of Landsat 8 Operational Land Imager and Sentinel-2 Multispectral Imager data to coordinate their spatial resolutions for continuous global monitoring. The 30 m spatial resolution Landsat 8 bands are downscaled to 10 m using available 10 m Sentinel-2 bands. To account for the land-cover/land-use (LCLU) changes that may have occurred between the Landsat 8 and Sentinel-2 images, the Landsat 8 panchromatic (PAN) band was also incorporated in the fusion process. The experimental results showed that the proposed approach is effective for fusing Landsat 8 with Sentinel-2 data, and the use of the PAN band can decrease the errors introduced by LCLU changes. By fusion of Landsat 8 and Sentinel-2 data, more frequent observations can be produced for continuous monitoring (this is particularly valuable for areas that can be covered easily by clouds, thereby, contaminating some Landsat or Sentinel-2 observations), and the observations are at a consistent fine spatial resolution of 10 m. The products have great potential for timely monitoring of rapid changes.","1558-0644","","10.1109/TGRS.2017.2683444","UK-China STFC Newton Agri-Tech Program “Remote Sensing for Sustainable Intensification in China through Improved Farm Decision-Making.”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894218","Downscaling;global monitoring;image fusion;Landsat 8 Operational Land Imager (OLI);Sentinel-2 Multispectral Imager (MSI)","Satellites;Remote sensing;Earth;Spatial resolution;Monitoring;Image fusion;MODIS","land cover;land use;remote sensing by radar","Landsat 8 OLI;Sentinel-2 MSI Data;fine spatial resolution satellite imaging mission;geographic coordinate system;Landsat 8 Operational Land Imager;Sentinel-2 Multispectral Imager data;land-cover/land-use;Landsat 8 panchromatic","","101","","50","IEEE","7 Apr 2017","","","IEEE","IEEE Journals"
"An Inversion-Based Fusion Method for Inland Water Remote Monitoring","Y. Guo; Y. Li; L. Zhu; Q. Wang; H. Lv; C. Huang; Y. Li","Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing Normal University, Nanjing, China; Jiangsu Provincial Key Laboratory of Carbon and Nitrogen Cycle Processes and Pollution Control, Nanjing Normal University, Nanjing, China; Satellite Environment Application Center, Ministry of Environment Protection, Beijing, China; Satellite Environment Application Center, Ministry of Environment Protection, Beijing, China; Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing Normal University, Nanjing, China; Jiangsu Center for Collaborative Innovation in Geographical Information Resource Development and Application, Nanjing Normal University, Nanjing, China; School of Tourism and City Management, Zhejiang Gongshang University, Hangzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","12","5599","5611","Although remote sensing technology has been widely used to monitor inland water bodies, the lack of suitable data with high spatial and spectral resolutions has seriously obstructed the practical development of inland water color remote sensing. An inversion-based fusion (IBF) algorithm is proposed to fuse water color and high-spatial resolution images. The algorithm was applied to two datasets: The Hyperion simulated dataset (dataset #1) and a pair of Environmental Satellite 1 (HJ1, launched by China in 2009) and medium resolution imaging spectrometer images (dataset #2). The fusions are quantitatively and qualitatively compared with three widely used algorithms. The results show that the IBF algorithm performs better using both evaluation indexes and visual comparisons. A discussion of the free parameter window size n and the contribution of the low-resolution image (LRI) to the spatial distribution (WLRI) shows that a largern will result in both greater model errors and better control of geometric errors, whereas WLRI helps stabilize the algorithm. Finally, chlorophyll-a concentration maps are developed from the fusions. The significant advantage of the IBF-derived chlorophyll-a concentration map indicates that the IBF algorithm has the potential to advance the monitoring of optical complex inland water.","2151-1535","","10.1109/JSTARS.2016.2615125","National Natural Science Foundation of China(grant numbers:41271343,41501374,41671340,41101378); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LQ16D010001); Priority Academic Program Development of Jiangsu Higher Education Institutions(grant numbers:164320H101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769316","HJ1-CCD;image fusion;inland optical complex inland water monitoring;medium resolution imaging spectrometer (MERIS)","Spatial resolution;Remote sensing;Monitoring;Satellites;Optical imaging;Optical sensors","environmental monitoring (geophysics);geophysical image processing;hydrochemistry;image fusion;inverse problems;organic compounds;remote sensing;water pollution","chlorophyll-a concentration map;low-resolution images;medium resolution imaging spectrometer images;AD 2009;China;Environmental satellite;Hyperion simulated dataset;inland water color remote sensing;inland water body monitoring;remote sensing technology;inland water remote monitoring;inversion-based fusion method","","11","","67","IEEE","19 May 2017","","","IEEE","IEEE Journals"
"A Technique for Simultaneous Visualization and Segmentation of Hyperspectral Data","A. Meka; S. Chaudhuri","Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India; Department of Electrical Engineering, Indian Institute of Technology Bombay, Mumbai, India","IEEE Transactions on Geoscience and Remote Sensing","9 Oct 2014","2015","53","4","1707","1717","In this paper, we propose an optimization-based method for simultaneous fusion and unsupervised segmentation of hyperspectral remote sensing images by exploiting redundancy in the data. The hyperspectral data set is visualized as a single image obtained by weighted addition of all spectral points at each pixel location in the data set. The weights are optimized to improve those statistical characteristics of the fused image, which invoke an enhanced response from a human observer. A piecewise-constant smoothness constraint is imposed on the weights instead of the fused image by minimization of its 3-D total-variation norm, thus preventing the fused image from blurring. The optimal recovery of the weight matrix additionally provides useful information in segmenting the hyperspectral data set spatially. We provide ample experimental results to substantiate the usefulness of the proposed method.","1558-0644","","10.1109/TGRS.2014.2346653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891192","Hyperspectral visualization;segmentation;TV-norm;Hyperspectral visualization;segmentation;TV-norm","Hyperspectral imaging;Image segmentation;Arrays;Data visualization;Optimization;Image fusion","geophysical image processing;hyperspectral imaging;image fusion;image segmentation;redundancy;remote sensing","hyperspectral data visualization;hyperspectral data segmentation;optimization based method;image fusion;unsupervised segmentation;hyperspectral remote sensing images;data redundancy;piecewise constant smoothness constraint;3D total variation norm minimization;blurring","","14","","36","IEEE","4 Sep 2014","","","IEEE","IEEE Journals"
"Multiangle BSAR Imaging Based on BeiDou-2 Navigation Satellite System: Experiments and Preliminary Results","T. Zeng; D. Ao; C. Hu; T. Zhang; F. Liu; W. Tian; K. Lin","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","10 Jun 2015","2015","53","10","5760","5773","This paper analyzes the multiangle imaging results for bistatic synthetic aperture radar (BSAR) based on global navigation satellite systems (GNSS-BSAR). Due to the shortcoming of GNSS-BSAR images, a multiangle observation and data processing strategy based on BeiDou-2 navigation satellites was put forward to improve the quality of images and the value of system application. Twenty-six BSAR experiments were conducted and analyzed in different configurations. Furthermore, a region-based fusion algorithm using region-of-interest (ROI) segmentation was proposed to generate a high-quality fusion image. Based on the fusion image, typical targets such as water area, vegetation area, and artificial targets were compared and interpreted among single/multiple-angle images. The results reveal that the multiangle imaging method was a good technique to enhance image information, which might extend the applications of GNSS-BSAR.","1558-0644","","10.1109/TGRS.2015.2430312","National Natural Science Foundation of China(grant numbers:61120106004,61225005,61427802,61471038); Chang Jiang Scholars Program(grant numbers:T2012122); Beijing Higher Education Young Elite Teacher Project(grant numbers:YETP1168); 111 Project of China(grant numbers:B14010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112619","Bistatic synthetic aperture radar (BSAR);global navigation satellite system (GNSS);image fusion;image interpretation;multiangle;Bistatic synthetic aperture radar (BSAR);global navigation satellite system (GNSS);image fusion;image interpretation;multiangle","Satellites;Imaging;Global Positioning System;Image resolution;Radar imaging;Synthetic aperture radar","geophysical image processing;Global Positioning System;image fusion;image segmentation;radar imaging;remote sensing by radar;synthetic aperture radar","image information enhancement;multiangle imaging method;image fusion;ROI segmentation;region of interest segmentation;region-based fusion algorithm;image quality improvement;data processing strategy;multiangle observation;GNSS-BSAR images;global navigation satellite system;bistatic synthetic aperture radar;BeiDou-2 navigation satellite system;multiangle BSAR imaging","","39","","37","IEEE","26 May 2015","","","IEEE","IEEE Journals"
"A Manifold Alignment Approach for Hyperspectral Image Visualization With Natural Color","D. Liao; Y. Qian; J. Zhou; Y. Y. Tang","College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; School of Information and Communication Technology, Griffith University, Nathan, Qld., Australia; Faculty of Science and Technology, University of Macau, Macau, China","IEEE Transactions on Geoscience and Remote Sensing","27 Apr 2016","2016","54","6","3151","3162","The trichromatic visualization of hundreds of bands in a hyperspectral image (HSI) has been an active research topic. The visualized image shall convey as much information as possible from the original data and facilitate easy image interpretation. However, most existing methods display HSIs in false color, which contradicts with user experience and expectation. In this paper, we propose a new framework for visualizing an HSI with natural color by the fusion of an HSI and a high-resolution color image via manifold alignment. Manifold alignment projects several data sets to a shared embedding space where the matching points between them are pairwise aligned. The embedding space bridges the gap between the high-dimensional spectral space of the HSI and the RGB space of the color image, making it possible to transfer natural color and spatial information in the color image to the HSI. In this way, a visualized image with natural color distribution and fine spatial details can be generated. Another advantage of the proposed method is its flexible data setting for various scenarios. As our approach only needs to search a limited number of matching pixel pairs that present the same object, the HSI and the color image can be captured from the same or semantically similar sites. Moreover, the learned projection function from the hyperspectral data space to the RGB space can be directly applied to other HSIs acquired by the same sensor to achieve a quick overview. Our method is also able to visualize user-specified bands as natural color images, which is very helpful for users to scan bands of interest.","1558-0644","","10.1109/TGRS.2015.2512659","National Basic Research Program of China(grant numbers:2012CB31640); National Natural Science Foundation of China(grant numbers:61571393,61273244); Research Grants of University of Macau(grant numbers:MYRG2015-00049-FST,MYRG2015-00050-FST); Science and Technology Development Fund(grant numbers:100-2012-A3,026-2013-A); Australian Research Councils DECRA Projects(grant numbers:DE120102948); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381681","Hyperspectral image (HSI);image fusion;manifold alignment;visualization;Hyperspectral image (HSI);image fusion;manifold alignment;visualization","Manifolds;Image color analysis;Data visualization;Color;Hyperspectral imaging;Imaging","hyperspectral imaging;image colour analysis;image fusion;remote sensing","hyperspectral image visualization;trichromatic visualization;image interpretation;hyperspectral image fusion;high-resolution color image;manifold alignment;RGB space;natural color distribution;hyperspectral data","","30","","38","IEEE","13 Jan 2016","","","IEEE","IEEE Journals"
"Subpixel Mapping Based on Multisource Remote Sensing Fusion Data for Land-Cover Classes","P. Wang; Y. Wang; L. Zhang; K. Ni","Hubei Key Laboratory of Regional Development and Environment Response, Hubei University, Wuhan, China; Key Laboratory of Radar Imaging and Microwave Photonics, Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Shanghai Autonomous Intelligent Unmanned System Science Center, Tongji University, Shanghai, China; Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Subpixel mapping (SPM) based on multisource remote sensing fusion data (MRSFD) for land-cover classes, called SPM-MRSFD, is proposed in this letter. First, the original hyperspectral image and the auxiliary panchromatic image are fused to produce the high spatial and spectral resolution fused image by pan-sharpening technology. Second, the fused image with spatial–spectral information and the auxiliary digital surface model (DSM) of light detection and ranging (LiDAR) with elevation information are fused to obtain the MRSFD with spatial–spectral elevation information by feature fusion. Finally, the fractional images with the proportions of subpixels belonging to land-cover classes are derived by unmixing the MRSFD, and the classes labels are allocated to subpixels to obtain the final SPM result according to these proportions’ information. The main contribution of this work is that multiple types of auxiliary information (i.e., spatial–spectral elevation information) from MRSFD is fully utilized and the accuracy of SPM result is improved. Experimental results show that SPM-MRSFD obtains more accurate mapping results than state-of-the-art SPM methods.","1558-0571","","10.1109/LGRS.2021.3072943","National Natural Science Foundation of China(grant numbers:61801211); Fundamental Research Funds for the Central Universities(grant numbers:NZ2020009); Open Research Project of the Hubei Key Laboratory of Intelligent Geo-Information Processing(grant numbers:KLIGIP-2019A05); Open Research Project of the State Key Laboratory of Geo-Information Engineering(grant numbers:SKLGIE2019-M-3-4); Open Research Project of the Hubei Key Laboratory of Regional Development and Environment Response Fundamental(grant numbers:2020(B)004); National Key Research and Development Program of China(grant numbers:2018YFE0101000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9411930","Feature fusion;land-cover classes;multisource remote sensing data;pan-sharpening;subpixel mapping (SPM)","Hyperspectral imaging;Laser radar;Principal component analysis;Laboratories;Spatial resolution;Mathematical model;Feature extraction","geophysical image processing;geophysical signal processing;geophysical techniques;geophysics computing;image classification;image fusion;image resolution;land cover;optical radar;remote sensing","subpixel mapping;multisource remote sensing fusion data;land-cover classes;original hyperspectral image;auxiliary panchromatic image;high spatial resolution fused image;spectral resolution fused image;spatial-spectral information;auxiliary digital surface model;spatial-spectral elevation information;feature fusion;fractional images;subpixels;classes labels;final SPM result;state-of-the-art SPM methods;SPM-MRSFD","","2","","21","IEEE","23 Apr 2021","","","IEEE","IEEE Journals"
"Change Detection in Multitemporal SAR Images Based on Slow Feature Analysis Combined With Improving Image Fusion Strategy","W. Li; X. Xiao; P. Xiao; H. Wang; F. Xu","Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 May 2022","2022","15","","3008","3023","Change detection in multitemporal synthetic aperture radar (SAR) images has been an important research content in the field of remote sensing for a long time. In this article, based on the slow feature analysis (SFA) theory and the nonsubsampled contourlet transform (NSCT) algorithm, we propose a novel unsupervised change detection method called NSCT nonlocal means (NSCT-NLM). The powerful extraction to the changed information of SFA and the superior information fusion of NSCT are jointly adopted in this method. The main framework consists of the following parts. First, SFA and the log-ratio operator are used to generate difference images (DIs) independently. Then, the NSCT is used to fuse two DIs into a new higher quality DI. The newly fused DI combines the complementary information of the two kinds of original DI. Therefore, the contrast of the changed regions and unchanged regions is greatly enhanced, as well as the changed details are preserved more completely. Furthermore, an NLM filtering algorithm is employed to suppress the strong speckles in the fused DI. We use the fuzzy C-means algorithm to generate the final binary change map. The experiments are carried out on two public datasets and three real-world SAR datasets from different scenarios. The results demonstrate that the proposed method has higher detection accuracy by comparing with the reference methods.","2151-1535","","10.1109/JSTARS.2022.3166234","Natural Science Foundation of Shanghai(grant numbers:22ZR1406700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755046","Nonsubsampled contourlet transform (NSCT);slow feature analysis (SFA);synthetic aperture radar (SAR) images;unsupervised change detection","Radar polarimetry;Change detection algorithms;Feature extraction;Transforms;Synthetic aperture radar;Image fusion;Data mining","fuzzy set theory;image filtering;radar detection;radar imaging;remote sensing by radar;speckle;synthetic aperture radar;transforms","multitemporal SAR images;slow feature analysis combined;improving image fusion strategy;multitemporal synthetic aperture radar images;remote sensing;slow feature analysis theory;SFA;nonsubsampled contourlet transform algorithm;unsupervised change detection method;NSCT-NLM;superior information fusion;log-ratio operator;difference images;NLM filtering algorithm;fuzzy C-means algorithm;final binary change map;real-world SAR datasets","","","","60","CCBY","11 Apr 2022","","","IEEE","IEEE Journals"
"Strategies Combining Spectral Angle Mapper and Change Vector Analysis to Unsupervised Change Detection in Multispectral Images","H. Zhuang; K. Deng; H. Fan; M. Yu","Jiangsu Key Laboratory of Resources and Environmental Information Engineering, China University of Mining and Technology, Xuzhou, China; Jiangsu Key Laboratory of Resources and Environmental Information Engineering, China University of Mining and Technology, Xuzhou, China; Jiangsu Key Laboratory of Resources and Environmental Information Engineering, China University of Mining and Technology, Xuzhou, China; School of Environment Science and Spatial Informatics, China University of Mining and Technology, Xuzhou, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2016","13","5","681","685","Change vector analysis (CVA) has been widely used in change detection of multitemporal multispectral images, whereas the use of a spectral angle mapper (SAM) is always ignored. CVA and SAM describe the difference from the perspective of vector magnitude and direction, respectively. Combining CVA and SAM can fully utilize the spectral vector and obtain a better change map. In this context, two strategies have been proposed: 1) a novel hybrid feature vector is constructed using a spectral angle and a change vector, which is utilized in SAM and CVA, respectively, and 2) a novel difference image is generated by using the autoadapted fusion strategy, which fuses the difference images acquired by utilizing CVA and SAM. During the fusion process, the autoadapted weight is defined by employing the entropy of the difference image. Experimental results on both simulated images and multitemporal multispectral images validate the effectiveness of the proposed methods.","1558-0571","","10.1109/LGRS.2016.2536058","Natural Science Foundation of China(grant numbers:41272389); Special Fund for Public Projects of National Administration of Surveying, Mapping, and Geoinformation of China(grant numbers:201412016); Basic Research Project of Jiangsu Province (Natural Science Foundation)(grant numbers:BK20130174); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436755","Autoadapted fusion strategy (AFS);change detection;change vector analysis (CVA);hybrid feature vector (HFV);multispectral image;spectral angle mapper (SAM);Autoadapted fusion strategy (AFS);change detection;change vector analysis (CVA);hybrid feature vector (HFV);multispectral image;spectral angle mapper (SAM)","Entropy;Remote sensing;Feature extraction;Gaussian distribution;Transforms;Context;Hybrid power systems","feature extraction;hyperspectral imaging;image fusion;remote sensing","spectral angle mapper;change vector analysis;multispectral image change detection;multitemporal multispectral image;vector magnitude;vector direction;spectral vector;hybrid feature vector;autoadapted fusion strategy;fusion process;simulated image","","63","","27","IEEE","18 Mar 2016","","","IEEE","IEEE Journals"
"Infrared Small Target Detection Using Local Feature-Based Density Peaks Searching","Q. Zhu; S. Zhu; G. Liu; Z. Peng","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","5 Apr 2022","2022","19","","1","5","In this letter, we propose a new local feature-based method for the detection of infrared small targets with the density feature map that can effectively suppress noise in the feature domain. First, we combine the local tetra pattern (LTrP) and the second-order LTrP to generate the density feature map. Second, we apply density peaks searching to the feature map to obtain candidate targets. Third, we generate two local features, i.e., the entropy and third-order moment, for each image patch whose center is a candidate target and then fuse them to employ the fused feature to find the real target. The experimental results demonstrate that our proposed method achieves better performance compared with the state-of-the-art approaches.","1558-0571","","10.1109/LGRS.2022.3157051","National Key Basic Research Program of China(grant numbers:U20A20184); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730832","Density peaks;feature;fusion;infrared small target;local tetra pattern (LTrP)","Feature extraction;Object detection;Entropy;Robustness;Indexes;Position measurement;Geoscience and remote sensing","entropy;geophysical image processing;image fusion;object detection;remote sensing;search problems","infrared small target detection;local feature-based density peaks searching;density feature map;second-order LTrP;candidate target;feature fusion;second-order local tetra pattern;entropy;third-order moment;image patch","","3","","15","IEEE","8 Mar 2022","","","IEEE","IEEE Journals"
"Spectral Image Classification From Multi-Sensor Compressive Measurements","J. M. Ramirez; H. Arguello","Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia","IEEE Transactions on Geoscience and Remote Sensing","27 Dec 2019","2020","58","1","626","636","Spectral image classification is an active research topic in remote sensing. In this sense, various multi-sensor spectral image fusion algorithms have been recently evaluated via pixel-based classification. In general, the sizes of multi-sensor images challenge the storing and processing capabilities of sensing systems. Therefore, different image fusion algorithms from measurements captured by multi-resolution compressive spectral imaging (CSI) sensors have been proposed. However, the computational costs for reconstructing and fusing spectral images from compressive measurements are high, and these approaches do not consider the huge amount of information embedded in acquired data. In this article, a spectral image classification scheme from multi-sensor CSI projections is developed. Specifically, this scheme includes a feature extraction procedure that exploits the fact that CSI data contain relevant information of the spectral image, and therefore, low-dimensional features can be obtained from measurements. Furthermore, a fusion model is presented to combine the information of the extracted features with the aim of estimating high-resolution classification attributes. Then, a pixel-based classifier is applied to the fused features with the goal of labeling the corresponding high-resolution spectral image. The performance of the proposed classification scheme is compared to other methods on the Salinas Valley data set for different supervised classifiers and various downsampling settings. Extensive simulations on the Pavia University data set are also shown, where the proposed method outperforms other classification approaches that reconstruct and fuse from compressive measurements. Finally, the effectiveness of the proposed classification approach is validated in real multi-sensor data.","1558-0644","","10.1109/TGRS.2019.2938724","Universidad Industrial de Santander; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851409","Compressive spectral imaging (CSI);feature extraction;feature fusion;spectral image classification","Feature extraction;Image coding;Apertures;Optical imaging;Optical sensors","feature extraction;geophysical image processing;image classification;image fusion;image resolution;image sensors;remote sensing","multisensor compressive measurements;remote sensing;high-resolution spectral image;multisensor data;classification approach;high-resolution classification;multisensor CSI projections;spectral image classification scheme;multiresolution compressive spectral imaging sensors;image fusion algorithms;sensing systems;multisensor images;pixel-based classification;multisensor spectral image fusion algorithms","","11","","41","IEEE","27 Sep 2019","","","IEEE","IEEE Journals"
"HMF-Former: Spatio-Spectral Transformer for Hyperspectral and Multispectral Image Fusion","T. You; C. Wu; Y. Bai; D. Wang; H. Ge; Y. Li","School of Software, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, Aberystwyth University, Aberystwyth, U.K.; School of Physics and Electronic Information, Yan’an University, Yan’an, China; Piesat Information Technology Company Ltd., Beijing, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","2 Feb 2023","2023","20","","1","5","The key to hyperspectral image (HSI) and multispectral image (MSI) fusion is to take advantage of the properties of interspectra self-similarities of HSIs and spatial correlations of MSIs. However, leading convolutional neural network (CNN)-based methods show shortcomings in capturing long-range dependencies and self-similarity prior. To this end, we propose a simple yet efficient Transformer-based network, hyperspectral and multispectral image fusion (HMF)-Former, for the HSI/MSI fusion. The HMF-Former adopts a U-shaped architecture with a spatio-spectral Transformer block (SSTB) as the basic unit. In the SSTB, embedded spatial-wise multihead self-attention (Spa-MSA) and spectral-wise multihead self-attention (Spe-MSA) effectively capture interactions of spatial regions and interspectra dependencies, respectively. They are consistent with the properties of spatial correlations of MSIs and interspectra self-similarities of HSIs. In addition, specially designed SSTB enables the HMF-Former to capture both local and global features while maintaining linear complexity. Extensive experiments on four benchmark datasets show that our method significantly outperforms state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3229692","National Natural Science Foundation of China(grant numbers:62271400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989404","Hyperspectral image (HSI) and multispectral image (MSI) fusion;multihead self-attention (MSA);remote sensing;Transformer","Transformers;Image reconstruction;Hyperspectral imaging;Computational complexity;Correlation;Task analysis;Satellites","","","","","","22","IEEE","15 Dec 2022","","","IEEE","IEEE Journals"
"Smoothing Filter-Based Panchromatic Spectral Decomposition for Multispectral and Hyperspectral Image Pansharpening","M. Wang; G. Xie; Z. Zhang; Y. Wang; S. Xiang; Y. Pi","State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; Institute of Beijing Remote Sensing Information, Beijing, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 May 2022","2022","15","","3612","3625","This article proposes an efficient and high-fidelity panchromatic (PAN) spectral decomposition method based on smoothing filtering for multispectral (MS) and hyperspectral (HS) image pansharpening. The proposed method assumes that the high-frequency spatial details at the same scale are the same for different spectral images captured by the satellite at the same time. When the PAN image is prefiltered and down-sampled to the MS scale, it will have the same high-frequency spatial detail as the MS image, with only low-frequency spectral differences. Prefiltering for antialiasing when downsampling. Then, the spectral decomposition coefficients from down-sampled PAN image to MS image can be calculated on MS scale. The low-frequency spectral information of an image at different scales is the same, the spectral decomposition coefficients on the MS scale can be up-sampled to the PAN scale, and the original PAN image can be spectrally decomposed to obtain the sharpened image. The proposed method only decomposes the low-frequency spectrum, and the high-frequency spatial details are the same as the original PAN image, the spatial detail is well preserved. This article verifies the effect of the proposed method on MS and HS image sharpening through experiments, and the results show that the proposed method is better than the comparison method. This article also controls other variables to compare with the HPM method. The results show that the hybrid quality with no reference and spatial distortion index ($D_{s}$) of the proposed method are better than the HPM method.","2151-1535","","10.1109/JSTARS.2022.3170488","IEEE Publication Technology Group; National Natural Science Foundation of China(grant numbers:61825103,91838303,61901307,91638301,91738302,42192583); Open Research Fund of the State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University(grant numbers:20E01); Key Research and Development Plan Project of Hubei Province(grant numbers:2020BIB006); Natural Science Foundation of Hubei Province(grant numbers:2020CFA001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763424","Hyperspectral (HS);multispectral (MS);pan-sharpening;spectral decomposition","Pansharpening;Spatial resolution;Satellites;Indexes;Hyperspectral imaging;Principal component analysis;Earth","geophysical image processing;hyperspectral imaging;image filtering;image fusion;image resolution;remote sensing;spectral analysis","MS scale;low-frequency spectral information;spectral decomposition coefficients;PAN scale;sharpened image;low-frequency spectrum;high-frequency spatial detail;HS image sharpening;smoothing filter-based panchromatic spectral decomposition;hyperspectral image pansharpening;MS image;low-frequency spectral differences;down-sampled PAN image","","2","","60","CCBY","26 Apr 2022","","","IEEE","IEEE Journals"
"Mapping Paddy Rice Area and Yields Over Thai Binh Province in Viet Nam From MODIS, Landsat, and ALOS-2/PALSAR-2","K. Guan; Z. Li; L. N. Rao; F. Gao; D. Xie; N. T. Hien; Z. Zeng","National Center for Supercomputing Applications, University of Illinois at Urbana Champaign, Urbana, IL, USA; School for the Environment, University of Massachusetts Boston, Boston, MA, USA; Asian Development Bank, Manila, Philippines; U. S. Department of Agriculture, Agricultural Research Service, Hydrology and Remote Sensing Laboratory, Beltsville, MD, USA; State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Center for Informatics and Statistics, Ministry of Agriculture and Rural Development, Hanoi, Viet Nam; Department of Civil and Environmental Engineering, Princeton University, Princeton, NJ, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Jul 2018","2018","11","7","2238","2252","This study uses multiple satellite datasets to map paddy rice areas and yields for the Thai Binh Province, Viet Nam, over the summer growing season of 2015. The major datasets used are: first, surface reflectance and vegetation indices (VI) by fusing the optical observations from the Landsat sensors and the MODerate Resolution Imaging Spectroradiometer; and second, the L-band radar data from the PALSAR-2 sensor onboard the Advanced Land Observing Satellite 2. We find that although the fused VI time series are not necessarily beneficial for paddy rice mapping, the fusion datasets reduce observational gaps and allow us to better identify peak VI values and derive their empirical relationships with crop-cutting yield data (R2 = 0.4 for all the rice types, and R2 = 0.69 for the dominant rice type -58% of all the sampled fields). The L-band radar data have slightly lower performance in rice mapping than the optical satellite data, while it has much less contribution to yield estimation than the optical data. Furthermore, our study suggests the geolocation errors of satellite images be taken into account when selecting small sample are as for crop cutting. This practice will ensure the representativeness of crop-cutting sample areas with regard to satellite observations and thus better linkages between field data and satellite pixels for yield modeling. We also highlight the need of crop-cutting data from multiple years and/or at different regions to account for the spatial and temporal variations of harvest index to improve the spatially explicit rice yield estimates through satellite observations.","2151-1535","","10.1109/JSTARS.2018.2834383","Asian Development Bank; NASA New Investigator(grant numbers:NNX16AI56G); ACES Office of International; University of Illinois at Urbana Champaign; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8375090","Advanced Land Observing Satellite 2 (ALOS-2);agriculture;crop yield;image fusion;Landsat;moderate resolution imaging spectroradiometer (MODIS);paddy rice;Viet Nam","Agriculture;Satellites;Optical sensors;Remote sensing;Production;Earth;MODIS","agriculture;crops;geophysical image processing;remote sensing;remote sensing by radar;synthetic aperture radar;time series;vegetation;vegetation mapping","satellite pixels;crop-cutting yield data;rice types;dominant rice type;optical satellite data;optical data;satellite images;crop-cutting sample areas;field data;peak VI values;observational gaps;fusion datasets;paddy rice mapping;fused VI time series;Advanced Land Observing Satellite 2;PALSAR-2 sensor;L-band radar data;MODerate Resolution Imaging Spectroradiometer;Landsat sensors;optical observations;vegetation indices;surface reflectance;summer growing season;map paddy rice areas;multiple satellite datasets;ALOS-2/PALSAR-2;Viet Nam;Thai Binh Province;mapping paddy rice area;satellite observations;spatially explicit rice yield;crop-cutting data;yield modeling","","18","","72","IEEE","7 Jun 2018","","","IEEE","IEEE Journals"
"MCANet: A Multidimensional Channel Attention Residual Neural Network for Pansharpening","D. Lei; P. Chen; L. Zhang; W. Li","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Geoscience and Remote Sensing","28 Sep 2022","2022","60","","1","16","In the remote sensing image fusion field, fusion methods based on deep learning (DL) are the latest techniques in panchromatic sharpening (pansharpening). However, existing pansharpening methods based on neural networks cannot adequately inject the spatial feature information of panchromatic (PAN) images into fusion images, and they do not exploit the feature relationships between spatial locations, such as rows and columns of feature maps. To solve these problems, a multidimensional channel attention residual neural network (MCANet) is proposed in this article. To preserve the structural information in PAN images, a two-stream detail injection (TSDI) module is proposed, and the local skip connection operation is adopted to mine more spectral and structural information. A multidimensional channel attention (MCA) module is also designed to enable the network to learn the nonlinear mapping relationships between image spatial locations. In addition, a multiscale feature fusion (MSFF) module is designed to improve feature representation in the image fusion process, which is conducive to improving the pansharpening effect. The experimental results on the WorldView-2 (WV-2), GaoFen-2 (GF-2), and QuickBird (QB) datasets demonstrate that the proposed method outperforms state-of-the-art methods both visually and quantitatively.","1558-0644","","10.1109/TGRS.2022.3205626","National Natural Science Foundation of China(grant numbers:61972060,U1713213,62027827); National Key Research and Development Program of China(grant numbers:2019YFE0110800); Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0025,cstc2019cxcylirc-td0270); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9885234","Channel attention;detail injection;journal;pansharpening;residual network","Feature extraction;Pansharpening;Neural networks;Deep learning;Task analysis;Residual neural networks;Data mining","feature extraction;geophysical image processing;geophysical signal processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","pansharpening effect;MCANet;multidimensional channel attention residual neural network;remote sensing image fusion field;fusion methods;panchromatic sharpening;existing pansharpening methods;neural networks;spatial feature information;panchromatic images;fusion images;feature relationships;feature maps;structural information;PAN images;two-stream detail injection module;spectral information;multidimensional channel attention module;image spatial locations;multiscale feature fusion module;feature representation;image fusion process","","7","","44","IEEE","12 Sep 2022","","","IEEE","IEEE Journals"
"A Novel Radiometric Control Set Sample Selection Strategy for Relative Radiometric Normalization of Multitemporal Satellite Images","A. Moghimi; A. Mohammadzadeh; T. Celik; M. Amani","Department of Photogrammetry and Remote Sensing, Geomatics Engineering Faculty, K. N. Toosi University of Technology, Tehran, Iran; Department of Photogrammetry and Remote Sensing, Geomatics Engineering Faculty, K. N. Toosi University of Technology, Tehran, Iran; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; Wood Environment and Infrastructure Solutions, Ottawa, Canada","IEEE Transactions on Geoscience and Remote Sensing","24 Feb 2021","2021","59","3","2503","2519","This article presents a new relative radiometric normalization (RRN) method for multitemporal satellite images based on the automatic selection and multistep optimization of the radiometric control set samples (RCSS). A novel image-fusion strategy based on the fast local Laplacian filter is employed to generate a difference index using the complementary information extracted from the change vector analysis and absolute gradient difference of the bitemporal satellite images. The difference index is then segmented into changed and unchanged pixels using a fast level-set method. A novel local outlier method is then applied to the unchanged pixels of the bitemporal images to identify the initial RCSS, which are then scored by a novel unchanged purity index, and the histogram of the scores is used to produce the final RCSS. The RRN between the bitemporal images is achieved by adjusting the subject image to the reference image using orthogonal linear regression on the final RCSS. The proposed method is applied to seven different data sets comprised of bitemporal images acquired by various satellites, including Landsat TM/ETM+, Sentinel 2B, Worldview 2/3, and Aster. The experimental results show that the method outperforms the state-of-the-art RRN methods. It reduces the average root-mean-square error (RMSE) of the best baseline method (IR-MAD) by up to 32% considering all data sets.","1558-0644","","10.1109/TGRS.2020.2995394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9133139","Change vector analysis (CVA);multitemporal satellite images;radiometric control set samples (RCSS);relative radiometric normalization (RRN)","Radiometry;Indexes;Linear regression;Satellite broadcasting;Remote sensing;Image sensors;Sensors","geophysical image processing;geophysical techniques;image fusion;image segmentation;radiometry","Worldview 2/3;Aster;Sentinel 2B;Landsat TM/ETM+;baseline method;state-of-the-art RRN methods;data sets;reference image;subject image;final RCSS;novel unchanged purity index;initial RCSS;local outlier method;fast level-set method;unchanged pixels;bitemporal satellite images;absolute gradient difference;change vector analysis;difference index;fast local Laplacian filter;novel image-fusion strategy;automatic selection;relative radiometric normalization method;multitemporal satellite images;novel radiometric control set sample selection strategy","","15","","56","IEEE","3 Jul 2020","","","IEEE","IEEE Journals"
"Nonparametric Image Registration of Airborne LiDAR, Hyperspectral and Photographic Imagery of Wooded Landscapes","J. Lee; X. Cai; C. -B. Schönlieb; D. A. Coomes","Image Analysis Group, Department of Applied Mathematics and Theoretical Physics (DAMTP), University of Cambridge, Cambridge, U.K.; Image Analysis Group, Department of Applied Mathematics and Theoretical Physics (DAMTP), University of Cambridge, Cambridge, U.K.; Image Analysis Group, Department of Applied Mathematics and Theoretical Physics (DAMTP), University of Cambridge, Cambridge, U.K.; Forest Ecology and Conservation Group, Department of Plant Sciences, University of Cambridge, Cambridge, U.K.","IEEE Transactions on Geoscience and Remote Sensing","11 Aug 2015","2015","53","11","6073","6084","There is much current interest in using multisensor airborne remote sensing to monitor the structure and biodiversity of woodlands. This paper addresses the application of nonparametric (NP) image-registration techniques to precisely align images obtained from multisensor imaging, which is critical for the successful identification of individual trees using object recognition approaches. NP image registration, in particular, the technique of optimizing an objective function, containing similarity and regularization terms, provides a flexible approach for image registration. Here, we develop a NP registration approach, in which a normalized gradient field is used to quantify similarity, and curvature is used for regularization (NGF-Curv method). Using a survey of woodlands in southern Spain as an example, we show that NGF-Curv can be successful at fusing data sets when there is little prior knowledge about how the data sets are interrelated (i.e., in the absence of ground control points). The validity of NGF-Curv in airborne remote sensing is demonstrated by a series of experiments. We show that NGF-Curv is capable of aligning images precisely, making it a valuable component of algorithms designed to identify objects, such as trees, within multisensor data sets.","1558-0644","","10.1109/TGRS.2015.2431692","Airborne Research and Survey Facility of the U.K.'s Natural Environment Research Council; King Abdullah University of Science Technology and Wellcome Trust; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116541","Aerial photograph;hyperspectral image;image registration;light detection and ranging (LiDAR);remote sensing;Aerial photograph;hyperspectral image;image registration;light detection and ranging (LiDAR);remote sensing","Laser radar;Hyperspectral imaging;Image registration;Feature extraction;Sensors","geophysical image processing;image fusion;image registration;remote sensing by laser beam;vegetation mapping","Airborne LiDAR;wooded landscape hyperspectral imagery;wooded landscape photographic imagery;multisensor airborne remote sensing;woodland biodiversity;woodland structure;nonparametric image-registration techniques;multisensor imaging;individual trees identification;NP registration approach;normalized gradient field;data set fusion;multisensor data sets","","17","1","62","CCBY","2 Jun 2015","","","IEEE","IEEE Journals"
"Bioinspired Scene Classification by Deep Active Learning With Remote Sensing Applications","L. Zhang; G. Su; J. Yin; Y. Li; Q. Lin; X. Zhang; L. Shao","Key Laboratory of Crop Harvesting Equipment Technology of Zhejiang Province, Jinhua Polytechnic, Jinhua, China; College of Computer Sciences, Zhejiang University, Hangzhou, China; College of Computer Sciences, Zhejiang University, Hangzhou, China; College of Computer Sciences, Zhejiang University, Hangzhou, China; College of Computer Sciences, Zhejiang University, Hangzhou, China; College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE","IEEE Transactions on Cybernetics","4 Jul 2022","2022","52","7","5682","5694","Accurately classifying sceneries with different spatial configurations is an indispensable technique in computer vision and intelligent systems, for example, scene parsing, robot motion planning, and autonomous driving. Remarkable performance has been achieved by the deep recognition models in the past decade. As far as we know, however, these deep architectures are incapable of explicitly encoding the human visual perception, that is, the sequence of gaze movements and the subsequent cognitive processes. In this article, a biologically inspired deep model is proposed for scene classification, where the human gaze behaviors are robustly discovered and represented by a unified deep active learning (UDAL) framework. More specifically, to characterize objects’ components with varied sizes, an objectness measure is employed to decompose each scenery into a set of semantically aware object patches. To represent each region at a low level, a local–global feature fusion scheme is developed which optimally integrates multimodal features by automatically calculating each feature’s weight. To mimic the human visual perception of various sceneries, we develop the UDAL that hierarchically represents the human gaze behavior by recognizing semantically important regions within the scenery. Importantly, UDAL combines the semantically salient region detection and the deep gaze shifting path (GSP) representation learning into a principled framework, where only the partial semantic tags are required. Meanwhile, by incorporating the sparsity penalty, the contaminated/redundant low-level regional features can be intelligently avoided. Finally, the learned deep GSP features from the entire scene images are integrated to form an image kernel machine, which is subsequently fed into a kernel SVM to classify different sceneries. Experimental evaluations on six well-known scenery sets (including remote sensing images) have shown the competitiveness of our approach.","2168-2275","","10.1109/TCYB.2020.2981480","National Natural Science Foundation of China(grant numbers:61922064,U2033210); Zhejiang Provincial Natural Science Foundation(grant numbers:LR17F030001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9364931","Active learning;bioinspired;contaminated;gaze behavior;machine learning;multimodal;remote sensing","Semantics;Visualization;Kernel;Feature extraction;Support vector machines;Image recognition;Biological system modeling","computer vision;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image representation;object detection;remote sensing;support vector machines;visual perception","local-global feature fusion scheme;multimodal features;human visual perception;UDAL;human gaze behavior;semantically important regions;semantically salient region detection;partial semantic tags;deep GSP features;remote sensing images;bioinspired scene classification;computer vision;intelligent systems;deep recognition models;deep architectures;gaze movements;subsequent cognitive processes;biologically inspired deep model;unified deep active learning framework;semantically aware object patches;deep gaze shifting path representation learning;image kernel machine;kernel SVM","Humans;Neural Networks, Computer;Remote Sensing Technology;Semantics;Visual Perception","2","","74","IEEE","26 Feb 2021","","","IEEE","IEEE Journals"
"Multiscale Convolutional Neural Networks for Geospatial Object Detection in VHR Satellite Images","Q. Yao; X. Hu; H. Lei","State Key Laboratory of Cognitive Neuroscience and Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China; School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Department of Space Microwave Remote Sensing System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","23 Dec 2020","2021","18","1","23","27","Automatic object detection is a fundamental but challenging problem in very high-resolution (VHR) satellite images' interpretation. Recently, context-based feature pyramid detection architecture has been proposed, which generates high-quality fusion feature levels for multiscale object detection and significantly improves the accuracy of traditional detection frameworks. However, in feature pyramid architecture, small objects are easily lost in deep feature levels, and the context cues will be weakened simultaneously. Moreover, each feature level in the pyramids is simply constructed from single-level layers of the backbone, which are not representative enough for multiscale object detection. In this letter, an end-to-end multiscale convolutional neural network (MSCNN) is proposed, which is based on a unified multiscale backbone named EssNet for extracting features of diverse-scale objects in HRS images. The proposed EssNet backbone maintains the resolution of deep feature levels, which improves the capability of multiscale objects' feature expression. Meanwhile, a dilated bottleneck structure is introduced into the backbone, which generates high-quality semantic features and improves the prediction capability of multiscale objects. Finally, the whole network can be optimized end-to-end by minimizing a multitask loss. Experiments on publicly available NWPU VHR-10 benchmark and demonstrate that the proposed method outperforms several state-of-the-art detection approaches.","1558-0571","","10.1109/LGRS.2020.2967819","National Key R&D Program of China(grant numbers:2017YFB0502700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8979153","Convolutional neural networks (CNNs);multiscale features;object detection;very high-resolution (VHR) satellite images","Feature extraction;Object detection;Geospatial analysis;Remote sensing;Satellites;Semantics;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;geophysical techniques;image fusion;image resolution;object detection;satellite communication","multiscale object detection;unified multiscale backbone;diverse-scale objects;deep feature levels;high-quality semantic features;multiscale convolutional neural networks;geospatial object detection;VHR satellite images;automatic object detection;very high-resolution satellite images;high-quality fusion feature levels;feature pyramid architecture;single-level layers;context-based feature pyramid detection;EssNet backbone","","19","","16","IEEE","3 Feb 2020","","","IEEE","IEEE Journals"
"Fusing Multiseasonal Sentinel-2 Imagery for Urban Land Cover Classification With Multibranch Residual Convolutional Neural Networks","C. Qiu; L. Mou; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, German; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, German; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, German; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Weßling, Germany","IEEE Geoscience and Remote Sensing Letters","24 Sep 2020","2020","17","10","1787","1791","Exploiting multitemporal Sentinel-2 images for urban land cover classification has become an important research topic, since these images have become globally available at relatively fine temporal resolution, thus offering great potential for large-scale land cover mapping. However, appropriate exploitation of the images needs to address problems such as cloud cover inherent to optical satellite imagery. To this end, we propose a simple yet effective decision-level fusion approach for urban land cover prediction from multiseasonal Sentinel-2 images, using the state-of-the-art residual convolutional neural networks (ResNet). We extensively tested the approach in a cross-validation manner over a seven-city study area in central Europe. Both quantitative and qualitative results demonstrated the superior performance of the proposed fusion approach over several baseline approaches, including observation- and feature-level fusion.","1558-0571","","10.1109/LGRS.2019.2953497","China Scholarship Council (CSC); European Research Council (ERC) under the European Union’s Horizon 2020 Research and Innovation Program (So2Sat: Big Data for 4D Global Urban Mapping–1016 Bytes from Social Media to EO Satellites)(grant numbers:ERC-2016-StG-714087); Helmholtz Association under the Framework of the Young Investigators Group Signal Processing in Earth Observation (SiPEO)(grant numbers:VH-NG-1018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951229","Classification;fusion;long short-term memory (LSTM);multitemporal;nonlocal;residual convolutional neural network (ResNet);Sentinel-2;urban land cover","Urban areas;Earth;Convolutional neural networks;Satellites;Europe;Remote sensing;Task analysis","geophysical image processing;geophysical techniques;image classification;image fusion;land cover;neural nets;terrain mapping","multiseasonal Sentinel-2 imagery;urban land cover classification;multibranch residual convolutional neural networks;multitemporal Sentinel-2 images;relatively fine temporal resolution;large-scale land cover mapping;cloud cover;optical satellite imagery;urban land cover prediction;multiseasonal Sentinel-2 images;state-of-the-art residual convolutional neural networks;effective decision-level fusion approach;feature-level fusion","","15","","15","CCBY","7 Jan 2020","","","IEEE","IEEE Journals"
"A New Fusion Algorithm for Depth Images Based on Virtual Views","J. Liu; L. Zhang; Z. Wang; R. Wang","College of Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geomatics, Shandong University of Science and Technology, Qingdao, China","IEEE Transactions on Geoscience and Remote Sensing","22 Jan 2020","2020","58","2","1171","1181","Common depth image fusion methods use each original image as a reference plane and fuse the depth images using mutual projection. These methods can eliminate inconsistency between the depth images, but they cannot alleviate the point cloud redundancy and computational complexity. This article proposes a virtual view method for depth image fusion, defines a limited number of virtual views by means of view clustering, reduces the redundant calculations, and covers all scenes as much as possible. The depth image is merged ray by ray, and a reliable depth value is obtained via the F-test. Compared with the modified semiglobal matching (TSGM) stereo dense matching algorithm, the accuracy is improved by approximately 50% and the roughness is improved by approximately 50%. Compared with the classic surface reconstruction (SURE) fusion algorithm, there is more fusion depth value in each ray, and the accuracy and roughness are slightly improved. In addition, the algorithm of this article greatly reduces the number of reference planes.","1558-0644","","10.1109/TGRS.2019.2944303","National Natural Science Foundation of China(grant numbers:41801385,41571443,41971339); Natural Science Foundation of Shandong Province(grant numbers:ZR2018BD004,ZR2019QD010); Shandong Province Key R&D Program of China(grant numbers:2019GGX101049); Shandong University of Science and Technology(grant numbers:2017RCJJ076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880619","Depth image;fusion algorithm;oblique image;virtual view","Three-dimensional displays;Image fusion;Optical imaging;Cameras;Surface reconstruction;Fuses;Redundancy","image fusion","virtual view method;depth image fusion methods;F-test","","","","27","IEEE","23 Oct 2019","","","IEEE","IEEE Journals"
"Nonlocal Coupled Tensor CP Decomposition for Hyperspectral and Multispectral Image Fusion","Y. Xu; Z. Wu; J. Chanussot; P. Comon; Z. Wei","Grenoble Images Speech Signal and Control Laboratory (GIPSA-lab), CNRS, Grenoble Institute of Technology (Grenoble INP), Université Grenoble Alpes, Grenoble, France; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Grenoble Images Speech Signal and Control Laboratory (GIPSA-lab), CNRS, Grenoble Institute of Technology (Grenoble INP), Université Grenoble Alpes, Grenoble, France; Grenoble Images Speech Signal and Control Laboratory (GIPSA-lab), CNRS, Université Grenoble Alpes, Grenoble, France; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","30 Dec 2019","2020","58","1","348","362","Hyperspectral (HS) super-resolution, which aims at enhancing the spatial resolution of hyperspectral images (HSIs), has recently attracted considerable attention. A common way of HS super-resolution is to fuse the HSI with a higher spatial-resolution multispectral image (MSI). Various approaches have been proposed to solve this problem by establishing the degradation model of low spatial-resolution HSIs and MSIs based on matrix factorization methods, e.g., unmixing and sparse representation. However, this category of approaches cannot well construct the relationship between the high-spatial-resolution (HR) HSI and MSI. In fact, since the HSI and the MSI capture the same scene, these two image sources must have common factors. In this paper, a nonlocal tensor decomposition model for hyperspectral and multispectral image fusion (HSI-MSI fusion) is proposed. First, the nonlocal similar patch tensors of the HSI are constructed according to the MSI for the purpose of calculating the smooth order of all the patches for clustering. Then, the relationship between the HR HSI and the MSI is explored through coupled tensor canonical polyadic (CP) decomposition. The fundamental idea of the proposed model is that the factor matrices in the CP decomposition of the HR HSI's nonlocal tensor can be shared with the matrices factorized by the MSI's nonlocal tensor. Alternating direction method of multipliers is used to solve the proposed model. Through this method, the spatial structure of the MSI can be successfully transferred to the HSI. Experimental results on three synthetic data sets and one real data set suggest that the proposed method substantially outperforms the existing state-of-the-art HSI-MSI fusion methods.","1558-0644","","10.1109/TGRS.2019.2936486","National Natural Science Foundation of China(grant numbers:61701238,61772274,61471199,61976117,91538108,11431015,61501241,61671243,61802190); Natural Science Foundation of Jiangsu Province(grant numbers:BK20170858,BK20180018); Fundamental Research Funds for the Central Universities(grant numbers:30919011234,30917015104,30919011103); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); China Postdoctoral Science Foundation(grant numbers:2017M611814,2015M570450,2018T110502); Jiangsu Province Postdoctoral Science Foundation(grant numbers:1701148B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8835149","Coupled canonical polyadic (CP) decomposition;data fusion;hyperspectral images (HSIs);multispectral images (MSIs);nonlocal tensor","Spatial resolution;Hyperspectral imaging;Sparse matrices;Matrix decomposition","hyperspectral imaging;image colour analysis;image fusion;image representation;image resolution;matrix decomposition;optimisation;tensors","nonlocal coupled tensor CP decomposition;hyperspectral super-resolution;spatial resolution;hyperspectral image fusion;multispectral image fusion;HSI-MSI fusion methods;matrix factorization;high spatial resolution;canonical polyadic decomposition;alternating direction method of multipliers;ADMM optimization;false-color images","","58","","67","IEEE","12 Sep 2019","","","IEEE","IEEE Journals"
"Random Walks for Pansharpening in Complex Tight Framelet Domain","J. Wang; X. Yang; R. Zhu","Key Laboratory of Mathematics, Informatics, and Behavioral Semantics, Ministry of Education, School of Mathematics and Systems Science, Beihang University, Beijing, China; Key Laboratory of Mathematics, Informatics, and Behavioral Semantics, Ministry of Education, School of Mathematics and Systems Science, Beihang University, Beijing, China; Key Laboratory of Mathematics, Informatics, and Behavioral Semantics, Ministry of Education, School of Mathematics and Systems Science, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","24 Jun 2019","2019","57","7","5121","5134","In this paper, a new random walk (RW) pansharpening method on the basis of the complex framelet domain is proposed. In the process of fusion, the hidden Markov tree model is first established based on the statistical properties of complex high-pass framelet coefficients. On this basis, a novel RW fusion algorithm is presented. Then, the probabilities of complex framelet coefficients being allotted original images are solved by the linear system of equations. Based on these probabilities, the spatial details of the panchromatic image are selectively injected into the multispectral (MS) image to get a space-enhanced MS image. In the end, the GeoGye-1, WorldView-3, and WorldView-2 remote sensing image data sets are used to evaluate the performance of the presented method quantitatively and qualitatively. The results of the experiment show that our method outperforms some state-of-the-art approaches. It can improve the spatial resolution of the MS image while keeping the spectral information.","1558-0644","","10.1109/TGRS.2019.2897010","National Natural Science Foundation of China(grant numbers:61671002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672085","Complex tight framelet (CFT);graph representation;hidden Markov tree;multispectral (MS) image;panchromatic (PAN) image;pansharpening;random walks (RWs)","Transforms;Hidden Markov models;Image fusion;Spatial resolution;Probability;Computational modeling;Image edge detection","geophysical image processing;image denoising;image fusion;image resolution;remote sensing;trees (mathematics)","complex tight framelet domain;random walk pansharpening method;complex framelet domain;hidden Markov tree model;statistical properties;high-pass framelet coefficients;probabilities;complex framelet coefficients;panchromatic image;multispectral image;space-enhanced MS image;WorldView-2 remote sensing image data sets;RW fusion algorithm;GeoGye-1 remote sensing image data sets;MS image","","6","","54","IEEE","20 Mar 2019","","","IEEE","IEEE Journals"
"Learning Spectral Cues for Multispectral and Panchromatic Image Fusion","Y. Xing; S. Yang; Y. Zhang; Y. Zhang","Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Image Processing","8 Nov 2022","2022","31","","6964","6975","Recently, deep learning based multispectral (MS) and panchromatic (PAN) image fusion methods have been proposed, which extracted features automatically and hierarchically by a series of non-linear transformations to model the complicated imaging discrepancy. But they always pay more attention to the extraction and compensation of spatial details and use the mean squared error or mean absolute error as a loss function, regardless of the preservation of spectral information contained in multispectral images. For the sake of the improvements in both spatial and spectral resolution, this paper presents a novel fusion model that takes the spectral preservation into consideration, and learns the spectral cues from the process of generating a spectrally refined multispectral image, which is constrained by a spectral loss between the generated image and the reference image. Then these spectral cues are used to modulate the PAN features to obtain final fusion result. Experimental results on reduced-resolution and full-resolution datasets demonstrate that the proposed method can obtain a better fusion result in terms of visual inspection and evaluation indices when compared with current state-of-the-art methods.","1941-0042","","10.1109/TIP.2022.3215906","National Natural Science Foundation of China (NFSC)(grant numbers:62201467,U19B2037); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110544); Natural Science Basic Research Program of Shaanxi(grant numbers:2022JQ-686); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935814","Image fusion;pansharpening;spectral cues;spectral loss;generative adversarial networks","Feature extraction;Image fusion;Pansharpening;Task analysis;Spatial resolution;Remote sensing;Modulation","feature extraction;geophysical image processing;image fusion;image resolution;learning (artificial intelligence);sensor fusion;wavelet transforms","spectral cues;spectrally refined multispectral image;spectral loss;reference image;PAN features;final fusion result;reduced-resolution;full-resolution datasets;panchromatic image;deep learning;nonlinear transformations;complicated imaging discrepancy;compensation;spatial details;mean squared error;absolute error;spectral information;multispectral images;spatial resolution;spectral resolution;fusion model;spectral preservation","","","","58","IEEE","2 Nov 2022","","","IEEE","IEEE Journals"
"Spatial Resolution Enhancement of Remote Sensing Hyperspectral Images With Localized Spatial-Spectral Dictionary Pair","Y. Zhang; J. Tian; T. Zhao; S. Mei","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Physics and Information Engineering, Shanxi Normal University, Linfen, China; Huawei Technologies Company, Ltd., Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Access","8 Apr 2020","2020","8","","61051","61069","The hyperspectral image (HSI) is capable of providing abundant and detailed spectral information in hundreds of contiguous spectral bands. While due to some practical reasons, its spatial resolution is generally lower than that of multispectral image (MSI) and panchromatic image. To deal with the limited spatial resolution issue of HSI, a low resolution (LR) HSI can be fused with a high resolution (HR) MSI of the same scene to generate an HR HSI. A novel dictionary-based HSI and MSI fusion method (SSLDF method) is proposed in this paper, in which a localized spatial-spectral dictionary pair incorporating both spatial and spectral information simultaneously is constructed and adopted, rather than the traditional spectral or spatial one. To construct the HR and LR dictionary pair, HR MSI and its spatial degradation (LR MSI) are divided into overlapped subimages. Furthermore, to reduce the dictionary scale and hence to efficiently reduce the computation cost, a localized strategy is employed for dictionary construction rather than a global one, which makes atoms of the spatial-spectral dictionary actually all patches within the subimage. Based on the appropriate assumption that the LR HSI and HR HSI (expected fusion result) can be collaboratively represented by LR dictionary and HR dictionary respectively sharing the same set of representation coefficients, the desired HR HSI is reconstructed by HR dictionary and the collaborative representation coefficients obtained with LR HSI and LR dictionary. In simulative experiments, the newly proposed SSLDF method is validated and compared with both state-of-the-art dictionary-based fusion methods and representative fusion methods not limited to the dictionary-based ones. Simulative experimental results illustrate that the proposed fusion method is capable of producing better or comparable fused results compared with these representative fusion methods. Its simple structure as well as low computation cost makes it quite promising in practical applications.","2169-3536","","10.1109/ACCESS.2020.2981690","National Natural Science Foundation of China(grant numbers:61671383); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040549","Collaborative representation;dictionary;hyperspectral;image fusion;multispectral;resolution enhancement","Dictionaries;Spatial resolution;Image reconstruction;Image fusion;Hyperspectral sensors","dictionaries;geophysical image processing;hyperspectral imaging;image classification;image coding;image fusion;image reconstruction;image representation;image resolution;remote sensing;spectral analysis","spatial resolution enhancement;remote sensing hyperspectral images;localized spatial-spectral dictionary pair;abundant information;spectral information;contiguous spectral bands;panchromatic image;spatial resolution issue;high resolution MSI;HR HSI;novel dictionary-based HSI;MSI fusion method;SSLDF method;spatial information;HR MSI;spatial degradation;LR MSI;dictionary scale;localized strategy;dictionary construction;LR HSI;expected fusion result;LR dictionary;HR dictionary;state-of-the-art dictionary-based fusion methods;representative fusion methods","","2","","60","CCBY","18 Mar 2020","","","IEEE","IEEE Journals"
"A Compressed-Sensing-Based Pan-Sharpening Method for Spectral Distortion Reduction","M. Ghahremani; H. Ghassemian","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","IEEE Transactions on Geoscience and Remote Sensing","10 Mar 2016","2016","54","4","2194","2206","Recently, the compressed sensing (CS) theory has become an interesting topic for pan-sharpening of multispectral images. The CS theory ensures that, under the sparsity regularization, an unknown sparse signal can be exactly recovered from a drastically smaller number of linear measurements. In this paper, we propose a CS-based approach for fusion of the multispectral and panchromatic satellite images. The contribution of this paper is twofold. First, with the spatial and spectral characteristics of the satellite images, we assume that each patch of the unknown high spatial resolution intensity (HRI) component can be represented as a linear combination of atoms in a dictionary trained only from the panchromatic image; thus, the problem of generating an optimal dictionary is solved. Second, we propose an iterative algorithm to obtain the sparsest coefficients. The sparsest coefficients ensure that the estimated HRI component can be correctly recovered from the panchromatic image. The IKONOS, QuickBird, and WorldView-2 data are used to evaluate the performance of the proposed method. The experimental results demonstrate that the proposed method generates high-quality pan-sharpened multispectral bands quantitatively and perceptually.","1558-0644","","10.1109/TGRS.2015.2497309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335613","Compressed sensing (CS);image fusion;multispectral data;panchromatic data;pan-sharpening;sparse representation;Compressed sensing (CS);image fusion;multispectral data;panchromatic data;pan-sharpening;sparse representation","Spatial resolution;Distortion;Satellites;Transforms;Image coding","compressed sensing;geophysical image processing;geophysical techniques;image fusion;image resolution;iterative methods","high-quality pan-sharpened multispectral bands;QuickBird data;WorldView-2 data;IKONOS data;sparse coefficients;iterative algorithm;optimal dictionary;atom linear combination;unknown high spatial resolution intensity component;spatial characteristic;spectral characteristic;panchromatic satellite image fusion;multispectral satellite image fusion;linear measurements;unknown sparse signal;sparsity regularization;multispectral image pan-sharpening;spectral distortion reduction;compressed-sensing-based pan-sharpening method","","66","","52","IEEE","23 Nov 2015","","","IEEE","IEEE Journals"
"An Energy-Based Model Encoding Nonlocal Pairwise Pixel Interactions for Multisensor Change Detection","R. Touati; M. Mignotte","Vision Laboratory, Université de Montréal, Montréal, QC, Canada; Vision Laboratory, Université de Montréal, Montréal, QC, Canada","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2018","2018","56","2","1046","1058","Image change detection (CD) is a challenging problem, particularly when images come from different sensors. In this paper, we present a novel and reliable CD model, which is first based on the estimation of a robust similarity-feature map generated from a pair of bitemporal heterogeneous remote sensing images. This similarity-feature map, which is supposed to represent the difference between the multitemporal multisensor images, is herein defined, by specifying a set of linear equality constraints, expressed for each pair of pixels existing in the before-and-after satellite images acquired through different modalities. An estimation of this overconstrained problem, also formulated as a nonlocal pairwise energy-based model, is then carried out, in the least square sense, by a fast linear-complexity algorithm based on a multidimensional scaling mapping technique. Finally, the fusion of different binary segmentation results, obtained from this similarity-feature map by different automatic thresholding algorithms, allows us to precisely and automatically classify the changed and unchanged regions. The proposed method is tested on satellite data sets acquired by real heterogeneous sensor, and the results obtained demonstrate the robustness of the proposed model compared with the best existing state-of-the-art multimodal CD methods recently proposed in the literature.","1558-0644","","10.1109/TGRS.2017.2758359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070964","Change detection (CD);energy-based model;FastMap;fusion of binary segmentations;heterogeneous sensors;multidimensional scaling (MDS) mapping;MDS;multimodal remote sensing;multisensors;multisource data;pairwise pixel interactions","Image sensors;Estimation;Remote sensing;Synthetic aperture radar;Optical sensors;Robustness","feature extraction;geophysical image processing;image classification;image coding;image fusion;image motion analysis;image segmentation;image sensors;object detection;remote sensing","unchanged regions;satellite data sets;heterogeneous sensor;multimodal CD methods;multisensor change detection;image change detection;bitemporal heterogeneous remote sensing images;multitemporal multisensor images;linear equality constraints;satellite images;overconstrained problem;fast linear-complexity algorithm;multidimensional scaling mapping technique;changed regions;nonlocal pairwise pixel interaction encoding;nonlocal pairwise energy-based model;automatic thresholding algorithms;binary segmentation;least square sense;robust similarity-feature map estimation","","39","","44","IEEE","18 Oct 2017","","","IEEE","IEEE Journals"
"A Fusion Approach for Water Area Classification Using Visible, Near Infrared and Synthetic Aperture Radar for South Asian Conditions","S. K. Ahmad; F. Hossain; H. Eldardiry; T. M. Pavelsky","Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Geological Sciences Department, University of North Carolina, Chapel Hill, USA","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2020","2020","58","4","2471","2480","Consistent estimation of water surface area from remote sensing remains challenging in regions such as South Asia with vegetation, mountainous topography, and persistent monsoonal cloud cover. High-resolution optical imagery, which is often used for global inundation mapping, is highly impacted by clouds, while synthetic aperture radar (SAR) imagery is not impacted by clouds and is affected by both topographic layover and vegetation. Here, we compare and contrast inundation extent measurements from visible (Landsat-8 and Sentinel-2) and SAR (Sentinel-1) imagery. Each data type (wavelength) has complementary strengths and weaknesses which were gauged separately over selected water bodies in Bangladesh. High-resolution cloud-free PlanetScope imagery at 3-m resolution was used as a reference to check the accuracy of each technique and data type. Next, the optical and radar images were fused for a rule-based water area classification algorithm to derive the optimal decision for the water mask. Results indicate that the fusion approach can improve the overall accuracy by up to 3.8%, 18.2%, and 8.3% during the wet season over using the individual products of Landsat8, Sentinel-1, and Sentinel-2, respectively, at three sites, while providing increased observational frequency. The fusion-derived products resulted in overall accuracy ranging from 85.8% to 98.7% and Kappa coefficient varying from 0.61 to 0.83. The proposed SAR-visible fusion technique has potential for improving satellite-based surface water monitoring and storage changes, especially for smaller water bodies in humid tropical climate of South Asia.","1558-0644","","10.1109/TGRS.2019.2950705","National Aeronautics and Space Administration(grant numbers:80NSSC18M0099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906149","Area classification;remote sensing;synthetic aperture radar (SAR);visible imagery;water bodies","Synthetic aperture radar;Remote sensing;Optical surface waves;Surface topography;Vegetation mapping;Satellites;Earth","geophysical image processing;hydrological techniques;image classification;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","satellite-based surface water monitoring;South Asia;water surface area;remote sensing;vegetation;mountainous topography;high-resolution optical imagery;global inundation mapping;synthetic aperture radar imagery;Sentinel-2;SAR imagery;Sentinel-1;high-resolution cloud-free PlanetScope imagery;optical radar images;rule-based water area classification algorithm;water mask;Landsat8;fusion-derived products;monsoonal cloud cover;SAR-visible fusion;water area classification;Bangladesh;PlanetScope imagery;humid tropical climate","","24","","39","IEEE","19 Nov 2019","","","IEEE","IEEE Journals"
"A Combiner-Based Full Resolution Quality Assessment Index for Pansharpening","G. Vivone; P. Addesso; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; CNRS, Grenoble INP, GIPSA-Lab, University of Grenoble Alpes, Grenoble, France","IEEE Geoscience and Remote Sensing Letters","26 Feb 2019","2019","16","3","437","441","Pansharpening refers to the problem of fusing a multispectral (MS) image and a panchromatic image in order to get an MS image at a finer spatial resolution than the one of the original MS image. Due to the lack of a reference image, the assessment of the quality of a pansharpened product is a challenging task. Typical solutions are related to the reduced resolution assessment by exploiting Wald's protocol or to indexes without reference. In this letter, an efficient approach based on the combination of the two above-mentioned solutions is proposed. The performance is assessed using real data acquired by sensors with very different features mounted on-board of the Pléiades, the WorldView-3, and the WorldView-4 satellite platforms.","1558-0571","","10.1109/LGRS.2018.2876629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8521583","Data fusion;pansharpening;quality assessment;remote sensing","Indexes;Spatial resolution;Estimation;Protocols;Degradation;Kalman filters","geophysical image processing;image fusion;image resolution;remote sensing","combiner-based full resolution quality assessment index;multispectral image;panchromatic image;finer spatial resolution;original MS image;reference image;pansharpened product;reduced resolution assessment;Pléiades satellite platform;WorldView-4 satellite platform","","9","","20","IEEE","4 Nov 2018","","","IEEE","IEEE Journals"
"Pan-Sharpening Based on Panchromatic Image Spectral Learning Using WorldView-2","Z. Xiong; Q. Guo; M. Liu; A. Li","Key Laboratory of Information Fusion Estimation and Detection, Heilongjiang University, Harbin, China; Chinese Academy of Sciences, Aerospace Information Research Institute, Beijing, China; Key Laboratory of Information Fusion Estimation and Detection, Heilongjiang University, Harbin, China; Chinese Academy of Sciences, Aerospace Information Research Institute, Beijing, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","In order to adequately preserve the spatial information of panchromatic (PAN) image and the spectral information of multispectral (MS) image, this study proposes a method to learn the spectral information of MS based on the PAN image. In the spectral learning model, the PAN is degraded to the spatial resolution of the MS. Then, a deep convolutional neural network (CNN) is adopted to learn the spectral information between the degraded PAN and the original MS. In addition, the spectral evaluation index—spectral angle mapper (SAM) is used for controlling the spectral losses. Finally, the high spatial resolution MS (HMS) can be obtained through the trained model by using the original PAN as the test input. Besides, seven representative pan-sharpening algorithms and seven widely recognized objective fusion metrics are used to compare and evaluate the performance on the WorldView-2 experimental data, respectively. The results show that the proposed method achieves the purpose of pan-sharpening well, especially maintaining the optimal spectral information.","1558-0571","","10.1109/LGRS.2021.3078178","National Natural Science Foundation of China(grant numbers:61771470); Strategic Priority Research Program of the Chinese Academy of Sciences(grant numbers:XDA19060103); Key Research Program of Frontier Sciences, Chinese Academy of Sciences(grant numbers:QYZDY-SSW DQC026); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9434951","Convolutional neural network (CNN);deep learning;pan-sharpening;remote sensing;spectral learning","Spatial resolution;Distortion;Training;Neural networks;Indexes;Multiresolution analysis;Deep learning","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","panchromatic image spectral learning;spatial information;multispectral image;PAN image;spectral learning model;degraded PAN;original MS;spectral evaluation index-spectral angle mapper;spectral losses;high spatial resolution MS;original PAN;seven representative pan-sharpening algorithms;optimal spectral information","","2","","20","IEEE","18 May 2021","","","IEEE","IEEE Journals"
"Block-Based Fusion Algorithm With Simulated Band Generation for Hyperspectral and Multispectral Images of Partially Different Wavelength Ranges","Y. Kim; J. Choi; D. Han; Y. Kim","Department of Civil Environmental Engineering, Seoul National University, Seoul, Korea; School of Civil Engineering, Chungbuk National University, Cheongju, Korea; Department of Marine and Civil Engineering, Chonnam National University, Yeosu, Korea; Department of Civil Environmental Engineering, Seoul National University, Seoul, Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","6","2997","3007","As current and future satellite systems provide both hyperspectral and multispectral images, a need has arisen for image fusion using hyperspectral and multispectral images to improve the fusion quality. This study introduces a hyperspectral image fusion algorithm using multispectral images with a higher spatial resolution and partially different wavelength range compared with the corresponding hyperspectral images. This study focuses on an image fusion technique that enhances the spatial quality and preserves the spectral information of hyperspectral images. The proposed algorithm generates a simulated multispectral band via a spectral unmixing technique and extracts high-frequency information based on blocks of associated bands. The algorithm was applied to Compact Airborne Spectrographic Imager (CASI) datasets acquired in two modes and was compared with two existing methods. Although the wavelength range of the multispectral image did not coincide with that of the hyperspectral image, the proposed algorithm efficiently improved the spatial details and preserved the spectral information of the fused results.","2151-1535","","10.1109/JSTARS.2015.2433673","Space Core Technology Development Program through the National Research Foundation of Korea; Ministry of Science, ICT and Future Planning(grant numbers:NRF-2012M1A3A3A02033469); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7122237","Block-based fusion;hyperspectral Image;multispectral Image;spectral unmixing;Block-based fusion;hyperspectral Image;multispectral Image;spectral unmixing","Hyperspectral imaging;Spatial resolution;Satellites;Image fusion;Distortion;Earth","geophysical image processing;image enhancement;image fusion;remote sensing","block-based fusion algorithm;simulated band generation;hyperspectral image fusion algorithm;multispectral image fusion;partially different wavelength ranges;future satellite systems;fusion quality improvement;spatial resolution;spatial quality enhancement;spectral unmixing technique;high-frequency information extraction;compact airborne spectrographic imager datasets;CASI datasets","","9","","25","IEEE","11 Jun 2015","","","IEEE","IEEE Journals"
"Comparative Analysis of Two Approaches for Multipath Ghost Suppression in Radar Imaging","G. Gennarelli; G. Vivone; P. Braca; F. Soldovieri; M. G. Amin","Institute for Electromagnetic Sensing of the Environment, National Research Council of Italy (CNR), Napoli, Italy; North Atlantic Treaty Organization Science and Technology Organization Centre for Maritime Research and Experimentation, La Spezia, Italy; North Atlantic Treaty Organization Science and Technology Organization Centre for Maritime Research and Experimentation, La Spezia, Italy; Institute for Electromagnetic Sensing of the Environment, National Research Council of Italy (CNR), Napoli, Italy; Center for Advanced Communications, Villanova University, Villanova, PA, USA","IEEE Geoscience and Remote Sensing Letters","5 Aug 2016","2016","13","9","1226","1230","Radar imaging is typically based on linear models of the electromagnetic scattering phenomenon. These models are robust and computationally efficient, but do not account for mutual interactions among targets in the scene and between the targets and the surrounding environment. As a result, the radar images are characterized by spurious targets, i.e., multipath ghosts, which appear at positions where no physical targets exist. In this letter, we compare two key approaches for clutter suppression. The first approach applies multiplicative fusion of the images corresponding to subapertures of the deployed array, whereas the second approach is based on coherence factor filtering, which enhances the image quality by suppressing low-coherence features. We assess the performance of these two methods in terms of imaging and detection capabilities. Numerical results based on synthetic data are reported to support the comparative analysis.","1558-0571","","10.1109/LGRS.2016.2577715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498604","Image fusion;multipath ghosts;radar imaging","Radar imaging;Clutter;Tomography;Scattering;Image reconstruction","image fusion;object detection;radar imaging","synthetic data;image fusion;target detection;image quality;coherence factor filtering;electromagnetic scattering phenomenon;radar imaging;multipath ghost suppression","","25","","24","IEEE","23 Jun 2016","","","IEEE","IEEE Journals"
"Self-Supervised Pansharpening Based on a Cycle-Consistent Generative Adversarial Network","J. Li; W. Sun; M. Jiang; Q. Yuan","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","19 Jan 2022","2022","19","","1","5","In the field of remote sensing image pansharpening, deep learning-based methods have shown impressive performances recently. However, most deep learning-based pansharpening methods are based on supervised learning, which requires a large number of training images. In addition, obtaining large amounts of images with a high spatial and spectral resolution for training may be difficult in practice. In this letter, a novel self-supervised learning method based on a cycle-consistent generative adversarial network (CycleGAN) is proposed for remote sensing image pansharpening, without requiring large volumes of data for training. The framework contains two generators and two discriminators, and applies a residual neural network to the first generator. The panchromatic (PAN) image and multispectral (MS) image are input into the first generator to obtain the fused image, and then the fused image is input into the second generator to obtain a PAN image, which should be consistent with the input PAN image. The experimental results show that the proposed method performs better than the state-of-the-art unsupervised pansharpening method, and also achieves a competitive performance when compared with a supervised method.","1558-0571","","10.1109/LGRS.2021.3137428","Special Foundation for National Science and Technology Basic Research Program of China(grant numbers:2019FY202502); National Natural Science Foundation of China(grant numbers:62071341,41922008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658482","Cycle-consistent generative adversarial network (CycleGAN);pansharpening;residual neural network;self-supervised learning","Pansharpening;Generators;Training;Spatial resolution;Decoding;Deep learning;Testing","geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","state-of-the-art unsupervised pansharpening method;supervised method;cycle-consistent generative adversarial network;remote sensing image pansharpening;deep learning-based methods;deep learning-based pansharpening methods;training images;high spatial resolution;spectral resolution;novel self-supervised learning method;generators;residual neural network;panchromatic image;multispectral image;fused image;input PAN image","","3","","14","IEEE","22 Dec 2021","","","IEEE","IEEE Journals"
"VP-Net: An Interpretable Deep Network for Variational Pansharpening","X. Tian; K. Li; Z. Wang; J. Ma","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2022","2022","60","","1","16","In this study, we propose an interpretable deep network for variational pansharpening (VP), named VP-Net. Different from traditional priors using linear operators, such as the gradient, we construct a prior based on the similarity between panchromatic (PAN) and high-resolution multispectral (HRMS) images by a nonlinear operator that can be learned through a deep network. Considering the spectral difference of various satellite multispectral (MS) imaging platforms, we specifically seek the aforementioned similarity from the PAN image and the intensity of the HRMS image to reduce the spectral distortion. Based on this prior, we propose a novel VP model by further incorporating a data fidelity term from the low-resolution MS image. Specifically, we build the VP-Net by unrolling the variable splitting method for an optimal solution to this model. Consequently, all modules in VP-Net have clear physical meanings and strong generalization capabilities. Meanwhile, all parameters and the aforementioned nonlinear operator are learned in VP-Net, avoiding the difficulty of selecting optimal handcrafted parameters in traditional methods. Therefore, VP-Net not only achieves an optimal balance between spatial and spectral qualities but also has a strong generalization capability across different types of training and testing data. In the experiment, we first demonstrate the superiority of the proposed method over the current state of the arts in terms of both visual effect and quantitative analysis on different satellite datasets. Moreover, we carry out an normalized difference vegetation index (NDVI) experiment to demonstrate its potential in remote sensing.","1558-0644","","10.1109/TGRS.2021.3089868","National Natural Science Foundation of China(grant numbers:61971315,61773295); Natural Science Foundation of Hubei Province(grant numbers:2018CFB435); Fundamental Research Funds for the Central Universities(grant numbers:2042018kf1009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9467572","Image fusion;interpretable deep network;pansharpening;remote sensing;variational model","Pansharpening;Satellites;Remote sensing;Training;Testing;Nonlinear distortion;Spatial resolution","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image resolution;remote sensing;variational techniques;vegetation mapping","VP-Net;satellite multispectral imaging platforms;PAN image;HRMS image;low-resolution MS image;interpretable deep network;variational pansharpening;panchromatic image;high-resolution multispectral image;data fidelity;normalized difference vegetation index;remote sensing","","7","","60","IEEE","29 Jun 2021","","","IEEE","IEEE Journals"
"Shadow Removal of Hyperspectral Remote Sensing Images With Multiexposure Fusion","P. Duan; S. Hu; X. Kang; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; School of Robotics, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","21 Sep 2022","2022","60","","1","11","Shadow removal is a challenging problem in hyperspectral remote sensing images due to its spatial-variant properties and diverse patterns. In this work, a shadow removal framework with multiexposure fusion is proposed for hyperspectral remote sensing images, which consists of three major steps. First, a color space conversion method is exploited to detect the shadow regions. Second, the principle of the intrinsic decomposition model is utilized to generate a set of differently exposed hyperspectral images (HSIs), i.e., multiexposure images. Third, the generated multiexposure images and the original HSIs are fused together with a two-stage image fusion method so as to remove the shadows in hyperspectral remote sensing images effectively. Experiments performed on three real hyperspectral datasets confirm that the performance of the proposed method outperforms other state-of-the-art shadow removal approaches.","1558-0644","","10.1109/TGRS.2022.3203808","National Key Research and Development Program of China(grant numbers:2021YFA0715203); Major Program of the National Natural Science Foundation of China(grant numbers:61890962); National Natural Science Foundation of China(grant numbers:61871179); Scientific Research Project of Hunan Education Department(grant numbers:19B105); National Science Foundation of Hunan Province(grant numbers:2019JJ50036,2020GK2038); Hunan Provincial Natural Science Foundation for Distinguished Young Scholars(grant numbers:2021JJ022); Huxiang Young Talents Science and Technology Innovation Program(grant numbers:2020RC3013); Changsha Natural Science Foundation(grant numbers:kq2202171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874905","Color space conversion;intrinsic image decomposition;multiexposure fusion;shadow removal;two-stage image fusion","Hyperspectral imaging;Image color analysis;Lighting;Image restoration;Image edge detection;Image synthesis;Image fusion","","","","2","","53","IEEE","2 Sep 2022","","","IEEE","IEEE Journals"
"Super-Resolution of GF-1 Multispectral Wide Field of View Images via a Very Deep Residual Coordinate Attention Network","R. Liu; B. Cui; X. Fang; B. Guo; Y. Ma; J. An","First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; First Institute of Oceanography, Ministry of Natural Resources, Qingdao, China; Information Science and Technology College, Dalian Maritime University, Dalian, China","IEEE Geoscience and Remote Sensing Letters","19 Jul 2022","2022","19","","1","5","GF-1 multispectral wide field of view (WFV) images, with a spatial resolution of 16 m, have been widely used in Earth monitoring. However, the spatial details provided by WFV images are not sufficient for many applications. Thus, this letter proposes a novel WFV image super-resolution (SR) algorithm called Gaofen residual coordinate attention network (GFRCAN) based on a very deep residual coordinate attention network. To form a very deep network, the residual-in-residual (RIR) structure consisting of several residual groups (RGs) with long skip connections is used. Meanwhile, the residual coordinate attention block (RCOAB) and adaptive multiscale spatial attention module (AMSA) are incorporated to focus on the high-frequency information and multiscale features adaptive weighted fusion. Besides, the spectral and spatial details of SR images are improved by incorporating peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) into the loss function. Both subjective and objective evaluation results show that the proposed model outperforms the state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3190018","National Natural Science Foundation of China(grant numbers:42076189,42106179); China High Resolution Earth Observation System Program(grant numbers:41-Y30F07-9001-20/22); China-Korea Joint Ocean Research Center, China(grant numbers:PI-2022-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826846","Coordinate attention;GF-1 multispectral wide field of view (WFV);remote sensing images;super-resolution (SR)","Feature extraction;Spatial resolution;Satellites;Training;Image sensors;Data mining;Superresolution","deep learning (artificial intelligence);geophysical image processing;image fusion;image resolution;remote sensing","long skip connections;SSIM;loss function;structural similarity index;PSNR;peak signal-to-noise ratio;multiscale feature adaptive weighted fusion;high-frequency information;AMSA;RCOAB;residual coordinate attention block;residual-in-residual structure;Gaofen residual coordinate attention network;SR algorithm;Earth monitoring;very deep residual coordinate attention network;GF-1 multispectral wide field of view image super-resolution;WFV image super-resolution algorithm;SR images;spectral details;adaptive multiscale spatial attention module;attention block;residual groups;deep network;spatial details;spatial resolution;view images;size 16.0 m","","","","20","IEEE","12 Jul 2022","","","IEEE","IEEE Journals"
"HCNNet: A Hybrid Convolutional Neural Network for Spatiotemporal Image Fusion","Z. Zhu; Y. Tao; X. Luo","Chongqing Engineering Research Center of Spatial Big Data Intelligent Technology, Chongqing, China; Chongqing Engineering Research Center of Spatial Big Data Intelligent Technology, Chongqing, China; Chongqing Engineering Research Center of Spatial Big Data Intelligent Technology, Chongqing, China","IEEE Transactions on Geoscience and Remote Sensing","27 Jul 2022","2022","60","","1","16","In recent years, leaps and bounds have developed spatiotemporal fusion (STF) methods for remote sensing (RS) images based on deep learning. However, most existing methods use 2-D convolution (Conv) to explore features. 3-D Conv can explore time-dimensional features, but it requires more memory footprint and is rarely used. In addition, the current STF methods based on convolutional neural networks (CNNs) are mainly the following two: 1) use 2-D Conv to extract features from multiple bands of the input image together and fuse the features to predict the multiband image directly and 2) use 2-D Conv to extract features from individual bands of the image, predict the reflectance data of individual bands, and finally stack the predicted individual bands directly to synthesize the multiband image. The former method does not sufficiently consider the spectral and reflectance differences between different bands, and the latter does not consider the similarity of spatial structures between adjacent bands and the spectral correlation. To solve these problems, we propose a 2-D/3-D hybrid CNN called HCNNet, in which the 2D-CNN branch extracts the spatial information features of single-band image, and the 3D-CNN branch extracts spatiotemporal features of single-band images. After fusing the features of the dual branches, we introduce neighboring band features to share spatial information so that the information is complementary to obtain single-band features and images, and finally stack each single-band image to generate multiband images. Visual assessment and metric evaluation of the three publicly available datasets showed that our method predicted better images compared with the five methods.","1558-0644","","10.1109/TGRS.2022.3177749","National Natural Science Foundation of China(grant numbers:41871226); Major Industrial Technology Research and Development Projects of High-Tech Industry in Chongqing(grant numbers:D2018-82); Intergovernmental International Scientific and Technological Innovation Cooperation Project of the National Key Research and Development Program(grant numbers:2021YFE0194700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781404","Feature fusion;hybrid convolution (Conv);spatiotemporal fusion (STF);spectral correlation","Feature extraction;Image reconstruction;Spatiotemporal phenomena;Spatial resolution;Data mining;Reflectivity;Earth","cellular neural nets;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing;spatiotemporal phenomena","single-band image;single-band features;multiband image;hybrid convolutional neural network;spatiotemporal image;spatiotemporal fusion methods;remote sensing images;2-D convolution;Conv;time-dimensional features;current STF methods;convolutional neural networks;multiple bands;input image;predicted individual bands;adjacent bands;2D-CNN branch;spatial information features;3D-CNN branch extracts spatiotemporal features","","","","48","CCBY","25 May 2022","","","IEEE","IEEE Journals"
"Analysis and Estimation of Net Primary Productivity of Vegetation in Nanjing Using Multi-Sourced Remote Sensing Data","G. Yang","College of Resources and Environmental Sciences, Nanjing Agricultural University, Nanjing, China","IEEE Access","8 Apr 2022","2022","10","","35665","35674","Net primary productivity (NPP) is an important indicator of Earth’s ecosystem. However, NPP estimation of cities by high spatial resolution is limited in China, especially Nanjing, and NPP evolution and quantitative analysis of influencing factors in time series are insufficient. Nanjing, as one of the central cities in China, plays a major role in China’s economic development. In this paper, high spatial resolution data combined with Landsat Thematic Mapper (TM) data are used for NPP estimation and analysis in Nanjing City. The research contents of this paper are as follows: (1) To improve the passive effect of “red border region” on GF-1 WFV forest and farmland classification ability, the multispectral and high-resolution images are fused, and the image fusion effect was also evaluated. (2) The NPP of four seasons in Nanjing in 2017 was estimated using the CASA model. (3) The results of NPP estimation are analyzed based on statistical principles. The advantage of this paper lies in the comprehensive utilization of multi-source data and the quantitative analysis of the response mechanism between meteorological data and the NPP estimation value.","2169-3536","","10.1109/ACCESS.2022.3162208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9741733","GF-1 WFV;Landsat_8;image fusion;NPP of Nanjing","Biological system modeling;Remote sensing;Vegetation mapping;Estimation;Earth;Artificial satellites;Productivity","ecology;geophysical image processing;image fusion;remote sensing;vegetation;vegetation mapping","net primary productivity;multisourced remote sensing data;NPP evolution;quantitative analysis;central cities;high spatial resolution data;Landsat Thematic Mapper data;Nanjing City;farmland classification ability;multispectral resolution images;high-resolution images;multisource data;meteorological data;NPP estimation value","","","","34","CCBY","25 Mar 2022","","","IEEE","IEEE Journals"
"Dilated Convolution and Feature Fusion SSD Network for Small Object Detection in Remote Sensing Images","J. Qu; C. Su; Z. Zhang; A. Razi","Xi’an Key Laboratory of Advanced Control and Intelligent Process, Xi’an, China; School of Communication and Information Engineering, Xi’an University of Posts and Telecommunications, Xi’an, China; School of Communication and Information Engineering, Xi’an University of Posts and Telecommunications, Xi’an, China; School of Informatics, Computing and Cyber Systems, Northern Arizona University, Flagstaff, USA","IEEE Access","13 May 2020","2020","8","","82832","82843","Noting the shortcomings of current methods in detecting small objects in image-based remote sensing applications, in this paper, we propose a novel implementation of single shot multibox detector (SSD) networks based on dilated convolution and feature fusion. We call this algorithm dilated convolution and feature fusion single shot multibox detector (DFSSD). This algorithm removes the random clipping steps of data preprocessing layers in conventional SSD networks and utilizes the structure of feature pyramid network (FPN) network to fuse the low-level feature map with high resolution and the high-level feature map with rich semantic information. It also enhances the receptive field of the third-level feature map of the DFSSD network by using dilated convolution. In the data processing step of the model, we use the image segmentation of the feature point region proposals to improve the training sample size. The mean average precision (mAP) value of the proposed DFSSD network, when tested on remote sensing datasets, achieves 76.51%, which is significantly higher than that of the SSD model (69.81%).","2169-3536","","10.1109/ACCESS.2020.2991439","National Natural Science Foundation of China(grant numbers:51875457); International Cooperation and Exchange Program of Shaanxi Province(grant numbers:2018KW-026); Natural Science Foundation of Shaanxi Province(grant numbers:2018JM6120,2019JM-606); Xi’an Science and Technology Projects(grant numbers:201805040YD18CG24(6)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9082636","Small object detection;feature fusion;dilated convolution;DFSSD network","Feature extraction;Remote sensing;Object detection;Image segmentation;Convolution;Semantics;Detectors","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image resolution;image segmentation;object detection;remote sensing","object detection;image-based remote sensing applications;single shot multibox detector networks;dilated convolution;feature pyramid network network;low-level feature map;high-level feature map;third-level feature map;DFSSD network;image segmentation;feature point region proposals;remote sensing datasets;feature fusion SSD network;remote sensing images;mean average precision","","24","","42","CCBY","30 Apr 2020","","","IEEE","IEEE Journals"
"Multi-Component Fusion Network for Small Object Detection in Remote Sensing Images","J. Liu; S. Yang; L. Tian; W. Guo; B. Zhou; J. Jia; H. Ling","Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Augmented Reality, College of Mathematics and Information Science, Hebei Normal University, Shijiazhuang, China; Key Laboratory of Augmented Reality, College of Mathematics and Information Science, Hebei Normal University, Shijiazhuang, China; Key Laboratory of Augmented Reality, College of Mathematics and Information Science, Hebei Normal University, Shijiazhuang, China; Key Laboratory of Augmented Reality, College of Mathematics and Information Science, Hebei Normal University, Shijiazhuang, China; Key Laboratory of Augmented Reality, College of Mathematics and Information Science, Hebei Normal University, Shijiazhuang, China; Center for Data Analytics and Biomedical Informatics, Temple University, Philadelphia, PA, USA","IEEE Access","18 Sep 2019","2019","7","","128339","128352","Small object detection is a major challenge in the field of object detection. With the development of deep learning, many methods based on deep convolutional neural networks (DCNNs) have greatly improved the speed of detection while ensuring accuracy. However, due to the contradiction between the spatial details and semantic information of DCNNs, previous deep learning methods often meet problems when detecting small objects. The challenge can be more serious in complex scenes involving similar background objects and/or occlusion, such as in remote sensing imagery. In this paper, we propose an end-to-end DCNN called the multi-component fusion network (MCFN) to improve the accuracy of small object detection in such cases. First, we propose a dual pyramid fusion network, which densely concatenates spatial information and semantic information to extract small object features via encoding and decoding operations. Then we use a relative region proposal network to adequately extract the features of small objects samples and parts of objects. Finally, to achieve robustness against background disturbance, we add contextual information to the proposal regions before final detection. Experimental evaluations demonstrate that the proposed method significantly improves the accuracy of object detection in remote sensing images compared with other state-of-the-art methods, especially in complex scenes with the conditions of occlusion.","2169-3536","","10.1109/ACCESS.2019.2939488","National Natural Science Foundation of China(grant numbers:61802109); Science and Technology Foundation of Hebei Province Higher Education(grant numbers:QN2019166); Natural Science Foundation of Hebei Province(grant numbers:F2017205066); Hebei Normal University(grant numbers:L2017B06,L2018K02,L2019K01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823855","Small object;remote sensing;multi-component;dual pyramid fusion;occlusion;complex scene","Feature extraction;Remote sensing;Object detection;Proposals;Deep learning;Training;Aircraft","convolutional neural nets;feature extraction;geophysical image processing;image fusion;learning (artificial intelligence);object detection;remote sensing","multicomponent fusion network;small object detection;remote sensing images;deep convolutional neural networks;semantic information;dual pyramid fusion network;relative region proposal network;background objects;deep learning methods;complex scenes;end-to-end DCNN;MCFN;spatial information;small object feature extraction;decoding operations;encoding operations;small object samples","","13","","46","CCBY","4 Sep 2019","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion Using Factor Smoothed Tensor Ring Decomposition","Y. Chen; J. Zeng; W. He; X. -L. Zhao; T. -Z. Huang","School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China; School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; School of Mathematical Sciences/Research Center for Image and Vision Computing, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Mathematical Sciences/Research Center for Image and Vision Computing, University of Electronic Science and Technology of China, Chengdu, Sichuan, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","17","Fusing a pair of low-spatial-resolution hyperspectral image (LR-HSI) and high-spatial-resolution multispectral image (HR-MSI) has been regarded as an effective and economical strategy to achieve HR-HSI, which is essential to many applications. Among existing fusion models, the tensor ring (TR) decomposition-based model has attracted rising attention due to its superiority in approximating high-dimensional data compared to other traditional matrix/tensor decomposition models. Unlike directly estimating HR-HSI in traditional models, the TR fusion model translates the fusion procedure into an estimate of the TR factor of HR-HSI, which can efficiently capture the spatial–spectral correlation of HR-HSI. Although the spatial–spectral correlation has been preserved well by TR decomposition, the spatial–spectral continuity of HR-HSI is ignored in existing TR decomposition models, sometimes resulting in poor quality of reconstructed images. In this article, we introduce a factor smoothed regularization for TR decomposition to capture the spatial–spectral continuity of HR-HSI. As a result, our proposed model is called factor smoothed TR decomposition model, dubbed FSTRD. In order to solve the suggested model, we develop an efficient proximal alternating minimization algorithm. A series of experiments on four synthetic datasets and one real-world dataset show that the quality of reconstructed images can be significantly improved by the introduced factor smoothed regularization, and thus, the suggested method yields the best performance by comparing it to state-of-the-art methods.","1558-0644","","10.1109/TGRS.2021.3114197","NSFC(grant numbers:12171072,62101222,61977038,61772003,61876203); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216,2021YJ0107); National Key Research and Development Program of China(grant numbers:2020YFA0714001); Japan Society for the Promotion of Science (KAKENHI)(grant numbers:19K20308); Thousand Talents Plan of Jiangxi Province(grant numbers:jxsq2019201124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9556548","Hyperspectral image (HSI) and multispectral image (MSI) fusion;proximal alternating minimization (PAM);smoothed regularization;tensor ring (TR) decomposition","Tensors;Matrix decomposition;Biological system modeling;Spatial resolution;Correlation;Degradation;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;matrix decomposition;minimisation;remote sensing;tensors","factor smoothed tensor ring decomposition;low-spatial-resolution hyperspectral image;high-spatial-resolution multispectral image;HR-HSI;tensor ring decomposition-based model;TR fusion model;spatial-spectral correlation;spatial-spectral continuity;factor smoothed TR decomposition model;image reconstruction;hyperspectral-multispectral image fusion;LR-HSI;HR-MSI;high-dimensional data approximation;matrix-tensor decomposition model;factor smoothed regularization;FSTRD;proximal alternating minimization algorithm","","5","","76","IEEE","1 Oct 2021","","","IEEE","IEEE Journals"
"A Target Detection Algorithm for Remote Sensing Images Based on a Combination of Feature Fusion and Improved Anchor","W. Zhao; Y. Kang; H. Chen; Z. Zhao; Y. Zhai; P. Yang","School of Control and Computer Engineering and the Engineering Research Center of Intelligent Computing for Complex Energy Systems, Ministry of Education, North China Electric Power University, Baoding, China; School of Control and Computer Engineering, North China Electric Power University, Baoding, China; School of Control and Computer Engineering, North China Electric Power University, Baoding, China; School of Electrical and Electronic Engineering, North China Electric Power University, Baoding, China; School of Control and Computer Engineering, North China Electric Power University, Baoding, China; School of Control and Computer Engineering, North China Electric Power University, Baoding, China","IEEE Transactions on Instrumentation and Measurement","29 Jun 2022","2022","71","","1","8","Aiming at the low average target accuracy in remote sensing images with complex background and multiple small targets, we propose a new algorithm for small target detection based on a combination of feature fusion and improved anchor. First, a data enhancement method is presented to increase the number of small targets in remote sensing images. Second, the high-level features and low-level features are efficiently fused using pixel-by-pixel summation and lightweight feature extraction, which can extract features that are more favorable for small target detection. Finally, by adjusting the aspect ratio of the anchor, the misdetection rate of small targets is reduced. The algorithm proposed in this article is applied on the PASCAL VOC dataset and UCAS-AOD dataset, with the mean average accuracy (mAP) of 82.5% and 87.9%, respectively. The test results demonstrate that the proposed method can not only achieve a comparable detection speed but also demonstrates considerable accuracy superiority over traditional algorithms.","1557-9662","","10.1109/TIM.2022.3181927","National Natural Science Foundation of China(grant numbers:61871182,61773160); Natural Science Foundation of Hebei Province of China(grant numbers:F2021502013); Fundamental Research Funds for the Central Universities(grant numbers:2020MS153,2021PT018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792398","Feature fusion;improved anchor;lightweight feature extraction;remote sensing images;target detection","Feature extraction;Remote sensing;Object detection;Convolution;Semantics;Power systems;Training","convolutional neural nets;feature extraction;image enhancement;image fusion;object detection;remote sensing","target detection algorithm;remote sensing images;feature fusion;improved anchor;high-level features;low-level features;pixel-by-pixel summation;lightweight feature extraction;complex background;multiple small targets;data enhancement;target misdetection;PASCAL VOC dataset;UCAS-AOD dataset;VGG16 network","","","","23","IEEE","9 Jun 2022","","","IEEE","IEEE Journals"
"Integrated Fusion for Panchromatic, Multispectral, Hyperspectral Remote Sensing Images With Different Swath Widths","X. Meng; X. Meng; Q. Liu; J. Shu; F. Shao; G. Yang; W. Sun","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Ningbo Institute of Surveying, Mapping and Remote Sensing, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China","IEEE Geoscience and Remote Sensing Letters","15 Sep 2022","2022","19","","1","5","Zi Yuan (ZY)-1 02D satellite simultaneously provides the low spatial resolution (LR) and narrow swath-width hyperspectral (HS) image, the moderate spatial resolution (MR) multispectral (MS) image with a wider swath width, and the high spatial resolution (HR) panchromatic (PAN) image with the same wide swath width to the MR MS. How to comprehensively integrate their complementary advantages to obtain the wide swath-width and high-fidelity HR HS image is interesting but challenging. In this letter, we propose an integrated fusion method for the HR PAN, MR MS, and LR HS images with different swath widths to generate the optimal wide swath-width HR HS image. The proposed method is based on the encoder–decoder learning framework. In the proposed fusion framework, a novel multibranch encoder (MBE) structure with an enhanced HS-encoder module (EHM) and the multilevel spatial–spectral aggregation block (MSSAB) is designed, by considering the difference in the spatial and spectral resolution among the multisensor images. The experiments on synthetic and real datasets from both qualitative and quantitative aspects demonstrated the competitive performance of the proposed method.","1558-0571","","10.1109/LGRS.2022.3203379","National Natural Science Foundation of China(grant numbers:42171326,62071261); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY22F010014); Fellowship of China Postdoctoral Science Foundation(grant numbers:2020M672490); 2025 Science and Technology Major Project of Ningbo City(grant numbers:2021Z107,2022Z032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9888017","Hyperspectral (HS);integrated fusion;multispectral (MS);panchromatic (PAN);swath width","Spatial resolution;Phase change materials;Feature extraction;Image resolution;Decoding;Convolutional codes;Sun","geophysical image processing;hyperspectral imaging;image fusion;image resolution;neural nets;remote sensing","hyperspectral remote sensing images;Zi Yuan-1 02D satellite;moderate spatial resolution multispectral image;swath width;high spatial resolution panchromatic image;MR MS;high-fidelity HR HS image;integrated fusion method;LR HS images;encoder-decoder learning framework;fusion framework;enhanced HS-encoder module;multilevel spatial-spectral aggregation block;spectral resolution;multisensor images","","","","24","IEEE","12 Sep 2022","","","IEEE","IEEE Journals"
"Spectral–Spatial Adaptive Area-to-Point Regression Kriging for MODIS Image Downscaling","Y. Zhang; P. M. Atkinson; F. Ling; Q. Wang; X. Li; L. Shi; Y. Du","University of Chinese Academy of Sciences, Beijing, China; Lancaster Environment Central, Faculty of Science and Technology, Lancaster University, Lancaster, U.K.; Institute of Geodesy and Geophysics, Chinese Academy of Sciences, Wuhan, China; Lancaster Environment Central, Faculty of Science and Technology, Lancaster University, Lancaster, U.K.; Institute of Geodesy and Geophysics, Chinese Academy of Sciences, Wuhan, China; University of Chinese Academy of Sciences, Beijing, China; Institute of Geodesy and Geophysics, Chinese Academy of Sciences, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 May 2017","2017","10","5","1883","1896","The moderate resolution imaging spectroradiometer (MODIS) sensor contains 36 bands at spatial resolutions of 250 m (e.g., bands 1-2), 500 m (e.g., bands 3-7), and 1000 m (e.g., bands 8-36). The first seven bands covering the visible to midinfrared wavelengths have been used widely for monitoring the Earth's surface. However, 500 m MODIS bands 3-7 present challenges for use in land cover/land use applications, as many land cover features on the Earth's surface possess complex structures with a spatial resolution finer than 500 m. Fusing MODIS 250 m bands 1-2 and 500 m bands 3-7 is an attractive proposition, that is, increasing the spatial resolution of bands 3-7. The geostatistical based downscaling approach, area-to-point regression kriging (ATPRK), has shown great potential for MODIS image downscaling. However, it considers the global relationship between bands 1-2 and each of bands 3-7 to select a 250 m PAN-like band from bands 1-2, which may not take full advantage of both bands 1 and 2. In this paper, a new geostatistical downscaling method of spectral-spatial adaptive ATPRK (SSAATPRK) is proposed for MODIS image downscaling. Both fine spatial resolution bands (i.e., bands 1 and 2) are used as the input to SSAATPRK, and there is no need to choose a PAN-like band for each coarse band, as in the original ATPRK method. SSAATPRK was compared to four benchmark image fusion methods, including principal component analysis, high-pass filtering, ATPRK, and adaptive ATPRK (AATPRK), using one synthetic MODIS image experiment and two real MODIS image experiments. Both visual and quantitative evaluations demonstrated that SSAATPRK produced results consistently with the greatest amount of spatial detail and the largest accuracy. Furthermore, SSAATPRK inherits completely the advantages of ATPRK and AATPRK, while extending them for MODIS image downscaling.","2151-1535","","10.1109/JSTARS.2017.2650260","Natural Science Foundation of China(grant numbers:6167012166); National Basic Research Program (973 Program) of China(grant numbers:2013cb733205); State Key Laboratory of Resources and Environmental Informational System; Youth Innovation Promotion Association CAS; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7839286","Adaptive;area-to-point regression kriging (ATPRK);downscaling;geostatistics;image fusion;moderate resolution imaging spectroradiometer (MODIS);pan-sharpening;spectral–spatial","Spatial resolution;MODIS;Earth;Monitoring;Land surface;Image fusion","geophysical image processing;high-pass filters;image filtering;image fusion;land cover;land use;principal component analysis;regression analysis;remote sensing","spectral-spatial adaptive ATPRK;area-to-point regression kriging;MODIS image downscaling;moderate resolution imaging spectroradiometer;MODIS sensor;Earth surface monitoring;land cover;land use;geostatistical-based downscaling approach;image fusion method;principal component analysis;MODIS image experiment","","9","","45","IEEE","1 Feb 2017","","","IEEE","IEEE Journals"
"Fusion of Panchromatic and Multispectral Images Using Multiscale Convolution Sparse Decomposition","K. Zhang; F. Zhang; Z. Feng; J. Sun; Q. Wu","School of Information Science and Engineering, Shandong Normal University, Ji'nan, China; School of Information Science and Engineering, Shandong Normal University, Ji'nan, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China; School of Information Science and Engineering, Shandong Normal University, Ji'nan, China; College of Geography and Environment, Shandong Normal University, Ji'nan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2021","2021","14","","426","439","In this article, we proposed a novel image fusion method based on multiscale convolution sparse decomposition (MCSD). A unified framework based on MCSD is first utilized to decompose panchromatic (PAN) image and the spatial component of upsampled low spatial resolution multispectral (LR MS) images, which can produce the corresponding low frequencies and feature maps. By combining convolution sparse decomposition with multiscale analysis, MCSD can efficiently approximate the spatial and spectral information in images. Next, a binary map generated from gradient information is utilized to integrate the low frequencies of LR MS and PAN images. For feature maps, the fusion gain for each pixel is calculated according to the similarity between the local patches from them. Finally, the fused image is reconstructed by the sum of fused low frequency and feature maps. Some experiments are conducted on QuickBird and GeoEye-1 satellite datasets. Compared with other methods, the proposed method performs better in visual and numerical evaluations.","2151-1535","","10.1109/JSTARS.2020.3043521","National Natural Science Foundation of China(grant numbers:61901246,U1736122); China Postdoctoral Science Foundation(grant numbers:2019TQ0190,2019M662432); Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education; Xidian University(grant numbers:IPIU2019008); Natural Science Foundation of Shandong Province(grant numbers:JQ201718); State Key Program of National Natural Science of China(grant numbers:61836009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288881","Convolution sparse representation (SR);image fusion;multiscale decomposition;multispectral (MS) image;panchromatic (PAN) image","High frequency;Image reconstruction;Transforms;Spatial resolution;Filtering theory;Convolution;Image fusion","geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing","fused low frequency;multiscale convolution sparse decomposition;image fusion method;MCSD;panchromatic image;spatial component;low spatial resolution multispectral images;multiscale analysis;binary map;fusion gain;fused image;QuickBird satellite dataset;GeoEye-1 satellite dataset","","5","","63","CCBY","9 Dec 2020","","","IEEE","IEEE Journals"
"Enhancing Spatio-Temporal Fusion of MODIS and Landsat Data by Incorporating 250 m MODIS Data","Q. Wang; Y. Zhang; A. O. Onojeghuo; X. Zhu; P. M. Atkinson","Department of Surveying and Geoinformatics, Nnamdi Azikiwe University, Anambra state, Awka, Nigeria; University of Chinese Academy of Sciences, Beijing, China; Department of Surveying and Geoinformatics, Nnamdi Azikiwe University, Anambra state, Awka, Nigeria; The Hong Kong Polytechnic University, Hong Kong; Faculty of Science and Technology, Lancaster University, Lancaster, U.K.","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Sep 2017","2017","10","9","4116","4123","Spatio-temporal fusion of MODIS and Landsat data aims to produce new data that have simultaneously the Landsat spatial resolution and MODIS temporal resolution. It is an ill-posed problem involving large uncertainty, especially for reproduction of abrupt changes and heterogeneous landscapes. In this paper, we proposed to incorporate the freely available 250 m MODIS images into spatio-temporal fusion to increase prediction accuracy. The 250 m MODIS bands 1 and 2 are fused with 500 m MODIS bands 3-7 using the advanced area-to-point regression kriging approach. Based on a standard spatio-temporal fusion approach, the interim 250 m fused MODIS data are then downscaled to 30 m with the aid of the available 30 m Landsat data on temporally close days. The 250 m data can provide more information for the abrupt changes and heterogeneous landscapes than the original 500 m MODIS data, thus increasing the accuracy of spatio-temporal fusion predictions. The effectiveness of the proposed scheme was demonstrated using two datasets.","2151-1535","","10.1109/JSTARS.2017.2701643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934036","Downscaling;geostatistics;image fusion;Landsat;MODIS;spatio-temporal fusion","MODIS;Remote sensing;Earth;Spatial resolution;Satellites;Monitoring;Image fusion","geophysical image processing;image fusion;regression analysis;remote sensing","MODIS data spatio-temporal fusion;Landsat data spatio-temporal fusion;Landsat spatial resolution;MODIS temporal resolution;MODIS images;regression kriging approach","","28","","28","IEEE","25 May 2017","","","IEEE","IEEE Journals"
"MHANet: A Multiscale Hierarchical Pansharpening Method With Adaptive Optimization","D. Lei; J. Huang; L. Zhang; W. Li","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Geoscience and Remote Sensing","1 Aug 2022","2022","60","","1","15","In recent years, the powerful nonlinear modeling capability of convolutional neural networks (CNN) has led to an increasing number of researchers focusing on deep-learning-based pansharpening methods. However, due to the diversity of remote sensing image features and the limitations of the convolution operation, the existing methods are still inadequate in restoring the spatial details of complex remote sensing scenes. Therefore, in this article, we propose a simple and effective network for pansharpening methods. Specifically, in our hierarchical feature integration architecture, a multiscale grouping dilated block is designed to adequately capture fine-grained representations of multilevel scale features. At the same time, we propose a spatially self-attention block to adaptively improve the feature extraction process by establishing associations between features. The above blocks are connected in a hierarchical design, with selective reuse of features between layers and a good ability to explore new levels of features while reusing low-level features. Our experiments with the GaoFen-2 satellite dataset, WorldView-2 satellite dataset, and WorldView-3 satellite dataset show that our proposed method is highly competitive with the existing excellent methods in both objective indicator evaluation and subjective visual evaluation.","1558-0644","","10.1109/TGRS.2022.3191660","National Natural Science Foundation of China(grant numbers:61972060,U1713213,62027827); National Key Research and Development Program of China(grant numbers:2019YFE0110800); Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0025,cstc2019cxcylirc-td0270); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832001","Adaptive optimization;hierarchical integration;multiscale;pansharpening","Pansharpening;Feature extraction;Deep learning;Remote sensing;Neural networks;Spatial resolution;Training","feature extraction;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","GaoFen-2 satellite dataset;WorldView-2 satellite dataset;WorldView-3 satellite dataset show;existing excellent methods;MHANet;multiscale hierarchical pansharpening method;adaptive optimization;powerful nonlinear modeling capability;convolutional neural networks;deep-learning-based pansharpening methods;remote sensing image features;convolution operation;spatial details;complex remote sensing scenes;simple network;effective network;hierarchical feature integration architecture;multiscale grouping dilated block;fine-grained representations;multilevel scale features;spatially self-attention block;feature extraction process;hierarchical design;low-level features","","","","43","IEEE","18 Jul 2022","","","IEEE","IEEE Journals"
"A Novel Feature Fusion Approach for VHR Remote Sensing Image Classification","S. Liu; Y. Zheng; Q. Du; A. Samat; X. Tong; M. Dalponte","College of Surveying and Geoinformatics, Tongji University, Shanghai, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Xinjiang Institute of Ecology and Geography, Chinese Academy of Sciences, Urumqi, China; College of Surveying and Geoinformatics, Tongji University, Shanghai, China; Department of Sustainable Agro-Ecosystems and Bioresources, Research and Innovation Centre, Fondazione E. Mach, San Michele all'Adige, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","464","473","This article develops a robust feature fusion approach to enhance the classification performance of very high resolution (VHR) remote sensing images. Specifically, a novel two-stage multiple feature fusion (TsF) approach is proposed, which includes an intragroup and an intergroup feature fusion stages. In the first fusion stage, multiple features are grouped by clustering, where redundant information between different types of features is eliminated within each group. Then, features are pairwisely fused in an intergroup fusion model based on the guided filtering method. Finally, the fused feature set is imported into a classifier to generate the classification map. In this work, the original VHR spectral bands and their attribute profiles are taken as examples as input spectral and spatial features, respectively, in order to test the performance of the proposed TsF approach. Experimental results obtained on two QuickBird datasets covering complex urban scenarios demonstrate the effectiveness of the proposed approach in terms of generation of more discriminative fusion features and enhancing classification performance. More importantly, the fused feature dimensionality is limited at a certain level; thus, the computational cost will not be significantly increased even if multiple features are considered.","2151-1535","","10.1109/JSTARS.2020.3041868","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB0505000); National Natural Science Foundation of China(grant numbers:42071324,41601354,42071424,42001387); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277624","Classification;feature fusion;guided filtering (GF);spectral-spatial features;very high resolution (VHR) image","Feature extraction;Filtering;Remote sensing;Spatial resolution;Entropy;Electronic mail;Computational efficiency","feature extraction;image classification;image fusion;image resolution;remote sensing;spectral analysis","VHR remote sensing image classification;intergroup feature fusion stages;feature dimensionality;two-stage multiple feature fusion;intragroup feature fusion stage;intergroup feature fusion stage;VHR spectral bands;TsF","","13","","32","CCBY","2 Dec 2020","","","IEEE","IEEE Journals"
"Multilayer Feature Extraction Network for Military Ship Detection From High-Resolution Optical Remote Sensing Images","P. Qin; Y. Cai; J. Liu; P. Fan; M. Sun","School of Geospatial Engineering and Science, Sun Yat-sen University, Guangzhou, China; College of Geodesy and Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geodesy and Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geodesy and Geomatics, Shandong University of Science and Technology, Qingdao, China; College of Geodesy and Geomatics, Shandong University of Science and Technology, Qingdao, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","10 Nov 2021","2021","14","","11058","11069","Rapid and accurate detection of maritime military targets is of great significance for maintaining national defense security. Few studies have used high-resolution optical images for the detailed classification of maritime military targets. This article, inspired by EfficientDet trackers, presents a method to classify military targets on the sea from high-resolution optical remote sensing images. In the first stage, a multilayer feature extraction network is constructed to extract various features. At the same time, residual connection and dilation convolution are introduced to prevent the deep network features from disappearing. Moreover, we use multilevel attention mechanism approaches to make more effective use of multilayer features. ReLU is introduced to replace the original swish activation function to reduce the computational cost in the pretreatment stage. After this, deep feature fusion networks and prediction networks are constructed to locate and distinguish different types of ships. Different types of ships use different degrees of data expansion methods to solve the problem of sample shortage and imbalance. The multiclassification method is used to solve low classification accuracy caused by little difference between civil and military ships. Experimental results suggested that the proposed method can accurately identify multiple types of military ships.","2151-1535","","10.1109/JSTARS.2021.3123080","National Natural Science Foundation of China; National Defense Science and Industry Administration Civil Aerospace; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591290","Attention mechanism;data enhancement;efficientdet network;feature extraction;feature fusion;military target detection;multiclassification","Marine vehicles;Feature extraction;Remote sensing;Data mining;Convolutional neural networks;Optical sensors;Optical imaging","feature extraction;image classification;image fusion;image resolution;marine engineering computing;military computing;neural nets;remote sensing;ships","maritime military targets;high-resolution optical remote sensing images;multilayer feature extraction network;deep network features;deep feature fusion networks;prediction networks;military ship detection;ReLU;residual connection;dilation convolution","","9","","50","CCBYNCND","27 Oct 2021","","","IEEE","IEEE Journals"
"Bidirectional Grid Fusion Network for Accurate Land Cover Classification of High-Resolution Remote Sensing Images","Y. Wang; H. Shi; Y. Zhuang; Q. Sang; L. Chen","Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Electronic Engineering and Computer Science, Peking University, Beijing, China; Beijing Key Laboratory of Embedded Realtime Information Processing Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Sep 2020","2020","13","","5508","5517","Land cover classification has achieved significant advances by employing deep convolutional network (ConvNet) based methods. Following the paradigm of learning deep models, land cover classification is modeled as semantic segmentation of very high resolution remote sensing images. In order to obtain accurate segmentation results, high-level categorical semantics and low-level spatial details should be effectively fused. To this end, we propose a novel bidirectional gird fusion network to aggregate the multilevel features across the ConvNet. Specifically, the proposed model is characterized by a bidirectional fusion architecture, which enriches diversity of feature interaction by encouraging bidirectional information flow. In this way, our model gains mutual benefits between top-down and bottom-up information flows. Moreover, a grid fusion architecture is then followed for further feature refinement in a dense and hierarchical fusion manner. Finally, effective feature upsampling is also critical for the multiple fusion operations. Consequently, a content-aware feature upsampling kernel is incorporated for further improvement. Our whole model consistently achieves significant improvement over state-of-the-art methods on two major datasets, ISPRS and GID.","2151-1535","","10.1109/JSTARS.2020.3023645","China Postdoctoral Science Foundation(grant numbers:2020M670162); Chang Jiang Scholars Program(grant numbers:T2012122); Hundred Leading Talent; Beijing Science and Technology(grant numbers:Z141101001514005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9195158","Deep learning;land cover classification;semantic segmentation","Semantics;Remote sensing;Image segmentation;Image resolution;Task analysis;Kernel;Earth","convolutional neural nets;geophysical image processing;image classification;image fusion;image resolution;image sampling;image segmentation;land cover;learning (artificial intelligence);remote sensing","bidirectional grid fusion network;semantic segmentation;bidirectional fusion architecture;bidirectional information flow;content-aware feature upsampling kernel;land cover classification;high-resolution remote sensing images;deep convolutional network based methods;deep ConvNet based methods;learning deep model;high-level categorical semantic;multiple fusion operation","","2","","42","CCBY","11 Sep 2020","","","IEEE","IEEE Journals"
"Automatic Detection and Positioning of Ground Control Points Using TerraSAR-X Multiaspect Acquisitions","S. Montazeri; C. Gisinger; M. Eineder; X. x. Zhu","Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany; Chair of Remote Sensing Technology, Technische Universität München, Munich, Germany; Signal Processing for Earth Observation, Technische Universität München, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","20 Apr 2018","2018","56","5","2613","2632","Geodetic stereo synthetic aperture radar (SAR) is capable of absolute 3-D localization of natural persistent scatterers, which allows for ground control point (GCP) generation using only SAR data. The prerequisite for the method to achieve high-precision results is the correct detection of common scatterers in SAR images acquired from different viewing geometries. In this contribution, we describe three strategies for automatic detection of identical targets in SAR images of urban areas taken from different orbit tracks. Moreover, a complete workflow for automatic generation of large number of GCPs using SAR data is presented and its applicability is shown by exploiting TerraSAR-X high-resolution spotlight images over the city of Oulu, Finland, and a test site in Berlin, Germany.","1558-0644","","10.1109/TGRS.2017.2769078","European Research Council through the European Union Horizon 2020 Research And Innovation Program(grant numbers:ERC-2016-StG-714087); Helmholtz Association through the framework of the Young Investigators Group “SiPEO”(grant numbers:VH-NG-1018); Munich Aerospace e.V. Fakultät für Luft- und Raumfahrt; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8254388","Geodetic stereo synthetic aperture radar (SAR);ground control point;positioning;SAR;TerraSAR-X (TS-X)","Synthetic aperture radar;Satellites;Geometry;Timing;Radar tracking;Orbits;Target tracking","geophysical image processing;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","SAR images;SAR data;automatic detection;geodetic stereo synthetic aperture radar;3-D localization;natural persistent scatterers;ground control point generation;high-precision results;correct detection;common scatterers;identical targets;orbit tracks;TerraSAR-X high-resolution spotlight images;GCP automatic generation;ground control point positioning;TerraSAR-X Multiaspect Acquisitions;Oulu city;Finland","","12","","40","OAPA","11 Jan 2018","","","IEEE","IEEE Journals"
"DeepSUM: Deep Neural Network for Super-Resolution of Unregistered Multitemporal Images","A. Bordone Molini; D. Valsesia; G. Fracastoro; E. Magli","Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy; Department of Electronics and Telecommunications, Politecnico di Torino, Turin, Italy","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2020","2020","58","5","3644","3656","Recently, convolutional neural networks (CNNs) have been successfully applied to many remote sensing problems. However, deep learning techniques for multi-image super-resolution (SR) from multitemporal unregistered imagery have received little attention so far. This article proposes a novel CNN-based technique that exploits both spatial and temporal correlations to combine multiple images. This novel framework integrates the spatial registration task directly inside the CNN, and allows one to exploit the representation learning capabilities of the network to enhance registration accuracy. The entire SR process relies on a single CNN with three main stages: shared 2-D convolutions to extract high-dimensional features from the input images; a subnetwork proposing registration filters derived from the high-dimensional feature representations; 3-D convolutions for slow fusion of the features from multiple images. The whole network can be trained end-to-end to recover a single high-resolution image from multiple unregistered low-resolution images. The method presented in this article is the winner of the PROBA-V SR challenge issued by the European Space Agency (ESA).","1558-0644","","10.1109/TGRS.2019.2959248","Smart-Data@PoliTO Center for Big Data and Machine Learning Technologies; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946717","Convolutional neural networks (CNNs);dynamic filter networks;multi-image super resolution (MISR);multitemporal images","Remote sensing;Image reconstruction;Spatial resolution;Deep learning;Feature extraction;Image registration","convolutional neural nets;feature extraction;geophysical image processing;image filtering;image fusion;image registration;image representation;image resolution;learning (artificial intelligence);remote sensing","image fusion;high-dimensional feature representations;registration filters;high-dimensional feature extraction;SR process;registration accuracy;spatial registration task;temporal correlations;spatial correlations;CNN-based technique;multitemporal unregistered imagery;multiimage super-resolution;deep learning techniques;remote sensing problems;convolutional neural networks;unregistered multitemporal images;deep neural network;DeepSUM","","52","","69","IEEE","31 Dec 2019","","","IEEE","IEEE Journals"
"A Dual-UNet With Multistage Details Injection for Hyperspectral Image Fusion","J. Xiao; J. Li; Q. Yuan; L. Zhang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","13","Enhancement of hyperspectral image (HSI) resolution is significant for better application in practice. In this article, a dual U-Net (D-UNet) is proposed to improve the spatial resolution of HSI. The whole network contains two parts. One is the detail extraction network, whose network architecture is encoder–decoder and mainly extracts various spatial features from multispectral images (MSIs). Another is the spatio-spectral fusion network (SSFN), which aims at injecting the features from the detail extraction network into HSI for better reconstruction. Furthermore, in the primary stage of the whole network, a novel multiscale spatio-spectral attention module (MSSAM) is utilized to pay more attention to important features at different scales. Considering the complex ground scenes, the features of different scale and depth are continually extracted and fused in the whole network. The experimental results show that the proposed method is more effective compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2021.3101848","National Natural Science Foundation of China(grant numbers:62071341,41922008,61971319); Natural Science Foundation of Hubei Province(grant numbers:2020CFA051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9513426","Attention module;hyperspectral image (HSI);multiscale;U-Net","Feature extraction;Convolution;Image reconstruction;Data mining;Tensors;Spatial resolution;Sparse matrices","feature extraction;geophysical image processing;image coding;image fusion;image representation;image resolution;object detection","spatial resolution;HSI;detail extraction network;network architecture;spatial features;multispectral images;spatio-spectral fusion network;multistage details;hyperspectral image resolution;dual U-Net;multiscale spatio-spectral attention module;multistage detail injection;hyperspectral image fusion enhancement;MSSAM;complex ground scenes","","5","","47","IEEE","13 Aug 2021","","","IEEE","IEEE Journals"
"SAR Image Change Detection Using Saliency Extraction and Shearlet Transform","Y. Zhang; S. Wang; C. Wang; J. Li; H. Zhang","Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; School of Communication Engineering, Jilin University, Changchun, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; School of Communication Engineering, Jilin University, Changchun, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4701","4710","Change detection has recently become a topic of great significance in the field of remote sensing. However, as one of the traditional effective detection methods, synthetic aperture radar (SAR) image change detection based on wavelet transform fusion remains limited because of the existence of speckle noise and because multidirectional information has been underutilized. Therefore, we propose an unsupervised change detection method using saliency extraction and the shearlet transform. Saliency extraction is first used to homogenize registered images to reduce speckle noise. Using a subtraction operation for preprocessed binary images, we then obtain a saliency-guided difference image (DI) that includes the main contour change information. Then, the Gauss-log-ratio DI includes the detailed change information at the edge of the image and acts as an auxiliary DI. Next, two DIs are fused with the shearlet transform. During this process, the DI is decomposed into one low-frequency and four high-frequency subimages. The low-frequency subimage contains image contour information and the high-frequency subimages contain the edge information of the images. Compared to wavelet fusion, in our method, no extra fusion noise occurs because the shearlet transform performs multiscale analysis. The final change map can be obtained through maximum entropy segmentation. Real SAR image pairs in areas of Bern, Switzerland, and Suzhou, China, are used to verify the proposed change detection method. The experimental results demonstrate the effectiveness of the proposed method when compared to the reference methods.","2151-1535","","10.1109/JSTARS.2018.2866540","National Natural Science Foundation of China(grant numbers:41331176,61631009); National Key Research and Development Plan of the 13th five-year(grant numbers:2017YFB0404800); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454801","Change detection;multidirectional information;saliency extraction;shearlet transform;synthetic aperture radar (SAR)","Synthetic aperture radar;Feature extraction;Wavelet transforms;Remote sensing;Image edge detection","entropy;image fusion;image segmentation;radar imaging;remote sensing;speckle;synthetic aperture radar;wavelet transforms","SAR image change detection;saliency extraction;traditional effective detection methods;synthetic aperture radar image change detection;speckle noise;multidirectional information;unsupervised change detection method;registered images;preprocessed binary images;saliency-guided difference image;main contour change information;Gauss-log-ratio DI;detailed change information;auxiliary DI;high-frequency subimages;low-frequency subimage;image contour information;edge information;wavelet fusion;extra fusion noise;final change map;SAR image pairs;reference methods","","18","","31","IEEE","5 Sep 2018","","","IEEE","IEEE Journals"
"Topology Design for Geosynchronous Spaceborne–Airborne Multistatic SAR","H. An; J. Wu; Z. Sun; J. Yang; Y. Huang; H. Yang","School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","2 Nov 2018","2018","15","11","1715","1719","Geosynchronous (GEO) spaceborne-airborne multistatic synthetic aperture radar (GEO MulSAR) is more flexible and accessible in remote sensing applications because of the high-altitude illuminator and the separation of the receivers and transmitter. In addition, the information obtained by the multiple airborne receivers can be fused to enhance the spatial resolution. However, the fused spatial resolution severely depends on the applied multistatic topology. To achieve the optimal fused spatial resolution by properly adjusting the imaging topology, a topology design method is proposed in this letter. First, the spatial resolution model of GEO MulSAR is given, and the dependence of the spatial resolution on the multistatic topology is analyzed in detail. Then, a topology design method is proposed to obtain the best multistatic topology that simultaneously optimizes the resolution cell area and resolution disequilibrium factor. Finally, the simulation results validate the effectiveness of the proposed method, and some insights into designing the multistatic topology are given.","1558-0571","","10.1109/LGRS.2018.2856502","National Natural Science Foundation of China(grant numbers:61771113); Fundamental Research Funds for Central Universities(grant numbers:2672018ZYGX2018J017); SAST Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434232","Geosynchronous (GEO) spaceborne–airborne synthetic aperture radar (SAR);multistatic SAR;resolution enhancement;topology design","Spatial resolution;Receivers;Synthetic aperture radar;Topology;Transmitters;Imaging","airborne radar;image fusion;image resolution;radar imaging;radar receivers;radar resolution;radar transmitters;remote sensing by radar;spaceborne radar;synthetic aperture radar","geosynchronous spaceborne-airborne multistatic synthetic aperture radar;GEO MulSAR;remote sensing applications;high-altitude illuminator;transmitter;multiple airborne receivers;optimal fused spatial resolution;imaging topology;topology design method;spatial resolution model;resolution cell area;resolution disequilibrium factor;fused spatial resolution;multistatic topology;resolution cell area optimization","","13","","11","IEEE","13 Aug 2018","","","IEEE","IEEE Journals"
"Pan-Sharpening by Multilevel Interband Structure Modeling","X. Lu; J. Zhang; T. Li; Y. Zhang","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2016","13","7","892","896","Pan-sharpening is designed to estimate multi-/hyperspectral (MS/HS) images that would have been observed with a sensor at higher resolution. It is a very important issue for many remote sensing and mapping applications. This letter proposes an improved pan-sharpening algorithm based on the ARSIS concept under the assumption that missing information of a low-resolution MS/HS image is linked to the high frequencies of MS/HS and panchromatic (PAN) images. The main object of this letter is to exhibit a multilevel interband structure model that better considers the inherent relationship between the hierarchical structures of MS/HS and PAN images. Several groups of data sets are used to demonstrate the performance of the proposed method. The results show that the proposed method outperforms the existing ARSIS-based and some other fusion techniques and can be extended to HS image sharpening as well.","1558-0571","","10.1109/LGRS.2016.2552379","National Natural Science Foundation of China(grant numbers:61271348,61471148); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460212","ARSIS;hyperspectral (HS);interband structure model (IBSM);multispectral (MS);pan-sharpening;ARSIS;hyperspectral (HS);interband structure model (IBSM);multispectral (MS);pan-sharpening","Spatial resolution;Multiresolution analysis;Computational modeling;Transforms;Adaptation models;Context modeling","geophysical image processing;image fusion;image resolution;remote sensing","multilevel interband structure modeling;remote sensing;mapping applications;pan-sharpening algorithm;low-resolution multispectral image;high frequencies;panchromatic images;inherent relationship;hierarchical structures;fusion technique;ARSIS-based technique;HS image sharpening;low-resolution hyperspectral image","","12","","14","IEEE","27 Apr 2016","","","IEEE","IEEE Journals"
"The Outcome of the 2021 IEEE GRSS Data Fusion Contest - Track DSE: Detection of Settlements Without Electricity","Y. Ma; Y. Li; K. Feng; Y. Xia; Q. Huang; H. Zhang; C. Prieur; G. Licciardi; H. Malha; J. Chanussot; P. Ghamisi; R. Hänsch; N. Yokoya","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Xi’an, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; GIPSA-Lab, Grenoble INP, Saint Martin, France; GIPSA-Lab, Grenoble INP, Saint Martin, France; AI Strategy and Solutions, Hewlett Packard Enterprise Co., Grenoble, France; GIPSA-Lab, Grenoble INP, Saint Martin, France; Institute of Advanced Research in Artificial Intelligence, Vienna, Austria; German Aerospace Center (DLR), Weßling, Germany; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Dec 2021","2021","14","","12375","12385","In this article, we elaborate on the scientific outcomes of the 2021 Data Fusion Contest (DFC2021), which was organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society, on the subject of geospatial artificial intelligence for social good. The ultimate objective of the contest was to model the state and changes of artificial and natural environments from multimodal and multitemporal remotely sensed data towards sustainable developments. DFC2021 consisted of two challenge tracks: Detection of settlements without electricity (DSE) and multitemporal semantic change detection. We focus here on the outcome of the DSE track. This article presents the corresponding approaches and reports the results of the best-performing methods during the contest.","2151-1535","","10.1109/JSTARS.2021.3130446","MIAI @ Grenoble Alpes(grant numbers:ANR-19-P3IA-0003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626564","Convolutional neural networks;deep learning;image analysis and data fusion;land cover change detection;multimodal;random forests;weak supervision","Remote sensing;Data integration;Earth;Data models;Task analysis;Semantics;Satellite broadcasting","data fusion;geophysical image processing;geophysical techniques;image fusion;remote sensing;sustainable development;terrain mapping","2021 IEEE GRSS Data Fusion Contest - track DSE;electricity;scientific outcomes;2021 Data Fusion Contest;DFC2021;Data Fusion Technical Committee;geospatial artificial intelligence;artificial environments;natural environments;multimodal sensed data;multitemporal remotely sensed data;multitemporal semantic change detection;DSE track","","2","","33","CCBY","24 Nov 2021","","","IEEE","IEEE Journals"
"An All-Scale Feature Fusion Network With Boundary Point Prediction for Cloud Detection","W. Wang; Z. Shi","State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","4 Jan 2022","2022","19","","1","5","Cloud detection is a significant pre-processing for remote sensing images. In recent years, many methods based on deep learning are proposed to detect clouds and multi-scale feature fusion is often used in these methods. However, most existing methods fuse features through concatenation and element-wise summation, which are simple and can be improved in spatial information recovery. Therefore, we explore the way of fusing features to recover the missing spatial information more sufficiently. Besides, we also observe that some cloud detection results are not accurate enough near the boundary of clouds. In view of the above observations, in this letter, we propose a cloud detection network, ABNet, which includes All-scale feature Fusion modules and a Boundary point Prediction module. The All-scale feature Fusion module can optimize the features and recover spatial information by integrating features of all scales. And the Boundary point Prediction module further remedies cloud boundary information by classifying the cloud boundary points separately. Experimental results demonstrate that our method improves the accuracy of cloud detection compared with other methods.","1558-0571","","10.1109/LGRS.2021.3110869","National Key Research and Development Program of China(grant numbers:2019YFC1510905); National Natural Science Foundation of China(grant numbers:62125102); Beijing Natural Science Foundation(grant numbers:4192034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548325","Boundary points;cloud detection;feature fusion","Feature extraction;Decoding;Fuses;Cloud computing;Image reconstruction;Convolution;Visualization","clouds;feature extraction;geophysical image processing;geophysical signal processing;image fusion;remote sensing;sensor fusion","remote sensing images;spatial information recovery;missing spatial information;cloud detection results;cloud detection network;All-scale feature Fusion modules;Boundary point Prediction module;remedies cloud boundary information;cloud boundary points;All-scale feature Fusion network;significant pre-processing","","1","","24","IEEE","27 Sep 2021","","","IEEE","IEEE Journals"
"DADR: Dual Attention Based Dual Regression Network for Remote Sensing Image Pan-Sharpening","X. Su; J. Li; Z. Hua","School of Information and Electronic Engineering, Shandong Technology and Business University, Institute of Network Technology, Yantai, China; School of Computer Science and Technology, Shandong Technology and Business University, Yantai, China; School of Information and Electronic Engineering, Shandong Technology and Business University, Institute of Network Technology, Yantai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13 Jun 2022","2022","15","","4397","4413","In order to solve the single imaging problem of remote sensing satellites, pan-sharpening technology is proposed. By fusing multispectral (MS) images and panchromatic (PAN) images, high-resolution multispectral resulting images are obtained. There is an urgent need to solve the problem of how to achieve higher spatial and spectral resolution in fused images. We propose an dual attention based dual regression network (DADR) architecture. The DADR network is mainly divided into three stages: feature extraction and feature fusion, image reconstruction, and dual regression network. Residual channel attention module (RCAM) and effective channel attention module (ECA) are added to the backbone network, so that the network can learn important spectral and spatial information and improve the ability of the network to retain information. The dual regression network solves the problem of adaptive and poor performance generated by the image reconstruction model. Because the dual regression network learns directly from feature fusion images, our model can better adapt to the data in real scenes and has stronger generalization ability. A series of experiments were conducted on GF-2, QB, and WV2 datasets, respectively. The validity of the DADR method was verified by quantitative comparison and qualitative analysis. Meanwhile, extensive experiments demonstrate that the DADR method is superior to other existing advanced pan-sharpening methods.","2151-1535","","10.1109/JSTARS.2022.3179825","National Natural Science Foundation of China(grant numbers:61772319,62002200,61972235,12001327); Shandong Natural Science Foundation of China(grant numbers:ZR2020QF012,ZR2021MF068); Yantai Science and Technology Innovation Development Plan(grant numbers:2022JCYJ031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787814","Attention mechanism;dual regression network;multispectral image;panchromatic image;pansharpening","Feature extraction;Remote sensing;Spatial resolution;Image reconstruction;Adaptation models;Satellites;Deep learning","feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;regression analysis;remote sensing","multispectral images;dual attention based dual regression network;feature fusion images;residual channel attention module;image reconstruction model;backbone network;DADR network;high-resolution multispectral resulting images;panchromatic images;single imaging problem;remote sensing image pan-sharpening;DADR method","","","","56","CCBYNCND","3 Jun 2022","","","IEEE","IEEE Journals"
"Progressive Band Subset Fusion for Hyperspectral Anomaly Detection","F. Li; M. Song; C. Yu; Y. Wang; C. -I. Chang","Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China; Department of Computer Science and Information Management, Providence University, Taichung, Taiwan","IEEE Transactions on Geoscience and Remote Sensing","12 Jul 2022","2022","60","","1","24","This article presents a new approach, called progressive band subset fusion (PBSF) for hyperspectral anomaly detection. Unlike band selection (BS) which selects bands according to band prioritization or band search strategies, PBSF fuses band subsets progressively during data collection processing. It is completely opposite to BS that must be done after data are acquired and then select bands by removing spectral redundancy as post-data processing. To accomplish PBSF, two versions of PBSF are derived: PBSF of the multiple-band subset (PBSF-MBS) and PBSF of uniform BS (PBSF-UBS). In particular, the fusion process takes place in an anomaly detector from a real-time processing perspective. Three approaches are developed to realize PBSF of two-band subsets simultaneously: PBSF-band sequential (PBSF-BSQ), PBSF-RT, and PBSF-zigzag. Extensive experiments demonstrate that PBSF has advantages over BS in many ways.","1558-0644","","10.1109/TGRS.2022.3186612","Cultivation Program for the Excellent Doctoral Dissertation of Dalian Maritime University(grant numbers:2022YBPY006); National Nature Science Foundation of China(grant numbers:61971082,61890964); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9807271","Anomaly detection (AD);band fusion (BF);progressive band subset fusion (PBSF)","Fuses;Hyperspectral imaging;Detectors;Correlation;Real-time systems;Anomaly detection;Object detection","geophysical image processing;hyperspectral imaging;image fusion;remote sensing;spectral analysis","progressive band subset fusion;PBSF-UBS;PBSF-MBS;multiple-band subset;post-data processing;data collection processing;PBSF fuses band subsets;band prioritization;band selection;hyperspectral anomaly detection;BS;PBSF-zigzag;PBSF-RT;PBSF-BSQ;PBSF-band sequential;two-band subsets;fusion process","","1","","44","IEEE","27 Jun 2022","","","IEEE","IEEE Journals"
"Hyperspectral and Panchromatic Image Fusion via Adaptive Tensor and Multi-Scale Retinex Algorithm","J. Qu; Y. Li; Q. Du; H. Xia","State Key Laboratory of Integrated Service Network, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, School of Telecommunications Engineering, Xidian University, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, USA; Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions, Ministry of Education, Henan University, Kaifeng, China","IEEE Access","19 Feb 2020","2020","8","","30522","30532","Fusion of panchromatic image (PANI) and hyperspectral image (HSI) to obtain an output image with high spatial and spectral resolutions has received increasing interests recently. We propose a new image fusion method for HSI and PANI by combining adaptive tensor with a multi-scale Retinex algorithm in this paper. In the proposed method, an adaptive tensor based method is presented to effectively extract the structure information of HSI, and multi-scale Retinex algorithm is introduced to obtain the spatial and structure details of PANI. To integrate spatial structure information, a gradient-based weighted fusion strategy is proposed to combine spatial details of HSI and PANI. The integrated structure details are injected to generate the fused HSI. Experiments using both simulated and real remote sensing data sets demonstrated that the proposed fusion algorithm performs better than the state-of-the-art algorithms in visual inspection and objective assessment.","2169-3536","","10.1109/ACCESS.2020.2972939","National Natural Science Foundation of China(grant numbers:61571345,91538101,61501346,61502367,61701360); Higher Education Discipline Innovation Project(grant numbers:B08038); Supported by Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; Xidian University; Henan University(grant numbers:GTYR201904); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8998270","Image fusion;hyperspectral image;panchromatic image;adaptive tensor;multi-scale Retinex algorithm","Tensile stress;Lighting;Eigenvalues and eigenfunctions;Hyperspectral imaging;Matrix decomposition;Heuristic algorithms;Spatial resolution","image colour analysis;image enhancement;image fusion;image resolution;image sensors;tensors","spectral resolutions;image fusion method;multiscale Retinex algorithm;adaptive tensor based method;spatial structure information;gradient-based weighted fusion strategy;spatial details;integrated structure details;fused HSI;fusion algorithm;hyperspectral image;high spatial resolutions;panchromatic image fusion;hyperspectral image fusion","","9","","49","CCBY","13 Feb 2020","","","IEEE","IEEE Journals"
"Road Detection via a Dual-Task Network Based on Cross-Layer Graph Fusion Modules","Z. Hu; W. Shi; H. Liu; X. Chen","School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China","IEEE Geoscience and Remote Sensing Letters","30 Aug 2022","2022","19","","1","5","Road detection based on remote sensing images is of great significance to intelligent traffic management. The performances of the mainstream road detection methods are mainly determined by their extracted features, whose richness and robustness can be enhanced by fusing features of different types and cross-layer connections. However, the features in the existing mainstream model frameworks are often similar in the same layer by the single-task training, and the traditional cross-layer fusion ways are too simple to obtain an efficient effect, so more complex fusion ways besides concatenation and addition deserve to be explored. Aiming at the above defects, we propose a dual-task network (DTnet) for road detection and cross-layer graph fusion module (CGM): the DTnet consists of two parallel branches for road area and edge detection while enhancing the feature diversity by fusing features between two branches through our designed feature bridge modules (FBMs). The CGM improves the cross-layer fusion effect by a complex feature stream graph, and four graph patterns are evaluated. Experimental results on three public datasets demonstrate that our method effectively improves the final detection result. The used code is available at https://github.com/huzican695/DTnet.","1558-0571","","10.1109/LGRS.2022.3198077","National Natural Science Foundation of China(grant numbers:62061002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854880","Cross-layer connections;dual-task network (DTnet);feature fusion;road detection","Roads;Feature extraction;Decoding;Task analysis;Semantics;Image edge detection;Bridges","edge detection;feature extraction;geophysical image processing;graph theory;image fusion;image representation;image segmentation;object detection;remote sensing;road vehicles;sensor fusion;traffic engineering computing","dual-task network;remote sensing images;intelligent traffic management;mainstream road detection methods;cross-layer connections;existing mainstream model frameworks;single-task training;traditional cross-layer fusion ways;complex fusion ways;cross-layer graph fusion module;road area;edge detection;feature diversity;designed feature bridge modules;cross-layer fusion effect;complex feature stream graph;final detection result","","","","20","IEEE","11 Aug 2022","","","IEEE","IEEE Journals"
"Novel Multiscale Decision Fusion Approach to Unsupervised Change Detection for High-Resolution Images","P. Shao; Y. Yi; Z. Liu; T. Dong; D. Ren","Hubei Engineering Technology Research Center for Farmland Environmental Monitoring and the College of Computer and Information Technology, China Three Gorges University, Yichang, China; Hubei Engineering Technology Research Center for Farmland Environmental Monitoring and the College of Computer and Information Technology, China Three Gorges University, Yichang, China; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong, China; Hubei Engineering Technology Research Center for Farmland Environmental Monitoring and the College of Computer and Information Technology, China Three Gorges University, Yichang, China; Hubei Engineering Technology Research Center for Farmland Environmental Monitoring and the College of Computer and Information Technology, China Three Gorges University, Yichang, China","IEEE Geoscience and Remote Sensing Letters","20 Jan 2022","2022","19","","1","5","High-resolution remote sensing images usually contain multiscale information, which can be used to enhance the change detection (CD) performance. How to make effective use of multiscale information needs intensive study. This letter presents a novel multiscale decision fusion (MDF) method for unsupervised CD based on Dempster–Shafer (DS) theory and modified conditional random field (CRF). The method consists of three main steps: 1) images of three different scales are created automatically by image segmentation, and then three-scale difference images (DIs) are produced by applying change vector analysis to the three-scale images; 2) the membership function of each scale DI is estimated by fuzzy clustering, and the fusion membership, as well as an initial CD map, is obtained by combining the estimated membership using DS theory; and 3) the initial CD map is refined with an improved CRF that incorporates a spatial attraction model. The proposed method can combine the multiscale information in images and the spatial contextual information. The effectiveness of the proposed method was validated by two experiments with high-resolution remote sensing images.","1558-0571","","10.1109/LGRS.2022.3140307","National Natural Science Foundation of China(grant numbers:41901341); Research Fund for Excellent Dissertation of China Three Gorges University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9668846","Change detection (CD);conditional random field (CRF);Dempster–Shafer theory (DS);multiscale decision fusion (MDF);remote sensing","Image segmentation;Remote sensing;Uncertainty;Sensors;Costs;Clustering algorithms;Shape","fuzzy set theory;geophysical image processing;image classification;image fusion;image resolution;image segmentation;pattern clustering;remote sensing;sensor fusion","high-resolution remote sensing images;multiscale information;change detection performance;novel multiscale decision fusion method;unsupervised CD;Dempster-Shafer theory;modified conditional random field;image segmentation;three-scale difference images;change vector analysis;three-scale images;scale DI;fusion membership;initial CD map;spatial contextual information;novel multiscale decision fusion approach;unsupervised change detection;high-resolution images","","3","","20","IEEE","4 Jan 2022","","","IEEE","IEEE Journals"
"Graph-Feature-Enhanced Selective Assignment Network for Hyperspectral and Multispectral Data Classification","W. Li; J. Wang; Y. Gao; M. Zhang; R. Tao; B. Zhang","Beijing Key Laboratory of Fractional Signals and Systems, Beijing, China; Beijing Key Laboratory of Fractional Signals and Systems, Beijing, China; Beijing Key Laboratory of Fractional Signals and Systems, Beijing, China; Beijing Key Laboratory of Fractional Signals and Systems, Beijing, China; Beijing Key Laboratory of Fractional Signals and Systems, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2022","2022","60","","1","14","Due to rich spectral and spatial information, the combination of hyperspectral and multispectral images (MSIs) has been widely used for Earth observation, such as wetland classification. However, mining of meaningful features and effective fusion of multisource remote sensing data are still urgent problems to be solved. In this article, graph-feature-enhanced selective assignment network (GSANet) is proposed. On the one hand, a graph feature extraction module (GFEM) is designed to extract topological structure information and combine with the rich spectral–spatial information. In particular, the features obtained by convolution are first mapped to the graph feature space, and the graph convolution operation is used to achieve propagation between nodes for preserving topological structure information. Moreover, to reduce the difference of graph features resulting from the mapping function and better explore the complementary properties of multisource data, a novel graph fusion strategy-graph dependence fusion is designed. A transition graph is generated to enhance the association and interaction between different graph features, so as to avoid the information loss caused by simple fusion operation. On the other hand, a selective feature assignment module (SFAM) is developed to adaptively assign weights to different discriminative features. SFAM assigns weights to different features to selectively emphasize informative features and suppress less useful ones. Extensive experiments are conducted on two multisource remote sensing datasets, and the improvement of at least 1.27% and 0.98% compared to other state-of-the-art work demonstrates the superiority of the proposed GSANet.","1558-0644","","10.1109/TGRS.2022.3166252","National Key Research and Development Program of China(grant numbers:2021YFB3900502); Beijing Natural Science Foundation(grant numbers:JQ20021); National Natural Science Foundation of China(grant numbers:61922013,61421001,U1833203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754579","Graph feature extraction module (GFEM);hyperspectral and multispectral classification;multisource remote sensing data;selective feature assignment module (SFAM)","Feature extraction;Hyperspectral imaging;Wetlands;Data mining;Data integration;Support vector machines;Convolution","feature extraction;geophysical image processing;graph theory;image classification;image fusion;remote sensing","graph-feature-enhanced selective assignment network;multispectral data classification;spectral information;multisource remote sensing data;graph feature extraction module;topological structure information;rich spectral-spatial information;graph feature space;graph convolution operation;transition graph;selective feature assignment module;discriminative features;informative features;graph fusion strategy-graph dependence fusion;hyperspectral data classification;multisource remote sensing datasets;GSANet","","3","","44","IEEE","11 Apr 2022","","","IEEE","IEEE Journals"
"HISTIF: A New Spatiotemporal Image Fusion Method for High-Resolution Monitoring of Crops at the Subfield Level","J. Jiang; Q. Zhang; X. Yao; Y. Tian; Y. Zhu; W. Cao; T. Cheng","National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China; National Engineering and Technology Center for Information Agriculture, MOE Engineering Research Center of Smart Agriculture, MARA Key Laboratory of Crop System Analysis and Decision Making, Jiangsu Key Laboratory for Information Agriculture, Institute of Smart Agriculture, Nanjing Agricultural University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Aug 2020","2020","13","","4607","4626","Satellite-based time-series crop monitoring at the subfield level is essential to the efficient implementation of precision crop management. Existing spatiotemporal image fusion techniques can be helpful, but they were often proposed to generate medium-resolution images. This study proposed a high-resolution spatiotemporal image fusion method (HISTIF) consisting of filtering for cross-scale spatial matching (FCSM) and multiplicative modulation of temporal change (MMTC). In FCSM, we considered both point spread function effect and geo-registration errors between fine and coarse resolution images. Subsequently, MMTC used pixel-based multiplicative factors to estimate the temporal change between reference and prediction dates without image classification. The performance of HISTIF was evaluated using both simulated and real datasets with one from real Gaofen-1 (GF-1) and simulated Landsat-like/Sentinel-like images, and the other from real GF-1 and real Landsat/Sentinel-2 data on two sites. HISTIF was compared with the existing methods spatial and temporal adaptive reflectance fusion model (STARFM), FSDAF, and Fit-FC. The results demonstrated that HISTIF produced substantial reduction in the fusion error from cross-scale spatial mismatch and accurate reconstruction in spatial details within fields, regardless of simulated or real data. The images predicted by STARFM exhibited pronounced blocky artifacts. While the images predicted by HISTIF and Fit-FC both showed clear within-field variability patterns, HISTIF was able to reduce the spectral distortion more significantly than Fit-FC. Furthermore, HISTIF exhibited the most stable performance across sensors. The findings suggest that HISTIF could be beneficial for the frequent and detailed monitoring of crop growth at the subfield level.","2151-1535","","10.1109/JSTARS.2020.3016135","National Key R&D Program(grant numbers:2016YFD0300601); Jiangsu Planned Projects for Postdoctoral Research Funds(grant numbers:2018K229C); National Natural Science Foundation of China(grant numbers:41871259,31725020); Jiangsu Collaborative Innovation Center for Modern Crop Production; Academic Program Development of Jiangsu Higher Education Institutions; Qinghai Project of Transformation of Scientific and Technological Achievements(grant numbers:2018-NK-126); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165877","Crops;heterogeneity;image fusion;spatiotemporal fusion;subfield monitoring","Agriculture;Monitoring;Spatiotemporal phenomena;Spatial resolution;Sensors;Reflectivity","agriculture;crops;image fusion;image matching;image registration;image resolution;time series","subfield level;satellite-based time-series crop monitoring;precision crop management;high-resolution spatiotemporal image fusion method;HISTIF;FCSM;filtering for cross-scale spatial matching;MMTC;multiplicative modulation of temporal change","","10","","58","CCBY","12 Aug 2020","","","IEEE","IEEE Journals"
"LFC-SSD: Multiscale Aircraft Detection Based on Local Feature Correlation","Y. Nie; C. Bian; L. Li; H. Chen; S. Chen","School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Electronics and Information Technologyfor Space Systems, National Space Science Center, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Electronics and Information Technologyfor Space Systems, National Space Science Center, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Electronics and Information Technologyfor Space Systems, National Space Science Center, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Electronics and Information Technologyfor Space Systems, National Space Science Center, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","8 Jun 2022","2022","19","","1","5","Interpreting airborne remote sensing images plays an important role in aviation control and battlefield situational awareness. However, highly dynamic aircraft detection remains challenging, owing to variable object sizes, flexible attitudes, and motion blur. This study develops a multiscale airborne aircraft dataset benchmark to overcome aircraft detection challenges, such as high intraclass variance, multiple scales and angles, motion blur, and partial occlusion. We also propose a trained-from-scratch aircraft detector, the local feature correlation single shot multibox detector (LFC-SSD), to detect multiscale aircraft. The LFC-SSD comprises a local correlation feature extraction module, called “Right-Residual,” and a feature fusion module using a reverse feature pyramid network (R-FPN). Right-Residual extends the global receptive field by aggregating contextual information while learning nonadjacent region features efficiently. R-FPN utilizes multipath information transfer horizontally with recursive integration to enhance the robust representation of the location information of multiscale object features. In addition, a specific default boxes method is designed for remote sensing images of aircraft. Extensive experimental results confirm the significant improvement of the proposed method over several existing state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3177836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781392","Aircraft detection;dataset benchmark;remote sensing image","Feature extraction;Aircraft;Remote sensing;Detectors;Object detection;Correlation;Convolution","aerospace engineering;aircraft;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);object detection;remote sensing","feature fusion module;reverse feature pyramid network;R-FPN;nonadjacent region features;multipath information transfer;multiscale object features;LFC-SSD;multiscale aircraft detection;airborne remote sensing images;aviation control;battlefield situational awareness;highly dynamic aircraft detection;variable object sizes;flexible attitudes;motion blur;multiscale airborne aircraft dataset benchmark;aircraft detection challenges;high intraclass variance;partial occlusion;trained-from-scratch aircraft detector;local feature correlation single shot multibox detector;local correlation feature extraction module","","","","18","IEEE","25 May 2022","","","IEEE","IEEE Journals"
"RFSDAF: A New Spatiotemporal Fusion Method Robust to Registration Errors","S. Hou; W. Sun; B. Guo; X. Li; J. Zhang; C. Xu; X. Li; Y. Shao; C. Li","China Academy of Space Technology (Xi’an), Xi’an, China; School of Aerospace Science and Technology, Xidian University, Xi’an, China; Institute of Intelligent Control and Image Engineering, Xidian University, Xi’an, China; Key Laboratory for Environment and Disaster Monitoring and Evaluation of Hubei, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; China Academy of Space Technology (Xi’an), Xi’an, China; China Academy of Space Technology (Xi’an), Xi’an, China; China Academy of Space Technology (Xi’an), Xi’an, China; China Academy of Space Technology (Xi’an), Xi’an, China; Institute of Intelligent Control and Image Engineering, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","3 Mar 2022","2022","60","","1","18","Spatiotemporal fusion is commonly used in remote sensing to generate fine spatial–temporal resolution images. Among most present spatiotemporal fusion methods, the registration error results in a mismatch in the reflectance and class abundance information within the MODIS pixel to be analyzed during the spatiotemporal fusion. In this article, a new spatiotemporal fusion model, robust flexible spatiotemporal data fusion (RFSDAF), is proposed; in other words, this method is robust to registration errors. The RFSDAF method uses a multiscale fusion strategy that is adaptive to different degrees of coregistration error. It incorporates multiscale information, which extends the analysis of the reflectance and class fraction from per-MODIS pixel to inter-MODIS pixels, and it is robust to coregistration errors. This method does not require high-accuracy preprocession coregistration algorithms and can automatically reduce the effect of the registration error to a great extent. The RFSDAF method is compared with four spatiotemporal image fusion algorithms, and the effectiveness of the method in resolving the registration error is demonstrated using both a simulated dataset and two actual satellite datasets. The experimental results show that the RFSDAF method can better reduce the impact of the registration error than the spatial and temporal adaptive reflectance fusion model (STARFM) and FSDAF, which adopts the phase cross-correlation algorithm to preprocess and coregister the MODIS–Landsat image through real image experiments. Consequently, the RFSDAF method has a great robustness for real spatiotemporal image fusion with registration errors and has the potential for monitoring land surface dynamics.","1558-0644","","10.1109/TGRS.2021.3138078","Natural Science Foundation of China(grant numbers:62071457,62171341,61801377); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9662296","Flexible spatiotemporal data fusion (FSDAF);multiscale analysis;registration error;spatiotemporal fusion","Remote sensing;Spatiotemporal phenomena;Earth;Artificial satellites;MODIS;Reflectivity;Geology","geophysical image processing;image fusion;image registration;image resolution;remote sensing;sensor fusion;spatiotemporal phenomena","spatiotemporal fusion method;fine spatial-temporal resolution images;registration error results;spatiotemporal fusion model;robust flexible spatiotemporal data fusion;RFSDAF method;multiscale fusion strategy;coregistration error;spatiotemporal image fusion algorithms","","1","","73","IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"Decision Fusion With Multiple Spatial Supports by Conditional Random Fields","D. Tuia; M. Volpi; G. Moser","GeoInformation Science and Remote Sensing Laboratory, Wageningen University and Research, Wageningen, The Netherlands; Swiss Data Science Center, ETH Zurich, Zürich, Switzerland; Department of Electrical, Electronic, Telecommunication Engineering, and Naval Architecture, University of Genoa, Genoa, Italy","IEEE Transactions on Geoscience and Remote Sensing","21 May 2018","2018","56","6","3277","3289","Classification of remotely sensed images into land cover or land use is highly dependent on geographical information at least at two levels. First, land cover classes are observed in a spatially smooth domain separated by sharp region boundaries. Second, land classes and observation scale are also tightly intertwined: they tend to be consistent within areas of homogeneous appearance, or regions, in the sense that all pixels within a roof should be classified as roof, independently on the spatial support used for the classification. In this paper, we follow these two observations and encode them as priors in an energy minimization framework based on conditional random fields (CRFs), where classification results obtained at pixel and region levels are probabilistically fused. The aim is to enforce the final maps to be consistent not only in their own spatial supports (pixel and region) but also across supports, i.e., by getting the predictions on the pixel lattice and on the set of regions to agree. To this end, we define an energy function with three terms: 1) a data term for the individual elements in each support (support-specific nodes); 2) spatial regularization terms in a neighborhood for each of the supports (support-specific edges); and 3) a regularization term between individual pixels and the region containing each of them (intersupports edges). We utilize these priors in a unified energy minimization problem that can be optimized by standard solvers. The proposed 2LCRF model consists of a CRF defined over a bipartite graph, i.e., two interconnected layers within a single graph accounting for interlattice connections. 2LCRF is tested on two very high-resolution data sets involving submetric satellite and subdecimeter aerial data. In all cases, 2LCRF improves the result obtained by the independent base model (either random forests or convolutional neural networks) and by standard CRF models enforcing smoothness in the spatial domain.","1558-0644","","10.1109/TGRS.2018.2797316","Swiss National Science Foundation through “Multimodal Machine Learning for Remote Sensing Information Fusion”(grant numbers:150593); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319920","Classification;conditional random fields (CRF);convolutional neural networks (CNNs);hierarchical models;region-based analysis;semantic labeling","Semantics;Remote sensing;Task analysis;Standards;Labeling;Lattices;Convolutional neural networks","convolution;edge detection;feedforward neural nets;geophysical image processing;graph theory;image classification;image fusion;image resolution;image segmentation;land cover;minimisation;probability;random processes;remote sensing;terrain mapping","decision fusion;multiple spatial supports;conditional random fields;remotely sensed images;geographical information;land cover classes;sharp region boundaries;land classes;observation scale;spatial support;energy minimization framework;classification results;pixel lattice;support-specific nodes;support-specific edges;regularization term;unified energy minimization problem;2LCRF model;high-resolution data sets;independent base model;random forests;standard CRF models;convolutional neural network","","19","","75","IEEE","19 Mar 2018","","","IEEE","IEEE Journals"
"A Pansharpening Based on the Non-Subsampled Contourlet Transform and Convolutional Autoencoder: Application to QuickBird Imagery","A. A. Smadi; S. Yang; A. Abugabah; A. A. Alzubi; L. Sanzogni","School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; College of Technological Innovation, Zayed University, Abu Dhabi, United Arab Emirates; Computer Science Department, Community College, King Saud University, Riyadh, Saudi Arabia; Department of Business Strategy and Innovation, Nathan Campus, Griffith University, Nathan, QLD, Australia","IEEE Access","2 May 2022","2022","10","","44778","44788","This paper presents a pansharpening technique based on the non-subsampled contourlet transform (NSCT) and convolutional autoencoder (CAE). NSCT is exceptionally proficient at presenting orientation information and capturing the internal geometry of objects. First, it’s used to decompose the multispectral (MS) and panchromatic (PAN) images into high-frequency and low-frequency components using the same number of decomposition levels. Second, a CAE network is trained to generate original low-frequency PAN images from their spatially degraded versions. Low-resolution multispectral images are then fed into the trained convolutional autoencoder network to generate estimated high-resolution multispectral images. Third, another CAE network is trained to generate original high-frequency PAN images from their spatially degraded versions. The result of low-pass CAE is fed to the trained high-pass CAE to generate estimated high-resolution multispectral images. The final pan-sharpened image is accomplished by injecting the detailed map of the spectral bands into the corresponding estimated high-resolution multispectral bands. The proposed method is tested on QuickBird datasets and compared with some existing pan-sharpening techniques. Objective and subjective results demonstrate the efficiency of the proposed method.","2169-3536","","10.1109/ACCESS.2022.3169698","Researchers Supporting Project, King Saud University, Riyadh, Saudi Arabia(grant numbers:RSP-2021/395); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761938","Pansharpening;multispectral image fusion;convolutional autoencoder;NSCT;remote sensing","Pansharpening;Transforms;Spatial resolution;Satellites;Image fusion;Encoding;Multispectral imaging;Remote sensing","geophysical image processing;image fusion;image resolution;remote sensing;transforms;wavelet transforms","nonsubsampled contourlet transform;QuickBird imagery;pansharpening technique;low-frequency components;CAE network;low-frequency PAN images;low-resolution multispectral images;trained convolutional autoencoder network;high-resolution multispectral images;high-frequency PAN images;low-pass CAE;high-pass CAE;final pan-sharpened image;high-resolution multispectral bands;existing pan-sharpening techniques","","","","41","CCBY","22 Apr 2022","","","IEEE","IEEE Journals"
"An Efficient Feature Pyramid Network for Object Detection in Remote Sensing Imagery","F. Qingyun; Z. Lin; W. Zhaokui","School of Aerospace Engineering, Tsinghua University, Beijing, China; Department of Aerospace Engineering and Engineering Mechanics, University of Cincinnati, Cincinnati, USA; School of Aerospace Engineering, Tsinghua University, Beijing, China","IEEE Access","28 May 2020","2020","8","","93058","93068","Scale diversity, small target, and power limitation have made remote sensing imagery a challenging field in object detection on satellites. Aiming at the aspects of scale diversity and small target, this paper provides a novel feature pyramid network with Adaptive Residual Spatial Bi-Fusion (ARSF) as a solution. ARSF nets introduce a robust fusion of multi-scale semantic information and fine spatial details. A spatial feature fusion module designed in networks with ARSF adapts to object size variation by learning the most crucial feature maps. Comparing to the original feature pyramid network, a shorter critical path for information transmission is formed in our method. Experiments show that a validation instance of YOLOv3-ARSF can achieve a state-of-the-art performance of 85.8 mAP on the NWPU-VHR10 dataset. YOLOv3-ARSF only 3MB larger than YOLOv3 but far exceeds YOLOv3 by 2.3% mAP, which shows our ARSF is efficient. As for the last challenge, two lightweight versions, ARSF(lite) and ARSF(lite+) are also validated for future research of online object detection on satellites in aerospace engineering. Visualizations and details are provided for a more comprehensive understanding.","2169-3536","","10.1109/ACCESS.2020.2993998","National Natural Science Foundation of China(grant numbers:11872034); Key Laboratory of Intelligent Infrared Perception, Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091190","Computer vision;object detection;remote sensing;satellites;aerospace engineering","Feature extraction;Object detection;Remote sensing;Satellites;Semantics;Convolution;Detectors","data visualisation;feature extraction;geophysical image processing;image fusion;learning (artificial intelligence);object detection;remote sensing","efficient feature pyramid network;remote sensing imagery;scale diversity;power limitation;satellites;Adaptive Residual Spatial Bi-Fusion;ARSF nets;robust fusion;multiscale semantic information;fine spatial details;spatial feature fusion module;crucial feature maps;shorter critical path;information transmission;YOLOv3-ARSF;NWPU-VHR10 dataset;online object detection","","7","","36","CCBY","11 May 2020","","","IEEE","IEEE Journals"
"FMAM-Net: Fusion Multi-Scale Attention Mechanism Network for Building Segmentation in Remote Sensing Images","H. Ye; R. Zhou; J. Wang; Z. Huang","School of Mechanical and Electrical Information, Yiwu Industrial & Commercial College, Jinhua, China; School of Mechanical and Electrical Information, Yiwu Industrial & Commercial College, Jinhua, China; School of Mechanical and Electrical Information, Yiwu Industrial & Commercial College, Jinhua, China; College of Physics and Electronic Information Engineering, Zhejiang Normal University, Jinhua, China","IEEE Access","30 Dec 2022","2022","10","","134241","134251","As the largest target in remote sensing images, buildings have important application value in urban planning and old city reconstruction. However, most networks have poor recognition ability on high resolution images, resulting in blurred boundaries in the segmented building maps. Then, the similarity between buildings and backgrounds will lead to inter-class indistinction. Finally, the diversity of buildings brings difficulties to segmentation, which requires the network to have better generalization ability. To address these problems, we propose Fusion Multi-scale Attention Mechanism Network (FMAM-Net). Firstly, we design Feature Refine Compensation Module(FRCM) to improve the boundary ambiguity problem, including Feature Refinement Module(FRM) and Feature Compensation Module(FCM). FRM utilizes the densely connected architecture to refine features and increase recognition capabilities. FCM introduces low-level features to make up for the lack of boundary information in high-level features. Secondly, to handle inter-class indistinction, we design Tandem Attention Module(TAM) and Parallel Attention Module(PAM). TAM is designed to sequentially filter some features from channels and spaces for adaptive feature refinement. PAM combines context information and uses high-level features to guide low-level features to select more distinguishable features. Finally, based on the binary cross entropy loss function, we add an evaluation index to reduce the error caused by determining the optimization direction only through cross entropy. On the Inria Aerial Image Labeling Dataset, FMAM-Net achieves mean IoU of 85.34%, which is 5.58% higher than AMUNet and 3.77% higher than our baseline(U-Net ResNet-34). On the WHU Dataset, IoU reached the maximum value of 91.06% on FMAM-Net, 1.67% higher than SARB-UNet and 0.2% higher than MAP-Net. The visualization results show that FMAM-Net improves the fuzzy boundary of building segmentation and reduces the inter-class indistinction.","2169-3536","","10.1109/ACCESS.2022.3231362","Department of Education of Zhejiang Province(grant numbers:2022JYTYB03); Zhejiang Federation of Social Sciences(grant numbers:2023B071); Jinhua Science and Technology Association(grant numbers:2022JKX37); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996387","Remote sensing image;building segmentation;attention mechanism;feature refinement;encoder-decoder","Remote sensing;Encoding;Decoding;Urban areas;Urban planning;Buildings;Image segmentation;Entropy;Visualization;Information filters","entropy;feature extraction;fuzzy set theory;geophysical image processing;image classification;image fusion;image reconstruction;image segmentation;object detection;remote sensing","adaptive feature refinement;blurred boundaries;boundary ambiguity problem;boundary information;building segmentation;buildings;distinguishable features;FMAM-Net achieves;fusion multiscale attention mechanism network;high resolution images;high-level features;Inria Aerial Image Labeling Dataset;inter-class indistinction;low-level features;MAP-Net;old city reconstruction;poor recognition ability;remote sensing images;segmented building maps;U-Net ResNet-34;urban planning","","","","47","CCBYNCND","21 Dec 2022","","","IEEE","IEEE Journals"
"A General Framework for Change Detection Using Multimodal Remote Sensing Data","S. Chirakkal; F. Bovolo; A. R. Misra; L. Bruzzone; A. Bhattacharya","Advanced Microwave and Hyperspectral Techniques Development Group, Space Applications Center, Indian Space Research Organisation, Ahmedabad, India; Center for Information and Communication Technology, Fondazione Bruno Kessler, Trento, Italy; Advanced Microwave and Hyperspectral Techniques Development Group, Space Applications Center, Indian Space Research Organisation, Ahmedabad, India; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Center of Studies in Resources Engineering, Indian Institute of Technology Bombay, Mumbai, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Nov 2021","2021","14","","10665","10680","A general framework for change detection is proposed to analyze multimodal remotely sensed data utilizing the Kronecker product between two data representations (vectors or matrices). The proposed method is sensor independent and provides comparable results to techniques that exist for specific sensors. The proposed fusion technique is a pixel-level approach that incorporates inputs from different modalities, rendering enriched multimodal data representation. Thus, the proposed hybridization procedure helps to assimilate multisensor information in a meaningful manner. A novel change index ($\zeta$) is defined for the general multimodal case. This index is then used to quantify the change in bitemporal remotely sensed data. This article explores the usability, consistency, and robustness of the proposed multimodal fusion framework, including the change index, with proper validation on two multimodal cases: 1) the dual-frequency ($C$- and $L$-band) fully polarimetric Danish EMISAR data and 2) the dual-polarimetric synthetic aperture radar and Sentinel-2 multispectral data. Detailed analysis and validation using extensive ground-truth data are presented to establish the proposed framework.","2151-1535","","10.1109/JSTARS.2021.3119358","India–Trento Program for Advanced Research; Department of Science and Technology of the Indian Government; Provincia Autonoma di Trento; Trentino Cultural Institute, now Bruno Kessler Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568726","Change detection (CD);dual-frequency PolSAR;Kronecker product of matrices;multimodal data;synthetic aperture radar (SAR) optical fusion","Synthetic aperture radar;Remote sensing;Indexes;Optical sensors;Data integration;Optical imaging;Radar polarimetry","geophysical image processing;image fusion;radar imaging;radar polarimetry;remote sensing;sensor fusion;synthetic aperture radar","matrices;specific sensors;fusion technique;pixel-level approach;enriched multimodal data representation;novel change index;general multimodal case;bitemporal remotely sensed data;multimodal fusion framework;multimodal cases;Sentinel-2 multispectral data;extensive ground-truth data;change detection;multimodal remote sensing data;Kronecker product;vectors","","2","","55","CCBY","12 Oct 2021","","","IEEE","IEEE Journals"
"PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","Q. Liu; H. Zhou; Q. Xu; X. Liu; Y. Wang","Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of the Computer Science and Engineering, Beihang University, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2021","2021","59","12","10227","10242","This article addresses the problem of remote sensing image pan-sharpening from the perspective of generative adversarial learning. We propose a novel deep neural network-based method named pansharpening GAN (PSGAN). To the best of our knowledge, this is one of the first attempts at producing high-quality pan-sharpened images with generative adversarial networks (GANs). The PSGAN consists of two components: a generative network (i.e., generator) and a discriminative network (i.e., discriminator). The generator is designed to accept panchromatic (PAN) and multispectral (MS) images as inputs and maps them to the desired high-resolution (HR) MS images, and the discriminator implements the adversarial training strategy for generating higher fidelity pan-sharpened images. In this article, we evaluate several architectures and designs, namely, two-stream input, stacking input, batch normalization layer, and attention mechanism to find the optimal solution for pan-sharpening. Extensive experiments on QuickBird, GaoFen-2, and WorldView-2 satellite images demonstrate that the proposed PSGANs not only are effective in generating high-quality HR MS images and superior to state-of-the-art methods but also generalize well to full-scale images.","1558-0644","","10.1109/TGRS.2020.3042974","NSFC(grant numbers:41871283,61601011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306912","Convolutional neural network (CNN);deep learning;generative adversarial network (GAN);pan-sharpening;residual learning","Generative adversarial networks;Generators;Neural networks;Computer architecture;Training;Spatial resolution;Data models","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","PSGAN;generative adversarial network;remote sensing image pan-sharpening;generative adversarial learning;deep neural network-based method named pansharpening GAN;high-quality pan-sharpened images;generative network;discriminative network;MS images;adversarial training strategy;higher fidelity pan-sharpened images;WorldView-2 satellite images;full-scale images","","49","","77","IEEE","24 Dec 2020","","","IEEE","IEEE Journals"
"A Deep-Learning-Based Fusion Approach for Global Cyclone Detection Using Multiple Remote Sensing Data","M. Xie; Y. Li; S. Dong","Navigation College, Dalian Maritime University, Dalian, China; Navigation College, Dalian Maritime University, Dalian, China; Navigation College, Dalian Maritime University, Dalian, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Nov 2022","2022","15","","9613","9622","Cyclone detection is a classic yet developing topic. Various methods have been developed for the purpose of cyclone detection based on sea level pressure, cloud imagery, and wind field. In this article, a data fusion approach that utilizes the data productions from multiple remote sensors is presented. A deep-learning-based object detection algorithm was adopted to form a global-scale cyclone detection model. Wind field data obtained from mean wind field-advanced scatterometer was integrated with the rainfall intensity data obtained from global precipitation measurement as the dataset for model training and testing. Feature pyramid network (FPN), which was designed for small target detection, was integrated with faster-regions with convolutional neural network to detect the cyclones within the fused dataset. The proposed model consists of two modules: a feature extractor and region proposal network based on FPN that searches for the potential areas of cyclones within the fused dataset, and a regions of interests processor that calibrate the locations of cyclone regions through a fully-connected neural network and a bounding box regression. An ablation experiment was also designed in the study in order to verify the necessity of data fusion. The results from ablation experiment suggested that the wind field data provided more contribution in the cyclone detection than the precipitation data.","2151-1535","","10.1109/JSTARS.2022.3219809","China National Key R&D Program(grant numbers:2019YFB1600600); Liaoning Revitalization Talents Program(grant numbers:XLYC2001002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9940529","Cyclone detection;data fusion;deep learning;precipitation;wind field","Cyclones;Remote sensing;Precipitation;Spatial resolution;Data models;Wind;Feature extraction","atmospheric techniques;feature extraction;geophysical image processing;image fusion;learning (artificial intelligence);neural nets;object detection;rain;regression analysis;remote sensing;sensor fusion;wind","cyclone regions;cyclones;data fusion approach;data productions;feature extractor;feature pyramid network;fused dataset;global cyclone detection;global precipitation measurement;global-scale cyclone detection model;mean wind field-advanced scatterometer;multiple remote sensing data;multiple remote sensors;object detection algorithm;precipitation data;rainfall intensity data;region proposal network;target detection;wind field data","","","","49","CCBY","7 Nov 2022","","","IEEE","IEEE Journals"
"Joint Variation Learning of Fusion and Difference Features for Change Detection in Remote Sensing Images","K. Jiang; W. Zhang; J. Liu; F. Liu; L. Xiao","Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","20 Dec 2022","2022","60","","1","18","Remote sensing (RS) image change detection (CD) is an Earth observation technique for detecting surface changes in the same area during a period. With the rapid development of deep learning, various deep neural networks especially Siamese ones have been widely used in the field of CD. However, they have the deficiency of insufficient contextual information aggregation, thus resulting in false and missed detections, and it is difficult to refine the detection of change edges. To alleviate these problems and obtain more accurate results, we propose an efficient self-weighted spatial–temporal attention network (SSANet). In contrast to the Siamese structure, our network is a novel joint learning framework composed of fusion subnetwork, difference subnetwork, and decoder. Fusion subnetwork is used to extract multiscale object features where we propose a multicore channel-aligning attention (MCA) module to capture the long-range semantic information for multiscale context aggregation. Difference subnetwork is used to extract the difference variation features, where we propose a feature differential reconfiguration (FDR) module to learn the temporal change information. FDR can effectively filter change information and reconstruct features to improve the perception of changed regions. To better balance the MCA and FDR modules, an asymmetric weighting (AW) module is proposed in the decoder to self-weight the multiscale features and generate the change map. Experiments demonstrate the efficiency of proposed subnetworks and modules, and the state-of-the-art performance of SSANet.","1558-0644","","10.1109/TGRS.2022.3226778","National Nature Science Foundation of China(grant numbers:62276133,61906093,61871226,61802190,61571230); Natural Science Foundation of Jiangsu Province, China(grant numbers:BK20220948); China Postdoctoral Science Foundation(grant numbers:2022M711637); Jiangsu Funding Program for Excellent Postdoctoral Talent(grant numbers:2022ZB266); Open Research Fund in 2021 of Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense(grant numbers:JSGP202101); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); National Major Research Plan of China(grant numbers:2016YFF0103604); Fundamental Research Funds for the Central Universities(grant numbers:JSGP202204,30918011104,30920021134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9970389","Asymmetric weighting (AW) module;change detection (CD);deep learning;feature differential reconfiguration (FDR) module;multicore channel-aligning attention (MCA) module","Feature extraction;Task analysis;Neural networks;Data mining;Decoding;Semantics;Deep learning","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image reconstruction;remote sensing","asymmetric weighting module;AW module;change detection;deep learning;deep neural networks;difference subnetwork;difference variation feature extraction;Earth observation technique;FDR module;feature differential reconfiguration module;fusion subnetwork;long-range semantic information capture;multicore channel-aligning attention module;multiscale context aggregation;multiscale object feature extraction;remote sensing images;self-weighted spatial-temporal attention network;Siamese structure;SSANet;temporal change information","","","","54","IEEE","5 Dec 2022","","","IEEE","IEEE Journals"
"Change Detection Based on Deep Features and Low Rank","B. Hou; Y. Wang; Q. Liu","State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","4 Dec 2017","2017","14","12","2418","2422","In this letter, we address the problem of change detection for remote sensing images from the perspective of visual saliency computation. The proposed method incorporates low-rank-based saliency computation and deep feature representation. First, multilevel convolutional neural network (CNN) features are extracted for superpixels generated using SLIC, in which a fixed-size CNN feature can be formed to represent each superpixel. Then, low-rank decomposition is applied to the change features of the two input images to generate saliency maps that indicate change probabilities of each pixel. Finally, binarized change map can be obtained with a simple threshold. To deal with scale variations, a multiscale fusion strategy is employed to produce more reliable detection results. Extensive experiments on Google Earth and GF-2 images demonstrate the feasibility and effectiveness of the proposed method.","1558-0571","","10.1109/LGRS.2017.2766840","Natural Science Foundation of China(grant numbers:61601011,61421003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8103911","Change detection (CD);convolutional neural network (CNN);low rank;remote sensing (RS);visual saliency","Feature extraction;Image segmentation;Sparse matrices;Matrix decomposition;Remote sensing;Visualization;Neural networks","feature extraction;geophysical image processing;image fusion;image representation;neural nets;object detection;remote sensing","change detection;remote sensing images;visual saliency computation;deep feature representation;multilevel convolutional neural network features;fixed-size CNN feature;low-rank decomposition;change features;change probabilities;binarized change map;GF-2 images;Google Earth;multiscale fusion strategy;saliency map generation;low-rank-based saliency computation","","58","","28","IEEE","10 Nov 2017","","","IEEE","IEEE Journals"
"Fusion of Deep Convolutional Neural Networks for Land Cover Classification of High-Resolution Imagery","G. J. Scott; R. A. Marcum; C. H. Davis; T. W. Nivin","Center for Geospatial Intelligence, University of Missouri–Columbia, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri–Columbia, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri–Columbia, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri–Columbia, Columbia, MO, USA","IEEE Geoscience and Remote Sensing Letters","25 Aug 2017","2017","14","9","1638","1642","Deep convolutional neural networks (DCNNs) have recently emerged as the highest performing approach for a number of image classification applications, including automated land cover classification of high-resolution remote-sensing imagery. In this letter, we investigate a variety of fusion techniques to blend multiple DCNN land cover classifiers into a single aggregate classifier. While feature-level fusion is widely used with deep neural networks, our approach instead focuses on fusion at the classification/information level. Herein, we train three different DCNNs: CaffeNet, GoogLeNet, and ResNet50. The effectiveness of various information fusion methods, including voting, weighted averages, and fuzzy integrals, is then evaluated. In particular, we used DCNN cross-validation results for the input densities of fuzzy integrals followed by evolutionary optimization. This novel approach produces the state-of-the-art classification results up to 99.3% for the UC Merced data set and the 99.2% for the RSD data set.","1558-0571","","10.1109/LGRS.2017.2722988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008745","Deep neural networks;high-resolution remote-sensing imagery;information fusion;land cover classification","Feature extraction;Remote sensing;Support vector machines;Fuses;Neural networks;Convolution","geophysical image processing;image classification;image fusion;land cover;neural nets;remote sensing","deep convolutional neural networks;image classification;automated land cover classification;high-resolution remote-sensing imagery;fusion technique;DCNN land cover classifier;single aggregate classifier;feature-level fusion;CaffeNet;GoogLeNet;ResNet50;information fusion method","","57","","26","IEEE","11 Aug 2017","","","IEEE","IEEE Journals"
"Land Cover Classification in Foggy Conditions: Toward Robust Models","W. Shi; W. Qin; Z. Yun; A. Chen; K. Huang; T. Zhao","School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China; Health Sciences Center, The University of Oklahoma, Oklahoma City, OK, USA; School of Instrument Science and Engineering, Southeast University, Nanjing, China; School of Instrument Science and Engineering, Southeast University, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","19 Jul 2022","2022","19","","1","5","Robust semantic labeling of high-resolution remote sensing images (RSIs) in foggy conditions is crucial for automatic monitoring of land covers. This remains a challenging task owing to the low interclass differentiation yet high intraclass variance and geometric size diversity. Although conventional convolutional neural networks (CNNs) have demonstrated state-of-the-art (SOTA) performance in semantic segmentation, most networks are primarily concerned with standard accuracy, while the influence on robustness is rarely explored. This letter proposes a reliable framework which is evaluated across various severity levels of fog corruptions. Utilizing HRNet as the backbone to maintain high-resolution representations, we develop a multimodal fusion module (MMF) to exploit the complementary information of lidar and multispectral data. Based on the evaluation experiment on fog corrupted datasets, our model demonstrates promising performance with an average mean Intersection over Union (mIoU) on the clean along with the corrupted datasets exceeding 80% and 56%, respectively.","1558-0571","","10.1109/LGRS.2022.3187779","Key Research and Development Program of Jiangsu Province(grant numbers:BE2019311); Jiangsu Modern Agricultural Industry Key Technology Innovation Project(grant numbers:CX(20)2013); National Key Research and Development Program of China(grant numbers:2020YFB160070301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812620","Attention mechanism;data fusion;remote sensing;robust deep learning;semantic segmentation","Semantics;Robustness;Automobiles;Feature extraction;Degradation;Remote sensing;Optical sensors","convolutional neural nets;image classification;image fusion;image resolution;image segmentation;land cover;remote sensing","foggy conditions;robust semantic labeling;high-resolution remote sensing images;RSI;automatic monitoring;land covers;low interclass differentiation;high intraclass variance;geometric size diversity;conventional convolutional neural networks;semantic segmentation;standard accuracy;reliable framework;severity levels;fog corruptions;high-resolution representations;multimodal fusion module;fog corrupted datasets;land cover classification","","1","","23","IEEE","1 Jul 2022","","","IEEE","IEEE Journals"
"Object Classification via Feature Fusion Based Marginalized Kernels","X. Bai; C. Liu; P. Ren; J. Zhou; H. Zhao; Y. Su","School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; College of Information and Control Engineering, China University of Petroleum (Huadong), Qingdao, China; School of Information and Communication Technology, Griffith University, Nathan, Qld., Australia; School of Instrumentation Science and Optoelectronics Engineering, Beihang University, Beijing, China; Beijing Institute of Space Mechanics and Electricity, Beijing, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2015","12","1","8","12","Various types of features can be extracted from very high resolution remote sensing images for object classification. It has been widely acknowledged that the classification performance can benefit from proper feature fusion. In this letter, we propose a softmax regression-based feature fusion method by learning distinct weights for different features. Our fusion method enables the estimation of object-to-class similarity measures and the conditional probabilities that each object belongs to different classes. Moreover, we introduce an approximate method for calculating the class-to-class similarities between different classes. Finally, the obtained fusion and similarity information are integrated into a marginalized kernel to build a support vector machine classifier. The advantages of our method are validated on QuickBird imagery.","1558-0571","","10.1109/LGRS.2014.2322953","National Natural Science Foundation of China(grant numbers:61370123,61105002); Shandong Outstanding Young Scientist Fund(grant numbers:BS2013DX006); Open Project Program of the National Laboratory of Pattern Recognition; Australian Research Councils DECRA Projects; High Resolution Earth Observation System Major Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844818","Feature fusion;kernel;object classification;remote sensing image;Feature fusion;kernel;object classification;remote sensing image","Kernel;Vectors;Feature extraction;Remote sensing;Support vector machines;Training;Shape","feature extraction;geophysical image processing;image classification;image fusion;image resolution;land cover;probability;regression analysis;support vector machines;terrain mapping","high resolution remote sensing images;object classification;classification performance;softmax regression-based feature fusion method;object-to-class similarity measures;conditional probabilities;class-to-class similarities;similarity information;support vector machine classifier;QuickBird imagery;feature fusion based marginalized kernels;land cover","","28","","14","IEEE","26 Jun 2014","","","IEEE","IEEE Journals"
"Toward Robust Segmentation Results Based on Fusion Methods for Very High Resolution Optical Image and LiDAR Data","M. M. Awad","National Council for Scientific Research, Beirut, Lebanon","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Apr 2017","2017","10","5","2067","2076","Using very high resolution remote sensing images to extracting urban features from very high resolution remote sensing images is a very complex and difficult task. The improvement in geospatial technologies brought forward many solutions that can help in improving the process of urban feature extraction. Data collection using light detection and ranging (LiDAR) and capturing very high resolution optical images concurrently is one of these solutions. This research proves that the fusion of high-resolution optical image with LiDAR data can improve image processing results. It is based on increasing urban features extraction success rate by reducing oversegmentation. The fusion process relies first on wavelet transform techniques, which are run several times with different parameters (rules). Then, an innovative technique is implemented to improve fusion process. The two techniques are compared, and both have reduced fragmented segments and created homogeneous urban features. However, the fused image with the innovative technique has improved the accuracy of the segmentation results. The average accuracy for building detection is 96% (maximum 100% and minimum 92%) using the innovative technique compared to 21% and 51% for no fusion and wavelet-fusion-based techniques. Furthermore, an index is used to measure the quality of the building details which are detected after using the innovative fusion technique. The result indicates that the quality index is greater or equal to 86%.","2151-1535","","10.1109/JSTARS.2017.2653061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843589","Three-dimensional (3-D) visualization;feature extraction;fusion;quality assurance;remote sensing;segmentation;urban","Laser radar;Optical imaging;Optical sensors;Adaptive optics;Image segmentation;Feature extraction;Remote sensing","buildings (structures);feature extraction;geophysical image processing;image fusion;image segmentation;land use;optical radar;remote sensing by laser beam;wavelet transforms","robust segmentation;very-high-resolution optical image;LiDAR data;geospatial technology;urban feature extraction;light detection and ranging;high-resolution optical image fusion;image processing;wavelet transform;homogeneous urban;building detection;innovative fusion technique","","13","","40","IEEE","6 Feb 2017","","","IEEE","IEEE Journals"
"SANet: A Sea–Land Segmentation Network Via Adaptive Multiscale Feature Learning","B. Cui; W. Jing; L. Huang; Z. Li; Y. Lu","College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","116","126","Sea-land segmentation of remote sensing images is of great significance to the dynamic monitoring of coastlines. However, the types of objects in the coastal zone are complex, and their spectra, textures, shapes, and distribution features are different. Therefore, sea-land segmentation for various types of coastlines is still a challenging task. In this article, a scale-adaptive semantic segmentation network, called SANet, is proposed for sea-land segmentation of remote sensing images. SANet has made two innovations on the basis of the classic encoder-decoder structure. First, to integrate the spectral, textural, and semantic features of ground objects at different scales, we designed an adaptive multiscale feature learning module (AML) to replace the conventional serial convolution operation. The AML module mainly contains a multiscale feature extraction unit and an adaptive feature fusion unit. The former can capture the multiscale detailed information and contextual semantic information of objects from an early stage, while the latter can adaptively fuse feature maps of different scales. Second, we adopted the squeeze-and-excitation module to bridge the corresponding layers of the codec so that SANet can selectively emphasize the features of the weak sea-land boundaries. Experiments on a set of Gaofen-1 remote sensing images demonstrated that SANet achieved more accurate segmentation results and obtained sharper boundaries than other methods for various natural and artificial coastlines.","2151-1535","","10.1109/JSTARS.2020.3040176","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFC1405600); National Natural Science Foundation of China(grant numbers:41 406 200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9269403","Adaptive learning;atrous convolution;remote sensing image;residual block;sea–land segmentation;squeeze-excitation module","Image segmentation;Feature extraction;Semantics;Convolution;Task analysis;Remote sensing;Adaptation models","convolutional neural nets;feature extraction;geophysical image processing;image fusion;image segmentation;image texture;learning (artificial intelligence);oceanographic techniques;remote sensing","SANet;scale-adaptive semantic segmentation network;spectral features;textural features;semantic features;multiscale feature extraction;adaptive feature fusion;feature maps;sea-land boundaries;Gaofen-1 remote sensing images;sea-land segmentation network;adaptive multiscale feature learning;dynamic coastline monitoring;coastal zone;encoder-decoder structure;serial convolution operation;squeeze-and-excitation module;natural coastlines;artificial coastlines","","12","","42","CCBY","24 Nov 2020","","","IEEE","IEEE Journals"
"Spatiotemporal Fusion of Land Surface Temperature Based on a Convolutional Neural Network","Z. Yin; P. Wu; G. M. Foody; Y. Wu; Z. Liu; Y. Du; F. Ling","Anhui Province Key Laboratory of Wetland Ecosystem Protection and Restoration, Anhui University, Hefei, China; Institute of Physical Science and Information Technology, Anhui University, Hefei, China; School of Geography, University of Nottingham, University Park, Nottingham, U.K; Institute of Physical Science and Information Technology, Anhui University, Hefei, China; Jiangsu Provincial Key Laboratory of Geographic Information Science and Technology, International Institute for Earth System Science, Nanjing University, Nanjing, China; Key Laboratory for Environment and Disaster Monitoring and Evaluation, Hubei, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key Laboratory for Environment and Disaster Monitoring and Evaluation, Hubei, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","20 Jan 2021","2021","59","2","1808","1822","Due to the tradeoff between spatial and temporal resolutions commonly encountered in remote sensing, no single satellite sensor can provide fine spatial resolution land surface temperature (LST) products with frequent coverage. This situation greatly limits applications that require LST data with fine spatiotemporal resolution. Here, a deep learning-based spatiotemporal temperature fusion network (STTFN) method for the generation of fine spatiotemporal resolution LST products is proposed. In STTFN, a multiscale fusion convolutional neural network is employed to build the complex nonlinear relationship between input and output LSTs. Thus, unlike other LST spatiotemporal fusion approaches, STTFN is able to form the potentially complicated relationships through the use of training data without manually designed mathematical rules making it is more flexible and intelligent than other methods. In addition, two target fine spatial resolution LST images are predicted and then integrated by a spatiotemporal-consistency (STC)-weighting function to take advantage of STC of LST data. A set of analyses using two real LST data sets obtained from Landsat and moderate resolution imaging spectroradiometer (MODIS) were undertaken to evaluate the ability of STTFN to generate fine spatiotemporal resolution LST products. The results show that, compared with three classic fusion methods [the enhanced spatial and temporal adaptive reflectance fusion model (ESTARFM), the spatiotemporal integrated temperature fusion model (STITFM), and the two-stream convolutional neural network for spatiotemporal image fusion (StfNet)], the proposed network produced the most accurate outputs [average root mean square error (RMSE) <; 1.40 °C and average structural similarity (SSIM) > 0.971].","1558-0644","","10.1109/TGRS.2020.2999943","National Natural Science Foundation of China(grant numbers:41501376); Strategic Priority Research Program of Chinese Academy of Sciences(grant numbers:XDA 2003030201); Innovation Group Project of Hubei Natural Science Foundation(grant numbers:2019CFA019); open fund for Discipline Construction, Institute of Physical Science, and Information Technology at Anhui University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115890","Complex nonlinear relationship;deep learning;land surface temperature;spatiotemporal fusion;spatiotemporal-consistency (STC)-weighting","Land surface temperature;Spatiotemporal phenomena;Spatial resolution;Remote sensing;Earth;Artificial satellites;MODIS","atmospheric techniques;geophysical image processing;geophysical techniques;image fusion;image resolution;land surface temperature;learning (artificial intelligence);neural nets;remote sensing;sensor fusion;spatiotemporal phenomena","STTFN;fine spatiotemporal resolution LST products;classic fusion methods [the enhanced spatial;temporal adaptive reflectance fusion model;spatiotemporal integrated temperature fusion model;two-stream convolutional neural network;spatiotemporal image fusion];spatial resolutions;temporal resolutions;fine spatial resolution land surface temperature products;LST data;deep learning-based spatiotemporal temperature fusion network method;multiscale fusion convolutional neural network;LST spatiotemporal fusion approaches;target fine spatial resolution LST images;spatiotemporal-consistency-weighting function;moderate resolution imaging spectroradiometer;temperature 1.4 degC","","40","","53","IEEE","12 Jun 2020","","","IEEE","IEEE Journals"
"A New Cross-Fusion Method to Automatically Determine the Optimal Input Image Pairs for NDVI Spatiotemporal Data Fusion","Y. Chen; R. Cao; J. Chen; X. Zhu; J. Zhou; G. Wang; M. Shen; X. Chen; W. Yang","School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; State Key Laboratory of Earth Surface Processes and Resource Ecology, Faculty of Geographical Science, Institute of Remote Sensing Science and Engineering, Beijing Normal University, Beijing, China; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; School of Resources and Environment, University of Electronic Science and Technology of China, Chengdu, China; Key Laboratory of Alpine Ecology, CAS Center for Excellence in Tibetan Plateau Earth Sciences, Institute of Tibetan Plateau Research, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Alpine Ecology, CAS Center for Excellence in Tibetan Plateau Earth Sciences, Institute of Tibetan Plateau Research, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology, Faculty of Geographical Science, Institute of Remote Sensing Science and Engineering, Beijing Normal University, Beijing, China; Center for Environmental Remote Sensing, Chiba University, Chiba, Japan","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2020","2020","58","7","5179","5194","Spatiotemporal data fusion is a methodology to generate images with both high spatial and temporal resolution. Most spatiotemporal data fusion methods generate the fused image at a prediction date based on pairs of input images from other dates. The performance of spatiotemporal data fusion is greatly affected by the selection of the input image pair. There are two criteria for selecting the input image pair: the “similarity” criterion, in which the image at the base date should be as similar as possible to that at the prediction date, and the “consistency” criterion, in which the coarse and fine images at the base date should be consistent in terms of their radiometric characteristics and imaging geometry. Unfortunately, the “consistency” criterion has not been quantitatively considered by previous selection strategies. We thus develop a novel method (called “cross-fusion”) to address the issue of the determination of the base image pair. The new method first chooses several candidate input image pairs according to the “similarity” criterion and then takes the “consistency” criterion into account by employing all of the candidate input image pairs to implement spatiotemporal data fusion between them. We applied the new method to MODIS-Landsat Normalized Difference Vegetation Index (NDVI) data fusion. The results show that the cross-fusion method performs better than four other selection strategies, with lower average absolute difference (AAD) values and higher correlation coefficients in various vegetated regions including a deciduous forest in Northeast China, an evergreen forest in South China, cropland in North China Plain, and grassland in the Tibetan Plateau. We simulated scenarios for the inconsistency between MODIS and Landsat data and found that the simulated inconsistency is successfully quantified by the new method. In addition, the cross-fusion method is less affected by cloud omission errors. The fused NDVI time-series data generated by the new method tracked various vegetation growth trajectories better than previous selection strategies. We expect that the cross-fusion method can advance practical applications of spatiotemporal data fusion technology.","1558-0644","","10.1109/TGRS.2020.2973762","“Strategic Priority Research Program” of the Chinese Academy of Sciences(grant numbers:XDA19070304); National Natural Science Foundation of China(grant numbers:41601381); Second Tibetan Plateau Scientific Expedition and Research Program(grant numbers:2019QZKK0106,2019QZKK0307); Top-Notch Young Talents Program of China (to Shen); CEReS Overseas Joint Research Program 2018(grant numbers:CI18-101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9013055","Landsat normalized difference vegetation index (NDVI);MODIS–Landsat;NDVI time series;spatiotemporal fusion;VIIRS NDVI","Earth;Artificial satellites;Remote sensing;MODIS;Spatiotemporal phenomena;Data integration;Spatial resolution","geophysical image processing;image fusion;image resolution;spatiotemporal phenomena;terrain mapping;time series;vegetation;vegetation mapping","spatiotemporal data fusion technology;NDVI time-series data;candidate input image pairs;base image pair;previous selection strategies;imaging geometry;radiometric characteristics;fine images;coarse images;consistency criterion;base date;similarity criterion;input image pair;prediction date;fused image;spatiotemporal data fusion methods;NDVI spatiotemporal data fusion;optimal input image pairs;new cross-fusion method","","19","","42","IEEE","26 Feb 2020","","","IEEE","IEEE Journals"
"Intrinsic Image Decomposition for Feature Extraction of Hyperspectral Images","X. Kang; S. Li; L. Fang; J. A. Benediktsson","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","9 Oct 2014","2015","53","4","2241","2253","In this paper, a novel feature extraction method based on intrinsic image decomposition (IID) is proposed for hyperspectral image classification. The proposed method consists of the following steps. First, the spectral dimension of the hyperspectral image is reduced with averaging-based image fusion. Then, the dimension reduced image is partitioned into several subsets of adjacent bands. Next, the reflectance and shading components of each subset are estimated with an optimization-based IID technique. Finally, pixel-wise classification is performed only on the reflectance components, which reflect the material-dependent properties of different objects. Experimental results show that, with the proposed feature extraction method, the support vector machine classifier is able to obtain much higher classification accuracy even when the number of training samples is quite small. This demonstrates that IID is indeed an effective way for feature extraction of hyperspectral images.","1558-0644","","10.1109/TGRS.2014.2358615","National Natural Science Foundation for Distinguished Young Scholars of China(grant numbers:61325007); National Natural Science Foundation of China(grant numbers:61172161); Fundamental Research Funds for the Central Universities, Hunan University; Chinese Scholarship Award for Excellent Doctoral Student; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915746","Feature extraction;hyperspectral image;image fusion;intrinsic image decomposition (IID);support vector machines (SVMs);Feature extraction;hyperspectral image;image fusion;intrinsic image decomposition (IID);support vector machines (SVMs)","Hyperspectral imaging;Feature extraction;Accuracy;Educational institutions;Training;Materials","feature extraction;hyperspectral imaging;image classification;image fusion;optimisation;reflectivity;support vector machines","intrinsic image decomposition;feature extraction method;hyperspectral image classification;spectral dimension;image fusion;reflectance component;shading component;adjacent band subset estimation;image partition;optimization-based IID technique;pixel-wise classification;material-dependent property;support vector machine classifier","","133","","61","IEEE","2 Oct 2014","","","IEEE","IEEE Journals"
"Fine-Grained Classification of Urban Functional Zones and Landscape Pattern Analysis Using Hyperspectral Satellite Imagery: A Case Study of Wuhan","J. Yuan; S. Wang; C. Wu; Y. Xu","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Institute of Advanced Research in Artificial Intelligence, Vienna, Austria","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 May 2022","2022","15","","3972","3991","Landscape mapping and pattern analysis of fine-grained urban functional zones (UFZ) are of great research value in the field of urban planning and urban development assessment. Previous studies employing multispectral images have largely enabled only coarse-grained land-use classification of urban areas. By contrast, hyperspectral images can provide very detailed spectral features and have shown some potential to facilitate fine-grained classification of urban areas. In this article, we evaluate and analyze the classification of fine-grained UFZ in the central city of Wuhan, Hubei, China, using GaoFen-5 (GF-5) hyperspectral satellite imagery. We first compare the performance of hyperspectral data (GF-5) and classical multispectral data (Landsat 8) for the classification of fine-grained functional zones of cities by employing two classical classification algorithms and two deep learning methods. We also propose the deep learning-based spectral-spatial unified networks combined with a fully connected conditional random field (SSUN-CRF) algorithm for fine-grained UFZ mapping to enable better landscape pattern analysis. We then analyze the landscape pattern of the main urban areas of Wuhan by combining ten landscape indicators based on the precise UFZ classification results. Experimental results illustrate the followings. First, compared to multispectral images, hyperspectral images can allow for a more accurate UFZ classification. Second, deep learning classification algorithms can better exploit hyperspectral image data, with the SSUN-CRF algorithm, in particular, being able to achieve an overall accuracy of 93.86% and a Kappa coefficient of 92.08%. Finally, the landscape pattern analysis demonstrates that hyperspectral remote sensing imagery shows significant potential in mapping fine-grained UFZ. It is beneficial for further urban study and planning.","2151-1535","","10.1109/JSTARS.2022.3174412","National Natural Science Foundation of China(grant numbers:T2122014,61971317,41801285); Natural Science Foundation of Hubei Province(grant numbers:2020CFB594); Science and Technology Major Project of Hubei Province(grant numbers:2019AEA170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9773336","Deep learning;GaoFen-5;hyperspectral images;spectral-spatial unified network combined with a fully connected conditional random field (SSUN-CRF);urban functional zone (UFZ) mapping","Urban areas;Hyperspectral imaging;Satellites;Spatial resolution;Earth;Classification algorithms;Rivers","geophysical image processing;image classification;image fusion;land use planning;learning (artificial intelligence);remote sensing;terrain mapping;town and country planning","fine-grained classification;landscape pattern analysis;hyperspectral satellite imagery;Wuhan;fine-grained urban functional zones;urban planning;urban development assessment;multispectral images;coarse-grained land-use;hyperspectral images;hyperspectral data;classical classification algorithms;deep learning-based spectral-spatial unified networks;fully connected conditional random field;fine-grained UFZ mapping;main urban areas;landscape indicators;precise UFZ classification results;accurate UFZ classification;deep learning classification algorithms;hyperspectral image data;hyperspectral remote sensing imagery;mapping fine-grained UFZ;urban study","","3","","46","CCBY","12 May 2022","","","IEEE","IEEE Journals"
"Spectral Mapping Based on Panchromatic Block Structure Analysis","X. Luo; L. Zhou; X. Shi; H. Wan; J. Yin","School of Astronautics, Beihang University, Beijing, China; School of Astronautics, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Astronautics, Beihang University, Beijing, China","IEEE Access","13 Aug 2018","2018","6","","40354","40364","Fusing panchromatic (PAN) and multispectral (MS) images, i.e., pansharpening, can obtain a high-resolution MS (HRMS) image. In this paper, we propose a spectral mapping framework for pansharpening problem based on PAN block structure analysis (PBSA). The PBSA employs boundary, inner uniqueness, and neighborhood comparisons as block structure characteristics to classify the PAN image blocks into pureness composition and mixture composition. For the PAN blocks with pureness composition, they can directly copy the spectral information of corresponding MS pixels. For the mixture PAN blocks, assuming that they can linearly represented by some pure PAN blocks, their spectral signals can get via a weighted average from the relative pure PAN blocks. The experiments on real PAN and MS pairs show that the proposed pansharpening method not only conforms to the structure of the PAN image but also preserves the spectral information of the MS image. The final fused HRMS image shows good performance in visual effect and objective assessment.","2169-3536","","10.1109/ACCESS.2018.2853150","National Key Research and Development Program of China(grant numbers:2018YFB0505100); National Natural Science Foundation of China(grant numbers:41471278); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404032","Remote sensing;image fusion;pansharpening","Spatial resolution;Transforms;Distortion;Principal component analysis;Image fusion;Multiresolution analysis","geophysical image processing;image fusion;image resolution;remote sensing;spectral analysis","spectral mapping framework;pansharpening problem;PAN block structure analysis;PBSA;PAN image blocks;pureness composition;mixture composition;spectral information;spectral signals;panchromatic block structure analysis;high resolution MS image;multispectral images;MS pixels;HRMS image fusion;visual effect","","1","","43","OAPA","5 Jul 2018","","","IEEE","IEEE Journals"
"Coupled Convolutional Neural Network With Adaptive Response Function Learning for Unsupervised Hyperspectral Super Resolution","K. Zheng; L. Gao; W. Liao; D. Hong; B. Zhang; X. Cui; J. Chanussot","College of Geoscience and Surveying Engineering, China University of Mining and Technology, Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Image Processing and Interpretation, IMEC Research Group, Ghent University, Ghent, Belgium; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Weßling, Germany; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; College of Geoscience and Surveying Engineering, China University of Mining and Technology, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","24 Feb 2021","2021","59","3","2487","2502","Due to the limitations of hyperspectral imaging systems, hyperspectral imagery (HSI) often suffers from poor spatial resolution, thus hampering many applications of the imagery. Hyperspectral super resolution refers to fusing HSI and MSI to generate an image with both high spatial and high spectral resolutions. Recently, several new methods have been proposed to solve this fusion problem, and most of these methods assume that the prior information of the point spread function (PSF) and spectral response function (SRF) are known. However, in practice, this information is often limited or unavailable. In this work, an unsupervised deep learning-based fusion method-HyCoNet-that can solve the problems in HSI-MSI fusion without the prior PSF and SRF information is proposed. HyCoNet consists of three coupled autoencoder nets in which the HSI and MSI are unmixed into endmembers and abundances based on the linear unmixing model. Two special convolutional layers are designed to act as a bridge that coordinates with the three autoencoder nets, and the PSF and SRF parameters are learned adaptively in the two convolution layers during the training process. Furthermore, driven by the joint loss function, the proposed method is straightforward and easily implemented in an end-to-end training manner. The experiments performed in the study demonstrate that the proposed method performs well and produces robust results for different data sets and arbitrary PSFs and SRFs.","1558-0644","","10.1109/TGRS.2020.3006534","National Natural Science Foundation of China(grant numbers:41722108,91638201); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141341","Adaptive learning;autoencoder;coupled convolutional neural network;hyperspectral image;super-resolution","Adaptive learning;Hyperspectral imaging;Convolutional neural networks;Image classification;Image fusion;Unsupervised learning;Superresolution","geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;image sensors;learning (artificial intelligence);neural nets;optical transfer function;remote sensing;sensor fusion;unsupervised learning","hyperspectral imagery;poor spatial resolution;high spectral resolutions;fusion problem;spectral response function;SRF;unsupervised deep learning-based fusion method-HyCoNet-that;HSI-MSI fusion;prior PSF;coupled autoencoder nets;linear unmixing model;special convolutional layers;convolution layers;joint loss function;convolutional neural network;adaptive response function learning;unsupervised hyperspectral super resolution;hyperspectral imaging systems","","56","","65","IEEE","15 Jul 2020","","","IEEE","IEEE Journals"
"Large-Scale Semantic 3-D Reconstruction: Outcome of the 2019 IEEE GRSS Data Fusion Contest—Part A","S. Kunwar; H. Chen; M. Lin; H. Zhang; P. D’Angelo; D. Cerra; S. M. Azimi; M. Brown; G. Hager; N. Yokoya; R. Hänsch; B. Le Saux","NestAI, Kathmandu, Nepal; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; German Aerospace Center, Weßling, Germany; German Aerospace Center, Weßling, Germany; German Aerospace Center, Weßling, Germany; Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA; Johns Hopkins University, Baltimore, MD, USA; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; German Aerospace Center, Weßling, Germany; Φ-Lab, ESA Centre for Earth Observation, Frascati, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","922","935","In this article, we present the scientific outcomes of the 2019 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2019 Contest addressed the problem of 3-D reconstruction and 3-D semantic understanding on a large scale. Several competitions were organized to assess specific issues, such as elevation estimation and semantic mapping from a single view, two views, or multiple views. In Part A, we report the results of the best-performing approaches for semantic 3-D reconstruction according to these various setups, whereas 3-D point cloud semantic mapping is discussed in Part B.","2151-1535","","10.1109/JSTARS.2020.3032221","Intelligence Advanced Research Projects Activity(grant numbers:2017-17032700004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9229514","Classification;convolutional neural network (CNN);Data Fusion Contest (DFC);deep learning;elevation model;height estimation;image analysis and data fusion (IADF);light detection and ranging (LiDAR);multiview;3-D reconstruction;point cloud;semantic labeling;semantic mapping;stereo","Semantics;Three-dimensional displays;Training;Laser radar;Data integration;Satellites;Earth","geophysical image processing;hyperspectral imaging;image classification;image fusion;remote sensing;terrain mapping","3-D reconstruction;scientific outcomes;image analysis;3-D semantic understanding;3-D point cloud semantic mapping;data fusion technical committee;2019 data fusion contest;2019 IEEE GRSS data fusion contest;IEEE geoscience and remote sensing society","","14","","55","CCBY","19 Oct 2020","","","IEEE","IEEE Journals"
"Fusing Sentinel-2 and Landsat-8 Surface Reflectance Data via Pixel-Wise Local Normalization","Y. Li; Q. Shi; L. He; R. Cai; L. Meng; J. Li; A. Plaza","Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hyperspectral Computing Laboratory Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Sep 2022","2022","15","","7359","7374","Medium spatial resolution surface reflectance image series from the combination of Landsat-8 Operational Land Imager and Sentinel-2 Multispectral Imager observations have great importance to the land surface monitoring tasks, for which great efforts have been paid for blending the two data. However, most of the efforts focus on the image series with spatial resolution of 30 m, which cannot meet the data demand of some applications. Therefore, it is necessary to fuse Landsat-8 and Sentinel-2 images to provide 10-m image series. Currently, there are three means to achieve that, including the area-to-point regression kriging fusion approach (ATPRK), spatiotemporal fusion methods, and deep-learning-based fusion models. However, the ATPRK and spatiotemporal fusion methods suffer from the limited fusion performance, while the deep-learning-based fusion models are hardware dependent, i.e., requiring the graphics processing units, which may not be satisfied sometimes. To address these issues, in this article, we develop a new pixel-wise local normalization-based fusion method (LN-FM) for fusing Sentinel-2 and Landsat-8 images. The newly proposed LN-FM is compared to the ATPRK and three representative spatiotemporal fusion methods in experiments, which use imagery collected from both a rural area and an urban area. The experimental results demonstrate that the newly developed LN-FM exhibits excellent qualitative and quantitative performance, as well as remarkable spatial, spectral, and pixel distribution fidelity. Furthermore, this approach is fast, which may improve its applicability","2151-1535","","10.1109/JSTARS.2022.3200713","National Natural Science Foundation of China(grant numbers:T2225019,42030111,61976234,62071184); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515011615,2019A1515011057,202002030402); Guangzhou Science and Technology Program(grant numbers:202002030395); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864237","Landsat-8;local normalization;remote sensing image fusion;Sentinel-2","Remote sensing;Earth;Artificial satellites;Spatial resolution;Image resolution;Spatiotemporal phenomena;Task analysis","geophysical image processing;geophysical techniques;image fusion;image resolution;land cover;learning (artificial intelligence);radiometry;reflectivity;remote sensing;sensor fusion;spatiotemporal phenomena;statistical analysis;vegetation mapping","Landsat-8 surface reflectance data;medium spatial resolution surface reflectance image series;Landsat-8 Operational Land Imager;Sentinel-2 Multispectral Imager observations;land surface monitoring tasks;efforts focus;data demand;Sentinel-2 images;area-to-point regression;ATPRK;deep-learning-based fusion models;fusion performance;pixel-wise local normalization-based fusion method;Landsat-8 images;representative spatiotemporal fusion methods;newly developed LN-FM exhibits;remarkable spatial;size 30.0 m","","","","61","CCBYNCND","22 Aug 2022","","","IEEE","IEEE Journals"
"Hyperspectral and LiDAR Classification With Semisupervised Graph Fusion","J. Xia; W. Liao; P. Du","Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan; Department of Telecommunications and Information Processing, Ghent University—IMEC, Ghent, Belgium; Collaborative Innovation Center of South China Sea Studies, Nanjing University, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","25 Mar 2020","2020","17","4","666","670","To fuse hyperspectral and Light Detection And Ranging (LiDAR), we propose a semisupervised graph fusion (SSGF) approach. We apply morphological filters to LiDAR and the first few components of hyperspectral data to model the height and spatial information, respectively. Then, the proposed SSGF is used to project the spectral, elevation, and spatial features onto a lower subspace to obtain the new features. In particular, the objective of SSGF is to maximize the class separation ability and preserve the local neighborhood structure by using both labeled and unlabeled samples. Experimental results on the hyperspectral and LiDAR data from the 2013 IEEE Geoscience and Remote Sensing Society (GRSS) Data Fusion Contest demonstrated the superiority of the SSGF.","1558-0571","","10.1109/LGRS.2019.2928009","National Natural Science Foundation of China(grant numbers:41631176); Japan Society for the Promotion of Science(grant numbers:KAKENHI 19K20309); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8777100","Graph fusion;hyperspectral;Light Detection And Ranging (LiDAR);semisupervised","Feature extraction;Hyperspectral imaging;Laser radar;Training;Data mining;Distance measurement","geophysical image processing;hyperspectral imaging;image classification;image fusion;optical radar;remote sensing;remote sensing by laser beam","Light Detection And Ranging;fuse hyperspectral LiDAR;hyperspectral classification;semisupervised graph fusion;IEEE GRSS Data Fusion Contest;spatial information;hyperspectral data;morphological filters;SSGF;LiDAR classification","","15","","22","IEEE","26 Jul 2019","","","IEEE","IEEE Journals"
"An Edge Preserving Multiresolution Fusion: Use of Contourlet Transform and MRF Prior","K. P. Upla; M. V. Joshi; P. P. Gajjar","Department of Electronics and Communication, S. V. National Institute of Technology, Gujarat, India; Dhirubhai Ambani Institute of Information and Communication Technology, Gandhinagar, India; Department of Electronics and Communication, Government Polytechnic for Girls, Gujarat, India","IEEE Transactions on Geoscience and Remote Sensing","5 Feb 2015","2015","53","6","3210","3220","In this paper, we propose a new approach for multiresolution fusion using contourlet transform (CT). The method is based on modeling the low spatial resolution (LR) and high spectral resolution multispectral (MS) image as the degraded and noisy version of their high spatial resolution version. Since this is an ill-posed problem, it requires regularization in order to obtain the final solution. In this paper, we first obtain the initial estimate of the fused image from the available MS image and the panchromatic (Pan) image by using the CT domain learning. Since CT provides better directional edges, the initial estimate has better edge details. Using the initial estimate, we obtain the degradation that accounts for the aliasing between the LR MS image and fused image. Regularization is carried out by modeling the texture of the final fused image as a homogeneous Markov random field (MRF) prior, where the MRF parameter is estimated using the initial estimate. The use of MRF prior on the final fused image takes care of the spatial dependencies among the pixels. A simple gradient-based optimization technique is used to obtain the final fused image. Although we use homogeneous MRF, the proposed approach preserves the edges in the final fused image by retaining the edges from the initial estimate and by carrying out the optimization on nonedge pixels only. Therefore, the advantage of the proposed method lies in preserving the discontinuities without using the discontinuity preserving prior, thus avoiding the use of computationally taxing optimization techniques for regularization purposes. In addition, the proposed method causes minimum spectral distortion since it learns the texture using contourlet coefficients and does not use actual Pan image pixel intensities. We demonstrate the effectiveness of our approach by conducting the experiments using subsampled and nonsubsampled CT on different data sets captured using Ikonos-2, Quickbird, and Worldview-2 satellites.","1558-0644","","10.1109/TGRS.2014.2371812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998945","Canny edge detector;Contourlet transform (CT);gradient descent;image fusion;Markov random field (MRF);maximum a posteriori (MAP);Canny edge detector;Contourlet transform (CT);gradient descent;image fusion;Markov random field (MRF);maximum a posteriori (MAP)","Image edge detection;Computed tomography;Spatial resolution;Degradation;Optimization;Transforms","feature extraction;geophysical image processing;image fusion;remote sensing","Quickbird satellite;Worldview-2 satellite;Ikonos-2 satellite;Pan image pixel intensities;minimum spectral distortion;computationally taxing optimization techniques;discontinuity preserving prior;nonedge pixels;MRF parameter;homogeneous Markov random field;final fused image texture;panchromatic image;high spectral resolution multispectral image;low spatial resolution image;MRF prior;contourlet transform;edge preserving multiresolution fusion","","42","","55","IEEE","29 Dec 2014","","","IEEE","IEEE Journals"
"SC-PNN: Saliency Cascade Convolutional Neural Network for Pansharpening","L. Zhang; J. Zhang; J. Ma; X. Jia","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia; School of Information Science and Technology, Beijing Foreign Studies University, Beijing, China; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia","IEEE Transactions on Geoscience and Remote Sensing","26 Oct 2021","2021","59","11","9697","9715","In many remote sensing tasks, different types of regions or targets differ in requirements for spectral and spatial quality. The discrepancy reveals that a uniform pansharpening strategy applying to the entire image may not fulfill the varying demands of different regions appropriately. From this aspect, we resort to saliency analysis to distinguish regions with different spatial and spectral requirements and then propose a new saliency cascade convolutional neural network for pansharpening (SC-PNN). SC-PNN is composed of two parts: a dilated deformable convolutional network (DDCN) for saliency analysis and a saliency cascade residual dense network (SC-RDN) for pansharpening. DDCN is a fully convolutional network based on hybrid dilated convolution and deformable convolution, aiming to separate salient regions, such as residential areas from nonsalient areas, including mountains and vegetation areas, with well-defined boundaries and integrity. In the fusion process, SC-RDN is specially designed with the help of saliency analysis. We first construct a deep regression network to estimate a primarily sharpened image and subsequently leverage the saliency map produced by DDCN to develop a saliency enhancement module. In this module, the quality of salient and nonsalient areas is further improved by two independent deep residual dense networks. Thus, a precise fused image can be predicted. Experiments on SPOT5, GeoEye-1, and WorldView-3 data sets reveal that, compared to state-of-the-art pansharpening methods, our proposal has a superior ability to improve the spatial quality and preserve spectral information. The effectiveness of the saliency enhancement module is also validated in the experiment.","1558-0644","","10.1109/TGRS.2021.3054641","Beijing Natural Science Foundation(grant numbers:L182029); National Natural Science Foundation of China(grant numbers:61571050,41771407); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351739","Deep learning;pansharpening;remote sensing;residual dense network (RDN);saliency analysis","Pansharpening;Remote sensing;Spatial resolution;Image restoration;Convolution;Task analysis;Proposals","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image resolution;image segmentation;learning (artificial intelligence);object detection;remote sensing","SC-PNN;saliency cascade convolutional neural network;spatial quality;uniform pansharpening strategy;saliency analysis;spatial requirements;spectral requirements;dilated deformable convolutional network;DDCN;saliency cascade residual dense network;fully convolutional network;hybrid dilated convolution;deformable convolution;separate salient regions;nonsalient areas;deep regression network;saliency map;saliency enhancement module;independent deep residual dense networks;state-of-the-art pansharpening methods","","9","","78","IEEE","9 Feb 2021","","","IEEE","IEEE Journals"
"Enhanced Spatiotemporal Fusion via MODIS-Like Images","J. Li; Y. Li; R. Cai; L. He; J. Chen; A. Plaza","Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; State Key Laboratory of Earth Surface Processes and Resource Ecology, Faculty of Geographical Science, Institute of Remote Sensing Science and Engineering, Beijing Normal University, Beijing, China; Department of Technology of Computers and Communications, Escuela Politécnica, Hyperspectral Computing Laboratory, University of Extremadura, Cáceres, Spain","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","17","Spatiotemporal fusion (STF) aims at generating remote-sensing data with both high spatial and temporal resolution. In the literature, one of the most widely used strategies to accomplish this goal is to fuse high temporal resolution images collected by the Moderate Resolution Imaging Spectroradiometer (MODIS) with images with finer spatial resolution than those provided by MODIS (e.g., those collected by other satellite instruments such as Landsat or Sentinel-2). Current STF methods generally fuse an upsampled MODIS image with finer spatial resolution images. This leads to two main problems. First of all, the model uncertainty errors (resulting from the ill-posed upsampling problem) will be propagated into the fusion results, leading to spatial and spectral distortion. Furthermore, the spatial details of the upsampled MODIS image may be significantly different from those of the finer spatial resolution images, making the STF problem even more challenging. In order to tackle these issues, in this work, we develop a new linear regression-based STF strategy (LiSTF), which performs the reconstruction from a MODIS-like image (instead of from an upsampled MODIS image), thus reducing the model uncertainty errors and preserving better the spatial information. The MODIS-like images are built from the finer spatial resolution images via downsampling. Our experimental results, conducted using two publicly available datasets of Landsat–MODIS image pairs and one publicly available dataset of Sentinel–MODIS image pairs, reveal that our newly proposed LiSTF approach can significantly enhance the quantitative and qualitative performance of STF, particularly in terms of preserving the spatial information.","1558-0644","","10.1109/TGRS.2021.3106338","National Natural Science Foundation of China(grant numbers:42030111); National Key Research and Development Program of China(grant numbers:2017YFB0502900); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526862","Landsat;Moderate Resolution Imaging Spectroradiometer (MODIS);MODIS-like images;Sentinel-2;spatiotemporal fusion (STF)","MODIS;Spatial resolution;Remote sensing;Earth;Artificial satellites;Superresolution;Uncertainty","geophysical image processing;image fusion;image resolution;image sampling;regression analysis;remote sensing;spatiotemporal phenomena","high temporal resolution images;Moderate Resolution Imaging Spectroradiometer;upsampled MODIS image;finer spatial resolution images;MODIS-like image;Landsat-MODIS image pairs;Sentinel-MODIS image pairs;enhanced spatiotemporal fusion","","2","","33","IEEE","1 Sep 2021","","","IEEE","IEEE Journals"
"Geographically Weighted Spatial Unmixing for Spatiotemporal Fusion","K. Peng; Q. Wang; Y. Tang; X. Tong; P. M. Atkinson","College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; Department of Geography and Environment, University of Southampton, Southampton, U.K","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","17","Spatiotemporal fusion is a technique applied to create images with both fine spatial and temporal resolutions by blending images with different spatial and temporal resolutions. Spatial unmixing (SU) is a widely used approach for spatiotemporal fusion, which requires only the minimum number of input images. However, ignorance of spatial variation in land cover between pixels is a common issue in existing SU methods. For example, all coarse neighbors in a local window are treated equally in the unmixing model, which is inappropriate. Moreover, the determination of the appropriate number of clusters in the known fine spatial resolution image remains a challenge. In this article, a geographically weighted SU (SU-GW) method was proposed to address the spatial variation in land cover and increase the accuracy of spatiotemporal fusion. SU-GW is a general model suitable for any SU method. Specifically, the existing regularized version and soft classification-based version were extended with the proposed geographically weighted scheme, producing 24 versions (i.e., 12 existing versions were extended to 12 corresponding geographically weighted versions) for SU. Furthermore, the cluster validity index of Xie and Beni (XB) was introduced to determine automatically the number of clusters. A systematic comparison between the experimental results of the 24 versions indicated that SU-GW was effective in increasing the prediction accuracy. Importantly, all 12 existing methods were enhanced by integrating the SU-GW scheme. Moreover, the identified most accurate SU-GW enhanced version was demonstrated to outperform two prevailing spatiotemporal fusion approaches in a benchmark comparison. Therefore, it can be concluded that SU-GW provides a general solution for enhancing spatiotemporal fusion, which can be used to update existing methods and future potential versions.","1558-0644","","10.1109/TGRS.2021.3115136","National Natural Science Foundation of China(grant numbers:42171345,41971297); Fundamental Research Funds for the Central Universities(grant numbers:22120210495); Tongji University(grant numbers:02502350047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560133","Geographical weighting (GW);image fusion;spatial unmixing (SU);spatiotemporal fusion","Spatiotemporal phenomena;Spatial resolution;Remote sensing;Earth;Artificial satellites;Uncertainty;Data integration","cartography;feature extraction;geophysical image processing;image classification;image fusion;image resolution;land cover;pattern clustering;remote sensing;spatiotemporal phenomena","spatial unmixing;temporal resolutions;land cover;unmixing model;fine spatial resolution image;soft classification;geographically weighted scheme;SU-GW scheme;spatiotemporal fusion;cluster validity index","","1","","62","IEEE","6 Oct 2021","","","IEEE","IEEE Journals"
"Multisensor Earth Observation Image Classification Based on a Multimodal Latent Dirichlet Allocation Model","R. Bahmanyar; D. Espinoza-Molina; M. Datcu","German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany","IEEE Geoscience and Remote Sensing Letters","23 Feb 2018","2018","15","3","459","463","Many previous researches have already shown the advantages of multisensor land-cover classification. Here, we propose an innovative land-cover classification approach based on learning a joint latent model of synthetic aperture radar (SAR) and multispectral satellite images using multimodal latent Dirichlet allocation (mmLDA), a probabilistic generative model. It has already been successfully applied to various other problems dealing with multimodal data. For our experiments, we chose overlapping SAR and multispectral images of two regions of interest. The images were tiled into patches and their local primitive features were extracted. Then each image patch is represented by SAR and multispectral bag-of-words (BoW) models. The BoW values are both fed to the mmLDA, resulting in a joint latent data model. A qualitative and quantitative validation of the topics based on ground-truth data demonstrate that the land-cover categories of the regions are correctly classified, outperforming the topics obtained using individual single modality data.","1558-0571","","10.1109/LGRS.2018.2794511","Deutscher Akademischer Austauschdienst; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8278834","Image fusion;land-cover classification;multimodal latent Dirichlet allocation (mmLDA);multispectral images;synthetic aperture radar (SAR) images","Synthetic aperture radar;Visualization;Feature extraction;Data models;Dictionaries;Optical sensors","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);sensor fusion;synthetic aperture radar;terrain mapping","innovative land-cover classification approach;joint latent model;SAR;multispectral satellite images;mmLDA;probabilistic generative model;multimodal data;multispectral images;local primitive features;image patch;bag-of-words models;joint latent data model;ground-truth data;land-cover categories;multisensor earth observation image classification;multimodal latent dirichlet allocation model;multisensor land-cover classification","","20","","16","IEEE","1 Feb 2018","","","IEEE","IEEE Journals"
"A Robust Pansharpening Algorithm Based on Convolutional Sparse Coding for Spatial Enhancement","R. Gogineni; A. Chaturvedi","Department of Electronics and Communication Engineering, National Institute of Technology Karnataka, Surathkal, India; Department of Electronics and Communication Engineering, National Institute of Technology Karnataka, Surathkal, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Nov 2019","2019","12","10","4024","4037","Pansharpening (PS) is a prominent remote sensing image fusion technique. It yields high-resolution multispectral (HRMS) images, which are imperative for the applications, such as recognition and detection. The PS methods based on conventional sparse representation induce blurring effects and are unable to preserve the essential spatial details in the fused outcome. In this article, to overcome these drawbacks, a robust fusion scheme is proposed based on convolutional sparse coding (CSC). The source images are decomposed into its constituent texture and cartoon components. The sparse coefficient maps are acquired from texture components by adapting CSC. Texture components are fused using activity level measurement, whereas averaging mechanism is used to fuse the cartoon components. The HRMS image is reconstructed by combining the fused components in proportion to the gradient information. Impact of number of filters on quality metrics estimation is analyzed. Comprehensive experiments are performed on the images acquired from distinct sensors. The proposed method is evaluated in terms of visual analysis and the quantitative metrics with reduced-scale and full-scale experiments. Extensive evaluations manifest the capability of the proposed method of maintaining the balanced tradeoff and retaining the desired spatial and spectral details.","2151-1535","","10.1109/JSTARS.2019.2945815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8889992","Cartoon plus texture (CPT) decomposition;convolutional sparse coding (CSC);fusion models;gradient strength;pansharpening (PS);sparse representation (SR)","Dictionaries;Convolutional codes;Image reconstruction;Distortion;Image coding;Remote sensing;Pansharpening","computer animation;convolutional codes;image coding;image enhancement;image filtering;image fusion;image recognition;image representation;image resolution;image texture;remote sensing","robust pansharpening algorithm;convolutional sparse coding;spatial enhancement;high-resolution multispectral images;PS methods;sparse representation;blurring effects;robust fusion scheme;CSC;source images;constituent texture;cartoon components;sparse coefficient maps;texture components;activity level measurement;HRMS image;fused components;remote sensing image fusion technique","","8","","49","IEEE","1 Nov 2019","","","IEEE","IEEE Journals"
"Object Tracking in Satellite Videos by Fusing the Kernel Correlation Filter and the Three-Frame-Difference Algorithm","B. Du; Y. Sun; S. Cai; C. Wu; Q. Du","International School of Software; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; International School of Software, Wuhan University, Wuhan, China; Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA","IEEE Geoscience and Remote Sensing Letters","23 Jan 2018","2018","15","2","168","172","Object tracking is a popular topic in the field of computer vision. The detailed spatial information provided by a very high resolution remote sensing sensor makes it possible to track targets of interest in satellite videos. In recent years, correlation filters have yielded promising results. However, in terms of dealing with object tracking in satellite videos, the kernel correlation filter (KCF) tracker achieves poor results due to the fact that the size of each target is too small compared with the entire image, and the target and the background are very similar. Therefore, in this letter, we propose a new object tracking method for satellite videos by fusing the KCF tracker and a three-frame-difference algorithm. A specific strategy is proposed herein for taking advantage of the KCF tracker and the three-frame-difference algorithm to build a strong tracker. We evaluate the proposed method in three satellite videos and show its superiority to other state-of-the-art tracking methods.","1558-0571","","10.1109/LGRS.2017.2776899","National Natural Science Foundation of China(grant numbers:61601333,61471274); China Postdoctoral Science Foundation(grant numbers:2016T90733); Natural Science Foundation of Hubei Province of China(grant numbers:2016CFB245); Open Research Fund of Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences(grant numbers:2015LDE001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8225723","Data fusion;kernel correlation filter (KCF);object tracking;satellite video;three-frame difference","Target tracking;Satellites;Videos;Object tracking;Kernel;Correlation;Remote sensing","computer vision;feature extraction;geophysical image processing;image filtering;image fusion;image resolution;image sensors;object tracking;remote sensing;target tracking;video signal processing","satellite videos;KCF tracker fusion;target tracking;spatial information;computer vision;kernel correlation filter tracker fusion;three-frame-difference algorithm object tracking method;correlation filters;high resolution remote sensing sensor","","62","","41","IEEE","18 Dec 2017","","","IEEE","IEEE Journals"
"FusionNDVI: A Computational Fusion Approach for High-Resolution Normalized Difference Vegetation Index","X. Tian; M. Zhang; C. Yang; J. Ma","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; College of Computer and Information Sciences, Fujian Agriculture and Forestry University, Fuzhou, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","5258","5271","Normalized difference vegetation index (NDVI), derived from the near-infrared and red bands of a multispectral (MS) image, has been widely used in remote sensing. To obtain a high-resolution (HR) NDVI, existing attempts typically first generate an HR-MS image using pansharpening and then calculate the HR NDVI accordingly. However, some inaccurate spatial information will be simultaneously introduced into NDVIs, influencing their spatial quality seriously. To overcome this challenge, we investigate a computational fusion approach from a novel perspective for HR NDVI in this study. Rather than pansharpening an HR-MS image, we define an HR vegetation index calculated based on an available HR panchromatic image and an estimated HR red band (VIPR) and fuse the low-resolution (LR) NDVI and HR VIPR directly to acquire an HR NDVI. In particular, we adopt a nonlocal gradient sparsity constraint to force a similar nonlocal spatial structure in the fused NDVI and VIPR, where the VIPR is dynamically updated by adding a constraint to reconstruct the HR red band. We further integrate a data fidelity term to constrain the relationship between the fused NDVI and its LR version, and an efficient strategy based on the alternative direction multiplier method is developed to solve the nonconvex optimization problem. The extensive experimental results demonstrate that the proposed method achieves superior fusion performance over the state of the art, exhibiting its wide application aspect in remote sensing.","1558-0644","","10.1109/TGRS.2020.3014698","National Natural Science Foundation of China(grant numbers:61971315,61773295); Natural Science Foundation of Hubei Province(grant numbers:2018CFB435,2019CFA037); Fundamental Research Funds for the Central Universities(grant numbers:2042018kf1009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9178768","Fusion;green normalized difference vegetation index (GNDVI);multispectral (MS) image;normalized difference vegetation index (NDVI);nonlocal gradient sparsity","Vegetation mapping;Indexes;Optimization;Spatial resolution;Distortion;Remote sensing;Image edge detection","concave programming;estimation theory;geophysical image processing;image enhancement;image fusion;image reconstruction;image resolution;remote sensing;vegetation mapping","computational fusion;normalized difference vegetation index;multispectral image;remote sensing;high-resolution NDVI;HR-MS image;HR NDVI;spatial quality;HR vegetation index;HR panchromatic image;VIPR;nonlocal gradient sparsity constraint;HR red band;alternative direction multiplier method;nonconvex optimization","","6","","50","IEEE","27 Aug 2020","","","IEEE","IEEE Journals"
"CIMFNet: Cross-Layer Interaction and Multiscale Fusion Network for Semantic Segmentation of High-Resolution Remote Sensing Images","W. Zhou; J. Jin; J. Lei; L. Yu","School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; School of Information and Electronic Engineering, Zhejiang University of Science and Technology, Hangzhou, China; Institute of Information and Communication Engineering, Zhejiang University, Hangzhou, China","IEEE Journal of Selected Topics in Signal Processing","14 Jul 2022","2022","16","4","666","676","Semantic segmentation of remote sensing images has received increasing attention in recent years; however, using a single imaging modality limits the segmentation performance. Thus, digital surface models have been integrated into semantic segmentation to improve performance. Nevertheless, existing methods based on neural networks simply combine data from the two modalities, mostly neglecting the similarities and differences between multimodal features. Consequently, the complementarity between multimodal features cannot be exploited, and excess noise is introduced during feature processing. To solve these problems, we propose a multimodal fusion module to explore the similarities and differences between features from the two information modalities for adequate fusion. In addition, although downsampling operations such as pooling and striding can improve the feature representativeness, they discard spatial details and often lead to segmentation errors. Thus, we introduce hierarchical feature interactions to mitigate the adverse effects of downsampling and introduce a two-way interactive pyramid pooling module to extract multiscale context features for guiding feature fusion. Extensive experiments performed on two benchmark datasets show that the proposed network integrating our novel modules substantially outperforms state-of-the-art semantic segmentation methods. The code and results can be found at https://github.com/NIT-JJH/CIMFNet.","1941-0484","","10.1109/JSTSP.2022.3159032","National Natural Science Foundation of China(grant numbers:61502429,62071427); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9735276","Cross-layer interaction;cross-modal fusion;high-resolution remote sensing;semantic segmentation","Semantics;Image segmentation;Feature extraction;Logic gates;Fuses;Data mining;Convolution","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;remote sensing","semantic segmentation methods;cross-layer interaction;multiscale fusion network;high-resolution remote sensing images;remote sensing images;single imaging modality;segmentation performance;digital surface models;neural networks;multimodal features;feature processing;information modalities;feature representativeness;segmentation errors;hierarchical feature interactions;two-way interactive pyramid pooling module;multiscale context features;CIMFNet","","19","","61","IEEE","15 Mar 2022","","","IEEE","IEEE Journals"
"Batch Methods for Resolution Enhancement of TIR Image Sequences","P. Addesso; M. Longo; A. Maltese; R. Restaino; G. Vivone","Department of Information Engineering, Electrical Engineering, and Applied Mathamatics. (DIEM), University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering, and Applied Mathamatics. (DIEM), University of Salerno, Fisciano, Italy; Dipartimento di Ingegneria Civile Ambientale, Aerospaziale e dei Materiali, Università degli Studi di Palermo, Palermo, Italy; Department of Information Engineering, Electrical Engineering, and Applied Mathamatics. (DIEM), University of Salerno, Fisciano, Italy; NATO STO Centre for Maritime Research and Experimentation, La Spezia, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","7","3372","3385","Thermal infrared (TIR) time series are exploited by many methods based on Earth observation (EO), for such applications as agriculture, forest management, and meteorology. However, due to physical limitations, data acquired by a single sensor are often unsatisfactory in terms of spatial or temporal resolution. This issue can be tackled by using remotely sensed data acquired by multiple sensors with complementary features. When nonreal-time functioning or at least near real-time functioning is admitted, the measurements can be profitably fed to a sequential Bayesian algorithm, which allows to account for the correlation embedded in the successive acquisitions. In this work, we focus on applications that allow the batch processing of the whole data sequences acquired in a fixed time interval. In this case, multiple options for improving the final product are offered by the Bayesian framework, based on both sequential and smoothing techniques. We consider several such Bayesian strategies and comparatively assess their performances in practical applications and through real thermal data acquired by the SEVIRI and MODIS sensors, encompassing the presence of multiple disturbance source, e.g., the cloud coverage of the illuminated scene.","2151-1535","","10.1109/JSTARS.2015.2440333","Regione Campania(grant numbers:B35C11000090004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150322","Bayesian smoothing methods;cloud detection;image enhancement;interpolation;remote sensing;thermal images;Bayesian smoothing methods;cloud detection;image enhancement;interpolation;remote sensing;thermal images","Spatial resolution;Bayes methods;Estimation;Smoothing methods;Clouds;Earth;Remote sensing","geophysical image processing;geophysical techniques;image enhancement;image fusion;remote sensing","batch methods;resolution enhancement;TIR image sequences;thermal infrared time series;Earth observation;forest management;remotely sensed data;real-time functioning;sequential Bayesian algorithm;Bayesian framework;Bayesian strategies;real thermal data;SEVIRI sensor;MODIS sensor","","9","","53","IEEE","6 Jul 2015","","","IEEE","IEEE Journals"
"Spatio–Temporal–Spectral Collaborative Learning for Spatio–Temporal Fusion with Land Cover Changes","X. Meng; Q. Liu; F. Shao; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","8 Jul 2022","2022","60","","1","16","Spatio-temporal fusion by combining the complementary spatial and temporal advantages of multisource remote-sensing images to obtain time-series high spatial resolution images is highly desirable in monitoring surface dynamics. Currently, deep learning (DL)-based fusion methods have received extensive attention. However, existing DL-based spatio-temporal fusion methods are generally limited in fusing the images with land cover changes. In this article, we propose a spatio-temporal–spectral collaborative learning framework for spatio-temporal fusion to alleviate this problem. Specifically, the proposed method integrates the convolutional neural network and recurrent neural network into a unified framework, consisting of three subnetworks: the multiscale Siamese convolutional neural network (MSCNN), the multilayer convolutional recurrent neural network, and the adaptive weighting fusion network (AWFNet). The MSCNN has a flexible weight-sharing network to extract multiscale spatial–spectral features from multisource remote-sensing images. The multilayer convolutional recurrent neural network is constructed on the convolutional long short-term memory units to comprehensively learn the land cover changes by spatial, spectral, and temporal joint features. The AWFNet with a spatio-temporal–spectral change (STSC) loss is proposed to further improve the interpretability and robustness. The experiments were performed on the publicly available benchmark datasets featured phenology and land cover type changes, respectively. The experimental results demonstrated the competitive performance of the proposed method than other state-of-the-art fusion methods.","1558-0644","","10.1109/TGRS.2022.3185459","National Natural Science Foundation of China(grant numbers:42171326,62071261,41801252); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY22F010014); Postdoctoral Research Foundation of China(grant numbers:2020M672490); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803281","Adaptive weighting fusion;convolutional neural network (CNN);land cover changes;recurrent neural network (RNN);spatio-temporal fusion","Feature extraction;Remote sensing;Spatial resolution;Sensors;MODIS;Adaptive systems;Earth","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image resolution;recurrent neural nets;remote sensing;spatiotemporal phenomena;terrain mapping","spatio-temporal-spectral collaborative learning;land cover changes;complementary spatial advantages;temporal advantages;multisource remote-sensing images;time-series high spatial resolution images;deep learning-based fusion methods;spatio-temporal fusion methods;multiscale Siamese convolutional neural network;multilayer convolutional recurrent neural network;adaptive weighting fusion network;flexible weight-sharing network;multiscale spatial-spectral features;spatial features;spectral features;temporal joint features;spatio-temporal-spectral change loss;state-of-the-art fusion methods;spatio-temporal-spectral collaborative learning","","1","","50","IEEE","22 Jun 2022","","","IEEE","IEEE Journals"
"MD³Net: Integrating Model-Driven and Data-Driven Approaches for Pansharpening","Y. Yan; J. Liu; S. Xu; Y. Wang; X. Cao","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Northwestern Polytechnical University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Faculty of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","17 Aug 2022","2022","60","","1","16","Pansharpening is a special image fusion task of reconstructing a high-resolution multispectral (HRMS) image by integrating a panchromatic (PAN) image of high spatial resolution and a low-resolution multispectral (LRMS) image. To handle such an ill-posed multimodal fusion task, in this article, we propose a novel pansharpening method, referred to as model-driven and data-driven network (MD3Net), which combines model-driven and data-driven approaches. The architecture design of MD3Net is inspired from the traditional model constructed based on domain knowledge and thus making its network topology explainable and its input-output predictable. To further explore the powerful learning ability of deep-learning-based approaches, we introduce the deep prior into the MD3Net as its implicit regularization, thus improving its data adaptability and representation capability. Comprehensive experiments conducted on both reduced and full resolution of several acknowledged datasets have qualitatively and quantitatively verified the superiority of our network compared with a benchmark consisting of several state-of-the-art approaches. The code can be downloaded from https://github.com/YinsongYan/M3DNet.","1558-0644","","10.1109/TGRS.2022.3196427","National Key Research and Development Program of China(grant numbers:2020AAA0105601); National Natural Science Foundation of China(grant numbers:61877049,U20B2075); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851415","Deep learning (DL);deep prior;model-driven and data-driven;pansharpening;remote sensing;unfolding algorithm","Pansharpening;Task analysis;Spatial resolution;Neural networks;Deep learning;Convolutional neural networks;Remote sensing","geophysical image processing;geophysical signal processing;image fusion;image resolution;learning (artificial intelligence)","full resolution;model-driven;data-driven approaches;special image fusion task;high-resolution multispectral image;panchromatic image;high spatial resolution;low-resolution multispectral image;multimodal fusion task;novel pansharpening method;data-driven network;MD;3 Net;network topology explainable;deep-learning-based approaches;data adaptability;representation capability;reduced resolution","","1","","66","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"Context-Adaptive Pansharpening Based on Image Segmentation","R. Restaino; M. Dalla Mura; G. Vivone; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Universite Grenoble Alpes, CNRS, GIPSA-lab, Grenoble, France; Centre for Maritime Research and Experimentation, North Atlantic Treaty Organization Science and Technology Organization, La Spezia, Italy; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","29 Dec 2016","2017","55","2","753","766","Pansharpened images are widely used synthetic representations of the Earth surface characterized by both a high spatial resolution and a high spectral diversity. They are usually generated by extracting spatial details from a high-resolution PANchromatic image and by injecting them into a low spatial resolution multispectral image. The details injection is performed through injection coefficients, whose values can be either uniform for the whole image (global methods) or spatially variant (context-adaptive (CA) approaches). In this paper, we propose a CA approach in which the injection coefficients are estimated over image segments achieved through a binary partition tree segmentation algorithm. The approach is applied to two credited pansharpening algorithms based on the Gram-Schmidt orthogonalization procedure and the generalized Laplacian pyramid technique. The performance assessment is performed using two different data sets acquired by the QuickBird and the WorldView-3 satellites. The validation procedure, both at full and at reduced resolution, shows the suitability of the proposed approach, which reaches a good tradeoff between accuracy and computational burden.","1558-0644","","10.1109/TGRS.2016.2614367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776839","Binary partition tree (BPT);context-adaptive (CA) algorithms;image fusion;image segmentation;pansharpening;remote sensing","Image segmentation;Spatial resolution;Partitioning algorithms;Earth;Satellites;Indexes","image fusion;image segmentation;Laplace equations;remote sensing","context-adaptive pansharpening;image segmentation;Earth surface;spectral diversity;high-resolution PANchromatic image;multispectral image;injection coefficients;context-adaptive approach;injection coefficient;binary partition tree segmentation algorithm;Gram-Schmidt orthogonalization procedure;generalized Laplacian pyramid technique;QuickBird satellites;WorldView-3 satellites","","92","","57","IEEE","7 Dec 2016","","","IEEE","IEEE Journals"
"Robust Band-Dependent Spatial-Detail Approaches for Panchromatic Sharpening","G. Vivone","Department of Information Engineering, University of Salerno, Fisciano, Italy","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","6421","6433","Pansharpening refers to the fusion of a multispectral (MS) image with a finer spectral resolution but coarser spatial resolution than a panchromatic (PAN) image. The classical pansharpening problem can be dealt with component substitution or multiresolution analysis techniques. One of the most notable approaches in the former class is the band-dependent spatial-detail (BDSD) method. It has been shown state-of-the-art performance, in particular, when the fusion of four band data sets is addressed. However, new sensors, such as the WorldView-2/-3 ones, usually acquire MS images with more than four spectral bands to be fused with the PAN image. The BDSD method has shown limitations in performance in these cases. Thus, in this paper, several BDSD-based approaches are provided to solve this issue getting a robustness of the BDSD with respect to the spectral bands to be fused. The experimental results conducted both at reduced and at full resolutions on four real data sets acquired by the IKONOS, the QuickBird, the WorldView-2, and the WorldView-3 sensors demonstrate the validity of the proposed approaches against the benchmark.","1558-0644","","10.1109/TGRS.2019.2906073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693555","Band-dependent spatial-detail (BDSD);component substitution (CS);image fusion;pansharpening;remote sensing;robust regression","Sensors;Spatial resolution;Multiresolution analysis;Estimation;Benchmark testing;Wavelet transforms","estimation theory;geophysical image processing;image fusion;image resolution;remote sensing","notable approaches;band-dependent spatial-detail;band data sets;MS images;spectral bands;PAN image;BDSD-based approaches;WorldView-3 sensors;robust band-dependent spatial-;panchromatic sharpening;multispectral image;finer spectral resolution;coarser spatial resolution;panchromatic image;classical pansharpening problem;component substitution;multiresolution analysis techniques;IKONOS;QuickBird","","87","","56","IEEE","17 Apr 2019","","","IEEE","IEEE Journals"
"Detail Injection-Based Deep Convolutional Neural Networks for Pansharpening","L. -J. Deng; G. Vivone; C. Jin; J. Chanussot","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; CNR-IMAA, National Research Council – Institute of Methodologies for Environmental Analysis, Tito Scalo, Italy; School of Optoelectronics, University of Electronic Science and Engineering of China, Chengdu, China; Inria, CNRS, Grenoble INP, Laboratoire Jean Kuntzmann (LJK), Univ. Grenoble Alpes, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6995","7010","The fusion of high spatial resolution panchromatic (PAN) data with simultaneously acquired multispectral (MS) data with the lower spatial resolution is a hot topic, which is often called pansharpening. In this article, we exploit the combination of machine learning techniques and fusion schemes introduced to address the pansharpening problem. In particular, deep convolutional neural networks (DCNNs) are proposed to solve this issue. The latter is combined first with the traditional component substitution and multiresolution analysis fusion schemes in order to estimate the nonlinear injection models that rule the combination of the upsampled low-resolution MS image with the extracted details exploiting the two philosophies. Furthermore, inspired by these two approaches, we also developed another DCNN for pansharpening. This is fed by the direct difference between the PAN image and the upsampled low-resolution MS image. Extensive experiments conducted both at reduced and full resolutions demonstrate that this latter convolutional neural network outperforms both the other detail injection-based proposals and several state-of-the-art pansharpening methods.","1558-0644","","10.1109/TGRS.2020.3031366","NSFC(grant numbers:61702083,61772003,61876203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240949","Component substitution (CS);deep convolutional neural network (DCNN);image fusion;multiresolution analysis (MRA);pansharpening;remote sensing","Spatial resolution;Computer architecture;Convolutional neural networks;Multiresolution analysis;Training","filtering theory;geophysical image processing;image colour analysis;image fusion;image resolution;image sampling;learning (artificial intelligence);neural nets;remote sensing","detail injection-based deep convolutional neural networks;high spatial resolution panchromatic;simultaneously acquired multispectral data;lower spatial resolution;machine learning techniques;pansharpening problem;particular networks;traditional component substitution;multiresolution analysis fusion schemes;nonlinear injection models;upsampled low-resolution MS image;PAN image;reduced resolutions;full resolutions;convolutional neural network;detail injection-based proposals;state-of-the-art pansharpening methods","","63","","64","IEEE","27 Oct 2020","","","IEEE","IEEE Journals"
"Pansharpening Based on Deconvolution for Multiband Filter Estimation","G. Vivone; P. Addesso; R. Restaino; M. Dalla Mura; J. Chanussot","Department of Information Engineering, Electrical Engineering, and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering, and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering, and Applied Mathematics, University of Salerno, Fisciano, Italy; Université Grenoble Alpes, CNRS, Grenoble INP, GIPSA-Lab, Grenoble, France; Université Grenoble Alpes, CNRS, Grenoble INP, GIPSA-Lab, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2018","2019","57","1","540","553","The combination of a multispectral (MS) image and a panchromatic (PAN) image, the so-called pansharpening, allows to produce very appealing images that are useful both for visual interpretation and for feature extraction. The state-of-the-art multiresolution analysis pansharpening algorithms are based on the extraction of spatial details from the PAN image through image filters matched with the MS sensors' modulation transfer function. However, this knowledge is often poor due to measurement inaccuracies and/or its aging. Thus, deconvolution algorithms have been proposed to overcome this limitation. In this paper, we propose a multiband filter estimation (FE) approach to improve the solutions in the literature. The main idea in this paper is to exploit a preliminary pansharpened image to estimate the spatial filter used for detail extraction associated with each spectral band. We demonstrate that the proposed method outperforms the state-of-the-art FE approaches by employing data sets acquired by the IKONOS, the Quickbird, and the WorldView-3 sensors.","1558-0644","","10.1109/TGRS.2018.2858288","ANR 14-CE22-0016-01 Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8435988","Deconvolution;image fusion;multiresolution analysis (MRA);pansharpening;remote sensing","Iron;Sensors;Feature extraction;Deconvolution;Spatial resolution;Image sensors;Estimation","deconvolution;feature extraction;filtering theory;geophysical image processing;geophysical signal processing;image fusion;image resolution;optical transfer function;remote sensing;spatial filters;spectral analysis","multispectral image;panchromatic image;visual interpretation;feature extraction;state-of-the-art multiresolution analysis pansharpening algorithms;PAN image;image filters;MS sensors;deconvolution algorithms;multiband filter estimation approach;preliminary pansharpened image;spatial filter;state-of-the-art FE approaches","","37","","58","IEEE","14 Aug 2018","","","IEEE","IEEE Journals"
"Comparative Performance Evaluation of Pixel-Level and Decision-Level Data Fusion of Landsat 8 OLI, Landsat 7 ETM+ and Sentinel-2 MSI for Crop Ensemble Classification","J. Useya; S. Chen","Department of Remote Sensing and GIS, College of GeoExploration Science and Technology, Jilin University, Changchun, China; Department of Remote Sensing and GIS, College of GeoExploration Science and Technology, Jilin University, Changchun, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Dec 2018","2018","11","11","4441","4451","Crops mapping unequivocally becomes a daunting task in humid, tropical, or subtropical regions due to unattainability of adequate cloud-free optical imagery. Objective of this study is to evaluate the comparative performance between decisionand pixel-levels data fusion ensemble classified maps using Landsat 8, Landsat 7, and Sentinel-2 data. This research implements parallel and concatenation approach to ensemble classify the images. The multiclassifier system comprises of Maximum Likelihood, Support Vector Machines, and Spectral Information Divergence as base classifiers. Decision-level fusion is achieved by implementing plurality voting method. Pixel-level fusion is achieved by implementing fusion by mosaicking approach, thus appending cloud-free pixels from either Sentinel-2 or Landsat 7. The comparison is based on the assessment of classification accuracy. Overall accuracy results show that decision-level fusion achieved an accuracy of 85.4%, whereas pixel-level fusion classification attained 82.5%, but their respective kappa coefficients of 0.84 and 0.80 but are not significantly different according to Z-testat α = 0.05. F1-score values reveal that decision-level performed better on most individual classes than pixel-level. Regression coefficient between planted areas from both approaches is 0.99. However, Support Vector Machines performed the best of the three classifiers. The conclusion is that both decision-level and pixel-level fusion approaches produced comparable classification results. Therefore, either of the procedures can be adopted in areas with inescapable cloud problems for updating crop inventories and acreage estimation at regional scales. Future work can focus on performing more comparison tests on different areas, run tests using different multiclassifier systems, and use different imagery","2151-1535","","10.1109/JSTARS.2018.2870650","JLU Science and Technology Innovative Research Team(grant numbers:2017TD-26); Fundamental Research Funds for the Central Universities; Changbai Mountain Scholars of Jilin Province(grant numbers:JJLZ[2015]54); Jilin province and Jilin University co-building project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528827","Data fusion;ensemble classifier;multiclassifier system;parallel and concatenation approach;plurality voting","Remote sensing;Agriculture;Earth;Artificial satellites;Cloud computing;Data integration;Security","crops;geophysical image processing;image classification;image fusion;regression analysis;remote sensing;support vector machines","Landsat 8 OLI;Landsat 7 ETM+;cloud-free optical imagery;multiclassifier system;maximum likelihood;spectral information divergence;regression coefficient;pixel-level fusion approaches;pixel-level fusion classification;classification accuracy;cloud-free pixels;decision-level fusion;Support Vector Machines;Sentinel-2 data;crops mapping;crop ensemble classification;Sentinel-2 MSI;decision-level data fusion","","19","","62","OAPA","9 Nov 2018","","","IEEE","IEEE Journals"
"Multi-Modal Object Tracking and Image Fusion With Unsupervised Deep Learning","N. LaHaye; J. Ott; M. J. Garay; H. M. El-Askary; E. Linstead","Computational and Data Sciences Department, Chapman University, Orange, CA, USA; Computational and Data Sciences Department, Chapman University, Orange, CA, USA; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA, USA; Center of Excellence in Earth Systems Modeling and Observations and the Schmid College of Science and Technology, Chapman University, Orange, CA, USA; Schmid College of Science and Technology, Machine Learning and Assistive Technologies Lab, Chapman University, Orange, CA, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Sep 2019","2019","12","8","3056","3066","The number of different modalities for remote sensors continues to grow, bringing with it an increase in the volume and complexity of the data being collected. Although these datasets individually provide valuable information, in aggregate they provide additional opportunities to discover meaningful patterns on a large scale. However, the ability to combine and analyze disparate datasets is challenged by the potentially vast parameter space that results from aggregation. Each dataset in itself requires instrument-specific and dataset-specific knowledge. If the intention is to use multiple, diverse datasets, one needs an understanding of how to translate and combine these parameters in an efficient and effective manner. While there are established techniques for combining datasets from specific domains or platforms, there is no generic, automated method that can address the problem in general. Here, we discuss the application of deep learning to track objects across different image-like data-modalities, given data in a similar spatio-temporal range, and automatically co-register these images. Using deep belief networks combined with unsupervised learning methods, we are able to recognize and separate different objects within image-like data in a structured manner, thus making progress toward the ultimate goal of a generic tracking and fusion pipeline requiring minimal human intervention.","2151-1535","","10.1109/JSTARS.2019.2920234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8738825","Big data applications;clustering;computer vision;deep belief networks (DBNs);deep learning","Instruments;Remote sensing;Earth;Data models;Deep learning;Cameras;Data integration","belief networks;image fusion;object detection;object tracking;pattern clustering;unsupervised learning","multimodal object tracking;image fusion;unsupervised deep learning;remote sensors;disparate datasets;dataset-specific knowledge;multiple datasets;diverse datasets;data-modalities;deep belief networks;unsupervised learning methods;generic tracking;fusion pipeline","","7","","33","IEEE","18 Jun 2019","","","IEEE","IEEE Journals"
"Deep-Learning-Based Multispectral Satellite Image Segmentation for Water Body Detection","K. Yuan; X. Zhuang; G. Schaefer; J. Feng; L. Guan; H. Fang","Department of Computer Science, Loughborough University, Loughborough, U.K.; Southwest Jiaotong University, Chengdu, China; Department of Computer Science, Loughborough University, Loughborough, U.K.; Information Engineering College, Dalian University, Dalian, China; Department of Computer Science, Loughborough University, Loughborough, U.K.; Department of Computer Science, Loughborough University, Loughborough, U.K.","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Aug 2021","2021","14","","7422","7434","Automated water body detection from satellite imagery is a fundamental stage for urban hydrological studies. In recent years, various deep convolutional neural network (DCNN)-based methods have been proposed to segment remote sensing data collected by conventional RGB or multispectral imagery for such studies. However, how to effectively explore the wider spectrum bands of multispectral sensors to achieve significantly better performance compared to the use of only RGB bands has been left underexplored. In this article, we propose a novel DCNN model-multichannel water body detection network (MC-WBDN)-that incorporates three innovative components, i.e., a multichannel fusion module, an Enhanced Atrous Spatial Pyramid Pooling module, and Space-to-Depth/Depth-to-Space operations, to outperform state-of-the-art DCNN-based water body detection methods. Experimental results convincingly show that our MC-WBDN model achieves remarkable water body detection performance, is more robust to light and weather variations, and can better distinguish tiny water bodies compared to other DCNN models.","2151-1535","","10.1109/JSTARS.2021.3098678","Government of Chengdu City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492784","Deep convolutional neural networks (DCNNs);feature fusion;multispectral remote sensing;semantic segmentation;water body detection","Feature extraction;Image segmentation;Remote sensing;Indexes;Satellites;Vegetation mapping;Urban areas","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;geophysics computing;image colour analysis;image fusion;image resolution;image segmentation;object detection;remote sensing","multichannel fusion module;MC-WBDN model;multispectral satellite image segmentation;automated water body detection;satellite imagery;urban hydrological studies;deep convolutional neural network;multispectral imagery;multispectral sensors;RGB bands;DCNN model;remote sensing data segmentation;multichannel water body detection network;enhanced atrous spatial pyramid pooling module;space-to-depth/depth-to-space operations","","19","","54","CCBY","21 Jul 2021","","","IEEE","IEEE Journals"
"Indicator-Kriging-Integrated Evidence Theory for Unsupervised Change Detection in Remotely Sensed Imagery","P. Shao; W. Shi; M. Hao","Hubei Engineering Technology Research Center for Farmland Environmental Monitoring, China Three Gorges University, Yichang, China; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Key Laboratory for Land Environment and Disaster Monitoring of SBSM, China University of Mining and Technology, Xuzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4649","4663","This study proposes a novel approach based on indicator kriging and Dempster-Shafer (DS) theory for unsupervised change detection (CD) in remote sensing images (DSK). Indicator kriging is integrated to the standard DS theory. A feature set with four difference images (DIs) providing complementary change information is initially generated. Subsequently, the mass functions for each DI are determined automatically using fuzzy logic, the four pieces of DI evidence are combined by DS theory, and a preliminary CD map is achieved. The preliminary CD map is then divided into three parts adaptively-weakly conflicting part of no change, weakly conflicting part of change, and strongly conflicting part-by calculating the evidence conflict degree for each pixel. Finally, the pixels in the weakly conflicting parts, which have little or no conflict, are labeled as the current class, and the pixels in the strongly conflicting part that contains misclassified pixels are reclassified based on indicator kriging. DSK combines the advantages of different DI features and solves the conflicting situations to a large extent. The main contributions of this study include the following: 1) introducing indicator kriging into CD to manage conflict information during DS fusion and 2) presenting a scheme for producing DI set with complementary change information, developing a novel DSK fusion model for information fusion, and defining the proposed CD framework. Experimental results verify that the proposed DSK is robust and effective for CD.","2151-1535","","10.1109/JSTARS.2018.2878759","National Natural Science Foundation of China(grant numbers:41331175); Hong Kong Polytechnic University(grant numbers:1_ZVF2,1_ZVE8); Three Gorges University; Hubei Engineering Technology Research Center for Farmland Environmental Monitoring(grant numbers:201608,201603); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8554088","Conflict management;Dempster–Shafer (DS) theory;indicator kriging;remote sensing;unsupervised change detection (CD)","Remote sensing;Solid modeling;Clustering algorithms;Wavelet transforms;Indexes","fuzzy logic;fuzzy set theory;geophysical image processing;image classification;image fusion;inference mechanisms;remote sensing;statistical analysis;uncertainty handling","indicator-kriging-integrated evidence theory;DI features;strongly conflicting part;CD framework;DS fusion;conflict information;evidence conflict degree;preliminary CD map;DI evidence;complementary change information;difference images;standard DS theory;DSK;remote sensing images;Dempster-Shafer theory;indicator kriging;remotely sensed imagery;unsupervised change detection","","5","","50","IEEE","30 Nov 2018","","","IEEE","IEEE Journals"
"PSMD-Net: A Novel Pan-Sharpening Method Based on a Multiscale Dense Network","J. Peng; L. Liu; J. Wang; E. Zhang; X. Zhu; Y. Zhang; J. Feng; L. Jiao","School of Information Science and Technology, Northwest University, Xi’an, China; School of Information Science and Technology, Northwest University, Xi’an, China; School of Information Science and Technology, Northwest University, Xi’an, China; School of Information Science and Technology, Northwest University, Xi’an, China; School of Information Science and Technology, Northwest University, Xi’an, China; School of Information Science and Technology, Northwest University, Xi’an, China; Laboratory of Intelligent Perception and Image Understanding, Xidian University, Xi’an, China; Laboratory of Intelligent Perception and Image Understanding, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","4957","4971","Pan sharpening is used to fuse a low-resolution multispectral (MS) image and a high-resolution panchromatic (PAN) image to obtain a high-resolution MS image. This article proposes PSMD-Net, an end-to-end pan-sharpening method based on a multi-scale dense network. A shallow feature extraction layer (SFEL) extracts the shallow features from the original images, and these are used as an input to a global dense feature fusion (GDFF) network to learn the global features for image reconstruction. A multiscale dense block (MDB) is designed to fully extract the spatial and spectral information from the shallow features in the GDFF network. In the proposed network, multiple MDBs are stacked to extract rich, multi-scale dense hierarchical features, and a global dense connection (GDC) is designed to allow direct connections from the state of the current MDB to all subsequent MDBs to extract more advanced features. The extracted hierarchical features are sent to the global feature fusion layer (GFFL) to adaptively learn the global features for image reconstruction. Finally, global residual learning (GRL) is adopted to force the network to pay more attention to the changing part of the image. We perform experiments on simulated and real data from WorldView-2 and WorldView-3 satellites. Visual and quantitative assessment results demonstrate that PSMD-Net yields higher-resolution fusion images than the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2020.3020162","National Key Research and Development Program of China(grant numbers:2017YFB1402103); Changjiang Scholars and Innovative Research Team in University(grant numbers:IRT_17R87); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ-294,2019JQ-454); China Postdoctoral Science Foundation(grant numbers:2018M643718); Open Fund of Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University(grant numbers:IPIU2019002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9198919","Convolutional neural network;multi-scale dense connection;pan sharpening;remote sensing;residual learning","Feature extraction;Spatial resolution;Image reconstruction;Kernel;Frequency modulation;Remote sensing","feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","shallow features;original images;global dense feature fusion network;global features;image reconstruction;multiscale dense block;GDFF network;multiscale dense hierarchical features;global dense connection;global feature fusion layer;global residual learning;PSMD-Net yields higher-resolution fusion images;novel pan-sharpening method;multiscale dense network;pan sharpening;low-resolution multispectral image;high-resolution panchromatic image;high-resolution MS image;end-to-end pan-sharpening method;shallow feature extraction layer","","12","","51","IEEE","16 Sep 2020","","","IEEE","IEEE Journals"
"Multi-Source Evidence Data Fusion Approach to Detect Daily Distribution and Coverage of Ulva Prolifera in the Yellow Sea, China","Z. Wang; Z. Fang; Y. Wu; J. Liang; X. Song","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; National Marine Data and Information Service, Tianjin, China; National Marine Data and Information Service, Tianjin, China","IEEE Access","27 Aug 2019","2019","7","","115214","115228","Ulva prolifera (U. prolifera), a seaweed species in the family Ulvaceae, has been causing green tides in the Yellow Sea, China, every year since 2008. This has attracted the attention of the government and the scientific community because of its influence on the region's marine economy and coastal zone environment. Remote sensing, surveillance ships, and airplanes are three common surveillance approaches used by the State Oceanic Administration (SOA) for U. prolifera detection in the Yellow Sea. However, improving the accuracy and convenience of daily U. prolifera detection by fusing the data from these sources, and obtaining the coverage area and distribution of U. prolifera without human intervention, are two challenges that need to be addressed. To this end, this study proposes an improved multi-source evidence fusion method based on Dempster-Shafer evidence theory to detect the distribution and coverage area of U. prolifera using data from multiple sources (MODIS images, Sentinel-2A images, and surveillance ship data). This improved approach can handle the conflicts caused by differences in image characteristics and spatial resolutions as well as differences between surveillance ship data and remote sensing data. The capability of the improved method to extract the distribution and coverage area of U. prolifera was tested experimentally. The results agree with the reference data supplied by SOA in terms of spatial position and geometry. The fusion of multi-source data demonstrates better results in the detection of U. prolifera coverage area and spatial distribution compared with those from individual data sources. The improved approach can be applied to daily and emergent monitoring tasks for U. prolifera detection.","2169-3536","","10.1109/ACCESS.2019.2936247","National key Research and Development Plan(grant numbers:2017YFC1405302); National Natural Science Foundation of China(grant numbers:41771473,41231171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805352","Ulva prolifera;Dempster-Shafer theory;data fusion;coverage;distribution","Indexes;Surveillance;MODIS;Marine vehicles;Vegetation mapping;Satellites;Remote sensing","image fusion;image resolution;inference mechanisms;marine engineering;oceanographic techniques;remote sensing;ships;surveillance","U. prolifera detection;multisource evidence data fusion approach;daily distribution;ulva prolifera;yellow sea;Dempster-Shafer evidence theory;surveillance ship data;remote sensing data;image characteristics;spatial resolutions","","4","","43","CCBY","19 Aug 2019","","","IEEE","IEEE Journals"
"Fusion of Multispectral and Panchromatic Images Based on Morphological Operators","R. Restaino; G. Vivone; M. Dalla Mura; J. Chanussot","Department of Information Engineering, University of Salerno, Fisciano, Italy; North Atlantic Treaty Organization, Science and Technology Organization, Centre for Maritime Research and Experimentation, La Spezia, Italy; GIPSA-Laboratory, Grenoble Institute of Technology, Grenoble, France; GIPSA-Laboratory, Grenoble Institute of Technology, Grenoble, France","IEEE Transactions on Image Processing","3 May 2016","2016","25","6","2882","2895","Nonlinear decomposition schemes constitute an alternative to classical approaches for facing the problem of data fusion. In this paper, we discuss the application of this methodology to a popular remote sensing application called pansharpening, which consists in the fusion of a low resolution multispectral image and a high-resolution panchromatic image. We design a complete pansharpening scheme based on the use of morphological half gradient operators and demonstrate the suitability of this algorithm through the comparison with the state-of-the-art approaches. Four data sets acquired by the Pleiades, Worldview-2, Ikonos, and Geoeye-1 satellites are employed for the performance assessment, testifying the effectiveness of the proposed approach in producing top-class images with a setting independent of the specific sensor.","1941-0042","","10.1109/TIP.2016.2556944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457244","Image enhancement;morphological operators;multispectral imaging;image fusion;remote sensing","Spatial resolution;Data integration;Satellites;Rendering (computer graphics);Remote sensing;Algorithm design and analysis","geophysical image processing;gradient methods;image fusion;image resolution;remote sensing","panchromatic images;multispectral images;morphological operators;nonlinear decomposition;data fusion;remote sensing;high-resolution panchromatic image;gradient operators","","117","","65","IEEE","20 Apr 2016","","","IEEE","IEEE Journals"
"Context and Difference Enhancement Network for Change Detection","D. Song; Y. Dong; X. Li","School of Optoelectronics, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Intelligent Interaction and Applications (Northwestern Polytechnical University), Ministry of Industry and Information Technology, Xi'an, China; Key Laboratory of Intelligent Interaction and Applications (Northwestern Polytechnical University), Ministry of Industry and Information Technology, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Nov 2022","2022","15","","9457","9467","At present, convolution neural networks have achieved good performance in remote sensing image change detection. However, due to the locality of convolution, these methods are difficult to capture the global context relationships among different-level features. To alleviate this issue, we propose a context and difference enhancement network (CDENet) for change detection, which can strongly model global context relationships and enhance the change difference. Specifically, our backbone is the dual TransUNet, which is based on U-Net and equipped with transformer block in the encoder. The dual TransUNet is used to extract bitemporal features. Then, the features are encoded as the input sequence, which is conducive to modeling the global context. Moreover, we design the content difference enhancement module to process the dual features of each layer in the encoder. The designed module can increase the spatial attention of difference regions to enhance the change difference features. In the decoder, we adopt a simple cross-layer feature fusion to combine the upsampled features with the high-resolution features, which is used to generate more accurate results. Finally, we adopt a novel loss to supervise the accuracy of results in regions and pixels. The experiments on two public change detection datasets demonstrate that our CDENet has strong competitiveness and performs better than the state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3217082","National Natural Science Foundation of China(grant numbers:61871470,62071171); Key Research Program of Frontier Sciences; Chinese Academy of Sciences(grant numbers:QYZDY-SSW-JSC044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9928582","Change detection (CD);content difference enhancement;global context;remote sensing","Transformers;Feature extraction;Context modeling;Task analysis;Remote sensing;Data mining;Semantics","convolutional neural nets;feature extraction;geophysical image processing;image enhancement;image fusion;image resolution;learning (artificial intelligence);object detection;remote sensing","bitemporal features;CDENet;change difference features;content difference enhancement module;convolution neural networks;difference enhancement network;difference regions;different-level features;dual features;dual TransUNet;encoder;global context relationships;high-resolution features;public change detection datasets;remote sensing image change detection;simple cross-layer feature fusion;upsampled features","","","","62","CCBY","25 Oct 2022","","","IEEE","IEEE Journals"
"Hyper-Sharpening Based on Spectral Modulation","X. Lu; J. Zhang; X. Yu; W. Tang; T. Li; Y. Zhang","Shanghai Radio Equipment Research Institute, Shanghai, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Shanghai Radio Equipment Research Institute, Shanghai, China; Shanghai Radio Equipment Research Institute, Shanghai, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","31 May 2019","2019","12","5","1534","1548","Hyperspectral (HS) image sharpening (namely hyper sharpening) with an auxiliary sensor, such as multispectral (MS) or panchromatic sensor, has attracted a great deal of attention for the past decade. A number of hyper-sharpening techniques, aiming at enhancing the spatial resolution of HS images, have been developed and demonstrated their effectiveness especially on synthetic or simulated data. Nevertheless, since the differences between different remote sensing systems or imaging conditions are complicate, it results in a serious spectral distortion when applying on real remote sensing data acquired by different sensors under different acquisition times or conditions. Unfortunately, very few works have considered this issue. In this paper, a new hyper-sharpening framework based on spectral modulation is proposed to better preserve spectral information when fusing with MS data acquired by a different sensor. The goal of this paper is to generate an adjusted MS image that would have been observed under the same imaging conditions with the corresponding HS sensor. Two approaches, originating in MS pan-sharpening field, are introduced as examples under this framework, namely high-pass details injection model and band-dependent spatial-detail model. Experiments on three HS and MS datasets acquired by different platforms demonstrate that the proposed framework is beneficial to the spectral fidelity of the fused image compared with some state-of-the-art hyper-sharpening techniques. Meanwhile, it is also easy to implement and has a certain advantage in enhancing the spatial details.","2151-1535","","10.1109/JSTARS.2019.2908984","National Natural Science Foundation of China(grant numbers:61871150,41701479); Defense Industrial Technology Development Program(grant numbers:JCKY 2016603C004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8697095","Hyperspectral;hyper-sharpening;image fusion;spectral modulation","Spatial resolution;Estimation;Modulation;Imaging;Hyperspectral sensors","hyperspectral imaging;image fusion;image resolution;remote sensing","spectral modulation;hyperspectral image sharpening;auxiliary sensor;panchromatic sensor;spatial resolution;synthetic simulated data;imaging conditions;serious spectral distortion;remote sensing data;hyper-sharpening framework;spectral information;band-dependent spatial-detail model;spectral fidelity;fused image;remote sensing systems;hyper-sharpening technique;multispectral datasets;hyperspectral sensor;multispectral image","","8","","58","IEEE","23 Apr 2019","","","IEEE","IEEE Journals"
"Hyperspectral Pansharpening With Guided Filter","J. Qu; Y. Li; W. Dong","School of Telecommunications Engineering, Xidian University, Xi’an, China; School of Telecommunications Engineering, Xidian University, Xi’an, China; School of Telecommunications Engineering, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","25 Oct 2017","2017","14","11","2152","2156","A new hyperspectral (HS) pansharpening method based on guided filter is proposed in this letter. The proposed method, which obtains the spatial detail difference of each band successively, is different from the traditional component substitution method. The detail information of each band is extracted at first. Then, the panchromatic (PAN) image is sharpened to enhance the details. The spatial information difference between the enhanced PAN image and the detail information of each band is obtained using the guided filter, without causing spectral and spatial distortion. In order to reduce spectral distortion and add enough spatial information, the injection gains matrix is generated. The fused HS image is finally achieved by injecting the corresponding spatial difference into each band of the interpolated HS image. Experiments demonstrate that the proposed method can obtain superior performance in terms of subjective and objective evaluations.","1558-0571","","10.1109/LGRS.2017.2755679","National Science Foundation of China(grant numbers:61222101,61272120,61301287,61301291,61350110239); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8062812","Guided filter;hyperspectral (HS) image;image fusion;panchromatic (PAN) image","Bayes methods;Indexes;Spatial resolution;Image fusion;Hyperspectral imaging;Distortion","hyperspectral imaging;image resolution","guided filter;panchromatic image;spectral distortion;spatial distortion;hyperspectral pansharpening;spatial information;PAN image","","28","","18","IEEE","9 Oct 2017","","","IEEE","IEEE Journals"
"Multilevel Feature Fusion-Based CNN for Local Climate Zone Classification From Sentinel-2 Images: Benchmark Results on the So2Sat LCZ42 Dataset","C. Qiu; X. Tong; M. Schmitt; B. Bechtel; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Information Engineering University, Zhengzhou, China; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Institute of Geography, Ruhr-University Bochum, Bochum, Germany; German Aerospace Center (DLR) and Signal Processing in Earth Observation (SiPEO), Remote Sensing Technology Institute (IMF), Technical University of Munich, Munich, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 Jun 2020","2020","13","","2793","2806","As a unique classification scheme for urban forms and functions, the local climate zone (LCZ) system provides essential general information for any studies related to urban environments, especially on a large scale. Remote sensing data-based classification approaches are the key to large-scale mapping and monitoring of LCZs. The potential of deep learning-based approaches is not yet fully explored, even though advanced convolutional neural networks (CNNs) continue to push the frontiers for various computer vision tasks. One reason is that published studies are based on different datasets, usually at a regional scale, which makes it impossible to fairly and consistently compare the potential of different CNNs for real-world scenarios. This article is based on the big So2Sat LCZ42 benchmark dataset dedicated to LCZ classification. Using this dataset, we studied a range of CNNs of varying sizes. In addition, we proposed a CNN to classify LCZs from Sentinel-2 images, Sen2LCZ-Net. Using this base network, we propose fusing multilevel features using the extended Sen2LCZ-Net-MF. With this proposed simple network architecture, and the highly competitive benchmark dataset, we obtain results that are better than those obtained by the state-of-the-art CNNs, while requiring less computation with fewer layers and parameters. Large-scale LCZ classification examples of completely unseen areas are presented, demonstrating the potential of our proposed Sen2LCZ-Net-MF as well as the So2Sat LCZ42 dataset. We also intensively investigated the influence of network depth and width, and the effectiveness of the design choices made for Sen2LCZ-Net-MF. This article will provide important baselines for future CNN-based algorithm developments for both LCZ classification and other urban land cover land use classification. Code and pretrained models are available at https://github.com/ChunpingQiu/benchmark-on-So2SatLCZ42-dataset-a-simple-tour.","2151-1535","","10.1109/JSTARS.2020.2995711","China Scholarship Council; European Research Council; European Union’s Horizon 2020 Research and Innovation Programme(grant numbers:ERC-2016-StG-714087); Helmholtz Association; Framework of Helmholtz Artificial Intelligence; Munich Unit @Aeronautics, Space and Transport (MASTr); Helmholtz Excellent Professorship; Data Science in Earth Observation - Big Data Fusion for Urban Research; German Federal Ministry of Education and Research; International Future AI lab; AI4EO: Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond; National Key R&D Program of China(grant numbers:2018YFB0505304); National Natural Science Foundation of China(grant numbers:41671409); Benjamin Bechtel; DFG(grant numbers:437467569); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103196","Benchmark;convolutional neural networks (CNNs);local climate zones (LCZ);sentinel-2;urban land cover","Feature extraction;Benchmark testing;Remote sensing;Earth;Urban areas;Computer architecture;Meteorology","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing","LCZ classification;urban land cover land use classification;local climate zone classification;Sentinel-2 images;large-scale mapping;deep learning;convolutional neural networks;So2Sat LCZ42 benchmark dataset;remote sensing data-based classification;Sen2LCZ-Net-MF;multilevel feature fusion-based CNN","","26","","77","CCBY","28 May 2020","","","IEEE","IEEE Journals"
"Airborne Circular W-Band SAR for Multiple Aspect Urban Site Monitoring","S. Palm; R. Sommer; D. Janssen; A. Tessmann; U. Stilla","Fraunhofer Institute for High Frequency Physics and Radar Techniques, Wachtberg, Germany; Fraunhofer Institute for High Frequency Physics and Radar Techniques, Wachtberg, Germany; Fraunhofer Institute for High Frequency Physics and Radar Techniques, Wachtberg, Germany; Fraunhofer Institute for Applied Solid State Physics IAF, Freiburg, Germany; Department of Photogrammetry and Remote Sensing, Technische Universität München, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","6996","7016","This paper presents a strategy for urban site monitoring by very high-resolution circular synthetic aperture radar (CSAR) imaging of multiple aspects. We analytically derive the limits of coherent azimuth processing for nonplanar objects in CSAR if no digital surface model (DSM) is available. The result indicates the level of maximum achievable resolution of these objects in this geometry. The difficulty of constantly illuminating a specific scene in full aspect mode (360°) for such small wavelengths is solved by a hardware- and software-side integration of the radar in a mechanical tracking mode. This results in the first demonstration of full aspect airborne subaperture CSAR images collected with an active frequency-modulated continuous wave (FMCW) radar at W-band. We describe the geometry and the implementation of the real-time beam-steering mode and evaluate resulting effects in the CSAR processing chain. The physical properties in W-band allow the use of extremely short subapertures in length while generating high azimuthal bandwidths. We use this feature to generate full aspect image stacks for CSAR video monitoring in very high frame rates. This technique offers the capability of detecting and observing moving objects in single channel data by shadow tracking. Due to the relatively strong echo of roads, the shadows of moving cars are rich in contrast. The image stack is further evaluated to present wide angular anisotropic properties of targets and first results on multiple aspect image fusion. Both topics show huge potential for further investigations in terms of image analysis and scene classification.","1558-0644","","10.1109/TGRS.2019.2909949","German Ministry of Defence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701523","Airborne circular synthetic aperture radar (CSAR);full aspect image analysis;millimeter-wave radar;nonplanar targets;radar signal processing;shadow tracking;video SAR","Synthetic aperture radar;Monitoring;Radar imaging;Radar tracking;Trajectory;Geometry","airborne radar;CW radar;FM radar;geophysical image processing;image classification;image fusion;object detection;radar imaging;remote sensing by radar;synthetic aperture radar","CSAR processing chain;high azimuthal bandwidths;aspect image stacks;CSAR video monitoring;high frame rates;image stack;multiple aspect image fusion;image analysis;scene classification;airborne circular W-band SAR;multiple aspect urban site monitoring;high-resolution circular synthetic aperture radar imaging;coherent azimuth processing;nonplanar objects;digital surface model;maximum achievable resolution;aspect mode;mechanical tracking mode;aspect airborne subaperture CSAR images;active frequency-modulated;real-time beam-steering mode","","25","","34","IEEE","29 Apr 2019","","","IEEE","IEEE Journals"
"Attention-Guided Label Refinement Network for Semantic Segmentation of Very High Resolution Aerial Orthoimages","J. Huang; X. Zhang; Y. Sun; Q. Xin","Guangdong Province Key Laboratory for Climate Change and Natural Disaster Studies, School of Atmospheric Sciences, Sun Yat-sen University, Guangzhou, China; College of Environment and Planning, Henan University, Kaifeng, China; Guangdong Key Laboratory for Urbanization and Geo-simulation and the School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory for Urbanization and Geo-simulation and the School of Geography and Planning, Sun Yat-sen University, Guangzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14 May 2021","2021","14","","4490","4503","The recent applications of fully convolutional networks (FCNs) have shown to improve the semantic segmentation of very high resolution (VHR) remote-sensing images because of the excellent feature representation and end-to-end pixel labeling capabilities. While many FCN-based methods concatenate features from multilevel encoding stages to refine the coarse labeling results, the semantic gap between features of different levels and the selection of representative features are often overlooked, leading to the generation of redundant information and unexpected classification results. In this article, we propose an attention-guided label refinement network (ALRNet) for improved semantic labeling of VHR images. ALRNet follows the paradigm of the encoder-decoder architecture, which progressively refines the coarse labeling maps of different scales by using the channelwise attention mechanism. A novel attention-guided feature fusion module based on the squeeze-and-excitation module is designed to fuse higher level and lower level features. In this way, the semantic gaps among features of different levels are declined, and the category discrimination of each pixel in the lower level features is strengthened, which is helpful for subsequent label refinement. ALRNet is tested on three public datasets, including two ISRPS 2-D labeling datasets and the Wuhan University aerial building dataset. Results demonstrated that ALRNet had shown promising segmentation performance in comparison with state-of-the-art deep learning networks. The source code of ALRNet is made publicly available for further studies.","2151-1535","","10.1109/JSTARS.2021.3073935","National Key Research and Development Program of China(grant numbers:2017YFA0604300,2017YFA0604400,2018YFB2100702); National Natural Science Foundation of China(grant numbers:42071441,41801351,41875122); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110441); Innovation Group Project of Southern Marine Science and Engineering Guangdong Laboratory(grant numbers:311020008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9410460","Attention mechanism;convolutional neural networks (CNNs);deep learning;semantic segmentation;urban object extraction;very high spatial resolution images","Semantics;Labeling;Feature extraction;Image segmentation;Remote sensing;Decoding;Sun","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image representation;image segmentation;learning (artificial intelligence);remote sensing","attention-guided label refinement network;semantic segmentation;fully convolutional networks;remote-sensing images;feature representation;end-to-end pixel labeling capabilities;FCN-based methods concatenate features;coarse labeling results;semantic gap;representative features;ALRNet;coarse labeling maps;channelwise attention mechanism;attention-guided feature fusion module;lower level features;label refinement;ISRPS 2-D labeling datasets;semantic labeling","","7","","62","CCBY","22 Apr 2021","","","IEEE","IEEE Journals"
"Ensemble Learning via Higher Order Singular Value Decomposition for Integrating Data and Classifier Fusion in Water Quality Monitoring","Z. Sun; N. -B. Chang; C. -F. Chen; C. Mostafiz; W. Gao","USDA UV-B Monitoring and Research Program, Natural Resource Ecology Laboratory, Colorado State University, Fort Collins, CO, USA; Department of Civil and Environmental Engineering, University of Central Florida, Orlando, FL, USA; Center for Space and Remote Sensing Research, National Central University, Taoyuan, Taiwan; Department of Civil and Environmental Engineering, University of Central Florida, Orlando, FL, USA; Department of Ecosystem Science and Sustainability, Colorado State University, Fort Collins, CO, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Apr 2021","2021","14","","3345","3360","Current multimodal learning in smart feature extraction and classification has reshaped the landscape of remote sensing. The recent developments in smart feature extraction mainly rely on different machine learning and data mining algorithms as powerful classifiers to improve prediction accuracy. This article presents an innovative ensemble learning algorithm for integrated data and classifier fusion via higher order singular value decomposition (IDCF-HOSVD). Based on the fused data, different analytical, semiempirical, and empirical classifiers can be selected and applied to perform information retrieval that can be further synergized via a tensor flow based feature extraction scheme over the classifier space. When preserving core fused image patterns via HOSVD, the final step of IDCF-HOSVD helps rank the contributions from different classifiers via nonlinear correlation information entropy. Practical implementation of the IDCF-HOSVD algorithm was assessed through its application to map the seasonal water quality conditions in Lake Nicaragua, Central America.","2151-1535","","10.1109/JSTARS.2021.3055798","National Central University; University of Central Florida(grant numbers:105U-046G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345352","Classifier fusion;ensemble learning;higher order singular value decomposition (HOSVD);multimodal learning;water quality monitoring","Lakes;Feature extraction;Water quality;Remote sensing;Monitoring;Classification algorithms;Spatial resolution","data mining;feature extraction;geophysical image processing;image fusion;information retrieval;learning (artificial intelligence);remote sensing;singular value decomposition;tensors;water quality","semiempirical classifiers;core fused image patterns;machine learning;remote sensing;smart feature extraction;multimodal learning;data integration;seasonal water quality conditions;IDCF-HOSVD algorithm;tensor flow based feature extraction scheme;empirical classifiers;analytical classifiers;fused data;higher order singular value decomposition;classifier fusion;integrated data;innovative ensemble learning algorithm;data mining algorithms","","6","","55","CCBY","2 Feb 2021","","","IEEE","IEEE Journals"
"HAM-MFN: Hyperspectral and Multispectral Image Multiscale Fusion Network With RAP Loss","S. Xu; O. Amira; J. Liu; C. -X. Zhang; J. Zhang; G. Li","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Science and Technology Department, China Special Equipment Inspection and Research Institute, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2020","2020","58","7","4618","4628","The fusion of hyperspectral image (HSI) and multispectral image (MSI) is one of the most significant topics in remote sensing image processing. Recently, deep learning (DL) has emerged as an important tool for this task. However, existing DL-based methods have two drawbacks, that is, limited ability for feature extraction and suffering from spectral distortion. To address these issues, this article presents a novel neural network, where sophisticated techniques are employed, including network-in-network convolutional unit, batch normalization, and skip connection. To make full use of the MSI, the proposed model fuses HSI and MSI at different scales. Besides, this article presents a new loss function, called RMSE, angle and Laplacian (RAP) loss (the combination of the relative mean squared error, angle loss, and Laplacian loss), to deal with both spatial and spectral distortions. Experiments conducted on four data sets have verified the rationality of network structure and the proposed loss function and demonstrated that the proposed novel model outperforms state-of-the-art counterparts.","1558-0644","","10.1109/TGRS.2020.2964777","National Basic Research Program of China (973 Program)(grant numbers:2018AAA0102201,2018YFC0809001); National Natural Science Foundation of China(grant numbers:61877049,11991023,11671317,61976174); Fundamental Research Funds for the Central Universities(grant numbers:xzy022019059); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972602","Angle loss;convolutional neural network (CNN);hyperspectral image (HSI);image fusion;Laplacian loss;multispectral image (MSI)","Feature extraction;Distortion;Tensors;Neural networks;Laplace equations;Spatial resolution","convolutional neural nets;feature extraction;hyperspectral imaging;image fusion;learning (artificial intelligence);mean square error methods;remote sensing","HAM-MFN;hyperspectral image multiscale fusion network;multispectral image multiscale fusion network;MSI;remote sensing image processing;deep learning;feature extraction;spectral distortion;neural network;network-in-network convolutional unit;HSI;Laplacian loss;spatial distortions;RAP loss","","29","","34","IEEE","28 Jan 2020","","","IEEE","IEEE Journals"
"An Unsupervised Laplacian Pyramid Network for Radiometrically Accurate Data Fusion of Hyperspectral and Multispectral Imagery","S. Huang; D. W. Messinger","Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA; Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA","IEEE Transactions on Geoscience and Remote Sensing","5 May 2022","2022","60","","1","17","Improving the spatial resolution of hyperspectral images (HSIs) has traditionally been an important topic in the field of remote sensing. Many approaches have been proposed based on various theories, including component substitution, multiresolution analysis, spectral unmixing, Bayesian probability, and tensor representation. However, these methods have some common disadvantages such that their performance degrades dramatically as the up-scale ratio increases, and they have little concern for the per-pixel radiometric accuracy of the sharpened image. Moreover, many learning-based methods have been proposed through decades of innovations, but most of them require a large set of training pairs, which is unpractical for many real problems. To solve these problems, we propose a stable hyperspectral sharpening method based on the Laplacian pyramid and the generative convolutional neural network (CNN), which achieves superior radiometric accuracy of the sharpened data in different up-scale ratios based on one single input pair. First, with a low-resolution HSI (LR-HSI) and high-resolution multispectral image (HR-MSI) pair, the preliminary high-resolution HSI (HR-HSI) is calculated via linear regression. Then, the high-frequency details of the preliminary HR-HSI are estimated via the subtraction between it and the CNN-generated-blurry version. By injecting the details to the output of the generative CNN with the LR-HSI as input, the final HR-HSI is obtained. Nine different state-of-the-art sharpening methods are chosen as our baselines, and three different datasets with different scene content are tested. Furthermore, the target detection method, the adaptive coherence estimator (ACE), is conducted on the reconstructed HR-HSI to evaluate the per-pixel radiometric accuracy. The results demonstrate that the proposed method has the best and the most stable performance in terms of spectral and spatial accuracies.","1558-0644","","10.1109/TGRS.2022.3168511","academic grant from the National Geospatial-Intelligence Agency(grant numbers:HM0476-19-1-2007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760380","Convolutional neural network (CNN);hyperspectral sharpening;image fusion;Laplacian pyramid;super-resolution","Hyperspectral imaging;Tensors;Bayes methods;Spatial resolution;Radiometry;Superresolution;Laplace equations","Bayes methods;convolutional neural nets;geophysical image processing;geophysical techniques;hyperspectral imaging;image enhancement;image fusion;image resolution;image sensors;object detection;remote sensing","unsupervised Laplacian pyramid network;multispectral imagery;spatial resolution;hyperspectral images;HSIs;remote sensing;component substitution;multiresolution analysis;spectral unmixing;Bayesian probability;tensor representation;performance degrades;per-pixel radiometric accuracy;sharpened image;learning-based methods;training pairs;stable hyperspectral sharpening method;generative convolutional neural network;superior radiometric accuracy;sharpened data;up-scale ratios;single input pair;low-resolution HSI;LR-HSI;high-resolution multispectral image pair;high-resolution HSI;CNN-generated-blurry version;generative CNN;sharpening methods;scene content;target detection method;reconstructed HR-HSI;spectral accuracies;spatial accuracies;up-scale ratio","","2","","51","IEEE","20 Apr 2022","","","IEEE","IEEE Journals"
"Hyperspectral Intrinsic Image Decomposition With Enhanced Spatial Information","Y. Gu; W. Xie; X. Li; X. Jin","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","18 Mar 2022","2022","60","","1","14","Hyperspectral intrinsic image decomposition (HyperIID) has been proven to be a very useful approach to reduce the spectral uncertainty in the remote sensing imaging process and improve the classification. In this article, a new HyperIID with enhanced spatial information, called ESI-IID, is proposed to overcome the deficiency of low spatial resolution in the existing HyperIID methods. With the aid of high-resolution (HR) panchromatic (PAN) image, the proposed method embeds the HR spatial information into the intrinsic decomposition model and enhances spatial details of the intrinsic component. The proposed ESI-IID introduces three constraints: 1) we make the constraint on spectral information to protect it from distortion during the spatial resolution enhancement process; 2) we add the constraint on spatial information to make sure that the details of edges will be well kept; and 3) based on the assumption that the reflectance component has a strong correlation in the local neighborhood, we add the self-constraint on reflectance component, in which the similarity matrix consists of two parts extracted from hyperspectral images and PAN image, respectively. Finally, we build a matrix energy function according to the aforementioned constraints and solve it by finding the minimum Frobenius norm iteratively. Both visual and quantitative experiments on simulated and real datasets demonstrate that the proposed method outperforms other alternative methods with high reliability.","1558-0644","","10.1109/TGRS.2022.3146063","National Science Fund for Outstanding Young Scholars(grant numbers:62025107); National Natural Science Foundation of Key International Cooperation(grant numbers:61720106002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691328","Classification;hyperspectral images (HSIs);image fusion;intrinsic image decomposition (IID);spatial information","Spatial resolution;Hyperspectral imaging;Pansharpening;Feature extraction;Task analysis;Image decomposition;Computer vision","geophysical image processing;geophysical signal processing;image classification;image colour analysis;image enhancement;image fusion;image processing;image resolution;image sensors;iterative methods;matrix algebra;remote sensing","hyperspectral images;PAN image;hyperspectral intrinsic image decomposition;enhanced spatial information;remote sensing imaging process;called ESI-IID;low spatial resolution;existing HyperIID methods;high-resolution panchromatic image;HR spatial information;intrinsic decomposition model;enhances spatial details;intrinsic component;spectral information;spatial resolution enhancement process;reflectance component","","1","","67","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Sparse representation over shared coefficients in multispectral pansharpening","L. Chen; X. Zhang; H. Ma","Tsinghua University, Beijing, Beijing, CN; Tsinghua University, Beijing, Beijing, CN; Tsinghua University, Beijing, Beijing, CN","Tsinghua Science and Technology","2 Jul 2018","2018","23","3","315","322","The pansharpening process is for obtaining an enhanced image with both high spatial and high spectral resolutions by fusing a panchromatic (PAN) image and a low spatial resolution multispectral (MS) image. Sparse Principal Component Analysis (SPCA) method has been proposed as a pansharpening method, which utilizes sparse coefficients and over-complete dictionaries to represent the remote sensing data. However, this method still has some drawbacks, such as the existence of the block effect. In this paper, based on SPCA, we propose the Sparse over Shared Coefficients (SSC), in which patches are extracted with a sliding distance of 1 pixel from a PAN image, and the MS image shares the sparse representation coefficients trained from the PAN image independently. The fused high-resolution MS image is reconstructed by K-SVD algorithm and iterations, and residual compensation is applied when the down-sampling constraint is not satisfied. The simulated experiment results demonstrate that the proposed SSC method outperforms SPCA and improves the overall effectiveness.","1007-0214","","10.26599/TST.2018.9010088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8400433","","Dictionaries;Spatial resolution;Image reconstruction;Principal component analysis;Optimization;Remote sensing","geophysical image processing;image fusion;image representation;image resolution;principal component analysis;remote sensing;singular value decomposition","K-SVD algorithm;SSC method;high-resolution MS image;sparse representation coefficients;PAN image;remote sensing data;over-complete dictionaries;sparse coefficients;pansharpening method;SPCA;Sparse Principal Component Analysis method;low spatial resolution multispectral image;panchromatic image;high spectral resolutions;pansharpening process;multispectral pansharpening;Shared Coefficients","","4","","","","2 Jul 2018","","","TUP","TUP Journals"
"Toward an Improved Global Longwave Downward Radiation Product by Fusing Satellite and Reanalysis Data","S. Wang; T. Wang; W. Leng; G. Wang; H. Letu","School of Geospatial Engineering and Science, Sun Yat-sen University, Zhuhai, China; School of Geospatial Engineering and Science, Sun Yat-sen University and Southern Marine Science and Engineering Guangdong Laboratory (Zhuhai), Zhuhai, China; School of Geospatial Engineering and Science, Sun Yat-sen University, Zhuhai, China; School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China; State Key Laboratory of Remote Sensing Science, The Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","14 Jun 2022","2022","60","","1","16","Surface longwave downward radiation (LWDR) plays an important role in modulating greenhouse effect and climate change. Constructing a global longtime series LWDR dataset is greatly necessary to systematically and in-depth study the LWDR effect on the climate. However, the current multisource LWDR products (satellite and reanalysis) show large differences in terms of both spatiotemporal resolutions and accuracy in various regions. Therefore, it is necessary to fuse multisource datasets to generate more accurate LWDR with high spatiotemporal resolution on a global scale. To this end, a downscaling strategy is first proposed to generate LWDR dataset with 0.25° resolution from CERES-SYN data with 1° scale, by incorporating the land surface temperature (LST), total column water vapor (TCWV), and elevation. Then, a machine learning-based fusion method is provided to generate a global hourly LWDR dataset with a spatial resolution of 0.25° by combining three products (CERES-SYN, ERA5, and GLDAS). Compared with ground measurements, the performance of generated LWDR product reveals that the correlation coefficient ( $R$ ), mean bias error (BIAS), and root-mean-square error (RMSE) were 0.97, −0.95 W/m2, and 22.38 W/m2 over the land and 0.99, −0.88 W/m2, and 10.96 W/m2 over the ocean, respectively. In particular, it shows improved accuracy in the low and middle latitude regions compared with other LWDR products. Considering its better accuracy and higher spatiotemporal resolution, the new LWDR product can provide essential data for deeply understanding the global energy balance and even the global warming. Moreover, the proposed fusion strategy can be enlightening for readers in the fields of multisource data combination and big data analysis.","1558-0644","","10.1109/TGRS.2022.3179017","National Key Research and Development Program of China(grant numbers:2018YFA0605401); National Natural Science Foundation of China (NSFC)(grant numbers:42022008); Innovation Group Project of Southern Marine Science and Engineering Guangdong Laboratory (Zhuhai)(grant numbers:311021008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785652","CERES-SYN;data fusion;ERA5;GLDAS;machine learning;surface longwave downward radiation (LWDR)","Spatial resolution;Land surface temperature;Remote sensing;Machine learning;Ocean temperature;Fuses;Data models;Climate change;Global warming","atmospheric humidity;atmospheric radiation;atmospheric techniques;atmospheric temperature;climatology;data analysis;geophysical signal processing;global warming;image fusion;land surface temperature;mean square error methods;remote sensing;sensor fusion;sunlight","global energy balance;global warming;multisource data combination;fusing satellite;greenhouse effect;climate change;global longtime series LWDR dataset;LWDR effect;spatiotemporal resolutions;multisource datasets;high spatiotemporal resolution;CERES-SYN data;total column water vapor;machine learning-based fusion method;global hourly LWDR dataset;generated LWDR product;mean bias error;root-mean-square error;higher spatiotemporal resolution;global longwave downward radiation product","","","","49","IEEE","30 May 2022","","","IEEE","IEEE Journals"
"Temporal Shape-Based Fusion Method to Generate Continuous Vegetation Index at Fine Spatial Resolution","Y. Liu; X. Gu; T. Cheng; Y. Zhan; H. Zhang; J. Li; X. Wei; M. Gao; Q. Zhang; Y. Zhang","University of Chinese Academy of Sciences, Beijing, China; School of Remote Sensing and Information Engineering, North China Institute of Aerospace Engineering, Langfang, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; School of Geographic and Environmental Sciences, Tianjin Normal University, Tianjin, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","25 Oct 2022","2022","60","","1","14","In this study, a temporal shape-based fusion method (TSFM) using a spatially and temporally moving window is proposed to incorporate the time lag of fine- and coarse-resolution observations and to fully utilize target fine-resolution pixels and similar coarse-resolution pixels in the process. This method provides high-accuracy fused images with Pearson’s r of ~0.95, a root-mean-square error (RMSE) of ~0.04, and a bias of ~0.01 for commonly used fine spatial resolution satellites, including Landsat 7 and 8, Sentinel 2, and Gaofen 1, over different heterogeneous regions, such as urban, mountain, forest, and savanna regions. The fused fine-resolution enhanced vegetation index (EVI) time series using different fine spatial resolution satellite data as input are all highly correlated with the PhenoCam-monitored green chromatic coordinate (GCC), with no temporal lag. Compared with the commonly used data fusion method, this method provides equivalent and slightly higher accuracy because both neighboring similar pixels and the annual temporal variation are fully considered. This TSFM does not require each input fine-resolution image to be cloud-free; therefore, it can be used at a large spatial scale without further preprocessing and generates continuous datasets over a long-time range with only one input preparation process. The factors that could affect the method accuracy are the cloud detection accuracy of fine-resolution data and the temporal continuity of the coarse-resolution data. The method may also be used to produce spatially and temporally continuous surface reflectance and other surface reflectance-derived indices.","1558-0644","","10.1109/TGRS.2022.3211269","National Key Research and Development Program of China(grant numbers:2019YFE0127300,2020YFE0200700); National Natural Science Foundation of China(grant numbers:41901367); Major Special Project—the China High-Resolution Earth Observation System(grant numbers:30-Y30F06-9003-20/22); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9907037","Data fusion;spatially and temporally moving window;temporal shape;vegetation dynamic","Spatial resolution;Time series analysis;Vegetation mapping;Satellites;Data integration;Shape;Remote sensing","geophysical image processing;geophysical signal processing;image classification;image fusion;image resolution;remote sensing;sensor fusion;terrain mapping;time series;vegetation;vegetation mapping","root-mean-square error;different heterogeneous regions;fine-resolution enhanced vegetation index time series;different fine spatial resolution satellite data;temporal lag;commonly used data fusion method;neighboring similar pixels;annual temporal variation;input fine-resolution image;spatial scale;method accuracy;fine-resolution data;temporal continuity;coarse-resolution data;temporal shape-based fusion method;generate continuous vegetation index;time lag;coarse-resolution observations;target fine-resolution pixels;coarse-resolution pixels;high-accuracy fused images","","","","50","IEEE","3 Oct 2022","","","IEEE","IEEE Journals"
"A Novel Region-Based Image Registration Method for Multisource Remote Sensing Images Via CNN","L. Zeng; Y. Du; H. Lin; J. Wang; J. Yin; J. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Science and Technology on Information System Engineering Laboratory, China Electronics Technology Group Corporation, Nanjing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Jan 2021","2021","14","","1821","1831","The comprehensive utilization of images from various satellite sensors can significantly increase the performance of remote sensing applications and has, therefore, attracted extensive research attention. One of the essential challenges that research encounters comes from multisource image registration. This article proposes a novel region-based image registration method for multisource images. The proposed method exploits the region features of input images, which provide more consistent and common information of the multisource data. The image region features are extracted based on image semantic segmentation using the deep convolutional neural network approach. The final registration result is a pixel-level output corresponding to the input images. The proposed registration scheme overcomes the limits of traditional feature extraction methods (e.g., point feature) adopted in previous registration schemes. Results indicate that the proposed method has good performance for the multisource remote sensing image registration and can serve as a building block for the fusion of multisource images.","2151-1535","","10.1109/JSTARS.2020.3047656","National Natural Science Foundation of China(grant numbers:61771043,61701454); National Key Research and Development Program of China(grant numbers:2017YFB0502703); Nature Science Foundation for Young Scientists of Jiangsu Province(grant numbers:BK20160147); China Postdoctoral Science Foundation(grant numbers:2020M680554); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309405","Image registration;radar imaging","Image registration;Radar polarimetry;Feature extraction;Optical imaging;Optical sensors;Image segmentation;Synthetic aperture radar","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image fusion;image registration;image segmentation;remote sensing","region-based image registration method;multisource remote sensing images;multisource image registration;image semantic segmentation;CNN;image region features extraction;deep convolutional neural network","","4","","52","CCBY","28 Dec 2020","","","IEEE","IEEE Journals"
"A New Coregistration Algorithm for Recent Applications on Urban SAR Images","A. Plyer; E. Colin-Koeniguer; F. Weissgerber","Département Traitement de l'Information et Modélisation, ONERA, Palaiseau Cedex, France; Département Traitement de l'Information et Modélisation, ONERA, Palaiseau Cedex, France; Département Traitement du Signal et des Images, Télécom ParisTech, France, France","IEEE Geoscience and Remote Sensing Letters","4 Nov 2015","2015","12","11","2198","2202","In this letter, a fast and robust optical-flow estimation algorithm is investigated for synthetic aperture radar (SAR) images' coregistration. The principle of the initial algorithm is described, as well as its adaptation to the case of radar images. A performance evaluation method is proposed to fix the choice of the parameters of the algorithm. Promising results in change detection or interferometry between SAR images of different resolutions are presented. They offer the opportunity to use this kind of algorithm in the case of high-resolution images containing many structural elements as in urban areas.","1558-0571","","10.1109/LGRS.2015.2455071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202849","Image fusion;image registration;radar interferometry;synthetic aperture radar (SAR);Image fusion;image registration;radar interferometry;synthetic aperture radar (SAR)","Synthetic aperture radar;Image resolution;Optical imaging;Optical sensors;Optical interferometry;Robustness","image registration;remote sensing by radar;synthetic aperture radar","urban area;high-resolution images;interferometry;change detection;synthetic aperture radar image coregistration;optical-flow estimation algorithm;urban SAR images;coregistration algorithm","","33","","9","IEEE","14 Aug 2015","","","IEEE","IEEE Journals"
"Vision Transformer for Pansharpening","X. Meng; N. Wang; F. Shao; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","2 May 2022","2022","60","","1","11","Pansharpening is a fundamental and hot-spot research topic in remote sensing image fusion. In recent years, self-attention-based transformer has attracted considerable attention in natural language processing (NLP) and introduced to attend to computer vision (CV) tasks. Inspired by great success of the vision transformer (ViT) in image classification, we propose an improved and advanced purely transformer-based model for pansharpening. In the proposed method, stacked multispectral (MS) and panchromatic (PAN) images are cropped into patches (i.e., tokens), and after a three-layer self-attention-based encoder, these tokens contain rich information. After upsampled and stitched, a high spatial resolution (HR) MS image is finally obtained. Instead of convolutional neural networks (CNNs) pursuing a short-distance dependency, our proposed method aims to build up a long-distance dependency, to make full use of more useful features. The experiments were conducted on an opening benchmark dataset, including IKONOS with four-band MS/PAN images and WorldView-2 MS images featured by eight bands. In addition, the experiments were performed on reduced and full-resolution datasets from both qualitative and quantitative evaluation aspects. The experimental results indicate the competitive performance of the proposed model than other pansharpening methods, including the state-of-the-art pansharpening algorithms based on CNN.","1558-0644","","10.1109/TGRS.2022.3168465","National Natural Science Foundation of China(grant numbers:42171326,62071261,41801252); Fellowship of the China Postdoctoral Science Foundation(grant numbers:2020M672490); Natural Science Foundation of Zhejiang Province(grant numbers:LY22F010014); K. C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9759519","Deep learning (DL);pansharpening;self-attention;transformer","Transformers;Pansharpening;Lenses;Task analysis;Wavelet transforms;Training;Standards","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image resolution;natural language processing;remote sensing;spectral analysis","pansharpening methods;pansharpening algorithms;vision transformer;hot-spot research topic;remote sensing image fusion;self-attention-based transformer;natural language processing;computer vision tasks;image classification;transformer-based model;three-layer self-attention-based encoder;high spatial resolution MS image;convolutional neural networks;short-distance dependency;long-distance dependency;WorldView-2 MS images;stacked multispectral;panchromatic images;IKONOS;four-band MS-PAN images","","7","","65","IEEE","19 Apr 2022","","","IEEE","IEEE Journals"
"A Multiresolution Details Enhanced Attentive Dual-UNet for Hyperspectral and Multispectral Image Fusion","J. Fang; J. Yang; A. Khader; L. Xiao","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Dec 2022","2023","16","","638","655","The fusion-based super-resolution of hyperspectral images (HSIs) draws more and more attention in order to surpass the hardware constraints intrinsic to hyperspectral imaging systems in terms of spatial resolution. Low-resolution (LR)-HSI is combined with a high-resolution multispectral image (HR-MSI) to achieve HR-HSI. In this article, we propose multiresolution details enhanced attentive dual-UNet to improve the spatial resolution of HSI. The entire network contains two branches. The first branch is the wavelet detail extraction module, which performs discrete wavelet transform on MSI to extract spatial detail features and then passes through the encoding–decoding. Its main purpose is to extract the spatial features of MSI at different scales. The latter branch is the spatio-spectral fusion module, which aims to inject the detail features of the wavelet detail extraction network into the HSI to reconstruct the HSI better. Moreover, this network uses an asymmetric feature selective attention model to focus on important features at different scales. Extensive experimental results on both simulated and real data show that the proposed network architecture achieves the best performance compared with several leading HSI super-resolution methods in terms of qualitative and quantitative aspects.","2151-1535","","10.1109/JSTARS.2022.3228941","National Natural Science Foundation of China(grant numbers:61871226,61571230,62001226); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200465); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9983482","Attention mechanism;discrete wavelet transform;hyperspectral image (HSI);multiscale;UNet","Feature extraction;Data mining;Decoding;Discrete wavelet transforms;Tensors;Image coding;Hyperspectral imaging","discrete wavelet transforms;feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;neural nets;remote sensing","asymmetric feature selective attention model;fusion-based super-resolution;hardware constraints;high-resolution multispectral image fusion;HR-MSI;HSI super-resolution methods;hyperspectral imaging systems;multiresolution details enhanced attentive Dual-UNet;network architecture;spatial detail features;spatial resolution;spatio-spectral fusion module;wavelet detail extraction module;wavelet detail extraction network","","","","59","CCBY","13 Dec 2022","","","IEEE","IEEE Journals"
"Detecting Changes Between Optical Images of Different Spatial and Spectral Resolutions: A Fusion-Based Approach","V. Ferraris; N. Dobigeon; Q. Wei; M. Chabert","IRIT/INP-ENSEEIHT, University of Toulouse, Toulouse, France; IRIT/INP-ENSEEIHT, University of Toulouse, Toulouse, France; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; IRIT/INP-ENSEEIHT, University of Toulouse, Toulouse, France","IEEE Transactions on Geoscience and Remote Sensing","27 Feb 2018","2018","56","3","1566","1578","Change detection (CD) is one of the most challenging issues when analyzing remotely sensed images. Comparing several multidate images acquired through the same kind of sensor is the most common scenario. Conversely, designing robust, flexible, and scalable algorithms for CD becomes even more challenging when the images have been acquired by two different kinds of sensors. This situation arises in the case of emergency under critical constraints. This paper presents, to the best of our knowledge, the first strategy to deal with optical images characterized by dissimilar spatial and spectral resolutions. Typical considered scenarios include CD between panchromatic, multispectral, and hyperspectral images. The proposed strategy consists of a three-step procedure: 1) inferring a high spatial and spectral resolution image by fusion of the two observed images characterized one by a low spatial resolution and the other by a low spectral resolution; 2) predicting two images with, respectively, the same spatial and spectral resolutions as the observed images by the degradation of the fused one; and 3) implementing a decision rule to each pair of observed and predicted images characterized by the same spatial and spectral resolutions to identify changes. To quantitatively assess the performance of the method, an experimental protocol is specifically designed, relying on synthetic yet physically plausible change rules applied to real images. The accuracy of the proposed framework is finally illustrated on real images.","1558-0644","","10.1109/TGRS.2017.2765348","Coordenação de Aperfeiçoamento de Ensino Superior, Brazil; EU FP7 through the ERANETMED JC-WATER Program, MapInvPlnt(grant numbers:ANR-15-NMED-0002-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8098550","Change detection (CD);different resolution;hyperspectral (HS) imagery;image fusion;multispectral (MS) imagery","Spatial resolution;Optical sensors;Optical imaging;Image sensors;Sensor phenomena and characterization","hyperspectral imaging;image fusion;image resolution;remote sensing","remotely sensed images;optical images;spectral resolutions;panchromatic images;hyperspectral images;high spatial resolution image;fusion-based approach;change detection;multispectral images","","32","","42","IEEE","7 Nov 2017","","","IEEE","IEEE Journals"
"Fast and High-Quality Blind Multi-Spectral Image Pansharpening","L. Yu; D. Liu; H. Mansour; P. T. Boufounos","Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA; Signal Processing Group, Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Signal Processing Group, Mitsubishi Electric Research Laboratories, Cambridge, MA, USA; Signal Processing Group, Mitsubishi Electric Research Laboratories, Cambridge, MA, USA","IEEE Transactions on Geoscience and Remote Sensing","14 Jan 2022","2022","60","","1","17","Blind pansharpening addresses the problem of generating a high spatial-resolution multi-spectral (HRMS) image given a low spatial-resolution multi-spectral (LRMS) image with the guidance of its associated spatially misaligned high spatial-resolution panchromatic (PAN) image without parametric side information. In this article, we propose a fast approach to blind pansharpening and achieve the state-of-the-art image reconstruction quality. Typical blind pansharpening algorithms are often computationally intensive since the blur kernel and the target HRMS image are often computed using iterative solvers and in an alternating fashion. To achieve fast blind pansharpening, we decouple the solution of the blur kernel and of the HRMS image. First, we estimate the blur kernel by computing the kernel coefficients with minimum total generalized variation that blur a downsampled version of the PAN image to approximate a linear combination of the LRMS image channels. Then, we estimate each channel of the HRMS image using local Laplacian prior (LLP) to regularize the relationship between each HRMS channel and the PAN image. Solving the HRMS image is accelerated by both parallelizing across the channels and by fast numerical algorithms for each channel. Due to the fast scheme and the powerful priors we used on the blur kernel coefficients (total generalized variation) and on the cross-channel relationship (LLP), numerical experiments demonstrate that our algorithm outperforms the state-of-the-art model-based counterparts in terms of both computational time and reconstruction quality of the HRMS images.","1558-0644","","10.1109/TGRS.2021.3091329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9491792","Blind image fusion;local Laplacian prior (LLP);pansharpening;total generalized variation","Kernel;Pansharpening;Numerical models;Channel estimation;Spatial resolution;Optimization;Convolution","computational complexity;geophysical image processing;image fusion;image reconstruction;image resolution;image restoration;iterative methods;remote sensing","low spatial-resolution multispectral image;image reconstruction quality;HRMS image;PAN image;LRMS image;blur kernel coefficients;fast blind multispectral image pansharpening;high spatial-resolution multispectral image;low spatial-resolution panchromatic image;high spatial-resolution panchromatic image;local Laplacian prior;LLP;computational time;high-quality blind multispectral image pansharpening;blind image fusion;total generalized variation","","","","34","IEEE","20 Jul 2021","","","IEEE","IEEE Journals"
"Matrix-Based Discriminant Subspace Ensemble for Hyperspectral Image Spatial–Spectral Feature Fusion","R. Hang; Q. Liu; H. Song; Y. Sun","Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","19 Jan 2016","2016","54","2","783","794","Spatial-spectral feature fusion is well acknowledged as an effective method for hyperspectral (HS) image classification. Many previous studies have been devoted to this subject. However, these methods often regard the spatial-spectral high-dimensional data as 1-D vector and then extract informative features for classification. In this paper, we propose a new HS image classification method. Specifically, matrix-based spatial-spectral feature representation is designed for each pixel to capture the local spatial contextual and the spectral information of all the bands, which can well preserve the spatial-spectral correlation. Then, matrix-based discriminant analysis is adopted to learn the discriminative feature subspace for classification. To further improve the performance of discriminative subspace, a random sampling technique is used to produce a subspace ensemble for final HS image classification. Experiments are conducted on three HS remote sensing data sets acquired by different sensors, and experimental results demonstrate the efficiency of the proposed method.","1558-0644","","10.1109/TGRS.2015.2465899","National Natural Science Foundation of China(grant numbers:61272223,61300162,61532009,41501377); Foundation of Jiangsu Province of China(grant numbers:BK2012045,BK20131003,15KJA520001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7270287","Contextual spatial–spectral feature fusion;hyperspectral (HS) image classification;matrix-based discriminant analysis (MDA);random sampling;support vector machine (SVM);Contextual spatial–spectral feature fusion;hyperspectral (HS) image classification;matrix-based discriminant analysis (MDA);random sampling;support vector machine (SVM)","Feature extraction;Training;Support vector machines;Remote sensing;Kernel;Sensors;Spatial resolution","correlation methods;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;image sampling;image sensors;matrix algebra;random processes;remote sensing","matrix-based discriminant subspace ensemble;hyperspectral image spatial-spectral feature fusion;HS image classification;1D vector;informative feature extraction;matrix-based spatial-spectral feature representation;spatial-spectral correlation;random sampling technique;HS remote sensing","","81","","42","IEEE","16 Sep 2015","","","IEEE","IEEE Journals"
"Attention GANs: Unsupervised Deep Feature Learning for Aerial Scene Classification","Y. Yu; X. Li; F. Liu","Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","27 Dec 2019","2020","58","1","519","531","With the development of deep learning, supervised feature learning methods have achieved prominent performance in the field of aerial scene classification. However, supervised feature learning methods require a large amount of labeled training data. To address this limitation, in this article, a novel unsupervised deep feature learning method, namely, Attention generative adversarial networks (Attention GANs), is proposed for aerial scene classification. First, Attention GANs integrates the attention mechanism into GANs to enhance the representation power of the discriminator. Then, to obtain contextual information, a context-aggregation-based feature fusion architecture is designed in the discriminator. Furthermore, the generator and discriminator losses are improved on basis of the Relativistic GAN. At the same time, a content loss is formed by using the feature representations from the context-aggregation-based feature fusion architecture. In the experiments, our Attention GANs is evaluated via comprehensive experiments with four publicly available remote sensing scene data sets, i.e., the UC-Merced data set with 21 scene classes, the RSSCN7 data set with 7 scene classes, the AID data set with 30 scene classes, and the NWPU-RESISC45 data set with 45 scene classes. Experimental results demonstrate that our Attention GANs can obtain the best performance compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2019.2937830","National Natural Science Foundation of China(grant numbers:71771216,71701209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842616","Aerial scene classification;attention mechanism;context aggregation;generative adversarial networks (GANs);unsupervised deep feature learning","Learning systems;Gallium nitride;Feature extraction;Generators;Task analysis;Remote sensing;Generative adversarial networks","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);remote sensing;unsupervised learning","AID data set;RSSCN7 data set;NWPU-RESISC45 data set;UC-Merced data set;attention GAN;scene classes;attention generative adversarial networks;deep feature learning method;learning methods;aerial scene classification;unsupervised deep feature learning;publicly available remote sensing scene data sets;feature representations;relativistic GAN;context-aggregation-based;attention mechanism","","62","","77","IEEE","17 Sep 2019","","","IEEE","IEEE Journals"
"Sentinel-2 Sharpening Using a Reduced-Rank Method","M. O. Ulfarsson; F. Palsson; M. Dalla Mura; J. R. Sveinsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Tokyo Tech World Research Hub Initiative (WRHI), School of Computing, Tokyo Institute of Technology, Tokyo, Japan; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","6408","6420","Recently, the Sentinel-2 (S2) satellite constellation was deployed for mapping and monitoring the Earth environment. Images acquired by the sensors mounted on the S2 platforms have three levels of spatial resolution: 10, 20, and 60 m. In many remote sensing applications, the availability of images at the highest spatial resolution (i.e., 10 m for S2) is often desirable. This can be achieved by generating a synthetic high-resolution image through data fusion. To this end, researchers have proposed techniques exploiting the spectral/spatial correlation inherent in multispectral data to sharpen the lower resolution S2 bands to 10 m. In this paper, we propose a novel method that formulates the sharpening process as a solution to an inverse problem. We develop a cyclic descent algorithm called S2Sharp and an associated tuning parameter selection algorithm based on generalized cross validation and Bayesian optimization. The tuning parameter selection method is evaluated on a simulated data set. The effectiveness of S2Sharp is assessed experimentally by comparisons to state-of-the-art methods using both simulated and real data sets.","1558-0644","","10.1109/TGRS.2019.2906048","Research Fund of the University of Iceland; Icelandic Research Fund(grant numbers:174075-05); Labex(grant numbers:PNTS-2016-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8694937","Cyclic descent (CD);data fusion;image sharpening;Sentinel-2 (S2) constellation;superresolution","Spatial resolution;Image sensors;Optimization;Thermal sensors;Remote sensing","Bayes methods;geophysical image processing;gradient methods;image fusion;image resolution;inverse problems;remote sensing","reduced-rank method;Sentinel-2 satellite constellation;Earth environment;remote sensing applications;data fusion;multispectral data;S2Sharp;tuning parameter selection method;Sentinel-2 sharpening;spatial resolution;tuning parameter selection algorithm;inverse problem;cyclic descent algorithm;Bayesian optimization","","27","","48","IEEE","22 Apr 2019","","","IEEE","IEEE Journals"
"cuFSDAF: An Enhanced Flexible Spatiotemporal Data Fusion Algorithm Parallelized Using Graphics Processing Units","H. Gao; X. Zhu; Q. Guan; X. Yang; Y. Yao; W. Zeng; X. Peng","National Engineering Research Center of GIS, China University of Geosciences, Wuhan, China; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong; National Engineering Research Center of GIS, China University of Geosciences, Wuhan, China; National Engineering Research Center of GIS, China University of Geosciences, Wuhan, China; National Engineering Research Center of GIS, China University of Geosciences, Wuhan, China; National Engineering Research Center of GIS, China University of Geosciences, Wuhan, China; National Engineering Research Center of GIS, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","4 Jan 2022","2022","60","","1","16","Spatiotemporal data fusion is a cost-effective way to produce remote sensing images with high spatial and temporal resolutions using multisource images. Using spectral unmixing analysis and spatial interpolation, the flexible spatiotemporal data fusion (FSDAF) algorithm is suitable for heterogeneous landscapes and capable of capturing abrupt land-cover changes. However, the extensive computational complexity of FSDAF prevents its use in large-scale applications and mass production. Besides, the domain decomposition strategy of FSDAF causes accuracy loss at the edges of subdomains due to the insufficient consideration of edge effects. In this study, an enhanced FSDAF (cuFSDAF) is proposed to address these problems, and includes three main improvements. First, the TPS interpolator is replaced by an accelerated inverse distance weighted (IDW) interpolator to reduce computational complexity. Second, the algorithm is parallelized based on the compute unified device architecture (CUDA), a widely used parallel computing framework for graphics processing units (GPUs). Third, an adaptive domain decomposition (ADD) method is proposed to improve the fusion accuracy at the edges of subdomains and to enable GPUs with varying computing capacities to deal with datasets of any size. Experiments showed while obtaining similar accuracies to FSDAF and an up-to-date deep-learning-based method, cuFSDAF reduced the computing time significantly and achieved speed-ups of 140.3–182.2 over the original FSDAF program. cuFSDAF is capable of efficiently producing fused images with both high spatial and temporal resolutions to support applications for large-scale and long-term land surface dynamics. Source code and test data available at https://github.com/HPSCIL/cuFSDAF.","1558-0644","","10.1109/TGRS.2021.3080384","National Key Research and Development Program of China(grant numbers:2019YFB2102903); National Natural Science Foundation of China(grant numbers:41801306,42022060,U1711267); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440781","Compute unified device architecture (CUDA);multisource satellite images;parallel computing;spatiotemporal data fusion","Spatial resolution;Spatiotemporal phenomena;Remote sensing;Graphics processing units;Land surface;Data integration;Sensors","computational complexity;geophysical image processing;geophysical techniques;geophysics computing;graphics processing units;image fusion;image resolution;interpolation;learning (artificial intelligence);parallel architectures;remote sensing;sensor fusion;terrain mapping","edge effects;enhanced FSDAF;TPS interpolator;compute unified device architecture;widely used parallel computing framework;adaptive domain decomposition method;fusion accuracy;computing capacities;up-to-date deep-learning-based method;original FSDAF program;high spatial resolutions;temporal resolutions;long-term land surface dynamics;test data;enhanced flexible spatiotemporal data fusion algorithm parallelized;graphics processing units;remote sensing images;multisource images;spectral unmixing analysis;spatial interpolation;abrupt land-cover changes;extensive computational complexity;domain decomposition strategy;accuracy loss","","9","","84","IEEE","25 May 2021","","","IEEE","IEEE Journals"
"MTF-Based Deblurring Using a Wiener Filter for CS and MRA Pansharpening Methods","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson; J. A. Benediktsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","6","2255","2269","Pansharpening is the fusion of low-resolution multispectral (MS) images and high-resolution panchromatic (PAN) images to yield a high-resolution MS image. The component substitution (CS) and multiresolution analysis (MRA) methods are usually computationally efficient, making them able to handle large datasets. However, these methods often produce images that suffer from spectral and spatial distortions. The CS and MRA methods can be described using general injection schemes where details extracted from the PAN image, modulated by a band-dependent gain constant, are added to the MS image, which has been interpolated to the size of the PAN image. In this paper, we propose a simple modification of these schemes where the interpolated MS image is replaced by its deblurred version, where the deblurring kernel is matched to the modulation transfer function (MTF) of the MS sensor. This can significantly enhance the quality of the fused image. Using two real datasets and one simulated dataset, our experimental results show that using the proposed preprocessing method can significantly increase both the spectral and spatial quality of the fused image according to quantitative quality metrics.","2151-1535","","10.1109/JSTARS.2016.2546061","Doctoral Grants of the University of Iceland Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458808","Component substitution (CS);deblurring;image fusion;modulation transfer function (MTF);multiresolution analysis (MRA);pansharpening;Wiener filter","Spatial resolution;Kernel;Multiresolution analysis;Measurement;Fourier transforms;Image edge detection","geophysical image processing;geophysical techniques;image fusion;image resolution;image restoration;interpolation;optical transfer function;remote sensing;Wiener filters","low-resolution multispectral image fusion;high-resolution panchromatic image fusion;high-resolution multispectral image;component substitution method;multiresolution analysis method;spectral distortion;spatial distortion;general injection schemes;PAN image;band-dependent gain constant;MS image;deblurred version;deblurring kernel;modulation transfer function;multispectral sensor;preprocessing method;simulated dataset;spectral quality;spatial quality;quantitative quality metrics;MTF-based deblurring;Wiener filter;CS pansharpening method;MRA pansharpening method","","39","","52","IEEE","25 Apr 2016","","","IEEE","IEEE Journals"
"Sentinel-1-Based Water and Flood Mapping: Benchmarking Convolutional Neural Networks Against an Operational Rule-Based Processing Chain","M. Helleis; M. Wieland; C. Krullikowski; S. Martinis; S. Plank","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","8 Mar 2022","2022","15","","2023","2036","In this study, the effectiveness of several convolutional neural network architectures (AlbuNet-34/FCN/DeepLabV3+/U-Net/U-Net++) for water and flood mapping using Sentinel-1 amplitude data is compared to an operational rule-based processor (S-1FS). This comparison is made using a globally distributed dataset of Sentinel-1 scenes and the corresponding ground truth water masks derived from Sentinel-2 data to evaluate the performance of the classifiers on a global scale in various environmental conditions. The impact of using single versus dual-polarized input data on the segmentation capabilities of AlbuNet-34 is evaluated. The weighted cross entropy loss is combined with the Lovász loss and various data augmentation methods are investigated. Furthermore, the concept of atrous spatial pyramid pooling used in DeepLabV3+ and the multiscale feature fusion inherent in U-Net++ are assessed. Finally, the generalization capacity of AlbuNet-34 is tested in a realistic flood mapping scenario by using additional data from two flood events and the Sen1Floods11 dataset. The model trained using dual polarized data outperforms the S-1FS significantly and increases the intersection over union (IoU) score by 5%. Using a weighted combination of the cross entropy and the Lovász loss increases the IoU score by another 2%. Geometric data augmentation degrades the performance while radiometric data augmentation leads to better testing results. FCN/DeepLabV3+/U-Net/U-Net++ perform not significantly different to AlbuNet-34. Models trained on data showing no distinct inundation perform very well in mapping the water extent during two flood events, reaching IoU scores of 0.96 and 0.94, respectively, and perform comparatively well on the Sen1Floods11 dataset.","2151-1535","","10.1109/JSTARS.2022.3152127","German Federal Ministry of Education and Research; Künstliche Intelligenz zur Analyse von Erdbeobachtungs- und Internetdaten zur Entscheidungsunterstützung im Katastrophenfall(grant numbers:13N15525); Helmholtz Artificial Intelligence Cooperation Unit; AI for Near Real Time Satellite-based Flood Response(grant numbers:ZT-IPF-5-39); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714780","Convolutional neural networks;data augmentation;semantic segmentation;Sen1Floods11;Sentinel-1;Sentinel-2;surface water monitoring","Floods;Data models;Synthetic aperture radar;Convolutional neural networks;Training;Sea surface;Thresholding (Imaging)","convolutional neural nets;emergency management;entropy;feature extraction;floods;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;remote sensing","convolutional neural networks;operational rule-based processing chain;operational rule-based processor;S-1FS;globally distributed dataset;geometric data augmentation;Sentinel-1-based water mapping;flood mapping;AlbuNet-34;FCN;DeepLabV3+;U-Net;U-Net++;ground truth water masks;environmental conditions;dual-polarized input data;single-polarized input data;spatial pyramid pooling;multiscale feature fusion","","5","","72","CCBY","16 Feb 2022","","","IEEE","IEEE Journals"
"MMDN: Multi-Scale and Multi-Distillation Dilated Network for Pansharpening","W. Tu; Y. Yang; S. Huang; W. Wan; L. Gan; H. Lu","School of Mathematics and Computer Science, Jiangxi Science and Technology Normal University, Nanchang, China; School of Computer Science and Technology, Tiangong University, Tianjin, China; School of Software, Tiangong University, Tianjin, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Mathematics and Computer Science, Jiangxi Science and Technology Normal University, Nanchang, China; Key Laboratory of Crop Harvesting Equipment Technology of Zhejiang Province and the College of Information Engineering, Jinhua Polytechnic, Jinhua, China","IEEE Transactions on Geoscience and Remote Sensing","14 Jun 2022","2022","60","","1","14","Pansharpening is a technology involving information integration and processing in remote sensing imagery. It is applied to generate a high-resolution multispectral (HRMS) image through an effective fusion of a low spatial resolution multispectral image and a panchromatic (PAN) image. In this article, we propose an end-to-end multiscale and multidistillation dilated network (MMDN) for pansharpening. In MMDN, to extract more abundant spatial details from source images, a clique structure-based multiscale dilated block (CSMDB) is presented. The clique structure in CSMDB can fully transfer the information between feature maps obtained by the multiscale dilated convolutional filters. Then, a multidistillation residual information block (MRIB) is constructed to help the network capture the spatial structure of different scales in MS and PAN images. Finally, to reuse and supplement the feature information, a feature embedding strategy is designed by feeding the sum result of the output of cascaded CSMDBs and the shallow features to each MRIB. Experimental results verify that the proposed MMDN outperforms other compared state-of-the-art approaches in terms of objective and subjective evaluations.","1558-0644","","10.1109/TGRS.2022.3179449","National Natural Science Foundation of China(grant numbers:62072218,61862030,61662026); Project of the Education Department of Jiangxi Province(grant numbers:GJJ150819); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785810","Deep learning (DL);multidistillation residual information block (MRIB);multiscale dilated block;pansharpening","Feature extraction;Pansharpening;Convolution;Spatial resolution;Hidden Markov models;Remote sensing;Data mining","feature extraction;geophysical image processing;image fusion;image resolution;remote sensing","MMDN;multidistillation dilated network;pansharpening;information integration;remote sensing imagery;high-resolution multispectral image;effective fusion;low spatial resolution multispectral image;panchromatic image;PAN;end-to-end multiscale;abundant spatial details;source images;clique structure-based multiscale dilated block;CSMDB;feature maps;multiscale dilated convolutional filters;multidistillation residual information block;network capture;spatial structure;feature information;feature embedding strategy;shallow features","","3","","39","IEEE","30 May 2022","","","IEEE","IEEE Journals"
"Multisource Data Fusion and Fisher Criterion-Based Nearest Feature Space Approach to Landslide Classification","Y. -L. Chang; Y. C. Wang; Y. -S. Fu; C. -C. Han; J. Chanussot; B. Huang","Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Electrical Engineering, National Taipei University of Technology, Taipei, Taiwan; Department of Computer Science and Information Engineering, National United University, Miaoli, Taiwan; Department for Signals and Images of the Grenoble-Image-sPeech-Signal-Automatics Laboratory, Grenoble Institute of Technology, Grenoble, France; Space Science and Engineering Center, University of Wisconsin-Madison, Madison, WI, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","2","576","588","In this paper, a novel technique known as the Fisher criterion-based nearest feature space (FCNFS) approach is proposed for supervised classification of multisource images for the purpose of landslide hazard assessment. The method is developed for land cover classification based upon the fusion of remotely sensed images of the same scene collected from multiple sources. This paper presents a framework for data fusion of multisource remotely sensed images, consisting of two approaches: 1) the band generation process(BGP); and 2) the FCNFS classifier. We propose the BGP to create a new set of additional bands that are specifically accommodated to the landslide class and are extracted from the original multisource images. In comparison to the original nearest feature space (NFS) method, the proposed improved FCNFS classifier uses the Fisher criterion of between-class and within-class discrimination to enhance the classifier. In the training phase, the labeled samples are discriminated by the Fisher criterion, which can be treated as a preprocessing step of the NFS method. After completion of the training, the classification results can be obtained from the NFS algorithm. In order for the proposed FCNFS to be effective for multispectral images, a multiple adaptive BGP is introduced to create an additional set of bands specially accommodated to landslide classes. Experimental results show that the proposed BGP/FCNFS framework is suitable for land cover classification in Earth remote sensing and improves the classification accuracy compared to conventional classifiers.","2151-1535","","10.1109/JSTARS.2014.2334636","Ministry of Science and Technology(grant numbers:NSC 102-2116-M-027-003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871324","Band generation process (BGP);classification;Fisher criterion;landslide;multisource data fusion;nearest feature space (NFS);Band generation process (BGP);classification;Fisher criterion;landslide;multisource data fusion;nearest feature space (NFS)","Terrain factors;Accuracy;Remote sensing;Training;Data integration;Earth;Image resolution","feature extraction;geomorphology;geophysical image processing;hazards;image capture;image classification;image fusion;land cover;remote sensing","multisource data fusion;Fisher criterion-based nearest feature space approach;landslide classification;FCNFS approach;multisource image supervised classification;landslide hazard assessment;land cover classification;remotely sensed image fusion;band generation process;FCNFS classifier;between-class discrimination;within-class discrimination;classifier enhancement;NFS algorithm;multispectral images;multiple adaptive BGP;Earth remote sensing;Taiwan","","10","","60","IEEE","5 Aug 2014","","","IEEE","IEEE Journals"
"Physics-Based GAN With Iterative Refinement Unit for Hyperspectral and Multispectral Image Fusion","J. Xiao; J. Li; Q. Yuan; M. Jiang; L. Zhang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Jul 2021","2021","14","","6827","6841","Hyperspectral image (HSI) fusion can effectively improve the spatial resolution of HSIs by integrating high-resolution multispectral images (MSIs). Considering the spatial and spectral degradation relationship between a fused image and input images, a physics-based GAN is proposed to fuse HSI and MSI. A physical model estimating degradation of image is introduced in the generator and in the discriminators. For the generator, a set of recursive modules including a physical degradation model and a multiscale residual channel attention fusion module integrate the spectral-spatial difference information between input images and estimated degradation images to restore the details of the fused image. Subsequently, the residual spatial attention fusion module is used to combine the results of all recursions to obtain the final reconstructed result. As for the discriminators, three networks with the final fused image, estimated LR HSI and estimated MSI as inputs share the same architecture. Finally, the loss function that contains adversarial losses and L1 losses of the fused image and estimated degradation images is used to optimize network parameters. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2021.3075727","National Natural Science Foundation of China(grant numbers:62071341,41922008,61971319); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435191","Attention module;GAN;hyperspectral image (HSI);physics model","Degradation;Mathematical model;Generators;Feature extraction;Physics;Training;Generative adversarial networks","geophysical image processing;hyperspectral imaging;image fusion;image restoration;iterative methods;neural nets","spatial degradation relationship;spectral degradation relationship;physics-based GAN;physical degradation model;multiscale residual channel attention fusion module;spectral-spatial difference information;residual spatial attention fusion module;iterative refinement unit;hyperspectral image fusion;high-resolution multispectral imaging;LR HSI estimation;degradation image estimation","","6","","64","CCBY","19 May 2021","","","IEEE","IEEE Journals"
"Target-Adaptive CNN-Based Pansharpening","G. Scarpa; S. Vitale; D. Cozzolino","Department of Electrical Engineering and Information Technology, University Federico II of Naples, Naples, Italy; Engineering Department, Parthenope University of Naples, Naples, Italy; Department of Electrical Engineering and Information Technology, University Federico II of Naples, Naples, Italy","IEEE Transactions on Geoscience and Remote Sensing","26 Aug 2018","2018","56","9","5443","5457","We recently proposed a convolutional neural network (CNN) for remote sensing image pansharpening obtaining a significant performance gain over the state of the art. In this paper, we explore a number of architectural and training variations to this baseline, achieving further performance gains with a lightweight network that trains very fast. Leveraging on this latter property, we propose a target-adaptive usage modality that ensures a very good performance also in the presence of a mismatch with respect to the training set and even across different sensors. The proposed method, published online as an off-the-shelf software tool, allows users to perform fast and high-quality CNN-based pansharpening of their own target images on general-purpose hardware.","1558-0644","","10.1109/TGRS.2018.2817393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8334206","Convolutional neural networks (CNN);image classification;image fusion;image resolution;object analysis;Urban areas","Training;Sensors;Spatial resolution;Transforms;Protocols;Remote sensing","cellular neural nets;image processing;remote sensing;software tools","convolutional neural network;remote sensing image pansharpening;lightweight network;target-adaptive usage modality;target-adaptive CNN;CNN-based pansharpening;software tool","","223","","75","IEEE","9 Apr 2018","","","IEEE","IEEE Journals"
"Multi-Attention Object Detection Model in Remote Sensing Images Based on Multi-Scale","X. Ying; Q. Wang; X. Li; M. Yu; H. Jiang; J. Gao; Z. Liu; R. Yu","College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; OpenBayes (Tianjin) IT Company Ltd., Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China","IEEE Access","25 Jul 2019","2019","7","","94508","94519","Ground object detection, based on remote sensing satellite imagery, provides the groundwork of numerous applications, so the detection accuracy is of vital importance. The background of remote sensing images is complex, the object size is various, and there are many small objects. In view of the above problems, a multi-attention object detection method (MA-FPN) based on multi-scale is proposed in this paper, which can effectively make the network pay attention to the location of the object and reduce the loss of small object information. According to feature pyramid network (FPN), we firstly put forward a global spatial attention module, which extracts spatial location-related information from shallow features and fuses it with deep features to enhance the position expression ability of deep features. Besides, the paper provides a pixel feature attention module: the multi-scale convolution kernel is employed to generate the feature map of the same size as the input, as well as channel attention is used to assign weights to each layer of feature maps to obtain pixel-level attention maps with good details. Experiments on NWPU, RSOD, and DOTA datasets show that the proposed algorithm outperforms state of the art methods.","2169-3536","","10.1109/ACCESS.2019.2928522","National Natural Science Foundation of China(grant numbers:61877043); Major Scientific and Technological Projects for a New Generation of Artificial Intelligence of Tianjin(grant numbers:18ZXZNSY00300); Key Project for Science and Technology Support from Key Research and Development Program of Tianjin(grant numbers:18YFZCGX00960); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762136","Object detection;satellite imagery;pixel-level attention;spatial attention","Feature extraction;Remote sensing;Object detection;Proposals;Convolution;Deep learning;Semantics","feature extraction;geophysical image processing;image fusion;object detection;remote sensing","shallow features;deep features;pixel feature attention module;multiscale convolution kernel;feature map;channel attention;pixel-level attention maps;ground object detection;remote sensing satellite imagery;detection accuracy;object size;multiattention object detection method;MA-FPN;network pay attention;object information;feature pyramid network;global spatial attention module;multiattention object detection model;spatial location-related information extraction;DOTA datasets;RSOD datasets;NWPU datasets","","26","","26","CCBY","15 Jul 2019","","","IEEE","IEEE Journals"
"Pansharpening Based on Spectral-Spatial Dependence for Multibands Remote Sensing Images","L. Wu; X. Jiang","College of Mathematics and Computer, Xinyu University, Xinyu, China; College of Mathematics and Computer, Xinyu University, Xinyu, China","IEEE Access","26 Jul 2022","2022","10","","76153","76167","Pansharpening based on detail injection (DI) explores panchromatic (PAN) images and injects their spatial geometrical information into multispectral (MS) images. Traditional DI algorithms fuse remote sensing images without considering spectral and spatial dependence, which causes spectral distortion. To overcome this problem, this article proposes an advanced version of the DI model that not only achieves adaptive DI but also addresses spectral-spatial dependence. This proposed method is called pansharpening based on spectral-spatial dependence for multi-bands remote sensing images. In the proposed method, three parameters including an adaptive DI based on the correlation between MS and PAN images, the spectral dependence based on the MS band and pixel dependence, and the spatial dependence based on the detail offset amplitude are designed. An improved GDI model based on DI and the combination of the spectral and spatial dependence (SSD) is constructed. In the model, SSD enables the fused image to retain the spectral fidelity, and DI ensures the sharpness of the fused image. A performance test is conducted on various satellite datasets and the results are compared with those of several state-of-the-art fusion methods. The results show that the proposed method benefits pansharpening.","2169-3536","","10.1109/ACCESS.2022.3192461","Natural Science Foundation of Jiangxi, China(grant numbers:20212BAB202028); Key Science and Technology Research Project in Jiangxi Province Department of Education(grant numbers:GJJ202301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832905","Pansharpening;spectral dependence;spatial dependence;detail injection","Pansharpening;Remote sensing;Sensors;Distortion;Intensity modulation;Information filters;Frequency modulation","geophysical image processing;geophysical signal processing;image fusion;image resolution;remote sensing;sensor fusion","multispectral images;traditional DI algorithms;spectral distortion;DI model;adaptive DI;spectral-spatial dependence;multibands remote sensing images;spectral dependence;pixel dependence;fused image;spectral fidelity;panchromatic images;spatial geometrical information","","","","41","CCBY","19 Jul 2022","","","IEEE","IEEE Journals"
"DMNet: A Network Architecture Using Dilated Convolution and Multiscale Mechanisms for Spatiotemporal Fusion of Remote Sensing Images","W. Li; X. Zhang; Y. Peng; M. Dong","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Sensors Journal","17 Sep 2020","2020","20","20","12190","12202","Since remote sensing images cannot have both high temporal resolution and high spatial resolution, spatiotemporal fusion of remote sensing images has attracted increasing attention in recent years. Additionally, with the successful application of deep learning in various fields, spatiotemporal fusion algorithms based on deep learning have also gradually diversified. We propose a network framework that is based on deep convolutional neural networks that incorporate dilated convolution and multiscale mechanisms, we refer to this network framework as DMNet. In this method, we concatenate the feature maps that need to be fused to avoid using complex fusion methods to introduce noise. Then, the multiscale mechanism can extract the context information of the image at various scales, and make the image details more abundant. By adding skip connections, feature maps in shallow convolutional layers can be obtained to avoid losing important features of the image during the convolution. Additionally, dilated convolution expands the receptive field of the convolution kernel, which is conducive to the extraction of small detail features. To evaluate the robustness of our method, we conduct experiments on two datasets and compare the results with those obtained by six representative spatiotemporal fusion methods. Both intuitive and objective results demonstrate the superior performance of our method.","1558-1748","","10.1109/JSEN.2020.3000249","Chongqing Graduate Student Scientific Research Innovation Project(grant numbers:CYB19174); Natural Science Foundation of China(grant numbers:61972060,U1713213); Natural Science Foundation of Chongqing(grant numbers:cstc2019cxcyljrc-td0270,cstc2019jcyj-cxttX0002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9109314","Spatiotemporal fusion;convolutional neural network;multiscale mechanism;dilated convolution;skip connection","Spatiotemporal phenomena;Prediction algorithms;Remote sensing;Spatial resolution;Convolution;Artificial satellites;Earth","convolutional neural nets;feature extraction;geophysical image processing;image fusion;remote sensing","DMNet;network architecture;dilated convolution;multiscale mechanism;remote sensing images;high temporal resolution;high spatial resolution;deep learning;spatiotemporal fusion algorithms;deep convolutional neural networks;feature maps;complex fusion methods;image details;shallow convolutional layers;convolution kernel;representative spatiotemporal fusion methods","","12","","40","IEEE","5 Jun 2020","","","IEEE","IEEE Journals"
"Monitoring the Spatio-Temporal Changes of Non-Cultivated Land via Long-Time Series Remote Sensing Images in Xinghua","S. Zhang; H. Zhang; X. Gu; J. Liu; Z. Yin; Q. Sun; Z. Wei; Y. Pan","Research Center of Information Technology, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; School of Surveying and Land Information Engineering, Henan Polytechnic University, Jiaozuo, China; Research Center of Information Technology, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; Institute of Land Engineering and Technology, Shaanxi Provincial Land Engineering Construction Group Company Ltd., Xi’an, China; Research Center of Information Technology, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; Research Center of Information Technology, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; Research Center of Information Technology, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China; Research Center of Information Technology, Beijing Academy of Agriculture and Forestry Sciences, Beijing, China","IEEE Access","17 Aug 2022","2022","10","","84518","84534","The amount of cultivated land per capita in China is relatively low, and the phenomenon of non-cultivated land (NCL) in recent years has negatively impacted the stability of grain production in China. In this study, long-time series images obtained via satellite remote sensing were used to monitor spatio-temporal changes in NCL at the county scale. Seven-phase images were acquired from 1990 to 2020 (every five years) using medium-resolution Landsat MSS, TM, ETM+, and Sentinel MSI. Vegetation indices and texture features were extracted for all images. Terrain features such as slope, aspect and elevation were extracted from the DEM data. Combining vegetation index features, texture features, terrain features and multispectral bands, the image classification was performed using the random forest (RF) algorithm. The indices of classification accuracy assessment indices included overall accuracy (OA) and multiclass F-scores (Fm). Zonal statistics were used to calculate the area of cultivated land in towns for 1990 and 2020, and to create grades for the reduction of cultivated land. Finally, indicators including land use dynamic degree (LUDD), land use type change (LUTC) and land use change rate (LUCR) were adopted to reflect the spatio-temporal of NCL in the study area. The results show that RF classification algorithm achieves accurate and efficient land use extraction. The OA were greater than 86%, and the Fm were over 0.88. The cultivated land area in the study area showed decreasing trend. From 1990 to 2020, the ratio of cultivated land decreased from 59.75% to 50.21%. Meanwhile, the dynamic degree of cultivated land increased annually. The conversion of cultivated land into construction land was dominant, accounting for 31.84% of the total change in cultivated land over the past 30 years. This study also reveals that NCL is highly related to local economic and land-use policies. Multi-source remote sensing data have been used to quantitatively analyse the spatio-temporal changes in cultivated land conversion, providing a reference for relevant land management departments to master cultivated land use changes and adjust land management policies.","2169-3536","","10.1109/ACCESS.2022.3197650","Key Research and Development Program of Shaanxi(grant numbers:2022ZDLNY02-10); National Key Research and Development Program of China(grant numbers:2021YFD1500203); Beijing Talents Fund(grant numbers:2020A58); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852424","Non-agricultural cultivated land;long-time series;random forest;spatio-temporal change;remote sensing","Remote sensing;Artificial satellites;Monitoring;Vegetation mapping;Radio frequency;Feature extraction;Spatial temporal resolution","geographic information systems;geophysical image processing;image classification;image fusion;land use planning;terrain mapping;vegetation;vegetation mapping","cultivated land conversion;spatio-temporal changes;noncultivated land;long-time series remote sensing images;cultivated land area;Xinghua;grain production;China;seven-phase images;vegetation index;random forest algorithm;multiclass F-scores;image classification;local economic policies;land-use policies;land management policies;land use changes","","","","71","CCBYNCND","8 Aug 2022","","","IEEE","IEEE Journals"
"A3 CLNN: Spatial, Spectral and Multiscale Attention ConvLSTM Neural Network for Multisource Remote Sensing Data Classification","H. -C. Li; W. -S. Hu; W. Li; J. Li; Q. Du; A. Plaza","Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Department of Technology of Computers and Communications, Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Transactions on Neural Networks and Learning Systems","3 Feb 2022","2022","33","2","747","761","The problem of effectively exploiting the information multiple data sources has become a relevant but challenging research topic in remote sensing. In this article, we propose a new approach to exploit the complementarity of two data sources: hyperspectral images (HSIs) and light detection and ranging (LiDAR) data. Specifically, we develop a new dual-channel spatial, spectral and multiscale attention convolutional long short-term memory neural network (called dual-channel  $A^{3}$ CLNN) for feature extraction and classification of multisource remote sensing data. Spatial, spectral, and multiscale attention mechanisms are first designed for HSI and LiDAR data in order to learn spectral- and spatial-enhanced feature representations and to represent multiscale information for different classes. In the designed fusion network, a novel composite attention learning mechanism (combined with a three-level fusion strategy) is used to fully integrate the features in these two data sources. Finally, inspired by the idea of transfer learning, a novel stepwise training strategy is designed to yield a final classification result. Our experimental results, conducted on several multisource remote sensing data sets, demonstrate that the newly proposed dual-channel  $A^{\,3}$ CLNN exhibits better feature representation ability (leading to more competitive classification performance) than other state-of-the-art methods.","2162-2388","","10.1109/TNNLS.2020.3028945","National Natural Science Foundation of China(grant numbers:61871335); Fundamental Research Funds for the Central Universities(grant numbers:2682020XG02,2682020ZT35); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234528","Attention mechanism;convolutional long short-term memory;feature extraction;fusion;multisource remote sensing data classification;transfer learning","Laser radar;Feature extraction;Data models;Learning systems;Neural networks;Hyperspectral sensors","convolutional neural nets;feature extraction;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence);optical radar;remote sensing","dual channel A3 CLNN;stepwise training strategy;transfer learning;composite attention learning mechanism;convolutional long short-term memory neural network;LiDAR data;ConvLSTM neural network;light detection and ranging data;hyperspectral images;information multiple data sources;multisource remote sensing data classification;competitive classification performance;feature representation ability;designed fusion network;multiscale information;spatial-enhanced feature representations;multiscale attention mechanisms;spectral attention mechanisms;spatial attention mechanisms;feature extraction","","26","","57","IEEE","21 Oct 2020","","","IEEE","IEEE Journals"
"PSTAF-GAN: Progressive Spatio-Temporal Attention Fusion Method Based on Generative Adversarial Network","Q. Liu; X. Meng; F. Shao; S. Li","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","14 Apr 2022","2022","60","","1","13","Spatio-temporal fusion aims to integrate multisource remote sensing images with complementary high spatial and temporal resolutions, so as to obtain time-series high spatial resolution fused images. Currently, deep learning (DL)-based spatio-temporal fusion methods have received broad attention. However, on one hand, most of the existing DL-based methods train the model in a band-by-band manner, ignoring the correlations among bands. On the other hand, the general coarse spatio-temporal changes in low spatial resolution images (e.g., MODIS) calculated at the pixel domain cannot completely cover the fine spatio-temporal changes in high spatial resolution images (e.g., Landsat), due to complex surface features and the general large spatial resolution ratio between fine and coarse images. Besides, the existing DL-based spatio-temporal fusion methods are insufficient in exploring multiscale information by only stacking convolutional kernels with different sizes. To alleviate the above challenges, we propose a progressive spatio-temporal attention fusion model in a multiband training manner based on generative adversarial network (PSTAF-GAN). Specifically, we design a flexible multiscale feature extraction architecture to extract multiscale feature hierarchies. Then, spatio-temporal changes are calculated on the feature domain in different feature hierarchies. Besides, a spatio-temporal attention fusion architecture is proposed to fuse the spatio-temporal changes and ground details in a coarse-to-fine manner, which can explore multiscale information more sufficient and gradually recover the target image. The results of quantitative and qualitative experiments on two publicly available benchmark datasets show that the proposed PSTAF-GAN can achieve the best performance compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3161563","National Natural Science Foundation of China(grant numbers:42171326,41801252,62071261); Natural Science Foundation of Zhejiang Province(grant numbers:LY22F010014); Fellowship of China Postdoctoral Science Foundation(grant numbers:2020M672490); K. C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740577","Generative adversarial network (GAN);multilevel feature;remote sensing;spatio-temporal attention fusion;weight sharing","Feature extraction;Spatial resolution;Image resolution;Generative adversarial networks;Convolution;Predictive models;Convolutional neural networks","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image resolution;object detection;remote sensing;spatiotemporal phenomena;time series","multiscale information;progressive spatio-temporal attention fusion model;multiband training manner;generative adversarial network;PSTAF-GAN;flexible multiscale feature extraction architecture;multiscale feature hierarchies;spatio-temporal attention fusion architecture;coarse-to-fine manner;target image;multisource remote sensing images;temporal resolutions;time-series high spatial resolution;deep learning-based spatio-temporal fusion methods;DL-based methods;band-by-band manner;low spatial resolution images;high spatial resolution images;complex surface features","","1","","53","IEEE","23 Mar 2022","","","IEEE","IEEE Journals"
"Deep Learning-Based Spatiotemporal Data Fusion Using a Patch-to-Pixel Mapping Strategy and Model Comparisons","Z. Ao; Y. Sun; X. Pan; Q. Xin","Faculty of Engineering, Beidou Research Institute, South China Normal University, Guangzhou, China; School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; State Key Laboratory of Desert and Oasis Ecology, Research Center for Ecology and Environment of Central Asia, Chinese Academy of Sciences, Ürümqi, China","IEEE Transactions on Geoscience and Remote Sensing","31 Mar 2022","2022","60","","1","18","Tradeoffs among the spatial, spectral, and temporal resolutions of satellite sensors make it difficult to acquire remote sensing images at both high spatial and high temporal resolutions from an individual sensor. Studies have developed methods to fuse spatiotemporal data from different satellite sensors, and these methods often assume linear changes in surface reflectance across time and adopt empirical rules and handcrafted features. Here, we propose a dense spatiotemporal fusion (DenseSTF) network based on the convolutional neural network (CNN) to deal with these problems. DenseSTF uses a patch-to-pixel modeling strategy that can provide abundant texture details for each pixel in the target fine image to handle heterogeneous landscapes and models both forward and backward temporal dependencies to account for land cover changes. Moreover, DenseSTF adopts a mapping function with few assumptions and empirical rules, which allows for establishing reliable relationships between the coarse and fine images. We tested DenseSTF in three contrast scenes with different degrees of heterogeneity and temporal changes, and made comparisons with three rule-based fusion approaches and three CNNs. Experimental results indicate that DenseSTF can provide accurate fusion results and outperform the other tested methods, especially when the land cover changes abruptly. The structure of the deep learning networks largely impacts the success of data fusion. Our study developed a novel approach based on CNN using a patch-to-pixel mapping strategy and highlighted the effectiveness of the deep learning networks in the spatiotemporal fusion of the remote sensing data.","1558-0644","","10.1109/TGRS.2022.3154406","National Key Research and Development Program of China(grant numbers:2017YFA0604300); Natural Science Foundation of China(grant numbers:U1811464,41901345,42171308,41875122); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011429); Western Talents(grant numbers:2018XBYJRC004); Guangdong Top Young Talents(grant numbers:2017TQ04Z359); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721293","Convolutional neural networks (CNNs);deep learning;spatiotemporal fusion","Remote sensing;Earth;Artificial satellites;Reflectivity;Spatiotemporal phenomena;Spatial resolution;MODIS","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image classification;image fusion;image resolution;image texture;land cover;spatiotemporal phenomena;terrain mapping","spatiotemporal data fusion;patch-to-pixel mapping strategy;model comparisons;spatial resolutions;spectral, resolutions;remote sensing images;high temporal resolutions;satellite sensors;linear changes;surface reflectance;empirical rules;handcrafted features;dense spatiotemporal fusion network;DenseSTF;convolutional neural network;CNN;patch-to-pixel modeling strategy;abundant texture details;target fine image;heterogeneous landscapes;backward temporal dependencies;land cover changes;mapping function;coarse images;fine images;temporal changes;rule-based fusion approaches;tested methods;deep learning networks;remote sensing data","","1","","66","CCBY","24 Feb 2022","","","IEEE","IEEE Journals"
"Improved Multiscale Edge Detection Method for Polarimetric SAR Images","R. Jin; J. Yin; W. Zhou; J. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2016","13","8","1104","1108","This letter presents a multiscale edge detection method for multilook polarimetric synthetic aperture radar (PolSAR) images based on the nonsubsampled contourlet transform (NSCT). The NSCT can provide flexible multiscale and directional decomposition. In the multiscale decomposition, the coefficients of the nonsubsampled pyramid in the NSCT are calculated via maximizing the polarimetric contrast between the adjacent subband levels, instead of using the difference of the adjacent subbands as used in the additive noise model. By this way, we make the NSCT applicable to PolSAR data and multiband data. Then, the edges are detected in the NSCT domain based on a fusion of the directional subband coefficients at different scales. Experimental results with both simulated and real PolSAR data show that the present approach is robust to noise and the extracted edges are complete and continuous.","1558-0571","","10.1109/LGRS.2016.2569534","National Natural Science Foundation of China(grant numbers:61490693,41171317); key project of the NSFC(grant numbers:61132008); Aeronautical Science Foundation of China; Fundamental Research Funds for the Central Universities(grant numbers:FRF-TP-15-090A1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484697","Edge detection;multiscale;nonsubsampled contourlet transform (CT) (NSCT);polarimetric synthetic aperture radar (PolSAR)","Image edge detection;Synthetic aperture radar;Computed tomography;Speckle;Image fusion;Wavelet transforms","feature extraction;geophysical image processing;radar imaging;remote sensing by radar;synthetic aperture radar","real PolSAR data;simulated PolSAR data;directional subband coefficient fusion;multiband data;PolSAR data;additive noise model;directional decomposition;flexible multiscale decomposition;nonsubsampled contourlet transform;synthetic aperture radar;polarimetric SAR images;multiscale edge detection method","","12","","16","IEEE","7 Jun 2016","","","IEEE","IEEE Journals"
"Unsupervised SAR Image Segmentation Using Ambiguity Label Information Fusion in Triplet Markov Fields Model","F. Wang; Y. Wu; P. Zhang; Q. Zhang; M. Li","Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; National Key Lab. of Radar Signal Processing, Xidian University, Xi’an, China; China Academy of Space Technology, Beijing Institute of Space System Engineering, Beijing, China; National Key Lab. of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","25 Aug 2017","2017","14","9","1479","1483","The recently proposed triplet Markov fields (TMF) model enhances the nonstationary image prior modeling ability by introducing an auxiliary field. Motivated by the TMF model, we propose a generalized TMF model based on ambiguity label information fusion (ALF-TMF) for synthetic aperture radar (SAR) image segmentation. The redefined auxiliary field in ALF-TMF indicates the dominant direction of local image contents and gives explicit nonstationary divisions of SAR images. To reduce the influence of unreliable observations caused by speckle noise, the original label field is adaptively generalized by introducing ambiguity class based on image observation and local nonstationary contextual information. Given the extended label field, prior and likelihood terms are constructed and merged to provide the posterior segmentation decision via the Bayesian fusion rule. Real SAR images are utilized in the experimental analysis, and the effectiveness of the proposed method is validated accordingly.","1558-0571","","10.1109/LGRS.2017.2715223","Natural Science Foundation of China(grant numbers:61272281,61271297,61301284); Fundamental Research Funds for the Central Universities(grant numbers:JB142001-1); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2015JM6288); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004442","Ambiguity label field extension;Bayesian fusion;nonstationary division;synthetic aperture radar (SAR) image segmentation;triplet Markov fields (TMF)","Synthetic aperture radar;Image segmentation;Uncertainty;Bayes methods;Markov processes;Speckle;Indexes","Bayes methods;image fusion;image segmentation;Markov processes;maximum likelihood estimation;radar imaging;synthetic aperture radar","unsupervised SAR image segmentation;ambiguity label information fusion;triplet Markov field model;generalized TMF model;nonstationary image enhancement;auxiliary field;synthetic aperture radar image segmentation;speckle noise;posterior segmentation decision;Bayesian fusion rule","","10","","10","IEEE","8 Aug 2017","","","IEEE","IEEE Journals"
"SARD: Towards Scale-Aware Rotated Object Detection in Aerial Imagery","Y. Wang; Y. Zhang; Y. Zhang; L. Zhao; X. Sun; Z. Guo","Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Access","9 Dec 2019","2019","7","","173855","173865","Multi-class object detection in remote sensing imagery is an important and challenging topic in computer vision. Compared with the object detection of natural scenes, remote sensing object detection has some challenges such as scale diversity, arbitrary directions and densely packed objects. To resolve these problems, this paper presents a scale-aware rotated object detection. Firstly, we propose a novel feature fusion module, which takes full advantage of high-level semantic information and low-level high resolution feature. The new feature maps are more suitable for detecting objects with a large difference in scale. Meanwhile, we design a specific weighted loss, which contains an intersection-over-union (IoU) loss and a smooth L1 loss to further address the scale diversity. Besides, in order to detect oriented and densely packed objects more accurately, we propose a normalization strategy for the representation of rotating bounding box. Our method is evaluated on two public aerial datasets DOTA and HRSC2016, and achieves competitive performances. On DOTA, we boost the mean Average Precision (mAP) to 72.95% on oriented object detection.","2169-3536","","10.1109/ACCESS.2019.2956569","National Natural Science Foundation of China(grant numbers:41801349,61725105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917630","Object detection;remote sensing;convolution neural network;rotation region","Object detection;Remote sensing;Feature extraction;Proposals;Detectors;Loss measurement;Training","computer vision;feature extraction;image fusion;image resolution;object detection;object tracking;remote sensing","aerial imagery;multiclass object detection;remote sensing imagery;computer vision;remote sensing object detection;feature fusion module;high-level semantic information;low-level high resolution feature;feature maps;intersection-over-union loss;L1 loss;SARD;scale-aware rotated object detection;normalization strategy","","27","","43","CCBY","28 Nov 2019","","","IEEE","IEEE Journals"
"Exploiting Low-Rank and Sparse Properties in Strided Convolution Matrix for Pansharpening","F. Zhang; H. Zhang; K. Zhang; Y. Xing; J. Sun; Q. Wu","School of Information Science and Engineering, Shandong Normal University, Ji'nan, China; School of Information Science and Engineering, Shandong Normal University, Ji'nan, China; School of Information Science and Engineering, Shandong Normal University, Ji'nan, China; National Engineering Laboratory for Integrated Aero-Space-Ground- Ocean Big Data Application Technology and the School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Information Science and Engineering, Shandong Normal University, Ji'nan, China; College of Geography and Environment, Shandong Normal University, Ji'nan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Mar 2021","2021","14","","2649","2661","Fusion of low spatial resolution multispectral (LR MS) and panchromatic (PAN) images to acquire high spatial resolution multispectral (HR MS) images has attracted increasing attention in recent years. In this article, we first utilize the form of convolution matrix (CM) to formulate the image fusion problem. In order to reduce the complexity of CM, the step size is introduced and strided convolution matrix (SCM) is constructed. Then, we explore the low-rank property in SCM and impose the prior on the spatial and spectral degradation model of LR MS and PAN images. Meanwhile, sparsity in SCM is considered to further enhance the local structures in the fused image. Finally, the proposed model is optimized efficiently by the alternative direction method of multipliers. By exploiting the low-rank and sparse priors in SCM of HR MS image, the local and global structures can be better preserved. The experimental results on the reduced-resolution and full-resolution datasets also show that the proposed method behaves well in qualitative and quantitative assessments.","2151-1535","","10.1109/JSTARS.2021.3058158","National Natural Science Foundation of China(grant numbers:61901246,U1736122); China Postdoctoral Science Foundation(grant numbers:2019TQ0190,2019M662432); Open Fund of National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology(grant numbers:20200208); Natural Science Foundation for Distinguished Young Scholars of Shandong Province(grant numbers:JQ201718); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351613","Image fusion;low-rank and sparse priors;multispectral image;panchromatic (PAN) image;strided convolution matrix (SCM)","Convolution;Sparse matrices;Degradation;Spatial resolution;Pansharpening;Indexes;Image fusion","geophysical image processing;image fusion;image resolution","sparse properties;strided convolution matrix;LR MS;high spatial resolution multispectral images;image fusion problem;SCM;low-rank property;spatial degradation model;spectral degradation model;fused image;HR MS image;reduced-resolution;full-resolution datasets","","1","","61","CCBY","9 Feb 2021","","","IEEE","IEEE Journals"
"Data Fusion Technique Using Wavelet Transform and Taguchi Methods for Automatic Landslide Detection From Airborne Laser Scanning Data and QuickBird Satellite Imagery","B. Pradhan; M. N. Jebur; H. Z. M. Shafri; M. S. Tehrany","Department of Civil Engineering, Geospatial Information Science Research Center (GISRC), Faculty of Engineering, Universiti Putra Malaysia, Serdang, Malaysia; Department of Civil Engineering, Geospatial Information Science Research Center (GISRC), Faculty of Engineering, Universiti Putra Malaysia, Serdang, Malaysia; Department of Civil Engineering, Geospatial Information Science Research Center (GISRC), Faculty of Engineering, Universiti Putra Malaysia, Serdang, Malaysia; Department of Civil Engineering, Geospatial Information Science Research Center (GISRC), Faculty of Engineering, Universiti Putra Malaysia, Serdang, Malaysia","IEEE Transactions on Geoscience and Remote Sensing","29 Feb 2016","2016","54","3","1610","1622","Landslide mapping is indispensable for efficient land use management and planning. Landslide inventory maps must be produced for various purposes, such as to record the landslide magnitude in an area and to examine the distribution, types, and forms of slope failures. The use of this information enables the study of landslide susceptibility, hazard, and risk, as well as of the evolution of landscapes affected by landslides. In tropical countries, precipitation during the monsoon season triggers hundreds of landslides in mountainous regions. The preparation of a landslide inventory in such regions is a challenging task because of rapid vegetation growth. Thus, enhancing the proficiency of landslide mapping using remote sensing skills is a vital task. Various techniques have been examined by researchers. This study uses a robust data fusion technique that integrates high-resolution airborne laser scanning data (LiDAR) with high-resolution QuickBird satellite imagery (2.6-m spatial resolution) to identify landslide locations in Bukit Antarabangsa, Ulu Klang, Malaysia. This idea is applied for the first time to identify landslide locations in an urban environment in tropical areas. A wavelet transform technique was employed to achieve data fusion between LiDAR and QuickBird imagery. An object-oriented classification method was used to differentiate the landslide locations from other land use/covers. The Taguchi technique was employed to optimize the segmentation parameters, whereas the rule-based technique was used for object-based classification. In addition, to assess the impact of fusion in classification and landslide analysis, the rule-based classification method was also applied on original QuickBird data which have not been fused. Landslide locations were detected, and the confusion matrix was used to examine the proficiency and reliability of the results. The achieved overall accuracy and kappa coefficient were 90.06% and 0.84, respectively, for fused data. Moreover, the acquired producer and user accuracies for landslide class were 95.86% and 95.32%, respectively. Results of the accuracy assessment for QuickBird data before fusion showed 65.65% and 0.59 for overall accuracy and kappa coefficient, respectively. It revealed that fusion made a significant improvement in classification results. The direction of mass movement was recognized by overlaying the final landslide classification map with LiDAR-derived slope and aspect factors. Results from the tested site in a hilly area showed that the proposed method is easy to implement, accurate, and appropriate for landslide mapping in a tropical country, such as Malaysia.","1558-0644","","10.1109/TGRS.2015.2484325","Putra Research Grant(grant numbers:GP-I/2014/9439200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322237","Fusion;landslide;laser scanning data (LiDAR);Malaysia;remote sensing;rule-based;Taguchi;wavelet transform;Fusion;landslide;laser scanning data (LiDAR);Malaysia;remote sensing;rule-based;Taguchi;wavelet transform","Terrain factors;Satellites;Wavelet transforms;Laser radar;Synthetic aperture radar;Image segmentation","geomorphology;geophysical techniques;image fusion","data fusion technique;wavelet transform;Taguchi methods;automatic landslide detection;airborne laser scanning data;quickbird satellite imagery;landslide mapping;efficient land use management;efficient land use planning;landslide inventory;landslide susceptibility;rapid vegetation growth;remote sensing;Bukit Antarabangsa;Ulu Klang;Malaysia;object-oriented classification method;LiDAR-derived slope","","69","","73","IEEE","9 Nov 2015","","","IEEE","IEEE Journals"
"Super-Resolution of VIIRS-Measured Ocean Color Products Using Deep Convolutional Neural Network","X. Liu; M. Wang","CIRA, Colorado State University, Fort Collins, CO, USA; Center for Satellite Applications and Research, National Environmental Satellite, Data, and Information Service (NESDIS)/National Oceanic and Atmospheric Administration (NOAA), College Park, MD, USA","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","114","127","Since its launch in October 2011, the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi National Polar-orbiting Partnership (SNPP) satellite has provided high quality global ocean color products, which include normalized water-leaving radiance spectra nLw(λ) of six moderate (M) bands (M1-M6) at the wavelengths of 410, 443, 486, 551, 671, and 745 nm with a spatial resolution of 750-m, and one imagery (I) band at a wavelength of 638 nm with a spatial resolution of 375-m. Because the high-resolution I-band measurements are highly correlated spectrally to those of M-band data, it can be used as a guidance to super-resolve the M-band nLw(λ) imagery from 750-to 375-m spatial resolution. Super-resolving images from coarse spatial resolution to finer ones have been a field of very active research in recent years. However, no previous studies have been applied to satellite ocean color remote sensing, in particular, for VIIRS ocean color applications. In this study, we employ the deep convolutional neural network (CNN) technique to glean the high-frequency content from the VIIRS I1 band and transfer to super-resolved M-band ocean color images. The network is trained to super-resolve each of the VIIRS six M-bands nLw(λ) separately. In our results, the super-resolved (375-m) nLw(λ) images are much sharper and show finer spatial structures than the original images. Quantitative evaluations show that biases between the super-resolved and original nLw(λ) images are small for all bands. However, errors in the super-resolved nLw(λ) images are wavelength-dependent. The smallest error is found in the superresolved nLw(551) and nLw(671) images, and error increases as the wavelength decreases from 486 to 410 nm. The results show that the networks have the capability to capture the correlations of the M-band and the I1 band images to super-resolved M-band images.","1558-0644","","10.1109/TGRS.2020.2992912","Joint Polar Satellite System (JPSS) Funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097401","Deep convolutional neural network (CNN);image fusion;ocean color remote sensing;super-resolution;Visible Infrared Imaging Radiometer Suite (VIIRS)","Spatial resolution;Oceans;Image color analysis;Signal resolution;Satellite broadcasting;Sensors","convolutional neural nets;geophysical image processing;image resolution;infrared imaging;radiometry;remote sensing;underwater optics","original images;superresolved nL;M-band images;super-resolution;VIIRS-measured ocean color products;Visible Infrared Imaging Radiometer Suite;Suomi National Polar-orbiting Partnership satellite;high quality global ocean color products;normalized water-leaving radiance spectra;moderate bands;I-band measurements;M-band data;super-resolving images;coarse spatial resolution;satellite ocean color remote sensing;VIIRS ocean color applications;deep convolutional neural network technique;high-frequency content;M-band ocean color images;finer spatial structures;AD 2011 10;wavelength 638.0 nm;wavelength 410.0 nm to 486.0 nm;wavelength 551.0 nm;wavelength 671.0 nm;wavelength 745.0 nm","","10","","65","IEEE","20 May 2020","","","IEEE","IEEE Journals"
"The PAN and MS Image Pansharpening Algorithm Based on Adaptive Neural Network and Sparse Representation in the NSST Domain","X. Wang; S. Bai; Z. Li; R. Song; J. Tao","College of Computer and Information Technology, Liaoning Normal University, Dalian, China; College of Computer and Information Technology, Liaoning Normal University, Dalian, China; Dalian Hausen Software Co. Ltd., Dalian, China; School of Urban and Environmental Sciences, Liaoning Normal University, Dalian, China; School of Urban and Environmental Sciences, Liaoning Normal University, Dalian, China","IEEE Access","26 Apr 2019","2019","7","","52508","52521","How to improve the spatial resolution as much as possible while maintaining the spectral information of multi-spectral (MS) image in the field of image fusion is of great significance for practical applications, such as map updating, feature classification, and target recognition. To analyze the coefficients of the subband distribution characteristics, in this paper, we propose a new panchromatic (PAN) and MS image pansharpening model based on an adaptive neural network and sparse representation in the non-subsample shearlet transform (NSST) domain. First, this algorithm is specific to regional directional characteristics in the high-frequency subband of PAN and MS images, and we propose an adaptive pulse coupled neural network (PCNN) model. The model can adaptively calculate the link strength of a neural cell based on the region energy. Furthermore, we apply the model to the high-frequency fusing process with the corresponding fusion rule, and the rule can distinguish the high-frequency coefficients by ignition times, which can more effectively capture the geometric texture information and detailed information in the PAN image, enhancing the spatial resolution of the fused image. Second, because of the low-frequency sub-bands from the PAN image and I component obtained by intensity-hue-saturation (IHS) transformation of the MS images with high similarity to the original image but poor sparsity, we select a set of PAN images for learning, a more targeted over-complete dictionary for low-frequency sub-band sparse representation is obtained. Then, the larger absolute value of the sparse matrix is selected to obtain the low-frequency coefficients for the fusion image while maintaining the MS spectral information effectively, and the representation of characteristic information of low-frequency subband is more effective. A large number of simulation experiments verify the effectiveness of the proposed method.","2169-3536","","10.1109/ACCESS.2019.2910656","National Natural Science Foundation of China(grant numbers:41671439); Innovation Team Support Program of Liaoning Higher Education Department(grant numbers:LT2017013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693681","Image;MS image;image fusion;NSST;neuron connection intensity;sparse representation","Transforms;Spatial resolution;Image fusion;Remote sensing;Adaptive systems;High frequency;Signal processing algorithms","geophysical image processing;image denoising;image fusion;image representation;image resolution;image texture;neural nets;sparse matrices;wavelet transforms","MS spectral information;characteristic information;low-frequency subband;adaptive neural network;NSST domain;spatial resolution;multispectral image;image fusion;subband distribution characteristics;regional directional characteristics;high-frequency subband;MS images;adaptive pulse;neural network model;neural cell;high-frequency fusing process;corresponding fusion rule;high-frequency coefficients;PAN image;fused image;original image;low-frequency coefficients;low-frequency subbands;low-frequency subband sparse representation","","20","","41","OAPA","17 Apr 2019","","","IEEE","IEEE Journals"
"A Multisensor Data Fusion Model for Semantic Segmentation in Aerial Images","Q. Weng; H. Chen; H. Chen; W. Guo; Z. Mao","Key Laboratory of Spatial Data Mining and Information Sharing, Ministry of Education, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Academy of Digital China, Fuzhou University, Fuzhou, China","IEEE Geoscience and Remote Sensing Letters","27 Jun 2022","2022","19","","1","5","Semantic segmentation in high-resolution aerial images is a fundamental and challenging task with a wide range of applications. Although many segmentation methods with convolutional neural networks have achieved inspiring results, it is still difficult to distinguish regions with similar spectral features only using high-resolution data. Besides, the traditional data-independent upsampling methods may lead to suboptimal results. This letter proposes a multisensor data fusion model (MSDFM). Following the classical encoder–decoder structure, MSDFM regards colored digital surface models (colored-DSMs) data as a complementary input for further detailed feature extraction. A data-dependent upsampling (DUpsampling) method is adopted in the decoder stage instead of the common upsampling approaches to improve the classification accuracy of pixels of the small objects. Extensive experiments on Vaihingen and Potsdam datasets demonstrate that our proposed MSDFM outperforms most related models. Significantly, segmentation performance for the car category surpasses state-of-the-art methods over the International Society of Photogrammetry and Remote Sensing (ISPRS) Vaihingen dataset.","1558-0571","","10.1109/LGRS.2022.3183613","National Natural Science Foundation of China(grant numbers:41801324); Natural Science Foundation of Fujian Province(grant numbers:2019J01244,2020J05114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797725","Digital surface model (DSM);high-resolution aerial images;semantic segmentation","Semantics;Automobiles;Image segmentation;Feature extraction;Vegetation;Decoding;Deconvolution","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image segmentation;neural nets;object detection;photogrammetry;remote sensing;sensor fusion","similar spectral features;high-resolution data;traditional data-independent;suboptimal results;multisensor data fusion model;MSDFM;classical encoder-decoder structure;colored digital surface models;detailed feature extraction;data-dependent upsampling method;common upsampling approaches;related models;segmentation performance;car category surpasses state-of-the-art methods;semantic segmentation;high-resolution aerial images;fundamental task;segmentation methods;convolutional neural networks","","","","16","IEEE","16 Jun 2022","","","IEEE","IEEE Journals"
"Abundance Estimation Based on Band Fusion and Prioritization Mechanism","F. Li; M. Song; B. Xue; C. Yu","Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China; Department of Computer Science and Electrical Engineering, Remote Sensing Signal and Image Processing Laboratory, University of Maryland at Baltimore County, Baltimore, MD, USA; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jul 2022","2022","60","","1","21","To achieve real-time abundance estimation of hyperspectral images and improve the accuracy and efficiency of estimation, this article proposes a new band processing approach for abundance estimation, to be called sequential band fusion (SBF). To achieve SBF, a new band priority mechanism is proposed. It is derived from the concept of orthogonal subspace projection (OSP) by orthogonalizing undesired targets using projection while minimizing the variance resulting from the background. By taking advantage of OSP, the interfering effects caused by all undesired targets can be eliminated, and then, the detector produced by a target of interest can be further used as a measure of prioritizing bands as well as a means of searching bands for this particular target. As a result, two ranking-based band priority criteria (RP), called maximum OSP-based RP (MaxOSP-RP) and minimum OSP-based RP (MinOSP-RP), and two searching-based band priority criteria (SP), called sequential feedforward band search (SFBS) and sequential backward band search (SBBS), can be derived. We provide a detailed theoretical description and formula derivation of the SBF and combine it with band sequence (BSQ), RP, and SP to propose three different fusion mechanisms: SBF based on BSQ (SBF-BSQ), SBF based on RP (SBF-RP), and SBF with SP (SBF-SP) to make the fusion mechanism applied to different scenarios. Experimental results show that the proposed methods perform well for abundance estimation.","1558-0644","","10.1109/TGRS.2022.3187867","National Natural Science Foundation of China(grant numbers:61971082,61890964); Fundamental Research Funds for the Central Universities(grant numbers:3132019341); Cultivation Program for the Excellent Doctoral Dissertation of Dalian Maritime University(grant numbers:2022YBPY006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812586","Abundance estimation;band fusion;band prioritization;orthogonal projection","Estimation;Hyperspectral imaging;Computational modeling;Real-time systems;Mathematical models;Search problems;Object detection","hyperspectral imaging;image fusion;optimisation;search problems","real-time abundance estimation;prioritization mechanism;fusion mechanism;SBF-SP;SBF-RP;SBF-BSQ;band sequence;formula derivation;sequential backward band search;called sequential feedforward band search;searching-based band priority criteria;MinOSP-RP;MaxOSP-RP;ranking-based band priority criteria;OSP;orthogonal subspace projection;band priority mechanism;sequential band fusion;band processing approach","","1","","69","IEEE","1 Jul 2022","","","IEEE","IEEE Journals"
"DifUnet++: A Satellite Images Change Detection Network Based on Unet++ and Differential Pyramid","X. Zhang; Y. Yue; W. Gao; S. Yun; Q. Su; H. Yin; Y. Zhang","School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Software, Northwestern Polytechnical University, Xi’an, China; School of Software, Northwestern Polytechnical University, Xi’an, China; School of Software, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Change detection (CD) is one of the most important topics in the field of remote sensing. In this letter, we propose an effective satellite images CD network named DifUnet++. As the presentation of explicit difference is more conducive to extract change features, we design a differential pyramid of two input images as the input of Unet++. Considering the scale diversity of changed regions in remote sensing images, a multiply side-outs fusion strategy is adopted to predict the detection results of different scales. Furthermore, a learning upsampling method is utilized to refine the details of CD. The proposed architecture is evaluated on two public satellite image CD data sets. The experimental results show that our method performs much better than state-of-the-art methods.","1558-0571","","10.1109/LGRS.2021.3049370","National Natural Science Foundation of China(grant numbers:61971356,61801395,61971273); National Natural Science Foundation of Shaanxi Province(grant numbers:2020GM-137); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333611","Change detection (CD);deep learning;differential information;semantic learning","Convolution;Feature extraction;Remote sensing;Semantics;Image segmentation;Decoding;Satellites","feature extraction;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing;terrain mapping","scale diversity;changed regions;remote sensing images;multiply side-outs fusion strategy;public satellite image CD data sets;satellite images change detection network;effective satellite images CD network;DifUnet;explicit difference;change features;differential pyramid;input images;Unet","","11","","21","IEEE","22 Jan 2021","","","IEEE","IEEE Journals"
"A Computationally Efficient Algorithm for Fusing Multispectral and Hyperspectral Images","R. Guerra; S. López; R. Sarmiento","Institute for Applied Microelectronics, University of Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; Institute for Applied Microelectronics, University of Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain; Institute for Applied Microelectronics, University of Las Palmas de Gran Canaria, Las Palmas de Gran Canaria, Spain","IEEE Transactions on Geoscience and Remote Sensing","11 Aug 2016","2016","54","10","5712","5728","Remote sensing systems equipped with multispectral and hyperspectral sensors are able to capture images of the surface of the Earth at different wavelengths. In these systems, hyperspectral sensors typically provide images with a high spectral resolution but a reduced spatial resolution, while on the contrary, multispectral sensors are able to produce images with a rich spatial resolution but a poor spectral resolution. Due to this reason, different fusion algorithms have been proposed during the last years in order to obtain remotely sensed images with enriched spatial and spectral resolutions by wisely combining the data acquired for the same scene by multispectral and hyperspectral sensors. However, the algorithms so far proposed that are able to obtain fused images with a good spatial and spectral quality require a formidable amount of computationally complex operations that cannot be executed in parallel, which clearly prevent the utilization of these algorithms in applications under real-time constraints in which high-performance parallel-based computing systems are normally required for accelerating the overall process. On the other hand, there are other state-of-the-art algorithms that are capable of fusing these images with a lower computational effort but at the cost of decreasing the quality of the resultant fused image. In this paper, a new algorithm named computationally efficient algorithm for fusing multispectral and hyperspectral images (CoEf-MHI) is proposed in order to obtain a high-quality image from hyperspectral and multispectral images of the same scene with a low computational effort. The proposed CoEf-MHI algorithm is based on incorporating the spatial details of the multispectral image into the hyperspectral image, without introducing spectral distortions. To achieve this goal, the CoEf-MHI algorithm first spatially upsamples, by means of a bilinear interpolation, the input hyperspectral image to the spatial resolution of the input multispectral image, and then, it independently refines each pixel of the resulting image by linearly combining the multispectral and hyperspectral pixels in its neighborhood. The simulations performed in this work with different images demonstrate that our proposal is much more efficient than state-of-the-art approaches, being this efficiency understood as the ratio between the quality of the fused image and the computational effort required to obtain such image.","1558-0644","","10.1109/TGRS.2016.2570433","Spanish Government and European Union (FEDER funds) in the context of the Resilient EmBedded Electronic systems for Controlling Cities under Atypical situations (REBECCA) project(grant numbers:TEC2014-58036-C4-4-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7485828","Data fusion;Gram–Schmidt orthogonalization;hyperspectral;multispectral;orthogonal projections;parallel processing;resolution enhancement","Hyperspectral imaging;Spatial resolution;Approximation algorithms;Sensors;Bayes methods","hyperspectral imaging;image fusion;image resolution;remote sensing","computationally efficient algorithm;multispectral-hyperspectral image fusion;remote sensing system;multispectral sensor;hyperspectral sensor;Earth surface image;spectral resolution;spatial resolution;fusion algorithm;high-quality image;bilinear interpolation;image pixel;fused image quality","","21","","21","IEEE","6 Jun 2016","","","IEEE","IEEE Journals"
"Haze Correction for Contrast-Based Multispectral Pansharpening","S. Lolli; L. Alparone; A. Garzelli; G. Vivone","NASA Goddard Space Flight Center Joint Center for Earth Systems Technology, Greenbelt, MD, USA; Department of Information Engineering, University of Florence, Florence, Italy; Department of Information Engineering and Mathematics, University of Siena, Siena; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy","IEEE Geoscience and Remote Sensing Letters","4 Dec 2017","2017","14","12","2255","2259","In this letter, we show that pansharpening of visible/near-infrared (VNIR) bands takes advantage from a correction of the path-radiance term introduced by the atmosphere during the fusion process. This holds whenever the fusion mechanism emulates the radiative transfer model ruling the acquisition of the Earth's surface from space, that is, for methods exploiting a contrast-based injection model of spatial details extracted from the panchromatic (Pan) image into the interpolated multispectral (MS) bands. Such methods are high-pass modulation (HPM), Brovey transform, synthetic variable ratio (SVR), University of New Brunswick pansharp, smoothing filter-based intensity modulation, and spectral distortion minimization. The path radiance should be estimated and subtracted from each band before the product by Pan is accomplished and added back after. Both empirical and model-based estimation techniques of MS path radiances are compared within the framework of optimized SVR and HPM algorithms. Simulations carried out on QuickBird and IKONOS data highlight that haze correction of MS before fusion is always beneficial, especially on vegetated areas and in terms of spectral quality.","1558-0571","","10.1109/LGRS.2017.2761021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8091124","Haze;image fusion;multispectral (MS) pansharpening;path radiance;radiative transfer model;remote sensing","Atmospheric modeling;Spatial resolution;Atmospheric measurements;Terrestrial atmosphere;Earth;Context modeling;Atmospheric waves","geophysical image processing;geophysical signal processing;geophysical techniques;image fusion;image resolution;radiative transfer;regression analysis;remote sensing","haze correction;multispectral pansharpening;visible/near-infrared bands;VNIR;path-radiance term;fusion process;fusion mechanism;radiative transfer model;Earth's surface;contrast-based injection model;spatial details;panchromatic image;interpolated multispectral bands;synthetic variable ratio;New Brunswick pansharp;intensity modulation;spectral distortion minimization;path radiance;empirical model-based estimation techniques;MS path radiances;optimized SVR;HPM algorithms","","58","1","18","IEEE","31 Oct 2017","","","IEEE","IEEE Journals"
"A Pansharpening Approach Based on Multiple Linear Regression Estimation of Injection Coefficients","R. Restaino; G. Vivone; P. Addesso; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; GIPSA-lab, CNRS, Grenoble INP, Université Grenoble Alpes, Grenoble, France","IEEE Geoscience and Remote Sensing Letters","27 Dec 2019","2020","17","1","102","106","Pansharpening techniques allow a detailed reproduction of the Earth surface by fusing a multispectral (MS) and a panchromatic (PAN) image acquired over the same area. Classical pansharpening methods consist in the extraction of the details from the PAN image and their subsequent injection into the MS image through a linear function. In this letter, we propose to apply a nonlinear injection procedure that implements the detail injection through a polynomial function. Optimal polynomial coefficients in the least squares sense can be easily obtained in the closed form, and the consequent pansharpening algorithm is shown to obtain superior performance with respect to the existing linear approaches, especially for MS bands with a reduced wavelength overlap with the PAN channel.","1558-0571","","10.1109/LGRS.2019.2914093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8717622","Image fusion;least squares (LS) estimation;multiresolution analysis (MRA);pansharpening;polynomial regression model;remote sensing","Estimation;Image resolution;Earth;Sensor fusion;Image sensors;Protocols","geophysical image processing;image fusion;image resolution;polynomials;regression analysis;remote sensing","optimal polynomial coefficients;MS bands;PAN channel;multiple linear regression estimation;injection coefficients;Earth surface;superior performance;polynomial function;nonlinear injection procedure;linear function;MS image;PAN image;classical pansharpening methods;panchromatic image","","17","","20","IEEE","17 May 2019","","","IEEE","IEEE Journals"
"Incorporating an Adaptive Image Prior Model Into Bayesian Fusion of Multispectral and Panchromatic Images","G. Khademi; H. Ghassemian","Image Processing and Information Analysis Laboratory, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Laboratory, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","IEEE Geoscience and Remote Sensing Letters","21 May 2018","2018","15","6","917","921","Reconstruction of a sharpened multispectral (MS) image from its coarser measurements, namely the low spatial resolution MS and panchromatic (Pan) images, is a severely illposed inverse problem which requires the definition of an appropriate prior model. This letter incorporates an adaptive Markov random field (MRF)-based prior model into a Bayesian framework to recover the desired MS image. The proposed MRF-based prior model combines the high frequency details of the Pan image with the spectral relation between the bands of the MS image into a single energy function. Consequently, unlike most pansharpening methods, not directly injecting the spatial information of the Pan image into the fused product, the proposed method offers a fused product with minimum spectral distortion, along with perfectly enhanced spatial resolution. Visual and quantitative assessments of the fused products of the proposed method compared to those of some famous pansharpening methods prove the superiority of the proposed method.","1558-0571","","10.1109/LGRS.2018.2817561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8332090","Adaptive prior model;Bayesian estimation;image fusion;pansharpening","Adaptation models;Spatial resolution;Mathematical model;Inverse problems;Estimation;Cost function","Bayes methods;image fusion;image reconstruction;image resolution;inverse problems;Markov processes","Bayesian framework;MRF;Pan image;adaptive image prior model;sharpened multispectral image;panchromatic images;adaptive Markov random field;MS image;spatial resolution;inverse problem","","27","","23","IEEE","5 Apr 2018","","","IEEE","IEEE Journals"
"A New Variational Model in Texture Space for Pansharpening","M. Lotfi; H. Ghassemian","Image Processing and Information Analysis Laboratory, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Laboratory, Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","IEEE Geoscience and Remote Sensing Letters","30 Jul 2018","2018","15","8","1269","1273","In this letter, a new variational model in texture space for pansharpening is proposed to increase spatial information of multispectral image, while preserving spectral and spatial consistencies of pansharpened image. Geometric structure consistency between panchromatic image and pansharpened image is very important for preserving spatial characteristics, especially in borders, edges, and textured regions. G-norm can extract more pure and accurate texture, curvature, and oscillating details than do previous first-order and second-order-based methods, such as Gradient and Hessian operators. Therefore, we aim to use the G-space to maintain spatial information. Experimental results show that the proposed method has a better performance in terms of both spatial and spectral qualities; however, it is not efficient in terms of computation time.","1558-0571","","10.1109/LGRS.2018.2836951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370040","Cartoon;G-norm;image fusion;pansharpening;texture;total variation","Image edge detection;Spatial resolution;Data mining;Energy resolution;Distortion;Computational modeling","feature extraction;hyperspectral imaging;image fusion;image texture;variational techniques","new variational model;texture space;spatial information;multispectral image;spectral consistencies;spatial consistencies;geometric structure consistency;panchromatic image;second-order-based methods;G-space;spatial qualities;spectral qualities;image pansharpening;spectral preservation;spatial characteristic preservation;G-norm;first-order-based methods;Hessian operators;gradient operators","","7","","19","IEEE","31 May 2018","","","IEEE","IEEE Journals"
"A Better View Over Titan Drainage Networks Through RGB Fusion of Cassini SAR Images","E. I. Alves; A. I. A. S. S. Andrade; D. A. Vaz","Center for Earth and Space Research, University of Coimbra, Coimbra, Portugal; Center for Earth and Space Research, University of Coimbra, Coimbra, Portugal; Center for Earth and Space Research, University of Coimbra, Coimbra, Portugal","IEEE Geoscience and Remote Sensing Letters","23 Feb 2018","2018","15","3","414","418","We present a simple method to enhance the view of Titan drainage networks, thus allowing extracting relevant hydrological parameters. The method uses RGB fusion of three Cassini synthetic aperture radar images acquired at different times, and is tested on one drainage network. Comparison with previous studies of the same network shows an increase in all the measured parameters. The present results help constrain previous estimates of erosion times, terrain, and tectonic models for the area and indicate that, whenever possible, geomorphological inference from drainage network geometry should be drawn on multiply sampled scenes.","1558-0571","","10.1109/LGRS.2018.2791018","Portuguese National Funds through the Foundation for Science and Technology(grant numbers:UID/Multi/00611/2013); European Regional Development Fund through COMPETE 2020, Operational Program for Competitiveness and Internationalization(grant numbers:POCI-01-0145-FEDER-006922); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8275480","Drainage networks;image enhancement;image fusion;synthetic aperture radar (SAR);Titan","Synthetic aperture radar;Saturn;Satellites;Radar imaging;Image resolution;Image color analysis","image colour analysis;image enhancement;image fusion;image sampling;inference mechanisms;radar imaging;synthetic aperture radar","RGB fusion;relevant hydrological parameter extraction;Cassini SAR imaging;Titan drainage network geometry;Cassini synthetic aperture radar imaging;geomorphological inference","","4","","23","IEEE","30 Jan 2018","","","IEEE","IEEE Journals"
"Remote Sensing Scene Classification Based on Multi-Structure Deep Features Fusion","W. Xue; X. Dai; L. Liu","Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, China","IEEE Access","14 Feb 2020","2020","8","","28746","28755","Convolutional neural networks (CNNs) have been widely used in remote sensing scene classification due to their excellent performance in natural image classification. However, the complementarity of features extracted by different CNNs is seldom exploited, which limits the further improvement of classification accuracy. To solve this problem, we propose a classification method based on multi-structure deep features fusion (MSDFF). First, a data augmentation method based on random-scale cropping is adopted to achieve the expansion of limited data. Then, three popular CNNs are respectively used as feature extractors to capture deep features from the image. Finally, a deep feature fusion network is adopted to fuse these features and implement the classification. The effectiveness of the proposed method is verified on UC Merced, AID, and NWPU-RESISC45 datasets. The proposed method can achieve better performance than state-of-the-art scene classification methods.","2169-3536","","10.1109/ACCESS.2020.2968771","National Natural Science Foundation of China(grant numbers:61805215); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966241","Convolutional neural network;scene classification;feature extraction;multi-structure deep features fusion","Feature extraction;Remote sensing;Training;Semantics;Image analysis;Sports;Deep learning","convolutional neural nets;feature extraction;geophysical image processing;image capture;image classification;image fusion;remote sensing","CNNs;feature extractors;remote sensing scene classification;multistructure deep features fusion;convolutional neural networks;natural image classification;data augmentation method;MSDFF;random-scale cropping;image capture","","31","","37","CCBY","22 Jan 2020","","","IEEE","IEEE Journals"
"Constructing 10-m NDVI Time Series From Landsat 8 and Sentinel 2 Images Using Convolutional Neural Networks","Z. Ao; Y. Sun; Q. Xin","School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions (Henan University), Ministry of Education, Kaifeng, China; State Key Laboratory of Desert and Oasis Ecology, Research Center for Ecology and Environment of Central Asia, Chinese Academy of Sciences, Urumqi, China","IEEE Geoscience and Remote Sensing Letters","21 Jul 2021","2021","18","8","1461","1465","Normalized difference vegetation index (NDVI) carries valuable information related to the photosynthetic activity of vegetation and is essential for monitoring phenological changes and ecosystem dynamics. The medium to high spatial resolution satellite images from Landsat 8 and Sentinel 2 offer opportunities to generate dense NDVI time series at 10-m resolution to improve our understanding of the land surface processes. However, synergistic use of Landsat 8 and Sentinel 2 for generating frequent and consistent NDVI data remains challenging as they have different spatial resolutions and spectral response functions. In this letter, we developed an attentional super resolution convolutional neural network (ASRCNN) for producing 10-m NDVI time series through fusion of Landsat 8 and Sentinel 2 images. We evaluated its performance in two heterogeneous areas. Quantitative assessments indicated that the developed network outperforms five commonly used fusion methods [i.e., enhanced deep convolutional spatiotemporal fusion network (EDCSTFN), super resolution convolutional neural network (SRCNN), spatial and temporal adaptive reflectance fusion model (STARFM), enhanced STARFM (ESTARFM), and flexible spatiotemporal data fusion (FSDAF)]. The influence of the method selection on the fusion accuracy is much greater than that of the fusion strategy in blending Landsat-Sentinel NDVI. Our results illustrate the advantages and potentials of the deep learning approaches on satellite data fusion.","1558-0571","","10.1109/LGRS.2020.3003322","National Key Research and Development Program of China(grant numbers:2017YFA0604300); Natural Science Foundation of China(grant numbers:U1811464,41875122); Western Talents(grant numbers:2018XBYJRC004); Guangdong Top Young Talents(grant numbers:2017TQ04Z359); Open Foundation of Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions (Henan University), Ministry of Education(grant numbers:GTYR201810); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9125996","Convolutional neural network (CNN);data fusion;deep learning;remote sensing;spatiotemporal data","Remote sensing;Artificial satellites;Earth;Spatiotemporal phenomena;Spatial resolution;Time series analysis;Training","geophysical image processing;geophysical techniques;image fusion;image resolution;neural nets;phenology;remote sensing;sensor fusion;spatiotemporal phenomena;time series;vegetation;vegetation mapping","Landsat 8;Sentinel 2 images;convolutional neural networks;normalized difference vegetation index;high spatial resolution satellite images;Sentinel 2 offer opportunities;dense NDVI time series;10-m resolution;frequent data;consistent NDVI data;attentional super resolution convolutional neural network;developed network;temporal adaptive reflectance fusion model;flexible spatiotemporal data fusion;Landsat-Sentinel NDVI;satellite data fusion","","6","","32","IEEE","25 Jun 2020","","","IEEE","IEEE Journals"
"Image Matching via Feature Fusion and Coherent Constraint","K. Sun; L. Liu; W. Tao","National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China; School of Biomedical Engineering, South-Central University for Nationalities, Wuhan, China; National Key Laboratory of Science and Technology on Multi-Spectral Information Processing, School of Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","3","289","293","The Gaussian mixture model (GMM)-based methods have achieved great success in point set registration. However, they cannot be directly applied to image matching, because the features extracted from two images usually contain a large portion of outliers. In this letter, we propose a new method to extend the powerful GMM to the field of image feature points matching. The algorithm consists of two main steps. In the first step, points extracted from the images are mapped into a new subspace, in which feature similarity information is fused to get the new representation of the points. The second step performs an improved progressive process with the GMM to find correspondences satisfying the coherent constraint. In this way, finding correspondences among large outliers is feasible and the iteration converges faster. Experimental results on benchmark data sets show that the proposed method can find more correct matches with high accuracy.","1558-0571","","10.1109/LGRS.2016.2631165","National Natural Science Foundation of China(grant numbers:61371140,61305044); National Natural Science Foundation of Hubei Province(grant numbers:2015CFA062,2015BAA133); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831456","Coherent constraint;image matching;subspace mapping","Feature extraction;Robustness;Image matching;Kernel;Sun;Acceleration;Remote sensing","convergence of numerical methods;feature extraction;Gaussian processes;image fusion;image matching;image registration;image representation;iterative methods;mixture models","Gaussian mixture model;GMM-based method;point set registration;feature extraction;image feature point matching;point mapping;feature similarity information fusion;point representation;improved progressive process;coherent constraint;iteration convergence;benchmark data sets","","10","","18","IEEE","24 Jan 2017","","","IEEE","IEEE Journals"
"A Novel Loss Function for Optical and SAR Image Matching: Balanced Positive and Negative Samples","Y. He; X. Wang; Y. Zhang; Y. Liu; Z. Jiang; G. Li; Y. He","Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Troops, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2022","2022","19","","1","5","Image matching is a primary technology for optical and synthetic aperture radar (SAR) image fusion but often shows limited performance due to the highly nonlinear differences between optical and SAR modalities. Recently, deep neural networks (DNNs) have been investigated to effectively extract nonlinear features for image matching tasks, where DNNs are trained based on the elaborated design of loss functions and a low loss value is often expected to obtain better image matching performance. In this letter, we first theoretically demonstrate that when the value of a state-of-the-art loss function decreases, the corresponding matching performance may not consistently improve due to the imbalanced effect of positive and negative samples. To tackle this issue, we proposed an improved loss function to train DNNs for image matching of SAR and optical images. We theoretically prove that the improved loss function ensures the improvement of the matching performance when the loss value decreases based on Taylor’s series expansion analysis. Experimental results on an open dataset with extensive optical and SAR image pairs show that: 1) the proposed loss function is better than the original one in terms of image matching performance and 2) the combination of our loss function and existing multiscale convolutional gradient feature (MCGF)-based network provides better matching performance than the other state-of-the-art approaches.","1558-0571","","10.1109/LGRS.2022.3225965","National Key Research and Development Program of China(grant numbers:2021YFA0715202); National Natural Science Foundation of China(grant numbers:61790551,61925106,62022092,62101303); Autonomous Research Project of Department of Electronic Engineering at Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968003","Loss function;optical and synthetic aperture radar (SAR) image matching;sample balance;Taylor’s series expansion","Optical imaging;Optical sensors;Optical losses;Image matching;Radar polarimetry;Synthetic aperture radar;Adaptive optics","convolution;deep learning (artificial intelligence);feature extraction;gradient methods;image fusion;image matching;optical information processing;radar computing;radar imaging;synthetic aperture radar","corresponding matching performance;deep neural network;DNN;highly nonlinear differences;image matching;image matching performance;improved loss function;low loss value;MCGF;multiscale convolutional gradient feature;negative samples;nonlinear feature extraction;novel loss function;optical image matching;SAR image matching;SAR image pairs;SAR modalities;state-of-the-art loss function;synthetic aperture radar image fusion;Taylor series expansion analysis","","1","","16","IEEE","1 Dec 2022","","","IEEE","IEEE Journals"
"Semantic Segmentation Network of Remote Sensing Images With Dynamic Loss Fusion Strategy","W. Liu; Y. Zhang; J. Yan; Y. Zou; Z. Cui","Key Laboratory of Intelligent Medical Image Analysis and Precise Diagnosis of Guizhou Province, College of Computer Science and Technology, Guizhou University, Guiyang, China; Key Laboratory of Intelligent Medical Image Analysis and Precise Diagnosis of Guizhou Province, College of Computer Science and Technology, Guizhou University, Guiyang, China; Zhuhai Orbita Aerospace Science & Technology Company Ltd., Zhuhai, China; Key Laboratory of Intelligent Medical Image Analysis and Precise Diagnosis of Guizhou Province, College of Computer Science and Technology, Guizhou University, Guiyang, China; Big Data Science and Intelligent Engineering Research Institute, Guizhou Education University, Guiyang, China","IEEE Access","17 May 2021","2021","9","","70406","70418","The remote sensing (RS) images are widely used in various industries, among which semantic segmentation of RS images is a common research direction. At the same time, because of the complexity of target information and the high similarity of features between the classes, this task is very challenging. In recent years, semantic segmentation algorithms of RS images have emerged in an endless stream, but most of them are improved around the scale features of the target, and the accuracy has great room for improvement. In this case, we propose a semantic segmentation framework for RS images with dynamic perceptual loss. The framework is improved based on the InceptionV-4 network to form a network that includes contextual semantic fusion and dual-channel atrous spatial pyramid pooling (ASPP). The semantic segmentation network is an encoder-decoder structure. In addition, we design a dynamic perceptual loss module and a dynamic loss fusion strategy by further observing the loss changes of the network, so as to better improve the classified details. Finally, experiment on the ISPRS 2D Semantic Labeling Contest Vaihingen Dataset and Massachusetts Building Dataset. Compared with some segmentation networks, our model has excellent performance.","2169-3536","","10.1109/ACCESS.2021.3078742","construction of automatic and intelligent integrated ground processing system for micro nano hyperspectral satellite data(grant numbers:ZH0405-1900-01PWC); Research Foundation for Advanced Talents of Guizhou University (2016)(grant numbers:49); Key Disciplines of Guizhou Province-Computer Science and Technology(grant numbers:ZDXK[2018]007); Key Supported Disciplines of Guizhou Province-Computer Application Technology(grant numbers:QianXueWeiHeZi ZDXK [2016] 20); National Natural Science Foundation of China(grant numbers:61462013,61661010); Zhuhai Innovation and Entrepreneurship Team (2017)(grant numbers:ZH01110405170027PWC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427069","Remote sensing;semantic segmentation;perceptual loss;loss fusion","Image segmentation;Semantics;Convolution;Feature extraction;Task analysis;Deep learning;Training","geophysical image processing;image fusion;image segmentation;remote sensing","RS images;InceptionV-4 network;contextual semantic fusion;dual-channel atrous spatial pyramid pooling;semantic segmentation network;dynamic perceptual loss module;loss changes;remote sensing images;target information;dynamic loss fusion;feature similarity;scale features;ASPP;encoder-decoder structure;ISPRS 2D semantic labeling contest Vaihingen dataset;Massachusetts building dataset","","3","","40","CCBY","10 May 2021","","","IEEE","IEEE Journals"
"A New Multi-Channel Deep Convolutional Neural Network for Semantic Segmentation of Remote Sensing Image","W. Liu; Y. Zhang; H. Fan; Y. Zou; Z. Cui","Key Laboratory of Intelligent Medical Image Analysis and Precise Diagnosis of Guizhou Province, College of Computer Science and Technology, Guizhou University, Guiyang, China; Key Laboratory of Intelligent Medical Image Analysis and Precise Diagnosis of Guizhou Province, College of Computer Science and Technology, Guizhou University, Guiyang, China; Zhuhai Orbita Aerospace Science and Technology Company Ltd., Oribita Tech Park, Zhuhai, China; Key Laboratory of Intelligent Medical Image Analysis and Precise Diagnosis of Guizhou Province, College of Computer Science and Technology, Guizhou University, Guiyang, China; Big Data Science and Intelligent Engineering Research Institute, Guizhou Education University, Guiyang, China","IEEE Access","24 Jul 2020","2020","8","","131814","131825","The semantic segmentation of remote sensing (RS) image is a hot research field. With the development of deep learning, the semantic segmentation based on a full convolution neural network greatly improves the segmentation accuracy. The amount of information on the RS image is very large, but the sample size is extremely uneven. Therefore, even the common network can segment RS images to a certain extent, but the segmentation accuracy can still be greatly improved. The common neural network deepens the network to improve the classification accuracy, but it has a lot of loss to the target spatial features and scale features, and the existing common feature fusion methods can only solve some problems. A segmentation network is built to solve the above problems very well. The network employs the InceptionV-4 network as the backbone and improves it. We modify the network structure and introduce the changed Atrous Spatial Pyramid Pooling module to extract the multi-scale features of the target from different training stages. Without losing the depth of the network, using Inception blocks to strengthen the width of the network can obtain more abstract features. At the same time, the backbone network is used for semantic fusion of the context, it can retain more spatial features, then an effective decoder network is designed. Finally, evaluate our model on the ISPRS 2D Semantic Labeling Contest Potsdam and Inria Aerial Image Labeling Dataset. The results show that the network has very superior performance, reaching 89.62% IOU score and 94.49% F1 score on the Potsdam dataset, and the IOU score on the Inria dataset has been greatly improved.","2169-3536","","10.1109/ACCESS.2020.3009976","Research Foundation for Advanced Talents of Guizhou University (2016)(grant numbers:49); Key Disciplines of Guizhou Province-Computer Science and Technology(grant numbers:ZDXK[2018]007); Key Supported Disciplines of Guizhou Province-Computer Application Technology(grant numbers:QianXueWeiHeZi ZDXK [2016]20); National Natural Science Foundation of China(grant numbers:61462013,61661010); 2017 Zhuhai introduces Innovation and Entrepreneurship Team(grant numbers:ZH01110405170027PWC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143076","Semantic segmentation;neural network;remote sensing;feature fusion","Image segmentation;Feature extraction;Convolution;Semantics;Machine learning;Neural networks;Image classification","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);remote sensing","convolution neural network;deep learning;remote sensing image;semantic segmentation;multichannel deep convolutional neural network;Inria Aerial Image Labeling Dataset;ISPRS 2D Semantic Labeling Contest Potsdam;semantic fusion;atrous spatial pyramid pooling module;InceptionV-4 network;feature fusion methods;spatial features;RS image","","2","","42","CCBY","17 Jul 2020","","","IEEE","IEEE Journals"
"Deep Residual Learning for Boosting the Accuracy of Hyperspectral Pansharpening","Y. Zheng; J. Li; Y. Li; K. Cao; K. Wang","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","21 Jul 2020","2020","17","8","1435","1439","Recently, deep learning (DL) has gained impressive achievements in the field of remote sensing image fusion. However, most of the previous DL-based fusion methods are originally designed for multispectral pansharpening, which cannot be readily employed to hyperspectral pansharpening due to the much wider spectral range and lower spatial resolution of a hyperspectral image (HSI). In this letter, a novel framework based on deep residual learning is proposed for hyperspectral pansharpening. The proposed framework consists mainly of two parts. First, the initialized HSI with the enhanced spatial resolution is generated through contrast limited adaptive histogram equalization (CLAHE) and guided filter. Then, a deep residual convolutional neural network (DRCNN) is introduced to map the residuals between the initialized HSI and the reference HSI for further boosting the fusion accuracy. Experimental results demonstrate that the proposed framework can achieve superior performance compared with the existing state-of-the-art pansharpening methods, especially in terms of edge details enhancement.","1558-0571","","10.1109/LGRS.2019.2945424","China Postdoctoral Science Foundation(grant numbers:2017M623124); China Postdoctoral Science Special Foundation(grant numbers:2018T111019); National Natural Science Foundation of China(grant numbers:61901343,61671383,61571345,91538101,61501346,61502367); Higher Education Discipline Innovation Project(grant numbers:B08038); Open Research Fund of CAS Key Laboratory of Spectral Imaging Technology(grant numbers:LSIT201924W); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8874962","Contrast limited adaptive histogram equalization (CLAHE);deep residual convolutional neural network (DRCNN);guided filter;hyperspectral pansharpening","Hyperspectral imaging;Image edge detection;Spatial resolution;Histograms;Training","convolutional neural nets;geophysical image processing;hyperspectral imaging;image enhancement;image fusion;image resolution;learning (artificial intelligence);remote sensing","deep residual learning;hyperspectral pansharpening;remote sensing image fusion;DL-based fusion methods;multispectral pansharpening;spatial resolution;hyperspectral image;deep residual convolutional neural network;contrast limited adaptive histogram equalization;edge detail enhancement","","18","","19","IEEE","17 Oct 2019","","","IEEE","IEEE Journals"
"Automatic Meta-Feature Engineering for CNN Fusion in Aerial Scene Classification Task","V. V. De Melo; L. F. D. P. Sotto; M. M. Leonardo; F. A. Faria","Institute of Science and Technology, Universidade Federal de São Paulo, São José dos Campos, Brazil; Institute of Science and Technology, Universidade Federal de São Paulo, São José dos Campos, Brazil; Institute of Science and Technology, Universidade Federal de São Paulo, São José dos Campos, Brazil; Institute of Science and Technology, Universidade Federal de São Paulo, São José dos Campos, Brazil","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1652","1656","The aerial scene-classification task is a challenging problem to remote sensing area with important applicability to civil and military affairs. A technique that has achieved excellent results in this task is the convolutional neural network (CNN). CNNs are powerful semantic-level feature-extraction techniques successfully applied to many application domains. Nevertheless, many works in the literature have shown that a single CNN cannot solve all kinds of application domains properly. Hence, an alternative solution might be the joining of CNN architectures as an ensemble of classifiers. In this sense, this letter proposes a new strategy of deep feature-based classifier fusion through a meta-feature engineering approach based on the Kaizen programming (KP) technique for the aerial scene-classification task. KP is a technique that continuously improves partial solutions and combines them into a complete solution. In the context, a partial solution is a meta-feature, and a complete solution is an ensemble of classifiers. In our experiments on three different public data sets, we show that KP can automatically engineer meta-features that significantly improve the performance of a stacked classifier while reducing the number of total meta-features.","1558-0571","","10.1109/LGRS.2019.2950415","Brazilian National Council for Scientific and Technological Development (CNPq) through the Universal Project(grant numbers:408919/2016-7); São Paulo Research Foundation (FAPESP)(grant numbers:2016/07095-5,2018/13202-4); NVIDIA Corporation for the donation of the graphics processing units (GPUs); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897024","Evolutionary computation;image classification;neural networks","Task analysis;Feature extraction;Continuous improvement;Stacking;Remote sensing;Indexes;Neural networks","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;neural net architecture;remote sensing","automatic meta-feature engineering;CNN fusion;aerial scene classification;remote sensing area;CNN architectures;deep feature-based classifier fusion;Kaizen programming;semantic-level feature-extraction;convolutional neural network","","","","29","IEEE","12 Nov 2019","","","IEEE","IEEE Journals"
"Fast Georeferenced Aerial Image Stitching With Absolute Rotation Averaging and Planar- Restricted Pose Graph","Y. Zhao; G. Liu; S. Xu; S. Bu; H. Jiang; G. Wan","College of Aeronautics, Northwestern Polytechnical University, Xi’an, China; College of Aeronautics, Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; College of Aeronautics, Northwestern Polytechnical University, Xi’an, China; College of Aeronautics, Northwestern Polytechnical University, Xi’an, China; School of Aerospace Information, Aerospace Engineering University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","24 Mar 2021","2021","59","4","3502","3517","Accurate digital orthophoto map generation from high-resolution aerial images is important in various applications. Compared with the existing commercial software and the current state-of-the-art mosaicing systems, a novel fast georeferenced orthophoto mosaicing framework is proposed in this study. The framework can adapt to the challenging requirements of high-accuracy orthoimage generations with relatively fast speed, even if the overlap rate is low. We provide appearance and spatial correlation-constrained fast low-overlap neighbor candidate query and matching. On the basis of GPS information, we introduce an absolute position and rotation-averaging strategy for global pose initialization, which is essential for the high convergence and efficiency of nonconvex pose optimization of every image. We also propose a planar-restricted global pose graph optimization method. The optimization is extremely efficient and robust considering that point clouds are parameterized to planes. Finally, we apply a matching graph-based exposure compensation and region reduction algorithm for large-scale and high-resolution image fusion with high efficiency and novel precision. Experimental results demonstrate that our method can achieve the state-of-the-art performance while maintaining high precision and robustness.","1558-0644","","10.1109/TGRS.2020.3008517","National Key Research and Development Program of China(grant numbers:2018YFB2100601); National Natural Science Foundation of China(grant numbers:91860124,91646207,51875459,61620106003,61971418,61771026,61671451,61573284); Aeronautical Science Foundation of China(grant numbers:20170253003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9147004","Aerial images;digital orthophoto map (DOM);georeferenced;low overlap;mosaicing;planar","Simultaneous localization and mapping;Image reconstruction;Optimization;Real-time systems;Global Positioning System;Robustness;Image fusion","cartography;geophysical image processing;Global Positioning System;graph theory;image fusion;image matching;image registration;image resolution;image segmentation;optimisation;pose estimation;robot vision","novel precision;robustness;high-resolution image fusion;region reduction algorithm;matching graph-based exposure compensation;graph optimization method;planar-restricted;nonconvex pose optimization;high convergence;global pose initialization;rotation-averaging strategy;absolute position;GPS information;low-overlap neighbor candidate query;spatial correlation-constrained;overlap rate;relatively fast speed;high-accuracy orthoimage generations;orthophoto mosaicing framework;current state-of-the-art mosaicing systems;existing commercial software;high-resolution aerial images;accurate digital orthophoto map generation;pose graph;absolute rotation averaging;fast georeferenced aerial image stitching","","5","","50","IEEE","24 Jul 2020","","","IEEE","IEEE Journals"
"Edge-Guided Recurrent Positioning Network for Salient Object Detection in Optical Remote Sensing Images","X. Zhou; K. Shen; L. Weng; R. Cong; B. Zheng; J. Zhang; C. Yan","School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; Institute Information Science and the Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China; School of Automation, Hangzhou Dianzi University, Hangzhou, China","IEEE Transactions on Cybernetics","23 Dec 2022","2023","53","1","539","552","Optical remote sensing images (RSIs) have been widely used in many applications, and one of the interesting issues about optical RSIs is the salient object detection (SOD). However, due to diverse object types, various object scales, numerous object orientations, and cluttered backgrounds in optical RSIs, the performance of the existing SOD models often degrade largely. Meanwhile, cutting-edge SOD models targeting optical RSIs typically focus on suppressing cluttered backgrounds, while they neglect the importance of edge information which is crucial for obtaining precise saliency maps. To address this dilemma, this article proposes an edge-guided recurrent positioning network (ERPNet) to pop-out salient objects in optical RSIs, where the key point lies in the edge-aware position attention unit (EPAU). First, the encoder is used to give salient objects a good representation, that is, multilevel deep features, which are then delivered into two parallel decoders, including: 1) an edge extraction part and 2) a feature fusion part. The edge extraction module and the encoder form a U-shape architecture, which not only provides accurate salient edge clues but also ensures the integrality of edge information by extra deploying the intraconnection. That is to say, edge features can be generated and reinforced by incorporating object features from the encoder. Meanwhile, each decoding step of the feature fusion module provides the position attention about salient objects, where position cues are sharpened by the effective edge information and are used to recurrently calibrate the misaligned decoding process. After that, we can obtain the final saliency map by fusing all position attention cues. Extensive experiments are conducted on two public optical RSIs datasets, and the results show that the proposed ERPNet can accurately and completely pop-out salient objects, which consistently outperforms the state-of-the-art SOD models.","2168-2275","","10.1109/TCYB.2022.3163152","National Key Research and Development Program of China(grant numbers:2020YFB1406604); National Natural Science Foundation of China(grant numbers:61901145,62002014,62001146,61931008,61972123); Beijing Nova Program(grant numbers:Z201100006820016); Beijing Municipal Natural Science Foundation(grant numbers:4222013); Young Elite Scientist Sponsorship Program by the China Association for Science and Technology(grant numbers:2020QNRC001); Zhejiang Province Nature Science Foundation of China(grant numbers:LR17F030006,LY19F030022,LZ22F020003); Higher Education Discipline Innovation Project(grant numbers:D17019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756846","Edge;feature aggregation;optical remote sensing images (RSIs);position attention;salient object detection (SOD)","Feature extraction;Optical imaging;Image edge detection;Optical network units;Decoding;Optical sensors;Integrated optics","edge detection;feature extraction;geophysical image processing;image coding;image fusion;image segmentation;object detection;remote sensing","accurate salient edge clues;cluttered backgrounds;cutting-edge SOD models;diverse object types;edge extraction module;edge extraction part;edge features;edge-aware position attention unit;edge-guided recurrent positioning network;effective edge information;existing SOD models;numerous object orientations;object features;object scales;optical remote sensing images;position attention cues;public optical RSI datasets;salient object detection;salient objects;state-of-the-art SOD models","","9","","100","IEEE","13 Apr 2022","","","IEEE","IEEE Journals"
"Capsule Feature Pyramid Network for Building Footprint Extraction From High-Resolution Aerial Imagery","Y. Yu; Y. Ren; H. Guan; D. Li; C. Yu; S. Jin; L. Wang","Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; School of Energy and Power Engineering, Nanjing Institute of Technology, Nanjing, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China","IEEE Geoscience and Remote Sensing Letters","22 Apr 2021","2021","18","5","895","899","Building footprint extraction plays an important role in a wide range of applications. However, due to size and shape diversities, occlusions, and complex scenarios, it is still challenging to accurately extract building footprints from aerial images. This letter proposes a capsule feature pyramid network (CapFPN) for building footprint extraction from aerial images. Taking advantage of the properties of capsules and fusing different levels of capsule features, the CapFPN can extract high-resolution, intrinsic, and semantically strong features, which perform effectively in improving the pixel-wise building footprint extraction accuracy. With the use of signed distance maps as ground truths, the CapFPN can extract solid building regions free of tiny holes. Quantitative evaluations on an aerial image data set show that a precision, recall, intersection-over-union (IoU), and F-score of 0.928, 0.914, 0.853, and 0.921, respectively, are obtained. Comparative studies with six existing methods confirm the superior performance of the CapFPN in accurately extracting building footprints.","1558-0571","","10.1109/LGRS.2020.2986380","Natural Science Foundation of Jiangsu Province(grant numbers:BK20160427,BK20191214); National Natural Science Foundation of China(grant numbers:61603146,51975239,41971414,41671454); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075171","Aerial imagery;building extraction;building footprint;capsule feature pyramid network (CapFPN);capsule network","Feature extraction;Buildings;Remote sensing;Training;Shape;Generative adversarial networks;Gallium nitride","building information modelling;cartography;feature extraction;geophysical image processing;image classification;image fusion;image resolution","IoU;intersection-over-union;quantitative evaluations;ground truths;signed distance maps;semantically strong feature extraction;intrinsic feature extraction;high-resolution feature extraction;occlusions;shape diversities;aerial image data set;solid building region extraction;pixel-wise building footprint extraction accuracy;CapFPN;high-resolution aerial imagery;capsule feature pyramid network","","18","","28","IEEE","21 Apr 2020","","","IEEE","IEEE Journals"
"An Intensity-Space Domain CFAR Method for Ship Detection in HR SAR Images","C. Wang; F. Bi; W. Zhang; L. Chen","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, North China University of Technology, Beijing, China; School of Information and Electronics, North China University of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","4","529","533","Synthetic aperture radar (SAR) is an indispensable and extensively used sensor in ship detection. As high-resolution SAR introduces more spatial details into images, this letter proposes an intensity-space (IS) domain constant false alarm rate (CFAR) ship detector to make good use of this information. The method fuses intensity of each pixel and correlations between pixels into one characteristic, i.e., IS index. All the detection procedures center on the calculation and analysis of IS index. First, a new transform maps an image into a new IS domain. Structures like ships and wakes are enhanced in IS domain. Second, a CFAR detector picks up high IS index pixels. Third, a chain of target features is checked to screen out false candidate target pixels. Also, enhanced wakes are taken to improve detection results. Experiments on real SAR images validate that the proposed transform does enhance these structures and the whole algorithm is of good performance, especially in the case of low-contrast targets.","1558-0571","","10.1109/LGRS.2017.2654450","National Natural Science Foundation of China(grant numbers:61601006,91438203); Chang Jiang Scholars Program(grant numbers:T2012122); Hundred Leading Talent Project of Beijing Science and Technology(grant numbers:Z141101001514005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851007","Constant false alarm rate (CFAR);high-resolution (HR) synthetic aperture radar (SAR);intensity-space (IS) domain;ship detection;target enhancement","Marine vehicles;Synthetic aperture radar;Detectors;Transforms;Indexes;Image resolution;Clutter","geophysical image processing;image fusion;image resolution;object detection;radar imaging;remote sensing by radar;synthetic aperture radar;transforms","intensity-space domain CFAR method;ship detection;HR SAR Images;synthetic aperture radar;intensity-space domain constant false alarm rate ship detector;intensity fusion;IS index pixels","","86","","13","IEEE","13 Feb 2017","","","IEEE","IEEE Journals"
"Object-Based Analysis and Fusion of Optical and SAR Satellite Data for Dwelling Detection in Refugee Camps","K. Spröhnle; E. -M. Fuchs; P. Aravena Pelizari","German Aerospace Center (DLR), German Remote Sensing Data Center (DFD), 82234 Oberpfaffenhofen, Germany; German Aerospace Center (DLR), German Remote Sensing Data Center (DFD), 82234 Oberpfaffenhofen, Germany; German Aerospace Center (DLR), German Remote Sensing Data Center (DFD), 82234 Oberpfaffenhofen, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Apr 2017","2017","10","5","1780","1791","This study investigates the potential of very high spatial resolution (VHSR) optical WorldView-2 (WV-2) and single-polarized TerraSAR-X (TSX) synthetic aperture radar (SAR) satellite data for an automated detection of different dwelling types in a refugee camp by means of object-based image analysis (OBIA). First, the optical data and SAR data are analyzed independently, and then a fusion of both data sets is performed applying two different approaches: 1) an overlay operation-based procedure integrating the independent results of the optical- and SAR-based dwelling detection, and 2) a feature-based analysis approach taking advantage of the conjoint analysis of both data sets. The results of the single-sensor and the data fusion approaches are evaluated in detail on the basis of object-based and area-based accuracy assessments. Advantages and limitations of the analysis approaches are discussed. The accuracy rates reveal that the use of optical satellite data shows promising results regardless of the dwelling material, while the SAR data are suitable for the detection of metal sheet dwellings only. In complex camp areas, with closely spaced containers, the results of the independent analyses can be improved significantly by the proposed fusion approaches. The combination of SAR and optical data allows for the separation of contiguous dwellings in cases this was not possible by the optical sensor information.","2151-1535","","10.1109/JSTARS.2017.2664982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872448","Data fusion;dwelling detection;feature extraction;object-based image analysis (OBIA);refugee camp mapping;rule-based classification;synthetic aperture radar (SAR);very high spatial resolution (VHSR) data","Synthetic aperture radar;Optical sensors;Optical imaging;Satellites;Feature extraction;Adaptive optics;Optical network units","hyperspectral imaging;image fusion;optical sensors;remote sensing by radar;synthetic aperture radar","object-based analysis;optical satellite data;SAR satellite data;data fusion;refugee camp;very-high-spatial resolution optical WorldView-2;single-polarized TerraSAR-X;synthetic aperture radar;automated dwelling type detection;object-based image analysis;optical-based dwelling detection;SAR-based dwelling detection;feature-based analysis approach;optical sensor information;metal sheet dwelling detection","","20","","40","IEEE","6 Mar 2017","","","IEEE","IEEE Journals"
"Framework for Fusion of Ascending and Descending Pass TanDEM-X Raw DEMs","R. Deo; C. Rossi; M. Eineder; T. Fritz; Y. S. Rao","Centre of Studies in Resources Engineering, Indian Institute of Technology Bombay, Mumbai, India; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Centre of Studies in Resources Engineering, Indian Institute of Technology Bombay, Mumbai, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Aug 2015","2015","8","7","3347","3355","A novel method for calculating optimum incidence angle for the TanDEM-X system using any available digital elevation model (DEM) for the given area is proposed in this study. This method includes the plotting of slopes and aspect of the test area in a statistical way and applying mathematical approach using acquisition geometry in ascending and descending pass TanDEM-X data to optimize the incidence angle for obtaining precise DEM. Furthermore, the TanDEM-X raw DEMs in ascending and descending pass over Mumbai, India are combined using a simple weighted fusion algorithm and the quality of fused DEM thus generated is enhanced. The method adopted for fusion is just an experimental study. The problem of optimum weight selection for fusion has been addressed using height error map and a robust layover shadow mask calculated in “Integrated TanDEM-X Processor” (ITP) during TanDEM-X DEM generation. The height error map is calculated from the interferometric coherence with geometrical considerations and the robust layover and shadow map is calculated using TanDEM-X DEM and the corresponding slant range. Results show a significant reduction in the number of invalid pixels after fusion. In the fused DEM, invalids are only 2.14%, while ascending and descending pass DEMs have 5.02% and 6.34%, respectively. Statistical analysis shows a slight improvement in standard deviation of the error in fused DEM by 8% in urban area and about 5% for the whole scene. Only slight improvement in accuracy of fused DEM can be attributed to the coarse resolution of the SRTM-X DEM used as reference.","2151-1535","","10.1109/JSTARS.2015.2431433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112083","Fusion;layover;optimum incidence angle;TanDEM-X digital elevation model (DEM);weightage;Fusion;layover;optimum incidence angle;TanDEM-X digital elevation model (DEM);weightage","Coherence;Accuracy;Satellites;Synthetic aperture radar;Standards;Image color analysis","digital elevation models;geophysical image processing;image fusion;remote sensing","ascending pass TanDEM-X raw DEM;descending pass TanDEM-X raw DEM;TanDEM-X system;digital elevation model;statistical way;TanDEM-X data;Mumbai;India;fused DEM quality;height error map;Integrated TanDEM-X Processor;layover shadow mask;interferometric coherence;statistical analysis;urban area","","17","","28","IEEE","22 May 2015","","","IEEE","IEEE Journals"
"Spatial and Spectral Anisotropic Tensor Total Variation-Driven Adaptive Pansharpening","P. Liu","School of Computer Science and the Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","11 Jan 2022","2022","19","","1","5","This letter proposed a spatial and spectral anisotropic tensor total variation (SSATTV)-driven adaptive pansharpening model for the fusion of low-resolution (LR) multispectral (MS) and panchromatic (Pan) images to the high-resolution (HR) MS images. Except for the local spectral consistency constraint-based fidelity term between HR and LR MS used for spectral preservation, the proposed model reformulated the adaptive linear constraint-based fidelity term between Pan and HR MS with the tensor-representation modeling, particularly proposed an SSATTV prior term which imposed the spatial anisotropic TV prior between HR MS and Pan for spatial gradient feature preservation, and the spectral TV sparsity prior on HR MS for further spectral feature preservation. Furthermore, the proposed model was efficiently solved via the alternating direction method of multipliers (ADMM) scheme. Specifically, the experiments validated the superiority of the proposed SSATTV method.","1558-0571","","10.1109/LGRS.2021.3124164","National Natural Science Foundation of China(grant numbers:61802202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9592764","Pansharpening;spatial and spectral anisotropic tensor total variation (SSATTV) prior;tensor modeling;variational model","Adaptation models;Tensors;Pansharpening;Correlation;Computational modeling;Satellite broadcasting;Mathematical models","convex programming;geophysical image processing;image denoising;image fusion;image reconstruction;image resolution;iterative methods;remote sensing;tensors","local spectral consistency constraint-based fidelity term;LR MS;spectral preservation;adaptive linear constraint-based fidelity term;HR MS;tensor-representation modeling;SSATTV prior term;spatial anisotropic TV;spatial gradient feature preservation;spectral TV sparsity;spectral feature preservation;spatial-spectral anisotropic tensor total variation-driven adaptive pansharpening;alternating direction method of multipliers scheme;ADMM","","1","","16","IEEE","28 Oct 2021","","","IEEE","IEEE Journals"
"Horizon Picking Using Two-Branch Network With Spatial and Time–Frequency Features","X. Liao; J. Cao; Y. -J. Xue; J. You; M. Cheng","State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, Chengdu University of Technology, Chengdu, China; State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, Chengdu University of Technology, Chengdu, China; School of Communication Engineering, Chengdu University of Information Technology, Chengdu, China; State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, Chengdu University of Technology, Chengdu, China; State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, Chengdu University of Technology, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","In seismic interpretation, horizon picking is a very essential but time-consuming and challenging task. Most existing auto-picking algorithms have been proposed to improve the horizon interpretation efficiency. Recently, deep learning approaches have shown promising performance in horizon identification. However, feeding directly seismic time series or images into a deep learning network only uses the amplitude information of seismic signal, which limits the classification accuracy. In this letter, we propose to learn more distinctive characteristics in the time–frequency domain from the continuous wavelet transform (CWT) coefficients. More importantly, we develop a novel two-branch convolutional neural network (TB-CNN) for horizon picking: a CWT branch can mine the time–frequency features in 2-D CWT coefficients of seismic time series. At the same time, a spatial branch further explores the local spatial features in seismic images. The features of the two branches are then fused to perform classification. The output is the class scores of voxels being horizon or background. Finally, we extract the horizon surface by finding all voxels with the highest score values of the horizon class in the vertical temporal direction. We conduct experiments on both synthetic and field data. The results show that the proposed method can effectively fuse the spatial features and time–frequency features to yield higher performance than the traditional 3-D auto-tracking method.","1558-0571","","10.1109/LGRS.2021.3118685","National Natural Science Foundation of China(grant numbers:42030812,41974160,42042046,41430323); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564070","Continuous wavelet transform (CWT);horizon picking;two-branch convolutional neural network (TB-CNN)","Continuous wavelet transforms;Time-frequency analysis;Convolutional neural networks;Feature extraction;Training;Three-dimensional displays;Time series analysis","convolutional neural nets;data mining;deep learning (artificial intelligence);geophysical image processing;image classification;image fusion;remote sensing;seismology;time series;time-frequency analysis;wavelet transforms","horizon picking;time-frequency features;seismic interpretation;auto-picking algorithms;horizon interpretation efficiency;horizon identification;seismic time series;deep learning network;seismic signal;time-frequency domain;two-branch convolutional neural network;spatial branch;local spatial features;seismic images;2D CWT coefficients;TB-CNN;branch fusion;branch mining","","","","15","IEEE","8 Oct 2021","","","IEEE","IEEE Journals"
"Spatiotemporal-Spectral Fusion for Gaofen-1 Satellite Images","J. Wei; H. Yang; W. Tang; Q. Li","Institute of Space Science and Technology, Nanchang University, Nanchang, China; School of Information Engineering, Nanchang University, Nanchang, China; Institute of Space Science and Technology, Nanchang University, Nanchang, China; Taiyuan University, Taiyuan, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Due to the limitations of hardware technology, satellite sensors cannot obtain images with high temporal, spatial, and spectral resolutions at the same time. Current spatiotemporal fusion methods try to solve the contradiction between temporal resolution and spatial resolution, which cannot achieve good reconstruction accuracy partly because the data sources are from heterogeneous platforms with long chains difficult to be modeled. Different from the crossing-platform fusion, this work proposes to improve the spatial and temporal resolutions on a single platform. For the 2-m panchromatic images, 8-m multispectral images, and 16-m wide-field-view images captured by the Gaofen-1 satellite, our goal is to produce 2-m multispectral images with high temporal resolutions. Two convolutional neural networks are built to solve this spatiotemporal-spectral fusion issue with pansharpening and spatiotemporal fusion in serial. In the validation stage, the 2-m multispectral images are built and evaluated with the panchromatic images and 8-m multispectral images. The digital and visual evaluations show that our method can produce visually acceptable fusion quality, which may enhance the feasibility of the Gaofen-1 data.","1558-0571","","10.1109/LGRS.2021.3111961","National Natural Science Foundation of China(grant numbers:61860130,41974195); 03 Special and 5G Project of the Jiangxi Province(grant numbers:20204ABC03A40); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607025","Gaofen-1;multispectral;pansharpen;spatiotemporal fusion","Spatiotemporal phenomena;Spatial resolution;Image resolution;Pansharpening;Satellites;Sensors;Sensor fusion","convolutional neural nets;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing;spatiotemporal phenomena;spectral analysis","gaofen-1 satellite images;satellite sensors;high temporal resolutions;spectral resolutions;temporal resolution;spatial resolution;crossing-platform fusion;spatial resolutions;panchromatic images;8-m multispectral images;16-m wide-field-view images;2-m multispectral images;visually acceptable fusion quality;Gaofen-1 data;spatiotemporal-spectral fusion;digital evaluation;visual evaluation","","","","11","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"A Blind Full Resolution Assessment Method for Pansharpened Images Based on Multistream Collaborative Learning","K. Bao; X. Meng; X. Chai; F. Shao","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China","IEEE Transactions on Geoscience and Remote Sensing","9 Jun 2022","2022","60","","1","11","Pansharpening aims to fuse a high spatial resolution (HR) panchromatic (PAN) image and a low spatial resolution (LR) multispectral (MS) image to obtain an HR-MS image. However, due to the lack of the real HR-MS reference image, determining pansharpened image quality at full resolution (FR) has always been a contentious issue in the community. We propose a blind FR assessment method for pansharpened images based on multistream collaborative learning. The proposed method designs a Siamese framework to collaboratively learn the spatial, spectral, and overall quality of the fused image. The parameters of the feature extraction layer in the spatial and spectral evaluation models are frozen for the overall evaluation model, thus improving accuracy and convergence speed. The proposed method was comprehensively tested and verified based on a large-scale dataset consisting of 13 620 fused images obtained by six pansharpening methods with four different thematic datasets. Furthermore, a large-scale subjective evaluation dataset, in which each of the 13 620 fused images was assessed by 28 participants, was utilized to comprehensively validate the proposed method. The experimental results demonstrated the superior performance of the proposed method to other state-of-the-art quality assessments.","1558-0644","","10.1109/TGRS.2022.3177607","National Natural Science Foundation of China(grant numbers:42171326,62071261,41801252); Natural Science Foundation of Zhejiang Province(grant numbers:LY22F010014); Fellowship of the China Postdoctoral Science Foundation(grant numbers:2020M672490); K. C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780364","Data-driven;deep learning (DL);full resolution (FR) evaluation;nonreference (NR) evaluation;pansharpening","Pansharpening;Feature extraction;Training;Spatial resolution;Principal component analysis;Energy resolution;Optimization","feature extraction;geophysical image processing;groupware;image fusion;image resolution;learning (artificial intelligence);remote sensing;spectral analysis","low spatial resolution multispectral image;HR-MS reference image;pansharpened image quality;blind FR assessment method;multistream collaborative learning;spatial quality;spectral quality;spatial evaluation models;spectral evaluation models;large-scale subjective evaluation dataset;high spatial resolution panchromatic image;blind full resolution assessment;image fusion;thematic dataset;remote sensing;feature extraction layer","","1","","53","IEEE","23 May 2022","","","IEEE","IEEE Journals"
"Learning-Shared Cross-Modality Representation Using Multispectral-LiDAR and Hyperspectral Data","D. Hong; J. Chanussot; N. Yokoya; J. Kang; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project (AIP), RIKEN, Tokyo, Japan; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IEEE Geoscience and Remote Sensing Letters","21 Jul 2020","2020","17","8","1470","1474","Due to the ever-growing diversity of the data source, multimodality feature learning has attracted more and more attention. However, most of these methods are designed by jointly learning feature representation from multimodalities that exist in both training and test sets, yet they are less investigated in the absence of certain modality in the test phase. To this end, in this letter, we propose to learn a shared feature space across multimodalities in the training process. By this way, the out-of-sample from any of multimodalities can be directly projected onto the learned space for a more effective cross-modality representation. More significantly, the shared space is regarded as a latent subspace in our proposed method, which connects the original multimodal samples with label information to further improve the feature discrimination. Experiments are conducted on the multispectral-Light Detection and Ranging (LIDAR) and hyperspectral data set provided by the 2018 IEEE GRSS Data Fusion Contest to demonstrate the effectiveness and superiority of the proposed method in comparison with several popular baselines.","1558-0571","","10.1109/LGRS.2019.2944599","German Research Foundation (DFG)(grant numbers:ZH 498/7-2); Helmholtz Association under the framework of the Young Investigators Group SiPEO (VH-NG-1018); European Research Council (ERC) under the European Unions Horizon 2020 Research and Innovation Program(grant numbers:ERC-2016-StG-714087, Acronym: So2Sat); Japan Society for the Promotion of Science (KAKENHI)(grant numbers:18K18067); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8976086","Cross-modality;feature learning;hyperspectral;multimodality;multispectral-Light Detection and Ranging (LIDAR);shared subspace learning","Laser radar;Hyperspectral imaging;Training;Feature extraction;Earth;Data models","feature extraction;geophysical image processing;image fusion;image representation;learning (artificial intelligence);optical radar;remote sensing by laser beam","multispectral-LiDAR;hyperspectral data;data source;multimodality feature learning;feature representation;test phase;shared feature space;training process;learned space;effective cross-modality representation;feature discrimination;2018 IEEE GRSS Data Fusion Contest;learning-shared cross-modality representation;multispectral-light detection and ranging","","27","","16","CCBY","30 Jan 2020","","","IEEE","IEEE Journals"
"TR-MISR: Multiimage Super-Resolution Based on Feature Fusion With Transformers","T. An; X. Zhang; C. Huo; B. Xue; L. Wang; C. Pan","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Feb 2022","2022","15","","1373","1388","Multiimage super-resolution (MISR), as one of the most promising directions in remote sensing, has become a needy technique in the satellite market. A sequence of images collected by satellites often has plenty of views and a long time span, so integrating multiple low-resolution views into a high-resolution image with details emerges as a challenging problem. However, most MISR methods based on deep learning cannot make full use of multiple images. Their fusion modules are incapable of adapting to an image sequence with weak temporal correlations well. To cope with these problems, we propose a novel end-to-end framework called TR-MISR. It consists of three parts: An encoder based on residual blocks, a transformer-based fusion module, and a decoder based on subpixel convolution. Specifically, by rearranging multiple feature maps into vectors, the fusion module can assign dynamic attention to the same area of different satellite images simultaneously. In addition, TR-MISR adopts an additional learnable embedding vector that fuses these vectors to restore the details to the greatest extent. TR-MISR has successfully applied the transformer to MISR tasks for the first time, notably reducing the difficulty of training the transformer by ignoring the spatial relations of image patches. Extensive experiments performed on the PROBA-V Kelvin dataset demonstrate the superiority of the proposed model that provides an effective method for transformers in other low-level vision tasks.","2151-1535","","10.1109/JSTARS.2022.3143532","National Key Research and Development Program of China(grant numbers:2018AAA0100400); National Natural Science Foundation of China(grant numbers:62071466,61802407); Natural Science Foundation of Guangxi Province(grant numbers:2018GXNSFBA281086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684717","Deep learning;end-to-end networks;feature extraction and fusion;multiimage super-resolution (MISR);remote sensing;transformers","Transformers;Superresolution;Satellites;Remote sensing;Task analysis;Deep learning;Image resolution","computer vision;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image resolution;image sequences;remote sensing","TR-MISR;feature fusion;satellite market;low-resolution views;high-resolution image;MISR methods;image sequence;end-to-end framework;transformer-based fusion module;multiple feature maps;MISR tasks;image patches","","5","","84","CCBY","18 Jan 2022","","","IEEE","IEEE Journals"
"Deep Encoder–Decoder Networks for Classification of Hyperspectral and LiDAR Data","D. Hong; L. Gao; R. Hang; B. Zhang; J. Chanussot","Université Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Grenoble, France; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Deep learning (DL) has been garnering increasing attention in remote sensing (RS) due to its powerful data representation ability. In particular, deep models have been proven to be effective for RS data classification based on a single given modality. However, with one single modality, the ability in identifying the materials remains limited due to the lack of feature diversity. To overcome this limitation, we present a simple but effective multimodal DL baseline by following a deep encoder–decoder network architecture, EndNet for short, for the classification of hyperspectral and light detection and ranging (LiDAR) data. EndNet fuses the multimodal information by enforcing the fused features to reconstruct the multimodal input in turn. Such a reconstruction strategy is capable of better activating the neurons across modalities compared with some conventional and widely used fusion strategies, e.g., early fusion, middle fusion, and late fusion. Extensive experiments conducted on two popular hyperspectral and LiDAR data sets demonstrate the superiority and effectiveness of the proposed EndNet in comparison with several state-of-the-art baselines in the hyperspectral-LiDAR classification task. The codes will be available at https://github.com/danfenghong/IEEE_GRSL_EndNet, contributing to the RS community.","1558-0571","","10.1109/LGRS.2020.3017414","National Key Research and Development Program of China(grant numbers:2016YFB0500502); National Natural Science Foundation of China(grant numbers:41722108); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9179756","Classification;deep learning (DL);encoder--decoder;hyperspectral;light detection and ranging (LiDAR);multimodality;remote sensing (RS)","Laser radar;Feature extraction;Training;Hyperspectral imaging;Testing;Image reconstruction","deep learning (artificial intelligence);hyperspectral imaging;image classification;image coding;image fusion;image reconstruction;image representation;optical radar;remote sensing by laser beam","LiDAR data sets;remote sensing;powerful data representation ability;RS data classification;single modality;feature diversity;deep encoder-decoder network architecture;fused features;reconstruction strategy;fusion strategies;hyperspectral-LiDAR classification task;hyperspectral and light detection and ranging data classification;multimodal deep learning baseline","","19","","23","IEEE","28 Aug 2020","","","IEEE","IEEE Journals"
"S²ENet: Spatial–Spectral Cross-Modal Enhancement Network for Classification of Hyperspectral and LiDAR Data","S. Fang; K. Li; Z. Li","College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","The effective utilization of multimodal data (e.g., hyperspectral and light detection and ranging (LiDAR) data) has profound implications for further development of the remote sensing (RS) field. Many studies have explored how to effectively fuse features from multiple modalities; however, few of them focus on information interactions that can effectively promote the complementary semantic content of multisource data before fusion. In this letter, we propose a spatial–spectral enhancement module (S2EM) for cross-modal information interaction in deep neural networks. Specifically, S2EM consists of SpAtial Enhancement Module (SAEM) for enhancing spatial representation of hyperspectral data by LiDAR features and SpEctral Enhancement Module (SEEM) for enhancing spectral representation of LiDAR data by hyperspectral features. A series of experiments and ablation studies on the Houston2013 dataset show that S2EM can effectively facilitate the interaction and understanding between multimodal data. Our source code is available at https://github.com/likyoo/Multimodal-Remote-Sensing-Toolkit, contributing to the RS community.","1558-0571","","10.1109/LGRS.2021.3121028","Natural Science Foundation of Shandong Province(grant numbers:ZR2020MF132); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583936","Classification;deep learning;hyperspectral;light detection and ranging (LiDAR);multimodal;remote sensing (RS)","Laser radar;Hyperspectral imaging;Feature extraction;Deep learning;Research and development;Distance measurement;Convolution","feature extraction;geophysical image processing;human computer interaction;image classification;image fusion;neural nets;object detection;optical radar;remote sensing;sensor fusion","spatial-spectral cross-modal Enhancement network;LiDAR data;multimodal data;hyperspectral detection;light detection;profound implications;remote sensing field;multiple modalities;information interactions;complementary semantic content;multisource data;spatial-spectral enhancement module;S 2 EM;cross-modal information interaction;deep neural networks;SpAtial Enhancement Module;spatial representation;hyperspectral data;LiDAR features;spectral representation;hyperspectral features","","6","","20","IEEE","21 Oct 2021","","","IEEE","IEEE Journals"
"OptFus: Optical Sensor Fusion for the Classification of Multisource Data: Application to Mineralogical Mapping","B. Rasti; P. Ghamisi; R. Gloaguen","Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","We propose a new fusion-based classification technique for optical multisource remote-sensing images called OptFus. OptFus is developed to merge and process optical imagery having different spatial and spectral resolutions. The spatial features are extracted using morphological filters from the RGB data containing high spatial resolution. A feature fusion technique is developed to combine all the sensor data in a subspace using a common set of representative features. Finally, the fused features are classified using a support vector machine to ensure a robust supervised spectral classification. The proposed method is designed to allocate varying weights to the data from various imaging sensors in the fusion process. OptFus is applied to two multisource optical datasets captured from geological drill-core samples. The classification accuracy demonstrates considerable improvements compared to the state-of-the-art. A MATLAB implementation of OptFus is available online: https://github.com/BehnoodRasti/OptFus.","1558-0571","","10.1109/LGRS.2021.3132701","Alexander-von-Humboldt-Stiftung/Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635804","Classification;fusion;hyperspectral image;multisource data;remote sensing","Minerals;Feature extraction;Training;Hyperspectral imaging;Optical sensors;TV;Minimization","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image sensors;learning (artificial intelligence);pattern classification;remote sensing;sensor fusion;support vector machines","fusion-based classification technique;remote-sensing images;optical imagery;different spatial;spectral resolutions;spatial features;RGB data;high spatial resolution;feature fusion technique;sensor data;representative features;fused features;robust supervised spectral classification;imaging sensors;fusion process;multisource optical datasets;classification accuracy;optical sensor fusion;multisource data","","","","12","IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"Spectral Superresolution of Multispectral Imagery With Joint Sparse and Low-Rank Learning","L. Gao; D. Hong; J. Yao; B. Zhang; P. Gamba; J. Chanussot","Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; GIPSA-lab, Grenoble INP, CNRS, University of Grenoble Alpes, Grenoble, France; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Department of Electrical, Computer and Biomedical Engineering, University of Pavia, Pavia, Italy; INRIA, CNRS, Grenoble INP, LJK, University of Grenoble Alpes, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","24 Feb 2021","2021","59","3","2269","2280","Extensive attention has been widely paid to enhance the spatial resolution of hyperspectral (HS) images with the aid of multispectral (MS) images in remote sensing. However, the ability in the fusion of HS and MS images remains to be improved, particularly in large-scale scenes, due to the limited acquisition of HS images. Alternatively, we super-resolve MS images in the spectral domain by the means of partially overlapped HS images, yielding a novel and promising topic: spectral superresolution (SSR) of MS imagery. This is challenging and less investigated task due to its high ill-posedness in inverse imaging. To this end, we develop a simple but effective method, called joint sparse and low-rank learning (J-SLoL), to spectrally enhance MS images by jointly learning low-rank HS-MS dictionary pairs from overlapped regions. J-SLoL infers and recovers the unknown HS signals over a larger coverage by sparse coding on the learned dictionary pair. Furthermore, we validate the SSR performance on three HS-MS data sets (two for classification and one for unmixing) in terms of reconstruction, classification, and unmixing by comparing with several existing state-of-the-art baselines, showing the effectiveness and superiority of the proposed J-SLoL algorithm. Furthermore, the codes and data sets will be available at https://github.com/danfenghong/IEEE_TGRS_J-SLoL, contributing to the remote sensing (RS) community.","1558-0644","","10.1109/TGRS.2020.3000684","National Natural Science Foundation of China(grant numbers:41722108,91638201); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120344","Dictionary learning;hyperspectral;joint learning;low-rank;multispectral;remote sensing;sparse representation;superresolution","Dictionaries;Image reconstruction;Task analysis;Spatial resolution;Imaging","geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;image reconstruction;image representation;image resolution;learning (artificial intelligence);remote sensing;spectral analysis","spectral superresolution;multispectral imagery;low-rank learning;hyperspectral images;multispectral images;MS images;spectral domain;partially overlapped HS images;MS imagery;inverse imaging;joint sparse;low-rank HS-MS;unknown HS signals;learned dictionary pair;HS-MS data sets","","77","","58","IEEE","18 Jun 2020","","","IEEE","IEEE Journals"
"Pan-Sharpening Based on Joint Visual Saliency Analysis and Parallel Bidirectional Network","W. Zhu; L. Zhang","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","12 Oct 2022","2022","19","","1","5","In remote sensing (RS) images, the demands for spectral and spatial quality of different regions are different, which means that the unified fusion strategy on the whole image is not suitable for pan-sharpening task. Saliency, derived from visual attention mechanism, provides an effective way to satisfy these demands. Inspired by this, we propose a novel pan-sharpening method based on joint visual saliency analysis and parallel bidirectional network (JSPBN). First, considering the complex scenes and uneven distribution of targets in RS images, we develop a Bayesian optimization-based joint visual saliency analysis (B-JVSA) method that integrates prior saliency based on global color contrast with likelihood saliency based on joint co-occurrence histogram, which can highlight common salient regions while suppressing individual ones and irrelevant background by exploring the correlation among multiple RS images. Second, we construct a parallel bidirectional feature pyramid (PBFP) network to obtain coarse fusion features, fully considering individual characteristics of panchromatic (PAN) images and multispectral (MS) images. Finally, we design a saliency-aware layer (SAL) according to B-JVSA to further refine the fusion effect in salient regions and nonsalient regions. With the help of SAL, diverse strategies for certain regions are learned through two independent residual dense networks (RDNs) and thereby generating accurate fusion results. Experimental results show that our proposal performs better than the competing methods in both spatial quality enhancement and spectral fidelity preservation.","1558-0571","","10.1109/LGRS.2022.3209787","National Natural Science Foundation of China(grant numbers:62271060,61571050,41771407); Beijing Natural Science Foundation(grant numbers:4222046); BNU Interdisciplinary Research Foundation for the First-Year Doctoral Candidates(grant numbers:BNUXKJC2120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903428","Joint visual saliency analysis;pan-sharpening;parallel bidirectional network;remote sensing (RS)","Visualization;Image color analysis;Feature extraction;Bayes methods;Histograms;Convolution;Generative adversarial networks","Bayes methods;deep learning (artificial intelligence);feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;optimisation;remote sensing","visual attention mechanism;pan-sharpening method;parallel bidirectional network;Bayesian optimization-based joint visual saliency analysis method;prior saliency;likelihood saliency;joint co-occurrence histogram;multiple RS images;parallel bidirectional feature pyramid network;coarse fusion features;panchromatic images;multispectral images;saliency-aware layer;independent residual dense networks;remote sensing images;spectral quality;spatial quality;unified fusion strategy;B-JVSA;RDNs;SAL;PAN;MS;global color contrast","","2","","16","IEEE","26 Sep 2022","","","IEEE","IEEE Journals"
"PGMAN: An Unsupervised Generative Multiadversarial Network for Pansharpening","H. Zhou; Q. Liu; Y. Wang","State Key Laboratory of Virtual Reality Technology and Systems and the Hangzhou Innovation Institute, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems and the Hangzhou Innovation Institute, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems and the Hangzhou Innovation Institute, Beihang University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jul 2021","2021","14","","6316","6327","Pansharpening aims at fusing a low-resolution multispectral (MS) image and a high-resolution (HR) panchromatic (PAN) image acquired by a satellite to generate an HR MS image. Many deep learning based methods have been developed in the past few years. However, since there are no intended HR MS images as references for learning, almost all of the existing methods downsample the MS and PAN images and regard the original MS images as targets to form a supervised setting for training. These methods may perform well on the down-scaled images; however, they generalize poorly to the full-resolution images. To conquer this problem, we design an unsupervised framework that is able to learn directly from the full-resolution images without any preprocessing. The model is built based on a novel generative multiadversarial network. We use a two-stream generator to extract the modality-specific features from the PAN and MS images, respectively, and develop a dual discriminator to preserve the spectral and spatial information of the inputs when performing fusion. Furthermore, a novel loss function is introduced to facilitate training under the unsupervised setting. Experiments and comparisons with other state-of-the-art methods on GaoFen-2, QuickBird, and WorldView-3 images demonstrate that the proposed method can obtain much better fusion results on the full-resolution images. Code is available. [Online]. Available: https://github.com/zhysora/PGMAN.","2151-1535","","10.1109/JSTARS.2021.3090252","National Natural Science Foundation of China(grant numbers:41871283,61601011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463717","Generative adversarial network (GAN);image fusion;pansharpening;unsupervised learning","Generators;Training;Deep learning;Convolution;Task analysis;Remote sensing;Spatial resolution","feature extraction;geophysical image processing;image classification;image fusion;image resolution;image sampling;learning (artificial intelligence);remote sensing","high-resolution panchromatic image;HR MS image;deep learning based methods;intended HR MS images;original MS images;full-resolution images;novel generative multiadversarial network;two-stream generator;WorldView-3 images;unsupervised generative multiadversarial network;low-resolution multispectral image","","19","","51","CCBY","23 Jun 2021","","","IEEE","IEEE Journals"
"A Novel Multispectral, Panchromatic and SAR Data Fusion for Land Classification","P. Iervolino; R. Guida; D. Riccio; R. Rea","Airbus Defence and Space, Guildford; Surrey Space Centre, University of Surrey, Guildford, U.K.; Dipartimento di Ingegneria Elettrica e Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli “Federico II”, Napoli, Italy; Dipartimento di Ingegneria Elettrica e Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli “Federico II”, Napoli, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Nov 2019","2019","12","10","3966","3979","Multisensor data fusion is addressed in this article for land classification purposes in a semiarid environment. A novel algorithm based on multispectral, panchromatic and synthetic aperture radar (SAR) data is here presented. The proposed multisensory data fusion approach relies on the generalized intensity-hue-saturation (G-IHS) transform and the À trous wavelet transform (ATWT). The fusion product is obtained by modulating the high features details of the panchromatic ATWT with the SAR texture and by replacing the high-pass details of the G-IHS Intensity component with this panchromatic-SAR modulation. After the fusion product is derived, a classification is performed by using a standard maximum likelihood classifier. The proposed algorithm is tested over a meaningful case study acquired over the Maspalomas Special Natural Reserve (Spain) and processing data from WorldView-2 (for both multispectral and panchromatic channels) and TerraSAR-X (for the SAR channel) missions. Results show a fine preservation of the spectral information contained in each multispectral band. Sharpened details are observed over built-up areas and a smoothing texture is perceived over homogeneous areas (lakes, sea, bare soil, and roads) due to the SAR-panchromatic modulation. This leads to a better overall classification accuracy of the fused image compared to outcomes obtained with a single sensor, resulting 7% and 2% more accurate than multispectral and pan-sharpening classification, respectively.","2151-1535","","10.1109/JSTARS.2019.2945188","University of Surrey; Universitá degli Studi di Napoli Federico II; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869906","Image classification;image fusion;multispectral imaging;synthetic aperture radar (SAR)","Synthetic aperture radar;Data integration;Earth;Remote sensing;Feature extraction;Satellites;Transforms","geophysical image processing;image classification;image fusion;image resolution;image texture;maximum likelihood estimation;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping;wavelet transforms","SAR channel;multispectral band;SAR-panchromatic modulation;classification accuracy;pan-sharpening classification;novel multispectral;SAR data fusion;land classification purposes;semiarid environment;synthetic aperture radar data;multisensory data fusion approach;generalized intensity-hue-saturation;fusion product;high features details;panchromatic ATWT;SAR texture;high-pass details;G-IHS Intensity component;panchromatic-SAR modulation;standard maximum likelihood classifier;Maspalomas Special Natural Reserve;WorldView-2;TerraSAR-X","","13","","51","IEEE","16 Oct 2019","","","IEEE","IEEE Journals"
"A Variational Approach for Fusion of Panchromatic and Multispectral Images Using a New Spatial–Spectral Consistency Term","M. Khateri; F. Shabanzade; F. Mirzapour; A. Zaji; Z. Liu","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran; Department of Electrical Engineering, Sadra Institute of Higher Education, Tehran, Iran; Faculty of Applied Science, School of Engineering, The University of British Columbia, Kelowna, BC, Canada; Faculty of Applied Science, School of Engineering, The University of British Columbia, Kelowna, BC, Canada","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jul 2020","2020","13","","3421","3436","In this article, we propose a variational approach for fusion of two coregistered high-resolution panchromatic (HRP) and low-resolution multispectral (LRM) images to reach the high-resolution multispectral (HRM) one, i.e., pan-sharpening. In this fusion technique, there is a tradeoff between structural information of an HRP image and spectral information of LRM one. To reconstruct the HRM image, which benefits from the best characteristics of both images, we consider several fidelity terms. The structural fidelity term is used to transfer structural information of an HRP image to HRM one, and a spectral fidelity term is utilized to preserve spectral consistency between HRM and LRM images throughout the fusion process. To reduce the spectral distortion occurred due to the discrepancy between intensity values of HRP and LRM images, a novel spatial-spectral fidelity term is designed to keep the intensity ratio between multispectral and panchromatic pixels in the high-resolution space as the same as the low-resolution space. Moreover, the total variation (TV) regularization term is employed as a prior to promote the sparseness of gradient in HRM bands. These fidelity terms were formulated in a convex optimization problem. However, the structural and TV terms made this optimization problem nondifferentiable. Therefore, we developed an efficient majorization-minimization algorithm for solving the optimization problem. The proposed method applied to three datasets, acquired by WorldView-3, Deimos-2, and QuickBird satellites. To assess the effectiveness of the proposed method, visual analysis, as well as quantitative comparison to various pan-sharpening methods, was carried out. The experimental results suggested that the proposed method outperformed the competitors visually and quantitatively.","2151-1535","","10.1109/JSTARS.2020.3002780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119131","Image fusion;majorization–minimization (MM) algorithm;multispectral;panchromatic;variational pan-sharpening","Spatial resolution;Remote sensing;TV;Distortion;Image reconstruction;Optimization","geophysical image processing;image fusion;image reconstruction;image resolution;optimisation;remote sensing","structural fidelity term;HRM image;spectral information;structural information;fusion technique;high-resolution multispectral;new spatial-spectral consistency term;multispectral images;variational approach;optimization problem;structural TV terms;fidelity terms;HRM bands;total variation regularization term;low-resolution space;high-resolution space;panchromatic pixels;multispectral pixels;novel spatial-spectral fidelity term;spectral distortion;fusion process;LRM images;HRP image","","4","","49","CCBY","16 Jun 2020","","","IEEE","IEEE Journals"
"Pansharpening Via Neighbor Embedding of Spatial Details","J. Liu; C. Zhou; R. Fei; C. Zhang; J. Zhang","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Information Science, Guangzhou University, Guangzhou, China; School of Electronic Information and Artificial Intelligence, Shaanxi University of Science and Technology, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","23 Apr 2021","2021","14","","4028","4042","The spatial details injection model has been considered as a general framework in the literature of pansharpening, and recently there have been significant advances in this framework based on sparse representation (SR) of spatial details. However, the SR-based methods have greater computational burden in estimating the sparse vectors and limited ability in detail edge preservation. In this article, we introduce the neighbor embedding (NE) instead of SR-based model and the edge-preserving filter into the spatial detail injection framework to address the aforementioned two drawbacks. By utilizing the best quality of NE, we propose the detail injection via NE (DINE) algorithm for pansharpening, and DINE+, an improved variant of DINE by using the edge-preserving filter to enhance the spatial details. Experiments carried on three datasets captured by different satellite sensors and compared with current state-of-the-art methods validate the effectiveness of the proposed methods.","2151-1535","","10.1109/JSTARS.2021.3067877","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2020AAA0105601); National Natural Science Foundation of China(grant numbers:61877049,61976174); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384242","Image fusion;neighbor embedding (NE);pansharpening;sparse representation (SR)","Pansharpening;Spatial resolution;Optimization;Image edge detection;Remote sensing;Image reconstruction;High frequency","filtering theory;geophysical image processing;image denoising;image fusion;image representation;image resolution;remote sensing","spatial details injection model;pansharpening;sparse representation;SR-based methods;greater computational burden;sparse vectors;detail edge preservation;SR-based model;edge-preserving filter;spatial detail injection framework;NE algorithm;DINE;neighbor embedding","","4","","80","CCBY","23 Mar 2021","","","IEEE","IEEE Journals"
"Spectral and Spatial Feature Fusion for Hyperspectral Image Classification","S. Hao; Y. Xia; L. Zhou; Y. Ye; W. Wang","College of Information and Control Engineering, Qingdao University of Technology, Qingdao, China; College of Information and Control Engineering, Qingdao University of Technology, Qingdao, China; College of Information and Control Engineering, Qingdao University of Technology, Qingdao, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Department of Information Engineering and Computer Science, University of Trento, Povo, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","1 Dec 2022","2022","19","","1","5","Compared with traditional images, hyperspectral images (HSIs) not only have spatial information, but also have rich spectral information. However, the mainstream hyperspectral image classification (HIC) methods are all based on convolutional neural network (CNN), which has great advantages in extracting spatial features, but it has certain limitations in dealing with spectral continuous sequence information. Therefore, Transformer, which is good at processing sequences, has also been gradually applied to HIC. Besides, since HSI is typical 3-D structures, we believe that the correlation of the three dimensions is also an important information. So, in order to fully extract the spectral–spatial information, as well as the correlation of the three dimensions, we propose a spectral and spatial feature fusion module (i.e., TransCNN) for HIC. TransCNN consists of CNNs and a Transformer. The former is in charge of mining the spatial and spectral information from different dimensions, while the latter not only undertakes the most critical fusion but also captures the deeper relationship characteristics. We transpose the data to extract features and their correlation through three CNNs branches. We believe that these feature maps still have deep spectral information. Therefore, we have embedded them into 1-D vectors and use Transformer’s encoder to extract features. However, some information will be lost when embedding into 1-D vectors. Therefore, we use decoder, which has been ignored in the field of vision, to fuse the features before passing encoder and the features after extracted by encoders. Two kinds of features are fused by decoder, and the obtained information is finally input into the classifier for classification. Experimental results on real HSIs show that the proposed architecture can achieve competitive performance compared with the state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3223090","National Natural Science Foundation of China(grant numbers:62171247,41921781); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954429","Convolutional neural networks (CNNs);deep learning;fusion;hyperspectral image classification (HIC);remote sensing;Transformer","Feature extraction;Transformers;Decoding;Data mining;Correlation;Semantics;Three-dimensional displays","convolutional neural nets;deep learning (artificial intelligence);feature extraction;hyperspectral imaging;image classification;image fusion","convolutional neural network;deep spectral information;feature maps;HIC;HSIs;mainstream hyperspectral image classification methods;spatial feature fusion module;spectral continuous sequence information;spectral feature fusion module;spectral-spatial information;TransCNN;transformers encoder","","","","13","IEEE","17 Nov 2022","","","IEEE","IEEE Journals"
"MLFcGAN: Multilevel Feature Fusion-Based Conditional GAN for Underwater Image Color Correction","X. Liu; Z. Gao; B. M. Chen","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1488","1492","Color correction for underwater images has received increasing interest, due to its critical role in facilitating available mature vision algorithms for underwater scenarios. Inspired by the stunning success of deep convolutional neural network (DCNN) techniques in many vision tasks, especially the strength in extracting features in multiple scales, we propose a deep multiscale feature fusion net based on the conditional generative adversarial network (GAN) for underwater image color correction. In our network, multiscale features are extracted first, followed by augmenting local features in each scale with global features. This design was verified to facilitate more effective and faster network learning, resulting in better performance in both color correction and detail preservation. We conducted extensive experiments and compared the results with state-of-the-art approaches quantitatively and qualitatively, showing that our method achieves significant improvements.","1558-0571","","10.1109/LGRS.2019.2950056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894129","Conditional generative adversarial network (cGAN);feature extraction and fusion;image enhancement;underwater image color correction","Feature extraction;Image color analysis;Gallium nitride;Loss measurement;Generative adversarial networks;Image restoration;Generators","computer vision;convolutional neural nets;feature extraction;image classification;image colour analysis;image fusion;learning (artificial intelligence)","feature fusion-based conditional GAN;underwater image color correction;underwater scenarios;deep convolutional neural network;deep multiscale feature fusion net;conditional generative adversarial network;network learning;mature vision algorithms;DCNN","","38","","34","IEEE","7 Nov 2019","","","IEEE","IEEE Journals"
"Fusion of Multispectral LiDAR, Hyperspectral, and RGB Data for Urban Land Cover Classification","R. Hänsch; O. Hellwich","German Aerospace Center (DLR), Weßling, Germany; Computer Vision and Remote Sensing, Technical University of Berlin, Berlin, Germany","IEEE Geoscience and Remote Sensing Letters","20 Jan 2021","2021","18","2","366","370","With the increasing importance of monitoring urban areas, the question arises which sensors are best suited to solve the corresponding challenges. This letter proposes novel node tests within the random forest (RF) framework, which allows them to apply them to optical RGB images, hyperspectral images, and light detection and ranging (LiDAR) data, either individually or in combination. This does not only allow to derive accurate classification results for many relevant urban classes without preprocessing or feature extraction but also provides insights into which sensor offers the most meaningful data to solve the given classification task. The achieved results on a public benchmark data set are superior to results obtained by deep learning approaches despite being based on only a fraction of training samples.","1558-0571","","10.1109/LGRS.2020.2972955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007361","Classification;data fusion;HIS;multispectral light detection and ranging (LiDAR);random forest (RF)","Laser radar;Hyperspectral imaging;Radio frequency;Training;Optical sensors","data fusion;geophysical image processing;hyperspectral imaging;image classification;image colour analysis;image fusion;land cover;optical radar;random forests;terrain mapping;town and country planning","RF framework;optical RGB images;hyperspectral images;light detection and ranging data;relevant urban classes;classification task;public benchmark data;multispectral LiDAR;RGB data;urban land cover classification;urban area monitoring;node tests;random forest framework;data fusion;deep learning","","19","","5","IEEE","24 Feb 2020","","","IEEE","IEEE Journals"
"VO+Net: An Adaptive Approach Using Variational Optimization and Deep Learning for Panchromatic Sharpening","Z. -C. Wu; T. -Z. Huang; L. -J. Deng; J. -F. Hu; G. Vivone","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; National Research Council, Institute of Methodologies for Environmental Analysis, CNR-IMAA, Tito Scalo, Italy","IEEE Transactions on Geoscience and Remote Sensing","13 Dec 2021","2022","60","","1","16","Pansharpening refers to a spatio-spectral fusion of a lower spatial resolution multispectral (MS) image with a high spatial resolution panchromatic image, aiming at obtaining an image with a corresponding high resolution both in the domains. In this article, we propose a generic fusion framework that is able to weightedly combine variational optimization (VO) with deep learning (DL) for the task of pansharpening, where these crucial weights directly determining the relative contribution of DL to each pixel are estimated adaptively. This framework can benefit from both VO and DL approaches, e.g., the good modeling explanation and data generalization of a VO approach with the high accuracy of a DL technique thanks to massive data training. The proposed method can be divided into three parts: 1) for the VO modeling, a general details injection term inspired by the classical multiresolution analysis is proposed as a spatial fidelity term and a spectral fidelity employing the MS sensor’s modulation transfer functions is also incorporated; 2) for the DL injection, a weighted regularization term is designed to introduce deep learning into the variational model; and 3) the final convex optimization problem is efficiently solved by the designed alternating direction method of multipliers. Extensive experiments both at reduced and full-resolution demonstrate that the proposed method outperforms recent state-of-the-art pansharpening methods, especially showing a higher accuracy and a significant generalization ability.","1558-0644","","10.1109/TGRS.2021.3066425","NSFC(grant numbers:61772003,61702083,61876203); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216); National Key Research and Development Program of China(grant numbers:2020YFA0714001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387460","Adaptive fusion;deep learning;image fusion;multiresolution analysis;pansharpening;remote sensing;variational models","Pansharpening;Training;Spatial resolution;Multiresolution analysis;Image fusion;Deep learning;Optimization","","","","16","","72","IEEE","26 Mar 2021","","","IEEE","IEEE Journals"
"Mapping Forest and Their Spatial–Temporal Changes From 2007 to 2015 in Tropical Hainan Island by Integrating ALOS/ALOS-2 L-Band SAR and Landsat Optical Images","B. Chen; X. Xiao; H. Ye; J. Ma; R. Doughty; X. Li; B. Zhao; Z. Wu; R. Sun; J. Dong; Y. Qin; G. Xie","Danzhou Investigation and Experiment Station of Tropical Crops, Chinese Academy of Tropical Agricultural Sciences, Danzhou, China; Department of Microbiology and Plant Biology, and the Center for Spatial Analysis, University of Oklahoma, Norman, OK, USA; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Ministry of Education Key Laboratory for Biodiversity Science and Ecological Engineering, Fudan University, Shanghai, China; Department of Microbiology and Plant Biology, University of Oklahoma, Norman, OK, USA; Ministry of Education Key Laboratory for Biodiversity Science and Ecological Engineering, Fudan University, Shanghai, China; Ministry of Education Key Laboratory for Biodiversity Science and Ecological Engineering, Fudan University, Shanghai, China; Danzhou Investigation and Experiment Station of Tropical Crops, Chinese Academy of Tropical Agricultural Sciences, Danzhou, China; Danzhou Investigation and Experiment Station of Tropical Crops, Chinese Academy of Tropical Agricultural Sciences, Danzhou, China; Department of Microbiology and Plant Biology, University of Oklahoma, Norman, OK, USA; Department of Microbiology and Plant Biology, University of Oklahoma, Norman, OK, USA; Danzhou Investigation and Experiment Station of Tropical Crops, Chinese Academy of Tropical Agricultural Sciences, Danzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Mar 2018","2018","11","3","852","867","Accurately monitoring forest dynamics in the tropical regions is essential for ecological studies and forest management. In this study, images from phase-array L-band synthetic aperture radar (PALSAR), PALSAR-2, and Landsat in 2006-2010 and 2015 were combined to identify tropical forest dynamics on Hainan Island, China. Annual forest maps were first mapped from PALSAR and PALSAR-2 images using structural metrics. Those pixels with a high biomass of sugarcane or banana, which are widely distributed in the tropics and subtropics and have similar structural metrics as forests, were excluded from the SAR-based forest maps by using phenological metrics from time series Landsat imagery. The optical-SAR-based forest maps in 2010 and 2015 had high overall accuracies (OA) of 92-97% when validated with ground reference data. The resultant forest map in 2010 shows good spatial agreement with public optical-based forest maps (OA = 88-90%), and the annual forest maps (2007-2010) were spatiotemporally consistent and more accurate than the PALSAR-based forest map from the Japan Aerospace Exploration Agency (OA = 82% in 2010). The areas of forest gain, loss, and net change on Hainan Island from 2007 to 2015 were 415 000 ha (+2.17% yr-1), 179 000 ha (-0.94% yr -1), and 236 000 ha (+1.23% yr-1), respectively. About 95% of forest gain and loss occurred in those areas with an elevation less than 400 m, where deciduous rubber, eucalyptus plantations, and urbanization expanded rapidly. This study demonstrates the potential of PALSAR/PALSAR-2/Landsat image fusion for monitoring annual forest dynamics in the tropical regions.","2151-1535","","10.1109/JSTARS.2018.2795595","Hainan Provincial Department of Science and Technology(grant numbers:ZDKJ2016021); National Natural Science Foundation of China(grant numbers:41571408,41701510); Earmarked Fund for China Agriculture Research System(grant numbers:CARS-34-GW5); Fundamental Research Funds of Rubber Research Institute; Chinese Academy of Tropical Agricultural Sciences(grant numbers:1630022017016,1630022015012); U.S. NASA Land Use; Land Cover Change program(grant numbers:NNX14AD78G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8307088","Forest loss and gain;high biomass crops;image data fusion;land surface water index (LSWI);normalized difference vegetation index (NDVI)","Remote sensing;Earth;Artificial satellites;Synthetic aperture radar;Agriculture;Biomedical optical imaging;Optical imaging","geophysical image processing;image classification;image fusion;remote sensing by radar;synthetic aperture radar;time series;vegetation;vegetation mapping","annual forest maps;forest gain;PALSAR/PALSAR-2/Landsat image fusion;annual forest dynamics;tropical regions;mapping forest;spatial-temporal changes;tropical Hainan Island;integrating ALOS/ALOS-2 L-band SAR;Landsat optical images;accurately monitoring forest dynamics;forest management;phase-array L-band synthetic aperture radar;tropical forest dynamics;PALSAR-2 images;subtropics;similar structural metrics;time series Landsat imagery;optical-SAR-based forest maps;forest map","","32","","61","IEEE","6 Mar 2018","","","IEEE","IEEE Journals"
"$M^3\text{Fusion}$: A Deep Learning Architecture for Multiscale Multimodal Multitemporal Satellite Data Fusion","P. Benedetti; D. Ienco; R. Gaetano; K. Ose; R. G. Pensa; S. Dupuy","UMR-TETIS Laboratory, IRSTEA, University of Montpellier, Montpellier, France; LIRMM Laboratory, Montpellier, France; UMR TETIS, University of Montpellier, AgroParisTech, CIRAD, CNRS, IRSTEA, Montpellier, France; UMR-TETIS Laboratory, IRSTEA, University of Montpellier, Montpellier, France; Department of Computer Science, University of Turin, Turin, Italy; UMR TETIS, University of Montpellier, AgroParisTech, CIRAD, CNRS, IRSTEA, Montpellier, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4939","4949","Modern Earth Observation systems provide remote sensing data at different temporal and spatial resolutions. Among all the available spatial mission, today the Sentinel-2 program supplies high temporal (every five days) and high spatial resolution (HSR) (10 m) images that can be useful to monitor land cover dynamics. On the other hand, very HSR (VHSR) imagery is still essential to figure out land cover mapping characterized by fine spatial patterns. Understanding how to jointly leverage these complementary sources in an efficient way when dealing with land cover mapping is a current challenge in remote sensing. With the aim of providing land cover mapping through the fusion of multitemporal HSR and VHSR satellite images, we propose a suitable end-to-end deep learning framework, namely M3Fusion, which is able to simultaneously leverage the temporal knowledge contained in time series data as well as the fine spatial information available in VHSR images. Experiments carried out on the Reunion Island study area confirm the quality of our proposal considering both quantitative and qualitative aspects.","2151-1535","","10.1109/JSTARS.2018.2876357","Agence Nationale de la Recherche; Investments for the Future Program(grant numbers:ANR-16-CONV-0004 (DigitAg)); GEOSUD project(grant numbers:ANR-10-EQPX-20); Programme National de Télédétection Spatiale; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516352","Data fusion;deep learning;land cover mapping;satellite image time series;sentinel-2;very high spatial resolution (VHSR)","Time series analysis;Spatial resolution;Remote sensing;Feature extraction;Satellites","geophysical image processing;image classification;image fusion;learning (artificial intelligence);remote sensing;terrain mapping;time series","Sentinel-2 program supplies high temporal;high spatial resolution images;land cover dynamics;HSR imagery;land cover mapping;fine spatial patterns;suitable end-to-end deep learning framework;temporal knowledge;time series data;fine spatial information;VHSR images;multiscale multimodal multitemporal satellite data Fusion;Modern Earth Observation systems;remote sensing data;different temporal resolutions;spatial resolutions;available spatial mission","","47","","33","IEEE","31 Oct 2018","","","IEEE","IEEE Journals"
"Multiple Attention Siamese Network for High-Resolution Image Change Detection","J. Huang; Q. Shen; M. Wang; M. Yang","Key Laboratory of Urban Land Resources Monitoring and Simulation, Ministry of Natural Resources, Shenzhen, China; State Key Laboratory Cultivation Base of Geographical Environment Evolution (Jiangsu Province), Nanjing, China; Key Laboratory of Urban Land Resources Monitoring and Simulation, Ministry of Natural Resources, Shenzhen, China; State Key Laboratory Cultivation Base of Geographical Environment Evolution (Jiangsu Province), Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","17 Feb 2022","2022","60","","1","16","Change detection (CD) remains an important issue in remote sensing applications, especially for high-resolution images, but it has yet to be fully resolved. In this study, we propose a novel Siamese network model, i.e., the multiple attention Siamese network (MASNet), for high-resolution image change detection (HRCD). The selective kernel convolution (SKConv) is first embedded into the encoders of the Siamese network to improve its feature extraction ability. The attention feature fusion module (AFFM) is then proposed and utilized to reform the network’s decoder, which realizes the fusion and selection of image features extracted from different scales and times. Experiments on three benchmark CD datasets, namely, the season-varying change detection dataset (SCDDS), Wuhan University (WHU) building change detection dataset (WBDS), and Google dataset (GDS), are conducted to validate the proposed model. The results show that the proposed attention module (the AFFM) is superior to multiple classic attention modules, including the squeeze-and-excitation block (SE), spatial SE block (sSE), spatial and channel SE block (scSE), convolutional block attention module (CBAM), and efficient channel attention module (ECA), in terms of the fusing of CD features. Furthermore, MASNet achieves a higher overall accuracy (OA), mean intersection over union (mIoU), F1-score (F1), and kappa coefficient (kappa) than ten state-of-the-art HRCD methods, such as the deeply supervised image fusion network (DSIFN), spatial–temporal attention neural network (STANet), and dual attentive fully convolutional Siamese network (DASNet) while maintaining low missed alarms and false alarms and having a fair method efficiency.","1558-0644","","10.1109/TGRS.2021.3127580","National Natural Science Foundation of China(grant numbers:42071301,41671341); Major Projects of High Resolution Earth Observation Systems of National Science and Technology(grant numbers:05-Y30B01-9001-19/20-2); Open Fund of Key Laboratory of Urban Land Resources Monitoring and Simulation, Ministry of Natural Resources(grant numbers:KF-2019-04-008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612208","Attention mechanism;change detection (CD);fully convolutional network (FCN);high-resolution imagery;semantic segmentation;Siamese network","Convolution;Feature extraction;Kernel;Decoding;Correlation;Neural networks;Image resolution","feature extraction;geophysical image processing;image fusion;image resolution;image segmentation;neural nets;remote sensing;supervised learning","multiple attention Siamese network;high-resolution image change detection;selective kernel convolution;feature extraction ability;attention feature fusion module;image features;benchmark CD datasets;season-varying change detection dataset;Wuhan University building change detection dataset;convolutional block attention module;channel attention module;spatial-temporal attention neural network;MASNet;HRCD;Google dataset;mean intersection over union;deeply supervised image fusion network","","8","","60","IEEE","11 Nov 2021","","","IEEE","IEEE Journals"
"End-to-End DSM Fusion Networks for Semantic Segmentation in High-Resolution Aerial Images","Z. Cao; K. Fu; X. Lu; W. Diao; H. Sun; M. Yan; H. Yu; X. Sun","Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Suzhou, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Network Information System Technology (NIST), Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","30 Oct 2019","2019","16","11","1766","1770","Semantic segmentation in high-resolution aerial images is a fundamental research problem in remote sensing field for its wide range of applications. However, it is difficult to distinguish regions with similar spectral features using only multispectral data. Recent research studies have indicated that the introduction of multisource information can effectively improve the robustness of segmentation method. In this letter, we use digital surface models (DSMs) information as a complementary feature to further improve the semantic segmentation results. To this end, we propose a lightweight and simple DSM fusion (DSMF) branch structure module. Compared with the existing feature extraction structures, proposed DSMF module is simple and can be easily applied to other networks. In addition, we investigate four fusion strategies based on DSMF module to explore the optimal feature fusion strategy and four end-to-end DSMFNets are designed according to the corresponding strategies. We evaluate our models on International Society for Photogrammetry and Remote Sensing Vaihingen data set and all DSMFNets achieve promising results. In particular, DSMFNet-1 achieves an overall accuracy of 91.5% on the test data set.","1558-0571","","10.1109/LGRS.2019.2907009","National Natural Science Foundation of China(grant numbers:41701508,41801349); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8688433","Convolutional neural networks (CNNs);deep learning;high-resolution aerial images;semantic segmentation","Feature extraction;Image segmentation;Semantics;Training;Convolution;Decoding;Fuses","feature extraction;geophysical image processing;image fusion;image segmentation;photogrammetry;remote sensing","fusion networks;high-resolution aerial images;fundamental research problem;remote sensing field;spectral features;multispectral data;multisource information;segmentation method;digital surface models information;complementary feature;semantic segmentation results;feature fusion strategy;end-to-end DSMFNets;Remote Sensing Vaihingen data;end-to-end DSM fusion networks;feature extraction structures;photogrammetry","","40","","13","IEEE","11 Apr 2019","","","IEEE","IEEE Journals"
"Constrained-Target Band Selection for Multiple-Target Detection","Y. Wang; L. Wang; C. Yu; E. Zhao; M. Song; C. -H. Wen; C. -I. Chang","Information and Technology College, Dalian Maritime University, Dalian, China; School of Physics and Optoelectronic Engineering, Xidian University, Xi’an, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; Center of Hyperspectral Imaging in Remote Sensing, Information and Technology College, Dalian Maritime University, Dalian, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Computer Science and Information Management, Providence University, Taichung, Taiwan; Remote Sensing Signal and Image Processing Laboratory, University of Maryland, Baltimore, MD, USA","IEEE Transactions on Geoscience and Remote Sensing","22 Jul 2019","2019","57","8","6079","6103","This paper develops a new approach to band selection for multiple-target detection, called constrained-target band selection (CTBS). Its idea is derived from the concept of constrained energy minimization (CEM) by constraining a target of interest, while minimizing the variance resulting from the background (BKG). By taking advantage of CEM, the variance produced by a target of interest can be further used as a measure of prioritizing bands as well as a means of selecting bands for this particular target. As a result, two CTBS-based band prioritization (BP) criteria, called minimal variance-based BP (MinV-BP) and maximal variance-based BP (MaxV-BP), and two CTBS-based BS methods, called sequential forward CTBS (SF-CTBS) and sequential backward CTBS (SB-CTBS), can be derived for multiple-target detection. Since the bands selected by CTBS vary with targets of interest used to constrain CEM, in order for CTBS to be applied to multiple targets, a new fusion technique, called band fusion selection (BFS), is further developed for CTBS to integrate bands selected by different targets so that CTBS can work for all targets. Unlike most BS methods for target detection which generally simultaneously select a fixed set of bands for all targets of interest, the ideas of constraining multiple-target detection and using BFS are novelty of this paper. Experimental results show that CTBS performs well for multiple-target detection.","1558-0644","","10.1109/TGRS.2019.2904264","National Natural Science Foundation of China(grant numbers:61801075); Fundamental Research Funds for the Central Universities(grant numbers:3132016331); Fundamental Research Funds for Central Universities(grant numbers:JB150508); Natural Science Foundation of Liaoning Province(grant numbers:20170540095); National Nature Science Foundation of China(grant numbers:41801231); National Nature Science Foundation of China(grant numbers:61601077); Fundamental Research Funds for the Central Universities(grant numbers:3132017124); Fundamental Research Funds for Central Universities(grant numbers:3132016331,ZD20180073); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8686257","Band fusion selection (BFS);band prioritization (BP);band selection (BS);constrained energy minimization (CEM);constrained-target band selection (CTBS);virtual dimensionality (VD)","Hyperspectral imaging;Correlation;Minimization;Computer science;Object detection","image fusion;minimisation;object detection","band fusion selection;sequential forward CTBS;minimal variance-based BP;multiple targets;CTBS-based BS methods;maximal variance-based BP;CTBS-based band prioritization criteria;particular target;prioritizing bands;constrained energy minimization;called constrained-target band selection;multiple-target detection","","29","","45","IEEE","11 Apr 2019","","","IEEE","IEEE Journals"
"Spatially Regularized Fusion of Multiresolution Digital Surface Models","G. Kuschk; P. d’Angelo; D. Gaudrie; P. Reinartz; D. Cremers","Technical University of Munich, Munich, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; Technical University of Munich, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","1 Mar 2017","2017","55","3","1477","1488","In this paper, we propose an algorithm for robustly fusing digital surface models (DSMs) with different ground sampling distances and confidences, using explicit surface priors to obtain locally smooth surface models. Robust fusion of the DSMs is achieved by minimizing the L1-distance of each pixel of the solution to each input DSM. This approach is similar to a pixel-wise median, and most outliers are discarded. We further incorporate local planarity assumption as an additional constraint to the optimization problem, thus reducing the noise compared with pixel-wise approaches. The optimization is also inherently able to include weights for the input data, therefore allowing to easily integrate invalid areas, fuse multiresolution DSMs, and to weight the input data. The complete optimization problem is constructed as a variational optimization problem with a convex energy functional, such that the solution is guaranteed to converge toward the global energy minimum. An efficient solver is presented to solve the optimization in reasonable time, e.g., running in real time on standard computer vision camera images. The accuracy of the algorithms and the quality of the resulting fused surface models are evaluated using synthetic data sets and spaceborne data sets from different optical satellite sensors.","1558-0644","","10.1109/TGRS.2016.2625040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7752839","3-D reconstruction;data fusion;digital surface model (DSM);variational methods","Signal to noise ratio;Optimization;Optical sensors;Image resolution;Robustness;Surface reconstruction","geophysical image processing;image fusion;image resolution;optimisation;terrain mapping;variational techniques","digital surface models;ground sampling distances;locally smooth surface models;pixel-wise median;local planarity assumption;optimization problem;pixel-wise approaches;global energy;standard computer vision camera images;synthetic data sets;optical satellite sensors","","8","","19","IEEE","22 Nov 2016","","","IEEE","IEEE Journals"
"Fast Hyperspectral Image Dehazing With Dark-Object Subtraction Model","Z. Li; P. Duan; S. Hu; M. Li; X. Kang","Institute of Remote Sensing Satellite, China Academy of Space Technology (CAST), Beijing, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Engineering, University of Sydney, Sydney, NSW, Australia; School of Robotics, Hunan University, Changsha, China","IEEE Geoscience and Remote Sensing Letters","9 Nov 2022","2022","19","","1","5","Haze in hyperspectral images (HSIs) is a common phenomenon, which severely degrades the quality of the acquired data and limits its applications. In this letter, a dark-object subtraction model is proposed for haze removal of HSIs, which mainly is composed of three steps. First, a haze density map is estimated according to the haze characteristic in different spectral channels. Then, we design a saliency measure method to automatically calculate the haze abundance of different channels. Finally, the haze-free image is obtained by solving the dark-object subtraction model. Experiments on real and simulated datasets demonstrate that our method consistently outperforms other state-of-the-art dehazing techniques in terms of reconstructed performance and computational cost.","1558-0571","","10.1109/LGRS.2022.3217766","National Key Research and Development Program of China(grant numbers:2021YFA0715203); Major Program of the National Natural Science Foundation of China(grant numbers:61890962); National Natural Science Foundation of China(grant numbers:62201207,61871179); Scientific Research Project of Hunan Education Department(grant numbers:19B105); National Science Foundation of Hunan Province(grant numbers:2019JJ50036,2020GK2038); Hunan Provincial Natural Science Foundation for Distinguished Young Scholars(grant numbers:2021JJ022); Huxiang Young Talents Science and Technology Innovation Program(grant numbers:2020RC3013); Changsha Natural Science Foundation(grant numbers:kq2202171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931669","Dark-object subtraction model;hyperspectral image (HSI);image dehazing","Hyperspectral imaging;Atmospheric modeling;Reflectivity;Image reconstruction;Computational modeling;Channel estimation;Indexes","geophysical image processing;image colour analysis;image enhancement;image filtering;image fusion;image restoration","HSIs;dark-object subtraction model;haze removal;haze density map;haze characteristic;haze abundance;haze-free image;fast hyperspectral image dehazing;hyperspectral images","","2","","25","IEEE","28 Oct 2022","","","IEEE","IEEE Journals"
"Variational Pansharpening by Exploiting Cartoon-Texture Similarities","X. Tian; Y. Chen; C. Yang; J. Ma","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; College of Computer and Information Science, Fujian Agriculture and Forestry University, Fuzhou, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","16","Pansharpening aims to fuse a multispectral (MS) image with low spatial resolution and a panchromatic (PAN) image with a high-spatial resolution to produce an image with both high spectral and high spatial resolution. In this study, we propose a variational pansharpening method by exploiting cartoon-texture similarities. After decomposition of the PAN image, the cartoon component always contains the global structure information, while the texture component includes the locally patterned information. This enables that the fused high-spatial resolution MS image can preserve the global and local spatial details (e.g., high-order information) well after leveraging the similarities of cartoon and texture components from PAN and MS images. To explore such cartoon-texture similarities, we describe cartoon similarity as gradient sparsity, formulated as a reweighted total variation term. Meanwhile, we use group low-rank constraint for texture similarity that is presented as repetitive texture patterns. By incorporating a data fidelity term for preserving the spectral information on the basis that the down-sampled fused MS image is consistent with the MS image, we further formulate pansharpening as an optimization problem and solve it efficiently using the alternative direction multiplier method. Extensive experiments have been conducted on a series of satellite data sets, and we also carry out a simulated vegetation coverage change experiment to verify the efficiency of the proposed method in remote sensing. The qualitative and quantitative results demonstrate that our method outperforms the state-of-the-art pansharpening methods in terms of both visual effect and objective metrics.","1558-0644","","10.1109/TGRS.2020.3048257","National Natural Science Foundation of China(grant numbers:61971315,61773295); Natural Science Foundation of Hubei Province(grant numbers:2018CFB435); Fundamental Research Funds for the Central Universities(grant numbers:2042018kf1009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325956","Alternating direction method of multipliers;cartoon-texture decomposition;low-rank;pansharpening;total variation (TV)","Pansharpening;Spatial resolution;Remote sensing;Optimization;Vegetation mapping;TV;Satellites","geophysical image processing;geophysical techniques;image denoising;image fusion;image reconstruction;image resolution;image sampling;image texture;iterative methods;remote sensing;vegetation","cartoon-texture similarities;pansharpening aims;multispectral image;low spatial resolution;panchromatic image;high spectral;high spatial resolution;variational pansharpening method;PAN image;cartoon component;global structure information;texture component;locally patterned information;high-spatial resolution MS image;global details;local spatial details;texture components;cartoon similarity;reweighted total variation term;texture similarity;repetitive texture patterns;down-sampled fused MS image;state-of-the-art pansharpening methods","","20","","57","IEEE","15 Jan 2021","","","IEEE","IEEE Journals"
"Investigating the Influence of Registration Errors on the Patch-Based Spatio-Temporal Fusion Method","L. Wang; X. Wang; Q. Wang; P. M. Atkinson","College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; Geography and Environment, University of Southampton, Southampton, U.K.","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Oct 2020","2020","13","","6291","6307","Spatio-temporal fusion is a common approach in remote sensing, used to create time-series image data with both fine spatial and temporal resolutions. However, geometric registration error, which is a common problem in remote sensing relative to the ground reference, is a particular problem for multiresolution remote sensing data, especially for images with very different spatial resolutions (e.g., Landsat and MODIS images). Registration error can, thus, have a significant impact on the accuracy of spatio-temporal fusion. To the best of our knowledge, however, almost no effective solutions have been provided to-date to cope with this important issue. This article demonstrates the robustness to registration error of the existing SParse representation-based spatio-temporal reflectance fusion model (SPSTFM). Different to conventional methods that are performed on a per-pixel basis, SPSTFM utilizes image patches as the basic unit. We demonstrate theoretically that the effect of registration error on patch-based methods is smaller than for pixel-based methods. Experimental results show that SPSTFM is highly robust to registration error and is far more accurate under various registration errors relative to pixel-based methods. The advantage is shown to be greater for heterogeneous regions than for homogeneous regions, and is large for the fusion of normalized difference vegetation index data. SPSTFM, thus, offers the remote sensing community a crucial tool to overcome one of the longest standing challenges to the effective fusion of remote sensing image time-series.","2151-1535","","10.1109/JSTARS.2020.3030122","National Natural Science Foundation of China(grant numbers:41971297,61675051); Fundamental Research Funds for the Central Universities(grant numbers:02502150021); Tongji University(grant numbers:02502350047); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9220778","Downscaling;Landsat;moderate resolution imaging spectroradiometer (MODIS);registration error;sparse representation based spatio-temporal reflectance fusion model (SPSTFM);spatio-temporal fusion","Spatial resolution;Remote sensing;Earth;Artificial satellites;MODIS;Dictionaries;Reflectivity","geophysical image processing;image fusion;image registration;spatiotemporal phenomena;time series;vegetation;vegetation mapping","patch-based spatiotemporal fusion;time-series image data;geometric registration error;multiresolution remote sensing data;sparse representation-based spatiotemporal reflectance fusion model;image patches;pixel-based methods;remote sensing image time-series;MODIS images;Landsat images;normalized difference vegetation index data","","3","","57","CCBY","12 Oct 2020","","","IEEE","IEEE Journals"
"Single Object Tracking in Satellite Videos: A Correlation Filter-Based Dual-Flow Tracker","Y. Chen; Y. Tang; Z. Yin; T. Han; B. Zou; H. Feng","School of Geosciences and Info-Physics, Central South University, Changsha, China; Key Laboratory of Metallogenic Prediction of Nonferrous Metals and Geological Environment Monitoring, Central South University, Ministry of Education, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; Key Laboratory of Metallogenic Prediction of Nonferrous Metals and Geological Environment Monitoring, Central South University, Ministry of Education, Changsha, China; Key Laboratory of Metallogenic Prediction of Nonferrous Metals and Geological Environment Monitoring, Central South University, Ministry of Education, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14 Sep 2022","2022","15","","6687","6698","Satellite video (SV) can acquire rich spatiotemporal information on the earth. Single object tracking (SOT) in SVs enables the continuous acquisition of the position and range of a specific object, expanding the field of remote-sensing applications. In SVs, objects are small with limited features and vulnerable to tracking drift. In this article, a correlation filter based dual-flow (DF) tracker is proposed to explore how the hybridization of spatial–spectral feature fusion and motion model can boost tracking. To represent small objects, the DF adaptively fuses complementary features using a state-aware indicator in feature flow. In motion flow, the indicator perceives the confidence of the feature flow. A dual-mode prediction model is then constructed to simulate the object's motion pattern and cooperate linear and nonlinear motion patterns to implement SOT in SVs. The ablation experiments demonstrate that the DF contributes to tracking. Experimental comparisons on 14 real SVs captured by the Jilin-1 satellite constellation show that DF achieves optimal performance with an area under the curve of 0.912 in the precision plot, 0.700 in the success plot, and a speed of 155.2 frames per second. This work would encourage the development of remote-sensing ground surveillance.","2151-1535","","10.1109/JSTARS.2022.3185328","National Natural Science Foundation of China(grant numbers:41971313); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803284","Correlation filter (CF);motion model;satellite video (SV);state-aware indicator (SAI);single object tracking (SOT)","Correlation;Videos;Remote sensing;Satellites;Object tracking;Adaptation models;Optical filters","feature extraction;filtering theory;image fusion;image motion analysis;object detection;object tracking;remote sensing;spatiotemporal phenomena;target tracking","tracking drift;spatial-spectral feature fusion;motion model;complementary features;state-aware indicator;feature flow;motion flow;dual-mode prediction model;nonlinear motion patterns;SVs;Jilin-1 satellite constellation show;single object tracking;satellite video;correlation filter-based dual-flow tracker;rich spatiotemporal information;specific object;remote-sensing applications","","2","","58","CCBY","22 Jun 2022","","","IEEE","IEEE Journals"
"MSLAENet: Multiscale Learning and Attention Enhancement Network for Fusion Classification of Hyperspectral and LiDAR Data","Y. Fan; Y. Qian; Y. Qin; Y. Wan; W. Gong; Z. Chu; H. Liu","Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China; Key Laboratory of Software Engineering, Urumqi, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","30 Nov 2022","2022","15","","10041","10054","The effective use of multimodal data to obtain accurate land cover information has become an interesting and challenging research topic in the field of remote sensing. In this article, we propose a new method, multiscale learning and attention enhancement network (MSLAENet), to implement hyperspectral image (HSI) and light detection and ranging (LiDAR) data fusion classification in an end-to-end manner. Specifically, our model consists of three main modules. First, we design the composite attention module, which adopts self-attention to enhance the feature representations of HSI and LiDAR data, respectively, and cross-attention to achieve cross-modal information enhancement. Second, the proposed multiscale learning module combines self-calibrated convolutions and hierarchical residual structure to extract different scales of information to further improve the representation capability of the model. Finally, the attention-based feature fusion module fully considers the complementary information properties between different modalities and adaptively fuses heterogeneous features from different modalities. To test the performance of MSLAENet, we conduct experiments on three multimodal remote sensing datasets and compare them with the state-of-the-art fusion model, which demonstrates the effectiveness and superiority of the model.","2151-1535","","10.1109/JSTARS.2022.3221098","National Natural Science Foundation of China(grant numbers:61966035); National Natural Science Foundation of China(grant numbers:U1803261); Xinjiang Uygur Autonomous Region Innovation Team(grant numbers:XJEDU2017T002); Autonomous Region Science and Technology Department International Cooperation Project(grant numbers:2020E01023); Tianshan Innovation Team Plan Project of Xinjiang Uygur Autonomous Region(grant numbers:202101642); Xinjiang University of Finance and Economics(grant numbers:2017XYB015); Sichuan Regional Innovation Cooperation Project(grant numbers:2020YFQ0018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944856","Attention mechanism;fusion classification;hyperspectral image (HSI) and light detection and ranging (LiDAR) data;multiscale feature;self-calibrated convolutions","Feature extraction;Remote sensing;Laser radar;Data mining;Deep learning;Data models;Task analysis","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence);object detection;optical radar;remote sensing;sensor fusion","accurate land cover information;attention enhancement network;attention-based;complementary information properties;composite attention module;cross-attention;cross-modal information enhancement;feature representations;fusion module;HSI;hyperspectral image;interesting research topic;LiDAR data;light detection;MSLAENet;multimodal data;multimodal remote sensing datasets;multiscale learning module;state-of-the-art fusion model","","","","46","CCBY","10 Nov 2022","","","IEEE","IEEE Journals"
"A Multivariate Empirical Mode DecompositionBased Approach to Pansharpening","S. M. U. Abdullah; N. u. Rehman; M. M. Khan; D. P. Mandic","Halliburton Worldwide Limited, Islamabad, Pakistan; Department of Electrical Engineering, COMSATS Institute of Information Technology, Islamabad, Pakistan; School of Electrical Engineering and Computer Science, National University of Sciences and Technology, Islamabad, Pakistan; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Geoscience and Remote Sensing","9 Mar 2015","2015","53","7","3974","3984","We propose a novel class of schemes for the pansharpening of multispectral (MS) images using a multivariate empirical mode decomposition (MEMD) algorithm. MEMD is an extension of the empirical mode decomposition (EMD) algorithm, which enables the decomposition of multivariate data into its intrinsic oscillatory scales. The ability of MEMD to process multichannel data directly by performing data-driven, local, and multiscale analysis makes it a perfect match for pansharpening applications, a task for which standard univariate EMD is ill-equipped due to the nonuniqueness, mode-mixing, and mode-misalignment issues. We show that MEMD overcomes the limitations of standard EMD and yields improved spatial and spectral performance in the context of pansharpening of MS images. The potential of the proposed schemes is further demonstrated through comparative analysis against a number of standard pansharpening algorithms on both simulated Pleiades and real-world IKONOS data sets.","1558-0644","","10.1109/TGRS.2015.2388497","Higher Education Commission, Pakistan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7031394","Image fusion;multi-resolution analysis;multivariate empirical mode decomposition;pansharpening;Image fusion;multi-resolution analysis;multivariate empirical mode decomposition;pansharpening","Vectors;Standards;Empirical mode decomposition;Spatial resolution;Context;Educational institutions","geophysical image processing;image fusion;image resolution;land cover;terrain mapping","multispectral image pansharpening;multivariate empirical mode decomposition algorithm;multivariate data decomposition;intrinsic oscillatory scales;multichannel data;data-driven local multiscale analysis;pansharpening applications;standard univariate empirical mode decomposition;mode-mixing issue;nonuniqueness issue;modemisalignment issue;spatial performance;spectral performance;comparative analysis;standard pansharpening algorithms;simulated Pleiades data sets;real-world IKONOS data sets;land cover types","","32","","34","IEEE","4 Feb 2015","","","IEEE","IEEE Journals"
"Pansharpening via Triplet Attention Network With Information Interaction","W. Diao; F. Zhang; H. Wang; J. Sun; K. Zhang","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 May 2022","2022","15","","3576","3588","Pansharpening aims to obtain high spatial resolution multispectral (MS) images by fusing the spatial and spectral information in low spatial resolution (LR) MS and panchromatic (PAN) images. Recently, deep neural network (DNN) based pansharpening methods have been advanced extensively. Although most DNN-based methods show good performance, it is difficult for them to preserve the spatial details in the fused image. In this article, we propose a new pansharpening method based on a triplet attention network with information interaction to efficiently enhance the spatial and spectral information in the fused image. First, different attention mechanisms are designed to model the spatial and spectral feature properties in LR MS and PAN images. Then, the complementarity among different feature maps is enhanced by information interaction, which promotes the compatibility of features from subnetworks. Finally, we utilize a graph attention module to capture the similarity within feature maps. According to the graph, the informative feature maps are selected to provide more details for the reconstruction of the fused image. Extensive experiments on QuickBird and GeoEye-1 satellite datasets show that the proposed method can produce competitive fused images when compared with some state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3171423","National Natural Science Foundation of China(grant numbers:61901246); China Postdoctoral Science Foundation(grant numbers:2019TQ0190,2019M662432); Scientific Research Leader Studio of Jinan(grant numbers:2021GXRC081); Joint Project for Smart Computing of Shandong Natural Science Foundation(grant numbers:ZR2020LZH015); Ningxia Natural Science Foundation(grant numbers:2021AAC03045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765738","Image fusion;information interaction;pansharpening;remote sensing;triplet attention network","Feature extraction;Spatial resolution;Pansharpening;Degradation;Data mining;High frequency;Correlation","geophysical image processing;image fusion;image resolution;neural nets;remote sensing","triplet attention network;information interaction;high spatial resolution multispectral images;spatial information;spectral information;low spatial resolution MS;panchromatic images;deep neural network;DNN-based methods;spatial details;fused image;pansharpening method;different attention mechanisms;spatial feature properties;spectral feature properties;different feature maps;graph attention module;informative feature maps;competitive fused images","","2","","58","CCBY","29 Apr 2022","","","IEEE","IEEE Journals"
"Research on the Vertical Stratification Characteristics and Dielectric Constant Calculation of Geological Bodies Based on Structural Plane Information of Borehole GPR Image and Digital Panoramic Image Fusion","L. Li; R. Wang; Z. Han","School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; State Key Laboratory of Geomechanics, Geotechnical Engineering Institute of Rock and Soil Mechanics, Chinese Academy of Sciences, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","23 Dec 2021","2022","19","","1","5","For Ground-penetrating radar (GPR) image and the digital panoramic image obtained from single-hole measuring, a novel and noncooperative measuring method for quantitatively calculating the relative dielectric constant of geo-bodies based on information fusion of the two types of image is proposed. Structural plane tilt angle in borehole optical image and GPR image is extracted to calculate the permittivity and quantify the geological medium properties. The vertical stratification characteristics of geo-bodies can be obtained through the analysis of changing dielectric constant. Experiment with borehole optical and GPR images in this letter reveals the continuity and discontinuity of medium change. It confirms that there are many kinds of layered structures in shallow geology, such as soil, clay, sand with different water content, crushed stone, shale, and solution cracks with varying degrees of development. On the contrary, the values of permittivity of materials in deep geology have slight variation, and it indicates that the deep geological vertical stratification characteristic is relatively simple. The method provides a new and efficient means of dielectric constant calculation and vertical stratification characteristics analysis, especially for complex shallow geology in engineering applications.","1558-0571","","10.1109/LGRS.2020.3048037","Open Research Fund of the State Key Laboratory of GeoMechanics and Geo-Technical Engineering, Institute of Rock and Soil Mechanics, Chinese Academy of Sciences(grant numbers:Z016006); National Natural Science Foundation of China(grant numbers:51879195,51679257); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321153","Dielectric constant calculation;ground-penetrating radar (GPR) and digital panoramic images fusion;noncooperative target;structural plane information;vertical stratification characteristics","Geology;Dielectric constant;Radar imaging;Radar;Optical imaging;Soil;Radar antennas","","","","","","14","IEEE","12 Jan 2021","","","IEEE","IEEE Journals"
"Image Reconstruction Based on Fused Features and Perceptual Loss Encoder-Decoder Residual Network for Space Optical Remote Sensing Images Compressive Sensing","S. Xiao; S. Wang; L. Chang","Key Laboratory of Space-Based Dynamic and Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, China; Key Laboratory of Space-Based Dynamic and Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, China; Key Laboratory of Space-Based Dynamic and Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, China","IEEE Access","5 Apr 2021","2021","9","","50413","50425","Compressive sensing (CS) technology is introduced into space optical remote sensing image acquisition stage, which could make wireless image sensor network node quickly and accurately obtain images in the case of two constraints of limited battery power and expensive sensor costs. On this basis, in order to further improve the quality of CS image reconstruction, we propose fused features and perceptual loss encoder-decoder residual network (FFPL-EDRNet) for image reconstruction. FFPL-EDRNet consists of a convolution layer and a reconstruction network. We train FFPL-EDRNet end-to-end, thus greatly simplifying the pre-processing and post-processing process and eliminating the block effect of reconstructed images. The reconstruction network is based on residual network, which introduces multi-scale feature extraction, multi-scale feature combination and multi-level feature combination. Feature fusion integrates low-level information with high-level information to reduce reconstruction error. The perceptual loss function based on pretrained InceptionV3 uses the weighted mean square error to define the loss value between the reconstructed image feature and the label image feature, which makes the reconstructed image more semantically similar to label image. In the measurement procedure, we use convolution to achieve block compression measurement, so as to obtain full image measurements. For image reconstruction, we firstly use a deconvolution layer to initially reconstruct the image and then use the residual network to refine the initial reconstructed image. The experimental results show that: in the case of measurement rates (MRs) of 0.25, 0.10, 0.04 and 0.01, the peak signal-to-noise ratio (PSNR) = 27.502, 26.804, 24.593, 21.359 and structural similarity (SSIM) = 0.842, 0.816, 0.720, 0.568 of the reconstructed images obtained by FFPL-EDRNet. Therefore, Our FFPL-EDRNet could enhance the quality of image reconstruction.","2169-3536","","10.1109/ACCESS.2021.3069086","National Natural Science Foundation of China(grant numbers:61805244); Key Science and Technology Research and Development Projects of Jilin Province(grant numbers:20190303094SF); Qian Xuesen Laboratory of Space Technology, China Academy of Space Technology (CAST)(grant numbers:GZZKFJJ2020003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387329","Image reconstruction;compressive sensing;encoder-decoder network;fused features;perceptual loss;residual block","Image reconstruction;Feature extraction;Convolution;Residual neural networks;Image coding;Deconvolution;Kernel","compressed sensing;convolutional neural nets;data compression;feature extraction;geophysical image processing;image fusion;image reconstruction;image sensors;mean square error methods;optical information processing;remote sensing;wireless sensor networks","peak signal-to-noise ratio;structural similarity;SSIM;SSIM;block compression measurement;pretrained InceptionV3;weighted mean square error;deconvolution layer;measurement procedure;high-level information;low-level information;feature fusion;multilevel feature combination;convolution layer;CS image reconstruction quality;expensive sensor costs;space optical remote sensing image compressive sensing;CS technology;FFPL-EDRNet;reconstructed image feature;reconstruction error reduction;multiscale feature extraction;reconstruction network;wireless image sensor network node;space optical remote sensing image acquisition stage;compressive sensing technology;perceptual loss encoder-decoder residual network;fused features;initial reconstructed image;image measurements;label image feature","","2","","35","CCBY","26 Mar 2021","","","IEEE","IEEE Journals"
"A Data-Driven Model-Based Regression Applied to Panchromatic Sharpening","P. Addesso; G. Vivone; R. Restaino; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; LJK, CNRS, Inria, Grenoble INP, Université Grenoble Alpes, Grenoble, France","IEEE Transactions on Image Processing","17 Jul 2020","2020","29","","7779","7794","Image fusion is growing interest in recent years, thanks to the huge amount of data acquired everyday by sensors on board of satellite platforms. The enhancement of the spatial resolution of a multispectral (MS) image through the use of a panchromatic (PAN) image, usually called pansharpening, is getting more and more relevant. In this work, we focus on the problem of the estimation of the injection coefficients that rule the enhancement of the spatial resolution of the MS image by properly adding the PAN details. In particular, a statistical analysis of the residuals coming from the linear multivariate regression between details extracted from the PAN image and the MS image is performed. A novel hybrid model is introduced for accurately describing the statistical distribution of these residuals, together with a procedure for efficiently estimating both the parameters of the residual distribution and the injection coefficients. The improvements achieved by the proposed approach are assessed using two very high resolution datasets acquired by the WorldView-3 and Worldview-4 satellites. The benefits of the proposed approach are particularly clear when vegetated areas are involved in the fusion process.","1941-0042","","10.1109/TIP.2020.3007824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139363","Multivariate linear regression;injection models;pansharpening;image fusion;remote sensing","Spatial resolution;Mathematical model;Linear regression;Estimation;Multiresolution analysis","geophysical image processing;geophysical signal processing;image fusion;image resolution;regression analysis;remote sensing;statistical analysis;vegetation","data-driven model-based regression applied;panchromatic sharpening;image fusion;satellite platforms;spatial resolution;multispectral image;panchromatic image;injection coefficients;MS image;PAN details;statistical analysis;linear multivariate regression;PAN image;hybrid model;statistical distribution;residual distribution;high resolution datasets;Worldview-4 satellites","","10","","66","IEEE","13 Jul 2020","","","IEEE","IEEE Journals"
"Self-Supervised SAR-Optical Data Fusion of Sentinel-1/-2 Images","Y. Chen; L. Bruzzone","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Geoscience and Remote Sensing","17 Feb 2022","2022","60","","1","11","The effective combination of the complementary information provided by huge amount of unlabeled multisensor data (e.g., synthetic aperture radar (SAR) and optical images) is a critical issue in remote sensing. Recently, contrastive learning methods have reached remarkable success in obtaining meaningful feature representations from multiview data. However, these methods only focus on image-level features, which may not satisfy the requirement for dense prediction tasks such as land-cover mapping. In this work, we propose a self-supervised framework for SAR-optical data fusion and land-cover mapping tasks. SAR and optical images are fused by using a multiview contrastive loss at image level and super-pixel level according to one of those possible strategies: in the early, intermediate, and late strategies. For the land-cover mapping task, we assign each pixel a land-cover class by the joint use of pretrained features and spectral information of the image itself. Experimental results show that the proposed approach not only achieves a comparable accuracy but also reduces the dimension of features with respect to the image-level contrastive learning method. Among three fusion strategies, the intermediate fusion strategy achieves the best performance. The combination of the pixel-level fusion approach and the self-training on spectral indices leads to further improvements in the land-cover mapping task with respect to the image-level fusion approach, especially with sparse pseudo labels. The code to reproduce our results will be found at https://github.com/yusin2it/SARoptical_fusion.","1558-0644","","10.1109/TGRS.2021.3128072","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9614157","Data fusion;land-cover mapping;pixel level;remote sensing;self-supervised learning;Sentinel-1/-2","Task analysis;Synthetic aperture radar;Optical imaging;Optical sensors;Deep learning;Training;Fuses","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);radar imaging;remote sensing;synthetic aperture radar","multiview contrastive loss;image level;super-pixel level;early strategies;late strategies;land-cover mapping task;land-cover class;pretrained features;image-level contrastive learning method;fusion strategies;intermediate fusion strategy;pixel-level fusion approach;image-level fusion approach;self-supervised SAR-optical data fusion;unlabeled multisensor data;synthetic aperture radar;meaningful feature representations;multiview data;image-level features;dense prediction tasks;optical images","","7","","46","IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"Integrating Coupled Dictionary Learning and Distance Preserved Probability Distribution Adaptation for Multispectral–Hyperspectral Image Collaborative Classification","B. Guo; T. Liu; Y. Gu","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","16 May 2022","2022","60","","1","13","With the development of observation technology in remote sensing (RS), large-area multispectral (MS) images can be easily obtained. However, due to the limitation of imaging devices, only a limited range of hyperspectral (HS) images with higher spectral resolution can be obtained. This article mainly focuses on how to use limited HS images to improve the classification performance of MS images. In order to solve this problem, this article proposes an MS–HS image collaborative classification method, which integrates coupled dictionary learning and distance preserved probability distribution. First, image reconstruction based on coupled dictionary learning is performed, in which sparse representation and dictionary learning are used to generate HS images from MS images through spectral superresolution, so that the spectral features of the MS data and HS data are converted to the same feature space for feature space alignment. Second, the probability distribution is adapted, in which the marginal and conditional probabilities are adapted to further narrow the difference between the real HS data and the generated HS data. At the same time, the consistency of the data structure of the source domain before and after the mapping is maintained, so that the same class of data is more compact after the mapping and reduces the spacing within the same class. Compared with the state-of-the-art methods, this article conducts the experiments on three MS–HS RS datasets, which demonstrate the superiority of the proposed method.","1558-0644","","10.1109/TGRS.2022.3169216","National Natural Science Foundation of China; Key International Cooperation Project(grant numbers:61720106002); Youth Science Foundation Project(grant numbers:62001142); Heilongjiang Postdoctoral Fund(grant numbers:LBH-Z20068); Postdoctoral Research Fund of the Harbin Institute of Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762270","Classification;hyperspectral (HS) image;multispectral (MS) image;probability distribution adaptation;remote sensing (RS)","Hyperspectral imaging;Image reconstruction;Collaboration;Probability distribution;Dictionaries;Transfer learning;Maintenance engineering","geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;image reconstruction;image representation;image resolution;remote sensing","coupled dictionary learning;distance preserved probability distribution adaptation;imaging devices;hyperspectral images;higher spectral resolution;image reconstruction;spectral features;MS data;generated HS data;MS-HS RS datasets;MS-HS image collaborative classification method;multispectral-hyperspectral image collaborative classification","","","","39","IEEE","22 Apr 2022","","","IEEE","IEEE Journals"
"Infrared Small Target Detection Based on Smoothness Measure and Thermal Diffusion Flowmetry","T. Ma; Z. Yang; X. Ren; J. Wang; Y. Ku","School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; School of Electrical Engineering, Zhengzhou University, Zhengzhou, China; School of Electrical Engineering, Zhengzhou University, Zhengzhou, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Infrared (IR) small target detection in a complex environment has become a hot topic. Due to the dim intensity and small scale of the target and the strong background clutter, the existing algorithms cannot achieve satisfactory results, namely poor detection performance and poor real-time performance. In this letter, a new real-time small target detection algorithm based on smoothness measure and thermal diffusion (STD) is proposed. First, the smoothness measure is proposed to extract smoothness features of small targets and suppress the clutter background. Then, we improve the existing thermal diffusion equation and design a thermal diffusion operator to extract the target’s thermal diffusion feature. The operator will further enhance the small and dim target’s energy. Finally, the fusion result image—in which the background is suppressed and the target is enhanced—is acquired after combining smoothness and thermal diffusion flowmetry. The experimental results demonstrate that the detection speed of the proposed algorithm is at least four times faster than that of existing algorithms under the same detection conditions. In addition, the proposed algorithm can achieve outstanding detection performance for IR images with different types of complex backgrounds and targets’ sizes within  $7\times7$ .","1558-0571","","10.1109/LGRS.2021.3096520","National Natural Science Foundation of China (NSFC)(grant numbers:61903340); Key Scientific Research Projects of Universities in Henan(grant numbers:19A413002); Project of Young Talent Promotion of Henan Association for Science and Technology(grant numbers:2020HYTP028); Postdoctoral Science Foundation of Henan Province, China(grant numbers:001701002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9493709","Infrared (IR) small target;real-time detection;smoothness feature;thermal diffusion feature","Object detection;Image edge detection;Feature extraction;Clutter;Real-time systems;Thermal conductivity;Clouds","clutter;image fusion;infrared imaging;object detection;thermal diffusion","infrared small target detection;smoothness measure;thermal diffusion flowmetry;complex environment;dim intensity;background clutter;thermal diffusion equation;image fusion;complex backgrounds","","","","15","IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Unsupervised Change Detection in Multitemporal Multispectral Satellite Images Using Parallel Particle Swarm Optimization","H. Kusetogullari; A. Yavariabdi; T. Celik","Department of Computer Engineering, Gediz University, Izmir, Turkey; Faculty of Medicine, Department of Medical Engineering, University of Auvergne, Clermont-Ferrand, France; Department of Electrical and Electronics Engineering, Meliksah University, Kayseri, Turkey","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 Jul 2015","2015","8","5","2151","2164","In this paper, a novel algorithm for unsupervised change detection in multitemporal multispectral images of the same scene using parallel binary particle swarm optimization (PBPSO) is proposed. The algorithm operates on a difference image, which is created by using a novel fusion algorithm on multitemporal multispectral images, by iteratively minimizing a cost function with PBPSO to produce a final binary change-detection mask representing changed and unchanged pixels. Each BPSO of parallel instances is run on a separate processor and initialized with a different starting population representing a set of change-detection masks. A communication strategy is applied to transmit data in between BPSOs running in parallel. The algorithm takes the full advantage of parallel processing to improve both the convergence rate and detection performance. We demonstrate the accuracy of the proposed method by quantitative and qualitative tests on semisynthetic and real-world data sets. The semisynthetic results for different levels of Gaussian noise are obtained in terms of false and miss alarm (MA) rates between the estimated change-detection mask and the ground truth image. The proposed method on the semisynthetic data with high level of Gaussian noise obtains the final change-detection mask with a false error rate of 1.50 and MA error rate of 14.51.","2151-1535","","10.1109/JSTARS.2015.2427274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105842","Change detection;difference image;multispace optimization;multispectral image;multitemporal images;parallel binary particle swarm optimization (PBPSO);remote sensing;Change detection;difference image;multispace optimization;multispectral image;multitemporal images;parallel binary particle swarm optimization (PBPSO);remote sensing","Statistics;Sociology;Change detection algorithms;Satellites;Cost function;Remote sensing","error analysis;Gaussian noise;geophysical image processing;image fusion;iterative methods;particle swarm optimisation;terrain mapping","unsupervised change detection;multitemporal multispectral satellite images;parallel binary particle swarm optimization;fusion algorithm;PBPSO;final binary change-detection mask;communication strategy;parallel processing;real-world data sets;Gaussian noise;miss alarm rates;change-detection mask;false error rate;MA error rate;semisynthetic data","","29","","49","IEEE","12 May 2015","","","IEEE","IEEE Journals"
"Cascaded Convolutional Neural Network-Based Hyperspectral Image Resolution Enhancement via an Auxiliary Panchromatic Image","X. Lu; J. Zhang; D. Yang; L. Xu; F. Jia","School of Information Science and Technology, Donghua University, Shanghai, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Information Science and Technology, Donghua University, Shanghai, China; School of Information Science and Technology, Donghua University, Shanghai, China; School of Information Science and Technology, Donghua University, Shanghai, China","IEEE Transactions on Image Processing","30 Jul 2021","2021","30","","6815","6828","Owing to the limits of incident energy and hardware system, hyperspectral (HS) images always suffer from low spatial resolution, compared with multispectral (MS) or panchromatic (PAN) images. Therefore, image fusion has emerged as a useful technology that is able to combine the characteristics of high spectral and spatial resolutions of HS and PAN/MS images. In this paper, a novel HS and PAN image fusion method based on convolutional neural network (CNN) is proposed. The proposed method incorporates the ideas of both hyper-sharpening and MS pan-sharpening techniques, thereby employing a two-stage cascaded CNN to reconstruct the anticipated high-resolution HS image. Technically, the proposed CNN architecture consists of two sub-networks, the detail injection sub-network and unmixing sub-network. The former aims at producing a latent high-resolution MS image, whereas the latter estimates the desired high-resolution abundance maps by exploring the spatial and spectral information of both HS and MS images. Moreover, two model-training fashions are presented in this paper for the sake of effectively training our network. Experiments on simulated and real remote sensing data demonstrate that the proposed method can improve the spatial resolution and spectral fidelity of HS image, and achieve better performance than some state-of-the-art HS pan-sharpening algorithms.","1941-0042","","10.1109/TIP.2021.3098246","Natural Science Foundation of Shanghai(grant numbers:19ZR1453800); Fundamental Research Funds for the Central Universities(grant numbers:2232021D-33); National Natural Science Foundation of China(grant numbers:61871150); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495100","Convolutional neural network;hyperspectral;image fusion;panchromatic;pan-sharpening","Spatial resolution;Image resolution;Image fusion;Sensors;Hyperspectral imaging;Earth;Mixture models","convolutional neural nets;hyperspectral imaging;image enhancement;image fusion;image resolution;spectral analysis","convolutional neural network;auxiliary panchromatic image;image fusion;MS pan-sharpening;two-stage cascaded CNN;high-resolution HS image;detail injection sub-network;unmixing sub-network;high-resolution MS image;high-resolution abundance maps;spatial information;spectral information;hyperspectral image resolution enhancement;hyper-sharpening","","10","","46","IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Object-Based Area-to-Point Regression Kriging for Pansharpening","Y. Zhang; P. M. Atkinson; F. Ling; G. M. Foody; Q. Wang; Y. Ge; X. Li; Y. Du","Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Lancaster Environment Centre, Faculty of Science and Technology, Lancaster University, Lancaster, U.K.; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; School of Geography, University of Nottingham, Nottingham, U.K.; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographic Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China; Key Laboratory of Monitoring and Estimate for Environment and Disaster of Hubei Province, Innovation Academy for Precision Measurement Science and Technology, Chinese Academy of Sciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","24 Sep 2021","2021","59","10","8599","8614","Optical earth observation satellite sensors often provide a coarse spatial resolution (CR) multispectral (MS) image together with a fine spatial resolution (FR) panchromatic (PAN) image. Pansharpening is a technique applied to such satellite sensor images to generate an FR MS image by injecting spatial detail taken from the FR PAN image while simultaneously preserving the spectral information of MS image. Pansharpening methods are mostly applied on a per-pixel basis and use the PAN image to extract spatial detail. However, many land cover objects in FR satellite sensor images are not illustrated as independent pixels, but as many spatially aggregated pixels that contain important semantic information. In this article, an object-based pansharpening approach, termed object-based area-to-point regression kriging (OATPRK), is proposed. OATPRK aims to fuse the MS and PAN images at the object-based scale and, thus, takes advantage of both the unified spectral information within the CR MS images and the spatial detail of the FR PAN image. OATPRK is composed of three stages: image segmentation, object-based regression, and residual downscaling. Three data sets acquired from IKONOS and Worldview-2 and 11 benchmark pansharpening algorithms were used to provide a comprehensive assessment of the proposed OATPRK approach. In both the synthetic and real experiments, OATPRK produced the most superior pan-sharpened results in terms of visual and quantitative assessment. OATPRK is a new conceptual method that advances the pixel-level geostatistical pansharpening approach to the object level and provides more accurate pan-sharpened MS images.","1558-0644","","10.1109/TGRS.2020.3041724","Key Research Program of Frontier Sciences, Chinese Academy of Sciences(grant numbers:ZDBS-LY-DQC034); National Natural Science Foundation of China(grant numbers:41801292,41971297); Hubei Provincial Natural Science Foundation for Innovation Groups(grant numbers:2019CFA019); Natural Science Foundation of Hubei Province(grant numbers:2018CFB274); Hubei Province Natural Science Fund for Distinguished Young Scholars(grant numbers:2018CFA062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9293157","Downscaling;geostatistics;image fusion;object-based;pansharpening;segmentation","Satellites;Sensors;Image segmentation;Image sensors;Spatial resolution;Optical sensors;Bandwidth","geophysical image processing;geophysical techniques;image fusion;image resolution;image segmentation;remote sensing;statistical analysis","optical earth observation satellite sensors;coarse spatial resolution multispectral image;fine spatial resolution panchromatic image;FR MS image;FR PAN image;pansharpening methods;land cover objects;FR satellite sensor images;spatially aggregated pixels;object-based pansharpening;termed object-based area-to-point regression kriging;OATPRK;object-based scale;unified spectral information;CR MS images;image segmentation;object-based regression;11 benchmark pansharpening algorithms;superior pan-sharpened results;pixel-level geostatistical pansharpening approach;object level;accurate pan-sharpened MS images","","8","","82","IEEE","14 Dec 2020","","","IEEE","IEEE Journals"
"Hyperspectral Pansharpening Based on Improved Deep Image Prior and Residual Reconstruction","W. G. C. Bandara; J. M. J. Valanarasu; V. M. Patel","Whiting School of Engineering, The Johns Hopkins University, Baltimore, MD, USA; Whiting School of Engineering, The Johns Hopkins University, Baltimore, MD, USA; Whiting School of Engineering, The Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Geoscience and Remote Sensing","3 Mar 2022","2022","60","","1","16","Hyperspectral pansharpening aims to synthesize a low-resolution hyperspectral image (LR-HSI) with a registered panchromatic (PAN) image to generate an enhanced HSI with high spectral and spatial resolution. Recently, the proposed HS pansharpening methods have obtained remarkable results using deep convolutional networks (ConvNets), which typically consist of three steps: 1) upsampling the LR-HSI; 2) predicting the residual image via a ConvNet; and 3) obtaining the final fused HSI by adding the outputs from first and second steps. Recent methods have leveraged deep image prior (DIP) to upsample the LR-HSI due to its excellent ability to preserve both spatial and spectral information, without learning from large datasets. However, we observed that the quality of upsampled HSIs can be further improved by introducing an additional spatial-domain constraint to the conventional spectral-domain energy function. We define our spatial-domain constraint as the  $L_{1}$  distance between the predicted PAN image and the actual PAN image. To estimate the PAN image of the upsampled HSI, we also propose a learnable spectral response function (SRF). Moreover, we noticed that the residual image between the upsampled HSI and the reference HSI mainly consists of edge information and very fine structures. In order to accurately estimate fine information, we propose a novel overcomplete network, called HyperKite, which focuses on learning high-level features by constraining the receptive from increasing in the deep layers. We perform experiments on three semisynthetic and one real HSI datasets to demonstrate the superiority of our DIP-HyperKite over the state-of-the-art pansharpening methods. The deployment codes, pretrained models, and final fusion outputs of our DIP-HyperKite and the methods used for the comparisons will be publicly made available at https://github.com/wgcban/DIP-HyperKite.git.","1558-0644","","10.1109/TGRS.2021.3139292","NSF CAREER Award(grant numbers:2045489); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664535","Deep image prior (DIP);hyperspectral image fusion;hyperspectral pansharpening;overcomplete representations;spatial and spectral constraints","Pansharpening;Spatial resolution;Electronics packaging;Feature extraction;Image reconstruction;Hyperspectral imaging;Bayes methods","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;image sensors;learning (artificial intelligence);remote sensing","DIP-HyperKite;hyperspectral pansharpening;residual reconstruction;low-resolution hyperspectral image;registered panchromatic image;high spectral resolution;spatial resolution;deep convolutional networks;ConvNet;LR-HSI;residual image;final fused HSI;spatial information;spectral information;upsampled HSI;spectral-domain energy function;actual PAN image;learnable spectral response function;reference HSI;fine information;semisynthetic HSI datasets;real HSI datasets;state-of-the-art pansharpening methods;final fusion outputs","","6","","59","IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"Bayesian Hyperspectral Image Super-Resolution in the Presence of Spectral Variability","F. Ye; Z. Wu; Y. Xu; H. Liu; Z. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Science, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","22 Dec 2022","2022","60","","1","13","Synthesizing a high-resolution (HR) hyperspectral image (HSI) by merging a low-resolution (LR) HSI with a corresponding HR multispectral image (MSI) has become a promising HSI super-resolution scheme. Most existing HSI-MSI fusion methods are effective to some extent, while several challenges remain. First, the spectral response of a given material exhibits considerable variability due to different acquisition times and conditions, however, variations in spectral signatures are often neglected. Second, a majority of off-the-shelf methods require predefined degradation operators, which can be unavailable in practice. To tackle the above issues, we introduce a novel fusion approach with a Bayesian framework. Specifically, we regard the up-sampled LR-HSI as the low-frequency component of the underlying HR-HSI. We characterize the texture features of high- and low-frequency components, respectively, which can enlarge modeling capacity and bypass the absence of degradation operators. Furthermore, we depict the relative smoothness of reflectance spectra with the Gaussian process. Extensive experiments on synthesized and real datasets illustrate the superiority of the proposed strategy in terms of fusion performance and robustness to spectral variability.","1558-0644","","10.1109/TGRS.2022.3228313","National Natural Science Foundation of China(grant numbers:62071233,61971223,61976117); Jiangsu Provincial Natural Science Foundation of China(grant numbers:BK20211570,BK20180018,BK20191409); Fundamental Research Funds for the Central Universities(grant numbers:30917015104,30919011103,30919011402,30921011209,JSGP202204); Key Projects of University Natural Science Fund of Jiangsu Province(grant numbers:19KJA360001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9980459","Bayesian inference;hyperspectral image (HSI);image fusion;multispectral image (MSI);spectral variability;super-resolution","Superresolution;Bayes methods;Mathematical models;Dictionaries;Degradation;Hyperspectral imaging;Spatial resolution","Bayes methods;Gaussian processes;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;image texture;remote sensing","Bayesian framework;Bayesian hyperspectral image super-resolution;corresponding HR multispectral image;degradation operators;different acquisition times;existing HSI-MSI fusion methods;fusion approach;fusion performance;given material exhibits considerable variability;high-resolution hyperspectral image;low-frequency component;low-resolution HSI;LR-HSI;off-the-shelf methods;promising HSI super-resolution scheme;real datasets;robustness;spectral response;spectral signatures;spectral variability;synthesized datasets;underlying HR-HSI","","1","","54","IEEE","12 Dec 2022","","","IEEE","IEEE Journals"
"A New Context-Aware Details Injection Fidelity With Adaptive Coefficients Estimation for Variational Pansharpening","J. -L. Xiao; T. -Z. Huang; L. -J. Deng; Z. -C. Wu; G. Vivone","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; National Research Council—Institute of Methodologies for Environmental Analysis (CNR-IMAA), Tito, Italy","IEEE Transactions on Geoscience and Remote Sensing","7 Apr 2022","2022","60","","1","15","Pansharpening is related to the fusion of a low spatial resolution multispectral (MS) image retaining an abundant spectral content and a high spatial resolution panchromatic (PAN) image to obtain a product with both the abundant spectral content of the former and the high spatial resolution of the latter. Many previous studies are only focused on the global or local relationship between the PAN image and the corresponding high-resolution multispectral (HRMS) image. However, we found that the relationship between PAN and HRMS images in the gradient domain can be better explored through the image context. In this article, we propose context-aware details injection fidelity (CDIF) with adaptive coefficients estimation, which can fully explore the complicated relationship between the PAN image and the HRMS image in the gradient domain. More specifically, we apply a clustering method to divide the pixels of an image into different context-based regions. Afterward, the adaptive coefficients are estimated by using a regression-based method for each region. The CDIF is effective in extracting the main features from the two inputs to be fused. In addition, we integrate the CDIF with a conventional fidelity term and a total variation regularization to formulate a novel variational pansharpening model that is solved by designing an algorithm based on the alternating direction method of multiplier (ADMM) framework. Qualitative and quantitative assessments on different datasets support the effectiveness and robustness of the proposed method. The code is available at https://github.com/liangjiandeng/CDIF.","1558-0644","","10.1109/TGRS.2022.3154480","National Natural Science Foundation of China(grant numbers:12171072,61702083); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216); National Key Research and Development Program of China(grant numbers:2020YFA0714001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721243","Adaptive coefficients;context-aware fidelity;image fusion;pansharpening;remote sensing;variational models","Pansharpening;Spatial resolution;Feature extraction;Sensors;Estimation;Task analysis;Satellites","image fusion;image resolution;regression analysis","context-aware details injection fidelity;adaptive coefficients estimation;low spatial resolution multispectral image;abundant spectral content;high spatial resolution panchromatic image;PAN image;high-resolution multispectral image;image context;CDIF;HRMS image;regression-based method;variational pansharpening","","5","","61","IEEE","24 Feb 2022","","","IEEE","IEEE Journals"
"ArbRPN: A Bidirectional Recurrent Pansharpening Network for Multispectral Images With Arbitrary Numbers of Bands","L. Chen; Z. Lai; G. Vivone; G. Jeon; J. Chanussot; X. Yang","College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; National Research Council, Institute of Methodologies for Environmental Analysis (CNR-IMAA), Tito Scalo, Italy; Department of Embedded Systems Engineering, Incheon National University, Incheon, South Korea; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","21 Feb 2022","2022","60","","1","18","Although the performance of pansharpening has been significantly improved by advanced deep-learning (DL) technologies in recent years, most DL-based methods fail to process multispectral (MS) images with arbitrary numbers of bands by a single model. Consequently, it is inevitable to train separate models for MS images with different numbers of bands, which is time- and storage-consuming as well as inefficient in practice. To tackle the above problem, we propose a bidirectional recurrent pansharpening network (named ArbRPN) for MS images with arbitrary numbers of bands. Our ArbRPN can dynamically reconstruct high-resolution (HR) MS images with different numbers of bands by adaptively changing the number of recurrence to the number of bands of the low-resolution (LR) MS images. Leveraging on the ability of the ArbRPN to process MS images with any number of bands, one can even customize the bands to be pansharpened. Moreover, to achieve superior performance, spectral discrepancy and dependence are considered in the ArbRPN. Details from the panchromatic (PAN) image are adaptively injected into the fused product according to the captured spectral dependence. Furthermore, training strategies of existing DL-based pansharpening methods can only group MS images with a constant number of bands into mini-batches. Therefore, we present a mask-based training method (called mask-training) to solve this problem. Benefiting from the mask-training, our ArbRPN can achieve superior performance and robustness during pansharpening. Extensive experiments show the superior performance of our ArbRPN with respect to the state-of-the-art (SOTA) methods applied to MS images with different numbers of bands. The code of our ArbRPN is available on https://github.com/Lihui-Chen/ArbRPN.git.","1558-0644","","10.1109/TGRS.2021.3131228","National Key Research and Development Program of China(grant numbers:2020AAA0104500); Sichuan University(grant numbers:2020SCUNG205); China Scholarship Council (CSC)(grant numbers:202006240191); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627886","Deep learning (DL);image fusion;multispectral (MS) images;pansharpening;recurrent neural networks (RNNs);remote sensing","Pansharpening;Training;Image reconstruction;Convolutional neural networks;Sensors;Recurrent neural networks;Distortion","deep learning (artificial intelligence);geophysical image processing;image fusion;image resolution;recurrent neural nets","ArbRPN;bidirectional recurrent pansharpening network;multispectral images;arbitrary numbers;mask-training;spectral dependence;DL-based pansharpening;panchromatic image;PAN;low-resolution MS images;deep learning","","3","","73","IEEE","29 Nov 2021","","","IEEE","IEEE Journals"
"Pansharpening for Multiband Images With Adaptive Spectral–Intensity Modulation","Y. Yang; L. Wu; S. Huang; Y. Tang; W. Wan","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, China; Division of Computer Science and Engineering, Chonbuk National University, Jeonju, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Sep 2018","2018","11","9","3196","3208","The pansharpening algorithm often faces an imbalance between spatial sharpness and spectral preservation, resulting in spectral and intensity inhomogeneities in the fused image. In this paper, to overcome this problem, we present a robust pansharpening method for multiband images with adaptive spectral–intensity modulation. In this method, we propose an adaptive spectral modulation coefficient (ASMC) and an adaptive intensity modulation coefficient (AIMC) to modulate the spectral and spatial information in the fused image, respectively. Among these coefficients, the ASMC is constructed based on two aspects: first, the details extracted from the panchromatic (PAN) and multispectral (MS) images; and second, the spectral relationship between each MS band. The AIMC is calculated by assessing the correlation and standard deviation between the PAN image and each MS band. Finally, we propose a mathematically linear model to combine ASMC and AIMC to achieve the fused image. Various remote-sensing satellite images were used in the evaluations. Experimental results indicate that the proposed method achieves outstanding performance in balancing spatial and spectral information and outperforms several state-of-the-art fusion methods in terms of both full-reference and no-reference metrics, and on visual inspection.","2151-1535","","10.1109/JSTARS.2018.2849011","National Natural Science Foundation of China(grant numbers:61662026,61462031); Natural Science Foundation of Jiangxi Province(grant numbers:20161ACB21015); Project of the Education Department of Jiangxi Province(grant numbers:KJLD14031,GJJ170312,GJJ170318); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402210","Intensity modulation;linear combination;pansharpening;spectral modulation","Transforms;Distortion;Image resolution;Remote sensing;Nonhomogeneous media;Intensity modulation","image fusion;intensity modulation;remote sensing","fused image;remote-sensing satellite images;spatial information;spectral information;multiband images;adaptive spectral-intensity modulation;pansharpening algorithm;spectral preservation;spectral intensity inhomogeneities;robust pansharpening method;adaptive spectral modulation coefficient;adaptive intensity modulation coefficient;PAN image","","17","","51","IEEE","3 Jul 2018","","","IEEE","IEEE Journals"
"A Multirepresentational Fusion of Time Series for Pixelwise Classification","D. Dias; A. Pinto; U. Dias; R. Lamparelli; G. Le Maire; R. d. S. Torres","Institute of Computing, University of Campinas (Unicamp), Campinas, Brazil; Institute of Computing, School of Physical Education, University of Campinas (Unicamp), Campinas, Brazil; School of Technology, University of Campinas (Unicamp), Limeira, Brazil; Núcleo Interdisciplinar de Planejamento Energético, University of Campinas, Campinas, Brazil; Eco&Sols, CIRAD, INRA, IRD, Montpellier SupAgro, University of Montpellier, Montpellier cedex 2, France; Department of ICT and Natural Sciences, Norwegian University of Science and Technology, Ålesund, Norway","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 Aug 2020","2020","13","","4399","4409","This article addresses the pixelwise classification problem based on temporal profiles, which are encoded in 2-D representations based on recurrence plots, Gramian angular/ difference fields, and Markov transition field. We propose a multirepresentational fusion scheme that exploits the complementary view provided by those time series representations and different data-driven feature extractors and classifiers. We validate our ensemble scheme in the problem related to the classification of eucalyptus plantations in remote sensing images. Achieved results demonstrate that our proposal overcomes recently proposed baselines, and now represents the new state-of-the-art classification solution for the target dataset.","2151-1535","","10.1109/JSTARS.2020.3012117","CNPq(grant numbers:307560/2016-3); São Paulo Research Foundation—FAPESP(grant numbers:2014/12236-1,2015/24494-8,2016/50250-1,2017/20945-0,2019/16253-1); FAPESP-Microsoft Virtual Institute(grant numbers:2013/50155-0,2014/50715-9); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior(grant numbers:001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149715","Classifier fusion;eucalyptus;pixelwise classification;time series representation","Time series analysis;Feature extraction;Remote sensing;Electronic mail;Vegetation mapping;Indexes;Earth","feature extraction;geophysical image processing;image classification;image fusion;image representation;Markov processes;remote sensing;time series;vegetation","pixelwise classification;temporal profiles;recurrence plots;Markov transition field;time series representations;ensemble scheme;eucalyptus plantations;remote sensing images;data-driven feature extractors;multirepresentational fusion;2D representations;Gramian angular fields;Gramian difference fields;classifiers","","7","","45","CCBY","27 Jul 2020","","","IEEE","IEEE Journals"
"CS-HSNet: A Cross-Siamese Change Detection Network Based on Hierarchical-Split Attention","Q. Ke; P. Zhang","School of Electronics and Communication Engineering, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China; School of Electronics and Communication Engineering, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14 Oct 2021","2021","14","","9987","10002","Change detection methods for optical remote sensing images play an important role in environmental resource management. Although recent methods based on deep learning demonstrate incredible ability by constructing networks, first, extracting bitemporal features in a separate manner; second, fusing bitemporal images before forwarding them into the single-level network. Both severely neglect the effect of spatial-temporal feature correlation between bitemporal images. In addition, most existing methods represent multiscale feature pairs in a layer-wise manner like ResNet, failing to consider the inner multilevel structure. In this work, we propose a new siamese change detection feature encoder backbone named cross-siamese Res2Net (CSRes2Net), by establishing crossed and hierarchical residual-like connections within one single residual block. The CSRes2Net represents dual features in a fine-grained manner and fully leads to the flow of bitemporal features. In addition, recent learning-based methods designed some spatial-temporal relation modules to capture the pixel-level pairwise relationship and channel dependency based on self-attention mechanism, but they only consider spatial and channel dimension corrections separately with excessive parameters. So we propose a lightweight cross spatial-channel triplet attention module to capture cross-dimensional long-range relationship between triplet combinations: channel with height, channel with width, channel with channel. Finally, we propose a hierarchical-split block for generating multiscale feature representations in a coarse-to-fine fashion. The experiments results on LEVIR-CD and season-varying change detection dataset outperform most state-of-the-art models.","2151-1535","","10.1109/JSTARS.2021.3113831","Shenzhen Science and Technology Program(grant numbers:KQTD20190929172704911); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9543576","Attention mechanism;hierarchical-split structure;image change detection;multiscale features","Feature extraction;Task analysis;Semantics;Remote sensing;Computational efficiency;Transformers;Licenses","correlation methods;deep learning (artificial intelligence);environmental management;environmental science computing;feature extraction;geophysical image processing;image fusion;image representation;remote sensing","CSRes2Net;spatial-temporal relation modules;pixel-level pairwise relationship;lightweight cross spatial-channel triplet attention module;cross-dimensional long-range relationship;hierarchical-split block;multiscale feature representations;hierarchical-split attention;optical remote sensing images;environmental resource management;deep learning;single-level network;spatial-temporal feature correlation;multiscale feature pairs;Siamese change detection feature encoder backbone;cross-Siamese change detection network;bitemporal images fusion;CS-HSNet","","5","","57","CCBY","21 Sep 2021","","","IEEE","IEEE Journals"
"Broad Area Search and Detection of Surface-to-Air Missile Sites Using Spatial Fusion of Component Object Detections From Deep Neural Networks","A. B. Cannaday II; C. H. Davis; G. J. Scott; B. Ruprecht; D. T. Anderson","Center for Geospatial Intelligence, University of Missouri, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri, Columbia, MO, USA; Center for Geospatial Intelligence, University of Missouri, Columbia, MO, USA; Mizzou Information and Data Fusion Laboratory, University of Missouri, Columbia, MO, USA; Mizzou Information and Data Fusion Laboratory, University of Missouri, Columbia, MO, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Aug 2020","2020","13","","4728","4737","Here, we demonstrate how deep neural network (DNN) detections of multiple constitutive or component objects that are part of a larger, more complex, and encompassing feature can be spatially fused to improve the search, detection, and retrieval (ranking) of the larger complex feature. First, scores computed from a spatial clustering algorithm are normalized to a reference space so that they are independent of image resolution and DNN input chip size. Then, multiscale DNN detections from various component objects are fused to improve the detection and retrieval of DNN detections of a larger complex feature. We demonstrate the utility of this approach for broad area search and detection of surface-to-air missile (SAM) sites that have a very low occurrence rate (only 16 sites) over a ~90 000 km2 study area in SE China. The results demonstrate that spatial fusion of multiscale componentobject DNN detections can reduce the detection error rate of SAM Sites by >85% while still maintaining a 100% recall. The novel spatial fusion approach demonstrated here can be easily extended to a wide variety of other challenging object search and detection problems in large-scale remote sensing image datasets.","2151-1535","","10.1109/JSTARS.2020.3015662","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164937","Broad area search;data fusion;deep neural networks (DNN);information retrieval;object detection;spatial clustering","Detectors;Missiles;Training;Remote sensing;Search problems;Feature extraction;Training data","image fusion;image resolution;missiles;neural nets;object detection;pattern clustering;remote sensing","broad area search;surface-to-air missile sites;component object detections;deep neural networks;spatial clustering algorithm;multiscale DNN detections;SAM sites;spatial fusion;large-scale remote sensing image datasets;image resolution","","2","","35","CCBY","11 Aug 2020","","","IEEE","IEEE Journals"
"Improving the Spatial Resolution of Landsat TM/ETM+ Through Fusion With SPOT5 Images via Learning-Based Super-Resolution","H. Song; B. Huang; Q. Liu; K. Zhang","Nanjing University of Information Science and Technology, Nanjing, China; Chinese University of Hong Kong, Shatin, Hong Kong; Nanjing University of Information Science and Technology, Nanjing, China; Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","16 Sep 2014","2015","53","3","1195","1204","To take advantage of the wide swath width of Landsat Thematic Mapper (TM)/Enhanced Thematic Mapper Plus (ETM+) images and the high spatial resolution of Système Pour l'Observation de la Terre 5 (SPOT5) images, we present a learning-based super-resolution method to fuse these two data types. The fused images are expected to be characterized by the swath width of TM/ETM+ images and the spatial resolution of SPOT5 images. To this end, we first model the imaging process from a SPOT image to a TM/ETM+ image at their corresponding bands, by building an image degradation model via blurring and downsampling operations. With this degradation model, we can generate a simulated Landsat image from each SPOT5 image, thereby avoiding the requirement for geometric coregistration for the two input images. Then, band by band, image fusion can be implemented in two stages: 1) learning a dictionary pair representing the high- and low-resolution details from the given SPOT5 and the simulated TM/ETM+ images; 2) super-resolving the input Landsat images based on the dictionary pair and a sparse coding algorithm. It is noteworthy that the proposed method can also deal with the conventional spatial and spectral fusion of TM/ETM+ and SPOT5 images by using the learned dictionary pairs. To examine the performance of the proposed method of fusing the swath width of TM/ETM+ and the spatial resolution of SPOT5, we illustrate the fusion results on the actual TM images and compare with several classic pansharpening methods by assuming that the corresponding SPOT5 panchromatic image exists. Furthermore, we implement the classification experiments on both actual images and fusion results to demonstrate the benefits of the proposed method for further classification applications.","1558-0644","","10.1109/TGRS.2014.2335818","Hong Kong Research Grant Council(grant numbers:CUHK 444612); National Natural Science Foundation of China(grant numbers:61272223); National Science Foundation of Jiangsu Province(grant numbers:BK2012045); Nanjing University of Information Science and Technology Scientific Research Foundation(grant numbers:S8113049001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6866167","Landsat Thematic Mapper (TM) or Enhanced Thematic Mapper Plus (ETM+) image;spatial resolution;Système Pour l'Observation de la Terre 5 (SPOT5) image;super-resolution;swath width;Landsat Thematic Mapper (TM) or Enhanced Thematic Mapper Plus (ETM+) image;spatial resolution;Système Pour l'Observation de la Terre 5 (SPOT5) image;super-resolution;swath width","Spatial resolution;Dictionaries;Training;Degradation;Sensors;Satellites","geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing","classification experiments;image fusion;Enhanced Thematic Mapper Plus;Landsat Thematic Mapper;learning-based superresolution;SPOT5 images;Landsat TM/ETM+ images;spatial resolution","","49","","34","IEEE","25 Jul 2014","","","IEEE","IEEE Journals"
"A Fast Cartesian Back-Projection Algorithm Based on Ground Surface Grid for GEO SAR Focusing","Q. Chen; W. Liu; G. -C. Sun; X. Chen; L. Han; M. Xing","National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; Academy of Advanced Interdisciplinary Research, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; Physics and Optoelectronic Engineering, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","15 Feb 2022","2022","60","","1","14","Geosynchronous-Earth-orbit (GEO) synthetic aperture radar (SAR) provides excellent continuous observing capability and large swath. However, the extremely long synthetic aperture time, the curved orbit, and the nonplanar ground surface cause serious spatial variance in the GEO SAR signal. In this article, a novel fast Cartesian back-projection (BP) algorithm based on subaperture imaging on ground and multistage fusion is proposed for accurately and efficiently imaging of GEO SAR. The imaging grids are arranged on the ground surface to avoid the azimuth defocusing caused by the flat ground approximation. Then, a new two-step spectrum compression method is derived to solve the spectrum aliasing of subaperture images. Also, a multistage image fusion method is adopted to combine all the subaperture images with high efficiency. The computational complexity and the approximation of the proposed algorithm are also discussed. Simulation results verify the effectiveness of the proposed algorithm.","1558-0644","","10.1109/TGRS.2021.3125797","National Natural Science Foundation of China(grant numbers:62101404,61825105,61931025,61404130124); Shanghai Aerospace Science and Technology Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610055","Back-projection algorithm;geosynchronous-Earth-orbit (GEO) synthetic aperture radar (SAR);ground surface curvature;spectrum compression","Imaging;Synthetic aperture radar;Signal processing algorithms;Surface topography;Orbits;Radar polarimetry;Trajectory","approximation theory;computational complexity;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","fast Cartesian back-projection algorithm;ground surface grid;Geosynchronous-Earth-orbit synthetic aperture radar;extremely long synthetic aperture time;curved orbit;nonplanar ground surface cause serious spatial variance;GEO SAR signal;subaperture imaging;multistage fusion;imaging grids;flat ground approximation;two-step spectrum compression method;subaperture images;multistage image fusion method;GEO SAR focusing","","1","","30","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Super-Resolution-Guided Progressive Pansharpening Based on a Deep Convolutional Neural Network","J. Cai; B. Huang","Department of Geography and Resource Management, The Chinese University of Hong Kong, Hong Kong; Department of Geography and Resource Management, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","5206","5220","Pansharpening and super-resolution (SR) methods share the same target to improve the spatial resolution of images. Based on this similarity, we propose and develop a novel pansharpening algorithm that is guided by a deep SR convolutional neural network. The proposed framework comprises three components: an SR process, a progressive pansharpening process, and a high-pass residual module. Specifically, the SR process extracts inner spatial detail that is present in multispectral images. Then, progressive pansharpening is used as a detailed pansharpening process, and the high-pass residual module helps by directly injecting spatial detail from panchromatic images. The performance of the proposed network has been compared with that of traditional and other deep-learning-based pansharpening algorithms based on QuickBird, WorldView-3, and Landsat-8 data, and the results demonstrate the superiority of our algorithm.","1558-0644","","10.1109/TGRS.2020.3015878","Strategic Priority Research Program of the Chinese Academy of Sciences(grant numbers:XDA19090108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172104","Deep learning;multispectral (MS) image;panchromatic image;pansharpening;super-resolution (SR)","Spatial resolution;Remote sensing;Neural networks;Dictionaries;Transforms","geophysical image processing;geophysical signal processing;image colour analysis;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing;spectral analysis","deep SR convolutional neural network;SR process;progressive pansharpening process;high-pass residual module;multispectral images;detailed pansharpening process;panchromatic images;super-resolution-guided progressive pansharpening;deep convolutional neural network;novel pansharpening algorithm","","34","","49","IEEE","20 Aug 2020","","","IEEE","IEEE Journals"
"Pansharpening by Convolutional Neural Networks in the Full Resolution Framework","M. Ciotola; S. Vitale; A. Mazza; G. Poggi; G. Scarpa","Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Department of Science and Technology, University of Naples Parthenope, Naples, Italy; Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy; Department of Electrical Engineering and Information Technology, University of Naples Federico II, Naples, Italy","IEEE Transactions on Geoscience and Remote Sensing","19 Apr 2022","2022","60","","1","17","In recent years, there has been a growing interest in deep learning-based pansharpening. Thus far, research has mainly focused on architectures. Nonetheless, model training is an equally important issue. A first problem is the absence of ground truths, unavoidable in pansharpening. This is often addressed by training networks in a reduced-resolution domain and using the original data as ground truth, relying on an implicit scale invariance assumption. However, on full-resolution images, results are often disappointing, suggesting such invariance not to hold. A further problem is the scarcity of training data, which causes a limited generalization ability and a poor performance on off-training-test images. In this article, we propose a full-resolution training framework for deep learning-based pansharpening. The framework is fully general and can be used for any deep learning-based pansharpening model. Training takes place in the high-resolution domain, relying only on the original data, thus avoiding any loss of information. To ensure spectral and spatial fidelity, a suitable two-component loss is defined. The spectral component enforces consistency between the pansharpened output and the low-resolution multispectral input. The spatial component, computed at high resolution, maximizes the local correlation between each pansharpened band and the panchromatic input. At testing time, the target-adaptive operating modality is adopted, achieving good generalization with a limited computational overhead. Experiments carried out on WorldView-3, WorldView-2, and GeoEye-1 images show that methods trained with the proposed framework guarantee a pretty good performance in terms of both full-resolution numerical indexes and visual quality.","1558-0644","","10.1109/TGRS.2022.3163887","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745494","Convolutional neural network (CNN);data fusion;deep learning;image enhancement;multiresolution analysis (MRA);spectral distortion;structural consistency;super resolution;unsupervised learning","Pansharpening;Training;Spatial resolution;Sensors;Task analysis;Remote sensing;Multiresolution analysis","geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","high-resolution domain;spectral component enforces consistency;pansharpened output;low-resolution multispectral input;pansharpened band;framework guarantee;full-resolution numerical indexes;convolutional neural networks;full resolution framework;deep learning-based pansharpening;model training;equally important issue;ground truth;training networks;reduced-resolution domain;implicit scale invariance assumption;full-resolution images;training data;off-training-test images;full-resolution training framework","","11","","94","IEEE","31 Mar 2022","","","IEEE","IEEE Journals"
"Fusion Methods for Land Surface Emissivity and Temperature Retrieval of the Landsat Data Continuity Mission Data","H. Emami; A. Safari; B. Mojaradi","School of Surveying and Geospatial Engineering, University College of Engineering, University of Tehran, Tehran, Iran; School of Surveying and Geospatial Engineering, University College of Engineering, University of Tehran, Tehran, Iran; Department of Geomatics Engineering, School of Civil Engineering, Iran University of Science and Technology, Tehran, Iran","IEEE Transactions on Geoscience and Remote Sensing","24 May 2016","2016","54","7","3842","3855","In this paper, a new approach based on two fusion schemes is proposed to overcome the uncertainties in land surface emissivity (LSE) estimation and, consequently, land surface temperature (LST) retrieval. The fusion schemes are called image-based weighted methods and knowledge-based weighted methods, in which each of them includes two LSE estimation methods. The effectiveness of the two proposed fusion schemes is empirically tested over two scenes of Landsat-8 (known as Landsat Data Continuity Mission) data sets, and the obtained LSEs by individual and proposed methods were compared to the LSE product of Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) by image-based and class-based cross-comparison. In both scenes, the adjusted normalized emissivity method (ANEM) and NDVI-based emissivity method (NBEM) provide appropriate results among five individual methods. In contrast, weighted to median (WMED) achieves superior results among the proposed fusion methods for both scenes. In addition, the root-mean-square error (rmse) values of LSE obtained by ANEM and WMED are 1.48% and 0.87%, which lead to 1.25 K and 0.73 K errors in the LST retrieval by the single-channel algorithm in the first scene, respectively. For the second scene, the error values of NBEM and WMED are 1.10% and 0.52%, which lead to 0.93 K and 0.44 K errors in the LST, respectively. Moreover, the error ranges and rmse of cross-comparison for the obtained LSE in the proposed methods were remarkably decreased. Also, in this research, for LST cross-comparison, an alternative scaling method based on LST products of the Moderate Resolution Imaging Spectroradiometer was proposed. The LST validation results demonstrated that the proposed methods provide better estimates in terms of three accuracy measures in both examined data sets.","1558-0644","","10.1109/TGRS.2016.2529422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453166","Fusion;image-based weighted methods (IBWMs);knowledge-based weighted methods (KBWMs);Landsat Data Continuity Mission (LDCM);land surface emissivity (LSE);land surface temperature (LST);Fusion;image-based weighted methods (IBWMs);knowledge-based weighted methods (KBWMs);Landsat Data Continuity Mission (LDCM);land surface emissivity (LSE);land surface temperature (LST)","Estimation;Land surface temperature;Uncertainty;Satellites;Remote sensing;Earth;Land surface","geophysical image processing;image fusion;land surface temperature;remote sensing;vegetation","fusion method;land surface emissivity;land surface temperature retrieval;Landsat-8 data;continuity mission data;fusion scheme;image-based weighted method;knowledge-based weighted method;Landsat Data Continuity Mission;Advanced Spaceborne Thermal Emission and Reflection Radiometer;adjusted normalized emissivity method;NDVI-based emissivity method;root-mean-square error;single-channel algorithm;Moderate Resolution Imaging Spectroradiometer","","7","","75","IEEE","15 Apr 2016","","","IEEE","IEEE Journals"
"Multibranch Feature Extraction and Feature Multiplexing Network for Pansharpening","D. Lei; Y. Huang; L. Zhang; W. Li","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Geoscience and Remote Sensing","29 Dec 2021","2022","60","","1","13","With the continuous development of deep neural networks in the visual field, their application to panchromatic sharpening has received increasing attention from researchers; however, the existing panchromatic sharpening methods generally lack the ability to combine knowledge of the panchromatic sharpening field for feature extraction with neural networks, which have certain limitations in feature extraction and the discovery of new features. This article proposes a simple, modular, multibranched feature extraction and reuses network architecture designed not only to support feature reuse but to learn well-expressed new features for use in panchromatic sharpening approaches. In addition, we fused field knowledge of panchromatic sharpening to extract spatial structure information of panchromatic maps through gradient calculators and design structural and spectral compensation to fully extract and preserve the spatial structural and spectral information of images. We conducted experiments on the QuickBird and WorldView-3 satellite data sets, and the experimental results reveal that our proposed method has advantages over the best methods currently available, achieving excellent results not only on objective evaluation metrics, such as full-reference and no-reference metrics, but also on subjective visual evaluation.","1558-0644","","10.1109/TGRS.2021.3074624","National Natural Science Foundation of China(grant numbers:61472055,61802148,U1401252); Chongqing Outstanding Youth Fund(grant numbers:cstc2014jcyjjq40001); Chongqing Innovative Project of Overseas Study(grant numbers:cx2018120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423980","Deep neural networks;feature extraction;feature multiplexing;multibranch architecture","Feature extraction;Pansharpening;Remote sensing;Spatial resolution;Multiplexing;Training;Biological neural networks","feature extraction;geophysical image processing;geophysical signal processing;geophysical techniques;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing;terrain mapping","visual field;panchromatic sharpening methods;panchromatic sharpening field;simple feature extraction;modular feature extraction;multibranched feature extraction;network architecture;feature reuse;panchromatic sharpening approaches;panchromatic maps;gradient calculators;spectral compensation;multiplexing network;deep neural networks","","4","","27","IEEE","5 May 2021","","","IEEE","IEEE Journals"
"Pansharpening-Based Spatio-Temporal Fusion for Predicting Intense Surface Changes","Y. Li; R. Cai; J. Li; Z. Liu; L. Meng; L. He","Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","3 Jun 2022","2022","60","","1","14","Spatio-temporal fusion is a feasible way to provide synthetic satellite images with high spatial and high temporal resolution simultaneously. Due to its practicability, spatio-temporal fusion has gotten increasing attention, for which many spatio-temporal fusion approaches have been developed. Most spatio-temporal fusion methods follow the “base fine image guided” (BFIG) fusion mode, resulting in the fact that their fusion results are similar to the base fine images. Therefore, these methods can perform well in areas with limited surface changes due to high similarity between the base and the predicted fine images. However, they might not be applicable in areas with intense surface changes. In this article, we develop a pansharpening-based spatio-temporal fusion model (PSTFM) by introducing the pansharpening fusion mode, which is “coarse image guided” (CIG), into spatio-temporal fusion. PSTFM first trains a pansharpening convolutional neural network (CNN), which then fuses the coarse images and reconstructed panchromatic (Pan) images of the predicted time to recover the missing fine images. The newly proposed PSTFM is compared with three representative BFIG spatio-temporal fusion methods on two Landsat–Moderate Resolution Imaging Spectroradiometer (MODIS) datasets, both of which contain intense surface changes. After that, the experimental results are analyzed and discussed in detail. The experiments and the analysis demonstrate that the newly proposed PSTFM has remarkably qualitative and quantitative performance in predicting the intense surface changes while it is mediocre in areas with low surface change intensity.","1558-0644","","10.1109/TGRS.2022.3169494","National Natural Science Foundation of China(grant numbers:42030111,62071184); Guangzhou Science and Technology Program(grant numbers:202002030395); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761827","Base fine image guided (BFIG);coarse image guided (CIG);convolutional neural network (CNN);pansharpening;spatio-temporal fusion","Pansharpening;Spatial resolution;Convolutional neural networks;Remote sensing;Sensors;Satellites;Land surface","geophysical image processing;geophysical techniques;image fusion;image resolution;neural nets;remote sensing;sensor fusion;spatiotemporal phenomena","high spatial resolution;high temporal resolution;spatio-temporal fusion approaches;base fine image guided fusion mode;fusion results;fine images;pansharpening-based spatio-temporal fusion model;pansharpening fusion mode;coarse image;representative BFIG spatio-temporal fusion methods;intense surface changes","","","","50","IEEE","22 Apr 2022","","","IEEE","IEEE Journals"
"MSCDUNet: A Deep Learning Framework for Built-Up Area Change Detection Integrating Multispectral, SAR, and VHR Data","H. Li; F. Zhu; X. Zheng; M. Liu; G. Chen","Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Division of Landscape Architecture, Faculty of Architecture, The University of Hong Kong, Hong Kong SAR, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jul 2022","2022","15","","5163","5176","Built-up area change detection (CD) plays an important role in city management, which always uses very high spatial resolution (VHR) remote sensing data to extract refined spatial information. Recently, many CD models based on deep learning with VHR data have been proposed. However, due to the complex background information and natural landscape changes, VHR with optical RGB features is hard to extract changes exactly. To this end, we tend to explore the abundant channel information of multispectral and SAR data as a supplement to the refined spatial features of VHR images. We propose a new deep learning framework called multisource CD UNet++ (MSCDUNet), integrating multispectral, SAR, and VHR data for built-up area CD. First, we label and reform two new built-up area CD datasets containing multispectral, SAR, and VHR data: multisource built-up change (MSBC) and multisource OSCD (MSOSCD) datasets. Second, a feature selection method based on random forest is introduced to choose effective features from multispectral and SAR images. Finally, a multilevel heterogeneous feature fusion module is embedded in MSCDUNet to combine multifeatures for CD. Experiments are conducted on both the MSOSCD and the MSBC datasets. Compared to other CD methods based on VHR images, our proposal achieves the highest accuracy on both datasets and proves the effectiveness of multispectral, SAR, and VHR data fusion for CD. The dataset in the article will be available for download from the following link.1","2151-1535","","10.1109/JSTARS.2022.3181155","National Natural Science Foundation of China(grant numbers:61976234); Guangzhou Applied Basic Research Project; Natural Science Foundation of Guangdong Province(grant numbers:2019A1515011057); Fundamental Research Funds for the Central Universities; Sun Yat-sen University(grant numbers:22qntd2001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791854","Benchmark dataset;built-up;change detection (CD);deep learning (DL);multispectral data fusion;very high resolution (VHR)","Feature extraction;Synthetic aperture radar;Data mining;Spatial resolution;Remote sensing;Task analysis;Semantics","convolutional neural nets;deep learning (artificial intelligence);feature extraction;feature selection;geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image colour analysis;image fusion;image resolution;object detection;random forests;remote sensing by radar;synthetic aperture radar","very high spatial resolution images;multispectral data fusion;multilevel heterogeneous feature fusion module;multispectral images;random forest;feature selection method;MSOSCD datasets;MSBC datasets;multisource OSCD datasets;multisource built-up change datasets;optical RGB feature extraction;complex background information;multisource CD UNet++;deep learning framework;VHR images;refined spatial features;natural landscape changes;complex background information;CD models;high spatial resolution remote sensing data;SAR;area change detection;MSCDUNet;VHR data fusion","","2","","68","CCBY","8 Jun 2022","","","IEEE","IEEE Journals"
"SDPNet: A Deep Network for Pan-Sharpening With Enhanced Information Representation","H. Xu; J. Ma; Z. Shao; H. Zhang; J. Jiang; X. Guo","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China","IEEE Transactions on Geoscience and Remote Sensing","21 Apr 2021","2021","59","5","4120","4134","In this article, we propose a surface- and deep-level constraint-based pan-sharpening network, termed SDPNet, to address the pan-sharpening problem. Focusing on the two primary goals of pan-sharpening, i.e., spatial and spectral information preservations, we first design two encoder-decoder networks to extract deep-level features from two types of source images, in addition to surface-level characteristics, as the enhanced information representation. The unique feature maps that characterize the unique information in source images can be obtained through the deep-level feature extraction. We further design a pan-sharpening network with densely connected blocks to strengthen feature propagation and reduce parameter number, where the unique feature maps are utilized to efficiently constrain the similarity between the pan-sharpened result and the ground truth, thus avoiding information distortion. Both qualitative and quantitative comparisons on the reduced-resolution and full-resolution source images demonstrate the advantages of our method over state-of-the-art methods. Our code is publicly available at https://github.com/hanna-xu/SDPNet.","1558-0644","","10.1109/TGRS.2020.3022482","National Natural Science Foundation of China(grant numbers:61773295,41890820,61971165,61772512); Natural Science Foundation of Hubei Province(grant numbers:2019CFA037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200533","Encoder-decoder;feature extraction;image fusion;pan-sharpening","Feature extraction;Spatial resolution;Information representation;Data mining;Satellites;Training","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image reconstruction;image representation;image resolution;neural nets","enhanced information representation;deep-level constraint-based pan-sharpening network;SDPNet;pan-sharpening problem;spatial information preservations;spectral information preservations;encoder-decoder networks;surface-level characteristics;feature maps;deep-level feature extraction;feature propagation;pan-sharpened result;information distortion;reduced-resolution;full-resolution source images","","33","","41","IEEE","18 Sep 2020","","","IEEE","IEEE Journals"
"Cascaded Detection Framework Based on a Novel Backbone Network and Feature Fusion","Z. Tian; W. Wang; R. Zhan; Z. He; J. Zhang; Z. Zhuang","Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China; Science and Technology on Automatic Target Recognition Laboratory, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Oct 2019","2019","12","9","3480","3491","Due to the ability of powerful feature representation, deep-learning-based object detection has attracted considerable research attention, and many methods have been proposed for remote sensing images. However, there are still some problems that need to be addressed. In this paper, a novel and effective detection framework based on faster region-based convolutional neural network is designed. Specifically, first, in order to locate the boundaries of large objects and find the missing small objects, DetNet is incorporated into the detection framework as the backbone network. DetNet fixes the spatial resolution in deep layers and adopts dilated bottleneck with convolution projection to increase the divergence between input and output feature maps. Then, the proposed framework uses the backbone network to extract the scene features and region features simultaneously, which are both mapped to feature vectors and then fused together. The feature fusion operation can improve the feature representation of the generated region. Last, to improve the performance of localization, the cascade structure is adopted in the framework. The cascade structure has multiple phases and every phase has independent classifier and regressor. The results obtained from the previous phase are used as the regions of interest in the next phase. Therefore, the multiphase detector can increase the detection accuracy phase by phase. Comprehensive evaluations on a public ten-class object detection dataset demonstrate the effectiveness of the proposed framework. Moreover, ablation experiments are also implemented to show the respective influence of different parts of the framework on the performance improvement.","2151-1535","","10.1109/JSTARS.2019.2924086","National Natural Science Foundation of China(grant numbers:61471370,61401479); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754690","Convolutional neural network (CNN);feature fusion;object detection;remote sensing images","Feature extraction;Object detection;Remote sensing;Detectors;Task analysis;Support vector machines;Deep learning","convolutional neural nets;feature extraction;image classification;image fusion;image representation;learning (artificial intelligence);object detection;regression analysis","public ten-class object detection dataset;deep-learning-based object detection;remote sensing images;faster region-based convolutional neural network;DetNet;deep layers;convolution projection;output feature maps;scene features;region features;feature fusion operation;generated region;cascade structure;feature representation;detection framework","","11","","27","IEEE","3 Jul 2019","","","IEEE","IEEE Journals"
"An Improved Spatiotemporal Fusion Algorithm for Monitoring Daily Snow Cover Changes With High Spatial Resolution","Y. Wang; L. Gu; X. Li; F. Gao; T. Jiang; R. Ren","College of Electronic Science and Engineering, Jilin University, Changchun, China; College of Electronic Science and Engineering, Jilin University, Changchun, China; Northeast Institute of Geography and Agroecology, Chinese Academy of Sciences, Changchun, China; College of Computer Science and Technology, Jilin University, Changchun, China; Northeast Institute of Geography and Agroecology, Chinese Academy of Sciences, Changchun, China; College of Electronic Science and Engineering, Jilin University, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","14 Dec 2022","2022","60","","1","17","Considering the tradeoff between spatial resolution and temporal resolution, spatiotemporal fusion has become a promising technique to monitor snow cover dynamics with both high spatial and temporal resolutions. The representative spatiotemporal fusion methods, e.g., spatial–temporal data fusion approach (STDFA), usually exist obvious phenomenon of spectral distortion when the surface reflectance changes nonlinearly, which affects the quality of the spatiotemporal fusion image. To address this issue, an effective STDFA-matching-Pix2pix-generative adversarial network (SMPG) algorithm combining the unmixing-based method, deep learning method, and prematching and postmatching module is proposed to reduce the spectral distortion of STDFA fusion image. The high-temporal-low-spatial (HTLS) resolution MOD09GA data and high-spatial-low-temporal resolution (HSLT) Landsat 8 data are selected in this study. An SMPG algorithm is first employed to obtain daily high-spatial-high-temporal (HSHT) images, and then, daily snow cover results with a spatial resolution of 30 m are obtained by calculating the normalized difference snow index (NDSI). The SMPG algorithm is further compared with STDFA, spatial and temporal adaptive reflectance fusion model (STARFM), Flexible Spatiotemporal DAta Fusion (FSDAF), Swin spatiotemporal fusion model (SwinSTFM), and generative adversarial network-based spatiotemporal fusion model (GAN-STFM). The experimental results indicate that the proposed algorithm yields better overall performance in daily spatiotemporal fusion image and snow cover result with a spatial resolution of 30 m. The mean correlation coefficient (CC) of SMPG can achieve 0.962, which is 0.06–0.36 higher than that of other spatiotemporal fusion methods. The error between the percentage of snow cover area obtained through SMPG and validation data is within 0.84%.","1558-0644","","10.1109/TGRS.2022.3224126","National Natural Science Foundation of China(grant numbers:41871225,41871248); Project of Jilin Province Development and Reform Commission(grant numbers:2021C044-7); Technological Research and Development Projects of Jilin(grant numbers:20220201017GX); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9963674","Pix2pixGAN;snow cover;spatial-temporal data fusion approach (STDFA);spatiotemporal fusion","Spatiotemporal phenomena;Spatial resolution;Snow;Remote sensing;Reflectivity;Monitoring;Sensors","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;sensor fusion;snow;spatiotemporal phenomena;vegetation","daily snow cover results;daily spatiotemporal fusion image;deep learning method;effective STDFA-matching-Pix2pix-generative adversarial network;Flexible Spatiotemporal DAta Fusion;generative adversarial network-based spatiotemporal fusion model;high spatial resolution;high spatial resolutions;high-spatial-high-temporal images;high-spatial-low-temporal resolution Landsat 8 data;high-temporal-low-spatial resolution MOD09GA data;improved Spatiotemporal Fusion algorithm;monitoring daily snow cover changes;normalized difference snow index;size 30.0 m;SMPG algorithm;snow cover area;snow cover dynamics;snow cover result;spatial-temporal data fusion approach;spatiotemporal fusion methods;spectral distortion;STDFA fusion image;surface reflectance changes;Swin spatiotemporal fusion;temporal adaptive reflectance fusion model;temporal resolutions;unmixing-based method","","","","64","IEEE","24 Nov 2022","","","IEEE","IEEE Journals"
"Change Detection Meets Visual Question Answering","Z. Yuan; L. Mou; Z. Xiong; X. X. Zhu","Data Science in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Data Science in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Data Science in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Data Science in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","29 Sep 2022","2022","60","","1","13","The Earth’s surface is continually changing, and identifying changes plays an important role in urban planning and sustainability. Although change detection techniques have been successfully developed for many years, these techniques are still limited to experts and facilitators in related fields. In order to provide every user with flexible access to change information and help them better understand land-cover changes, we introduce a novel task: change detection-based visual question answering (CDVQA) on multitemporal aerial images. In particular, multitemporal images can be queried to obtain high-level change-based information according to content changes between two input images. We first build a CDVQA dataset, including multitemporal image–question–answer triplets using an automatic question–answer generation method. Then, a baseline CDVQA framework is devised in this work, and it contains four parts: multitemporal feature encoding, multitemporal fusion, multimodal fusion, and answer prediction. In addition, we also introduce a change enhancing module to multitemporal feature encoding, aiming at incorporating more change-related information. Finally, the effects of different backbones and multitemporal fusion strategies are studied on the performance of CDVQA task. The experimental results provide useful insights for developing better CDVQA models, which are important for future research on this task. The dataset will be available at https://github.com/YZHJessica/CDVQA.","1558-0644","","10.1109/TGRS.2022.3203314","China Scholarship Council; European Research Council (ERC); European Union’s Horizon 2020 Research and Innovation Programme (Acronym: So2Sat)(grant numbers:ERC-2016-StG-714087); Helmholtz Association; Helmholtz Excellent Professorship “Data Science in Earth Observation—Big Data Fusion for Urban Research”(grant numbers:W2-W3-100); German Federal Ministry of Education and Research (BMBF) in the Framework of the International Future AI Lab “AI4EO—Artificial Intelligence for Earth Observation: Reasoning, Uncertainties, Ethics and Beyond”(grant numbers:01DD20001); German Federal Ministry for Economic Affairs and Climate Action in the Framework of the “National Center of Excellence ML4Earth”(grant numbers:50EE2201C); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9901476","Change detection;deep learning;multitemporal aerial images;visual question answering (VQA)","Task analysis;Semantics;Visualization;Remote sensing;Earth;Feature extraction;Question answering (information retrieval)","geophysical image processing;geophysical signal processing;image classification;image fusion;question answering (information retrieval);remote sensing;sensor fusion;town and country planning","multitemporal image-question-answer triplets;automatic question-answer generation method;baseline CDVQA framework;multitemporal feature encoding;answer prediction;change-related information;multitemporal fusion strategies;CDVQA task;CDVQA models;change detection meets visual question;Earth's surface;identifying changes;urban planning;sustainability;change detection techniques;change information;land-cover changes;change detection-based visual question answering;multitemporal aerial images;particular images;multitemporal images;high-level change-based information;content changes;input images;CDVQA dataset","","","","45","CCBY","23 Sep 2022","","","IEEE","IEEE Journals"
"BSSNet: Building Subclass Segmentation From Satellite Images Using Boundary Guidance and Contrastive Learning","H. Xie; X. Hu; H. Jiang; J. Zhang","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Hubei Luojia Laboratory, Wuhan, China; National Geomatics Center of China, Beijing, China; Key Laboratory of Network Information System Technology, Institute of Electronic, and The Aerospace Information Research Institute, Chinese Academic of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Sep 2022","2022","15","","7700","7711","Building subclass segmentation, aimed at predicting classes of buildings (high-rise zone, low-rise zone, single high-rise, and single low-rise) from satellite images, is beneficial in numerous applications, including human geography, urban planning, and humanitarian aid. However, problems, such as complex scenes and similar characteristics of different building categories make it difficult for general models to balance the accuracy of localization and classification in building subclass segmentation. Therefore, this article proposes a novel network for building subclass segmentation called building subclass segmentation network (BSSNet), which uses two subnetworks to divide and conquer the problem. The first network guides the building locations through binary building segmentation, called localization network. The spatial gradient fusion module in the localization network improves the binary segmentation result by supervising the spatial gradient map of prediction. The second network is a classification network, which predicts building subclasses. Intermediate features of the second network are optimized by contrastive learning loss to improve feature consistency. Finally, predictions of the two networks are combined to obtain the final result. The experimental results demonstrate that our BSSNet can perform significant improvements on the Hainan dataset we produced and the xBD dataset. In particular, the BSSNet achieves the best performance compared to current methods on the Hainan dataset.","2151-1535","","10.1109/JSTARS.2022.3202524","National Natural Science Foundation of China(grant numbers:41771363,92038301); Special Fund of Hubei Luojia Laboratory(grant numbers:220100028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869688","Building subclass segmentation;contrastive learning loss;convolutional neural network (CNN);feature fusion;satellite image;spatial gradient fusion (SGF)","Buildings;Image segmentation;Task analysis;Location awareness;Remote sensing;Semantics;Head","feature extraction;geography;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);town and country planning","BSSNet;satellite images;high-rise zone;single high-rise;different building categories;building subclass segmentation called building subclass segmentation network;building locations;binary building segmentation;called localization network;binary segmentation result;building subclasses","","","","57","CCBY","29 Aug 2022","","","IEEE","IEEE Journals"
"Variational Regularization Network With Attentive Deep Prior for Hyperspectral–Multispectral Image Fusion","J. Yang; L. Xiao; Y. -Q. Zhao; J. C. -W. Chan","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Electronics and Informatics, Vrije Universiteit Brussel, Brussels, Belgium","IEEE Transactions on Geoscience and Remote Sensing","4 Jan 2022","2022","60","","1","17","Hyperspectral–multispectral image (HSI-MSI) fusion relies on a robust degradation model and data prior, where the former describes the degeneration of HSI in the spectral and spatial domains, and the latter reveals the latent statistics of the expected high-resolution (HR) HSI. In practice, the degradation model is often unknown, and the data prior is usually too complicated to be expressed analytically. In this study, we propose a variational network for HSI-MSI fusion (VaFuNet), in which the degradation model and data prior are implicitly represented by a deep learning network and jointly learned from the training data. A variational fusion model regularized by deep prior is first proposed, and then, it is optimized via a half-quadratic splitting and unfolded into a deep network. The deep prior is implicitly represented by a proximity operator. Due to the structural self-similarity, HSI possesses structural recurrences across different scales. To exploit such nonlocal prior and enhance the representability of network, we also propose a multiscale nonlocal attention and embed it into the deep prior proximity. The degradation model and deep prior proximity are jointly learned via end-to-end training. Experimental results on simulated and real-life HSI datasets demonstrate the effectiveness of the proposed VaFuNet HSI-MSI fusion method.","1558-0644","","10.1109/TGRS.2021.3080697","National Natural Science Foundation of China(grant numbers:62001226,61871226,61771391); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200465); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); Fundamental Research Funds for the Central Universities(grant numbers:30920021134); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442911","Attention;deep;enhancement;fusion;hyperspectral;multispectral;nonlocal","Degradation;Data models;Training data;Spatial resolution;Feature extraction;TV;Bandwidth","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image fusion","half-quadratic splitting;hyperspectral-multispectral imaging;high-resolution HSI datasets;deep prior proximity;multiscale nonlocal attention;variational fusion model;training data;deep learning network;variational network;spatial domains;spectral domains;robust degradation model;variational regularization network;VaFuNet HSI-MSI fusion method","","11","","53","IEEE","27 May 2021","","","IEEE","IEEE Journals"
"Synthetic Aperture Radar Image Change Detection via Siamese Adaptive Fusion Network","Y. Gao; F. Gao; J. Dong; Q. Du; H. -C. Li","Qingdao Key Laboratory of Mixed Reality, and Virtual Ocean, School of Information Science, and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality, and Virtual Ocean, School of Information Science, and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality, and Virtual Ocean, School of Information Science, and Engineering, Ocean University of China, Qingdao, China; Department of Electrical, and Computer Engineering, Mississippi State University, Starkville, MS, USA; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Nov 2021","2021","14","","10748","10760","Synthetic aperture radar (SAR) image change detection is a critical yet challenging task in the field of remote sensing image analysis. The task is nontrivial due to the following challenges: First, intrinsic speckle noise of SAR images inevitably degrades the neural network because of error gradient accumulation. Furthermore, the correlation among various levels or scales of feature maps is difficult to be achieved through summation or concatenation. Toward this end, we proposed a siamese adaptive fusion (AF) network for SAR image change detection. To be more specific, two-branch CNN is utilized to extract high-level semantic features of multitemporal SAR images. Besides, an AF module is designed to adaptively combine multiscale responses in convolutional layers. Therefore, the complementary information is exploited, and feature learning in change detection is further improved. Moreover, a correlation layer is designed to further explore the correlation between multitemporal images. Thereafter, robust feature representation is utilized for classification through a fully connected layer with softmax. Experimental results on four real SAR datasets demonstrate that the proposed method exhibits superior performance against several state-of-the-art methods. Our codes are available at https://github.com/summitgao/SAR_CD_SAFNet.","2151-1535","","10.1109/JSTARS.2021.3120381","National Key Research, and Development Program of China(grant numbers:2018AAA0100602); National Natural Science Foundation of China(grant numbers:U1706218,61871335); Key Technology Research and Development Program of Shandong(grant numbers:2019GHY112048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576639","Attention mechanism;change detection;deep learning;siamese adaptive fusion network (SAFNet);synthetic aperture radar (SAR)","Feature extraction;Radar polarimetry;Correlation;Synthetic aperture radar;Convolutional neural networks;Task analysis;Speckle","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;radar imaging;remote sensing;remote sensing by radar;speckle;synthetic aperture radar","synthetic aperture radar image change detection;siamese adaptive fusion network;critical yet challenging task;remote sensing image analysis;intrinsic speckle noise;neural network;error gradient accumulation;feature maps;SAR image change detection;high-level semantic features;multitemporal SAR images;feature learning;correlation layer;multitemporal images;robust feature representation;SAR datasets","","10","","42","CCBY","15 Oct 2021","","","IEEE","IEEE Journals"
"Spatial–Spectral Fusion by Combining Deep Learning and Variational Model","H. Shen; M. Jiang; J. Li; Q. Yuan; Y. Wei; L. Zhang","Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Resource and Environmental Science, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; MCFLY Technology, Beijing, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","22 Jul 2019","2019","57","8","6169","6181","In the field of spatial–spectral fusion, the variational model-based methods and the deep learning (DL)-based methods are state-of-the-art approaches. This paper presents a fusion method that combines the deep neural network with a variational model for the most common case of spatial–spectral fusion: panchromatic (PAN)/multispectral (MS) fusion. Specifically, a deep residual convolutional neural network (CNN) is first trained to learn the gradient features of the high spatial resolution multispectral image (HR-MS). The image observation variational models are then formulated to describe the relationships of the ideal fused image, the observed low spatial resolution multispectral image (LR-MS) image, and the gradient priors learned before. Then, fusion result can then be obtained by solving the fusion variational model. Both quantitative and visual assessments on high-quality images from various sources demonstrate that the proposed fusion method is superior to all the mainstream algorithms included in the comparison, in terms of overall fusion accuracy.","1558-0644","","10.1109/TGRS.2019.2904659","National Natural Science Foundation of China(grant numbers:41701400,61671334); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685700","Deep learning (DL);gradient network;model-based optimization;spatial–spectral fusion","Spatial resolution;Deep learning;Optimization;Distortion;Mathematical model;Remote sensing","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);sensor fusion;variational techniques","high spatial resolution multispectral image;image observation variational models;fusion variational model;fusion method;spatial-spectral fusion;variational model-based methods;deep learning-based methods;deep neural network;deep residual convolutional neural network;low spatial resolution multispectral image;panchromatic-multispectral fusion","","46","","53","IEEE","11 Apr 2019","","","IEEE","IEEE Journals"
"Data Fusion for Increasing Monitoring Capabilities of Sentinel Optical Data in Marine Environment","M. Kremezi; V. Karathanassi","Laboratory of Remote Sensing, School of Rural and Surveying Engineering, National Technical University of Athens, Athens, Greece; Laboratory of Remote Sensing, School of Rural and Surveying Engineering, National Technical University of Athens, Athens, Greece","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","31 Aug 2020","2020","13","","4809","4815","Marine monitoring constitutes one of the main thematic areas of the Sentinel mission. The Sentinel 3 OLCI (S3) sensor provides satellite data for services relevant to the ocean and land. While the spatial resolution of S3 images (300 m) is suitable for most marine applications, there are some applications such as floating debris detection, suspended mater estimation, etc., that require higher resolution. To fulfill this requirement this study applies an unmixing-based data fusion technique on S3 and BRDF-corrected Sentinel 2 (S2) images and evaluates the fused data by calculating the correlation coefficient and the spectral angle distance (SAD) indexes. Then, it explores the increased monitoring capabilities of the fused image by applying improved chlorophyll-a (Chl-a) and total suspended matter (TSM) algorithms, developed for satellite data. The fused image presents spectral similarity to S3 data and spatial similarity to S2 image. Consequently, the products provided by the fused image have much better resolution than those of S3 image, which enables detailed estimations of Chl-a and TSM concentrations. However, the dynamic nature of the marine environment that results in the formation of time-varying patterns at sea surface, in relation to the time lag between S2 and S3 image acquisitions may locally affect the accuracy of the products in the neighborhood of these patterns. This study exploits the effective elimination of directional reflectance effects in S2 ocean images, interprets the fused image and the generated ocean products, and points out the constraints regarding the synergy of Sentinel optical data for ocean areas.","2151-1535","","10.1109/JSTARS.2020.3018050","European Space Agency(grant numbers:4000131235/20/NL/GLC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171412","Bidirectional reflectance distribution function (BRDF) correction;chlorophyll-a (Chl-a);data fusion;marine monitoring;sentinel;spectral unmixing;total suspended matter (TSM)","Spatial resolution;Oceans;Cloud computing;Data integration;Monitoring;Optical imaging","environmental monitoring (geophysics);geophysical image processing;image fusion;ocean composition;oceanographic techniques;remote sensing;underwater optics","S2 image acquisitions;TSM algorithms;total suspended matter;chlorophyll-a;correlation coefficient;unmixing-based data fusion;spectral angle distance indexes;BRDF-corrected Sentinel 2 images;Sentinel 3 OLCI sensor;Sentinel mission;marine monitoring;sentinel optical data;S2 ocean images;S3 image acquisitions;marine environment;fused image","","","","29","CCBY","19 Aug 2020","","","IEEE","IEEE Journals"
"E-D-Net: Automatic Building Extraction From High-Resolution Aerial Images With Boundary Information","Y. Zhu; Z. Liang; J. Yan; G. Chen; X. Wang","School of Electronic, and Communication Engineering, Sun Yat-sen University, Guangzhou, China; Department of Electronic Engineering, Shantou University, Shantou, China; Department of Electronic Engineering, Shantou University, Shantou, China; School of Electrical Engineering, and Intelligentization, Dongguan University of Technology, Dongguan, China; School of Electronic, and Communication Engineering, Sun Yat-sen University, Guangzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2021","2021","14","","4595","4606","The automatic extraction of buildings from high-resolution aerial imagery plays a significant role in many urban applications. Recently, the convolution neural network (CNN) has gained much attention in remote sensing field and achieved a remarkable performance in building segmentation from visible aerial images. However, most of the existing CNN-based methods still have the problem of tending to produce predictions with poor boundaries. To address this problem, in this article, a novel semantic segmentation neural network named edge-detail-network (E-D-Net) is proposed for building segmentation from visible aerial images. The proposed E-D-Net consists of two subnetworks E-Net and D-Net. On the one hand, E-Net is designed to capture and preserve the edge information of the images. On the other hand, D-Net is designed to refine the results of E-Net and get a prediction with higher detail quality. Furthermore, a novel fusion strategy, which combines the outputs of the two subnetworks is proposed to integrate edge information with fine details. Experimental results on the INRIA aerial image labeling dataset and the ISPRS Vaihingen 2-D semantic labeling dataset demonstrate that, compared with the existing CNN-based model, the proposed E-D-Net provides noticeably more robust and higher building extraction performance, thus making it a useful tool for practical application scenarios.","2151-1535","","10.1109/JSTARS.2021.3073994","National Natural Science Foundation of China(grant numbers:61672335,61601276); Department of Education of Guangdong Province(grant numbers:2016KZDXM012,2017KCXTD015,2016A030310077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408358","Convolutional neural networks (CNNs);edge information;fully convolutional networks;high resolution;remote-sensing;semantic segmentation","Training;Image segmentation;Image edge detection;Buildings;Semantics;Neural networks;Urban planning","convolutional neural nets;edge detection;feature extraction;image fusion;image segmentation;remote sensing;terrain mapping","E-D-Net;CNN-based methods;building extraction performance;CNN-based model;ISPRS Vaihingen 2-D semantic labeling dataset;INRIA aerial image labeling dataset;edge information;subnetworks E-Net;edge-detail-network;visible aerial images;remote sensing field;convolution neural network;urban applications;high-resolution aerial imagery;boundary information;high-resolution aerial images;automatic building extraction","","20","","38","CCBY","19 Apr 2021","","","IEEE","IEEE Journals"
"CoSpace: Common Subspace Learning From Hyperspectral-Multispectral Correspondences","D. Hong; N. Yokoya; J. Chanussot; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan; GIPSA-Lab, CNRS, Grenoble INP, Université Grenoble Alpes, Grenoble, France; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","24 Jun 2019","2019","57","7","4349","4359","With a large amount of open satellite multispectral (MS) imagery (e.g., Sentinel-2 and Landsat-8), considerable attention has been paid to global MS land cover classification. However, its limited spectral information hinders further improving the classification performance. Hyperspectral imaging enables discrimination between spectrally similar classes but its swath width from space is narrow compared to MS ones. To achieve accurate land cover classification over a large coverage, we propose a cross-modality feature learning framework, called common subspace learning (CoSpace), by jointly considering subspace learning and supervised classification. By locally aligning the manifold structure of the two modalities, CoSpace linearly learns a shared latent subspace from hyperspectral-MS (HS-MS) correspondences. The MS out-of-samples can be then projected into the subspace, which are expected to take advantages of rich spectral information of the corresponding hyperspectral data used for learning, and thus leads to a better classification. Extensive experiments on two simulated HS-MS data sets (University of Houston and Chikusei), where HS-MS data sets have tradeoffs between coverage and spectral resolution, are performed to demonstrate the superiority and effectiveness of the proposed method in comparison with previous state-of-the-art methods.","1558-0644","","10.1109/TGRS.2018.2890705","H2020 European Research Council(grant numbers:ERC-2016-StG-714087); Helmholtz-Gemeinschaft(grant numbers:VH-NG-1018); German Research Foundation (DFG)(grant numbers:ZH 498/7-2); Japan Society for the Promotion of Science(grant numbers:15K20955); Alexander von Humboldt Fellowship for Post-Doctoral Researchers; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672122","Common subspace learning (CoSpace);cross-modality learning;hyperspectral;landcover classification;multispectral (MS);remote sensing","Hyperspectral imaging;Optimization;Satellites;Earth;Manifolds","geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing","spectral information hinders;classification performance;hyperspectral imaging;spectrally similar classes;swath width;cross-modality feature learning;CoSpace;shared latent subspace;hyperspectral-MS correspondences;MS out-of-samples;simulated HS-MS data sets;spectral resolution;hyperspectral-multispectral correspondences;open satellite multispectral imagery;Sentinel-2;Landsat-8;global MS land cover classification;spectral information;University of Houston and Chikusei;common subspace learning","","154","","34","OAPA","20 Mar 2019","","","IEEE","IEEE Journals"
"A Regression-Based High-Pass Modulation Pansharpening Approach","G. Vivone; R. Restaino; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; CNRS, Grenoble INP, GIPSA-Lab, University Grenoble Alpes, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2018","2018","56","2","984","996","Pansharpening usually refers to the fusion of a high spatial resolution panchromatic (PAN) image with a higher spectral resolution but coarser spatial resolution multispectral (MS) image. Owing to the wide applicability of related products, the literature has been populated by many papers proposing several approaches and studies about this issue. Many solutions require a preliminary spectral matching phase wherein the PAN image is matched with the MS bands. In this paper, we propose and properly justify a new approach for performing this step, demonstrating that it yields state-of-the-art performance. The comparison with existing spectral matching procedures is performed by employing four data sets, concerning different kinds of landscapes, acquired by the Pléiades, WorldView-2, and GeoEye-1 sensors.","1558-0644","","10.1109/TGRS.2017.2757508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8068931","Data fusion;high-pass modulation (HPM);least squares estimation;linear regression model;multiresolution analysis (MRA);pansharpening;remote sensing","Spatial resolution;Sensors;Multiresolution analysis;Modulation;Wavelet transforms;Image sensors","image fusion;image resolution;remote sensing","high spatial resolution panchromatic image;PAN image;GeoEye-1 sensors;WorldView-2 sensors;Pléiades sensors;regression-based high-pass modulation pansharpening approach;spectral matching phase;spatial resolution multispectral image;spectral matching procedures","","47","","54","IEEE","16 Oct 2017","","","IEEE","IEEE Journals"
"A Bayesian Procedure for Full-Resolution Quality Assessment of Pansharpened Products","G. Vivone; R. Restaino; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Université Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","20 Jul 2018","2018","56","8","4820","4834","Pansharpening regards the fusion of a high-spatial resolution panchromatic image with a low-spatial resolution multispectral image. One of the most debated topics about pansharpening is related to the quality assessment of fused products. Two main assessment procedures are usually exploited in the literature: the reduced resolution validation and the full-resolution (FR) validation. The former has the advantage to be accurate, but the hypothesis of invariance among scales has to be assumed. The latter overcomes this limitation but paying it with a lower accuracy. In this paper, we will focus on the FR assessment proposing an approach for estimating an overall quality index at FR by using multiscale FR measurements. The problem is recast into the sequential Bayesian framework exploiting a Kalman filter to find its solution. The proposed procedure for quality evaluation has been tested on four real data sets acquired by the Pléiades, the GeoEye-1, the WorldView-3, and the WorldView-4 sensors assessing the quality of 19 pansharpened methods. The proposed approach has demonstrated its superiority with respect to the benchmark consisting of state-of-the-art quality assessment procedures.","1558-0644","","10.1109/TGRS.2018.2839564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8385162","Data fusion;Kalman filter (KF);multiscale analysis;pansharpening;quality assessment;remote sensing","Indexes;Quality assessment;Image resolution;Protocols;Distortion;Bayes methods;Sensors","Bayes methods;geophysical image processing;image fusion;image resolution;Kalman filters;remote sensing","Bayesian procedure;full-resolution quality assessment;pansharpened products;high-spatial resolution panchromatic image;low-spatial resolution multispectral image;debated topics;fused products;reduced resolution validation;full-resolution validation;FR assessment;quality index;multiscale FR measurements;sequential Bayesian framework;quality evaluation;state-of-the-art quality assessment procedures;Pléiades sensor;GeoEye-1 sensor;WorldView-3 sensor;WorldView-4 sensors;Kalman filter","","29","","54","IEEE","14 Jun 2018","","","IEEE","IEEE Journals"
"NMF-DuNet: Nonnegative Matrix Factorization Inspired Deep Unrolling Networks for Hyperspectral and Multispectral Image Fusion","A. Khader; J. Yang; L. Xiao","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Nanjing Skyworth Institute of Information Technology Company, Ltd., Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Jul 2022","2022","15","","5704","5720","The fusion of high-resolution multispectral image (HrMSI) and low-resolution hyperspectral image (LrHSI) has been acknowledged as a promising method for generating a high-resolution hyperspectral image (HrHSI), which is also termed to be an essential part for precise recognition and cataloguing of the underlying materials. In order to improve the fusion of the LrHSI and HrMSI performance, in this article, we propose a novel nonnegative matrix factorization inspired deep unrolling networks (NMF-DuNet) for fusing LrHSI and HrMSI. For this aim, initially, a variational fusion model regularized by nonnegative sparse prior is proposed and then is solved through the gradient descent optimization method and unrolled towards the deep network. The nonnegative coefficient matrices and orthogonal of the proposed transform coefficients constraints are both incorporated into the proposed method. Moreover, the fusion of HrMSI and LrHSI heavily depends on an imaging model that explains the degeneracy of HSI in the spectral and spatial regions. Practically, the imaging model is often unknown. The degradation model is represented implicitly via a proposed network, and both the degradation model and sparse priors are jointly optimized through the training process of the proposed network. Instead of being hand-crafted, all the parameters of NMF-DuNet are learned end-to-end. Compared to the previous state-of-the-art model-based and learning-based fusion approaches, the hardware-friendly proposed NMF-DuNet outperforms both the model-based and learning-based fusion approaches and requires a far smaller number of trainable parameters and storage space while preserving the real-time performance.","2151-1535","","10.1109/JSTARS.2022.3189551","National Natural Science Foundation of China(grant numbers:61871226,61571230,62001226); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200465); Fundamental Research Funds for the Central Universities(grant numbers:30920021134,JSGP202204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9822395","Deep learning;hyperspectral image superresolution;hyperspectral imaging;images fusion;sparse coding (SC)","Hyperspectral imaging;Imaging;Sparse matrices;Deep learning;Computational modeling;Spatial resolution;Tensors","geophysical image processing;gradient methods;image classification;image fusion;image representation;image resolution;learning (artificial intelligence);matrix algebra;matrix decomposition;optimisation;sensor fusion","NMF-DuNet outperforms;learning-based fusion approaches;previous state-of-the-art model-based;imaging model;nonnegative coefficient matrices;deep network;variational fusion model;novel nonnegative matrix factorization;HrMSI performance;high-resolution hyperspectral image;LrHSI;low-resolution hyperspectral image;multispectral image fusion;nonnegative matrix factorization inspired deep unrolling networks","","2","","61","CCBYNCND","8 Jul 2022","","","IEEE","IEEE Journals"
"Alternating Direction Iterative Nonnegative Matrix Factorization Unmixing for Multispectral and Hyperspectral Data Fusion","X. Zhou; Y. Zhang; J. Zhang; S. Shi","Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Sep 2020","2020","13","","5223","5232","Most image fusion algorithms based on hyperspectral unmixing perform worse with the lower spatial resolution of hyperspectral image (HSI) for the reason that the estimated endmembers and abundance deviate from the truth value. Therefore, it is more meaningful to unmix the low spatial resolution hyperspectral image (LRHSI) accurately, which is also helpful to improve the image fusion performance. In order to enhance the spatial resolution of LRHSI, this article proposes an alternating direction iterative nonnegative matrix factorization (ADINMF) based on linear hyperspectral unmixing algorithm. It takes multispectral image as a constraint to improve the spatial resolution of LRHSI. First, we use blind source separation to initialize the endmember and abundance of hyperspectral and multispectral images, respectively. Then, we alternately update the endmembers and abundance in the framework of nonnegative matrix factorization by multiplication iterative algorithm. The updated endmembers and abundance are constrained to each other. We compare the experimental results of simulated dataset and three groups of real datasets. Experimental results show that the proposed method not only accurately extracts the endmembers of LRHSI, but also obtains a significant fusion performance improvement.","2151-1535","","10.1109/JSTARS.2020.3020586","National Natural Science Foundation of China(grant numbers:61871150,61771170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184213","Alternating direction iterative nonnegative matrix factorization (ADINMF);hyperspectral unmixing (HU);low spatial resolution hyperspectral image (LRHSI)","Spatial resolution;Hyperspectral imaging;Image fusion;Convergence;Blind source separation","blind source separation;geophysical image processing;hyperspectral imaging;image fusion;image resolution;iterative methods;matrix decomposition","blind source separation;ADINMF;LRHSI;hyperspectral data fusion;alternating direction iterative nonnegative matrix factorization unmixing;updated endmembers;multiplication iterative algorithm;multispectral image;linear hyperspectral unmixing algorithm;image fusion performance;low spatial resolution hyperspectral image","","5","","33","CCBY","1 Sep 2020","","","IEEE","IEEE Journals"
"Spatial and Spectral Extraction Network With Adaptive Feature Fusion for Pansharpening","K. Zhang; A. Wang; F. Zhang; W. Diao; J. Sun; L. Bruzzone","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Geoscience and Remote Sensing","12 Jul 2022","2022","60","","1","14","Pansharpening methods based on deep neural networks (DNNs) have been attracting great attention due to their powerful representation capabilities. In this article, to combine the feature maps from different subnetworks efficiently, we propose a novel pansharpening method based on a spatial and spectral extraction network (SSE-Net). Different from the other methods based on DNNs that directly concatenate the features from different subnetworks, we design adaptive feature fusion modules (AFFMs) to merge these features according to their information content. First, the spatial and spectral features are extracted by the subnetworks from low spatial resolution multispectral (LR MS) and panchromatic (PAN) images. Then, by fusing the features at different levels, the desired high spatial resolution MS (HR MS) images are generated by the fusion network consisting of AFFMs. In the fusion network, the features from different subnetworks are integrated adaptively, and the redundancy among them is reduced. Moreover, the spectral ratio loss and the gradient loss are defined to ensure the effective learning of spatial and spectral features. The spectral ratio loss captures the nonlinear relationships among the bands in the MS image to reduce the spectral distortions in the fusion result. Extensive experiments were conducted on QuickBird and GeoEye-1 satellite datasets. Visual and numerical results demonstrate that the proposed method produces better fusion results compared with literature techniques. The source code is available at https://github.com/RSMagneto/SSE-Net.","1558-0644","","10.1109/TGRS.2022.3187025","Natural Science Foundation of China(grant numbers:61901246); China Postdoctoral Science Foundation Grant(grant numbers:2019TQ0190,2019M662432); China Scholarship Council(grant numbers:202008370035); Scientific Research Leader Studio of Jinan(grant numbers:2021GXRC081); Ningxia Natural Science Foundation(grant numbers:2021AAC03045); Joint Project for Smart Computing of the Shandong Natural Science Foundation(grant numbers:ZR2020LZH015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810290","Adaptive feature fusion;pansharpening;remote sensing;spatial extraction network;spectral extraction network;spectral ratio loss","Feature extraction;Pansharpening;Data mining;Spatial resolution;Image reconstruction;Degradation;Redundancy","feature extraction;geophysical image processing;geophysical signal processing;image fusion;image resolution;neural nets;remote sensing;sensor fusion;spectral analysis","spatial extraction network;spectral extraction network;deep neural networks;DNNs;powerful representation capabilities;feature maps;different subnetworks;novel pansharpening method;design adaptive feature fusion modules;spatial features;spectral features;desired high spatial resolution MS images;fusion network;spectral ratio loss;MS image;spectral distortions;fusion result","","1","","52","IEEE","29 Jun 2022","","","IEEE","IEEE Journals"
"Building Layout Reconstruction With Transmissive and Reflective Signals","J. Chen; S. Guo; G. Cui; L. Kong","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","29 Aug 2022","2022","60","","1","12","Building layout reconstruction (BLR) is an important topic in the field of through-the-wall imaging. Traditionally, reflective signals are commonly used to generate an accurate building map. However, due to the inherent features of electromagnetic (EM) waves, the reconstructed walls will inevitably suffer from problems such as deviation and cavities. Alternatively, as an extension of computed tomography, the transmissive signals can be exploited for BLR with high efficiency, but its performance degrades seriously when the sampling views are sparse. In this article, to fully combine the superiority of the two types of implementations of BLR, we proposed a hybrid imaging framework by jointly exploiting the reflective and transmissive signals to retrieve the unknown layout. Specifically, first, the time delay of the transmissive signal will be estimated and used to reconstruct the spatial tomographic map. Then, a series of reflected echoes sampled by different routes will be compensated iteratively and used to generate the back-projection image. Finally, we fuse the two images generated by different types of signals using the feature-level detector. Both simulated and experimental results reveal that the proposed imaging framework can yield better performance compared with the image derived from single-type signals.","1558-0644","","10.1109/TGRS.2022.3199270","National Natural Science Foundation of China(grant numbers:61871080,62001091); Changjiang Scholar Program; 111 Project B17008; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858175","Building layout reconstruction (BLR);image fusion;transmissive and reflective signals","Buildings;Image reconstruction;Layout;Radar imaging;Delay effects;Detectors;Antenna measurements","image fusion;image reconstruction;object detection","building layout reconstruction;reflective signals;BLR;through-the-wall imaging;building map;reconstructed walls;hybrid imaging framework;transmissive signals;spatial tomographic map;reflected echoes;single-type signals;backprojection image;image fusion;feature-level detector","","","","35","IEEE","17 Aug 2022","","","IEEE","IEEE Journals"
"Spectral and Spatial Classification of Hyperspectral Images Based on ICA and Reduced Morphological Attribute Profiles","N. Falco; J. A. Benediktsson; L. Bruzzone","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Geoscience and Remote Sensing","11 Aug 2015","2015","53","11","6223","6240","The availability of hyperspectral images with improved spectral and spatial resolutions provides the opportunity to obtain accurate land-cover classification. In this paper, a novel methodology that combines spectral and spatial information for supervised hyperspectral image classification is proposed. A feature reduction strategy based on independent component analysis is the main core of the spectral analysis, where the exploitation of prior information coupled to the evaluation of the reconstruction error assures the identification of the best class-informative subset of independent components. Reduced attribute profiles (APs), which are designed to address well-known issues related to information redundancy that affect the common morphological APs, are then employed for the modeling and fusion of the contextual information. Four real hyperspectral data sets, which are characterized by different spectral and spatial resolutions with a variety of scene typologies (urban, agriculture areas), have been used for assessing the accuracy and generalization capabilities of the proposed methodology. The obtained results demonstrate the classification effectiveness of the proposed approach in all different scene typologies, with respect to other state-of-the-art techniques.","1558-0644","","10.1109/TGRS.2015.2436335","research funds of the University of Iceland; research funds of the University of Trento; European Union's Seventh Framework Program (FP7/2007-2013) Theme Space project North State(grant numbers:606962); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7147815","Dimensionality reduction;hyperspectral images;independent component analysis (ICA);mathematical morphology (MM);reduced attribute profiles (rAPs);remote sensing (RS);supervised classification;Dimensionality reduction;hyperspectral images;independent component analysis (ICA);mathematical morphology (MM);reduced attribute profiles (rAPs);remote sensing (RS);supervised classification","Feature extraction;Hyperspectral imaging;Data mining;Matrix decomposition;Training;Spectral analysis","feature extraction;geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;land cover","hyperspectral image spectral classification;hyperspectral image spatial classification;reduced morphological attribute profiles;spectral resolutions;spatial resolutions;land-cover classification;supervised hyperspectral image classification;feature reduction strategy;independent component analysis;reconstruction error;contextual information fusion;hyperspectral data sets;state-of-the-art techniques","","69","","48","IEEE","2 Jul 2015","","","IEEE","IEEE Journals"
"Classification of Hyperspectral and LiDAR Data Using Coupled CNNs","R. Hang; Z. Li; P. Ghamisi; D. Hong; G. Xia; Q. Liu","Department of Computer Science and Electrical Engineering, University of Missouri-Kansas City, Kansas City, USA; Department of Computer Science and Electrical Engineering, University of Missouri-Kansas City, Kansas City, USA; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Freiberg, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2020","2020","58","7","4939","4950","In this article, we propose an efficient and effective framework to fuse hyperspectral and light detection and ranging (LiDAR) data using two coupled convolutional neural networks (CNNs). One CNN is designed to learn spectral-spatial features from hyperspectral data, and the other one is used to capture the elevation information from LiDAR data. Both of them consist of three convolutional layers, and the last two convolutional layers are coupled together via a parameter-sharing strategy. In the fusion phase, feature-level and decision-level fusion methods are simultaneously used to integrate these heterogeneous features sufficiently. For the feature-level fusion, three different fusion strategies are evaluated, including the concatenation strategy, the maximization strategy, and the summation strategy. For the decision-level fusion, a weighted summation strategy is adopted, where the weights are determined by the classification accuracy of each output. The proposed model is evaluated on an urban data set acquired over Houston, USA, and a rural one captured over Trento, Italy. On the Houston data, our model can achieve a new record overall accuracy (OA) of 96.03%. On the Trento data, it achieves an OA of 99.12%. These results sufficiently certify the effectiveness of our proposed model.","1558-0644","","10.1109/TGRS.2020.2969024","Natural Science Foundation of United States(grant numbers:1747751); Natural Science Foundation of China(grant numbers:61825601,61532009,61906096,61802198); Natural Science Foundation of Jiangsu Province, China(grant numbers:BK20180786,BK20180788,18KJB520032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8985546","Convolutional neural networks (CNNs);decision fusion;feature fusion;hyperspectral data;light detection and ranging (LiDAR) data;parameter sharing","Hyperspectral imaging;Laser radar;Feature extraction;Fuses;Data models","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);optical radar;remote sensing by radar","LiDAR data;coupled CNNs;hyperspectral detection;light detection and ranging;coupled convolutional neural networks;spectral-spatial features;hyperspectral data;elevation information;convolutional layers;parameter-sharing strategy;fusion phase;decision-level fusion methods;heterogeneous features;feature-level fusion;concatenation strategy;maximization strategy;weighted summation strategy;classification accuracy;urban data;Houston data;Trento data","","124","","39","IEEE","6 Feb 2020","","","IEEE","IEEE Journals"
"An Optimization Procedure for Robust Regression-Based Pansharpening","M. Carpentiero; G. Vivone; R. Restaino; P. Addesso; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Institute of Methodologies for Environmental Analysis (IMAA), National Research Council (CNR), Tito, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Jean Kuntzmann Laboratory (LJK), Centre National de la Recherche Scientifique (CNRS), Inria, Grenoble Institute of Technology (Grenoble INP), Université Grenoble Alpes, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","14 Jun 2022","2022","60","","1","16","Model-based approaches to pansharpening still constitute a class of widely employed methods, thanks to their straightforward applicability to many problems, dispensing the user from time-consuming training phases. The injection scheme based on an accurate estimation (exploiting regression) of the relationship between the details contained in the panchromatic (PAN) image and those required for the enhancement of the multispectral (MS) image represents the most updated approach to this problem, being characterized by both theoretical and practical optimality. We elaborated on this scheme by designing a procedure for estimating the key parameters required for the optimal setting of such a regression-based approach. We tested this approach on several datasets acquired by the WorldView satellites comparing the proposed approach with a benchmark consisting of some state-of-the-art pansharpening methods.","1558-0644","","10.1109/TGRS.2022.3179105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785620","Image fusion;pansharpening;pyramidal decomposition;regression-based data fusion;remote sensing","Pansharpening;Estimation;Maximum likelihood estimation;Optimization;Sensors;Laplace equations;Electronic mail","geophysical image processing;geophysical techniques;optimisation;regression analysis;remote sensing","optimization procedure;robust regression-based pansharpening;time-consuming training phases;injection scheme;accurate estimation;panchromatic image;multispectral image;theoretical optimality;practical optimality;optimal setting","","3","","63","IEEE","30 May 2022","","","IEEE","IEEE Journals"
"Fusion of Spectral and Spatial Information for Classification of Hyperspectral Remote-Sensed Imagery by Local Graph","W. Liao; M. Dalla Mura; J. Chanussot; A. Pižurica","Department of Telecommunications and Information Processing, Ghent University—iMinds—Image Processing and Interpretation, Ghent, Belgium; Grenoble Images Parole Signals Automatics Laboratory (GIPSA-Lab), Grenoble Institute of Technology, Saint Martin d’Hères, France; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Department of Telecommunications and Information Processing, Ghent University—iMinds—Image Processing and Interpretation, Ghent, Belgium","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","2","583","594","Hyperspectral imagery contains a wealth of spectral and spatial information that can improve target detection and recognition performance. Conventional feature extraction methods cannot fully exploit both spectral and spatial information. Data fusion by simply stacking different feature sources together does not take into account the differences between feature sources. In this paper, a local graph-based fusion (LGF) method is proposed to couple dimension reduction and feature fusion of the spectral information (i.e., the spectra in the HS image) and the spatial information [extracted by morphological profiles (MPs)]. In the proposed method, the fusion graph is built on the full data by moving a sliding window from the first pixel to the last one. This yields a clear improvement over a previous approach with fusion graph built on randomly selected samples. Experimental results on real hyperspectral images are very encouraging. Compared to the methods using only single feature and stacking all the features together, the proposed LGF method improves the overall classification accuracy on one of the data sets for more than 20% and 5%, respectively.","2151-1535","","10.1109/JSTARS.2015.2498664","Fund for Strategic Basic Research; Agency for Innovation by Science and Technology in Flanders (SBO-IWT); Fund for Scientific Research in Flanders (FWO)(grant numbers:G037115N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7342891","Classification;data fusion;graph-based;hyperspectral image;remote sensing;Classification;data fusion;graph-based;hyperspectral image;remote sensing","Hyperspectral imaging;Stacking;Feature extraction;Data integration;Kernel","geophysical image processing;geophysical techniques;graph theory;hyperspectral imaging;image classification;image fusion;image recognition;object detection","hyperspectral remote-sensed imagery classification;local graph;spatial information fusion;spectral information fusion;target detection;target recognition;local graph-based fusion method;LGF;couple dimension reduction;HS image;fusion graph;morphological profile","","26","","35","IEEE","1 Dec 2015","","","IEEE","IEEE Journals"
"A Convex Formulation for Hyperspectral Image Superresolution via Subspace-Based Regularization","M. Simões; J. Bioucas‐Dias; L. B. Almeida; J. Chanussot","Instituto de Telecomunicações, Instituto Superior Técnico, Université de Grenoble, Saint-Martin-d'Hères, France; Instituto de Telecomunicações, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Instituto de Telecomunicações, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Grenoble Images Parole Signal Automatique (GIPSA-Lab), University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","5 Feb 2015","2015","53","6","3373","3388","Hyperspectral remote sensing images (HSIs) usually have high spectral resolution and low spatial resolution. Conversely, multispectral images (MSIs) usually have low spectral and high spatial resolutions. The problem of inferring images that combine the high spectral and high spatial resolutions of HSIs and MSIs, respectively, is a data fusion problem that has been the focus of recent active research due to the increasing availability of HSIs and MSIs retrieved from the same geographical area. We formulate this problem as the minimization of a convex objective function containing two quadratic data-fitting terms and an edge-preserving regularizer. The data-fitting terms account for blur, different resolutions, and additive noise. The regularizer, a form of vector total variation, promotes piecewise-smooth solutions with discontinuities aligned across the hyperspectral bands. The downsampling operator accounting for the different spatial resolutions, the nonquadratic and nonsmooth nature of the regularizer, and the very large size of the HSI to be estimated lead to a hard optimization problem. We deal with these difficulties by exploiting the fact that HSIs generally “live” in a low-dimensional subspace and by tailoring the split augmented Lagrangian shrinkage algorithm (SALSA), which is an instance of the alternating direction method of multipliers (ADMM), to this optimization problem, by means of a convenient variable splitting. The spatial blur and the spectral linear operators linked, respectively, with the HSI and MSI acquisition processes are also estimated, and we obtain an effective algorithm that outperforms the state of the art, as illustrated in a series of experiments with simulated and real-life data.","1558-0644","","10.1109/TGRS.2014.2375320","Fundação para a Ciência e Tecnologia, Portuguese Ministry of Science and Higher Education(grant numbers:PEst-OE/EEI/0008/2013,PTDC/EEI-PRO/1470/2012,SFRH/BD/87693/2012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000523","Alternating direction method of multipliers (ADMM);convex nonsmooth optimization;data fusion;hyperspectral imaging;superresolution;vector total variation (VTV);Alternating direction method of multipliers (ADMM);convex nonsmooth optimization;data fusion;hyperspectral imaging;superresolution;vector total variation (VTV)","Spatial resolution;Hyperspectral imaging;Optimization;Sensors;Vectors","geophysical image processing;hyperspectral imaging;image fusion;remote sensing","convex formulation;hyperspectral image superresolution;subspace-based regularization;hyperspectral remote sensing images;multispectral images;low spectral resolutions;high spatial resolutions;geographical area;convex objective function;quadratic data-fitting terms;edge-preserving regularizer;data-fitting terms;vector total variation;piecewise-smooth solutions;hyperspectral bands;low-dimensional subspace;split augmented Lagrangian shrink-age algorithm;SALSA;alternating direction method of multipliers;ADMM;optimization problem;spatial blur;convenient variable splitting;spectral linear operators;HSI acquisition process;MSI acquisition process","","426","","58","IEEE","31 Dec 2014","","","IEEE","IEEE Journals"
"Fusion of Dual Spatial Information for Hyperspectral Image Classification","P. Duan; P. Ghamisi; X. Kang; B. Rasti; S. Li; R. Gloaguen","Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Changsha, China; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Changsha, China; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2021","2021","59","9","7726","7738","The inclusion of spatial information into spectral classifiers for fine-resolution hyperspectral imagery has led to significant improvements in terms of classification performance. The task of spectral-spatial hyperspectral image (HSI) classification has remained challenging because of high intraclass spectrum variability and low interclass spectral variability. This fact has made the extraction of spatial information highly active. In this work, a novel HSI classification framework using the fusion of dual spatial information is proposed, in which the dual spatial information is built by both exploiting pre-processing feature extraction and post-processing spatial optimization. In the feature extraction stage, an adaptive texture smoothing method is proposed to construct the structural profile (SP), which makes it possible to precisely extract discriminative features from HSIs. The SP extraction method is used here for the first time in the remote sensing community. Then, the extracted SP is fed into a spectral classifier. In the spatial optimization stage, a pixel-level classifier is used to obtain the class probability followed by an extended random walker-based spatial optimization technique. Finally, a decision fusion rule is utilized to fuse the class probabilities obtained by the two different stages. Experiments performed on three data sets from different scenes illustrate that the proposed method can outperform other state-of-the-art classification techniques. In addition, the proposed feature extraction method, i.e., SP, can effectively improve the discrimination between different land covers.","1558-0644","","10.1109/TGRS.2020.3031928","Major Program of the National Natural Science Foundation of China(grant numbers:61890962); National Natural Science Foundation of China(grant numbers:61871179); National Natural Science Fund of China for International Cooperation and Exchanges(grant numbers:61520106001); Fund of Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province(grant numbers:2018TP1013); Fund of Hunan Province for Science and Technology Plan Project(grant numbers:2017RS3024); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9256984","Decision fusion;dual spatial information;feature extraction;hyperspectral classification;structural profile (SP)","Support vector machines;Smoothing methods;Fuses;Imaging;Feature extraction;Minerals;Task analysis","feature extraction;hyperspectral imaging;image classification;image fusion;image resolution;image texture;land cover;optimisation;probability;random processes;remote sensing","spectral-spatial hyperspectral image classification;HSI classification;post-processing spatial optimization;feature extraction;spectral classifier;spatial optimization;extended random walker-based spatial optimization technique;discriminative feature extraction;fine-resolution hyperspectral imagery;classification performance;interclass spectral variability;intraclass spectrum variability;dual spatial information fusion;adaptive texture smoothing method;structural profile;remote sensing;pixel-level classifier;decision fusion rule;class probabilities;land cover","","40","","57","IEEE","11 Nov 2020","","","IEEE","IEEE Journals"
"Hypersharpening by Joint-Criterion Nonnegative Matrix Factorization","M. S. Karoui; Y. Deville; F. Z. Benhalouche; I. Boukerch","Institut de Recherche en Astrophysique et Planétologie, Université de Toulouse, Toulouse, France; Institut de Recherche en Astrophysique et Planétologie, Université de Toulouse, Toulouse, France; Institut de Recherche en Astrophysique et Planétologie, Université de Toulouse, Toulouse, France; Centre des Techniques Spatiales, Arzew, Algeria","IEEE Transactions on Geoscience and Remote Sensing","1 Mar 2017","2017","55","3","1660","1670","Hypersharpening aims at combining an observable low-spatial resolution hyperspectral image with a high-spatial resolution remote sensing image, in particular a multispectral one, to generate an unobservable image with the high spectral resolution of the former and the high spatial resolution of the latter. In this paper, two such new fusion methods are proposed. These methods, related to linear spectral unmixing techniques, and based on nonnegative matrix factorization (NMF), optimize a new joint criterion and extend the recently proposed joint NMF (JNMF) method. The first approach, called gradient-based joint-criterion NMF (Grd-JCNMF), is a gradient-based method. The second one, called multiplicative JCNMF (Mult-JCNMF), uses new designed multiplicative update rules. These two JCNMF approaches are applied to synthetic and semireal data, and their effectiveness, in spatial and spectral domains, is evaluated with commonly used performance criteria. Experimental results show that the proposed JCNMF methods yield sharpened hyperspectral data with good spectral and spatial fidelities. The obtained results are compared with the performance of two NMF-based methods and one approach based on a sparse representation. These results show that the proposed methods significantly outperform the well-known coupled NMF sharpening method for most performance figures. Also, the proposed Mult-JCNMF method provides the results that are similar to those obtained by JNMF, with a lower computational cost. Compared with the tested sparse-representation-based approach, the proposed methods give better results. Moreover, the proposed Grd-JCNMF method considerably surpasses all other tested methods.","1558-0644","","10.1109/TGRS.2016.2628889","French ANR project on “Hyperspectral Imagery for Environmental Urban Planning”(grant numbers:HYEP ANR 14-CE22-0016-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7778148","Data fusion;hypersharpening;hyperspectral/multispectral imaging;linear spectral unmixing (LSU);nonnegative matrix factorization (NMF);pansharpening","Hyperspectral imaging;Spatial resolution;Algorithm design and analysis;Sparse matrices","geophysical image processing;gradient methods;image fusion;image resolution;matrix decomposition;remote sensing","hypersharpening;low-spatial resolution hyperspectral image;high-spatial resolution remote sensing image;high spectral resolution;linear spectral unmixing techniques;joint nonnegative matrix factorization method;gradient-based joint-criterion nonnegative matrix factorization;multiplicative update;spatial domain;spectral domain;computational cost;sparse-representation-based approach","","31","","44","IEEE","8 Dec 2016","","","IEEE","IEEE Journals"
"Unsupervised and Unregistered Hyperspectral Image Super-Resolution With Mutual Dirichlet-Net","Y. Qu; H. Qi; C. Kwan; N. Yokoya; J. Chanussot","Department of Electrical Engineering and Computer Science, Advanced Imaging and Collaborative Information Processing Group, The University of Tennessee, Knoxville, TN, USA; Department of Electrical Engineering and Computer Science, Advanced Imaging and Collaborative Information Processing Group, The University of Tennessee, Knoxville, TN, USA; Applied Research LLC, Rockville, MD, USA; Geoinformatics Unit, RIKEN Center for Advanced Intelligence Project (AIP), Tokyo, Japan; Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","4 Jan 2022","2022","60","","1","18","Hyperspectral images (HSIs) provide rich spectral information that has contributed to the successful performance improvement of numerous computer vision and remote sensing tasks. However, it can only be achieved at the expense of images’ spatial resolution. HSI super-resolution (HSI-SR), thus, addresses this problem by fusing low-resolution (LR) HSI with the multispectral image (MSI) carrying much higher spatial resolution (HR). Existing HSI-SR approaches require the LR HSI and HR MSI to be well registered, and the reconstruction accuracy of the HR HSI relies heavily on the registration accuracy of different modalities. In this article, we propose an unregistered and unsupervised mutual Dirichlet-Net ( $u^{2}$ -MDN) to exploit the uncharted problem domain of HSI-SR without the requirement of multimodality registration. The success of this endeavor would largely facilitate the deployment of HSI-SR since registration requirement is difficult to satisfy in real-world sensing devices. The novelty of this work is threefold. First, to stabilize the fusion procedure of two unregistered modalities, the network is designed to extract spatial information and spectral information of two modalities with different dimensions through a shared encoder–decoder structure. Second, the mutual information (MI) is further adopted to capture the nonlinear statistical dependencies between the representations from two modalities (carrying spatial information) and their raw inputs. By maximizing the MI, spatial correlations between different modalities can be well characterized to further reduce the spectral distortion. We assume that the representations follow a similar Dirichlet distribution for their inherent sum-to-one and nonnegative properties. Third, a collaborative  $l_{2,1}$ -norm is employed as the reconstruction error instead of the more common  $l_{2}$ -norm to better preserve the spectral information. Extensive experimental results demonstrate the superior performance of  $u^{2}$ -MDN as compared to the state of the art.","1558-0644","","10.1109/TGRS.2021.3079518","National Aeronautics and Space Administration (NASA)(grant numbers:NNX12CB05C,NNX16CP38P); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442804","Hyperspectral image (HSI);mutual information (MI);super-resolution (SR);unregistered;unsupervised deep learning","Spatial resolution;Image reconstruction;Data mining;Bayes methods;Superresolution;Tensors;Optimization","computer vision;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image registration;image representation;image resolution;remote sensing;statistical analysis","registration requirement;mutual information;spatial correlations;computer vision;remote sensing tasks;HSI superresolution;low-resolution HSI;multispectral image;LR HSI;unregistered Dirichlet-Net;unsupervised mutual Dirichlet-Net;unregistered hyperspectral image superresolution;unsupervised hyperspectral image superresolution;image spatial resolution;MSI;HR HSI reconstruction accuracy;fusion procedure;shared encoder-decoder structure;nonlinear statistical dependencies;Dirichlet distribution;sum-to-one properties;collaborative l2,1-norm;u2-MDN","","12","","85","IEEE","27 May 2021","","","IEEE","IEEE Journals"
"Effective Pan-Sharpening With Transformer and Invertible Neural Network","M. Zhou; X. Fu; J. Huang; F. Zhao; A. Liu; R. Wang","Hefei Institute of Physical Science, Chinese Academy of Sciences, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; University of Science and Technology of China, Hefei, China; Intelligent Agriculture Engineering Laboratory of Anhui Province, Institute of Intelligent Machines, and Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, China","IEEE Transactions on Geoscience and Remote Sensing","3 Mar 2022","2022","60","","1","15","In remote sensing imaging systems, pan-sharpening is an important technique to obtain high-resolution multispectral images from a high-resolution panchromatic image and its corresponding low-resolution multispectral image. Due to the powerful learning capability of convolution neural networks (CNNs), CNN-based methods have dominated this field. However, due to the limitation of the convolution operator, long-range spatial features are often not accurately obtained, thus limiting the overall performance. To this end, we propose a novel and effective method by exploiting a customized transformer architecture and information-lossless invertible neural module for long-range dependencies modeling and effective feature fusion in this article. Specifically, the customized transformer formulates the panchromatic (PAN) and multispectral (MS) features as queries and keys to encourage joint feature learning across two modalities, while the designed invertible neural module enables effective feature fusion to generate the expected pan-sharpened results. To the best of our knowledge, this is the first attempt to introduce a transformer and a invertible neural network into the pan-sharpening field. Extensive experiments over different kinds of satellite datasets demonstrate that our method outperforms state-of-the-art algorithms both visually and quantitatively with fewer parameters and flops. Furthermore, the ablation experiments also prove the effectiveness of the proposed customized long-range transformer and effective invertible neural feature fusion module for pan-sharpening.","1558-0644","","10.1109/TGRS.2021.3137967","National Natural Science Foundation of China (NSFC)(grant numbers:61901433); USTC Research Funds of the Double First-Class Initiative(grant numbers:YD2100002003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9662053","Invertible neural network;pan-sharpening;transformer","Transformers;Feature extraction;Neural networks;Convolution;Computer architecture;Task analysis;Spatial resolution","feature extraction;geophysical image processing;image classification;image fusion;image processing;image resolution;learning (artificial intelligence);neural nets;remote sensing","convolution operator;long-range spatial features;customized transformer architecture;long-range dependencies;effective feature fusion;joint feature;designed invertible neural module;expected pan-sharpened results;invertible neural network;pan-sharpening field;customized long-range transformer;effective invertible neural feature fusion module;effective pan-sharpening;remote sensing imaging systems;high-resolution multispectral images;high-resolution panchromatic image;low-resolution multispectral image;powerful learning capability;convolution neural networks;CNN-based methods","","12","","70","IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"MSACon: Mining Spatial Attention-Based Contextual Information for Road Extraction","Y. Xu; H. Chen; C. Du; J. Li","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","23 Dec 2021","2022","60","","1","17","With the boost of deep learning methods, road extraction has been widely used in city planning and autonomous driving. However, it is very challenging to extract roads around the thorny occlusion areas, even in high-resolution remote sensing images. Existing approaches regard road extraction as an isolated binary segmentation task and ignore the surroundings’ contextual information in the optical image itself, especially the potential dependence implied between roads and buildings. To address the occlusion problem, we proposed a spatial attention-based road extraction neural network using contextual relation between roads and buildings named MSACon to extract the roads more precisely. First, we employed an existing building extraction method to predict buildings in the optical images. Second, we calculated the signed distance map (SDM) based on the building extraction results (which may be inaccurate) as ambiguous auxiliary information to infer the optical images’ potential roads. Due to the color, lines, and texture between the optical images and the SDM are distinct, we then designed the two-branch encoder to extract features and integrated the cross-domain features into the road decoder by a spatial attention-based fusion mechanism. Experiments demonstrate that the proposed method achieves superior performance than other state-of-the-art approaches even with ambiguous auxiliary information. Furthermore, MSACon shows obvious advantages in finding inconspicuous roads in the optical images and eliminating noisy roads, especially when dealing with areas where buildings are located along the roads.","1558-0644","","10.1109/TGRS.2021.3073923","National Natural Science Foundation of China(grant numbers:61806211,41971362,U19A2058); Natural Science Foundation of Hunan Province China(grant numbers:2020JJ4103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419675","Contextual information;road extraction;roads and buildings;signed distance map (SDM);spatial attention mechanism","Roads;Data mining;Buildings;Feature extraction;Optical imaging;Optical sensors;Image segmentation","data mining;deep learning (artificial intelligence);feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;image segmentation;image texture;optical images;remote sensing;roads","optical image;spatial attention-based road extraction neural network;MSACon;ambiguous auxiliary information;road decoder;spatial attention-based fusion mechanism;mining spatial attention-based contextual information;high-resolution remote sensing images;noisy roads;building extraction;deep learning;isolated binary segmentation;contextual relation;signed distance map;SDM;image color;image lines;image texture;two-branch encoder;feature extraction;cross-domain features","","8","","73","IEEE","29 Apr 2021","","","IEEE","IEEE Journals"
"A Dual-Branch Detail Extraction Network for Hyperspectral Pansharpening","J. Qu; S. Hou; W. Dong; S. Xiao; Q. Du; Y. Li","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; Beijing Electronic Science and Technology Institute, Beijing, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","17 Feb 2022","2022","60","","1","13","Hyperspectral (HS) pansharpening aims at creating a high-resolution hyperspectral (HR-HS) image by integrating a high spatial resolution panchromatic (HR-PAN) image with a low-resolution hyperspectral (LR-HS) image. It is an important preprocessing procedure in many remote sensing tasks. Most of the existing pansharpening methods train a specific convolutional neural network (CNN) model for each type of dataset with the same number of spectral bands. The main contribution of this study is to propose a new dual-branch detail extraction pansharpening network (called DBDENet) that can sharpen HS images with any number of spectral bands using a single pre-trained model by fine-tuning the parameters of a small module in the network. Specifically, DBDENet extracts spatial details from LR-HS and HR-PAN images by two bidirectional branches of the dual-branch detail extraction network level by level. For each level, the spatial details captured from the HR-PAN and those of the LR-HS images are fused by a spatial cross attention fusion module (SCAFM). The spatial details fused by the last SCAFM module are injected into the upsampled HS image to obtain an HR-HS image. Experimental results prove to show the proposed DBDENet is superior to other widely accepted state-of-the-art methods in terms of objective indicators and visual appearance.","1558-0644","","10.1109/TGRS.2021.3130420","National Defense Pre-Research Foundation; Higher Education Discipline Innovation Project(grant numbers:B08038); National Natural Science Foundation of China(grant numbers:62101414); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2021JQ-194,2021JQ-197); Fundamental Research Funds for the Central Universities(grant numbers:XJS210108,XJS210104); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110856); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9625980","Detail extraction;dual-branch;fusion;hyperspectral (HS) pansharpening","Pansharpening;Feature extraction;Convolutional neural networks;Bayes methods;Superresolution;Spatial resolution;Hyperspectral imaging","convolutional neural nets;geophysical image processing;image fusion;image representation;image resolution;learning (artificial intelligence);remote sensing","hyperspectral pansharpening;high-resolution hyperspectral;high spatial resolution panchromatic image;low-resolution hyperspectral image;remote sensing tasks;spectral bands;dual-branch detail extraction pansharpening network;single pretrained model;DBDENet;spatial details;HR-PAN images;bidirectional branches;dual-branch detail extraction network level;LR-HS images;spatial cross attention fusion module;upsampled HS image;HR-HS image;convolutional neural network model","","5","","45","IEEE","23 Nov 2021","","","IEEE","IEEE Journals"
"Unsupervised Spatial-Spectral CNN-Based Feature Learning for Hyperspectral Image Classification","S. Zhang; M. Xu; J. Zhou; S. Jia","Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of Ministry of Natural Resources, Shenzhen University, Shenzhen, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of Ministry of Natural Resources, Shenzhen University, Shenzhen, China; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of Ministry of Natural Resources, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","31 Mar 2022","2022","60","","1","17","The rapid development of remote sensing sensors makes the acquisition, analysis, and application of hyperspectral images (HSIs) more and more extensive. However, the limited sample sets, high-dimensional features, highly correlated bands, and mixing spectral information make the classification of HSIs a great challenge. In this article, an unsupervised multiscale and diverse feature learning (UMsDFL) method is proposed for HSI classification, which deeply considers the spatial–spectral features via convolutional neural networks (CNNs). Specifically, after employing the simple noniterative clustering (SNIC) algorithm with the heuristic calculation of superpixel size, the HSIs are segmented into superpixels for feature learning. The unsupervised network is designed with the convolutional encoder and decoder, the additional clustering branch, and the multilayer feature fusion to enhance the distinguishability of feature learning and the reusability of feature maps. Then, the spatial relationships and object attributes in large- and small-scale contexts are learned collaboratively through the unsupervised network to utilize the complementary multiscale characteristics. Moreover, the diverse features of hyperspectral information and nonsubsampled contourlet transform (NSCT) textures are learned simultaneously via the unsupervised network to alleviate the insufficiency of geometric representation. Finally, the random forest (RF) is adopted as the comprehensive classifier for land cover mapping based on the UMsDFL, and superpixel regularization is adopted to optimize the classification results. A series of experiments are performed on three real-world HSI datasets to demonstrate the effectiveness of our UMsDFL approach. The experimental results show that the proposed UMsDFL can achieve the overall accuracy of 79.23%, 96.49%, and 77.26% for Houston, Pavia, and Dioni datasets, respectively, when there are only five samples per class for training.","1558-0644","","10.1109/TGRS.2022.3153673","National Natural Science Foundation of China(grant numbers:41971300,61901278); Key Project of the Department of Education of Guangdong Province(grant numbers:2020ZDZX3045); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011413); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718213","Convolutional neural network (CNN);feature fusion;hyperspectral image (HSI);superpixel segmentation;unsupervised feature learning","Representation learning;Feature extraction;Hyperspectral imaging;Decoding;Convolutional neural networks;Training;Image segmentation","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;image segmentation;land cover;pattern clustering;remote sensing;transforms;unsupervised learning","spatial-spectral CNN-based feature;hyperspectral image classification;remote sensing sensors;hyperspectral images;HSIs;high-dimensional features;spectral information;UMsDFL;HSI classification;spatial-spectral features;convolutional neural networks;noniterative clustering algorithm;superpixel size;unsupervised network;convolutional encoder;decoder;clustering branch;multilayer feature fusion;feature maps;spatial relationships;object attributes;complementary multiscale characteristics;diverse features;hyperspectral information;unsupervised spatial-spectral CNN;random forest;land cover mapping;feature learning;simple noniterative clustering;nonsubsampled contourlet transform texture;superpixel regularization","","5","","63","IEEE","22 Feb 2022","","","IEEE","IEEE Journals"
"Self-Distillation Feature Learning Network for Optical and SAR Image Registration","D. Quan; H. Wei; S. Wang; R. Lei; B. Duan; Y. Li; B. Hou; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; School of Telecommunications, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","25 May 2022","2022","60","","1","18","Optical and synthetic aperture radar (SAR) image registration is important for multimodal remote sensing image information fusion. Recently, deep matching networks have shown better performances than traditional methods of image matching. However, due to significant differences between optical and SAR images, the performances of existing deep learning methods still need to be further improved. This article proposes a self-distillation feature learning network (SDNet) for optical and SAR image registration, improving performance from network structure and network optimization. First, we explore the impact of different weight-sharing strategies on optical and SAR image matching. Then, we design a partially unshared feature learning network for multimodal image feature learning. It has fewer parameters than the fully unshared network and has more flexibility than the fully shared network. In addition, the limited binary supervised information (matching or nonmatching) is insufficient to train the deep matching networks for optical-SAR image registration. Thus, we propose a self-distillation feature learning method to exploit more similarity information for deep network optimization enhancement, such as the similarity ordering between a series of nonmatching patch pairs. The exploited rich similarity information will significantly enhance network training and improve matching accuracy. Finally, existing deep learning methods brute-force make the matching features of the optical and SAR image patches similar, which will lead to the loss of discriminative information and degeneration of the matching performances. Thus, we build an auxiliary task reconstruction learning to optimize the feature learning network to keep more discriminative information. Extensive experiments demonstrate the effectiveness of our proposed method on multimodal image registration.","1558-0644","","10.1109/TGRS.2022.3173476","Key Research and Development Program of Shaanxi(grant numbers:2021ZDLGY01-06,2022ZDLGY0112); National Key Research and Development Program of China(grant numbers:2021ZD0110404); National Natural Science Foundation of China(grant numbers:62171347); Key Scientific Technological Innovation Research Project by Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770793","Deep matching;image registration;multimodal image;optical image;self-distillation learning;synthetic aperture radar (SAR) image","Feature extraction;Optical imaging;Optical sensors;Image registration;Radar polarimetry;Adaptive optics;Representation learning","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image matching;image registration;learning (artificial intelligence);radar imaging;remote sensing;synthetic aperture radar","deep network optimization enhancement;exploited rich similarity information;network training;matching accuracy;deep learning methods brute-force;matching features;optical SAR image patches;discriminative information;matching performances;multimodal image registration;self-distillation feature learning network;synthetic aperture radar image registration;multimodal remote sensing image information fusion;deep matching networks;SAR images;network structure;different weight-sharing strategies;SAR image matching;partially unshared feature learning network;multimodal image feature;fully unshared network;fully shared network;optical-SAR image registration","","3","","48","IEEE","9 May 2022","","","IEEE","IEEE Journals"
"Fusion of Panchromatic and Multispectral Images via Coupled Sparse Non-Negative Matrix Factorization","K. Zhang; M. Wang; S. Yang; Y. Xing; R. Qu","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","12","5740","5747","In this paper, we construct a new coupled sparse non-negative matrix factorization (CSNMF) model for the fusion of panchromatic (PAN) and multispectral (MS) images. Two CSNMFs are developed for a joint sparse representation of MS and PAN images. Moreover, a sequential iterative algorithm is proposed to simultaneously find the solution to CSNMF. Because learned dictionaries can reveal the latent structure of images in spatial and spectral domains, the fused high-resolution MS images can be calculated by multiplying the dictionary of PAN image and the sparse coefficients of MS images. Some experiments are taken on simulated and real QuickBird data, and the results show that CSNMF outperforms its counterparts in both visual quality and numerical guidelines.","2151-1535","","10.1109/JSTARS.2015.2475754","National Basic Research Program of China (973 Program)(grant numbers:2013CB329402); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438103,91438201); Central Universities(grant numbers:BDY021429); Huawei Innovation Research Program, the Kunshan Innovation Institute of Xidian University; National Natural Science Foundation of China(grant numbers:61072108,61173090); Foreign Scholars in University Research and Teaching Programs(grant numbers:B07048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560598","Coupled sparse non-negative matrix factorization (CSNMF);L  1/2 -norm regularization;sparse representation","Dictionaries;Spatial resolution;Matrix decomposition;Sparse matrices;Transforms;Optimization;Remote sensing","image fusion;iterative methods;matrix decomposition","panchromatic image fusion;multispectral image fusion;coupled sparse nonnegative matrix factorization;CSNMF model;iterative algorithm;learned dictionary;multispectral image sparse coefficient;QuickBird data;visual quality;numerical guideline","","30","","32","IEEE","5 Sep 2016","","","IEEE","IEEE Journals"
"Self-Supervised Learning for Invariant Representations From Multi-Spectral and SAR Images","P. Jain; B. Schoen-Phelan; R. Ross","School of Computer Science, Technological University Dublin, Dublin, Ireland; School of Computer Science, Technological University Dublin, Dublin, Ireland; School of Computer Science, Technological University Dublin, Dublin, Ireland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Sep 2022","2022","15","","7797","7808","Self-supervised learning (SSL) has become the new state of the art in several domain classification and segmentation tasks. One popular category of SSL are distillation networks, such as Bootstrap Your Own Latent (BYOL). This work proposes RS-BYOL, which builds on BYOL in the remote sensing (RS) domain where data are nontrivially different from natural RGB images. Since multispectral (MS) and synthetic aperture radar (SAR) sensors provide varied spectral and spatial resolution information, we utilize them as an implicit augmentation to learn invariant feature embeddings. In order to learn RS-based invariant features with SSL, we trained RS-BYOL in two ways, i.e., single channel feature learning and three channel feature learning. This work explores the usefulness of single channel feature learning from random 10 MS bands of 10–20 m resolution and VV-VH of SAR bands compared to the common notion of using three or more bands. In our linear probing evaluation, these single channel features reached a 0.92 F1 score on the EuroSAT classification task and 59.6 mIoU on the IEEE Data Fusion Contest segmentation task for certain single bands. We also compare our results with ImageNet weights and show that the RS-based SSL model outperforms the supervised ImageNet-based model. We further explore the usefulness of multimodal data compared to single modality data, and it is shown that utilizing MS and SAR data allows better invariant representations to be learnt than utilizing only MS data.","2151-1535","","10.1109/JSTARS.2022.3204888","Science Foundation Ireland(grant numbers:13/RC/2106_P2); ADAPT SFI Research Centre at Technological University Dublin ADAPT; SFI Research Centre for AI-Driven Digital Content Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880533","Optical-synthetic aperture radar (SAR) fusion;satellite images;self-supervised learning (SSL);unsupervised learning","Synthetic aperture radar;Task analysis;Optical sensors;Optical imaging;Representation learning;Remote sensing;Satellites","feature extraction;geophysical image processing;image classification;image fusion;image resolution;image segmentation;radar imaging;remote sensing by radar;supervised learning;synthetic aperture radar","segmentation tasks;RS-BYOL;remote sensing domain;natural RGB images;synthetic aperture radar;spectral resolution information;spatial resolution information;invariant feature embeddings;RS-based invariant features;single channel feature learning;random 10 MS bands;SAR bands;single channel features;single bands;RS-based SSL model;supervised ImageNet-based model;single modality data;invariant representations;MS data;multispectral;SAR images;self-supervised learning;domain classification;IEEE data fusion contest segmentation task;size 10.0 m to 20.0 m","","3","","70","CCBY","7 Sep 2022","","","IEEE","IEEE Journals"
"Fusion of Spectral–Spatial Classifiers for Hyperspectral Image Classification","S. Zhong; S. Chen; C. -I. Chang; Y. Zhang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Electrical Engineering, Zhejiang University, Hangzhou, China; Department of Computer Science and Information Management, Providence University, Taichung, Taiwan; Department of Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","5008","5027","A spectral-spatial (SS) hyperspectral classifier generally implements a spectral classifier (SC) followed by a spatial filter (SF) for classification. This article develops a new approach to fusing multiple SC-SF classifiers for hyperspectral image classification (HSIC) as to improve classification performance. To accomplish this goal an iterative process is particularly designed to fuse the spatial-filtered classification maps (SFMaps) produced by each of SC-SF classifiers into one single SFMap via maximum a posteriori (MAP) criterion. Such fused SFMaps are then fed back and added to the current data cube to create a new data set for next round SC-SF classifier fusion. The same process is repeated iteratively until it satisfies an automatic stopping rule. To further fuse more than two SS methods, two approaches are also developed, called simultaneous multiple SC-SF fusion (SMSSF) method and progressive multiple SC-SF fusion (PMSSF) method. Experimental results demonstrate that fusing multiple SC-SF classifiers can indeed perform better than using an individual single SC-SF classifier alone without fusion.","1558-0644","","10.1109/TGRS.2020.3024258","National Natural Science Foundation of China(grant numbers:61973162); Fundamental Research Funds for Central Universities(grant numbers:3132019341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207854","Constrained energy minimization (CEM);edge-preserving filtering (EPF);hyperspectral image classification (HSIC);progressive multiple SS fusion (PMSSF);simultaneous multiple SC-SF fusion (SMSSF);spatial filtered classification map (SFMap);spectral classifier-spatial filter (SC-SF);spectral–spatial (SS)","Hyperspectral imaging;Fuses;Support vector machines;Data mining;Signal processing algorithms;Computer science","geophysical image processing;hyperspectral imaging;image classification;image fusion;iterative methods;maximum likelihood estimation;spatial filters","hyperspectral image classification;spectral-spatial hyperspectral classifier;spectral classifier;spatial filter;iterative process;spatial-filtered classification maps;maximum a posteriori criterion;SC-SF classifier fusion;spectral-spatial classifiers;progressive multiple SC-SF fusion;simultaneous multiple SC-SF fusion;SFMaps;HSIC;MAP;data cube;automatic stopping rule;SMSSF;PMSSF","","9","","50","IEEE","28 Sep 2020","","","IEEE","IEEE Journals"
"Sequential Band Fusion for Hyperspectral Target Detection","F. Li; M. Song","Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China; Center for Hyperspectral Imaging in Remote Sensing (CHIRS), Information and Technology College, Dalian Maritime University, Dalian, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","24","Due to the enormous data and redundant information, how to process hyperspectral images reasonably and efficiently has become a research focus. This article proposes a sequential band fusion (SBF) approach for hyperspectral target detection and gives a detailed derivation of the fusion theory. Then four fusion algorithms—SBF based on band sequence (SBF-BSQ), SBF driven by an initial band (SBF-IBD), SBF based on band priority (SBF-BP), and SBF based on band selection (SBF-BS)—are introduced for application. Experimental results prove that the method proposed in this article can not only effectively improve the efficiency of target detection but also provide the trend of the detection value during the fusion process. Band fusion breaks through the limitation of band selection in the application and provides a new processing method for hyperspectral data.","1558-0644","","10.1109/TGRS.2021.3118808","National Natural Science Foundation of China(grant numbers:61971082,61890964); Fundamental Research Funds for the Central Universities(grant numbers:3132019341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580549","Band fusion;band sequential;sequential band fusion (SBF);target detection","Hyperspectral imaging;Object detection;Real-time systems;Fuses;Market research;Data processing;Detectors","cartography;data fusion;feature extraction;geophysical image processing;hyperspectral imaging;image fusion;object detection","hyperspectral target detection;hyperspectral images;sequential band fusion approach;SBF;band priority;SBF-BP;band selection;SBF-BS;detection value;SBF-BSQ;SBF-IBD","","1","","37","IEEE","19 Oct 2021","","","IEEE","IEEE Journals"
"A Complementary Spectral-Spatial Method for Hyperspectral Image Classification","L. Shi; C. Li; T. Li; Y. Peng","State Key Laboratory of High Performance Computing, Changsha, China; State Key Laboratory of High Performance Computing, Changsha, China; Beijing Institute for Advanced Study and the College of Advanced Interdisciplinary Studies, National University of Defense Technology, Changsha, China; State Key Laboratory of High Performance Computing, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","16 Jun 2022","2022","60","","1","17","In hyperspectral image (HSI) classification, using spatial information as a supplement to spectral information is an effective way to improve classification accuracy. In this article, a novel robust complementary method using spectral-spatial information is proposed to reduce the information loss in feature extraction, thus improving the classification effect. In short, two complementary feature extraction stages are used to get the probability maps for decision fusion. In the stage of pre-processing feature extraction, we propose an adaptive cubic total variational smoothing method (ACTVSP), which is first proposed and applied in the remote sensing research field, to obtain the first-stage probability map. At the same time, we utilize edge-preserving filtering in the post-processing stage and obtain the second probability map by means of pixel-level classifier. Finally, the probability-like maps obtained in the above two stages are integrated by decision fusion rules. Experiments on ten public datasets show the effectiveness of our proposed method and demonstrate the superiority of distinguishing different land covers on the basis of very few training samples. Therefore, it can be applied to practical applications in different scenes.","1558-0644","","10.1109/TGRS.2022.3180935","National Natural Science Foundation of China(grant numbers:91948303-1,61803375,12002380,62106278,62101575,61906210); National University of Defense Technology Foundation(grant numbers:ZK20-52); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791303","Adaptive cubic total variation;augmented Lag-range multiplier;feature extraction;hyperspectral classification","Feature extraction;Hyperspectral imaging;Image edge detection;Support vector machines;Kernel;Training;Smoothing methods","feature extraction;geophysical image processing;image classification;image fusion;probability;remote sensing;smoothing methods","complementary spectral-spatial method;hyperspectral image classification;spectral information;classification accuracy;novel robust complementary method;spectral-spatial information;information loss;classification effect;complementary feature extraction stages;pre-processing feature extraction;adaptive cubic total variational smoothing method;remote sensing research field;first-stage probability map;edge-preserving filtering;post-processing stage;probability-like maps;decision fusion rules","","1","","62","IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"Level-Aware Consistent Multilevel Map Translation From Satellite Imagery","Y. Fu; Z. Fang; L. Chen; T. Song; D. Lin","School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Astronautics, Beijing Institute of Technology, Beijing, China; School of Astronautics, Beijing Institute of Technology, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2023","2023","61","","1","14","With the rapid development of remote sensing technology, the quality of satellite imagery (SI) is getting higher, which contains rich cartographic information that can be translated into maps. However, existing methods either only focus on generating single-level map or do not fully consider the challenges of multilevel translation from satellite imageries, i.e., the large domain gap, level-dependent content differences, and main content consistency. In this article, we propose a novel level-aware fusion network for the SI-based multilevel map generation (MLMG) task. It aims to tackle these three challenges. To deal with the large domain gap, we propose to generate maps in a coarse-to-fine way. To well-handle the level-dependent content differences, we design a level classifier to explore different levels of the map. Besides, we use a map element extractor to extract the major geographic element features from satellite imageries, which is helpful to keep the main content consistency. Next, we design a multilevel fusion generator to generate a consistent multilevel map from the multilevel preliminary map, which further ensures the main content consistency. In addition, we collect a high-quality multilevel dataset for SI-based MLMG. Experimental results show that the proposed method can provide substantial improvements over the state-of-the-art alternatives in terms of both objective metric and visual quality.","1558-0644","","10.1109/TGRS.2022.3220423","National Natural Science Foundation of China(grant numbers:62171038,61827901,62088101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950295","Large domain gap;level-dependent content differences;main content consistency;satellite imagery (SI)","Generators;Feature extraction;Task analysis;Satellites;Training;Roads;Visualization","cartography;feature extraction;geographic information systems;geophysical image processing;image classification;image fusion;remote sensing","domain gap;high-quality multilevel dataset;level classifier;level-aware consistent multilevel map translation;level-aware fusion network;map element extractor;multilevel fusion generator;multilevel preliminary map;remote sensing technology;rich cartographic information;satellite imagery;SI-based MLMG;SI-based multilevel map generation task;single-level map","","","","53","IEEE","14 Nov 2022","","","IEEE","IEEE Journals"
"Joint Classification of Hyperspectral and LiDAR Data Using a Hierarchical CNN and Transformer","G. Zhao; Q. Ye; L. Sun; Z. Wu; C. Pan; B. Jeon","School of Computer and Science, Nanjing University of Information Science and Technology, Nanjing, China; School of Information Science and Technology, Nanjing Forestry University, Nanjing, China; School of Computer and Science, the Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET), and the Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing University of Information Science and Technology (NUIST), Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronics and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China; School of Electronic and Electrical Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Transactions on Geoscience and Remote Sensing","9 Jan 2023","2023","61","","1","16","The joint use of multisource remote-sensing (RS) data for Earth observation missions has drawn much attention. Although the fusion of several data sources can improve the accuracy of land-cover identification, many technical obstacles, such as disparate data structures, irrelevant physical characteristics, and a lack of training data, exist. In this article, a novel dual-branch method, consisting of a hierarchical convolutional neural network (CNN) and a transformer network, is proposed for fusing multisource heterogeneous information and improving joint classification performance. First, by combining the CNN with a transformer, the proposed dual-branch network can significantly capture and learn spectral–spatial features from hyperspectral image (HSI) data and elevation features from light detection and ranging (LiDAR) data. Then, to fuse these two sets of data features, a cross-token attention (CTA) fusion encoder is designed in a specialty. The well-designed deep hierarchical architecture takes full advantage of the powerful spatial context information extraction ability of the CNN and the strong long-range dependency modeling ability of the transformer network based on the self-attention (SA) mechanism. Four standard datasets are used in experiments to verify the effectiveness of the approach. The experimental results reveal that the proposed framework can perform noticeably better than state-of-the-art methods. The source code of the proposed method will be available publicly at https://github.com/zgr6010/Fusion_HCT.git.","1558-0644","","10.1109/TGRS.2022.3232498","National Natural Science Foundation of China(grant numbers:61971233,62076137,U20B2065,U20B2061); Natural Science Foundation of Jiangsu Province(grant numbers:BK 20211539); National Nature Science Foundation of China(grant numbers:61931004); Jiangsu Innovation & Entrepreneurship Group Talents Plan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999457","Convolutional neural network (CNN);hyperspectral image (HSI);joint classification;light detection and ranging (LiDAR) data;tokenization;transformer","Feature extraction;Laser radar;Transformers;Convolutional neural networks;Data mining;Convolution;Tokenization","convolutional neural nets;geophysical image processing;hyperspectral imaging;image classification;image fusion;land cover;optical radar;remote sensing","cross-token attention fusion encoder;data features;data sources;deep hierarchical architecture;disparate data structures;dual-branch method;dual-branch network;Earth observation missions;elevation features;hierarchical CNN;hierarchical convolutional neural network;improving joint classification performance;irrelevant physical characteristics;land-cover identification;LiDAR data;light detection;multisource heterogeneous information;multisource remote-sensing data;self-attention mechanism;spatial context information extraction ability;spectral-spatial features;strong long-range dependency modeling ability;transformer network","","","","59","IEEE","26 Dec 2022","","","IEEE","IEEE Journals"
"Cross-Guided Pyramid Attention-Based Residual Hyperdense Network for Hyperspectral Image Pansharpening","J. Qu; T. Zhang; W. Dong; Y. Yang; Y. Li","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","18 Nov 2022","2022","60","","1","14","Hyperspectral (HS) image pansharpening is of great importance in improving the spatial resolution for many commercial platforms and remote sensing tasks. Convolutional neural network (CNN) has recently been applied in pansharpening. However, most existing CNN-based pansharpening models followed an early-fusion/late-fusion strategy, which integrates the low-level/high-level features of panchromatic (PAN) and HS streams at the input-output of the network. It is difficult to learn more complex combinations between PAN and HS streams. This article proposes a novel end-to-end residual hyperdense pansharpening network with a cross-guided pyramid attention (called RHDcgpaNet). The overall architecture of the proposed method is a residual hyperdense network, which extends the definition of dense connections to two-stream pansharpening problem. The proposed RHDcgpaNet allows guidance from the state of the preceding layers to all the layers in- between PAN and HS streams in a feed-forward manner, significantly increasing the learning representation. A cross-guided pyramid attention is designed and embedded to the proposed residual hyperdense network to yield more useful spatial–spectral feature transfer in network. Extensive experiments on widely used datasets demonstrate that the proposed RHDcgpaNet achieves favorable performance in comparison to the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3220079","Higher Education Discipline Innovation Project(grant numbers:B08038); National Natural Science Foundation of China(grant numbers:62101414,62201423); Young Talent Fund of Xi’an Association for Science and Technology(grant numbers:095920221320); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2021JQ-194,2021JQ-197); Fundamental Research Funds for the Central Universities(grant numbers:XJS210108,XJS210104); China Postdoctoral Science Foundation(grant numbers:2021M702546,2021M702548); China Postdoctoral Science Foundation(grant numbers:2022T150508); Scientific and Technological Activities for Overseas Students of Shaanxi Province(grant numbers:2020-017); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110856); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9941009","Cross-guided pyramid attention;hyperspectral (HS) image;HS image pansharpening","Pansharpening;Streaming media;Hyperspectral imaging;Task analysis;Convolutional neural networks;Spatial resolution;Feature extraction","convolutional neural nets;feature extraction;geophysical image processing;image colour analysis;image fusion;image representation;image resolution;learning (artificial intelligence);remote sensing","CNN-based pansharpening;convolutional neural network;cross-guided pyramid attention-based residual hyperdense network;early-fusion/late-fusion strategy;end-to-end residual hyperdense pansharpening network;HS image pansharpening;hyperspectral image pansharpening;learning representation;panchromatic streams;remote sensing;RHDcgpaNet;spatial resolution;spatial-spectral feature transfer;two-stream pansharpening problem","","","","48","IEEE","7 Nov 2022","","","IEEE","IEEE Journals"
"Multiscale and Cross-Level Attention Learning for Hyperspectral Image Classification","F. Xu; G. Zhang; C. Song; H. Wang; S. Mei","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Millimeter Wave Imaging Technology, Shanghai Institute of Satellites Engineering, Shanghai, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","19 Jan 2023","2023","61","","1","15","Transformer-based networks, which can well model the global characteristics of inputted data using the attention mechanism, have been widely applied to hyperspectral image (HSI) classification and achieved promising results. However, the existing networks fail to explore complex local land cover structures in different scales of shapes in hyperspectral remote sensing images. Therefore, a novel network named multiscale and cross-level attention learning (MCAL) network is proposed to fully explore both the global and local multiscale features of pixels for classification. To encounter local spatial context of pixels in the transformer, a multiscale feature extraction (MSFE) module is constructed and implemented into the transformer-based networks. Moreover, a cross-level feature fusion (CLFF) module is proposed to adaptively fuse features from the hierarchical structure of MSFEs using the attention mechanism. Finally, the spectral attention module (SAM) is implemented prior to the hierarchical structure of MSFEs, by which both the spatial context and spectral information are jointly emphasized for hyperspectral classification. Experiments over several benchmark datasets demonstrate that the proposed MCAL obviously outperforms both the convolutional neural network (CNN)-based and transformer-based state-of-the-art networks for hyperspectral classification.","1558-0644","","10.1109/TGRS.2023.3235819","National Natural Science Foundation of China(grant numbers:62171381); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10013757","Hyperspectral image (HSI) classification;multihead self-attention (MHSA);multiscale convolution (MSC);transformer","Feature extraction;Transformers;Hyperspectral imaging;Data mining;Correlation;Convolution;Task analysis","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);remote sensing","complex local land cover structures;convolutional neural network;cross-level attention learning;cross-level feature fusion module;global features;hierarchical structure;hyperspectral image classification;hyperspectral remote sensing images;inputted data;local spatial context;MSFE module;multiscale feature extraction module;spectral attention module;transformer-based networks","","","","59","IEEE","10 Jan 2023","","","IEEE","IEEE Journals"
"Regularizing Hyperspectral and Multispectral Image Fusion by CNN Denoiser","R. Dian; S. Li; X. Kang","Instituto de Telecomunicacõoes, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China","IEEE Transactions on Neural Networks and Learning Systems","1 Mar 2021","2021","32","3","1124","1135","Hyperspectral image (HSI) and multispectral image (MSI) fusion, which fuses a low-spatial-resolution HSI (LR-HSI) with a higher resolution multispectral image (MSI), has become a common scheme to obtain high-resolution HSI (HR-HSI). This article presents a novel HSI and MSI fusion method (called as CNN-Fus), which is based on the subspace representation and convolutional neural network (CNN) denoiser, i.e., a well-trained CNN for gray image denoising. Our method only needs to train the CNN on the more accessible gray images and can be directly used for any HSI and MSI data sets without retraining. First, to exploit the high correlations among the spectral bands, we approximate the desired HR-HSI with the low-dimensional subspace multiplied by the coefficients, which can not only speed up the algorithm but also lead to more accurate recovery. Since the spectral information mainly exists in the LR-HSI, we learn the subspace from it via singular value decomposition. Due to the powerful learning performance and high speed of CNN, we use the well-trained CNN for gray image denoising to regularize the estimation of coefficients. Specifically, we plug the CNN denoiser into the alternating direction method of multipliers (ADMM) algorithm to estimate the coefficients. Experiments demonstrate that our method has superior performance over the state-of-the-art fusion methods.","2162-2388","","10.1109/TNNLS.2020.2980398","Major Program of the National Natural Science Foundation of China(grant numbers:61890962); National Natural Science Foundation of China(grant numbers:61601179,6187119); National Natural Science Fund of China for International Cooperation and Exchanges(grant numbers:61520106001); Fund of Hunan Province for Science and Technology Plan Project(grant numbers:2017RS3024); Fund of Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province(grant numbers:2018TP1013); Natural Science Foundation of Hunan Province(grant numbers:2019JJ50036); Fundação para a Ciência e a Tecnologia(grant numbers:UID/EEA/50008/2019); Hunan Provincial Innovation Foundation for Postgraduate; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069930","Convolutional neural network (CNN);fusion;hyperspectral imaging;superresolution","Spatial resolution;Tensile stress;Estimation;Hyperspectral imaging;Dictionaries;Correlation","geophysical image processing;hyperspectral imaging;image classification;image denoising;image enhancement;image fusion;image resolution;image sensors;learning (artificial intelligence);neural nets;remote sensing;singular value decomposition","CNN denoiser;MSI;low-spatial-resolution HSI;LR-HSI;higher resolution multispectral image;high-resolution HSI;novel HSI;CNN-Fus;convolutional neural network denoiser;gray image denoising;accessible gray images;desired HR-HSI;low-dimensional subspace;state-of-the-art fusion methods;multispectral image fusion","","101","","56","IEEE","17 Apr 2020","","","IEEE","IEEE Journals"
"Hyperspectral Image Denoising by Fusing the Selected Related Bands","X. Zheng; Y. Yuan; X. Lu","Center for OPTical IMagery Analysis and Learning, State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Center for OPTical IMagery Analysis and Learning, State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China; Center for OPTical IMagery Analysis and Learning, State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","23 Apr 2019","2019","57","5","2596","2609","Hyperspectral images (HSIs) convey more useful information than RGB or gray images, which are widely used in many remote sensing tasks. In real scenarios, HSIs are inevitably corrupted by noise because of sensors' imperfectness or atmospheric influence. Recently, many HSI denoising methods have been proposed to utilize the interband information between different spectral bands. However, these methods regard the HSI as a whole and treat the different spectral bands with the same noise level. In fact, the noise levels in different bands are different. Especially, only few certain bands are corrupted by noise, named the target noised bands. Under this circumstance, an HSI denoising method is proposed by considering the band relationship and different noise levels. The target noised bands are adaptively denoised by fusing some selected bands. Specifically, some related but quality superior bands are selected according to the target noised bands. Then, the target noised bands can be denoised by fusing the selected related bands. Experimental results show that the proposed method achieves considerable performances in comparison with several state-of-the-art hyperspectral denoising methods.","1558-0644","","10.1109/TGRS.2018.2875304","National Natural Science Foundation of China(grant numbers:61632081,61472413,61772510,61806193); Chinese Academy of Sciences(grant numbers:QYZDB-SSW-JSC015); State Key Laboratory of Transient Optics and Photonics(grant numbers:SKLST2017010); CAS “Light of West China” Program(grant numbers:XAB2017B26); Xi’an Postdoctoral Innovation Base Scientific Research Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527652","Band information;band selection;hyperspectral image (HSI) denoising;image fusion","Noise reduction;Noise level;Hyperspectral imaging;Image denoising;Image quality;Optics","approximation theory;correlation methods;geophysical image processing;hyperspectral imaging;image denoising;remote sensing","selected related bands;hyperspectral images;gray images;remote sensing tasks;HSI denoising method;interband information;noise level;band relationship;selected bands;related but quality superior bands;target noised bands;state-of-the-art hyperspectral denoising methods;hyperspectral image denoising;spectral bands;noise levels","","28","","49","IEEE","8 Nov 2018","","","IEEE","IEEE Journals"
"Full-Scale Assessment of Pansharpening Through Polynomial Fitting of Multiscale Measurements","R. Carlà; L. Santurri; B. Aiazzi; S. Baronti","“N. Carrara” Institute of Applied Physics, National Research Council (IFAC-CNR), Sesto Fiorentino, Italy; “N. Carrara” Institute of Applied Physics, National Research Council (IFAC-CNR), Sesto Fiorentino, Italy; “N. Carrara” Institute of Applied Physics, National Research Council (IFAC-CNR), Sesto Fiorentino, Italy; “N. Carrara” Institute of Applied Physics, National Research Council (IFAC-CNR), Sesto Fiorentino, Italy","IEEE Transactions on Geoscience and Remote Sensing","4 Sep 2015","2015","53","12","6344","6355","Pansharpening techniques aim at improving the spatial resolution of a multispectral data set (MS) by using a panchromatic image (Pan) acquired on the same scene with a greater spatial resolution and consequently a lower ground sample distance (GSD). Usually, a quantitative assessment of the fused products cannot be directly performed because of the lack of a reference MS data set with the same GSD of the Pan. A well-known solution is Wald's protocol: The original MS and Pan are spatially degraded, the reducing factor being the ratio between their GSDs. Pansharpening is then performed between the reduced MS and Pan data sets, and the fused products are compared with the original MS, which can be used as reference. In this protocol, fusion performances are assumed to be independent of the scale so that the results at the reduced scale are an estimation of those at the original resolution. This hypothesis can be more or less reliable, depending on the sensor and/or the scene content. The objective of this paper is to propose a new methodology to infer the unknown performances of a pansharpening method at full scale. For this purpose, multiple sets of fused images are computed at degraded scales by downsampling Pan and MS data sets by means of the sensor modulation transfer function. Multiscale quality/distortion measurements are fitted by linear and quadratic polynomials in order to extrapolate their full-scale values. Once the proposed protocol has been assessed in the presence of reference originals, the obtained results are extended to the case where the reference image is not available.","1558-0644","","10.1109/TGRS.2015.2436699","Italian Space Agency (ASI) under the PRISMA mission(grant numbers:I/055/07/0); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7147807","Full-scale estimations;image pansharpening;modulation transfer function (MTF) filtering;multiscale measures;polynomial fitting;quality assessment;Wald's protocol;Full-scale estimations;image pansharpening;modulation transfer function (MTF) filtering;multiscale measures;polynomial fitting;quality assessment;Wald's protocol","Protocols;Estimation;Distortion;Indexes;Polynomials;Spatial resolution","geophysical image processing;image fusion;remote sensing","pansharpening assessment;multiscale measurement;pansharpening technique;panchromatic image;ground sample distance;multispectral dataset reduction;pansharpening dataset reduction;fusion performance;image fusion;sensor modulation transfer function;multiscale quality measurement;multiscale distortion measurement;linear polynomial fitting;quadratic polynomial fitting","","31","","43","IEEE","2 Jul 2015","","","IEEE","IEEE Journals"
"PanCSC-Net: A Model-Driven Deep Unfolding Method for Pansharpening","X. Cao; X. Fu; D. Hong; Z. Xu; D. Meng","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Faculty of Information Technology, Macau University of Science and Technology, Taipa, Macau","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","13","Recently, deep learning (DL) approaches have been widely applied to the pansharpening problem, which is defined as fusing a low-resolution multispectral (LRMS) image with a high-resolution panchromatic (PAN) image to obtain a high-resolution multispectral (HRMS) image. However, most DL-based methods handle this task by designing black-box network architectures to model the mapping relationship from LRMS and PAN to HRMS. These network architectures always lack sufficient interpretability, which limits their further performance improvements. To address this issue, we adopt the model-driven method to design an interpretable deep network structure for pansharpening. First, we present a new pansharpening model using the convolutional sparse coding (CSC), which is quite different from the current pansharpening frameworks. Second, an alternative algorithm is developed to optimize this model. This algorithm is further unfolded to a network, where each network module corresponds to a specific operation of the iterative algorithm. Therefore, the proposed network has clear physical interpretations, and all the learnable modules can be automatically learned in an end-to-end way from the given dataset. Experimental results on some benchmark datasets show that our network performs better than other advanced methods both quantitatively and qualitatively.","1558-0644","","10.1109/TGRS.2021.3115501","National Key Research and Development Program of China(grant numbers:2020YFA0713900); China NSFC Project(grant numbers:61906151,61901433,11690011,61721002,U1811461); Macao Science and Technology Development Fund(grant numbers:061/2020/A2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559907","Deep neural network;deep unfold;multispectral image;pansharpening","Pansharpening;Optimization;Satellites;Feature extraction;Contracts;Network architecture;Deep learning","deep learning (artificial intelligence);geophysical image processing;image coding;image colour analysis;image fusion;image representation;image resolution;iterative methods;remote sensing;unsupervised learning","high-resolution multispectral image;HRMS;black-box network architectures;interpretable deep network structure;convolutional sparse coding;iterative algorithm;PanCSC-net;model-driven deep unfolding method;deep learning approaches;pansharpening problem;low-resolution multispectral image fusion;LRMS;high-resolution panchromatic image;DL-based methods;learnable modules;end-to-end learning","","14","","61","IEEE","5 Oct 2021","","","IEEE","IEEE Journals"
"CNN-Based Hyperspectral Pansharpening With Arbitrary Resolution","L. He; J. Zhu; J. Li; A. Plaza; J. Chanussot; Z. Yu","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Hyperspectral Computing Laboratory, University of Extremadura, Cáceres, Spain; LJK, CNRS, Inria, Grenoble INP, University Grenoble Alpes, Grenoble, France; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","25 Feb 2022","2022","60","","1","21","Traditional hyperspectral (HS) pansharpening aims at fusing a HS image with its panchromatic (PAN) counterpart, to bring the spatial resolution of the HS image to that of the PAN image. However, in many practical applications, arbitrary resolution HS (ARHS) pansharpening is required, where the HS and PAN images need to be integrated to generate a pansharpened HS image with arbitrary resolution (usually higher than that of the PAN image). Such an innovative task brings forth new challenges for the pansharpening technique, mainly including how to reconstruct HS images beyond the training scale and how to guarantee spectral fidelity at any spatial resolutions. To tackle the challenges, we present a novel convolutional neural network (CNN)-based method for ARHS pansharpening called ARHS-CNN. It is based on a two-step relay optimization process, which is associated with a multilevel enhancement subnetwork and a rescaling subnetwork. With a careful design following the thread, our ARHS-CNN is able to pansharpen HS images to any spatial resolutions using just a single CNN model trained on a limited number of scales while meantime to keep spectral fidelity at those resolutions, which wins an obvious advantage over traditional pansharpening methods. Experimental results obtained on several datasets verify the excellent performance of our ARHS-CNN method.","1558-0644","","10.1109/TGRS.2021.3132997","National Natural Science Foundation of China(grant numbers:62071184,61571195,61836003,61771496,61633010); Guangdong Provincial Natural Science Foundation(grant numbers:2016A030313516,2017A030313382); Guangzhou Science and Technology Program(grant numbers:202002030395); Key Realm Research and Development Program of Guangzhou, China(grant numbers:202007030007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9638577","Arbitrary resolution HS (ARHS) pansharpening;convolutional neural networks (CNNs);dynamic convolution;hyperspectral (HS) pansharpening;spectral fidelity","Spatial resolution;Pansharpening;Task analysis;Training;Image reconstruction;Hyperspectral imaging;Convolutional neural networks","convolutional neural nets;geophysical image processing;hyperspectral imaging;image enhancement;image fusion;image reconstruction;image resolution;optimisation;remote sensing","hyperspectral pansharpening;panchromatic image;PAN image;convolutional neural network;ARHS-CNN;HS image spatial resolution;HS image fusion;arbitrary resolution HS image pansharpening;ARHS image pansharpening;two-step relay optimization process;multilevel enhancement subnetwork;rescaling subnetwork;HS image reconstruction","","3","","71","IEEE","6 Dec 2021","","","IEEE","IEEE Journals"
"Pansharpening via Super-Resolution Iterative Residual Network With a Cross-Scale Learning Strategy","S. Chen; H. Qi; K. Nan","Sichuan Highway Planning, Survey, Design and Research Institute Ltd., Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Sichuan Highway Planning, Survey, Design and Research Institute Ltd., Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","8 Mar 2022","2022","60","","1","16","Pansharpening exploits the high-spatial-resolution panchromatic (HR PAN) images to restore the spatial resolution of the corresponding low-spatial-resolution multi-spectral (LR MS) image, producing a fused image and high-spatial-resolution multi-spectral (HR MS) image. Recently, many methods based on convolutional neural networks (CNNs) have been put forth for the pansharpening task, but most of them still have some limitations, such as the simple stacked convolutional architectures resulting in information distortion, and some scale-related problems caused by the supervised learning strategy. Therefore, we propose a method named super-resolution iterative residual (SRIR) network with a cross-scale (CS) learning strategy to overcome these drawbacks. Regarding the SRIR we propose, we design an upsampling network based on a sub-pixel convolution structure to replace the traditional upsampling pre-processing. We adopt the iterative networks framework and design a new spatial information injection module to continuously inject spatial and spectral features into the network, which can enhance the information flow and transmission. We produce approximate HR MS with a guidance filter and map the residual information between the approximate HR MS and the reference HR MS by SRIR to enhance the quality of fused images. Regarding the CS we propose, we train the network at degraded scale, which is named deep prior, and then design a finer-scale unsupervised fine-tuning loss function to refine the network parameters with deep priors, to overcome the scale effect. Experiments show the following: 1) SRIR-based pansharpening method can obtain the best result at the degraded scale; 2) the scale-effect is negatively correlated with the depth of the network, meaning that the deeper the network, the stronger the robustness to scale effect; 3) the CS learning strategy can widely improve the performance of CNNs-based pansharpening methods in full-resolution; and 4) our method can produce better results at full-resolution scale than all the other traditional and deep learning methods.","1558-0644","","10.1109/TGRS.2021.3138096","Key Research and Development Projects in Sichuan, China(grant numbers:2021YFS0334); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9662300","A super-resolution iterative residual (SRIR) network;cross-scale (CS) learning strategy;finer-scale unsupervised fine-tuning loss function;pansharpening","Pansharpening;Supervised learning;Superresolution;Spatial resolution;Residual neural networks;Convolutional neural networks;Convolution","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image enhancement;image fusion;image resolution;image sampling;iterative methods;remote sensing","super-resolution iterative residual network;cross-scale learning strategy;high-spatial-resolution panchromatic images;low-spatial-resolution multispectral image;LR MS;image fusion;high-spatial-resolution multispectral image;convolutional neural networks;convolutional architectures;information distortion;supervised learning strategy;upsampling network;sub-pixel convolution structure;iterative networks framework;spatial information injection module;spatial features;spectral features;information flow;residual information;finer-scale unsupervised fine-tuning loss function;network parameters;CS learning strategy;CNNs-based pansharpening methods;full-resolution scale;deep learning methods;HR MS;SRIR-based pansharpening method;upsampling pre-processing","","","","61","IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic Images via GANs","F. Ozcelik; U. Alganci; E. Sertel; G. Unal","Artificial Intelligence and Data Science Research and Application Center, Istanbul Technical University (ITU), Istanbul, Turkey; Research Center for Satellite Communications and Remote Sensing (CSCRS), Istanbul Technical University (ITU), Istanbul, Turkey; Research Center for Satellite Communications and Remote Sensing (CSCRS), Istanbul Technical University (ITU), Istanbul, Turkey; Artificial Intelligence and Data Science Research and Application Center, Istanbul Technical University (ITU), Istanbul, Turkey","IEEE Transactions on Geoscience and Remote Sensing","24 Mar 2021","2021","59","4","3486","3501","Convolutional neural network (CNN)-based approaches have shown promising results in the pansharpening of the satellite images in recent years. However, they still exhibit limitations in producing high-quality pansharpening outputs. To that end, we propose a new self-supervised learning framework, where we treat pansharpening as a colorization problem, which brings an entirely novel perspective and solution to the problem compared with the existing methods that base their solution solely on producing a super-resolution version of the multispectral image. Whereas the CNN-based methods provide a reduced-resolution panchromatic image as the input to their model along with the reduced-resolution multispectral images and, hence, learn to increase their resolution together, we instead provide the grayscale transformed multispectral image as the input and train our model to learn the colorization of the grayscale input. We further address the fixed downscale ratio assumption during training, which does not generalize well to the full-resolution scenario. We introduce a noise injection into the training by randomly varying the downsampling ratios. Those two critical changes, along with the addition of adversarial training in the proposed PanColorization generative adversarial network (PanColorGAN) framework, help overcome the spatial-detail loss and blur problems that are observed in CNN-based pansharpening. The proposed approach outperforms the previous CNN-based and traditional methods, as demonstrated in our experiments.","1558-0644","","10.1109/TGRS.2020.3010441","Research Fund of the Istanbul Technical University Project(grant numbers:MGA-2017-40811); Turkcell-ITU Researcher Funding Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153037","AI;colorization;convolutional neural networks (CNNs);deep learning;generative adversarial networks (GANs);image fusion;PanColorization generative adversarial network (PanColorGAN);pansharpening;self-supervised learning;super-resolution (SR)","Task analysis;Spatial resolution;Training;Standards;Sensors;Multiresolution analysis","convolutional neural nets;geophysical image processing;image colour analysis;image resolution;supervised learning","adversarial training;grayscale transformed multispectral image;blur problems;PanColorGAN;panchromatic image resolution;PanColorization generative adversarial network;CNN based pansharpening;panchromatic image colorization;self supervised learning;satellite images;convolutional neural network;multispectral image resolution","","37","","44","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"Fast and Accurate Spatiotemporal Fusion Based Upon Extreme Learning Machine","X. Liu; C. Deng; S. Wang; G. -B. Huang; B. Zhao; P. Lauren","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Engineering and Computer Science, Oakland University, Rochester, MI, USA","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2016","13","12","2039","2043","Spatiotemporal fusion is important in providing high spatial resolution earth observations with a dense time series, and recently, learning-based fusion methods have been attracting broad interest. These algorithms project image patches onto a feature space with the enforcement of a simple mapping to predict the fine resolution patches from the corresponding coarse ones. However, the sophisticated projection, e.g., sparse representation, is always computationally complex and difficult to be implemented on large patches, which cannot grasp enough local structural information in the coarse patches. To address these issues, a novel spatiotemporal fusion method is proposed in this letter, using a powerful learning technique, i.e., extreme learning machine (ELM). Unlike traditional approaches, we devote to learning a mapping function on difference images directly, rather than the sophisticated feature representation followed by a simple mapping. Characterized by good generalization performance and fast speed, the ELM is employed to achieve accurate and fast fine patches prediction. The proposed algorithm is evaluated by five actual data sets of Landsat enhanced thematic mapper plus-moderate resolution imaging spectroradiometer acquisitions and experimental results show that our method obtains better fusion results while achieving much greater speed.","1558-0571","","10.1109/LGRS.2016.2622726","National Natural Science Foundation of China(grant numbers:61301090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748638","Extreme learning machine (ELM);feature representation;local structural information;mapping function;spatiotemporal image fusion","Satellites;Earth;Remote sensing;MODIS;Spatiotemporal phenomena;Spatial resolution;Training","geophysical image processing;image fusion;spatiotemporal phenomena;time series","moderate resolution imaging spectroradiometer;Landsat enhanced thematic mapper;ELM;powerful learning technique;coarse patches;local structural information;learning-based fusion method;time series;extreme learning machine;spatiotemporal fusion method","","56","","19","IEEE","18 Nov 2016","","","IEEE","IEEE Journals"
"Deep Self-Paced Residual Network for Multispectral Images Classification Based on Feature-Level Fusion","J. Zhang; D. Zhang; W. Ma; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center of Intelligent Perception and Computation, International Collaboration Joint Laboratory in Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","2 Nov 2018","2018","15","11","1740","1744","The classification methods based on fusion techniques of multisource multispectral (MS) images have been studied for a long time. However, it may be difficult to classify these data based on a feature level while avoiding the inconsistency of data caused by multisource and multiple regions or cities. In this letter, we propose a deep learning structure called 2-branch SPL-ResNet which combines the self-paced learning with deep residual network to classify multisource MS data based on the feature-level fusion. First, a 2-D discrete wavelet is used to obtain the multiscale features and sparse representation of MS data. Then, a 2-branch SPL-ResNet is established to extract respective characteristics of the two satellites. Finally, we implement the feature-level fusion by cascading the two feature vectors and then classify the integrated feature vector. We conduct the experiments on Landsat_8 and Sentinel_2 MS images. Compared with the commonly used classification methods such as support vector machine and convolutional neural networks, our proposed 2-branch SPL-ResNet framework has higher accuracy and more robustness.","1558-0571","","10.1109/LGRS.2018.2854847","Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438103,91438201); National Natural Science Foundation of China(grant numbers:61472306,61573267); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424470","Deep residual network (ResNet);image classification;image fusion;multisource multispectral (MS) data;self-paced learning (SPL)","Feature extraction;Satellites;Fuses;Data mining;Urban areas;Remote sensing;Support vector machines","convolution;discrete wavelet transforms;feature extraction;feedforward neural nets;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence);support vector machines;vectors","2-branch SPL-ResNet framework;deep self-paced residual network;multispectral images classification;feature-level fusion;fusion techniques;multisource multispectral images;multiple regions;cities;deep learning structure;multisource MS data;2-D discrete wavelet;multiscale features;integrated feature vector;Sentinel_2 MS images;self-paced learning;sparse representation;satellites;Landsat_8;support vector machine;convolutional neural networks","","7","","15","IEEE","2 Aug 2018","","","IEEE","IEEE Journals"
"TEBCF: Real-World Underwater Image Texture Enhancement Model Based on Blurriness and Color Fusion","J. Yuan; Z. Cai; W. Cao","Faculty of Information Technology, Macau University of Science and Technology, Taipa, Macau; Faculty of Information Technology, Macau University of Science and Technology, Taipa, Macau; School of Information Technology, Beijing Institute of Technology, Zhuhai, China","IEEE Transactions on Geoscience and Remote Sensing","2 Feb 2022","2022","60","","1","15","Real-world underwater images suffer from quality degeneration caused by the scattering and absorption of light propagation. The damage of the detailed textures in underwater images shows the negative effect of detection and recognition. To recovery the image visibility and sharpness for the above applications, a new image enhancement method is proposed for extracting the image textures. To enhance the image textures with high quality, we propose a multiscale fusion enhancement. Two new fusion inputs are built on different color methods. One input is devoted to improve the sharpness by contrast-based dark channel prior dehazing in the red–green–blue (RGB) model. The other input is designed based on multiple morphological operation and color compensation from the opponent color in the CIE  $1976~L^{\ast}a^{\ast}b^{\ast}$  color space (CIELAB) model. This input is used to enhance the counter brightness and adjust the color distribution. The dominant features of the two inputs are merged. Therefore, the contrast of the fusion output is enhanced adaptively to recover the final enhanced result. Compared with the state-of-the-art methods, our results reveal that the proposed method can enrich the image textures based on an impressive visual perception of contrast, saturation, and sharpness. Moreover, our method also shows strong robustness in challenge scenes and improves the performance of several underwater applications.","1558-0644","","10.1109/TGRS.2021.3110575","Science and Technology Development Fund of Macau(grant numbers:0052/2020/AFJ,0059/2020/A2); Open Fund of the State Key Laboratory of Remote Sensing Science(grant numbers:OFSLRSS201901); Open Project Program of the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University(grant numbers:VRLAB2019C02); Guangdong Provincial College Innovative(grant numbers:2019KZDXM060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559904","Image fusion;texture enhancement;underwater;visibility recovery","Image color analysis;Visualization;Mathematical models;Image texture;Image restoration;Scattering;Optical imaging","image colour analysis;image enhancement;image fusion;image restoration;image texture;light propagation","color space model;color distribution;fusion output;final enhanced result;image textures;sharpness;real-world underwater image texture enhancement model;color fusion;real-world underwater images;detailed textures;image visibility;image enhancement method;multiscale fusion enhancement;fusion inputs;different color methods;contrast-based dark channel;red-green-blue model;color compensation;opponent color;TEBCF;blurriness;CIELAB model.","","1","","55","IEEE","5 Oct 2021","","","IEEE","IEEE Journals"
"Spectral Image Fusion From Compressive Measurements","E. Vargas; Ó. Espitia; H. Arguello; J. -Y. Tourneret","Department of Electrical Engineering, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Systems Engineering and Informatics, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Systems Engineering and Informatics, Universidad Industrial de Santander, Bucaramanga, Colombia; University of Toulouse, INP-ENSEEIHT/IRIT/TéSA, Toulouse, France","IEEE Transactions on Image Processing","31 Jan 2019","2019","28","5","2271","2282","Compressive spectral imagers reduce the number of sampled pixels by coding and combining the spectral information. However, sampling compressed information with simultaneous high spatial and high spectral resolution demands expensive high-resolution sensors. This paper introduces a model allowing data from high spatial/low spectral and low spatial/high spectral resolution compressive sensors to be fused. Based on this model, the compressive fusion process is formulated as an inverse problem that minimizes an objective function defined as the sum of a quadratic data fidelity term and smoothness and sparsity regularization penalties. The parameters of the different sensors are optimized and the choice of an appropriate regularization is studied in order to improve the quality of the high resolution reconstructed images. Simulation results conducted on synthetic and real data, with different compressive sampling imagers, allow the quality of the proposed fusion method to be appreciated.","1941-0042","","10.1109/TIP.2018.2884081","Department of Science, Technology and Innovation, COLCIENCIAS; Ministerio de Educación Nacional of Colombia; ICETEX; French Government(grant numbers:ECOS Nord); Intercambio de Investigadores Colombia-Francia, with the Project “Diseño de Aperturas Codificadas de Color Para Realizar Separación Espectral de Imágenes de Productos agrícolas Adquiridas Mediante Muestreo Compresivo.”; STIC-AmSud project HYPERMED; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552442","Spectral imaging;compressive sampling;data fusion;remote sensing","Image coding;Sensors;Sparse matrices;Spatial resolution;Image reconstruction;Inverse problems","compressed sensing;image fusion;image reconstruction;image resolution;image sampling;inverse problems","spectral image fusion;compressive spectral imagers;compressive sampling imagers;spectral information coding;high spatial low spectral resolution compressive sensors;low spatial high spectral resolution compressive sensors;compressive fusion;inverse problem;image reconstruction","","18","","37","IEEE","29 Nov 2018","","","IEEE","IEEE Journals"
"Unsupervised Detection of Earthquake-Triggered Roof-Holes From UAV Images Using Joint Color and Shape Features","S. Li; H. Tang; S. He; Y. Shu; T. Mao; J. Li; Z. Xu","State Key Laboratory of Earth Surface Processes and Resource Ecology and the Key Laboratory of Environment Change and Natural Disaster, Ministry of Education, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Key Laboratory of Environment Change and Natural Disaster, Ministry of Education, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Key Laboratory of Environment Change and Natural Disaster, Ministry of Education, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Key Laboratory of Environment Change and Natural Disaster, Ministry of Education, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Key Laboratory of Environment Change and Natural Disaster, Ministry of Education, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Key Laboratory of Environment Change and Natural Disaster, Ministry of Education, Beijing Normal University, Beijing, China; State Key Laboratory of Earth Surface Processes and Resource Ecology and the Key Laboratory of Environment Change and Natural Disaster, Ministry of Education, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","7 Aug 2015","2015","12","9","1823","1827","Many methods have been developed to detect damaged buildings due to earthquake. However, little attention has been paid to analyze slightly affected buildings. In this letter, an unsupervised method is presented to detect earthquake-triggered “roof-holes” on rural houses from unmanned aerial vehicle (UAV) images. First, both orthomosaic and gradient images are generated from a set of UAV images. Then, a modified Chinese restaurant franchise model is used to learn an unsupervised model of the geo-object classes in the area by fusing both oversegmented orthomosaic and gradient images. Finally, “roof-holes” on rural houses are detected using the learned model. The performance of the proposed method is evaluated in terms of both qualitative and quantitative indexes.","1558-0571","","10.1109/LGRS.2015.2429894","National Basic Research Program of China(grant numbers:2011CB707102); Program for New Century Excellent Talents in University(grant numbers:NECT-11-0039); National Key Technology Research and Development Program of the Ministry of Science and Technology of China(grant numbers:2012BAH27B01,2012BAH27B03); National High Technology Research and Development Program(grant numbers:2012AA121302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7111263","Chinese restaurant franchise (CRF);image fusion;roof-hole detection;unmanned aerial vehicle (UAV) images;Chinese restaurant franchise (CRF);image fusion;roof-hole detection;unmanned aerial vehicle (UAV) images","Buildings;Earthquakes;Image color analysis;Shape;Remote sensing;Feature extraction;Joints","autonomous aerial vehicles;earthquakes;geophysical techniques","unsupervised detection;earthquake-triggered roof-hole;UAV image;joint color feature;joint shape feature;damaged building detection method;unsupervised method;rural house;unmanned aerial vehicle;oversegmented orthomosaic image;oversegmented gradient image;modified Chinese restaurant franchise model;geo-object class;learned model;qualitative index;quantitative index","","37","","24","IEEE","21 May 2015","","","IEEE","IEEE Journals"
"Multimodal Sensor Fusion Using Symmetric Skip Autoencoder Via an Adversarial Regulariser","S. Bhagat; S. D. Joshi; B. Lall; S. Gupta","Department of Electrical Engineering, IIT Delhi, New Delhi, India; Department of Electrical Engineering, IIT Delhi, New Delhi, India; Bharti School of Telecommunications Technology & Management, IIT Delhi, New Delhi, India; Mastercard, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2021","2021","14","","1146","1157","The fusion of the spatial characteristics, of visual image, and spectral aspects, of infrared image, is of immense practical importance. In this work, we propose a novel spatially constrained adversarial autoencoder that extracts deep features from the infrared and visible images to obtain a more exhaustive and global representation. A residual autoencoder architecture, regularised by a residual adversarial network has been employed, to generate a more realistic fused image. The residual module serves as a primary building block for the encoder, decoder, and adversarial network, as an add on the symmetric skip connections, perform the functionality of embedding the spatial characteristics directly from the initial layers of encoder structure to the decoder part of the network. The spectral information in the infrared image is incorporated by adding the feature maps over several layers in the encoder part of the fusion structure. The encoder section is made up of two separate branches to carry out independent inference on both the visual as well as infrared images. The loss function has been designed to incorporate the characteristics of both the modalities by optimizing over the textural content of the visible image and the spectral content of its infrared counterpart. In order to efficiently optimize the network's parameters, an adversarial regulariser network has been proposed that would perform supervised learning on the fused image and the original visual image since the visual image contains most of the structural content in comparison to the infrared image. The adversarial game has been incorporated in the structure by the addition of classification loss in the generator and discriminator loss functions in addition to the content loss.","2151-1535","","10.1109/JSTARS.2020.3035633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9247398","Autoencoder;generative modeling;image fusion;TNO dataset","Image fusion;Feature extraction;Visualization;Generators;Task analysis;Image reconstruction;Transforms","geophysical image processing;image classification;image fusion;infrared imaging;learning (artificial intelligence);pattern classification;sensor fusion","infrared image;visible image;infrared counterpart;adversarial regulariser network;original visual image;adversarial game;multimodal sensor fusion;symmetric skip autoencoder;spatial characteristics;adversarial autoencoder;infrared images;visible images;residual autoencoder architecture;residual adversarial network;realistic fused image;symmetric skip connections;encoder structure;fusion structure","","","","54","CCBYNCND","3 Nov 2020","","","IEEE","IEEE Journals"
"Fast Multispectral Fusion and High-Precision Interdetector Image Stitching of Agile Satellites Based on Velocity Vector Field","J. Du; X. Yang; M. Zhou; Z. Tu; S. Wang; X. Tang; L. Cao; X. Zhao","and the Key Laboratory of Space-Based Dynamic & Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, Jilin, China; and the Key Laboratory of Space-Based Dynamic & Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, Jilin, China; and the Key Laboratory of Space-Based Dynamic & Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, Jilin, China; and the Key Laboratory of Space-Based Dynamic & Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, Jilin, China; and the Key Laboratory of Space-Based Dynamic & Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, Jilin, China; and the Key Laboratory of Space-Based Dynamic & Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, Jilin, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; and the Key Laboratory of Space-Based Dynamic & Rapid Optical Imaging Technology, Chinese Academy of Sciences, Changchun, Jilin, China","IEEE Sensors Journal","11 Nov 2022","2022","22","22","22134","22147","When the targets are distributed irregularly, it is of great significance to quickly acquire color images with a wide field by agile satellite. Attributed to the changing of the three-axis attitude and Earth’s rotation, there are misalignment and distortion during the processing of imaging by agile satellite. It makes the fusion of interband images slower and the stitching of interdetector images poorer. In this article, a rigorous velocity vector field (VVF) model is established through tracing rays between the image and the object. It calculates the misalignment among the images of bands in each detector to align and fuse multispectral images. Based on the reference image selected by VVF, the fused images are stitched with the method of leveraging line-point consistency. Finally, both field and in-orbit experiments are conducted. The time of calculating misalignment in image fusion with VVF is 13.19% of the minimum time of the traditional registration algorithm. In image stitching, our RMSE values are 1.15, which is the smallest among the RMSE values of SIFT, SURF, and SPW. In preserving linear structure, compared with SPW, the values of  ${E_{\text {dis}}}$ ,  ${E_{\text {err}}}$ , and  ${E_{\text {cross}}}$ , respectively, reduce 60%, 50%, and 50%. The results show that images of the agile satellite can be fast fused and precisely stitched using VVF while correcting distortion. The compensation strategy of the VVF provides a theoretical basis for the agile satellite in image fusion and stitching.","1558-1748","","10.1109/JSEN.2022.3209681","Natural Science Foundation of Jilin Province(grant numbers:20210101099JC); National Natural Science Foundation of China (NSFC)(grant numbers:62171430,62101071); Innovation and Entrepreneurship Team Project of Zhuhai City(grant numbers:ZH0405190001PWC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9908453","Agile satellite;fast multispectral images fusion;image stitching;spliced detector;velocity vector field (VVF)","Satellites;Distortion;Detectors;Image stitching;Earth;Sensors;Image fusion","artificial satellites;geophysical signal processing;image colour analysis;image fusion;image matching;image registration","agile satellite;color images;fast multispectral fusion;high-precision interdetector image stitching;image fusion;interband images;interdetector images;multispectral images;reference image;rigorous velocity vector field model;VVF","","","","31","IEEE","3 Oct 2022","","","IEEE","IEEE Journals"
"Poisson Reconstruction-Based Fusion of Infrared and Visible Images via Saliency Detection","J. Li; H. Huo; C. Sui; C. Jiang; C. Li","Department of Information Technology and Cyber Security, People’s Public Security University of China, Beijing, China; Department of Information Technology and Cyber Security, People’s Public Security University of China, Beijing, China; School of Opto-Electronic Information Science and Technology, Yantai University, Yantai, China; Department of Information Technology and Cyber Security, People’s Public Security University of China, Beijing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China","IEEE Access","25 Feb 2019","2019","7","","20676","20688","Saliency-based methods have been widely used in the fusion of infrared (IR) and visible (VIS) images, which can highlight the salient object region and preserve the detailed background information simultaneously. However, most existing methods ignore the salient information in the VIS image or they fail to highlight the boundaries of objects, which makes the final saliency map incomplete and the edges of the object blurred. To address the above-mentioned issues, we propose a novel IR and VIS images’ fusion algorithm based on the Poisson reconstruction and saliency detection using the Dempster–Shafer (DS) theory. In detail, we mix the gradient using a mask map derived from the saliency map, which could avoid low contrast and halo effects in the results. Besides, both the intensity saliency of the IR image and the structural saliency of all source images are considered by DS to suppress some noise in the IR image. Thus, we could obtain smooth object contours and enhance the edge information of the salient region. Moreover, we also propose a novel probability mass function to calculate the probabilistic map in the process of applying DS to decrease the error from manually assigning the prior probability. Finally, the extensive qualitative and quantitative experiments have demonstrated the advantages and effectiveness of our method compared with other nine state-of-the-art IR and VIS image fusion methods.","2169-3536","","10.1109/ACCESS.2019.2897320","National Key Research and Development Program of China(grant numbers:2016YFC0803006); Ministry of Public Security Technology Research Program(grant numbers:2018JSYJA01); National Key Research and Development Program of China(grant numbers:2017YFC0822405); National Natural Science Foundation of China(grant numbers:61601397); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8633896","Image fusion;object detection;context;image reconstruction","Image fusion;Image edge detection;Image reconstruction;Feature extraction;Saliency detection;Fuses;Transforms","image fusion;image reconstruction;image segmentation;infrared imaging;object detection;probability;stochastic processes;uncertainty handling","infrared images;visible images;saliency detection;detailed background information;VIS image;Dempster-Shafer theory;DS;halo effects;intensity saliency;IR image;structural saliency;smooth object contours;Poisson reconstruction;image fusion algorithm;probability mass function","","13","","63","OAPA","3 Feb 2019","","","IEEE","IEEE Journals"
"Coupled Tensor Decomposition for Hyperspectral Pansharpening","H. Li; W. Li; G. Han; F. Liu","School of Computer Science, XianYang Normal University, Xianyang, China; School of Computer Science, XianYang Normal University, Xianyang, China; School of Computer Science, XianYang Normal University, Xianyang, China; School of Computer Science and Technology, Xidian University, Xi’an, China","IEEE Access","10 Jul 2018","2018","6","","34206","34213","This paper aims to effectively preserve the spatial and spectral information of hyperspectral (HS) images. For this purpose, the author proposed a new image fusion method based on coupled tensor decomposition (CTD). First, the wanted high spatial resolution HS (HRHS) images were decomposed into the core tensor and basis matrices. Assuming that the basis matrices can be calculated from low spatial resolution HS (LRHS) images, the core tensor was estimated from the high spatial resolution multispectral (HRMS) images based on the relationship between HRHS and HRMS images. Finally, the HRHS images were obtained by reconstructing the core tensor with basis matrices. Owing to the good properties of tensor, the proposed method achieved better fusion results on different data sets than those of the contrastive methods. The research findings shed new light on hyperspectral pansharpening.","2169-3536","","10.1109/ACCESS.2018.2850340","National Basic Research Program (973 Program) of China(grant numbers:2013CB329402); National Natural Science Foundation of China(grant numbers:61573267,61173090,81473559); Fund for Foreign Scholars in University Research and Teaching Programs(grant numbers:B07048); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103); Natural Science Foundation of Shaanxi Province(grant numbers:2017JM6086); Science Basic Research Program in Shaanxi Province of China(grant numbers:16JK1823); Innovation Program in Shaanxi Province of China(grant numbers:2018KRM145); Xianyang Normal University(grant numbers:XSYK17030); Education Scientific Program of the 13th Five-year Plan in Shaanxi Province of China(grant numbers:SGH16H189); Serving Local Economic and Social Development Program at the Xianyang Development Institute of China(grant numbers:16XFY014); Xianyang Normal University(grant numbers:2015Z004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8395427","Multispectral (MS) image;hyperspectral (HS) image;image fusion;tensor decomposition","Tensile stress;Matrix decomposition;Spatial resolution;Hyperspectral imaging;Image fusion;Dictionaries","geophysical image processing;hyperspectral imaging;image fusion;image resolution;matrix algebra;tensors","hyperspectral pansharpening;coupled tensor decomposition;spatial information;spectral information;hyperspectral images;image fusion method;wanted high spatial resolution HS images;core tensor;basis matrices;low spatial resolution HS images;high spatial resolution multispectral images;HRHS images","","10","","35","OAPA","25 Jun 2018","","","IEEE","IEEE Journals"
"Progressive Pan-Sharpening via Cross-Scale Collaboration Networks","Z. Yang; X. Fu; A. Liu; Z. -J. Zha","School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, China","IEEE Geoscience and Remote Sensing Letters","6 May 2022","2022","19","","1","5","Pan-sharpening aims to produce a high-quality image by fusing a low-resolution multispectral (LRMS) image and a high-resolution panchromatic (PAN) image. Although deep-learning-based methods have dominated the pan-sharpening, they fail to fully utilize spatial and spectral information from the cross-scale perspective. In this work, we propose a novel cross-scale collaboration network to achieve accurate pan-sharpening. Specifically, we first design a progressive framework in a pyramid fashion to achieve a gradual pan-sharpening process, which consists of several subnetworks to handle specific pyramid levels. Then, in each subnetwork, we deploy two cross-scale attention modules to, respectively, capture global and local spatial interactions from MS and PAN images. To better utilize spectral information from different pyramid levels, we further deploy a fusion module between subnetworks to extract cross-scale representations. Through the above-mentioned cross-scale collaboration, our network can fully consider the cross-scale nature of MS and PAN images to better adapt to this specific task. Extensive experiments confirmed that our method outperforms several state-of-the-art (SOTA) methods both qualitatively and quantitatively.","1558-0571","","10.1109/LGRS.2022.3170376","University of Science and Technology of China (USTC) Research Funds of the Double First-Class Initiative(grant numbers:YD2100002003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762924","Cross-scale networks;deep learning (DL);image fusion;pan-sharpening;remote sensing","Feature extraction;Collaboration;Spatial resolution;Convolution;Wavelet transforms;Training;Task analysis","deep learning (artificial intelligence);geophysical image processing;image fusion;image resolution","progressive pan-sharpening;cross-scale collaboration networks;high-quality image;low-resolution multispectral image;high-resolution panchromatic image;deep-learning-based methods;spectral information;accurate pan-sharpening;progressive framework;gradual pan-sharpening process;subnetwork;specific pyramid levels;cross-scale attention modules;local spatial interactions;cross-scale representations;cross-scale collaboration network","","1","","18","IEEE","25 Apr 2022","","","IEEE","IEEE Journals"
"Residual Encoder–Decoder Conditional Generative Adversarial Network for Pansharpening","Z. Shao; Z. Lu; M. Ran; L. Fang; J. Zhou; Y. Zhang","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1573","1577","Due to the limitation of the satellite sensor, it is difficult to acquire a high-resolution (HR) multispectral (HRMS) image directly. The aim of pansharpening (PNN) is to fuse the spatial in panchromatic (PAN) with the spectral information in multispectral (MS). Recently, deep learning has drawn much attention, and in the field of remote sensing, several pioneering attempts have been made related to PNN. However, the big size of remote sensing data will produce more training samples, which require a deeper neural network. Most current networks are relatively shallow and raise the possibility of detail loss. In this letter, we propose a residual encoder-decoder conditional generative adversarial network (RED-cGAN) for PNN to produce more details with sharpened images. The proposed method combines the idea of an autoencoder with generative adversarial network (GAN), which can effectively preserve the spatial and spectral information of the PAN and MS images simultaneously. First, the residual encoder-decoder module is adopted to extract the multiscale features from the last step to yield pansharpened images and relieve the training difficulty caused by deepening the network layers. Second, to further enhance the performance of the generator to preserve more spatial information, a conditional discriminator network with the input of PAN and MS images is proposed to encourage that the estimated MS images share the same distribution as that of the referenced HRMS images. The experiments conducted on the Worldview2 (WV2) and Worldview3 (WV3) images demonstrate that our proposed method provides better results than several state-of-the-art PNN methods.","1558-0571","","10.1109/LGRS.2019.2949745","National Natural Science Foundation of China(grant numbers:61671312,61922029); Sichuan Science and Technology Program(grant numbers:2018HH0070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894479","Deep learning;generative adversarial network (GAN);multispectral (MS) image;panchromatic (PAN);pansharpening (PNN)","Feature extraction;Generative adversarial networks;Gallium nitride;Training;Generators;Task analysis;Decoding","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","residual encoder-decoder conditional generative adversarial network;high-resolution multispectral image;PNN;PAN;remote sensing data;deeper neural network;spectral information;residual encoder-decoder module;conditional discriminator network;referenced HRMS images;Worldview3 images;MS image estimation;Worldview2 images;RED-cGAN;satellite sensor;pansharpening;spatial fusion;feature extraction","","36","","22","IEEE","8 Nov 2019","","","IEEE","IEEE Journals"
"Adaptive Cross-Attention-Driven Spatial–Spectral Graph Convolutional Network for Hyperspectral Image Classification","J. -Y. Yang; H. -C. Li; W. -S. Hu; L. Pan; Q. Du","School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China; Southwest Institute of Electronic Technology, Chengdu, China; Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA","IEEE Geoscience and Remote Sensing Letters","10 Jan 2022","2022","19","","1","5","Recently, graph convolutional networks (GCNs) have been developed to explore the spatial relationship between pixels, achieving better classification performance of hyperspectral images (HSIs). However, these methods fail to sufficiently leverage the relationship between spectral bands in HSI data. As such, we propose an adaptive cross-attention-driven spatial–spectral graph convolutional network (ACSS-GCN), which is composed of a spatial GCN (Sa-GCN) subnetwork, a spectral GCN (Se-GCN) subnetwork, and a graph cross-attention fusion module (GCAFM). Specifically, Sa-GCN and Se-GCN are proposed to extract the spatial and spectral features by modeling the correlations between spatial pixels and between spectral bands, respectively. Then, by integrating attention mechanism into information aggregation of the graph, the GCAFM, including three parts, i.e., the spatial graph attention block, the spectral graph attention block, and the fusion block, is designed to fuse the spatial and spectral features, and suppress noise interference in Sa-GCN and Se-GCN. Moreover, the idea of the adaptive graph is introduced to explore an optimal graph through backpropagation during the training process. Experiments on two HSI datasets show that the proposed method achieves better performance than other classification methods.","1558-0571","","10.1109/LGRS.2021.3131615","National Natural Science Foundation of China(grant numbers:61871335,62001437); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628098","Attention mechanism;feature extraction;graph convolutional networks (GCNs);hyperspectral image (HSI) classification","Feature extraction;Correlation;Data preprocessing;Geoscience and remote sensing;Fuses;Deep learning;Convolution","adaptive signal processing;backpropagation;convolutional neural nets;feature extraction;geophysical image processing;graph theory;hyperspectral imaging;image classification;image fusion;remote sensing","spectral bands;adaptive cross-attention-driven spatial-spectral graph convolutional network;spatial GCN subnetwork;Sa-GCN;spectral GCN subnetwork;Se-GCN;graph cross-attention fusion module;spatial feature extraction;spectral feature extraction;spatial pixels;spatial graph attention block;spectral graph attention block;ACSS-GCN;backpropagation;hyperspectral image classification","","10","","17","IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"An Optimal Use of SCE-UA Method Cooperated With Superpixel Segmentation for Pansharpening","H. Hallabia; H. Hamam; A. B. Hamida","Advanced Technologies for Medicine and Signal, University of Sfax, Sfax, Tunisia; Advanced Technologies for Medicine and Signal, University of Sfax, Sfax, Tunisia; Faculty of Engineering, University of Moncton, Moncton, NB, Canada","IEEE Geoscience and Remote Sensing Letters","27 Aug 2021","2021","18","9","1620","1624","Pansharpening is achieved by inferring spatial details derived from a PANchromatic (PAN) image into its corresponding expanded multispectral (MS) bands. In this letter, we propose to apply an adaptive superpixel-based injection scheme that modulates the PAN details through an optimization procedure. Optimal injection coefficients can be locally estimated by using the shuffled complex evolution developed in the University of Arizona (SCE-UA) algorithm over multiple local segments (i.e., superpixels) resulting from the simple linear iterative clustering (SLIC) method. The performance of the proposed approach is assessed using degraded and real data sets acquired from WorldView-3 and WorldView-4 satellites. Experimental results show the suitability of the proposed adaptive injection scheme compared with other state-of-the-art pansharpening methods in terms of spatial and spectral qualities.","1558-0571","","10.1109/LGRS.2020.3004320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127516","Local injection gains;pansharpening;shuffled complex evolution developed at University of Arizona (SCE-UA) method;superpixel segmentation;WorldView-3/-4 images","Image segmentation;Task analysis;Clustering algorithms;Optimization;Satellites;Remote sensing","geophysical image processing;image fusion;image resolution;image segmentation;iterative methods;pattern clustering;remote sensing;spectral analysis","SCE-UA method cooperated;superpixel segmentation;spatial details;PANchromatic image;corresponding expanded multispectral bands;MS;adaptive superpixel-based injection scheme;PAN details;optimization procedure;optimal injection coefficients;shuffled complex evolution;multiple local segments;simple linear iterative clustering method;WorldView-3;WorldView-4 satellites;adaptive injection scheme;state-of-the-art pansharpening methods;spatial qualities;spectral qualities","","5","","19","IEEE","29 Jun 2020","","","IEEE","IEEE Journals"
"Fusformer: A Transformer-Based Fusion Network for Hyperspectral Image Super-Resolution","J. -F. Hu; T. -Z. Huang; L. -J. Deng; H. -X. Dou; D. Hong; G. Vivone","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Science, Xihua University, Chengdu, China; Key Laboratory of Computational Optical Imaging Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Institute of Methodologies for Environmental Analysis, National Research Council, Tito Scalo, Italy","IEEE Geoscience and Remote Sensing Letters","8 Aug 2022","2022","19","","1","5","Hyperspectral image super-resolution (HISR) is to fuse a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI), aiming to obtain a high-resolution hyperspectral image (HR-HSI). Recently, various convolution neural network (CNN)-based techniques have been successfully applied to address the HISR problem. However, these methods generally only consider the relation of a local neighborhood by convolution kernels with a limited receptive field, thus ignoring the global relationship in a feature map. In this letter, we design a Transformer-based architecture (called Fusformer) for the HISR problem, which is the first attempt to apply the Transformer architecture to this task to the best of our knowledge. Because of the excellent ability of feature representations, especially by the self-attention (SA) in the Transformer, our approach can globally explore the intrinsic relationship within features. Considering the specific HISR problem, since the LR-HSI holds the primary spectral information, our method estimates the spatial residual between the upsampled low-resolution multispectral image (LR-MSI) and the desired HR-HSI, reducing the burden of training the whole data in a smaller mapping space. Various experiments show that our approach outperforms current state-of-the-art (SOTA) HISR methods. The code is available at https://github.com/J-FHu/Fusformer.","1558-0571","","10.1109/LGRS.2022.3194257","National Natural Science Foundation of China(grant numbers:12171072,61702083,61876203); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216); National Key Research and Development Program of China(grant numbers:2020YFA0714001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9841513","Hyperspectral image super-resolution (HISR);image fusion;remote sensing;Transformer","Transformers;Hyperspectral imaging;Task analysis;Computer architecture;Training;Superresolution;Feature extraction","geophysical image processing;hyperspectral imaging;image fusion;image representation;image resolution;neural nets","Transformer-based fusion network;hyperspectral image super-resolution;low-resolution hyperspectral image;high-resolution multispectral image;Transformer-based architecture;Transformer architecture;specific HISR problem;LR-HSI;low-resolution multispectral image;current state-of-the-art HISR methods","","2","","37","IEEE","27 Jul 2022","","","IEEE","IEEE Journals"
"Ensemble and Random RX With Multiple Features Anomaly Detector for Hyperspectral Image","X. Yang; X. Huang; M. Zhu; S. Xu; Y. Liu","School of Information Engineering, Guangdong University of Technology, Guangzhou, China; School of Information Engineering, Guangdong University of Technology, Guangzhou, China; School of Information Engineering, Guangdong University of Technology, Guangzhou, China; School of Information Engineering, Guangdong University of Technology, Guangzhou, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Geoscience and Remote Sensing Letters","18 May 2022","2022","19","","1","5","Hyperspectral anomaly detection (HAD) has always been a hot topic in hyperspectral image (HSI) processing. The Reed-Xiaoli (RX) detector is one of the most widely used methods for HAD. However, most improved versions of RX are usually only improved from the view of reducing the contamination of anomaly pixels on the background modeling. Considering the rich spatial information of HSI and the influence of anomaly pixels on background modeling, we propose an ensemble and random RX with multiple features (ERRX MFs) anomaly detector. First, the spectral, Gabor, extended morphological profile (EMP), and extended multiattribute profile (EMAP) four features are extracted. Then, random subsampling is used on RX to obtain multiple detection results for each feature. Finally, the detection results of these four features are further fused to the final detection result by ensemble learning. Experimental results on four real hyperspectral datasets validate that our proposed method can achieve more satisfactory results compared with other state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3167758","Key Laboratory open Foundation(grant numbers:WDZ20215250118); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011141); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2018B010115001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9758719","Anomaly detection;ensemble;hyperspectral image (HSI);multiple features;random","Feature extraction;Detectors;Anomaly detection;Hyperspectral imaging;Geoscience and remote sensing;Covariance matrices;Sensors","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);object detection;remote sensing","multiple detection results;final detection result;ensemble learning;hyperspectral datasets;multiple features anomaly detector;hyperspectral anomaly detection;hyperspectral image processing;Reed-Xiaoli detector;improved versions;anomaly pixels;background modeling;ensemble RX;random RX;extended morphological profile;multiattribute profile;random subsampling","","1","","21","IEEE","18 Apr 2022","","","IEEE","IEEE Journals"
"Quantum Processing in Fusion of SAR and Optical Images for Deep Learning: A Data-Centric Approach","S. R. Majji; A. Chalumuri; R. Kune; B. S. Manoj","Indian Institute of Space Science and Technology, Thiruvananthapuram, India; Indian Institute of Space Science and Technology, Thiruvananthapuram, India; Advanced Data Processing Research Institute, Hyderabad, India; Indian Institute of Space Science and Technology, Thiruvananthapuram, India","IEEE Access","19 Jul 2022","2022","10","","73743","73757","Deep learning techniques are very prominent in processing remotely sensed synthetic aperture radar (SAR) images for real-time, high-impact applications, such as image classification, object detection, and semantic segmentation. The accuracy of deep learning models, such as convolutional neural networks (CNNs), depends on the quality of the input data. Compared to the model-centric approach, where the model parameters are optimized during training, the data-centric approach can enhance the performance accuracy as data quality is improved before training the models. Improving the data quality of SAR images is challenging as SAR image properties are different from optical images. Image fusion techniques proved to enhance the quality of SAR images when combined with optical images. Many fusion techniques exist for combining SAR and optical images in the classical domain. This paper proposes a novel approach to using quantum computing for the image fusion of SAR and optical images. Eight different quantum techniques are used to process and fuse the images. We designed and created a dataset for land-use classification by collecting data using the Google Earth Engine. The quality metric measurements show that the quality of SAR images has improved by using the proposed quantum processing techniques. In addition, performance evaluation of the deep learning CNNs on the dataset was carried out for all quantum processing techniques. Our approach improved the classification accuracy from 82.64%, with only SAR images for training, to 95.36% using the proposed image fusion techniques.","2169-3536","","10.1109/ACCESS.2022.3189474","IIT Palakkad Technology IHub Foundation Doctoral Fellowship(grant numbers:IPTIF/HRD/DF/032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9819905","Quantum processing;image fusion;deep learning;image classification;synthetic aperture radar (SAR) imagery;optical imagery","Radar polarimetry;Synthetic aperture radar;Optical sensors;Optical imaging;Adaptive optics;Data models;Deep learning","image classification;image fusion;image representation;image segmentation;learning (artificial intelligence);neural nets;object detection;radar imaging;remote sensing;synthetic aperture radar","optical images;data-centric approach;deep learning techniques;synthetic aperture radar images;image classification;deep learning models;model-centric approach;data quality;SAR images;SAR image properties;image fusion techniques;different quantum techniques;quantum processing techniques;deep learning CNNs","","1","","55","CCBY","7 Jul 2022","","","IEEE","IEEE Journals"
"High-Order Coupled Fully Connected Tensor Network Decomposition for Hyperspectral Image Super-Resolution","D. Jin; J. Liu; J. Yang; Z. Wu","Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","5 Oct 2022","2022","19","","1","5","Hyperspectral image (HSI) super-resolution addresses the problem of fusing a low-resolution HSI (LR-HSI) and a high-resolution multispectral image (HR-MSI) to produce a high-resolution HSI (HR-HSI). Tensor analysis has been proven to be an efficient method for HSI processing. However, the existing tensor-based methods of HSI super-resolution (HSI-SR) like the tensor train and tensor ring decomposition only establish an operation between adjacent two factors and are highly sensitive to the permutation of tensor modes, leading to an inadequate and inflexible representation. In this letter, we propose a novel method for HSI-SR by utilizing the specific properties of high-order tensors in fully-connected tensor network decomposition (FCTN). The proposed method first tensorizes the target HR-HSI into a high-order tensor that has multiscale spatial structures. Then, a coupled FCTN model is proposed to fuse the corresponding high-order tensors of LR-HSI and HR-MSI. Moreover, a weighted-graph regularization (WGR) is imposed on the spectral core tensors to preserve spectral information. In the proposed model, the superiorities of the FCTN lie in the outstanding capability for characterizing adequately the intrinsic correlations between any two modes of tensors and the essential invariance for transposition. Experimental results on three datasets show the effectiveness of the proposed approach as compared to other HSI-SR methods.","1558-0571","","10.1109/LGRS.2022.3207548","National Natural Science Foundation of China(grant numbers:62071204); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201338); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894417","Fully-connected tensor network decomposition (FCTN);high-order tensor;hyperspectral image (HSI)","Tensors;Hyperspectral imaging;Superresolution;Correlation;Imaging;Geoscience and remote sensing;Fuses","geophysical image processing;graph theory;hyperspectral imaging;image fusion;image reconstruction;image representation;image resolution;remote sensing;tensors","hyperspectral image super-resolution;low-resolution HSI;LR-HSI;high-resolution multispectral image;high-resolution HSI;tensor analysis;HSI processing;existing tensor-based methods;HSI super-resolution;tensor train;tensor ring decomposition;tensor modes;high-order tensor;fully-connected tensor network decomposition;target HR-HSI;spectral core tensors;HSI-SR methods","","","","23","IEEE","16 Sep 2022","","","IEEE","IEEE Journals"
"MC-JAFN: Multilevel Contexts-Based Joint Attentive Fusion Network for Pansharpening","Z. Xiang; L. Xiao; W. Liao; W. Philips","Image Processing and Interpretation (IPI) Research Group at imec and Ghent University, Ghent, Belgium; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; IPI Research Group at imec and Ghent University, Ghent, Belgium; IPI Research Group at imec and Ghent University, Ghent, Belgium","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Pansharpening refers to a spatial–spectral contexts fusion procedure to produce high-quality multispectral (MS) images by retaining the fine spatial resolution of the panchromatic (PAN) images and the high spectral content of the MS images. This letter presents a novel end-to-end dual-branch deep learning-based fusion framework, exploiting the network to extract spatial and spectral contexts progressively in two separate branches level by level. For each level contexts extraction layer, a dual-branch weighted attentive fusion module is integrated to boost the important contexts aggregation and details injection while suppressing unimportant ones. Experimental results on two real datasets show that our method outperforms state-of-the-art methods in both objective metrics and image quality by visual appearance.","1558-0571","","10.1109/LGRS.2021.3099966","National Natural Science Foundation of China(grant numbers:61871226,61571230); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); Fundamental Research Funds for the Central Universities(grant numbers:30918011104); National Major Research Plan of China(grant numbers:2016YFF0103604); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507001","Attention mechanism;data fusion;deep learning;multilevel;pansharpening;remote sensing","Feature extraction;Pansharpening;Spatial resolution;Sensors;Training;Image reconstruction;Neural networks","feature extraction;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing;sensor fusion","contexts-based joint attentive fusion network;pansharpening;spatial-spectral contexts fusion procedure;high-quality multispectral images;fine spatial resolution;panchromatic images;high spectral content;MS images;end-to-end dual-branch;deep learning-based fusion framework;spatial contexts;separate branches level;extraction layer;dual-branch weighted attentive fusion module;important contexts aggregation;details injection;objective metrics;image quality;MC-JAFN","","4","","18","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"SARF: A Simple, Adjustable, and Robust Fusion Method","X. Meng; G. Yang; F. Shao; W. Sun; H. Shen; S. Li","College of Electrical and Information Engineering, Hunan University, Changsha, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; School of Resources and Environmental Sciences, Wuhan University, Wuhan, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Pansharpening aims to sharpen a low spatial resolution (LR) multispectral (MS) image using a high spatial resolution (HR) panchromatic (PAN) image to obtain the HR MS image. Though large numbers of pansharpening methods have been proposed, and many advanced methods have shown high quantitative results, few of them are widely used in real applications. This may be attributed to their instability for different images with different ground surface features, or the complexity to be implemented and the time-consuming process for some state-of-the-art methods. In this letter, we proposed a simple, adjustable, and robust fusion (SARF) method. In the proposed method, a spatial-spectral coenhanced strategy was proposed, and several details of the proposed fusion model were specifically designed for the “simple, adjustable, robust” features. It was tested and verified by four-band and eight-band MS images based on reduced resolution (RR) and full resolution (FR) experiments. The experimental results demonstrated the promising spatial visuality of the proposed method, and the spectral fidelity was more robust than most of component substitution (CS)-based and multiresolution analysis (MRA)-based methods.","1558-0571","","10.1109/LGRS.2021.3060095","National Natural Science Foundation of China(grant numbers:41801252); Fellowship of China Postdoctoral Science Foundation(grant numbers:2020M672490); Natural Science Foundation of Ningbo City(grant numbers:2019A610098); K. C.Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9361993","Fusion;multispectral (MS);panchromatic (PAN);pansharpening;remote sensing","Pansharpening;Spatial resolution;Image resolution;Adaptation models;Radiometry;Context modeling;Training","geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion","adjustable features;robust features;eight-band MS images;promising spatial visuality;SARF;low spatial resolution multispectral image;high spatial resolution panchromatic image;HR MS image;high quantitative results;different ground surface features;time-consuming process;spatial-spectral coenhanced strategy;fusion model;simple features","","4","","23","IEEE","24 Feb 2021","","","IEEE","IEEE Journals"
"A Hardware Accelerator for Onboard Spatial Resolution Enhancement of Hyperspectral Images","F. Viel; W. D. Parreira; A. A. Susin; C. A. Zeferino","Laboratory of Embedded and Distributed Systems (LEDS), University of Vale do Itajaí, Itajaí, Brazil; Laboratory of Embedded and Distributed Systems (LEDS), University of Vale do Itajaí, Itajaí, Brazil; Laboratory of Embedded and Distributed Systems (LEDS), University of Vale do Itajaí, Itajaí, Brazil; Laboratory of Embedded and Distributed Systems (LEDS), University of Vale do Itajaí, Itajaí, Brazil","IEEE Geoscience and Remote Sensing Letters","27 Sep 2021","2021","18","10","1796","1800","Hyperspectral images (HSIs) are the images that have a high spectral resolution with hundreds of bands. HSIs are widely used in the precise identification and classification of the materials and surfaces. However, they have a low spatial resolution caused by the limited capacity of the sensors responsible for capturing the images. In view of this, the preprocessing techniques are used to increase the spatial resolution of the HSIs and enable greater precision in the subsequent processing stages. This letter presents the development of a hardware accelerator specially designed to process a pansharpening algorithm that fuses the hyperspectral and panchromatic images to produce a high spatial resolution HSI. The experimental results demonstrate that the processor is energy-efficient and can be used for onboard processing in a small spacecraft, such as the satellites.","1558-0571","","10.1109/LGRS.2020.3009019","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES), Brazil—the Brazilian Federal Agency for Support and Evaluation of Graduate Education—Finance(grant numbers:Code 001); Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq), Brazil—the Brazilian National Council for Scientific and Technological Development—Processes(grant numbers:315287/2018-7,436982/2018-8); Centre Spatial Universitaire de Montpellier (CSUM), France – University Space Center of Montpellier, France; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146516","Electromagnetic and remote sensing;field-programmable gate array (FPGA);hardware (HW) acceleration;hyperspectral image (HSI) processing;spatial resolution enhancement","Spatial resolution;Hyperspectral imaging;Fuses;Computer architecture;Hardware","geophysical image processing;geophysical techniques;hyperspectral imaging;image enhancement;image fusion;image resolution;image sensors;remote sensing","hardware accelerator;onboard spatial resolution enhancement;hyperspectral images;HSIs;high spectral resolution;precise identification;classification;low spatial resolution;greater precision;subsequent processing stages;panchromatic images;high spatial resolution HSI;onboard processing","","","","26","IEEE","23 Jul 2020","","","IEEE","IEEE Journals"
"A Variational Pansharpening Method Based on Gradient Sparse Representation","X. Tian; Y. Chen; C. Yang; X. Gao; J. Ma","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; College of Computer and Information Science, Fujian Agriculture and Forestry University, Fuzhou, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Signal Processing Letters","17 Jul 2020","2020","27","","1180","1184","By exploiting the gradient similarity between multispectral (MS) and panchromatic (PAN) images, a variational pansharpening method based on gradient sparse representation is proposed, based on the observation that the gradients of corresponding MS and PAN images with different resolutions have the similar sparse coefficients under certain specific dictionaries. By adding a data fidelity term to preserve the spectral information, an optimization model is constructed as a minimization problem of an energy function. The problem can be solved by the gradient descent method efficiently. Experiments on different satellite data reveal that the proposed method outperforms the state-of-the-art methods in terms of visual effect and objective quality analysis.","1558-2361","","10.1109/LSP.2020.3007325","National Natural Science Foundation of China(grant numbers:61971315,61773295); Natural Science Foundation of Hubei Province(grant numbers:2018CFB435); Fundamental Research Funds for the Central Universities(grant numbers:2042018kf1009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9133492","Pansharpening;variational model;gradient sparse representation;remote sensing","Dictionaries;Spatial resolution;Optimization;Distortion;Image reconstruction","geophysical image processing;gradient methods;image fusion;image reconstruction;image representation;image resolution;minimisation;optimisation;remote sensing","similar sparse coefficients;gradient similarity;gradient sparse representation;variational pansharpening method;different satellite data;gradient descent method;data fidelity term","","28","","27","IEEE","6 Jul 2020","","","IEEE","IEEE Journals"
"A Two-Stream Multiscale Deep Learning Architecture for Pan-Sharpening","J. Wei; Y. Xu; W. Cai; Z. Wu; J. Chanussot; Z. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; GIPSA-lab, CNRS, Grenoble INP, Universite Grenoble Alpes, Grenoble, France; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; GIPSA-lab, CNRS, Grenoble INP, Universite Grenoble Alpes, Grenoble, France; Inria, CNRS, Grenoble INP, LJK, Universite Grenoble Alpes, Grenoble, France; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Sep 2020","2020","13","","5455","5465","Pan-sharpening, which fuses the high-resolution panchromatic (PAN) image and the low-resolution multispectral image (MSI), is a hot topic in remote sensing. Recently, deep learning technology has been successfully applied in pan-sharpening. However, the existing methods ignore that the MSI and PAN image are at different resolutions and use the same networks to extract features of the two images. To address this problem, we propose a two-stream deep learning architecture, called coupled multiscale convolutional neural network, for pan-sharpening. The proposed network has three components, feature extraction subnetworks, fusion layer, and super-resolution subnetwork. In the feature extraction subnetworks, two subnetworks are used to extract the features of the MSI and PAN image separately. Different sizes of convolutional kernels are used in the first layers due to the different spatial resolutions. Thus, the source images are mapped to the similar scale. Then a multiscale asymmetric convolution factorization is used to extract features at different scales. In the fusion layer, the two feature extraction subnetworks are coupled. Features at the same scale are first summed, and then the features of all scales are concatenated as one feature map. At last, a super-resolution subnetwork is used to generate the high-resolution MSI. Experimental results on both synthetic and real data sets demonstrate that the proposed method outperforms the other state-of-the-art pan-sharpening methods.","2151-1535","","10.1109/JSTARS.2020.3021074","National Natural Science Foundation of China(grant numbers:61701238,61772274,61471199,61976117,11431015,61501241,61671243,61802190); Jiangsu Provincial Natural Science Foundation of China(grant numbers:BK20170858,BK20180018,BK20191409); Fundamental Research Funds for the Central Universities(grant numbers:30919011234,30917015104,30919011103,30919011402); China Postdoctoral Science Foundation(grant numbers:2017M611814,2018T110502); Jiangsu Province Postdoctoral Science Foundation(grant numbers:1701148B); University Natural Science Fund of Jiangsu Province(grant numbers:19KJA360001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185040","Convolutional neural network (CNN);image fusion;multiscale;pan-sharpening","Feature extraction;Deep learning;Neural networks;Kernel;Spatial resolution;Convolution","convolutional neural nets;feature extraction;image enhancement;image fusion;image resolution;learning (artificial intelligence);remote sensing","feature extraction subnetworks;fusion layer;super-resolution subnetwork;PAN image;source images;multiscale asymmetric convolution factorization;feature map;high-resolution MSI;pan-sharpening methods;two-stream multiscale deep learning architecture;high-resolution panchromatic image;low-resolution multispectral image;deep learning technology;coupled multiscale convolutional neural network","","14","","46","CCBY","2 Sep 2020","","","IEEE","IEEE Journals"
"Land-Use Classification With Compressive Sensing Multifeature Fusion","M. L. Mekhalfi; F. Melgani; Y. Bazi; N. Alajlan","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia; Department of Computer Engineering, College of Computer and Information Sciences, King Saud University, Riyadh, Saudi Arabia","IEEE Geoscience and Remote Sensing Letters","7 Aug 2015","2015","12","10","2155","2159","In this letter, we formulate a land-use (LU) classification problem within a compressive sensing (CS) fusion framework. CS aims at providing a compact representation form after a given query image has been processed with an opportune feature extraction type. In particular, residuals are generated from the image reconstruction with dictionaries associated with the available set of possible LUs and gathered to form a single-feature image pattern. The patterns obtained from different types of features are then fused to provide the final LU estimate. Two simple fusion strategies are adopted for such purpose. As demonstrated by experiments ran on the basis of a public benchmark database, the proposed method can achieve substantial classification accuracy gains over reference methods.","1558-0571","","10.1109/LGRS.2015.2453130","Deanship of Scientific Research of the King Saud University; International Research Group(grant numbers:IRG14-20); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7172465","Compressive sensing (CS);cooccurrence of adjacent local binary patterns (CoALBP);data fusion;gradient local autocorrelations (GLAC);histogram of oriented gradients (HOG);land-use (LU) classification;Compressive sensing (CS);cooccurrence of adjacent local binary patterns (CoALBP);data fusion;gradient local autocorrelations (GLAC);histogram of oriented gradients (HOG);land-use (LU) classification","Remote sensing;Probes;Dictionaries;Image reconstruction;Accuracy;Histograms;Matching pursuit algorithms","compressed sensing;feature extraction;geophysical image processing;geophysical techniques;image classification;image fusion;image reconstruction;land use","land use classification;compressive sensing multifeature fusion;feature extraction type;image reconstruction;single-feature image pattern;land use estimation;image fusion strategy;public benchmark database;substantial classification accuracy","","66","","23","IEEE","29 Jul 2015","","","IEEE","IEEE Journals"
"Hyperspectral Image Superresolution via Structure-Tensor-Based Image Matting","H. Gao; G. Zhang; M. Huang","Key Laboratory of Computational Optics Imaging Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Opto-Electronics, University of Chinese Academy of Sciences, Beijing, China; School of Opto-Electronics, University of Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Aug 2021","2021","14","","7994","8007","Hyperspectral (HS) imaging has achieved breakthroughs in many applications, such as remote sensing and object recognition. However, the spatial resolution of HS images is still insufficient due to the limitations of sensor technology and cost. In this article, we propose an HS image superresolution method that combines low-resolution (LR) HS images and high-resolution (HR) panchromatic (PAN) images. To exploit the spectral signatures in the LR-HS images while introducing details from the HR-PAN images during the image fusion procedure, an image matting model is used to fuse the original LR-HS images and the HR-PAN images. Specifically, to preserve the spectral components during the fusion procedure, two different alpha channels in the image matting model are generated based on the HS and PAN image structure tensors, which suppress spectral distortion and improve the quality of the reconstructed HR-HS image. Experimental results based on public datasets demonstrate the advantage of our proposed method in both preserving spectral information and enhancing HS image spatial resolution.","2151-1535","","10.1109/JSTARS.2021.3102579","Key Program of Marine Economy Development Special Foundation of the Department of Natural Resources of Guangdong Province(grant numbers:[2020]012); Chinese Academy of Sciences(grant numbers:Y9F0600Z2F); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9508188","Hyperspectral (HS) image;image matting;structure tensor;superresolution","Hyperspectral imaging;Tensors;Image reconstruction;Superresolution;Imaging;Spatial resolution;Distortion","distortion;geophysical image processing;hyperspectral imaging;image enhancement;image fusion;image reconstruction;image resolution;remote sensing;tensors","hyperspectral image superresolution;HS image superresolution method;low-resolution HS images;high-resolution panchromatic images;HR-PAN images;spectral information preservation;spectral signature;original LR-HS image fusion;spectral components;alpha channels;spectral distortion suppression;reconstructed HR-HS image quality;HS image spatial resolution enhancement;structure-tensor-based image matting model","","1","","53","CCBY","5 Aug 2021","","","IEEE","IEEE Journals"
"Processing of Extremely High-Resolution LiDAR and RGB Data: Outcome of the 2015 IEEE GRSS Data Fusion Contest–Part A: 2-D Contest","M. Campos-Taberner; A. Romero-Soriano; C. Gatta; G. Camps-Valls; A. Lagrange; B. Le Saux; A. Beaupère; A. Boulch; A. Chan-Hon-Tong; S. Herbin; H. Randrianarivo; M. Ferecatu; M. Shimoni; G. Moser; D. Tuia","Universitat de València, Valencia, Spain; Universitat de Barcelona, Barcelona, Spain; Universitat Autònoma de Barcelona, Barcelona, Spain; Universitat de València, Valencia, Spain; Office National d’Etudes et de Recherches Aérospatiales—The French Aerospace Lab, Palaiseau, France; Office National d’Etudes et de Recherches Aérospatiales—The French Aerospace Lab, Palaiseau, France; Office National d’Etudes et de Recherches Aérospatiales—The French Aerospace Lab, Palaiseau, France; Office National d’Etudes et de Recherches Aérospatiales—The French Aerospace Lab, Palaiseau, France; Office National d’Etudes et de Recherches Aérospatiales—The French Aerospace Lab, Palaiseau, France; Office National d’Etudes et de Recherches Aérospatiales—The French Aerospace Lab, Palaiseau, France; Office National d’Etudes et de Recherches Aérospatiales—The French Aerospace Lab, Palaiseau, France; Conservatoire National des Arts et Metiers – Cedric, Paris, France; Signal and Image Centre, Department of Electrical Engineering, Royal Military Academy, Brussels, Belgium; Department of Electrical, Electronic, Telecommunications Engineering and Naval Architecture, University of Genoa, Genoa, Italy; Department of Geography, University of Zurich, Zurich, Switzerland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","12","5547","5559","In this paper, we discuss the scientific outcomes of the 2015 data fusion contest organized by the Image Analysis and Data Fusion Technical Committee (IADF TC) of the IEEE Geoscience and Remote Sensing Society (IEEE GRSS). As for previous years, the IADF TC organized a data fusion contest aiming at fostering new ideas and solutions for multisource studies. The 2015 edition of the contest proposed a multiresolution and multisensorial challenge involving extremely high-resolution RGB images and a three-dimensional (3-D) LiDAR point cloud. The competition was framed in two parallel tracks, considering 2-D and 3-D products, respectively. In this paper, we discuss the scientific results obtained by the winners of the 2-D contest, which studied either the complementarity of RGB and LiDAR with deep neural networks (winning team) or provided a comprehensive benchmarking evaluation of new classification strategies for extremely high-resolution multimodal data (runner-up team). The data and the previously undisclosed ground truth will remain available for the community and can be obtained at http://www.grss-ieee.org/community/technical-committees/data-fusion/2015-ieee-grss-data-fusion-contest/. The 3-D part of the contest is discussed in the Part-B paper [1].","2151-1535","","10.1109/JSTARS.2016.2569162","MICINN; European Union Seventh Framework Program FP7/2007-2013(grant numbers:606983); LSA SAF; European Research Council(grant numbers:ERC-CoG-2014); SEDAL Consolidator(grant numbers:647423); Spanish Ministry of Economy and Competitiveness; LIFE-VISION(grant numbers:TIN2012-38102-C03-01); Swiss National Science Foundation(grant numbers:PP00P2-150593); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7536139","Deep neural networks;extremely high spatial resolution;image analysis and data fusion (IADF);landcover classification;LiDAR;multiresolution-;multisource-;multimodal-data fusion","Data integration;Laser radar;Spatial resolution;Three-dimensional displays;Remote sensing;Earth","image classification;image fusion;neural nets;optical radar;remote sensing by laser beam","extremely high-resolution multimodal data;classification strategy;deep neural networks;3D LiDAR point cloud;extremely high-resolution RGB image;IEEE GRSS data fusion contest;AD 2015;extremely high-resolution RGB data processing;extremely high-resolution LiDAR data processing","","74","","66","OAPA","8 Aug 2016","","","IEEE","IEEE Journals"
"Global Land-Cover Mapping With Weak Supervision: Outcome of the 2020 IEEE GRSS Data Fusion Contest","C. Robinson; K. Malkin; N. Jojic; H. Chen; R. Qin; C. Xiao; M. Schmitt; P. Ghamisi; R. Hänsch; N. Yokoya","AI for Good Research Lab, Microsoft Research, Redmond, WA, USA; Department of Mathematics, Yale University, New Haven, CT, USA; Microsoft Research, Redmond, WA, USA; Department of Civil, Environmental and Geodetic Engineering and the Environmental Science Graduate Program, The Ohio State University, Columbus, OH, USA; Department of Civil, Environmental and Geodetic Engineering, the Department of Electrical, and Computer Engineering, and the Translational Data Analytics Institute, The Ohio State University, Columbus, OH, USA; Department of Civil, Environmental and Geodetic Engineering, The Ohio State University, Columbus, OH, USA; Department of Geoinformatics, Munich University of Applied Sciences, München, Germany; Institute of Advanced Research in Artificial Intelligence, Vienna, Austria; German Aerospace Center, Weßling, Germany; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Mar 2021","2021","14","","3185","3199","This article presents the scientific outcomes of the 2020 Data Fusion Contest (DFC2020) organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2020 Contest addressed the problem of automatic global land-cover mapping with weak supervision, i.e., estimating high-resolution semantic maps while only low-resolution reference data are available during training. Two separate competitions were organized to assess two different scenarios: 1) high-resolution labels are not available at all; and 2) a small amount of high-resolution labels are available additionally to low-resolution reference data. In this article, we describe the DFC2020 dataset that remains available for further evaluation of corresponding approaches and report the results of the best-performing methods during the contest.","2151-1535","","10.1109/JSTARS.2021.3063849","Office of Naval Research(grant numbers:N000141712928); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369830","Convolutional neural networks (CNNs);deep learning;image analysis and data fusion;land-cover mapping;multimodal;random forests (RFs);weak supervision","Earth;Data integration;Remote sensing;Satellites;Training;Tensors;Synthetic aperture radar","geophysical image processing;geophysical techniques;image fusion;image resolution;land cover;remote sensing;terrain mapping","DFC2020 dataset;high-resolution labels;low-resolution reference data;high-resolution semantic maps;automatic global land-cover mapping;2020 IEEE GRSS Data Fusion Contest","","19","","41","CCBY","4 Mar 2021","","","IEEE","IEEE Journals"
"GF-4 Satellite and Automatic Identification System Data Fusion for Ship Tracking","Y. Liu; L. Yao; W. Xiong; Z. Zhou","School of Electronic Science, National University of Defense Technology, Changsha, China; Institute of Information Fusion, Naval Aeronautical and Astronautical University, Yantai, China; Institute of Information Fusion, Naval Aeronautical and Astronautical University, Yantai, China; School of Electronic Science, National University of Defense Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","21 Jan 2019","2019","16","2","281","285","GF-4 satellite, as the first geostationary orbit optical remote sensing satellite with medium resolution, has the abilities to point and stare at a particular sea area for maritime surveillance. The integration of GF-4 image sequence and automatic identification system (AIS) reports has the appealing potential to provide a better maritime situational awareness by tracking ships, especially noncooperative targets. The aim of this letter is to provide a track-level fusion architecture for GF-4 and AIS heterogeneous data based on the characteristics of different data sources to form a consolidated and comprehensive maritime picture. The proposed fusion strategy is tested in the specific area of the East China Sea, and experimental results verify the effectiveness of our method.","1558-0571","","10.1109/LGRS.2018.2869561","National Natural Science Foundation of China(grant numbers:91538201,61790551); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8472351","Automatic identification system (AIS);data fusion;GF-4 geostationary satellite;maritime surveillance","Marine vehicles;Artificial intelligence;Target tracking;Satellites;Data integration;Surveillance","artificial satellites;geophysical image processing;image fusion;image sequences;remote sensing;ships;target tracking","data sources;maritime picture;noncooperative targets;AIS heterogeneous data;track-level fusion architecture;maritime situational awareness;automatic identification system reports;GF-4 image sequence;maritime surveillance;geostationary orbit optical remote sensing satellite;GF-4 satellite;ship tracking;automatic identification system data fusion","","24","","15","IEEE","26 Sep 2018","","","IEEE","IEEE Journals"
"Hypersharpening by an NMF-Unmixing-Based Method Addressing Spectral Variability","S. E. Brezini; M. S. Karoui; F. Z. Benhalouche; Y. Deville; A. Ouamri","Université des Sciences et de la Technologie d’Oran, Oran, Algeria; Université des Sciences et de la Technologie d’Oran, Oran, Algeria; Université des Sciences et de la Technologie d’Oran, Oran, Algeria; Institut de Recherche en Astrophysique et Planétologie (IRAP), Université de Toulouse, UPS, CNRS, OMP, CNES, Toulouse, France; Université des Sciences et de la Technologie d’Oran, Oran, Algeria","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Hypersharpening consists in generating an unobservable high-spatial-resolution hyperspectral image by fusing an observed low-spatial-resolution hyperspectral image with an observed high-spatial-resolution panchromatic or multispectral one. The obtained image preserves the high spectral resolution of the first image and the high spatial resolution of the second one. Unlike standard hypersharpening methods that do not consider the spectral variability phenomenon, in this letter, a new approach, which addresses this phenomenon, is proposed for fusing hyperspectral and multispectral remote sensing images. This approach, linked to linear spectral unmixing methods, is based on an extension of nonnegative matrix factorization (NMF), namely the inertia-constrained pixel-by-pixel NMF (IP-NMF) algorithm. The developed fusion algorithm, called hyperspectral and multispectral data fusion based on IP-NMF (HMF-IPNMF), is applied to synthetic and real data sets. Experimental results clearly show that the developed fusion method yields sharpened hyperspectral images with higher spectral and spatial fidelities when compared to those provided by tested state-of-the-art methods that do not take spectral variability into account.","1558-0571","","10.1109/LGRS.2021.3072405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9410649","Data fusion;hyperspectral/multispectral imaging;hypersharpening;nonnegative matrix factorization (NMF);pansharpening;spectral variability","Hyperspectral imaging;Spatial resolution;Sensors;Covariance matrices;Uninterruptible power systems;Standards;Matrix decomposition","geophysical image processing;hyperspectral imaging;image fusion;image processing;image resolution;image sensors;matrix decomposition;remote sensing;sensor fusion","IP-NMF;fusion method;hyperspectral images;higher spectral fidelities;tested state-of-the-art methods;NMF-unmixing-based method addressing spectral variability;high-spatial-resolution hyperspectral image;low-spatial-resolution hyperspectral image;high spectral resolution;high spatial resolution;standard hypersharpening methods;spectral variability phenomenon;hyperspectral sensing images;multispectral remote sensing images;spectral unmixing methods;inertia-constrained pixel-by-pixel NMF algorithm;fusion algorithm;hyperspectral data fusion;multispectral data fusion","","4","","16","IEEE","22 Apr 2021","","","IEEE","IEEE Journals"
"Visual Saliency-Based Extended Morphological Profiles for Unsupervised Feature Learning of Hyperspectral Images","X. Liu; X. Yin; Y. Cai; M. Wang; Z. Cai; B. Huang","Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, China University of Geosciences, Wuhan, China; Beibu Gulf Big Data Resources Utilisation Laboratory, Beibu Gulf University, Qingzhou, China; China Unicom Hubei Branch, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","26 Oct 2020","2020","17","11","1963","1967","Classification of hyperspectral images (HSIs) by making full use of the spectral and the spatial information has become a research hotspot in the field of remote sensing technology. Aiming at the problems of information redundancy and low utilization of spatial information, this letter proposes a visual saliency-based extended morphological profile (VS-EMP) scheme. First, the morphological features are extracted by the EMP from the HSIs on several principal components. Second, the local binary pattern (LBP) is performed to extract the texture features from morphological scenes. Third, saliency features are captured according to the texture features in an approach of Boolean mapping saliency (BMS). Finally, spectral-spatial features are constructed by feature fusion and are further used for the classification of the HSIs. A number of experiments are performed, including using different classifiers to verify the performance of the proposed scheme, comparing with related variant algorithms, comparing time with deep learning, and testing learning ability in the absence of labeled samples. Experimental results indicate that the proposed method is significantly superior to the previous methods.","1558-0571","","10.1109/LGRS.2019.2957851","National Nature Science Foundation of China(grant numbers:61603355,61773355,61973285,61873249); National Nature Science Foundation of Hubei Province, China(grant numbers:2018CFB528); Fundamental Research Funds for National University, China University of Geosciences (Wuhan)(grant numbers:CUGL17022,1910491T06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938660","Extended morphological profiles (EMP);feature extraction;hyperspectral image (HSI);visual saliency (VS)","Feature extraction;Hyperspectral imaging;Principal component analysis;Visualization;Geology;Task analysis","Boolean algebra;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image texture;remote sensing;unsupervised learning","visual saliency-based extended morphological profiles;unsupervised feature learning;hyperspectral image classification;HSIs;remote sensing;information redundancy;VS-EMP;morphological feature extraction;local binary pattern;texture features;morphological scenes;saliency features;Boolean mapping saliency;spectral-spatial features;feature fusion;classifiers","","4","","16","IEEE","23 Dec 2019","","","IEEE","IEEE Journals"
"TAL: Topography-Aware Multi-Resolution Fusion Learning for Enhanced Building Footprint Extraction","Y. Wu; L. Xu; Y. Chen; A. Wong; D. A. Clausi","Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada; Department of Systems Design Engineering, University of Waterloo, Waterloo, Canada","IEEE Geoscience and Remote Sensing Letters","1 Mar 2022","2022","19","","1","5","Automatic building footprint extraction from remote sensing imagery is a challenging task with important applications in geomatics and environmental science. Significant advances have been made in this field as a result of the emergence of deep convolutional neural networks (CNNs) designed for semantic segmentation. Although CNNs have demonstrated state-of-the-art performance in coarse annotation and identification of buildings, the accuracy of extracted building footprints is still insufficient for high-precision applications such as mapping and navigation. We propose the topography-aware multi-resolution fusion learning strategy tailored to the problem of enhanced building footprint extraction. More specifically, we introduce a topography-aware loss (TAL) for enhancing a deep CNN’s ability to learn heterogeneous building features for better boundary preservation during segmentation. We then incorporate the proposed TAL loss within a multi-resolution fusion architecture to boost high-resolution segmentation performance. Finally, we introduce a novel metric named average thresholded contour accuracy (tCA) which specifically measures the accuracy of segmentation boundaries. The experimental results on the SpaceNet buildings dataset show significant improvements in boundary integrity of extracted building footprints when compared with previously proposed methods. Hence, this method enables accurate boundary annotation toward automatic production of building footprint maps for high-precision applications.","1558-0571","","10.1109/LGRS.2022.3149709","Mitacs through the Mitacs Accelerate Program; Natural Sciences and Engineering Research Council of Canada (NSERC)(grant numbers:RGPIN-2017-04869,DGDND-2017-00078,RGPAS2017-50794,RGPIN-2019-06744); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706196","Boundary preservation;building footprint;deep learning;high-resolution segmentation","Buildings;Feature extraction;Training;Image segmentation;Convolution;Annotations;Spatial resolution","feature extraction;geophysical image processing;image fusion;image resolution;image segmentation;learning (artificial intelligence);neural nets;object detection;remote sensing","enhanced building footprint extraction;automatic building footprint extraction;remote sensing imagery;deep convolutional neural networks;CNNs;coarse annotation;extracted building footprints;high-precision applications;topography-aware multiresolution fusion learning strategy;topography-aware loss;heterogeneous building features;TAL loss;multiresolution fusion architecture;high-resolution segmentation performance;SpaceNet buildings;building footprint maps","","1","","24","IEEE","7 Feb 2022","","","IEEE","IEEE Journals"
"Assessment of Spatial–Spectral Feature-Level Fusion for Hyperspectral Target Detection","J. R. Kaufman; M. T. Eismann; M. Celenk","School of Electrical Engineering and Computer Science, Ohio University, Athens, OH, USA; Air Force Research Laboratory, WPAFB, OH, USA; School of Electrical Engineering and Computer Science, Ohio University, Athens, OH, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Aug 2015","2015","8","6","2534","2544","In this work, we assess the detection and classification of specially constructed targets in coincident airborne hyperspectral imagery (HSI) and high spatial resolution panchromatic imagery (HRI) in spectral, spatial, and joint spatial-spectral feature spaces. The target discrimination powers of the data-level and feature-level fusion of HSI and HRI are also directly compared in the spatial-spectral context using airborne imagery collected explicitly for this research. We show that in the case of Bobcat 2013 imagery, feature-level fusion of the HSI spectrum with spatial features derived from the coincident HRI data consistently results in fewer false alarms on the scene background as well as fewer misclassifications among the tested targets. Furthermore, this approach also outperforms schemes in which data-level fusion of the HSI and HRI imagery is performed prior to extracting spatial-spectral features.","2151-1535","","10.1109/JSTARS.2015.2420651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103293","Hyperspectral imagery (HSI);image fusion;material identification;pansharpening;spatial–spectral feature extraction;target identification;Hyperspectral imagery (HSI);image fusion;material identification;pansharpening;spatial–spectral feature extraction;target identification","Feature extraction;Niobium;Hyperspectral imaging;Object detection;Spatial resolution;Robustness;Noise measurement","feature extraction;hyperspectral imaging;image fusion;object detection","spatial-spectral feature-level fusion assessment;hyperspectral target detection;specially constructed target detection;HSI;hyperspectral imagery;high spatial resolution panchromatic imagery;HRI;joint spatial-spectral feature spaces;data-level fusion;airborne imagery;Bobcat 2013 imagery;spatial-spectral feature extraction","","14","","27","IEEE","7 May 2015","","","IEEE","IEEE Journals"
"Unambiguous Reconstruction for Multichannel Nonuniform Sampling SAR Signal Based on Image Fusion","L. Zhou; X. Zhang; Y. Wang; L. Li; L. Pu; J. Shi; S. Wei","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Access","27 Apr 2020","2020","8","","71558","71571","Multichannel signal processing in azimuth is a vital technique to enable a wide-swath Synthetic Aperture Radar (SAR) with high azimuth resolution. However, the multichannel high-resolution and wide-swath (HRWS) SAR system always suffers from the problem of the azimuth nonuniform sampling resulting in the image ambiguity, when it does not satisfy the uniform sampling condition. In this paper, to suppress the azimuth image ambiguity, we propose a novel unambiguous reconstruction method based on image fusion. During this reconstruction processing, the Back Projection (BP) algorithm is first utilized for SAR imaging to obtain the designed sub-images. Then, the reconstruction expression is derived as the summation of the sub-images weighted by the interpolation coefficient. This method integrates the reconstruction into the imaging process and the image fusion makes the procedure simple. In addition, the interpolation period, which affects the reconstruction image quality and efficiency, is further analyzed. Moreover, as the curved trajectory platform brings more challenges for the unambiguous reconstruction, the performance of the proposed method applied to the curved trajectory platform is studied. Finally, experimental results clearly verify the effectiveness of the proposed method for ambiguity suppression and demonstrate its applicability to the curved trajectory.","2169-3536","","10.1109/ACCESS.2020.2987196","National Basic Research Program of China (973 Program)(grant numbers:2017YFB0502700); High Resolution Earth Observation Youth Foundation(grant numbers:GFZX04061502); National Natural Science Foundation of China(grant numbers:61671113,61501098,61571099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9064772","High resolution and wide swath;nonuniform sampling;signal reconstruction;synthetic aperture radar (SAR)","Image reconstruction;Azimuth;Reconstruction algorithms;Interpolation;Antennas;Synthetic aperture radar;Nonuniform sampling","image fusion;image reconstruction;image resolution;image sampling;interpolation;radar imaging;synthetic aperture radar","curved trajectory platform;multichannel nonuniform sampling SAR signal;image fusion;multichannel signal processing;wide-swath Synthetic Aperture Radar;high azimuth resolution;multichannel high-resolution;wide-swath SAR system;azimuth nonuniform sampling;uniform sampling condition;azimuth image ambiguity;novel unambiguous reconstruction method;reconstruction processing;SAR imaging;reconstruction expression;imaging process;reconstruction image quality;interpolation coefficient;back projection algorithm","","2","","30","CCBY","13 Apr 2020","","","IEEE","IEEE Journals"
"Fisher Vector Encoding of Supervoxel-Based Features for Airborne LiDAR Data Classification","P. H. Akwensi; Z. Kang; J. Yang","Subcenter of International Cooperation and Research on Lunar and Planetary Exploration, Center of Space Exploration, Ministry of Education of The People’s Republic of China, Beijing, China; Subcenter of International Cooperation and Research on Lunar and Planetary Exploration, Center of Space Exploration, Ministry of Education of The People’s Republic of China, Beijing, China; Subcenter of International Cooperation and Research on Lunar and Planetary Exploration, Center of Space Exploration, Ministry of Education of The People’s Republic of China, Beijing, China","IEEE Geoscience and Remote Sensing Letters","26 Feb 2020","2020","17","3","504","508","Point cloud feature extraction as a classification task is crucial in maximizing the efficient downstream applicability of raw point clouds. With the goal of learning optimum features for efficient classification of a multi-class point cloud for downstream applications, this letter presents a supervoxelFisher vector (FV)-based approach for airborne light detection and ranging (LiDAR) data classification. In our approach, FV encoding is implemented to deduce compact global descriptors from aggregated supervoxels to establish a more descriptive and discriminative representation, transforming the low-level visual features into high-level semantic features. As a result, the proposed approach combines local and global feature properties through the quantization and aggregation of higher order statistics to harnesses their combined advantages for producing good classification results. Experiments were conducted on the international society for photogrammetry and remote sensing 3-D semantic labeling benchmark data set. Results indicate that the proposed approach is robust and efficient, attained the third best position with an overall accuracy of 81.79%, and ranked first with an F1-score of 72.31%.","1558-0571","","10.1109/LGRS.2019.2922308","National Natural Science Foundation of China(grant numbers:41872207); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8754737","Airborne light detection and ranging (LiDAR);classification;Fisher vectors;image fusion;supervoxels","Feature extraction;Three-dimensional displays;Semantics;Encoding;Visualization;Training;Shape","feature extraction;geophysical image processing;image classification;image coding;image representation;learning (artificial intelligence);optical radar;photogrammetry;quantisation (signal);remote sensing by laser beam","remote sensing 3-D semantic labeling benchmark data;Fisher vector encoding;supervoxel-based features;airborne LiDAR data classification;point cloud feature extraction;classification task;efficient downstream applicability;raw point clouds;optimum features;multiclass point cloud;downstream applications;supervoxelFisher vector-based approach;airborne light detection;FV encoding;compact global descriptors;aggregated supervoxels;descriptive representation;discriminative representation;low-level visual features;high-level semantic features;global feature properties","","3","","22","IEEE","3 Jul 2019","","","IEEE","IEEE Journals"
"CSAFNet: Channel Similarity Attention Fusion Network for Multispectral Pansharpening","S. Luo; S. Zhou; Y. Qi","key laboratory of Dependable Service Computing in Cyber Society, Ministry of Education, College of Computer Science, Chongqing University, Chongqing, China; key laboratory of Dependable Service Computing in Cyber Society, Ministry of Education, College of Computer Science, Chongqing University, Chongqing, China; key laboratory of Dependable Service Computing in Cyber Society, Ministry of Education, College of Computer Science, Chongqing University, Chongqing, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Multispectral (MS) pansharpening involves fusing a low-spatial-resolution MS image and its associated high-spatial-resolution panchromatic image. Recently, convolutional neural network (CNN)-based fusion models have been widely used in pansharpening domain, but most of them treat diversity features equally and also neglect the contribution of multilevel features, thereby impeding the representation ability of CNNs. To deal with these issues, we propose a novel channel similarity attention fusion network (CSAFNet) in this letter, where several channel attention residual dense blocks (CARDBs) are stacked to fully exploit discriminative features, and then the features produced by all CARDBs are combined via a multilevel feature fusion module. Such network enables the network to focus on more informative features and make full use of them. Both visual and quantitative assessments validate the superior performance of the proposed network over the current pansharpening methods with respect to spectral fidelity and spatial enhancement.","1558-0571","","10.1109/LGRS.2020.3040893","National Nature Science Foundation of China(grant numbers:61762025); Special Project of Basic Research and Frontier Exploration of Chongqing Natural Science Foundation(grant numbers:cstc2019jcyj-msxmx0657); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9290367","Channel attention;Convolutional neural network (CNN);Data fusion;Multispectral (MS) pansharpening","Training;Convolution;Feature extraction;Correlation;Spatial resolution;Tensors;Sensors","convolutional neural nets;geophysical image processing;hyperspectral imaging;image fusion;image resolution;remote sensing","CSAFNet;channel attention residual dense blocks;multilevel feature fusion module;spatial enhancement;multispectral pansharpening;high-spatial-resolution panchromatic image;convolutional neural network-based fusion models;spectral fidelity;channel similarity attention fusion network;CARDB;low-spatial-resolution MS image fusion;CNN-based fusion models","","1","","23","IEEE","10 Dec 2020","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and Panchromatic Images Using Generative Adversarial Network and Image Segmentation","W. Dong; Y. Yang; J. Qu; W. Xie; Y. Li","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","29 Dec 2021","2022","60","","1","13","Hyperspectral (HS) image fusion aims at integrating a panchromatic (PAN) image and an HS image, featuring the fused image with the spatial quality of the former and the spectral diversity of the latter. The classic fusion algorithm generally includes three consecutive procedures that are upsampling, detail extraction, and detail injection. In this article, we propose an HS and PAN image fusion method based on generative adversarial network and local estimation of injection gain. Instead of upsampling the HS image by classical interpolation techniques, a generative adversarial super-resolution network (GASN) is designed to obtain the interpolated HS image in the fusion framework. GASN establishes a spectral-information-based discriminator to conduct adversarial learning with the generator, so as to preserve the spectral information of the low-resolution HS image. An image segmentation-based injection gain estimation (ISGE) algorithm is subsequently proposed for HS and PAN images fusion. The injection gain is estimated over image segments obtained by a binary partition tree approach to improve the fusion performance. The proposed GASN and ISGE are implemented into two credible global estimation pansharpening methods, and experimental results prove the performance improvement of the proposed method. The proposed method is also compared with existing state-of-the-art methods, and experiments on several public databases demonstrate that the proposed method is competitive or superior to the state-of-the-art fusion methods.","1558-0644","","10.1109/TGRS.2021.3078711","National Defense Pre-Research Foundation; Higher Education Discipline Innovation Project(grant numbers:B08038); National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2021JQ-194,2021JQ-197); Fundamental Research Funds for the Central Universities(grant numbers:XJS210108,XJS210104); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110856); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437429","Details injection;hyperspectral (HS) fusion;image segment;injection gains","Estimation;Spatial resolution;Pansharpening;Image segmentation;Generators;Generative adversarial networks;Feature extraction","geophysical image processing;image fusion;image representation;image resolution;image sampling;image segmentation;interpolation;learning (artificial intelligence);neural nets;spectral analysis","spectral-information-based discriminator;adversarial learning;low-resolution HS image;image segmentation-based injection gain estimation algorithm;GASN;panchromatic image;hyperspectral image fusion;interpolation techniques;generative adversarial super-resolution network;interpolated HS image;HS image upsampling;PAN images fusion;global estimation pansharpening methods","","6","","43","IEEE","20 May 2021","","","IEEE","IEEE Journals"
"Characterizing Tree Species of a Tropical Wetland in Southern China at the Individual Tree Level Based on Convolutional Neural Network","Y. Sun; Q. Xin; J. Huang; B. Huang; H. Zhang","School of Geography and Planning, Sun Yat-Sen University, Guangzhou, China; School of Geography and Planning, Sun Yat-Sen University, Guangzhou, China; School of Geography and Planning, Sun Yat-Sen University, Guangzhou, China; Department of Geography and Resource Management, The Chinese University of Hong Kong, Shatin, Hong Kong; Department of Geography, The University of Hong Kong, Pokfulam, Hong Kong","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2020","2019","12","11","4415","4425","Classification of species at the individual tree level would be beneficial to many applications including forest landscape visualization, forest management, and biodiversity monitoring. This article develops a patch-based classification algorithm of individual tree species based on convolutional neural network. The individual trees are first detected using the local maximum method from the canopy height model, as derived from light detection and ranging (LiDAR) data. The detected individual trees are then cropped into patches for classification based on the tree apexes, and three spatial scale image patches are chosen for analysis and discussion. A modified ResNet50 deep network is further employed for the cropped individual tree patches classification. The patch-based method accounts for the contexture information of a tree and does not require the feature selection or the feature reduction processes. About 1388 training samples including Ficus microcarpa Linn. f., Delonix regia, Chorisia speciosa A.St.-Hil., Dimocarpus longan Lour., Musa nana Lour., Carica papaya, and Others (the other tree species except the above six) were collected from both field work and visual interpretation. Aerial images, LiDAR data, and Worldview images were used for the tree species classification. For 362 test tree samples, the results of patch size 64 achieve the best accuracies, and the proposed method outperforms the traditional machine learning method with the overall accuracy of 89.06% + 0.58% using aerial images only. Transferability Study to the Luhu Park also indicated the feasibility of our method. While challenges in individual tree detection and multisource data fusion remain, the solution shows the potential in characterizing tree species at the individual tree level using remote sensing data.","2151-1535","","10.1109/JSTARS.2019.2950721","National Natural Science Foundation of China(grant numbers:41801351,41875122); Fundamental Research Funds for the Central Universities(grant numbers:19lgpy44,17lgzd02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8905783","Aerial images;convolutional neural network (CNN);individual tree level;tree species classification;worldview","Vegetation;Feature extraction;Wetlands;Training;Laser radar;Convolutional neural networks","convolutional neural nets;crops;geophysical image processing;image classification;image fusion;learning (artificial intelligence);optical radar;remote sensing by laser beam;remote sensing by radar;trees (mathematics);vegetation mapping","individual trees;light detection;tree apexes;spatial scale image patches;modified ResNet50 deep network;cropped individual tree patches classification;patch-based method accounts;tree species classification;362 test tree samples;individual tree detection;multisource data fusion;characterizing tree species;individual tree level;convolutional neural network;patch-based classification algorithm;individual tree species","","14","","46","IEEE","19 Nov 2019","","","IEEE","IEEE Journals"
"ROS-Det: Arbitrary-Oriented Ship Detection in High Resolution Optical Remote Sensing Images via Rotated One-Stage Detector","M. Zhu; G. Hu; S. Li; H. Zhou; S. Wang; Y. Zhang; S. Yue","Graduate College, Air Force Engineering University, Xi’an, China; Air and Missile Defense College, Air Force Engineering University, Xi’an, China; Aeronautics Engineering College, Air Force Engineering University, Xi’an, China; Air and Missile Defense College, Air Force Engineering University, Xi’an, China; Air and Missile Defense College, Air Force Engineering University, Xi’an, China; Graduate College, Air Force Engineering University, Xi’an, China; Graduate College, Air Force Engineering University, Xi’an, China","IEEE Access","5 Apr 2021","2021","9","","50209","50221","To address these problems, namely, arbitrary orientations, various sizes, and dense distributions of ship detection, we propose an arbitrary-oriented ship detection method via rotated single-stage detector (ROS-Det), which integrates a feature pyramid network (FPN) based on an improved ResNet50, rotated anchors, a classification network, and a regression network together. Firstly, to improve robustness against various sizes of ships, the FPN is used to fusion multiscale convolutional feature maps. Through several tweaks, the improved ResNet50 can receive more information and reduce the computational cost. Secondly, for the purpose of arbitrary-oriented ship detection, rotated anchors, skew intersection over union (IoU), and skew non-maximum suppression (NMS) are introduced to RetinaNet. Then, on account of the disadvantages that the arbitrary-oriented object detection methods usually cause loss discontinuity problem, we improve the traditional smooth L1 loss function by introducing an IoU constant factor. Finally, based on several techniques such as data augmentation and transfer learning, we achieve ship detection on a public ship dataset HRSC2016. Through comparison experiments, we have analyzed and discussed the validity of our proposed ROS-Det, which achieves the state-of-the-art performance.","2169-3536","","10.1109/ACCESS.2021.3058386","Natural Science Foundation Research Project of Shaanxi Province, China(grant numbers:2020JM-345); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351927","Ship detection;feature pyramid network;data augmentation;transfer learning","Marine vehicles;Detectors;Feature extraction;Object detection;Transfer learning;Task analysis","convolution;feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);object detection;regression analysis;remote sensing;ships","public ship dataset HRSC2016;arbitrary-oriented object detection methods;fusion multiscale convolutional feature maps;ships;rotated anchors;improved ResNet50;feature pyramid network;rotated single-stage detector;arbitrary-oriented ship detection method;arbitrary orientations;rotated one-stage detector;high resolution optical remote sensing images;ROS-Det","","2","","34","CCBY","10 Feb 2021","","","IEEE","IEEE Journals"
"Multimodal Data Fusion Using Non-Sparse Multi-Kernel Learning With Regularized Label Softening","P. Wang; C. Qiu; J. Wang; Y. Wang; J. Tang; B. Huang; J. Su; Y. Zhang","Department of Medical Informatics, Nantong University, Nantong, China; Department of Medical Informatics, Nantong University, Nantong, China; Department of Medical Informatics, Nantong University, Nantong, China; Department of Medical Informatics, Nantong University, Nantong, China; Department of Medical Informatics, Nantong University, Nantong, China; Department of Medical Informatics, Nantong University, Nantong, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; Department of Medical Informatics, Nantong University, Nantong, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Jul 2021","2021","14","","6244","6252","Due to the need of practical application, multiple sensors are often used for data acquisition, so as to realize the multimodal description of the same object. How to effectively fuse multimodal data has become a challenge problem in different scenarios including remote sensing. Nonsparse multi-Kernel learning has won many successful applications in multimodal data fusion due to the full utilization of multiple Kernels. Most existing models assume that the nonsparse combination of multiple Kernels is infinitely close to a strict binary label matrix during the training process. However, this assumption is very strict so that label fitting has very little freedom. To address this issue, in this article, we develop a novel nonsparse multi-Kernel model for multimodal data fusion. To be specific, we introduce a label softening strategy to soften the binary label matrix which provides more freedom for label fitting. Additionally, we introduce a regularized term based on manifold learning to anti over fitting problems caused by label softening. Experimental results on one synthetic dataset, several UCI multimodal datasets and one multimodal remoting sensor dataset demonstrate the promising performance of the proposed model.","2151-1535","","10.1109/JSTARS.2021.3087738","National Natural Science Foundation of China(grant numbers:82072019); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201441); Jiangsu Post-doctoral Research Funding Program(grant numbers:2020Z020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9449986","Label softening;manifold learning;multi-Kernel learning;remote sensing;semantic-based multimodal fusion","Kernel;Matrix decomposition;Data integration;Softening;Fitting;Task analysis;Training","data acquisition;geophysical image processing;image fusion;learning (artificial intelligence);remote sensing","multimodal data fusion;data acquisition;multimodal description;strict binary label matrix;label fitting;UCI multimodal datasets;multimodal remoting sensor dataset;nonsparse multikernel learning;regularized label softening strategy","","2","","45","CCBY","9 Jun 2021","","","IEEE","IEEE Journals"
"Spectral–Temporal Fusion of Satellite Images via an End-to-End Two-Stream Attention With an Effective Reconstruction Network","T. Benzenati; Y. Kessentini; A. Kallel","LIMOSE Laboratory, University of Boumerdes, Boumerdes, Algeria; Laboratory of Signals, Systems, Artificial Intelligence and Networks Sfax, Digital Research Centre of Sfax, Sfax, Tunisia; Laboratory of Signals, Systems, Artificial Intelligence and Networks Sfax, Digital Research Centre of Sfax, Sfax, Tunisia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","23 Jan 2023","2023","16","","1308","1320","Due to technical and budget constraints on current optical satellites, the acquisition of satellite images with the best resolutions is not practicable. In this article, aiming to produce products with high spectral (HS) and temporal resolutions, we introduced a two-stream spectral–temporal fusion technique based on attention mechanism called STA-Net. STA-Net aims to combine high spectral and low temporal (HSLT) resolution images with low spectral and high temporal (LSHT) resolution images to generate products with the best characteristics. The proposed technique involves two stages. In the first one, two fused images are generated by a two-stream architecture based on residual attention blocks. The temporal difference estimator stream estimates the temporal difference between HS images at desired and neighboring dates. The reflectance difference estimator is the second stream. It predicts the reflectance difference between the input images (HS–LS) to map LS images into HS products. In the second stage, a reconstruction network combines the latter two-stream outputs via an effective learnable weighted-sum strategy. The two-stage model is trained in an end-to-end fashion using an effective loss function to ensure the best fusion quality. To the best of our knowledge, this work represents the first attempt to address the spectral–temporal fusion using an end-to-end deep neural network model. Experimental results conducted on two actual datasets of Sentinel-2 (HSLT:10 spectral bands and long revisit period) and Planetscope (LSHT: four spectral bands and daily images) images, which proved the effectiveness of the proposed technique with respect to baseline technique.","2151-1535","","10.1109/JSTARS.2023.3234722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10008044","Attention mechanism;convolutional neural network (CNN);image fusion;multisensor image fusion;Planetscope;Sentinel-2;spectral–temporal fusion","Image resolution;Spatial resolution;Satellites;Monitoring;Earth;Remote sensing;Image reconstruction","","","","","","61","CCBY","6 Jan 2023","","","IEEE","IEEE Journals"
"Scene Classification Based on Two-Stage Deep Feature Fusion","Y. Liu; Y. Liu; L. Ding","School of Geography, South China Normal University, Guangzhou, China; School of Geography, South China Normal University, Guangzhou, China; School of Geography, South China Normal University, Guangzhou, China","IEEE Geoscience and Remote Sensing Letters","23 Jan 2018","2018","15","2","183","186","In convolutional neural networks (CNNs), higher layer information is more abstract and more task specific, so people usually concern themselves with fully connected (FC) layer features, believing that lower layer features are less discriminative. However, a few researchers showed that the lower layers also provide very rich and powerful information for image representation. In view of these study findings, in this letter, we attempt to adaptively and explicitly combine the activations from intermediate and FC layers to generate a new CNN with directed acyclic graph topology, which is called the converted CNN. After that, two converted CNNs are integrated together to further improve the classification performance. We validate our proposed two-stage deep feature fusion model over two publicly available remote sensing data sets, and achieve a state-of-the-art performance in scene classification tasks.","1558-0571","","10.1109/LGRS.2017.2779469","National Natural Science Foundation of China(grant numbers:61673184); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237195","1 × 1 convolution;composite convolutional neural networks (CNNs);converted CNN;deep feature fusion;global average pooling (GAP)","Training;Convolution;Image representation;Data models;Adaptation models","convolution;directed graphs;feature extraction;feedforward neural nets;image classification;image fusion;image representation","image representation;directed acyclic graph topology;two-stage deep feature fusion model;scene classification tasks;convolutional neural networks;CNNs;fully connected layer features;FC layers","","86","","18","IEEE","22 Dec 2017","","","IEEE","IEEE Journals"
"No Reference Pansharpened Image Quality Assessment Through Deep Feature Similarity","N. Badal; R. Soundararajan; A. Garg; A. Patil","Space Applications Centre, Ahmedabad, India; Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India; Space Applications Centre, Ahmedabad, India; Space Applications Centre, Ahmedabad, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Sep 2022","2022","15","","7235","7247","Pansharpening refers to the process of enhancing the spatial resolution of a multispectral image with the help of a high spatial resolution panchromatic (PAN) image. Quality assessment (QA) of pansharpened images helps provide a formal framework for the analysis and design of pansharpening methods and is thus extremely important. However, lack of availability of a reference multispectral image makes QA of pansharpening algorithms a challenging task. Given the popular use of QA algorithms that use a reference, this article focuses on predicting the quality under a “no-reference” (NR) setting. Specifically, a learning based NR pansharpened image quality assessment (IQA) approach is adopted to predict state-of-the-art reference-based measures such as $Q2^{n}$ and spectral angle mapper without the need of a reference. We design an end-to-end deep pansharpening IQA network to compute the similarity of deep features fused from the PAN and input low-resolution multispectral with similar features extracted from the given pansharpened image. To train and test our learning-based approach, we create a large corpus of pansharpened images belonging to different satellites and thematic scenes by applying different pansharpening algorithms. Our experiments demonstrate that our NR pansharpened IQA algorithm achieves excellent performance and generalizes well across different satellites and resolutions.","2151-1535","","10.1109/JSTARS.2022.3199446","Indian Space Research Organization; Indian Institute of Science (IISc) Space Technology Cell(grant numbers:STC/EEE/RS/442); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858614","Deep learning;image fusion;pansharpening;quality assessment (QA)","Pansharpening;Image resolution;Satellites;Spatial resolution;Distortion measurement;Quality assessment;Prediction algorithms","feature extraction;geophysical image processing;geophysical techniques;image classification;image colour analysis;image fusion;image resolution;learning (artificial intelligence);remote sensing","deep feature similarity;high spatial resolution panchromatic image;pansharpened images;pansharpening methods;reference multispectral image;QA algorithms;no-reference setting;image quality assessment approach;state-of-the-art reference-based measures;end-to-end deep pansharpening IQA network;deep features;input low-resolution;given pansharpened image;learning-based approach;different pansharpening algorithms;IQA algorithm","","","","50","CCBY","17 Aug 2022","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion Under Spectrally Varying Spatial Blurs – Application to High Dimensional Infrared Astronomical Imaging","C. Guilloteau; T. Oberlin; O. Berné; N. Dobigeon","Institut de Recherche en Astrophysique et Planétologie (IRAP), University of Toulouse, 9 avenue du Colonel Roche, BP 44346, 31028, Toulouse, Cedex 4, France; DISC, ISAE-SUPAERO, University of Toulouse, Toulouse, France; Institut de Recherche en Astrophysique et Planétologie (IRAP), University of Toulouse, 9 avenue du Colonel Roche, BP 44346, 31028, Toulouse, Cedex 4, France; Institut Universitaire de France (IUF)","IEEE Transactions on Computational Imaging","22 Sep 2020","2020","6","","1362","1374","Hyperspectral imaging has become a significant source of valuable data for astronomers over the past decades. Current instrumental and observing time constraints allow direct acquisition of multispectral images, with high spatial but low spectral resolution, and hyperspectral images, with low spatial but high spectral resolution. To enhance scientific interpretation of the data, we propose a data fusion method which combines the benefits of each image to recover a high spatio-spectral resolution datacube. The proposed inverse problem accounts for the specificities of astronomical instruments, such as spectrally variant blurs. We provide a fast implementation by solving the problem in the frequency domain and in a low-dimensional subspace to efficiently handle the convolution operators as well as the high dimensionality of the data. We conduct experiments on a realistic synthetic dataset of simulated observation of the upcoming James Webb Space Telescope, and we show that our fusion algorithm outperforms state-of-the-art methods commonly used in remote sensing for Earth observation.","2333-9403","","10.1109/TCI.2020.3022825","ANR-3IA Artificial; Natural Intelligence Toulouse Institute; Conseil National de la Recherche Scientifique; Centre National d’Études Spatiales; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9199293","Data fusion;hyperspectral imaging;high dimensional imaging;infrared astronomy;super-resolution;deconvolution","Spatial resolution;Instruments;Imaging;Degradation;Hyperspectral imaging;Earth","astronomical image processing;astronomical instruments;astronomical telescopes;hyperspectral imaging;image fusion;image resolution;image restoration;image sensors;inverse problems;remote sensing;sensor fusion","instrumental time constraints;spectrally varying spatial blurs;hyperspectral images;low spectral resolution;multispectral images;direct acquisition;observing time constraints;astronomers;hyperspectral imaging;high dimensional infrared astronomical;fusion algorithm;low-dimensional subspace;spectrally variant blurs;astronomical instruments;inverse problem accounts;high spatio-spectral resolution datacube;data fusion method","","11","","46","IEEE","17 Sep 2020","","","IEEE","IEEE Journals"
"A New Method for Land Cover Characterization and Classification of Polarimetric SAR Data Using Polarimetric Signatures","M. Jafari; Y. Maghsoudi; M. J. Valadan Zoej","Department of Geomatics Engineering, K. N. Toosi University of Technology, Tehran, Iran; Department of Geomatics Engineering, K. N. Toosi University of Technology, Tehran, Iran; Department of Geomatics Engineering, K. N. Toosi University of Technology, Tehran, Iran","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Aug 2015","2015","8","7","3595","3607","Conventional methods for analyzing polarimetric synthetic aperture RADAR (PolSAR) data such as scattering matrix show polarimetric information just in a restricted number of polarization bases, whereas backscattering of the targets has information on wide range of polarizations. In order to solve this problem, polarimetric signatures have been investigated to have a better illustration of the target responses. Polarimetric signatures depict more details of physical information from target backscattering in various polarization bases. This paper presents a new method for generating polarimetric signatures for different features in PolSAR data by changing the polarization basis in the covariance matrix. Furthermore, various land cover classes were evaluated using their polarimetric signatures and the pattern recognition matching methods. On the basis of this background, an object-oriented and knowledge-based classification algorithm is proposed. The main idea of this method is to apply polarimetric signatures of various PolSAR features in the land cover classification. A Radarsat-2 image, acquired in leaf-off season of the forest areas, was chosen for this study. The backscattering from different classes, including six land cover classes: 1) red oak (Or); 2) white pine (Pw); 3) black spruce (Sb); 4) urban (Ur); 5) water (Wa); and 6) ground vegetation (GV) was analyzed by the proposed method. The results reported that the polarimetric signatures of PolSAR features introduce new concepts for the various targets which are different from the polarimetric power signatures. Also, the proposed classification was compared with the object-based form of the supervised Wishart classification as the baseline method. The mean accuracy of the proposed method is 6% better than the supervised Wishart classification.","2151-1535","","10.1109/JSTARS.2014.2387374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018944","Knowledge based;land cover classification;object oriented;polarimetric signatures;polarimetric synthetic aperture radar (PolSAR);scattering contributions;Knowledge based;land cover classification;object oriented;polarimetric signatures;polarimetric synthetic aperture radar (PolSAR);scattering contributions","Feature extraction;Covariance matrices;Scattering;Synthetic aperture radar;Pattern recognition;Remote sensing;Backscatter","geophysical image processing;image classification;image fusion;image segmentation;remote sensing by radar;synthetic aperture radar","land cover characterization;polarimetric SAR data classification;polarimetric signatures;polarimetric synthetic aperture radar data;PolSAR data;scattering matrix;polarimetric information;physical information;covariance matrix;pattern recognition matching methods;object-oriented classification algorithm;knowledge-based classification algorithm;Radarsat-2 image;polarimetric power signatures;supervised Wishart classification","","52","","40","IEEE","23 Jan 2015","","","IEEE","IEEE Journals"
"A Comparative Assessment of Multisensor Data Merging and Fusion Algorithms for High-Resolution Surface Reflectance Data","X. Wei; N. -B. Chang; K. Bai","Ministry of Education, Key Laboratory of Geographic Information Science, East China Normal University, Shanghai, China; Department of Civil, Environmental, and Construction Engineering, University of Central Florida, Orlando, FL, USA; Institute of Eco-Chongming, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Jul 2020","2020","13","","4044","4059","The improvement of the spatial and temporal resolution of reflectance data products has been challenging due to the diversity of data sources and availability of many data merging and fusion algorithms. In the algorithmic domain, methods for data merging and fusion may include, but are not limited to, the modified quantile-quantile adjustment (MQQA), the Bayesian maximum entropy (BME), and the spatial and temporal adaptive reflectance fusion model (STARFM). This article presents a synergistic integration of the data merging and fusion algorithms of MQQA and BME in dealing with heterogeneous and nonstationary surface reflectance data at both the top of atmosphere (TOA) and land surface for a comparative study. Emphasis has been placed on the distinctive performance between BME and MQQA-BME algorithms in the spatial domain and the MQQA-BME and STARFM in the temporal domain at both TOA and land surface levels. The results indicate that the BME and MQQA-BME outperform the MQQA in terms of the spatial coverage at both TOA and land surface levels. Moreover, the MQQA-BME algorithm shows a higher prediction accuracy than STARFM at the blue band over the temporal domain at both TOA and land surface levels. The results of this comparison will greatly empower the MQQA-BME to be used for urban air quality monitoring and related epidemiological assessment in the future, once finer aerosol optical depth predictions via integrated data merging and fusion can be made possible.","2151-1535","","10.1109/JSTARS.2020.3008746","National Natural Science Foundation of China(grant numbers:41701413); UCF Global Program; UCF Advanced Research Computing Center; China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139371","AOD;Bayesian maximum entropy (BME);data fusion;data merging;modified quantile–quantile adjustment (MQQA);MQQA–BME;spatial and temporal adaptive reflectance fusion model (STARFM)","Data integration;Remote sensing;Merging;Land surface;Earth;Heuristic algorithms;Artificial satellites","aerosols;air pollution;atmospheric optics;Bayes methods;geophysical image processing;image fusion;image resolution;remote sensing;sensor fusion;vegetation mapping","MQQA-BME algorithm;land surface levels;temporal domain;spatial domain;nonstationary surface reflectance data;heterogeneous surface reflectance data;STARFM;temporal adaptive reflectance fusion model;spatial reflectance fusion model;modified quantile-quantile adjustment;algorithmic domain;data sources;reflectance data products;temporal resolution;spatial resolution;high-resolution surface reflectance data;fusion algorithms;multisensor data merging;integrated data","","8","","94","CCBY","13 Jul 2020","","","IEEE","IEEE Journals"
"A New Geometry Enforcing Variational Model for Pan-Sharpening","P. Liu; L. Xiao; S. Tang","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","12","5726","5739","In this paper, a new variational method for pan-sharpening is proposed to obtain a high-resolution multispectral (MS) image from a low-resolution MS image and a high-resolution panchromatic (PAN) image. In addition to using the data generative fidelity term and wavelet-based spectral information preserving term, we also associate the Hessian structural information of the PAN image with the desired pan-sharpened MS image to enforce geometry correspondence in the fusion process. More specifically, we introduce a new geometry enforcing term called “vectorial Hessian feature consistence” and combine it with the data generative fidelity term and wavelet-based spectral information preserving term to form an unified variational model for pan-sharpening. Then, the optimal solution of the proposed variational pan-sharpening model is effectively obtained using the fast iterative shrinkage thresholding algorithm (FISTA) method. In addition to well preserving spectral information, our algorithm is also able to eliminate some undesired blocky or blurry artifacts by incorporating the curvature information. Experimental results demonstrate that the proposed method outperforms various well-known pan-sharpening methods in terms of both excellent spatial and spectral qualities.","2151-1535","","10.1109/JSTARS.2016.2537925","Fundamental Research Funds for the Central Universities(grant numbers:30915012204); National Natural Science Foundation of China(grant numbers:61171165,11431015,61571230); Training Program of the Major Research Plan of the National Nature Science Foundation of China(grant numbers:91538108); National Scientific Equipment Developing Project of China(grant numbers:2012YQ050250); Six Top Talents Project of Jiangsu Province(grant numbers:2012DZXX-036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442522","Geometry enforcing;pan-sharpening;variational method;vectorial Hessian feature consistence;wavelet fusion","Spatial resolution;Geometry;Image edge detection;Distortion;Earth;Remote sensing","Hessian matrices;hyperspectral imaging;image fusion;iterative methods;remote sensing","pan-sharpening method;blurry artifact;blocky artifact;FISTA method;fast iterative shrinkage thresholding algorithm;Hessian feature consistence;fusion process;pan-sharpened multispectral image;Hessian structural information;wavelet-based spectral information;high-resolution panchromatic image;low-resolution multispectral image;high-resolution multispectral image;geometry enforcing variational model","","7","","43","IEEE","28 Mar 2016","","","IEEE","IEEE Journals"
"Multicomponent Driven Consistency Priors for Simultaneous Decomposition and Pansharpening","P. Liu; L. Xiao","Jiangsu Key Laboratory of Big Data Security & Intelligent Processing, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2020","2019","12","11","4589","4605","Pansharpening is also known as the fusion of a low-resolution (LR) multispectral (MS) image and a high-resolution (HR) panchromatic (Pan) image of the same scene, which is an effective way to improve the spatial resolution of the LR MS image so as to obtain an HR MS image (i.e., pansharpened MS image). In this article, we propose a novel multicomponent consistency priors driven variational model for simultaneous decomposition and pansharpening (SDP) in a unified optimization framework. Specifically, the proposed SDP model particularly decomposes the Pan and MS images into cartoon, structure and texture components, and fully exploits the multicomponent consistency priors on these cartoon, structure and texture components of the Pan and MS images. Thus, the proposed SDP model can suitably characterize the different properties of these multicomponents so that these multicomponents can be well preserved. Moreover, the proposed SDP model is actually a band-coupled model, which can fully preserve the intrinsic structural correlation among the MS bands, because the MS image is actually a spatial-spectral strongly correlated cube. Then, an efficient iterative algorithm based on the forward-backward splitting technique is designed to solve the proposed SDP model. Finally, we compare the proposed SDP method with some state-of-the-art methods on various satellite datasets, and the experimental results demonstrate the effectiveness of the proposed method, which can perform higher spectral and spatial qualities than the other methods.","2151-1535","","10.1109/JSTARS.2019.2953140","National Natural Science Foundation of China(grant numbers:61802202,61571230); Natural Science Foundation of Jiangsu Province(grant numbers:BK20170905); Jiangsu Provincial Key Developing Project(grant numbers:BE2018727); NUPTSF(grant numbers:NY2018025,NY217137); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8915992","Multicomponent consistency priors;pansharpening;simultaneous decomposition and pansharpening (SDP);variational model","Spatial resolution;Correlation;Optimization;Remote sensing;Multiresolution analysis;Pansharpening","geophysical image processing;image fusion;image resolution;image texture;iterative methods;optimisation;remote sensing;spectral analysis;variational techniques","multicomponent driven consistency priors;low-resolution multispectral image;high-resolution panchromatic image;spatial resolution;LR MS image;HR MS image;pansharpened MS image;variational model;SDP model;texture components;band-coupled model;MS bands;spatial-spectral strongly correlated cube;SDP method;simultaneous decomposition and pansharpening;unified optimization framework;structure components;iterative algorithm;forward-backward splitting technique;satellite datasets","","6","","49","IEEE","27 Nov 2019","","","IEEE","IEEE Journals"
"Estimation of Actual Evapotranspiration in Fragmented Mediterranean Areas by the Spatio-Temporal Fusion of NDVI Data","M. Pieri; M. Chiesi; P. Battista; L. Fibbi; L. Gardin; B. Rapi; M. Romani; F. Sabatini; L. Angeli; C. Cantini; A. Giovannelli; F. Maselli","Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; LaMMA Consortium, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy; Institute of Research on Terrestrial Ecosystems of the National Research Council, Sesto Fiorentino, Italy; Institute of BioEconomy of the National Research Council, Sesto Fiorentino, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","3 Feb 2020","2019","12","12","5108","5117","Actual evapotranspiration (ETA) is a fundamental component of the land water cycle that can be predicted by the combination of meteorological data and remotely sensed normalized difference vegetation index (NDVI) observations. The proficient application of this approach to the retrospective study of fragmented areas, however, depends on the preliminary use of spatio-temporal fusion (STF) methods capable of integrating different satellite datasets. One of these methods is the Spatial Enhancer of Vegetation Index image Series (SEVIS), which has been recently developed to improve the annual NDVI datasets based on one or a few high spatial resolution images. This STF method is currently applied to moderate resolution imaging spectroradiometer (MODIS) and TM/ETM+/OLI imagery taken over three fragmented areas in Tuscany (Central Italy), representative of different Mediterranean ecosystems, i.e., an urban grassland, a tomato field, and an olive grove. The performance of SEVIS is evaluated by comparing the ETA estimates obtained from the original (MODIS) and synthetic (MODIS plus TM/ETM+/OLI) NDVI datasets to ground ETA observations. The experimental results indicate that the original MODIS NDVI data cannot properly characterize the seasonal vegetation evolutions of the three study sites, which negatively affects the performance of ETA simulation. In contrast, such evolutions are reasonably reproduced by the synthetic NDVI datasets, which improves the accuracy of the ETA estimates both in terms of correlation and errors. The improvements are particularly evident during the summer dry period when the MODIS images are incapable of characterizing the actual vegetation response to water stress.","2151-1535","","10.1109/JSTARS.2019.2955513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937742","Moderate Resolution Imaging Spectroradiometer (MODIS);Normalized Difference Vegetation Index (NDVI);Thematic Mapper (TM), Enhanced Thematic Mapper Plus (ETM+), Operational Land Imager (OLI)","Vegetation mapping;MODIS;Remote sensing;Agriculture;Ecosystems","evaporation;geophysical image processing;hydrological techniques;image fusion;image resolution;remote sensing;transpiration;vegetation;vegetation mapping","Spatial Enhancer of Vegetation Index image Series;actual vegetation;MODIS images;synthetic NDVI datasets;ETA simulation;seasonal vegetation evolutions;original MODIS NDVI data;ETA observations;Mediterranean ecosystems;moderate resolution imaging spectroradiometer;STF method;high spatial resolution images;annual NDVI datasets;SEVIS;satellite datasets;spatio-temporal fusion methods;fragmented areas;normalized difference vegetation index observations;meteorological data;land water cycle;fragmented Mediterranean areas;actual evapotranspiration","","3","","32","IEEE","20 Dec 2019","","","IEEE","IEEE Journals"
"Polarimetric SAR Image Classification Based on Ensemble Dual-Branch CNN and Superpixel Algorithm","W. Hua; C. Zhang; W. Xie; X. Jin","Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China; Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China; Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China; Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Apr 2022","2022","15","","2759","2772","Recently, convolutional neural networks (CNNs) have been successfully utilized in polarimetric synthetic aperture radar (PolSAR) image classification and obtained promising results. However, most CNN-based classification methods require a large number of labeled samples and it is difficult to obtain sufficient labeled samples. For this reason, an ensemble dual-branch CNN (EDb-CNN) is proposed for PolSAR image classification with small samples. First, to solve the problem of the small sample in PolSAR image classification, a new data enhancement method based on the superpixel algorithm is proposed to expand the number of labeled samples. Second, to obtain different scales of features from PolSAR images, a Db-CNN model is proposed. This model contains two parallel CNN structures. One CNN branch is used to extract the polarization features from the complex coherency matrix. The other branch is utilized to extract the spatial features based on weighted spatial neighborhood. On the top of these two branches, a feature fusion model is adopted to combine these two deep features, and a weighted loss function is employed to improve the learning procedure. Then, the ensemble learning algorithm is used for each CNN branch and Db-CNN network to obtain the better classification results. Finally, a postprocess algorithm based on the superpixel algorithm is proposed to improve the consistency of classification results. Experiments on two PolSAR datasets show that the proposed method achieves a much better performance than other classification methods, especially when only a few labeled samples are available.","2151-1535","","10.1109/JSTARS.2022.3162953","National Natural Science Foundation of China(grant numbers:61901368); Natural Science Foundation of Shaanxi Province(grant numbers:2019JQ-377); Key Special project of China High Resolution Earth Observation System Young Scholar Innovation Fund(grant numbers:GFZX04061502); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744483","Dual-branch convolutional neural network (Db-CNN);polarimetric SAR;superpixels;terrain classification","Feature extraction;Image classification;Licenses;Classification algorithms;Covariance matrices;Remote sensing;Image recognition","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image resolution;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","Db-CNN network;superpixel algorithm;PolSAR datasets;polarimetric SAR image classification;ensemble dual-branch CNN;convolutional neural networks;polarimetric synthetic aperture radar image classification;CNN-based classification methods;labeled samples;EDb-CNN;PolSAR image classification;data enhancement method;PolSAR images;Db-CNN model;parallel CNN structures;CNN branch;polarization features;spatial features;feature fusion model;ensemble learning algorithm;complex coherency matrix","","","","43","CCBY","29 Mar 2022","","","IEEE","IEEE Journals"
"Robust Fusion of Multiband Images With Different Spatial and Spectral Resolutions for Change Detection","V. Ferraris; N. Dobigeon; Q. Wei; M. Chabert","IRIT/INP-ENSEEIHT, University of Toulouse, France; IRIT/INP-ENSEEIHT, University of Toulouse, France; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA; IRIT/INP-ENSEEIHT, University of Toulouse, France","IEEE Transactions on Computational Imaging","20 May 2017","2017","3","2","175","186","Archetypal scenarios for change detection generally consider two images acquired through sensors of the same modality. However, in some specific cases such as emergency situations, the only images available may be those acquired through different kinds of sensors. More precisely, this paper addresses the problem of detecting changes between two multiband optical images characterized by different spatial and spectral resolutions. This sensor dissimilarity introduces additional issues in the context of operational change detection. To alleviate these issues, classical change detection methods are applied after independent preprocessing steps (e.g., resampling) used to get the same spatial and spectral resolutions for the pair of observed images. Nevertheless, these preprocessing steps tend to throw away relevant information. Conversely, in this paper, we propose a method that more effectively uses the available information by modeling the two observed images as spatial and spectral versions of two (unobserved) latent images characterized by the same high spatial and high spectral resolutions. As they cover the same scene, these latent images are expected to be globally similar except for possible changes in sparse spatial locations. Thus, the change detection task is envisioned through a robust multiband image fusion method, which enforces the differences between the estimated latent images to be spatially sparse. This robust fusion problem is formulated as an inverse problem, which is iteratively solved using an efficient block-coordinate descent algorithm. The proposed method is applied to real panchromatic, multispectral, and hyperspectral images with simulated realistic and real changes. A comparison with state-of-the-art change detection methods evidences the accuracy of the proposed strategy.","2333-9403","","10.1109/TCI.2017.2692645","Coordenação de Aperfeiçoamento de Ensino Superior(grant numbers:ANR-15-NMED-0002-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7895167","Change detection;different resolutions;hyperspectral imagery;image fusion;multispectral imagery","Spatial resolution;Optical sensors;Optical imaging;Degradation;Robustness","geophysical image processing;hyperspectral imaging;image fusion;image resolution;inverse problems;remote sensing","spatial resolutions;spectral resolutions;archetypal scenarios;multiband optical images;sensor dissimilarity;operational change detection;classical change detection methods;independent preprocessing steps;latent images;sparse spatial locations;robust multiband image fusion method;inverse problem;block-coordinate descent algorithm;panchromatic image;multispectral images;hyperspectral images","","21","","56","IEEE","12 Apr 2017","","","IEEE","IEEE Journals"
"GETNET: A General End-to-End 2-D CNN Framework for Hyperspectral Image Change Detection","Q. Wang; Z. Yuan; Q. Du; X. Li","School of Computer Science, the Center for OPTical IMagery Analysis and Learning, Unmanned System Research Institute, Northwestern Polytechnical University, Xi’an, China; School of Computer Science and the Center for OPTical IMagery Analysis and Learning, Northwestern Polytechnical University, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2018","2019","57","1","3","13","Change detection (CD) is an important application of remote sensing, which provides timely change information about large-scale Earth surface. With the emergence of hyperspectral imagery, CD technology has been greatly promoted, as hyperspectral data with high spectral resolution are capable of detecting finer changes than using the traditional multispectral imagery. Nevertheless, the high dimension of the hyperspectral data makes it difficult to implement traditional CD algorithms. Besides, endmember abundance information at subpixel level is often not fully utilized. In order to better handle high-dimension problem and explore abundance information, this paper presents a general end-to-end 2-D convolutional neural network (CNN) framework for hyperspectral image CD (HSI-CD). The main contributions of this paper are threefold: 1) mixed-affinity matrix that integrates subpixel representation is introduced to mine more cross-channel gradient features and fuse multisource information; 2) 2-D CNN is designed to learn the discriminative features effectively from the multisource data at a higher level and enhance the generalization ability of the proposed CD algorithm; and 3) the new HSI-CD data set is designed for objective comparison of different methods. Experimental results on real hyperspectral data sets demonstrate that the proposed method outperforms most of the state of the arts.","1558-0644","","10.1109/TGRS.2018.2849692","National Key R&D Program of China(grant numbers:2017YFB1002202); National Natural Science Foundation of China(grant numbers:61773316); Natural Science Foundation of Shaanxi Province(grant numbers:2018KJXX-024); Fundamental Research Funds for the Central Universities(grant numbers:3102017AX010); Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8418840","2-D convolutional neural network (CNN);change detection (CD);deep learning;hyperspectral image (HSI);mixed-affinity matrix;spectral unmixing","Hyperspectral imaging;Machine learning;Task analysis;Neural networks;Principal component analysis","convolution;data mining;feedforward neural nets;geophysical image processing;geophysical techniques;hyperspectral imaging;image fusion;image resolution;image segmentation;learning (artificial intelligence);remote sensing","hyperspectral image change detection;hyperspectral imagery;CD technology;hyperspectral image CD;fuse multisource information;HSI-CD data;hyperspectral data sets;multispectral imagery;GETNET;general End-to-End 2-D CNN;cross-channel gradient features;general end-to-end 2-D convolutional neural network","","294","","35","IEEE","24 Jul 2018","","","IEEE","IEEE Journals"
"Deep Multiple Instance Learning-Based Spatial–Spectral Classification for PAN and MS Imagery","X. Liu; L. Jiao; J. Zhao; J. Zhao; D. Zhang; F. Liu; S. Yang; X. Tang","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","27 Dec 2017","2018","56","1","461","473","Panchromatic (PAN) and multispectral (MS) imagery classification is one of the hottest topics in the field of remote sensing. In recent years, deep learning techniques have been widely applied in many areas of image processing. In this paper, an end-to-end learning framework based on deep multiple instance learning (DMIL) is proposed for MS and PAN images’ classification using the joint spectral and spatial information based on feature fusion. There are two instances in the proposed framework: one instance is used to capture the spatial information of PAN and the other is used to describe the spectral information of MS. The features obtained by the two instances are concatenated directly, which can be treated as simple fusion features. To fully fuse the spatial–spectral information for further classification, the simple fusion features are fed into a fusion network with three fully connected layers to learn the high-level fusion features. Classification experiments carried out on four different airborne MS and PAN images indicate that the classifier provides feasible and efficient solution. It demonstrates that DMIL performs better than using a convolutional neural network and a stacked autoencoder network separately. In addition, this paper shows that the DMIL model can learn and fuse spectral and spatial information effectively, and has huge potential for MS and PAN imagery classification.","1558-0644","","10.1109/TGRS.2017.2750220","National Basic Research Program (973 Program) of China(grant numbers:2013CB329402); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048556","Deep learning;feature fusion;image classification;joint features;multiple instance learning","Feature extraction;Spatial resolution;Machine learning;Neural networks;Convolution;Fuses","feature extraction;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","deep multiple instance learning;deep learning techniques;end-to-end learning framework;feature fusion;spatial-spectral information;fusion network;high-level fusion features;spatial-spectral classification;panchromatic-and-multispectral imagery classification;MS imagery classification;PAN imagery classification","","50","","52","IEEE","22 Sep 2017","","","IEEE","IEEE Journals"
"Classification of Remotely Sensed Images Using the GeneSIS Fuzzy Segmentation Algorithm","S. K. Mylonas; D. G. Stavrakoudis; J. B. Theocharis; P. A. Mastorocostas","Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Electrical and Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Department of Computer Engineering, Technological Education Institute of Central Macedonia, Serres, Greece","IEEE Transactions on Geoscience and Remote Sensing","10 Jun 2015","2015","53","10","5352","5376","In this paper, we propose an integrated framework of the recently proposed Genetic Sequential Image Segmentation (GeneSIS) algorithm. GeneSIS segments the image in an iterative manner, whereby at each iteration, a single object is extracted via a genetic algorithm-based object extraction method. This module evaluates the fuzzy content of candidate regions, and through an effective fitness function design provides objects with optimal balance between fuzzy coverage, consistency and smoothness. GeneSIS exhibits a number of interesting properties, such as reduced over-/undersegmentation, adaptive search scale, and region-based search. To enhance the capabilities of GeneSIS, we incorporate here several improvements of our initial proposal. On one hand, two modifications are introduced pertaining to the object extraction algorithm. Specifically, we consider a more flexible representation of the structural elements used for the object's extraction. Furthermore, in view of its importance, the consistency criterion is redefined, thus providing a better handling of the ambiguous areas of the image. On the other hand we incorporate three tools properly devised, according to the fuzzy principles characterizing GeneSIS. First, we develop a marker selection strategy that creates reliable markers, particularly when dealing with ambiguous components of the image. Furthermore, using GeneSIS as the essential part, we consider a generalized experimental setup embracing two different classification schemes for remote sensing images: the spectral-spatial classification and the supervised segmentation methods. Finally, exploiting the inherent property of GeneSIS to produce multiple segmentations, we propose a segmentation fusion scheme. The effectiveness of the proposed methodology is validated after thorough experimentation on four data sets.","1558-0644","","10.1109/TGRS.2015.2421640","European Union (European Social Fund-ESF); Greek national funds; Operational Program “Education and Lifelong Learning” of the National Strategic Reference Framework (NSRF)-Research Funding Program: ARCHIMEDES III; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7104101","Fuzzy clustering;genetic algorithms;image segmentation;marker selection;segmentation fusion;spectral-spatial classification;Fuzzy clustering;genetic algorithms;image segmentation;marker selection;segmentation fusion;spectral-spatial classification","Image segmentation;Support vector machines;Clustering algorithms;Reliability;Genetic algorithms;Feature extraction;Image edge detection","feature extraction;fuzzy set theory;genetic algorithms;geophysical image processing;image classification;image fusion;image representation;image segmentation;iterative methods;remote sensing","remotely sensed image classification;GeneSIS fuzzy segmentation algorithm;genetic sequential image segmentation;iterative method;genetic algorithm-based object extraction method;effective fitness function design;fuzzy coverage;consistency;smoothness;flexible structural elements representation;fuzzy principle;marker selection strategy;spectral-spatial classification;supervised segmentation method;segmentation fusion scheme","","13","","47","IEEE","8 May 2015","","","IEEE","IEEE Journals"
"Automatic Parameter Selection for Intensity-Based Registration of Imagery to LiDAR Data","E. G. Parmehr; C. S. Fraser; C. Zhang","Center for Urban Research, RMIT University, Melbourne, Vic., Australia; Cooperative Research Center for Spatial Information, University of Melbourne, Melbourne, Vic., Australia; RMIT University, Melbourne, Vic., Australia","IEEE Transactions on Geoscience and Remote Sensing","30 Sep 2016","2016","54","12","7032","7043","Automatic registration of multisensor data, for example, imagery and Light Detection And Ranging (LiDAR), is a basic step in data fusion in the field of geospatial information processing. Mutual information (MI) has recently attracted research attention as a statistical similarity measure for intensity-based registration of multisensor images in the related fields of computer vision and remote sensing. Since MI-based registration methods rely on joint probability density functions (pdfs) for the data sets, errors in pdf estimation can affect the MI value, causing registration failure due to the presence of nonmonotonic surfaces of similarity measure. The quality of the estimated pdf is highly dependent upon both the bin size and the smoothing technique used in the pdf estimation procedure. The lack of a general approach to assign an appropriate bin size value for the pdf of multisensor data reduces both the level of automation and the robustness of the registration. In this paper, a novel bin size selection approach is proposed to improve registration reliability. The proposed method determines the best (uniform or variable) bin size for the pdf estimation via an analysis of the relationship between the similarity measure values of the data and the adopted geometric transformation. This highlights the role of the component of MI sensitive to the transformation, rather than the MI component that is unrelated to the transformation, such as noise. The performance of the proposed method for the registration of aerial imagery to LiDAR point clouds is investigated, and experimental results are compared with those achieved through a feature-based registration method.","1558-0644","","10.1109/TGRS.2016.2594294","CRC for Spatial Information (CRCSI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542534","Image registration;multisensor data;mutual information (MI);probability density function (pdf) parameter estimation","Probability density function;Laser radar;Entropy;Estimation;Reliability;Three-dimensional displays;Feature extraction","geographic information systems;geophysical image processing;image fusion;image registration;remote sensing by laser beam","automatic parameter selection;LiDAR data;multisensor data automatic registration;Light Detection And Ranging;data fusion;geospatial information processing;mutual information;multisensor image intensity-based registration;MI-based registration methods;joint probability density functions;registration failure;nonmonotonic surfaces;pdf estimation procedure;LiDAR point clouds;feature-based registration method","","6","","48","IEEE","12 Aug 2016","","","IEEE","IEEE Journals"
"Guided Nonlocal Patch Regularization and Efficient Filtering-Based Inversion for Multiband Fusion","U. V. S.; P. Nair; K. N. Chaudhury","Department of Electrical Engineering, Indian Institute of Science, Bengaluru, India; Department of Electrical Engineering, Indian Institute of Science, Bengaluru, India; Department of Electrical Engineering, Indian Institute of Science, Bengaluru, India","IEEE Transactions on Computational Imaging","24 Nov 2022","2022","8","","1012","1024","In multiband fusion, an image with a high spatial and low spectral resolution is combined with an image with a low spatial but high spectral resolution to produce a single multiband image having high spatial and spectral resolutions. This comes up in remote sensing applications such as pansharpening (MS+PAN), hyperspectral sharpening (HS+PAN), and HS-MS fusion (HS+MS). Remote sensing images are textured and have repetitive structures. Motivated by nonlocal patch-based methods for image restoration, we propose a convex regularizer that (i) takes into account long-distance correlations, (ii) penalizes patch variation, which is more effective than pixel variation for capturing texture information, and (iii) uses the higher spatial resolution image as a guide image for weight computation. We come up with an efficient ADMM algorithm for optimizing the regularizer along with a standard least-squares loss function derived from the imaging model. The novelty of our algorithm is that by expressing patch variation as filtering operations and by judiciously splitting the original variables and introducing latent variables, we are able to solve the ADMM subproblems efficiently using FFT-based convolution and soft-thresholding. As far as the reconstruction quality is concerned, our method is shown to outperform state-of-the-art variational and deep learning techniques.","2333-9403","","10.1109/TCI.2022.3214149","Science and Engineering Research Board(grant numbers:CRG/2020/000527); Government of India; ISRO-IISc Space Technology Cell(grant numbers:ISTC/EEE/KNC/440); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925568","Hyperspectral imaging;image fusion;patch regularization;filtering;optimization","Spatial resolution;Image resolution;Optimization;Imaging;Image reconstruction;TV;Convolution","convolution;fast Fourier transforms;geophysical image processing;image denoising;image filtering;image fusion;image reconstruction;image representation;image resolution;image restoration;image sensors;image texture;learning (artificial intelligence);optimisation;remote sensing;wavelet transforms","account long-distance correlations;convex regularizer;deep learning techniques;efficient ADMM algorithm;efficient filtering-based inversion;filtering operations;guide image;high spectral resolution;higher spatial resolution image;HS+MS;HS+PAN;HS-MS fusion;hyperspectral sharpening;image restoration;imaging model;introducing latent variables;low spatial;low spectral resolution;MS+PAN;multiband fusion;nonlocal patch regularization;nonlocal patch-based methods;original variables;patch variation;pixel variation;remote sensing applications;repetitive structures;sensing images;single multiband image;spectral resolutions;standard least-squares loss function;texture information","","","","85","IEEE","20 Oct 2022","","","IEEE","IEEE Journals"
"Multi-Temporal Scene Classification and Scene Change Detection With Correlation Based Fusion","L. Ru; B. Du; C. Wu","Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Image Processing","29 Dec 2020","2021","30","","1382","1394","Classifying multi-temporal scene land-use categories and detecting their semantic scene-level changes for remote sensing imagery covering urban regions could straightly reflect the land-use transitions. Existing methods for scene change detection rarely focus on the temporal correlation of bi-temporal features, and are mainly evaluated on small scale scene change detection datasets. In this work, we proposed a CorrFusion module that fuses the highly correlated components in bi-temporal feature embeddings. We first extract the deep representations of the bi-temporal inputs with deep convolutional networks. Then the extracted features will be projected into a lower-dimensional space to extract the most correlated components and compute the instance-level correlation. The cross-temporal fusion will be performed based on the computed correlation in CorrFusion module. The final scene classification results are obtained with softmax layers. In the objective function, we introduced a new formulation to calculate the temporal correlation more efficiently and stably. The detailed derivation of backpropagation gradients for the proposed module is also given. Besides, we presented a much larger scale scene change detection dataset with more semantic categories and conducted extensive experiments on this dataset. The experimental results demonstrated that our proposed CorrFusion module could remarkably improve the multi-temporal scene classification and scene change detection results.","1941-0042","","10.1109/TIP.2020.3039328","National Natural Science Foundation of China(grant numbers:61971317,61822113); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)(grant numbers:2019AEA170); Natural Science Foundation of Hubei Province(grant numbers:2018CFA050,2020CFB594); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271913","Change detection;scene change detection;multi-temporal scene classification;canonical correlation analysis;convolutional neural network","Feature extraction;Correlation;Remote sensing;Semantics;Task analysis;Training;Spatial resolution","feature extraction;geophysical image processing;image classification;image fusion;image resolution;remote sensing","multitemporal scene land-use;scene change detection dataset;scene change detection results;multitemporal scene classification;final scene classification results;cross-temporal fusion;instance-level correlation;bi-temporal inputs;bi-temporal feature embeddings;highly correlated components;CorrFusion module;semantic scene-level changes","","22","","51","IEEE","25 Nov 2020","","","IEEE","IEEE Journals"
"A Multiscale and Multipath Network With Boundary Enhancement for Building Footprint Extraction From Remotely Sensed Imagery","H. Zhang; X. Zheng; N. Zheng; W. Shi","School of Environment and Spatial Informatics, China University of Mining and Technology, Xuzhou, China; School of Environment and Spatial Informatics, China University of Mining and Technology, Xuzhou, China; School of Environment and Spatial Informatics, China University of Mining and Technology, Xuzhou, China; Department of Land Surveying and Geo-Informatics, The Hong Kong Polytechnic University, Hong Kong","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Oct 2022","2022","15","","8856","8869","Due to its high efficiency and low cost, automatic extraction of building footprints from remotely sensed imagery has long been an important means to obtain building footprint information, which can be easily implemented using existing fully convolutional network (FCN)-based methods. However, such methods suffer from imperfections and thus accurately extracting building footprints from remotely sensed imagery remains a challenging task. For example, cascaded convolutions generally cannot preserve the spatial details well, leading to blurred boundaries and omission of small buildings. Insufficient multiscale features fusion without considering semantic gaps between different level features could yield misclassification. In addition, the fixed receptive field always produces discontinuous holey extracted large buildings. To this end, we propose a novel multiscale and multipath network with boundary enhancement (MMB-Net) that accurately extracts building footprints from remotely sensed imagery. More specially, a parallel multipath feature extraction module is first designed to capture high spatial information-preserved multiscale features with less semantic distances. In addition, the receptive field is enlarged and broadened by a multiscale features enhancement module. Then, an attention-based multiscale features fusion module is built to appropriately aggregate multiscale features. Lastly, a spatial enhancement module is presented to refine the extracted building boundaries by capturing boundary information from low-level features. The proposed MMB-Net has been tested on two benchmark data sets together with other SOTA approaches. The results show that MMB-Net can achieve promising building footprints extraction performances and it outperforms the SOTA methods. The implementation of MMB-Net is available at.","2151-1535","","10.1109/JSTARS.2022.3214485","National Natural Science Foundation of China(grant numbers:41971400,41974039); Fundamental Research Funds for the Central Universities(grant numbers:2019ZDPY09); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919349","Attention mechanism;boundary enhancement;building footprint extraction;fully convolutional network (FCN);semantic gap","Feature extraction;Buildings;Data mining;Remote sensing;Image edge detection;Semantics;Decoding","feature extraction;geophysical image processing;geophysical signal processing;image fusion;remote sensing","multipath network;boundary enhancement;building footprint extraction;remotely sensed imagery;automatic extraction;building footprint information;fully convolutional network-based methods;blurred boundaries;omission;buildings;insufficient multiscale features fusion;different level features;MMB-Net;parallel multipath feature extraction module;high spatial information-preserved multiscale features;multiscale features enhancement module;attention-based multiscale features fusion module;appropriately aggregate multiscale features;spatial enhancement module;extracted building boundaries;boundary information;low-level features;promising building footprints extraction performances","","","","46","CCBY","14 Oct 2022","","","IEEE","IEEE Journals"
"Quantifying the Effect of Registration Error on Spatio-Temporal Fusion","Y. Tang; Q. Wang; K. Zhang; P. M. Atkinson","College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; College of Surveying and Geo-Informatics, Tongji University, Shanghai, China; Key Laboratory of Urban Natural Resources Monitoring and Simulation, Ministry of Natural Resources, Shenzhen, PR China; State Key Laboratory of Resources and Environmental Information System, Institute of Geographical Sciences and Natural Resources Research, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Feb 2020","2020","13","","487","503","It is challenging to acquire satellite sensor data with both fine spatial and fine temporal resolution, especially for monitoring at global scales. Among the widely used global monitoring satellite sensors, Landsat data have a coarse temporal resolution, but fine spatial resolution, while moderate resolution imaging spectroradiometer (MODIS) data have fine temporal resolution, but coarse spatial resolution. One solution to this problem is to blend the two types of data using spatio-temporal fusion, creating images with both fine temporal and fine spatial resolution. However, reliable geometric registration of images acquired by different sensors is a prerequisite of spatio-temporal fusion. Due to the potentially large differences between the spatial resolutions of the images to be fused, the geometric registration process always contains some degree of uncertainty. This article analyzes quantitatively the influence of geometric registration error on spatio-temporal fusion. The relationship between registration error and the accuracy of fusion was investigated under the influence of different temporal distances between images, different spatial patterns within the images and using different methods (i.e., spatial and temporal adaptive reflectance fusion model (STARFM), and Fit-FC; two typical spatio-temporal fusion methods). The results show that registration error has a significant impact on the accuracy of spatio-temporal fusion; as the registration error increased, the accuracy decreased monotonically. The effect of registration error in a heterogeneous region was greater than that in a homogeneous region. Moreover, the accuracy of fusion was not dependent on the temporal distance between images to be fused, but rather on their statistical correlation. Finally, the Fit-FC method was found to be more accurate than the STARFM method, under all registration error scenarios.","2151-1535","","10.1109/JSTARS.2020.2965190","National Natural Science Foundation of China(grant numbers:41971297); Tongji University(grant numbers:0250141304,02502350047); Ministry of Natural Resources(grant numbers:KF-2019-04-003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966561","Landsat;MODIS;registration error;remote sensing data;spatio-temporal fusion","Spatial resolution;MODIS;Remote sensing;Earth;Artificial satellites;Monitoring","geophysical image processing;image fusion;image registration;image resolution;radiometry;sensor fusion","moderate resolution imaging spectroradiometer;Landsat data;coarse temporal resolution;widely used global monitoring satellite sensors;satellite sensor data;temporal distance;typical spatio-temporal fusion methods;temporal adaptive reflectance fusion model;spatial reflectance fusion model;geometric registration error;coarse spatial resolution;fine temporal resolution;moderate resolution imaging spectroradiometer data;fine spatial resolution","","21","","45","CCBY","22 Jan 2020","","","IEEE","IEEE Journals"
"SWDet: Anchor-Based Object Detector for Solid Waste Detection in Aerial Images","L. Zhou; X. Rao; Y. Li; X. Zuo; Y. Liu; Y. Lin; Y. Yang","School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; Henan Province Engineering Research Center of Spatial Information Processing and Shenzhen Research Institute, Henan University, Kaifeng, China; School of Computer and Information Engineering, Henan University, Kaifeng, China; Institute of Plant Stress Biology, State Key Laboratory of Cotton Biology, Department of Biology, Kaifeng, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Dec 2022","2023","16","","306","320","As we all know, waste pollution is one of the most serious environmental issues in the world. Efficient detection of solid waste (SW) in aerial images can improve subsequent waste classification and automatic sorting on the ground. However, traditional methods have some problems, such as poor generalization and limited detection performance. This article presents an anchor-based object detector for solid waste in aerial images (SWDet). Specifically, we construct asymmetric deep aggregation (ADA) network with structurally reparameterized asymmetric blocks to extract waste features with inconspicuous appearance. Besides, considering the waste with blurred boundaries caused by the resolution of aerial images, this article constructs efficient attention fusion pyramid network (EAFPN) to obtain contextual information and multiscale geospatial information via attention fusion. And the model can capture the scattering features of irregular shape waste. In addition, we construct the dataset for solid waste aerial detection (SWAD) by collecting aerial images of SW in Henan Province, China, to validate the effectiveness of our method. Experimental results show that SWDet outperforms most of existing methods for SW detection in aerial images.","2151-1535","","10.1109/JSTARS.2022.3218958","National Key Research and Development Program of China(grant numbers:2019YFE0126600); Major Project of Science and Technology of Henan Province(grant numbers:201400210300); Key Scientific and Technological Project of Henan Province(grant numbers:212102210496); Key Research and Promotion Projects of Henan Province(grant numbers:212102210393,202102110121,222102320163); Kaifeng Science and Technology Development plan(grant numbers:2002001); National Natural Science Foundation of China(grant numbers:62176087); Shenzhen Science and Technology Innovation Commission; Shenzhen Virtual University Park(grant numbers:2021Szvup032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935119","Asymmetric block (AB);attention fusion;remote sensing;solid waste (SW);waste detection;YOLOv5","Feature extraction;Waste materials;Remote sensing;Semantics;Scattering;Object detection;Detectors","image fusion;object detection;public utilities;waste management","ADA;aerial images;anchor-based object detector;asymmetric deep aggregation network;China;efficient attention fusion pyramid network;Henan province;irregular shape waste;limited detection performance;solid waste aerial detection;solid waste detection;SW detection;SWAD;SWDet;waste classification;waste features;waste pollution","","","","61","CCBY","2 Nov 2022","","","IEEE","IEEE Journals"
"Subpixel Land Cover Mapping Based on Dual Processing Paths for Hyperspectral Image","P. Wang; G. Zhang; L. Wang; H. Leung; H. Bi","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Key Laboratory of Radar Imaging and Microwave Photonics Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, China; Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada; Key Laboratory of Radar Imaging and Microwave Photonics Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Jul 2019","2019","12","6","1835","1848","The subpixel mapping (SPM) technique can handle coarse fractional images derived by unmixing coarse original hyperspectral (HS) image to produce a fine land cover map at the subpixel scale. A popular SPM approach is a two-step model. It first increases the spatial resolution of coarse fractional images by subpixel sharpening to produce fine fractional images and then assigns class labels to each subpixel by the class allocation method. However, there is only a single processing path of the current SPM algorithm, and the information type of the fine fractional images is not rich. To enrich the information type, SPM based on dual processing paths (DPP) is proposed. DPP contains two processing paths, namely spatial-spectral path and multiscale path. First, the coarse original HS image and the high spatial resolution multispectral image are fused by component substitution to produce the fine fractional images with more spatial-spectral information in the spatial-spectral path. At the same time, deep Laplacian pyramid networks are used to obtain the fine fractional images with multiscale information in the multiscale path. The fine fractional images from the two paths are then integrated to generate the improved fraction images with multiscale spatial-spectral information. Finally, the multiscale spatial-spectral information is utilized to allocate class labels by the class allocation method. Experimental results on three real HS remote sensing data show that the proposed DPP outperforms the other SPM methods, demonstrating the effectiveness of the use of DPP in enriching the information type of the fine fractional images.","2151-1535","","10.1109/JSTARS.2019.2910539","National Natural Science Foundation of China(grant numbers:61801211,61871218,61675051); Fundamental Research Funds for the Central Universities(grant numbers:1004-YAH18050,3082017NP2017421); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703395","Deep Laplacian pyramid networks (DLPN);dual processing paths (DPP);hyperspectral image;hyperspectral image and multispectral image fusion;subpixel mapping (SPM)","Spatial resolution;Resource management;Hyperspectral imaging;Interpolation","geophysical image processing;hyperspectral imaging;image fusion;image resolution;land cover;terrain mapping","information type;fine fractional images;subpixel land cover mapping;dual processing paths;subpixel mapping technique;coarse fractional images;coarse original hyperspectral image;fine land cover map;spatial-spectral path;multiscale path;coarse original HS image;high spatial resolution multispectral image;improved fraction images;multiscale spatial-spectral information;SPM approach;two-step model;subpixel sharpening;class allocation method;single processing path;deep Laplacian pyramid networks;real HS remote sensing data","","9","","52","IEEE","30 Apr 2019","","","IEEE","IEEE Journals"
"Evaluation of Optical and Radar Images Integration Methods for LULC Classification in Amazon Region","L. O. Pereira; C. C. Freitas; S. J. S. Sant´Anna; M. S. Reis","Department of Geography, College of Life and Environmental Sciences, University of Exeter, Exeter, U.K.; Division of Image Processing (DPI), Brazilian National Institute for Space Research (INPE), São José dos Campos, Brazi; Division of Image Processing (DPI), Brazilian National Institute for Space Research (INPE), São José dos Campos, Brazi; Division of Image Processing (DPI), Brazilian National Institute for Space Research (INPE), São José dos Campos, Brazi","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Sep 2018","2018","11","9","3062","3074","The main objective of this study is to evaluate different methods to integrate (fusion and combination) Synthetic Aperture Radar (SAR) Advanced Land Observing Satellite (ALOS) Phased Arrayed L-band SAR (PALSAR-1) (Fine Beam Dual mode-FDB) and LANDSAT images in order to identify those which lead to higher accuracy of land-use and land-cover (LULC) mapping in an agricultural frontier region in Amazon. One method used to integrate the multipolarized information in SAR images before the fusion process was also evaluated. In this method, the first principal component (PC1) of SAR data was used. Color compositions of fused data that presented better LULC classification were visually analyzed. Considering the proposed objective, the following fusion methods must be highlighted: Ehlers, Wavelet á trous, Intensity, Hue and Saturation (IHS), and selective principal component analysis (SPC). These latter three methods presented good results when processed using PC1 from ALOS/PALSAR-1 FBD backscatter filtered image or three SAR extracted and selected features. These results corroborate with the applicability of the proposed method for SAR data information integration. Distinct methods better discriminate different LULC classes. In general, densely forested classes were better characterized by the Ehlers_TM6 fusion method, in which at least the polarization HV was used. Intermediate and initial regeneration classes were better discriminated using SPC-fused data with PC1 of ALOS/PALSAR-1 FBD data. Bare soil and pasture classes were better discriminated in optical features and the PC1 of ALOS/PALSAR-1 FBD data fused by the IHS method. Soybean with approximately 40 days from seeding was better discriminated in image classification obtained from ALOS/PALSAR-1 FBD image.","2151-1535","","10.1109/JSTARS.2018.2853647","Conselho Nacional de Desenvolvimento Científico e Tecnológico(grant numbers:301118/2017-5); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; Monitoramento Ambiental por Satélite no Bioma Amazônia(grant numbers:1022114003005-MSA-BNDES); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416958","Brazilian Amazon;data integration;land-use and land-cover (LULC);multipolarized-synthetic aperture radar (SAR)","Synthetic aperture radar;Optical imaging;Optical sensors;Optical polarization;Data integration;Feature extraction;Earth","geophysical image processing;image classification;image fusion;land cover;land use;principal component analysis;radar polarimetry;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar;terrain mapping","LULC classification;Amazon region;L-band SAR;land-cover mapping;agricultural frontier region;SAR images;fusion process;fusion methods;SAR data information integration;SPC-fused data;IHS method;image classification;land-use mapping;principal component analysis;radar image integration method;optical image integration method;Synthetic Aperture Radar Advanced Land Observing Satellite;fine beam dual mode;ALOS-PALSAR-1 FBD backscatter;ALOS-PALSAR-1 FBD image;Ehlers_TM6 fusion method;densely forested class;LANDSAT image","","10","","52","IEEE","20 Jul 2018","","","IEEE","IEEE Journals"
"Bayesian Hyperspectral and Multispectral Image Fusions via Double Matrix Factorization","B. Lin; X. Tao; M. Xu; L. Dong; J. Lu","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; School of Electronics and Information Engineering, Beijing University of Aeronautics and Astronautics, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","25 Sep 2017","2017","55","10","5666","5678","This paper focuses on fusing hyperspectral and multispectral images with an unknown arbitrary point spread function (PSF). Instead of obtaining the fused image based on the estimation of the PSF, a novel model is proposed without intervention of the PSF under Bayesian framework, in which the fused image is decomposed into double subspace-constrained matrix-factorization-based components and residuals. On the basis of the model, the fusion problem is cast as a minimum mean square error estimator of three factor matrices. Then, to approximate the posterior distribution of the unknowns efficiently, an estimation approach is developed based on variational Bayesian inference. Different from most previous works, the PSF is not required in the proposed model and is not pre-assumed to be spatially invariant. Hence, the proposed approach is not related to the estimation errors of the PSF and has potential computational benefits when extended to spatially variant imaging system. Moreover, model parameters in our approach are less dependent on the input data sets and most of them can be learned automatically without manual intervention. Exhaustive experiments on three data sets verify that our approach shows excellent performance and more robustness to the noise with acceptable computational complexity, compared with other state-of-the-art methods.","1558-0644","","10.1109/TGRS.2017.2711640","National Natural Science Foundation of China(grant numbers:61622110,61471220,91538107); National Basic Research Project of China (973)(grant numbers:2013CB329006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959610","Double matrix factorization;image fusion;subspace-constrained image model;variational Bayesian inference","Bayes methods;Matrix decomposition;Computational modeling;Spatial resolution;Mathematical model;Estimation error;Imaging","Bayes methods;estimation theory;hyperspectral imaging;image fusion;inference mechanisms;matrix decomposition;mean square error methods;optical transfer function","Bayesian hyperspectral image fusions;Bayesian multispectral image fusions;double subspace-constrained matrix-factorization;point spread function;PSF;mean square error estimator;estimation approach;variational Bayesian inference","","18","","56","IEEE","27 Jun 2017","","","IEEE","IEEE Journals"
"Subpixel-Pixel-Superpixel Guided Fusion for Hyperspectral Anomaly Detection","Z. Huang; L. Fang; S. Li","Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2020","2020","58","9","5998","6007","Most of the existing hyperspectral anomaly detectors are designed based on a single pixel-level feature. These detectors may not adequately utilize spectral-spatial information in hyperspectral images (HSIs) for detecting anomalies. To overcome this problem, this article introduces a novel subpixel-pixel-superpixel guided fusion (SPSGF) method for hyperspectral anomaly detection. This approach comprises three main steps. First, subpixel-, pixel-, and superpixel-level features are extracted from an HSI by employing the spectral unmixing, morphological operation, and superpixel segmentation techniques, respectively. Then, based on the spatial consistency of three features, a guided filtering-based weight optimization technique is developed to construct weight maps for fusion. Finally, a simple yet effective decision fusion method is adopted to utilize the complemental information of three features, and then generates a fused detection result. The performance of the proposed approach is evaluated on three real-scene HSIs and one synthetic HSI. Experimental results validate the advantages of the SPSGF method.","1558-0644","","10.1109/TGRS.2019.2961703","National Natural Science Foundation of China for International Cooperation and Exchanges(grant numbers:61520106001); Fund of Hunan Province for Science and Technology Plan Project(grant numbers:2017RS3024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018386","Anomaly detection;guided filtering;hyperspectral images (HSIs);image fusion;subpixel","Feature extraction;Detectors;Hyperspectral imaging;Anomaly detection;Optimization;Object detection","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image segmentation","hyperspectral anomaly detection;hyperspectral anomaly detectors;single pixel-level feature;spectral-spatial information;hyperspectral images;superpixel-level features;decision fusion;filtering-based weight optimization;subpixel-pixel-superpixel guided fusion;superpixel segmentation;SPSGF method;HSI","","17","","55","IEEE","28 Feb 2020","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and Multispectral Images Accounting for Localized Inter-Image Changes","X. Fu; S. Jia; M. Xu; J. Zhou; Q. Li","Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources and the College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources and the College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources and the College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","14 Feb 2022","2022","60","","1","18","The high spectral resolution of hyperspectral images (HSIs) generally comes at the expense of low spatial resolution, which hinders the application of HSIs. Fusing an HSI and a multispectral image (MSI) from different sensors to get an image with the high spatial and spectral resolution is an economic and effective approach, but localized spatial and spectral changes between images acquired at different time instants can have negative impacts on the fusion results, which has rarely been considered in many fusion methods. In this article, we propose a novel group sparsity constrained fusion (GSFus) method to fuse hyperspectral and MSIs based on matrix factorization. Specifically, we imposed  $\ell _{2,1}$  norm on the residual term of the MSI to account for the localized interimage changes occurring during the acquisition of the hyperspectral and MSIs. Furthermore, by exploiting the plug-and-play framework, we plugged a state-of-the-art denoiser, namely block-matching and 3-D filtering (BM3D), as the prior of the subspace coefficients. We refer to the proposed fusion method as GSFus method. We performed fusion experiments on two kinds of datasets, i.e., with and without obvious localized changes between the HSIs and MSIs, and a full resolution dataset. Extensive experiments in comparison with seven state-of-the-art fusion methods suggest that the proposed fusion method is more effective on fusing hyperspectral and MSIs than the competitors.","1558-0644","","10.1109/TGRS.2021.3124240","National Natural Science Foundation of China(grant numbers:41971300,61901278,62001303); Key Project of Department of Education of Guangdong Province(grant numbers:2020ZDZX3045); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011413); China Postdoctoral Science Foundation(grant numbers:2021M692162); Shenzhen Scientific Research and Development Funding Program(grant numbers:20200803152531004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9594847","Group sparsity;hyperspectral image (HSI);image fusion;interimage changes;multispectral image (MSI)","Spatial resolution;Hyperspectral imaging;Tensors;Image resolution;Fuses;Matrix decomposition;Sensor fusion","geophysical image processing;hyperspectral imaging;image classification;image colour analysis;image denoising;image filtering;image fusion;image resolution;matrix decomposition;optimisation","spectral resolution;hyperspectral images;HSI;spatial resolution;economic approach;localized spatial changes;spectral changes;localized interimage changes;hyperspectral MSI;GSFus method;resolution dataset;multispectral images;group sparsity constrained fusion;l2,1 norm;plug-and-play framework;block-matching and 3D filtering;BM3D;subspace coefficients;full resolution dataset;denoiser","","3","","75","IEEE","29 Oct 2021","","","IEEE","IEEE Journals"
"Siamese Networks Based Deep Fusion Framework for Multi-Source Satellite Imagery","H. Adeel; J. Tahir; M. M. Riaz; S. S. Ali","Department of Electrical & Computer Engineering, COMSATS University Islamabad, Islamabad, Pakistan; Department of Computer Science, COMSATS University Islamabad, Islamabad, Pakistan; Centre for Advanced Studies in Telecommunication, COMSATS University Islamabad, Islamabad, Pakistan; CGG Services, Crawley, U.K","IEEE Access","26 Jan 2022","2022","10","","8728","8737","A critical aim of pansharpening is to fuse coherent spatial and spectral features from panchromatic and multispectral images respectively. This study proposes deep siamese network based pansharpening model as a two-stage framework in a multiscale setting. In the first stage, a siamese network learns a common feature space between panchromatic and multispectral bands. The second stage follows by fusing the output feature maps of the siamese network. The parameters of these two stages are shared across scales in order to add spatial information consistently (across scales). The spectral information is preserved by adding appropriate skip connections from input multispectral image. Multi-level network parameters sharing mechanism in pyramidal reconstruction of pansharpened image, better preserves spatial and spectral details simultaneously. Experimental work carried out using deep siamese network in multi-scale setting (to obtain inter-band similarity among different sensor data) outperforms several latest pansharpening methods.","2169-3536","","10.1109/ACCESS.2022.3143847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684359","Pansharpening;image fusion;deep-learning;siamese networks;remote sensing;depth-of-field","Pansharpening;Image resolution;Spatial resolution;Feature extraction;Optimization;Distortion;Estimation","feature extraction;geophysical image processing;image colour analysis;image fusion;image reconstruction;image representation;image resolution;learning (artificial intelligence);remote sensing","panchromatic images;multispectral images;deep siamese network based pansharpening model;two-stage framework;common feature space;panchromatic bands;multispectral bands;output feature maps;spatial information;spectral information;input multispectral image;multilevel network parameters;pansharpened image;spectral details;multiscale setting;latest pansharpening methods;deep fusion framework;multisource satellite imagery;coherent spatial features;spectral features","","","","45","CCBY","18 Jan 2022","","","IEEE","IEEE Journals"
"An Efficient Dual Spatial–Spectral Fusion Network","Q. Guo; S. Li; A. Li","Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","29 Nov 2022","2022","60","","1","13","The spectral fidelity and spatial richness enhancement are two primary objectives for the deep learning pan-sharpening algorithms. Consequently, the two-stream fusion architecture is used to focus on individual features of panchromatic (PAN) and multispectral (MS) images. However, most existing two-stream networks have a high degree of extracted feature redundancy and large fusion workload. Based on these, we propose a dual spatial–spectral fusion network (DSSN) to implement the divided-objective fusion, with one stream fusing spatial information and the other stream fusing spectral information. We propose a bifurcation spatial fusion stream consisted of backbone and gradient branch. The gradient feature extracted as prior knowledge to direct the backbone to enhance the spatial information richness. Moreover, the multiscale residual block (MRB) is designed to guarantee the sufficiency of extracted features. In MRBs, the densely connected small size and asymmetric convolution kernels are used to replace the large-size ones to reduce the network complexity. In the spectral fusion stream, 1-D convolution is adopted to efficiently fuse the spectral features of PAN and MS in the channel dimensions. Finally, the fused spatial and spectral information optimally integrate with a certain proportion to be the final result. The ablation studies confirm that each stream of DSSN is indispensable in the spectral preservation and spatial detail recovery. Three different types of optical data are used to assess practicability and universality of DSSN. The experimental results show that the fusion quality of DSSN exceeds the commonly used pan-sharpening methods, with fast training convergence speed.","1558-0644","","10.1109/TGRS.2022.3222223","Leading Foundation on Frontier Sciences and Disruptive Technology Research of the Aerospace Information Research Institute, Chinese Academy of Sciences(grant numbers:E0Z218010F); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954048","Asymmetric convolution;deep learning;multiscale residual blocks (MRBs);multispectral (MS) image fusion;pan-sharpening;panchromatic (PAN);two-stream networks","Feature extraction;Convolution;Kernel;Convolutional neural networks;Deep learning;Streaming media;Fuses","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image resolution","bifurcation spatial fusion stream;deep learning pan-sharpening algorithms;divided-objective fusion;DSSN;extracted feature redundancy;fused spatial information;fusion quality;fusion workload;gradient feature;spatial detail recovery;spatial information;spatial information richness;spatial richness enhancement;spatial-spectral fusion network;spectral features;spectral fidelity;spectral fusion stream;spectral information;spectral preservation;two-stream fusion architecture;two-stream networks","","","","39","IEEE","17 Nov 2022","","","IEEE","IEEE Journals"
"Quantitative Quality Evaluation of Pansharpened Imagery: Consistency Versus Synthesis","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson; J. A. Benediktsson","Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","29 Feb 2016","2016","54","3","1247","1259","Pansharpening is the process of fusing a high-resolution panchromatic image and a low-spatial-resolution multispectral image to yield a high-spatial-resolution multispectral image. This is a typical ill-posed inverse problem, and in the past two decades, many methods have been proposed to solve it. Still, there is no general consensus on the best way to quantitatively evaluate the spectral and spatial quality of the fused image. In this paper, we compare the two most widely used and accepted methods for quality evaluation. The first method is the verification of the synthesis property which states that the fused image should be as identical as possible to the multispectral image that the sensor would observe at a higher resolution. This is impossible to verify unless the observed images are spatially degraded so that the original observed multispectral image can be used as reference. The second method is to use metrics that do not use a reference, such as the quality no reference (QNR) metrics. However, there is another property, i.e., the consistency property, which states that the fused image reduced to the resolution of the original multispectral image should be as identical to the original image as possible. This has generally been considered a necessary condition that does not have to imply correct fusion. Using real WorldView-2 and QuickBird data and a total of 18 component substitution and multiresolution analysis methods, we demonstrate that the consistency property can indeed be used to give reliable assessment of the relative performance of pansharpening methods and is superior to using the QNR metrics.","1558-0644","","10.1109/TGRS.2015.2476513","Doctoral Grants of the University of Iceland Research Fund and the University of Iceland Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7287786","Consistency;image fusion;pansharpening;quality no reference (QNR);quantitative quality evaluation;synthesis;Consistency;image fusion;pansharpening;quality no reference (QNR);quantitative quality evaluation;synthesis","Measurement;Protocols;Spatial resolution;Indexes;Reliability;Quality assessment","image enhancement;image resolution;sensor fusion","pansharpened imagery;high-resolution panchromatic image;low-spatial-resolution multispectral image;ill-posed inverse problem;quality no reference metrics;QNR metrics;consistency property;pansharpening method","","63","","56","IEEE","2 Oct 2015","","","IEEE","IEEE Journals"
"A Two-Level Approach for Species Identification of Coniferous Trees in Central Ontario Forests Based on Multispectral Images","J. Li; B. Hu; M. Woods","Department of Earth and Space Science and Engineering, York University, Toronto, ON, Canada; Department of Earth and Space Science and Engineering, York University, Toronto, ON, Canada; Ontario Ministry of Natural Resources and Forestry, North Bay, ON, Canada","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","4","1487","1497","This study aims to provide detailed spatial information of valuable tree species to support improved management of winter habitat of white-tailed deer. To achieve this, we proposed a novel approach using information from two spatial scales and a suite of methods for analysis and classification of remotely sensed data. High-spatial resolution, multispectral images were employed to test the proposed method. A new structure-based remote sensing feature [local binary pattern (LBP) index] was developed and proved to be effective for species classification. A simple but effective fusion approach based on information entropy theory was proposed to incorporate features derived from different methods and their uncertainties. Based on tenfold cross validation, an overall accuracy (OA) of 77% was obtained for the classification of three tree species groups. The proposed approach has high potential to improve species mapping for operational ecological modeling.","2151-1535","","10.1109/JSTARS.2015.2423272","Natural Sciences and Engineering Research Council (NSERC) of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7098334","Entropy;forestry;image processing;Entropy;forestry;image processing","Vegetation;Spatial resolution;Remote sensing;Feature extraction;Earth;Image segmentation","ecology;entropy;geophysical image processing;image classification;image fusion;image resolution;vegetation;vegetation mapping","two-level approach;coniferous trees;Central Ontario forests;spatial information;valuable tree species identification;winter habitat management;white-tailed deer;spatial scales;remotely sensed data;high-spatial resolution;multispectral images;structure-based remote sensing feature;local binary pattern index;valuable tree species classification;effective fusion approach;information entropy theory;tree species groups;species mapping;operational ecological modeling","","3","","31","IEEE","30 Apr 2015","","","IEEE","IEEE Journals"
"ComNet: Combinational Neural Network for Object Detection in UAV-Borne Thermal Images","M. Li; X. Zhao; J. Li; L. Nan","College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electronic and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Faculty of Architecture and the Built Environment, Delft University of Technology, Delft, BL, The Netherlands","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6662","6673","We propose a deep learning-based method for object detection in UAV-borne thermal images that have the capability of observing scenes in both day and night. Compared with visible images, thermal images have lower requirements for illumination conditions, but they typically have blurred edges and low contrast. Using a boundary-aware salient object detection network, we extract the saliency maps of the thermal images to improve the distinguishability. Thermal images are augmented with the corresponding saliency maps through channel replacement and pixel-level weighted fusion methods. Considering the limited computing power of UAV platforms, a lightweight combinational neural network ComNet is used as the core object detection method. The YOLOv3 model trained on the original images is used as a benchmark and compared with the proposed method. In the experiments, we analyze the detection performances of the ComNet models with different image fusion schemes. The experimental results show that the average precisions (APs) for pedestrian and vehicle detection have been improved by 2%~5% compared with the benchmark without saliency map fusion and MobileNetv2. The detection speed is increased by over 50%, while the model size is reduced by 58%. The results demonstrate that the proposed method provides a compromise model, which has application potential in UAV-borne detection tasks.","1558-0644","","10.1109/TGRS.2020.3029945","National Natural Science Foundation of China(grant numbers:41801342); Natural Science Foundation of Jiangsu Province, China(grant numbers:BK20170781); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234491","Combinational neural networks;model compression;saliency map;thermal image","Object detection;Computational modeling;Feature extraction;Task analysis;Lighting;Neural networks;Image segmentation","autonomous aerial vehicles;deep learning (artificial intelligence);image fusion;infrared imaging;neural nets;object detection","UAV-borne detection tasks;UAV-borne thermal images;deep learning-based method;visible images;boundary-aware salient object detection network;UAV platforms;lightweight combinational neural network ComNet;core object detection method;detection performances;image fusion schemes;pedestrian;vehicle detection;saliency map fusion;detection speed;YOLOv3 model","","10","","48","IEEE","21 Oct 2020","","","IEEE","IEEE Journals"
"FusionLane: Multi-Sensor Fusion for Lane Marking Semantic Segmentation Using Deep Neural Networks","R. Yin; Y. Cheng; H. Wu; Y. Song; B. Yu; R. Niu","Lappeenranta University of Technology (LUT), Lappeenranta, Finland; Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, China; Mechanical Department, School of Energy Systems, Lappeenranta University of Technology (LUT), Lappeenranta, Finland; Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, China; Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, China; Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, China","IEEE Transactions on Intelligent Transportation Systems","2 Feb 2022","2022","23","2","1543","1553","Effective semantic segmentation of lane marking is crucial for construction of high-precision lane level maps. In recent years, a number of different methods for semantic segmentation of images have been proposed. These methods concentrate mainly on analysis of camera images, due to limitations with the sensor itself, and thus far, the accurate three-dimensional spatial position of the lane marking could not be obtained, which hinders lane level map construction.This article proposes a lane marking semantic segmentation method based on LIDAR and camera image fusion using a deep neural network. In the approach, the object of the semantic segmentation is a bird’s-eye view converted from a LIDAR points cloud instead of an image captured by a camera. First, the DeepLabV3+ network image segmentation method is used to segment the image captured by the camera, and the segmentation result is then merged with the point clouds collected by the LIDAR as the input of the proposed network. A long short-term memory (LSTM) structure is added to the neural network to assist the network in semantic segmentation of lane markings by enabling use of time series information. Experiments on datasets containing more than 14,000 images, which were manually labeled and expanded, showed that the proposed method provides accurate semantic segmentation of the bird’s-eye view LIDAR points cloud. Consequently, automation of high-precision map construction can be significantly improved. Our code is available at https://github.com/rolandying/FusionLane.","1558-0016","","10.1109/TITS.2020.3030767","National Key Research and Development Program of China(grant numbers:2016YFD0701401,2017YFD0700303,2018YFD0700602); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2017488); Key Supported Project in the Thirteenth Five-Year Plan of Hefei Institutes of Physical Science, Chinese Academy of Sciences(grant numbers:KP-2017-35,KP-2017-13,KP-2019-16); Independent Research Project of Research Institute of Robotics and Intelligent Manufacturing Innovation, Chinese Academy of Sciences(grant numbers:C2018005); Technological Innovation Project for New Energy and Intelligent Networked Automobile Industry of Anhui Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9237136","Lane marking;semantic segmentation;LIDAR-camera fusion;convolutional neural network;LSTM","Semantics;Image segmentation;Cameras;Laser radar;Three-dimensional displays;Neural networks;Meters","geophysical image processing;image fusion;image segmentation;optical radar;recurrent neural nets;remote sensing by laser beam;remote sensing by radar;statistical analysis;time series","multisensor fusion;lane marking semantic segmentation;deep neural network;effective semantic segmentation;high-precision lane level maps;camera images;lane level map construction;semantic segmentation method;camera image fusion;LIDAR points cloud;DeepLabV3+ network image segmentation method;accurate semantic segmentation;bird;high-precision map construction","","6","","27","IEEE","22 Oct 2020","","","IEEE","IEEE Journals"
"Scene Classification Based on the Sparse Homogeneous–Heterogeneous Topic Feature Model","Q. Zhu; Y. Zhong; S. Wu; L. Zhang; D. Li","Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","20 Apr 2018","2018","56","5","2689","2703","High spatial resolution (HSR) imagery scene classification has been the subject of increased interest in recent years, and has great potential for many applications, such as urban functional analysis. Rooted in natural information processing, the use of the probabilistic topic model (PTM) to capture latent topics to represent HSR images has been an effective way to bridge the semantic gap. However, how to effectively discover discriminative information to recognize the HSR scenes is a challenging task. In this paper, the sparse homogeneous-heterogeneous topic feature model (SHHTFM) is proposed for HSR image scene classification. Differing from the conventional PTM-based scene classification methods, which utilize only heterogeneous features, SHHTFM explores the effect of the homogeneous information. Based on the union of uniform grid sampling and simple linear iterative clustering superpixel sampling, SHHTFM exploits both the heterogeneous and homogeneous information. After separately mining different types of low-level features and latent topics, the sparse topic inference procedure of SHHTFM further improves the fusion of the sparse heterogeneous and homogeneous topics. In addition, multisource geographical data are effectively integrated, where the water and vegetation boundaries define a more accurate way to restrict the boundaries of different scenes, and are then combined with the road network data to further improve the scene annotation performance. This provides more reliable and applicable results for us to better understand the complex scenes. The experimental results obtained with two HSR image classification data sets and an HSR image annotation data set demonstrate that the proposed SHHTFM framework can solve the scene classification problem, with a high classification accuracy as well as a high time efficiency.","1558-0644","","10.1109/TGRS.2017.2781712","National Key Research and Development Program of China(grant numbers:2017YFB0504202); National Natural Science Foundation of China(grant numbers:41622107,41771385,41371344); Natural Science Foundation of Hubei Province in China(grant numbers:2016CFA029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288819","Geographical;high spatial resolution (HSR) imagery;homogeneous topics;multisource;probabilistic topic model (PTM);scene classification;scene understanding","Feature extraction;Visualization;Semantics;Roads;Remote sensing;Image segmentation;Training","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;inference mechanisms;iterative methods;probability","sparse topic inference procedure;heterogeneous information;homogeneous information;heterogeneous features;scene classification methods;HSR image scene classification;SHHTFM;HSR scenes;latent topics;probabilistic topic model;natural information processing;high spatial resolution imagery scene classification;sparse homogeneous-heterogeneous topic feature model;high classification accuracy;scene classification problem;HSR image annotation data;HSR image classification data sets;complex scenes;scene annotation performance","","49","","46","IEEE","9 Feb 2018","","","IEEE","IEEE Journals"
"A New Method to Enhance the Spatial Features of Multitemporal NDVI Image Series","F. Maselli; M. Chiesi; M. Pieri","National Research Council of Italy, Institute of Biometeorology, Sesto Fiorentino, Italy; National Research Council of Italy, Institute of Biometeorology, Sesto Fiorentino, Italy; LaMMA Consortium, Sesto Fiorentino, Italy","IEEE Transactions on Geoscience and Remote Sensing","24 Jun 2019","2019","57","7","4967","4979","A novel spatiotemporal fusion (STF) method is presented to enhance the spatial features of low-spatial-resolution (LR) normalized difference vegetation index (NDVI) image series based on single-date high-spatial-resolution (HR) imagery. The method is particularly suitable for areas where the main vegetation types show asynchronous NDVI evolutions whose spatial distribution cannot be properly characterized by a single-date HR image. In contrast with previous STF methods, the new algorithm identifies these vegetation types by automatically decomposing the LR multitemporal data series, which offers a complete description of major seasonal NDVI evolutions. The new method, named Spatial Enhancer of Vegetation Index image Series (SEVIS), is tested in two Italian study areas using annual MODIS NDVI data sets and some Landsat 8 OLI images taken in different seasons. The performances of SEVIS are analyzed in comparison with those of two other STF methods, the classical Spatial and Temporal Adaptive Fusion Model (STARFM) and the more recent Flexible Spatiotemporal DAta Fusion (FSDAF) algorithm. The results obtained indicate that the three methods perform differently depending mainly on the synchronicity of the NDVI evolutions from the base to the prediction dates. Specifically, SEVIS outperforms the other two methods when the NDVI values evolve differently during the prediction period, i.e., when the base and prediction images are poorly correlated.","1558-0644","","10.1109/TGRS.2019.2894850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8645762","Maximum likelihood classification (MLC);Moderate Resolution Imaging Spectroradiometer (MODIS);Operational Land Imager (OLI);Spatial Enhancer of Vegetation Index image Series (SEVIS);Sequential Maximum Angle Convex Cone (SMACC)","Vegetation mapping;Spatial resolution;MODIS;Indexes;Spatiotemporal phenomena;Sensors;Remote sensing","geophysical image processing;image fusion;image resolution;spatiotemporal phenomena;vegetation;vegetation mapping","spatial features;multitemporal NDVI image Series;novel spatiotemporal fusion method;low-spatial-resolution normalized difference vegetation index image series;single-date high-spatial-resolution imagery;main vegetation types;asynchronous NDVI evolutions;spatial distribution;single-date HR image;LR multitemporal data series;seasonal NDVI evolutions;SEVIS;annual MODIS NDVI data sets;Landsat 8 OLI images;NDVI values;prediction images;STF methods;Flexible Spatiotemporal DAta Fusion algorithm;Spatial Enhancer","","11","","44","IEEE","20 Feb 2019","","","IEEE","IEEE Journals"
"Dual-Stream Convolutional Neural Network With Residual Information Enhancement for Pansharpening","Y. Yang; W. Tu; S. Huang; H. Lu; W. Wan; L. Gan","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Computer Science and Technology, Tiangong University, Tianjin, China; College of Information Engineering, Jinhua Polytechnic, Jinhua, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Mathematics and Computer Science, Jiangxi Science and Technology Normal University, Nanchang, China","IEEE Transactions on Geoscience and Remote Sensing","17 Jan 2022","2022","60","","1","16","Deep-learning-based pansharpening methods have achieved remarkable results due to their powerful feature representation ability. However, the existing deep-learning-based pansharpening methods not only lack information exchange and sharing between features of different resolutions but also cannot effectively use the residual information at different levels. These disadvantages may lead to the loss of spatial information and spectral information in the pansharpened image. To address the above problems, we propose a novel dual-stream convolutional neural network with residual information enhancement (DSCNN-RIE) for pansharpening. The proposed network is mainly composed of a set of dual-stream information complementation blocks (DSICBs), which can extract various spatial details at two different resolutions using convolutional filters of various sizes simultaneously, and can transfer complementary information effectively between two different resolutions. Furthermore, to improve the learning ability of the network and enhance the feature extraction, an RIE strategy is presented to stack different levels of residuals into the outputs of cascaded DSICBs. The final pansharpened image is obtained by integrating the extracted features using the shallow feature information of the source images. Experimental results on three datasets demonstrate that DSCNN-RIE outperforms ten other state-of-the-art pansharpening methods in both subjective and objective image-quality evaluations.","1558-0644","","10.1109/TGRS.2021.3098752","National Natural Science Foundation of China(grant numbers:62072218,61862030); Natural Science Foundation of Jiangxi Province(grant numbers:20192ACB20002,20192ACBL21008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9503110","Dual-stream convolutional neural network (DSCNN);information complementation block;pansharpening;residual information enhancement (RIE)","Feature extraction;Pansharpening;Spatial resolution;Image resolution;Remote sensing;Data mining;Convolution","feature extraction;filtering theory;geophysical image processing;image fusion;image representation;image resolution;learning (artificial intelligence);neural nets","final pansharpened image;shallow feature information;state-of-the-art pansharpening methods;residual information enhancement;deep-learning-based pansharpening methods;powerful feature representation ability;existing deep-learning-based pansharpening;methods not only lack information exchange;spatial information;spectral information;novel dual-stream convolutional neural network;dual-stream information complementation blocks;convolutional filters;complementary information;learning ability;feature extraction","","6","","45","IEEE","2 Aug 2021","","","IEEE","IEEE Journals"
"Convolutional LSTM-Based Hierarchical Feature Fusion for Multispectral Pan-Sharpening","D. Wang; Y. Bai; C. Wu; Y. Li; C. Shang; Q. Shen","National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, Northwestern Polytechnical University, Xi’an, China; School of Communication and Information Engineering, Xi’an University of Posts and Telecommunications, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, Northwestern Polytechnical University, Xi’an, China; Department of Computer Science, Faculty of Business and Physical Sciences, Aberystwyth University, Aberystwyth, U.K.; Department of Computer Science, Faculty of Business and Physical Sciences, Aberystwyth University, Aberystwyth, U.K.","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","16","Multispectral (MS) pan-sharpening aims at producing high-resolution (HR) MS images in both spatial and spectral domains, by merging single-band panchromatic (PAN) images and corresponding MS images with low spatial resolution. The intuitive way to accomplish such MS pan-sharpening tasks, or to reconstruct ideal HR-MS images, is to extract feature pairs from the given PAN and MS images and to fuse the results. Therefore, feature extraction and feature fusion are two key components for MS pan-sharpening. This article presents a novel MS pan-sharpening network (MPNet), including a heterogeneous pair of feature extraction pathways (FEPs) and a convolutional long short-term memory (ConvLSTM)-based hierarchical feature fusion module (HFFM). Specifically, we design a PAN FEP to extract 2-D feature maps via 2-D convolutions and dual attention, while an MS FEP is introduced in an effort to obtain 3-D representations of MS image by 3-D convolutions and triple attention. To merge the resulting hierarchical features, the ConvLSTM-based HFFM is developed, leveraging intralevel fusion, interlevel fusion, and information exchange within one single framework. Here, the interlevel fusion is implemented with the ConvLSTM to capture the dependencies among hierarchical features, reduce redundant information, and effectively integrate them via its recurrent architecture. The information exchange between different FEPs helps enhance the representations for subsequent processing. Systematic comparative experiments have been conducted on three publicly available datasets at both reduced resolution and full resolution, demonstrating that the proposed MPNet outperforms state-of-the-art methods in the literature.","1558-0644","","10.1109/TGRS.2021.3104221","National Natural Science Foundation of China(grant numbers:61871460); Shaanxi Provincial Key Research and Development Program of China(grant numbers:2020KW-003); Strategic Partner Acceleration Award, U.K.(grant numbers:80761-AU201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520136","Convolutional long short-term memory (ConvLSTM);hierarchical feature fusion;information fusion;multispectral (MS) pan-sharpening;triple attention","Feature extraction;Spatial resolution;Remote sensing;Task analysis;Information exchange;Fuses;Computer architecture","feature extraction;geophysical image processing;geophysical signal processing;image fusion;image resolution","spectral domains;single-band panchromatic images;corresponding MS images;low spatial resolution;MS pan-sharpening tasks;ideal HR-MS images;feature pairs;feature extraction;novel MS pan-sharpening network;PAN FEP;2-D feature maps;MS FEP;MS image;resulting hierarchical features;ConvLSTM-based HFFM;intralevel fusion;interlevel fusion;convolutional LSTM-based hierarchical feature fusion;multispectral pan-sharpening aims;spatial domains","","2","","64","IEEE","20 Aug 2021","","","IEEE","IEEE Journals"
"Automatic Feature-Based Geometric Fusion of Multiview TomoSAR Point Clouds in Urban Area","Y. Wang; X. X. Zhu","Helmholtz Young Investigators Group “SiPEO”, Technische Universität München, Munich, Germany; Helmholtz Young Investigators Group “SiPEO”, Technische Universität München, Munich, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","3","953","965","Interferometric synthetic aperture radar (InSAR) techniques, such as persistent scatterer interferometry (PSI) or SAR tomography (TomoSAR), deliver three-dimensional (3-D) point clouds of the scatterers' positions together with their motion information relative to a reference point. Due to the SAR side-looking geometry, minimum of two point clouds from cross-heading orbits, i.e., ascending and descending, are required to achieve a complete monitoring over an urban area. However, these two point clouds are usually not coregistered due to their different reference points with unknown 3-D positions. In general, no exact identical points from the same physical object can be found in such two point clouds. This article describes a robust algorithm for fusing such two point clouds of urban areas. The contribution of this paper is finding the theoretically exact point correspondence, which is the end positions of façades, where the two point clouds close. We explicitly define this algorithm as “L-shape detection and matching,” in this paper, because the façades commonly appear as L-shapes in InSAR point cloud. This algorithm introduces a few important features for a reliable result, including point density estimation using adaptive directional window for better façade points detection and L-shape extraction using weighed Hough transform. The algorithm is fully automatic. Its accuracy is evaluated using simulated data. Furthermore, the proposed method is applied on two TomoSAR point clouds over Berlin with ascending and descending geometry. The result is compared with the first PSI point cloud fusion method (S. Gernhardt and R. Bamler, “Deformation monitoring of single buildings using meter-resolution SAR data in PSI,” ISPRS J. Photogramm. Remote Sens., vol. 73, pp. 68-79, 2012.) for urban area. Submeter consistency is achieved.","2151-1535","","10.1109/JSTARS.2014.2361430","International Graduate School of Science and Engineering and Technische Universit¨t München; Helmholtz Association under the framework of the Young Investigators Group; German Research Foundation; Gauss Centre for Supercomputing e V; GCS Supercomputer SuperMUC at Leibniz Supercomputing Centre; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6942160","Point cloud fusion;SAR tomography (TomoSAR);synthetic aperture radar (SAR);TerraSAR-X;Point cloud fusion;SAR tomography (TomoSAR);synthetic aperture radar (SAR);TerraSAR-X","Three-dimensional displays;Synthetic aperture radar;Orbits;Estimation;Urban areas;Image color analysis;Robustness","feature extraction;geophysical image processing;image fusion;radar interferometry;remote sensing by radar;synthetic aperture radar","automatic feature-based geometric fusion;multiview TomoSAR point clouds;urban area;interferometric synthetic aperture radar;InSAR techniques;persistent scatterer interferometry;SAR tomography;scatterer position 3-D point clouds;SAR side-looking geometry;reference points;L-shape matching;L-shape detection;L-shape extraction;PSI point cloud fusion method","","21","","55","OAPA","31 Oct 2014","","","IEEE","IEEE Journals"
"A Benchmarking Protocol for Pansharpening: Dataset, Preprocessing, and Quality Assessment","G. Vivone; M. Dalla Mura; A. Garzelli; F. Pacifici","Institute of Methodologies for Environmental Analysis, National Research Council, Tito Scalo, Italy; Institut Universitaire de France (IUF),; Department of Information Engineering and Mathematics, University of Siena, Siena, Italy; Maxar Technologies Inc., Westminster, CO, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","29 Jun 2021","2021","14","","6102","6118","Comparative evaluation is a requirement for reproducible science and objective assessment of new algorithms. Reproducible research in the field of pansharpening of very high resolution images is a difficult task due to the lack of openly available reference datasets and protocols. The contribution of this article is threefold, and it defines a benchmarking framework to evaluate pansharpening algorithms. First, it establishes a reference dataset, named PAirMax, composed of 14 panchromatic and multispectral image pairs collected over heterogeneous landscapes by different satellites. Second, it standardizes various image preprocessing steps, such as filtering, upsampling, and band coregistration, by providing a reference implementation. Third, it details the quality assessment protocols for reproducible algorithm evaluation.","2151-1535","","10.1109/JSTARS.2021.3086877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447896","Benchmarking;image fusion;open source;pansharpening;quality assessment;remote sensing;reproducible science;very high resolution optical images","Pansharpening;Indexes;Distortion;Spatial resolution;Quality assessment;Benchmark testing;Protocols","geophysical image processing;image registration;image resolution;remote sensing","multispectral image pairs;heterogeneous landscapes;image preprocessing;quality assessment protocols;pansharpening algorithms;panchromatic image pairs;PAirMax;satellites;image filtering;band coregistration","","33","","45","CCBY","7 Jun 2021","","","IEEE","IEEE Journals"
"Orientated Silhouette Matching for Single-Shot Ship Instance Segmentation","Z. Huang; R. Li","Beijing University of Chemical Technology, Beijing, China; Beijing University of Chemical Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","31 Dec 2021","2022","15","","463","477","Object detection and semantic segmentation have achieved remarkable performance propelled by deep convolutional neural networks. However, neither of them can well parse and deal with swarms of rotating ships in remote sensing images. In this article, we pay more attention to the instance-level segmentation task, which recognizes objects more effectively and straightly. We propose a new network architecture, called orientated silhouette matching network, employing multiscale features and instance-level masks to enable single-shot and anchor-box-free instance segmentation. To be specific, we propose a novel-orientated polar template mask with orientated mask IoU to better match the ship silhouette. We also design a multiscale feature propagation and fusion module to improve the precision of detection. To further improve the performance, our network adopts Res2Net and Soft-NMS. Extensive experiments on the open datasets, namely Airbus Ship, demonstrate that our method improves the average precision by 14.2 and 10.0 percentage points on Res2Net101, compared with PolarMask and YOLACT. The source code will be open source after the reviewing process.","2151-1535","","10.1109/JSTARS.2021.3132005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9633225","Anchor-free;instance segmentation;orientation estimation;ship detection;silhouette matching;single shot","Licenses","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image matching;image segmentation;neural net architecture;object detection;object recognition;remote sensing;ships","fusion module;Res2Net101;object detection;semantic segmentation;deep convolutional neural networks;remote sensing images;instance-level segmentation;network architecture;instance-level masks;anchor-box-free instance segmentation;ship silhouette;multiscale feature propagation;orientated silhouette matching network;single-shot ship instance segmentation;airbus ship;object recognition;novel-orientated polar template mask;IoU;soft-NMS","","2","","51","CCBY","2 Dec 2021","","","IEEE","IEEE Journals"
"Joint Nonlocal, Spectral, and Similarity Low-Rank Priors for Hyperspectral–Multispectral Image Fusion","T. Gelvez-Barrera; H. Arguello; A. Foi","Department of Electrical Engineering, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Systems Engineering, Universidad Industrial de Santander, Bucaramanga, Colombia; Unit of computing Sciences, Tampere University, Tampere, Finland","IEEE Transactions on Geoscience and Remote Sensing","21 Sep 2022","2022","60","","1","12","The fusion of a low-spatial-and-high-spectral resolution hyperspectral image (HSI) with a high-spatial-and-low-spectral resolution multispectral image (MSI) allows synthesizing a high-resolution image (HRI), supporting remote sensing applications, such as disaster management, material identification, and precision agriculture. Unlike existing variational methods using low-rank regularizations separately, we present an HSI-MSI fusion method promoting various low-rank regularizations jointly. Our method refines the HRI spatial and spectral correlations from the individual HSI and MSI data through the proper plug-and-play (PnP) of a nonlocal patch-based denoiser in the alternating direction method of multipliers (ADMM). Notably, we consider the nonlocal self-similarity, the spectral low-rank, and introduce a rank-one similarity prior. Furthermore, we demonstrate via an extensive empirical study that the rank-one similarity prior is an inherent characteristic of the HRI. Simulations over standard benchmark datasets show the effectiveness of the proposed HSI-MSI fusion outperforming state-of-the-art methods, particularly in recovering low-contrast areas.","1558-0644","","10.1109/TGRS.2022.3203294","Academy of Finland(grant numbers:310779,326439,326473); Sistema General de Regalías (SGR)(grant numbers:8933); Colciencias scholarship Doctorados Nacionales—785; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9872034","Hyperspectral image-multispectral image (HSI-MSI) fusion;low-rank regularizations;nonlocal patch-based denoiser;plug-and-play with alternating direction method of multipliers (PnP-ADMM)","Human-robot interaction;Indexes;Image color analysis;Spatial resolution;Hyperspectral imaging;Histograms;Colored noise","","","","","","50","IEEE","31 Aug 2022","","","IEEE","IEEE Journals"
"Locality Preserving Composite Kernel Feature Extraction for Multi-Source Geospatial Image Analysis","Y. Zhang; S. Prasad","Hyperspectral Image Analysis Group, Electrical and Computer Engineering Department, University of Houston, Houston, TX, USA; Hyperspectral Image Analysis Group, Electrical and Computer Engineering Department, University of Houston, Houston, TX, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","3","1385","1392","Multi-source data, either from different sensors or disparate features extracted from the same sensor, are valuable for geospatial image analysis due to their potential for providing complementary features. In this paper, a composite-kernel-based feature extraction method is proposed for multi-source remote sensing data classification. Features from different sources are first fused via a weighted composite kernel mapping, and then projected to a lower-dimensional subspace in which kernel local Fisher discriminant analysis (KLFDA) is used to extract the most discriminative information. We hypothesize that after such a projection, multi-source data would have better class separability between classes, and an efficient linear classification model-multinomial logistic regression (MLR) would be suitable for classification. The efficacy of the proposed method is demonstrated via experiments using two different sets of multi-source geospatial data. For feature fusion, the raw spectral data and extended multi-attribute profiles (EMAPs) derived from the hyperspectral image are used as a testbed for multi-source image analysis. The second multi-source testbed used for validation involves sensor fusion, in which the hyperspectral and light detection and ranging (LiDAR) data are utilized. Experimental results show that composite kernel local Fisher's discriminant analysis when combined with MLR based classifier (CKLFDA-MLR) is very effective at feature extraction and classification of multi-source geospatial images.","2151-1535","","10.1109/JSTARS.2014.2348537","National Aeronautics and Space Administration(grant numbers:NNX12AL49G,NNX14AI47G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6897949","Classification;composite kernel;feature extraction;hyperspectral imagery;multi-source data;Classification;composite kernel;feature extraction;hyperspectral imagery;multi-source data","Kernel;Hyperspectral imaging;Feature extraction;Laser radar;Image analysis","feature extraction;geophysical image processing;image classification;image fusion;remote sensing by laser beam","locality preserving composite kernel feature extraction;multisource geospatial image analysis;multisource data;complementary features;composite-kernel-based feature extraction method;multisource remote sensing data classification;weighted composite kernel mapping;kernel local Fisher discriminant analysis;KLFDA;projection data;efficient linear classification model;multinomial logistic regression;multisource geospatial data;feature fusion;raw spectral data;extended multiattribute profiles;LiDAR data;feature extraction","","24","","19","IEEE","12 Sep 2014","","","IEEE","IEEE Journals"
"Joint Classification of Hyperspectral and Multispectral Images for Mapping Coastal Wetlands","C. Liu; R. Tao; W. Li; M. Zhang; W. Sun; Q. Du","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","982","996","It is significant for restoration and protection of natural resources and ecological services in coastal wetlands to map different land cover types with satellite remote sensing data. Considering difficulties of wetland species classification, hyperspectral images (HSIs) with high spectral resolution and multispectral images (MSI) with high spatial resolution are considered to achieve complementary advantages of multisource data. An effective approach, named as multistream convolutional neural network, is proposed to achieve fine classification of coastal wetlands. First, regression processing is adopted to make chaotically scattered coastal wetland data more compact and different. Second, through appropriate feature extraction and feature fusion strategies, high-level information of multisource data in regression domain is fused to distinguish different land cover. Experiments on GF-5 HSIs and Sentinel-2 MSIs are carried out in order to validate the classification performance of the proposed approach in two coastal wetlands of research value in China, i.e., Yellow River Estuary and Yancheng coastal wetland. Experimental results demonstrate the effectiveness of the proposed method compared with the state-of-the-art methods in the field, especially when the number of sample size is extremely small.","2151-1535","","10.1109/JSTARS.2020.3040305","Beijing Natural Science Foundation(grant numbers:JQ20021); National Natural Science Foundation of China(grant numbers:61922013,61421001,U1833203); Beijing Natural Science Foundation(grant numbers:L191004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268458","Coastal wetlands;convolutional neural network (CNN);data fusion;hyperspectral imagery (HSI);least squares regression (LSR);multispectral imagery (MSI)","Wetlands;Sea measurements;Feature extraction;Vegetation mapping;Hyperspectral imaging;Spatial resolution;Earth","ecology;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;neural nets;regression analysis;remote sensing;rivers","multispectral images;mapping coastal;natural resources;ecological services;coastal wetlands;land cover types;satellite remote sensing data;considering difficulties;wetland species classification;hyperspectral images;high spectral resolution;high spatial resolution;multisource data;multistream convolutional neural network;fine classification;chaotically scattered coastal wetland data;high-level information;classification performance","","14","","73","CCBY","24 Nov 2020","","","IEEE","IEEE Journals"
"Inverse Coefficient of Variation Feature and Multilevel Fusion Technique for Hyperspectral and LiDAR Data Classification","F. Jahan; J. Zhou; M. Awrangjeb; Y. Gao","Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Feb 2020","2020","13","","367","381","Multisource remote sensing data contain complementary information on land covers, but fusing them is a challenging problem due to the heterogeneous nature of the data. This article aims to extract and integrate information from hyperspectral image (HSI) and light detection and ranging (LiDAR) data for land cover classification. As there is a scarcity of a large number of training samples for remotely sensed hyperspectral and LiDAR data, in this article, we propose a model, which is able to perform impressively using a limited number of training samples by extracting effective features representing different characteristics of objects of interest from these two complementary data sources (HSI and LiDAR). A novel feature extraction method named inverse coefficient of variation (ICV) is introduced for HSI, which considers the Gaussian probability of neighborhood between every pair of bands. We, then, propose a two-stream feature fusion approach to integrate the ICV feature with several features extracted from HSI and LiDAR data. We incorporate a fusion unit named canonical correlation analysis as a basic unit for fusing two different sets of features within each stream. We also incorporate the concept of ensemble classification where the features produced by two-stream fusion are distributed into subsets and transformed to improve the feature quality. We compare our method with the existing state-of-the-art methods, which are based on deep learning or handcrafted feature extraction or using both of them. Experimental results show that our proposed approach performs better than other existing methods with a limited number of training samples.","2151-1535","","10.1109/JSTARS.2019.2962659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961976","Canonical correlation analysis (CCA);fusion;hyperspectral;light detection and ranging (LiDAR);multisource","Feature extraction;Laser radar;Hyperspectral imaging;Correlation;Training","feature extraction;geophysical image processing;image classification;image fusion;land cover;learning (artificial intelligence);optical radar;remote sensing","inverse coefficient;variation feature;multilevel fusion technique;hyperspectral LiDAR data classification;remote sensing data;complementary information;hyperspectral image;HSI;light detection and ranging data;land cover classification;training samples;remotely sensed hyperspectral data;complementary data sources;novel feature extraction method;two-stream feature fusion approach;ICV feature;fusion unit;ensemble classification;feature quality;handcrafted feature extraction;deep learning","","14","","34","CCBY","17 Jan 2020","","","IEEE","IEEE Journals"
"Adaptive Morphological Filtering Method for Structural Fusion Restoration of Hyperspectral Images","Y. Teng; Y. Zhang; Y. Chen; C. Ti","Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","2","655","667","Recovering hyperspectral image (HSI) from mixed noise degradation is a challenging and promising theme in remote sensing, particularly when stripes and deadlines exist in several contiguous bands. This paper proposes a HSI's restoration method making use of adaptive morphological filtering (AMF) and fusing structure information of an auxiliary color image. An adaptive structuring element (ASE) indicating morphological features of each pixel is generated through information fusion, to simultaneously remove the mixed noise and preserve fine spatial structures. This key technology contains three main steps. First, edges are extracted from the auxiliary image exploiting its color information; then, an edge-constraint growing algorithm is used to generate the clustering kernel; finally, the ASE is obtained via goal-guided k-means clustering. The ASE has extensive application value, for it can be an enhancing module for most filters-based restoration methods, to mitigate the structural damage due to the fixed mask. Among these methods, Gaussian filter for preprocessing and majority voting for postprocessing are introduced in this paper as representatives. In addition, the auxiliary image can be both visible image of multisensor and false RGB component of the undamaged bands of the HSI, so it is relatively available. Experiments on simulated and real data sets show obvious effects on denoising and destriping both subjectively and objectively. The advantage of ASE on structure details preserving, compared to conventional approaches, is clearly demonstrated. The application value of the proposed restoration frame and ASE is further proved through the decision-level postprocessing experiments.","2151-1535","","10.1109/JSTARS.2015.2468593","National Natural Science Foundation of China(grant numbers:61471148,61301206); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7225126","Adaptive morphological filtering (AMF);hyperspectral image (HSI);information fusion;postprocessing restoration;preprocessing restoration;Adaptive morphological filtering (AMF);hyperspectral image (HSI);information fusion;postprocessing restoration;preprocessing restoration","Image restoration;Noise;Image edge detection;Kernel;Clustering algorithms;Image color analysis;Noise reduction","adaptive filters;edge detection;geophysical image processing;hyperspectral imaging;image colour analysis;image denoising;image fusion;image restoration;image segmentation;pattern clustering;remote sensing","structural fusion restoration;hyperspectral images;HSI restoration method;adaptive morphological filtering;auxiliary color image;adaptive structuring element;information fusion;AMF;mixed noise removal;edge extraction;edge-constraint growing algorithm;clustering kernel;ASE;goal-guided k-means clustering;Gaussian filter;majority voting;multisensor;remote sensing","","12","","33","IEEE","26 Aug 2015","","","IEEE","IEEE Journals"
"HighStitch: High Altitude Georeferenced Aerial Images Stitching for Rocking Telephoto Lens","Y. Zhao; S. Xu; S. Bu; H. Jiang; P. Han","Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Haidian, Beijing, China; Northwestern Polytechnical University, Xi’an, Shaanxi, China; Northwestern Polytechnical University, Xi’an, Shaanxi, China; Northwestern Polytechnical University, Xi’an, Shaanxi, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Nov 2021","2021","14","","11500","11507","A georeferenced orthophoto built from aerial images is the basic resource for various of remote sensing applications. As traditional low flight altitude aerial photographic survey is hard to handle large areas, higher altitude brings better survey efficiency, and a telephoto camera is required to maintain the resolution. Since existed structure from motion (SfM) methods are not suitable to process the low field-of-view images with high altitude, this article presents a novel georeferenced orthophoto stitching solution for telephoto images, which contain both translation and rotation movements. A mission planning strategy is designed to guarantee the consistent quality, according to the flight plan and camera intrinsic parameters. The potential matching topology is obtained by projecting images to the prior ground plan,e and overlap scores are estimated for neighbors query, followed with a feature-based keypoint matching. The ground plane and camera poses are optimized by joint considering feature matches, GPS and gimbal rotation information, while the final orthophoto is fused with a fast exposure leveling and weighted multiband-blending algorithm. The experiments show that our system is able to output novel quality orthophoto with high robustness, and we shared a trial version of our algorithm.1","2151-1535","","10.1109/JSTARS.2021.3124745","National Natural Science Foundation of China(grant numbers:61971418,U2003109,62171321,62071157,62162044,61771026,TC210H00L/42); Open Research Fund of Key Laboratory of Space Utilization; Chinese Academy of Sciences(grant numbers:LSU-KFJJ-2021-05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599576","Georeferenced orthophoto stitching;high altitude;matching topology;weighted multiband blending","Cameras;Three-dimensional displays;Simultaneous localization and mapping;Image reconstruction;Planning;Pipelines;Manifolds","autonomous aerial vehicles;cameras;feature extraction;image fusion;image matching;image processing;image resolution;image sensors;object detection;photogrammetry;remote sensing;robot vision","ground plane;camera poses;joint considering feature matches;final orthophoto;output novel quality orthophoto;high altitude georeferenced aerial images stitching;rocking telephoto lens;basic resource;remote sensing applications;traditional low flight altitude aerial photographic survey;higher altitude;survey efficiency;telephoto camera;motion methods;field-of-view images;novel georeferenced orthophoto stitching solution;telephoto images;rotation movements;mission planning strategy;flight plan;camera intrinsic parameters;potential matching topology;feature-based keypoint matching","","","","39","CCBY","2 Nov 2021","","","IEEE","IEEE Journals"
"From Subpixel to Superpixel: A Novel Fusion Framework for Hyperspectral Image Classification","T. Lu; S. Li; L. Fang; X. Jia; J. A. Benediktsson","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; School of Engineering and Information Technology, University of New South Wales, Canberra, Australia; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavk, Iceland","IEEE Transactions on Geoscience and Remote Sensing","20 Jul 2017","2017","55","8","4398","4411","Supervised classification of hyperspectral images (HSI) is a very challenging task due to the existence of noisy and mixed spectral characteristics. Recently, the widely developed spectral unmixing techniques offer the possibility to extract spectral mixture information at a subpixel level, which can contribute to the categorization of seriously mixed spectral pixels. Besides, it has been demonstrated that the discrimination between different materials will be improved by integrating the geometry and structure information, which can be derived from the variance between neighboring pixels. Furthermore, by incorporating the spatial context, the superpixel-based spectral-spatial similarity information can be used to smooth classification results in homogeneous regions. Therefore, a novel fusion framework for HSI classification that combines subpixel, pixel, and superpixel-based complementary information is proposed in this paper. Here, both feature fusion and decision fusion schemes are introduced. For the feature fusion scheme, the first step is to extract subpixel-level, pixel-level, and superpixel-level features from HSI, respectively. Then, the multiple feature-induced kernels are fused to form one composite kernel, which is incorporated with a support vector machine (SVM) classifier for label assignment. For the decision fusion scheme, class probabilities based on three different features are estimated by the probabilistic SVM classifier first. Then, the class probabilities are adaptively fused to form a probabilistic decision rule for classification. Experimental results tested on different real HSI images can demonstrate the effectiveness of the proposed fusion schemes in improving discrimination capability, when compared with the classification results relied on each individual feature.","1558-0644","","10.1109/TGRS.2017.2691906","National Natural Science Fund of China for Distinguished Young Scholars(grant numbers:61325007); National Natural Science Fund of China for International Cooperation and Exchanges(grant numbers:61520106001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7921801","Decision fusion;feature fusion;hyperspectral image (HSI) classification;pixel;subpixel;superpixel","Feature extraction;Hyperspectral imaging;Support vector machines;Data mining;Geometry;Probabilistic logic","feature extraction;hyperspectral imaging;image classification;image fusion;remote sensing;support vector machines","fusion framework;supervised hyperspectral images classification;spectral characteristics;spectral unmixing techniques;spectral mixture information;spectral pixels;superpixel-based spectral-spatial similarity information;superpixel-based complementary information;feature fusion schemes;decision fusion schemes;subpixel-level features;superpixel-level features;support vector machine;probabilistic SVM classifier","","59","","56","IEEE","9 May 2017","","","IEEE","IEEE Journals"
"Spatial Resolution Improvement in GNSS-Based SAR Using Multistatic Acquisitions and Feature Extraction","F. Santi; M. Bucciarelli; D. Pastina; M. Antoniou; M. Cherniakov","Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Electronic, Electrical and Computer Engineering, University of Birmingham, Birmingham, U.K.; Department of Electronic, Electrical and Computer Engineering, University of Birmingham, Birmingham, U.K.","IEEE Transactions on Geoscience and Remote Sensing","11 Aug 2016","2016","54","10","6217","6231","This paper considers the exploitation of navigation satellite systems as opportunity transmitters for bistatic and multistatic synthetic aperture radar (SAR). The simultaneous availability of multiple satellites over a scene of interest at different viewing angles allows multistatic SAR acquisitions using a single receiver on or near the ground. The resulting spatial diversity could be used to drastically improve image resolution or to enhance image information space. To exploit the availability of multiple satellites, two data fusion approaches are here considered. In the former, point features of the single images obtained from different perspectives are extracted and then combined, whereas in the latter, a multistatic image is first obtained by combining the single channel data at the image level and then the point features are extracted. This is achieved by considering ad hoc CLEAN-like techniques. These techniques have been developed on both the analytical and simulation levels and experimentally verified with real GNSS-based SAR imagery. The techniques described here are not limited to GNSS-based SAR but may be applied to any multistatic SAR system.","1558-0644","","10.1109/TGRS.2016.2583784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515203","Bistatic synthetic aperture radar (BSAR);CLEAN;feature extraction;global navigation satellite system (GNSS)-based SAR;multistatic SAR (MSAR);passive SAR","Synthetic aperture radar;Satellites;Feature extraction;Receivers;Spatial resolution;Transmitters","feature extraction;image enhancement;image fusion;image resolution;remote sensing by radar;satellite navigation;synthetic aperture radar","multistatic SAR system;GNSS-based SAR imagery;ad hoc CLEAN-like technique;image level;single channel data;multistatic image;data fusion approach;image information space;image resolution;spatial diversity;single receiver;multistatic SAR acquisition;bistatic SAR;navigation satellite system;feature extraction;multistatic acquisition;synthetic aperture radar;spatial resolution improvement","","53","","35","IEEE","18 Jul 2016","","","IEEE","IEEE Journals"
"A Variational Pan-Sharpening Method Based on Spatial Fractional-Order Geometry and Spectral–Spatial Low-Rank Priors","P. Liu; L. Xiao; T. Li","School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","27 Feb 2018","2018","56","3","1788","1802","Pan-sharpening refers to the fusion of a low-resolution (LR) multispectral (MS) image and a high-resolution (HR) panchromatic (PAN) image to obtain an HR MS image (i.e., pan-sharpened MS image). From the point of view of variational complementary data fusion, it becomes an optimization problem with geometry and spectral preserving constraints. In this paper, a novel unified optimizing pan-sharpening model is proposed by integrating a data-generative fidelity term and a compound prior term, which incorporates both spatial fractional-order geometry and spectral-spatial low-rank priors. Specifically, the proposed model consists of three important ingredients: 1) data-generative fidelity term, which models the degradation relationship between the LR and HR MS images to enforce the geometry and spectral preserving constraints; 2) fractional-order total variation-based spatial fractional-order geometry prior term, which especially exploits the spatial fractional-order gradient feature consistence between the PAN and pan-sharpened MS images to transfer the spatial structure information of the PAN image into the pan-sharpened MS image; and 3) weighted nuclear norm-based spectral-spatial low-rank prior term, which exploits the nonlocal patches-based low-rank structural sparsity simultaneously in the pan-sharpened MS image and the LR MS image for further preserving image spatial structures and spectral information. Thus, the main novelty behind the proposed model is an optimizing mechanism by fully taking advantage of the spatial details and texture expressive power of the spatial fractional-order geometry prior as well as the spectral-spatial correlation preserving capacity of the low-rank prior. Finally, the proposed model can be implemented in an alternating direction method of multipliers framework, and thus, an efficient algorithm is presented. To verify the validity, the new proposed method is systematically compared with some state-of-the-art techniques using the Pleiades, GeoEye-1, QuickBird, and WorldView2 satellite data sets in the subjective, objective, and efficiency aspects. The results show that the proposed method performs better than the compared methods in terms of higher spatial and spectral qualities.","1558-0644","","10.1109/TGRS.2017.2768386","Natural Science Foundation of Jiangsu Province(grant numbers:BK20170905,BK20161500); National Natural Science Foundation of China(grant numbers:61571230); National Major Research Plan of China(grant numbers:2016YFF0103604); NUPTSF(grant numbers:NY217137); Open Research Fund in 2017 of Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense(grant numbers:3091601410408); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8167324","Pan-sharpening;spatial fractional-order geometry prior;spectral–spatial low-rank prior;weighted nuclear norm (WNN)","Geometry;Spatial resolution;Correlation;Distortion;Multiresolution analysis;Degradation;Imaging","geophysical image processing;image fusion;image resolution;image texture;optimisation;remote sensing;variational techniques","variational pan-sharpening method;low-resolution multispectral image;high-resolution panchromatic image;HR MS image;pan-sharpened MS image;variational complementary data fusion;spectral preserving constraints;optimizing pan-sharpening model;data-generative fidelity term;spatial fractional-order geometry;fractional-order total variation;fractional-order gradient feature consistence;spatial structure information;PAN image;low-rank prior term;low-rank structural sparsity;LR MS image;image spatial structures;spectral-spatial correlation preserving capacity;spectral qualities;spatial qualities;spectral-spatial low-rank priors","","52","","46","IEEE","7 Dec 2017","","","IEEE","IEEE Journals"
"Unsupervised Pansharpening Based on Self-Attention Mechanism","Y. Qu; R. K. Baghbaderani; H. Qi; C. Kwan","Department of Electrical Engineering and Computer Science, Advanced Imaging and Collaborative Information Processing Group, University of Tennessee, Knoxville, TN, USA; Department of Electrical Engineering and Computer Science, Advanced Imaging and Collaborative Information Processing Group, University of Tennessee, Knoxville, TN, USA; Department of Electrical Engineering and Computer Science, Advanced Imaging and Collaborative Information Processing Group, University of Tennessee, Knoxville, TN, USA; Applied Research LLC, Rockville, MD, USA","IEEE Transactions on Geoscience and Remote Sensing","24 Mar 2021","2021","59","4","3192","3208","Pansharpening is to fuse a multispectral image (MSI) of low-spatial-resolution (LR) but rich spectral characteristics with a panchromatic image (PAN) of high spatial resolution (HR) but poor spectral characteristics. Traditional methods usually inject the extracted high-frequency details from PAN into the upsampled MSI. Recent deep learning endeavors are mostly supervised assuming that the HR MSI is available, which is unrealistic especially for satellite images. Nonetheless, these methods could not fully exploit the rich spectral characteristics in the MSI. Due to the wide existence of mixed pixels in satellite images where each pixel tends to cover more than one constituent material, pansharpening at the subpixel level becomes essential. In this article, we propose an unsupervised pansharpening (UP) method in a deep-learning framework to address the abovementioned challenges based on the self-attention mechanism (SAM), referred to as UP-SAM. The contribution of this article is threefold. First, the SAM is proposed where the spatial varying detail extraction and injection functions are estimated according to the attention representations indicating spectral characteristics of the MSI with subpixel accuracy. Second, such attention representations are derived from mixed pixels with the proposed stacked attention network powered with a stick-breaking structure to meet the physical constraints of mixed pixel formulations. Third, the detail extraction and injection functions are spatial varying based on the attention representations, which largely improves the reconstruction accuracy. Extensive experimental results demonstrate that the proposed approach is able to reconstruct sharper MSI of different types, with more details and less spectral distortion compared with the state-of-the-art.","1558-0644","","10.1109/TGRS.2020.3009207","NASA(grant numbers:NNX12CB05C,NNX16CP38P); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146721","Attention mechanism;multispectral image (MSI);pansharpening;unsupervised deep learning","Spatial resolution;Image reconstruction;Sensors;Satellites;Image segmentation;Machine learning","deep learning (artificial intelligence);geophysical image processing;geophysical techniques;image fusion;image reconstruction;image resolution;remote sensing;unsupervised learning","low-spatial-resolution;MSI reconstruction;multispectral image;spectral distortion;mixed pixel formulations;attention network;attention representations;injection functions;SAM;self-attention mechanism;unsupervised pansharpening;subpixel level;satellite images;HR MSI;deep learning;upsampled MSI;high-frequency details;spectral characteristics;high spatial resolution;PAN;panchromatic image","","37","","52","IEEE","23 Jul 2020","","","IEEE","IEEE Journals"
"Hyperspectral and Lidar Intensity Data Fusion: A Framework for the Rigorous Correction of Illumination, Anisotropic Effects, and Cross Calibration","M. Brell; K. Segl; L. Guanter; B. Bookhagen","Helmholtz Centre Potsdam, GFZ German Research Centre for Geosciences, Potsdam, Germany; Helmholtz Centre Potsdam, GFZ German Research Centre for Geosciences, Potsdam, Germany; Helmholtz Centre Potsdam, GFZ German Research Centre for Geosciences, Potsdam, Germany; Institute of Earth and Environmental Science, University of Potsdam, Potsdam, Germany","IEEE Transactions on Geoscience and Remote Sensing","17 Mar 2017","2017","55","5","2799","2810","The fusion of hyperspectral imaging (HSI) sensor and airborne lidar scanner (ALS) data provides promising potential for applications in environmental sciences. Standard fusion approaches use reflectance information from the HSI and distance measurements from the ALS to increase data dimensionality and geometric accuracy. However, the potential for data fusion based on the respective intensity information of the complementary active and passive sensor systems is high and not yet fully exploited. Here, an approach for the rigorous illumination correction of HSI data, based on the radiometric cross-calibrated return intensity information of ALS data, is presented. The cross calibration utilizes a ray tracing-based fusion of both sensor measurements by intersecting their particular beam shapes. The developed method is capable of compensating for the drawbacks of passive HSI systems, such as cast and cloud shadowing effects, illumination changes over time, across track illumination, and partly anisotropy effects. During processing, spatial and temporal differences in illumination patterns are detected and corrected over the entire HSI wavelength domain. The improvement in the classification accuracy of urban and vegetation surfaces demonstrates the benefit and potential of the proposed HSI illumination correction. The presented approach is the first step toward the rigorous in-flight fusion of passive and active system characteristics, enabling new capabilities for a variety of applications.","1558-0644","","10.1109/TGRS.2017.2654516","The Helmholtz Centre Potsdam, GFZ German Research Centre for Geoscience; “Zentrales Innovationsprogramm Mittelstand” program founded by the Federal Ministry for Economic Affairs and Energy Germany; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852477","Airborne laser scanning (ALS);deshadowing;imaging spectroscopy;in-flight;mosaicking;pixel-level fusion;preprocessing;radiometric alignment;ray tracing;sensor alignment;sensor fusion","Atmospheric measurements;Radiometry;Calibration;Lighting;Laser radar;Measurement by laser beam;Surface emitting lasers","geophysical image processing;hyperspectral imaging;image classification;image fusion;land use;optical radar;remote sensing by laser beam;vegetation","hyperspectral data;lidar data;data fusion;hyperspectral imaging sensor;airborne lidar scanner;data dimensionality;geometric accuracy;active sensor system;passive sensor system;sensor measurements;passive hyperspectral imaging system;cloud shadowing effect;illumination change;anisotropy effect;urban classification accuracy;vegetation surface classification accuracy;illumination correction","","36","","33","IEEE","13 Feb 2017","","","IEEE","IEEE Journals"
"Pansharpening With Multiscale Geometric Support Tensor Machine","Y. Xing; M. Wang; S. Yang; K. Zhang","Xidian University, Xi’an, China; Xidian University, Xi’an, China; Xidian University, Xi’an, China; Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","20 Apr 2018","2018","56","5","2503","2517","In this paper, a new pansharpening method is proposed by constructing a set of multiscale geometric support tensor filters (MGSTFs). First, a least-square ridgelet support tensor machine is developed to derive a series of MGSTFs. Then the source images are formulated as tensors and filtered by MGSTFs to capture geometric and salient features of images. These features are then fused at each scale and direction to obtain the fused products. The distortions can be reduced by exploring the tensor formulation of multispectral data and endowing the filters’ directionality to capture the geometric details of images. Some experiments are carried out on several groups of QuickBird and GeoEye-1 images, and the results show that our proposed method can simultaneously reduce spectral distortions and preserve spatial details in the fused image.","1558-0644","","10.1109/TGRS.2017.2742002","National Science Foundation of China(grant numbers:61771380,U1730109,91438103,61771376,61703328,91438201,U1701267,61703328); Equipment Pre-research Project of the 13th Five-Years Plan(grant numbers:6140137050206,414120101026,6140312010103,6141A020223,6141B06160301,6141B07090102); Major Research Plan in Shaanxi Province of China(grant numbers:2017ZDXM-GY-103,017ZDCXL-GY-03-02); Foundation of State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System(grant numbers:2017K0202B,2018K0101B); Science Basis Research Program in Shaanxi Province of China(grant numbers:16JK1823,2017JM6086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8295005","Geometric support tensor filters;least-square ridgelet support tensor machine (LS-RSTM);pansharpening","Tensile stress;Distortion;Spatial resolution;Image color analysis;Transforms;Support vector machines","geophysical image processing;image fusion;image resolution;least squares approximations;remote sensing","QuickBird and GeoEye-1 images;spectral distortions;multiscale geometric support tensor machine;fused image;tensor formulation;fused products;geometric features;source images;least-square ridgelet support tensor machine;MGSTFs;multiscale geometric support tensor filters;pansharpening method","","18","","44","IEEE","19 Feb 2018","","","IEEE","IEEE Journals"
"Fusion of Heterogeneous Earth Observation Data for the Classification of Local Climate Zones","G. Zhang; P. Ghamisi; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Helmholtz-Zentrum Dresden-Rossendorf (HZDR), Helmholtz Institute Freiberg for Resource Technology (HIF), Freiberg, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","25 Sep 2019","2019","57","10","7623","7642","This paper proposes a novel framework for fusing multi-temporal, multispectral satellite images and OpenStreetMap (OSM) data for the classification of local climate zones (LCZs). Feature stacking is the most commonly used method of data fusion but does not consider the heterogeneity of multimodal optical images and OSM data, which becomes its main drawback. The proposed framework processes two data sources separately and then combines them at the model level through two fusion models (the landuse fusion model and building fusion model) that aim to fuse optical images with landuse and buildings layers of OSM data, respectively. In addition, a new approach to detecting building incompleteness of OSM data is proposed. The proposed framework was trained and tested using the data from the 2017 IEEE GRSS Data Fusion Contest and further validated on one additional test (AT) set containing test samples that are manually labeled in Munich and New York. The experimental results have indicated that compared with the feature stacking-based baseline framework, the proposed framework is effective in fusing optical images with OSM data for the classification of LCZs with high generalization capability on a large scale. The classification accuracy of the proposed framework outperforms the baseline framework by more than 6% and 2% while testing on the test set of 2017 IEEE GRSS Data Fusion Contest and the AT set, respectively. In addition, the proposed framework is less sensitive to spectral diversities of optical satellite images and thus achieves more stable classification performance than the state-of-the-art frameworks.","1558-0644","","10.1109/TGRS.2019.2914967","H2020 European Research Council(grant numbers:ERC-2016-StG-714087); Helmholtz-Gemeinschaft(grant numbers:VH-NG-1018); Bayerische Akademie der Wissenschaften; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765334","Canonical correlation forest (CCF);heterogeneous data fusion;local climate zones (LCZs);OpenStreetMap (OSM);satellite images","Satellites;Urban areas;Data integration;Meteorology;Artificial neural networks;Training;Radio frequency","geophysical image processing;geophysical signal processing;geophysical techniques;image classification;image fusion;pattern classification;remote sensing;sensor fusion;terrain mapping","data sources;fusion models;landuse fusion model;building fusion model;OSM data;2017 IEEE GRSS Data Fusion Contest;feature stacking-based baseline framework;optical satellite images;heterogeneous earth observation Data;local climate zones;multispectral satellite images;multimodal optical images","","18","","44","OAPA","17 Jul 2019","","","IEEE","IEEE Journals"
"Pan-Sharpening via Multiscale Dynamic Convolutional Neural Network","J. Hu; P. Hu; X. Kang; H. Zhang; S. Fan","School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Robot, Hunan University, Changsha, China; Key Laboratory of Electric Power Robot of Hunan Province, Changsha University of Science and Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","24 Feb 2021","2021","59","3","2231","2244","Pan-sharpening is an effective method to obtain high-resolution multispectral images by fusing panchromatic (PAN) images with fine spatial structure and low-resolution multispectral images with rich spectral information. In this article, a multiscale pan-sharpening method based on dynamic convolutional neural network is proposed. The filters in dynamic convolution are generated dynamically and locally by the filter generation network which is different from the standard convolution and strengthens the adaptivity of the network. The dynamic filters are adaptively changed according to the input images. The proposed multiscale dynamic convolutions extract detail feature of PAN image at different scales. Multiscale network structure is beneficial to obtain effective detail features. The weights obtained by the weight generation network are used to adjust the relationship among the detail features in each scale. The GeoEye-1, QuickBird, and WorldView-3 data are used to evaluate the performance of the proposed method. Compared with the widely used state-of-the-art pan-sharpening approaches, the experimental results demonstrate the superiority of the proposed method in terms of both objective quality indexes and visual performance.","1558-0644","","10.1109/TGRS.2020.3007884","National Natural Science Foundation of China(grant numbers:61601061,61971071); Scientific Research Fund of Hunan Provincial Education Department(grant numbers:14B006); Open Research Fund of Key Laboratory of Electric Power Robot of Hunan Province(grant numbers:PROF1902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142456","Convolutional neural network;multiscale dynamic convolution;pan-sharpening;weight generation network","Convolution;Feature extraction;Spatial resolution;Standards;Image reconstruction;Convolutional neural networks","geophysical image processing;image fusion;image resolution;neural nets;remote sensing","fusing panchromatic images;fine spatial structure;low-resolution multispectral images;rich spectral information;multiscale pan-sharpening method;dynamic convolution;filter generation network;standard convolution;dynamic filters;input images;multiscale dynamic convolutions;PAN image;multiscale network structure;effective detail features;weight generation network;widely used state-of-the-art pan-sharpening approaches;multiscale dynamic convolutional neural network;high-resolution multispectral images","","16","","45","IEEE","16 Jul 2020","","","IEEE","IEEE Journals"
"Model Inspired Autoencoder for Unsupervised Hyperspectral Image Super-Resolution","J. Liu; Z. Wu; L. Xiao; X. -J. Wu","Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China","IEEE Transactions on Geoscience and Remote Sensing","15 Mar 2022","2022","60","","1","12","This article focuses on hyperspectral image (HSI) super-resolution that aims to fuse a low-spatial-resolution HSI and a high-spatial-resolution multispectral image to form a high-spatial-resolution HSI (HR-HSI). Existing deep learning-based approaches are mostly supervised that rely on a large number of labeled training samples, which is unrealistic. The commonly used model-based approaches are unsupervised and flexible but rely on handcrafted priors. Inspired by the specific properties of model, we make the first attempt to design a model-inspired deep network for HSI super-resolution in an unsupervised manner. This approach consists of an implicit autoencoder network built on the target HR-HSI that treats each pixel as an individual sample. The nonnegative matrix factorization (NMF) of the target HR-HSI is integrated into the autoencoder network, where the two NMF parts, spectral and spatial matrices, are treated as decoder parameters and hidden outputs, respectively. In the encoding stage, we present a pixelwise fusion model to estimate hidden outputs directly and then reformulate and unfold the model’s algorithm to form the encoder network. With the specific architecture, the proposed network is similar to a manifold prior-based model and can be trained patch by patch rather than the entire images. Moreover, we propose an additional unsupervised network to estimate the point spread function and spectral response function. Experimental results conducted on both synthetic and real datasets demonstrate the effectiveness of the proposed approach.","1558-0644","","10.1109/TGRS.2022.3143156","National Natural Science Foundation of China(grant numbers:62071204,61871226,61772274); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201338,BK20180018); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); China Postdoctoral Science Foundation(grant numbers:2021M691275); Jiangsu Postdoctoral Research Funding Program(grant numbers:2021K148B); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681709","Autoencoder;hyperspectral image (HSI);nonnegative matrix factorization (NMF);super-resolution;unfolding","Superresolution;Spatial resolution;Hyperspectral imaging;Fuses;Decoding;Energy resolution;Tensors","geophysical image processing;hyperspectral imaging;image classification;image coding;image fusion;image reconstruction;image representation;image resolution;matrix algebra;matrix decomposition;neural nets;remote sensing;unsupervised learning","pixelwise fusion model;hidden outputs;encoder network;manifold prior-based model;model inspired autoencoder;unsupervised hyperspectral image super-resolution;low-spatial-resolution HSI;high-spatial-resolution multispectral image;high-spatial-resolution HSI;deep learning-based approaches;labeled training samples;model-based approaches;model-inspired deep network;HSI super-resolution;unsupervised manner;implicit autoencoder network;target HR-HSI;spectral matrices;spatial matrices;nonnegative matrix factorization;NMF;unsupervised network;point spread function;synthetic dataset;real dataset","","12","","70","IEEE","14 Jan 2022","","","IEEE","IEEE Journals"
"Two Stages Pan-Sharpening Details Injection Approach Based on Very Deep Residual Networks","T. Benzenati; A. Kallel; Y. Kessentini","Advanced Technologies for Image and Signal Processing, University of Sfax, Sfax, Tunisia; Advanced Technologies for Image and Signal Processing, University of Sfax, Sfax, Tunisia; MIRACL Laboratory, University of Sfax, Sfax, Tunisia","IEEE Transactions on Geoscience and Remote Sensing","21 May 2021","2021","59","6","4984","4992","Pan-sharpening is a fusion task, which aims to combine a low spatial resolution multispectral (MS) image with a high spatial resolution single band panchromatic (PAN) image to produce a high spatial and spectral Pan-sharpened image. The success of a Pan-sharpening technique depends on its ability to boost the spatial quality of the MS image while preserving its spectral feature. To this end, we propose in this article a new two-stage detail injection approach allowing to reconstruct fine structures based on convolutional neural networks (CNNs). First, generalized Laplacian pyramid gain injections CNN is performed to estimate the optimal values of the injection gains for each MS band to inject spatial details extracted from the PAN image. Next, the result is enhanced by injecting the details missing using the power of deep residual learning. The quantitative and qualitative results on data sets from different satellites show that the proposed approach can achieve higher performances in both spatial and spectral qualities compared to the state of the art as well as the new CNN-based methods.","1558-0644","","10.1109/TGRS.2020.3019835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9198089","Convolutional neural networks (CNNs);deep learning;details injection;image processing;pan-sharpening;residual learning","Spatial resolution;Computer architecture;Signal resolution;Estimation;Task analysis","geophysical image processing;geophysical signal processing;high-pass filters;image fusion;image resolution;neural nets;remote sensing","spatial details;PAN image;deep residual learning;spatial qualities;spectral qualities;CNN-based methods;stages Pan-sharpening details;deep residual networks;fusion task;low spatial resolution multispectral image;high spatial resolution single band panchromatic image;high spatial Pan-sharpened image;spectral Pan-sharpened image;Pan-sharpening technique;spatial quality;MS image;spectral feature;two-stage detail injection approach;convolutional neural networks;generalized Laplacian pyramid gain injections CNN;injection gains;MS band","","12","","45","IEEE","15 Sep 2020","","","IEEE","IEEE Journals"
"High Spatio-Temporal Resolution Deformation Time Series With the Fusion of InSAR and GNSS Data Using Spatio-Temporal Random Effect Model","N. Liu; W. Dai; R. Santerre; J. Hu; Q. Shi; C. Yang","School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; Center for Research in Geomatics, Laval University, Quebec, Canada; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2018","2019","57","1","364","380","High spatio-temporal resolution deformation series can be used to improve the understanding of deformation mechanism, thereby contributing to prevention and control of geological disasters such as mine subsidence, landslide, and earthquake. Among ground deformation monitoring technologies, global navigation satellite system has high temporal resolution but low spatial resolution, and interferometric synthetic aperture radar (InSAR) has high spatial resolution but low temporal resolution. Fusing these two data may generate high spatio-temporal resolution deformation series. Existing fusion methods usually use the bi-direction interpolation, which does not consider the spatio-temporal cross correlation and is computationally extensive. We propose a dynamic filtering fusion model based on the spatio-temporal random effect (a spatio-temporal Kalman filter) model. Experiments with simulated data and real data from the Los Angeles area are conducted to validate this method. Simulated experimental results are compared with truth data and the Los Angeles experiment data results are verified using the leave-one InSAR image-out validation method. The RMS results for them are around 13.8 and 5 mm, respectively, indicating that the proposed method can achieve high accuracy and high spatial-temporal resolution deformation time series.","1558-0644","","10.1109/TGRS.2018.2854736","National Natural Science Foundation of China(grant numbers:41674011); State Key Development Program of Basic Research of China(grant numbers:2013CB733303); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8439087","Deformation;fusion;global navigation satellite system (GNSS);high spatio-temporal resolution;interferometric synthetic aperture radar (InSAR)","Global navigation satellite system;Strain;Spatial resolution;Computational modeling;Mathematical model;Data models;Geologic measurements","disasters;geophysical techniques;image filtering;image fusion;image resolution;interpolation;Kalman filters;radar imaging;radar interferometry;radar resolution;remote sensing by radar;satellite navigation;synthetic aperture radar","temporal resolution;spatial resolution;InSAR data fusion;GNSS data fusion;geological disasters;mine subsidence;landslide;earthquake;global navigation satellite system;leave-one InSAR image-out validation method;Los Angeles;dynamic filtering fusion model;bidirection interpolation;interferometric synthetic aperture radar;spatio-temporal Kalman filter;spatio-temporal cross correlation;ground deformation monitoring technologies;spatio-temporal random effect model;high spatio-temporal resolution deformation time series","","11","","32","IEEE","17 Aug 2018","","","IEEE","IEEE Journals"
"Multistage Dual-Attention Guided Fusion Network for Hyperspectral Pansharpening","P. Guan; E. Y. Lam","Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, SAR, China; Department of Electrical and Electronic Engineering, The University of Hong Kong, Hong Kong, SAR, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","14","Deep learning, especially the convolutional neural network, has been widely applied to solve the hyperspectral pansharpening problem. However, most do not explore the intraimage characteristics and the interimage correlation concurrently due to the limited representation ability of the networks, which may lead to insufficient fusion of valuable information encoded in the high-resolution panchromatic images (HR-PANs) and low-resolution hyperspectral images (LR-HSIs). To cope with this problem, we develop a hyperspectral pansharpening method called multistage dual-attention guided fusion network (MDA-Net) to fully extract the important information and accurately fuse them. It employs a three-stream structure, which enables the network to incorporate the intrinsic characteristics of each input and correlation among them simultaneously. In order to combine as much information as possible, we merge the features extracted from three streams in multiple stages, where a dual-attention guided fusion block (DAFB) with spectral and spatial attention mechanisms is utilized to fuse the features efficiently. It identifies the useful components in both spatial and spectral domains, which are beneficial to improving the fusion accuracy. Moreover, we design a multiscale residual dense block (MRDB) to extract dense and hierarchical features, which improves the representation power of the network. Experiments are conducted on both real and simulated datasets. The evaluation results validate the superiority of the MDA-Net.","1558-0644","","10.1109/TGRS.2021.3114552","Research Grants Council of Hong Kong (GRF)(grant numbers:17201818,17200019,17201620); The University of Hong Kong(grant numbers:104005864); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559899","Convolutional neural network (CNN);dual-attention guided fusion;hyperspectral pansharpening;multistage fusion","Feature extraction;Data mining;Pansharpening;Hyperspectral imaging;Fuses;Streaming media;Correlation","feature extraction;geophysical image processing;geophysical signal processing;image classification;image coding;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing;sensor fusion","dual-attention guided fusion network;MDA-Net;three-stream structure;intrinsic characteristics;dual-attention guided fusion block;spectral attention mechanisms;spatial attention mechanisms;spatial domains;spectral domains;fusion accuracy;deep learning;convolutional neural network;hyperspectral pansharpening problem;intraimage characteristics;interimage correlation;representation ability;insufficient fusion;high-resolution panchromatic images;low-resolution hyperspectral images;LR-HSIs;hyperspectral pansharpening method","","10","","47","IEEE","5 Oct 2021","","","IEEE","IEEE Journals"
"Multiresolution Analysis Based on Dual-Scale Regression for Pansharpening","P. Wang; H. Yao; C. Li; G. Zhang; H. Leung","Hubei Key Laboratory of Regional Development and Environment Response, Hubei University, Hubei, China; Key Laboratory of Radar Imaging and Microwave Photonics, Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China; State Key Laboratory of Tropical Oceanography, South China Sea Institute of Oceanology, Chinese Academy of Sciences, Guangzhou, China; Key Laboratory of Radar Imaging and Microwave Photonics, Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Electrical and Computer Engineering, University of Calgary, Calgary, AB, Canada","IEEE Transactions on Geoscience and Remote Sensing","21 Feb 2022","2022","60","","1","19","Pansharpening technique is used to merge the original multispectral image (MS) with a high spatial resolution panchromatic image (PAN). Due to its robustness, the multiresolution analysis (MRA) is an important part of pansharpening. The scale regression model is effective for improving MRA. However, the existing MRA based on scale regression results into single-scale regression information, thus affecting the final pansharpening result. To address this problem, in this work, we propose a dual-scale regression-based MRA for pansharpening. First, we establish a scale regression-based model. Then, this model is improved using a high-pass modulation (HPM) injection scheme. Finally, the dual-scale information is added to the scale regression to construct the dual-scale regression for obtaining the final pansharpening result. We perform experiments using five datasets. The results show that the proposed method obtains a better pansharpening result as compared to various state-of-the-art MRA methods. In addition, the quantitative and qualitative analysis of the results shows that the proposed method achieves appropriate spatial and spectral resolution fusion. Therefore, it has a great potential in pansharpening technique.","1558-0644","","10.1109/TGRS.2021.3131477","National Natural Science Foundation of China(grant numbers:61801211); Fundamental Research Funds for the Central Universities in the Nanjing University of Aeronautics and Astronautics(grant numbers:NZ2020009); Open Project Program of Hubei Key Laboratory of Regional Development and Environment Response Fundamental(grant numbers:2020(B)004); Open Project Program of State Key Laboratory of Tropical Oceanography, South China Sea Institute of Oceanology, Chinese Academy of Sciences(grant numbers:LTO2118); Foundation of Graduate Innovation Center in the Nanjing University of Aeronautics and Astronautics(grant numbers:xcxjh20210405); Graduate Education and Teaching Reform Research Project in the Nanjing University of Aeronautics and Astronautics(grant numbers:2021YJXGG11); Open Research Project of Hubei Key Laboratory of Intelligent Geo-Information Processing(grant numbers:KLIGIP-2019A05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628091","Dual scale regression;multiresolution analysis (MRA);multispectral image (MS);panchromatic image (PAN);pansharpening","Pansharpening;Spatial resolution;Image resolution;Reactive power;Modulation;Matched filters;Laboratories","geophysical image processing;image fusion;image resolution;regression analysis;remote sensing;wavelet transforms","high-pass modulation injection scheme;dual-scale information;final pansharpening result;MRA methods;pansharpening technique;multiresolution analysis;original multispectral image;high spatial resolution panchromatic image;scale regression model;scale regression results;single-scale regression information;dual-scale regression-based MRA;scale regression-based model","","8","","56","CCBY","30 Nov 2021","","","IEEE","IEEE Journals"
"A Multilevel Encoder–Decoder Attention Network for Change Detection in Hyperspectral Images","J. Qu; S. Hou; W. Dong; Y. Li; W. Xie","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","17 Feb 2022","2022","60","","1","13","Convolutional neural networks (CNNs) have attracted much attention in change detection (CD) for their superior feature learning ability. However, most of the existing CNN-based CD methods adopt an early- or late-fusion strategy to fuse low-level spatial details or high-level semantic information. So far, the impact of multilevel fusion strategy across multitemporal hyperspectral (HS) images, and its application to CD, remains unexplored. In this article, we propose a multilevel encoder–decoder attention network (ML-EDAN), which allows the network to make full use of the hierarchical features for CD in HS images. A two-stream encoder–decoder framework is taken as the backbone to exploit and fuse the hierarchical features from all the convolutional layers of multitemporal HS images. Within the encoder–decoder, a contextual-information-guided attention module is developed to yield more effective spatial–spectral feature transfer in the network. After fully obtaining the multilevel hierarchical features, the long short-term memory (LSTM) subnetwork is devised to analyze temporal dependence between multitemporal images. Moreover, the proposed ML-EDAN is trained in an end-to-end manner with a new joint loss function considering both reconstruction error and pixelwise classification error. The experiments are conducted on three datasets, demonstrating the effectiveness of the proposed ML-EDAN in HS CD in comparison with widely accepted state-of-the-art methods.","1558-0644","","10.1109/TGRS.2021.3130122","National Defense Pre-Research Foundation; Higher Education Discipline Innovation Project(grant numbers:B08038); National Natural Science Foundation of China(grant numbers:62101414); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2021JQ-194,2021JQ-197); Fundamental Research Funds for the Central Universities(grant numbers:XJS210108,XJS210104); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110856); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9624977","Attention mechanism;change detection (CD);deep learning (DL);hyperspectral image (HSI);multilevel features","Feature extraction;Decoding;Convolutional neural networks;Task analysis;Fuses;Semantics;Deep learning","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence);neural nets;object detection;remote sensing","multitemporal HS images;contextual-information-guided attention module;effective spatial-spectral feature transfer;multilevel hierarchical features;multitemporal images;ML-EDAN;HS CD;multilevel encoder-decoder attention network;change detection;convolutional neural networks;superior feature learning ability;early- fusion strategy;late-fusion strategy;low-level spatial details;high-level semantic information;multilevel fusion strategy;multitemporal hyperspectral images;two-stream encoder-decoder framework;CNN-based CD methods","","6","","41","IEEE","23 Nov 2021","","","IEEE","IEEE Journals"
"A Nonconvex Pansharpening Model With Spatial and Spectral Gradient Difference-Induced Nonconvex Sparsity Priors","P. Liu; L. Xiao","Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","29 Dec 2021","2022","60","","1","15","This article proposed a nonconvex variational model for pansharpening with spatial and spectral gradient difference-induced nonconvex sparsity priors (PSSGDNSP), which can fuse the panchromatic (Pan) and low-resolution (LR) multispectral (MS) images to generate the high-resolution (HR) MS image. More particularly, the proposed PSSGDNSP model exploits the spatial gradient difference-induced nonconvex  $l_{1/2}$  sparsity prior between HR MS and Pan, and the spectral gradient difference-induced nonconvex  $l_{1/2}$  sparsity prior between HR and LR MS. Consequently, our proposed PSSGDNSP model well preserves both the spatial and spectral information. In fact, our proposed band-coupled model treats the MS image like a third-order tensor so that the intrinsic band correlation of the MS image can be fully kept. Moreover, we solve our proposed PSSGDNSP model by applying the alternating direction method of multipliers (ADMM) method. Finally, the experiments fully validate the superiority and performance of our proposed PSSGDNSP method.","1558-0644","","10.1109/TGRS.2021.3078334","National Natural Science Foundation of China(grant numbers:61802202,61871226,61571230); Jiangsu Provincial Key Developing Project(grant numbers:BE2018727); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9434419","Nonconvex sparsity prior;nonconvex variational model;pansharpening;spatial and spectral gradient differences","Pansharpening;Correlation;Computational modeling;TV;Data models;Adaptation models;Tensors","concave programming;gradient methods;image fusion;image resolution;image restoration;iterative methods;minimisation;remote sensing;spectral analysis;tensors;variational techniques","nonconvex pansharpening model;spectral gradient difference-induced nonconvex sparsity priors;nonconvex variational model;high-resolution MS image;PSSGDNSP model;spatial information;spectral information;spatial gradient difference-induced nonconvex sparsity priors;low-resolution multispectral images;third-order tensor;intrinsic band correlation;alternating direction method of multipliers;ADMM method","","5","","48","IEEE","18 May 2021","","","IEEE","IEEE Journals"
"Multiobjective Guided Divide-and-Conquer Network for Hyperspectral Pansharpening","X. Wu; J. Feng; R. Shang; X. Zhang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","12 Apr 2022","2022","60","","1","17","Deep learning methods have gained rapid development in hyperspectral pansharpening (HP) due to powerful spatial–spectral feature extraction ability. However, most of these methods are optimized using a single reconstruction objective. It is difficult for these methods to find a balance between spectral preservation and spatial preservation. Furthermore, these methods adopt interpolation or convolution to upsample the hyperspectral images (HSIs), which tends to cause noticeable spectral distortion. To conquer these issues, a novel multiobjective guided divide-and-conquer network (MO-DCN) is proposed for HP. It consists of a deconvolution long short-term memories (LSTMs) network (DLSTM) and a divide-and-conquer network (DCN). DLSTM leverages bi-direction learning to upsample HSIs by considering 3-D spatiotemporal dependencies. Then, DCN designs a two-branch architecture to reconstruct spatial and spectral information from upsampled HSIs and panchromatic images (PANIs), respectively, where the spatial branch designs an attention-in-attention module (AIAM) to emphasize complementary attention in a coarse-to-fine way. Finally, co-improvement of spatial and spectral information is formulated as an Epsilon-constraint-based multiobjective optimization. The Epsilon constraint method transforms one objective into a constraint and regards it as a penalty bound to make an excellent tradeoff between different objectives. Experimental results demonstrated that the proposed method markedly improves pansharpening performance in both the spatial and spectral domains and has superior fusion performance than state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3159999","National Natural Science Foundation of China(grant numbers:61871306,61836009,62172600,62077038); Innovation Capability Support Program of Shaanxi(grant numbers:2021KJXX-08); Natural Science Basic Research Program of Shaanxi(grant numbers:2022JC-45); Key Research and Development Program of Shaanxi(grant numbers:2022GY-065); Association for Science and Technology Youth Talent Support Program Project(grant numbers:095920201301); Fundamental Research Funds for the Central Universities(grant numbers:JB211901); Aeronautical Science Fund of China(grant numbers:2019ZC081002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9736970","Deep learning;hyperspectral image (HSI);hyperspectral pansharpening (HP);multiobjective optimization;spatial–spectral fusion","Optimization;Pansharpening;Spatial resolution;Image reconstruction;Hyperspectral imaging;Bayes methods;Feature extraction","feature extraction;filtering theory;geophysical image processing;image colour analysis;image fusion;image reconstruction;image resolution;interpolation;learning (artificial intelligence);optimisation;remote sensing;spectral analysis","attention-in-attention module;spatial information;spectral information;Epsilon-constraint-based multiobjective optimization;Epsilon constraint method;spatial domains;spectral domains;hyperspectral pansharpening;HP;powerful spatial-spectral feature extraction ability;single reconstruction objective;spectral preservation;spatial preservation;hyperspectral images;noticeable spectral distortion;divide- conquer network;-conquer network;MO-DCN;short-term memories network;DLSTM leverages bi-direction learning;upsample HSIs;upsampled HSIs;spatial branch designs","","4","","37","IEEE","16 Mar 2022","","","IEEE","IEEE Journals"
"Multitask Deep Learning Framework for Spatiotemporal Fusion of NDVI","D. Jia; C. Cheng; S. Shen; L. Ning","Faculty of Geographical Science, Beijing Normal University, Beijing, China; National Tibetan Plateau Data Center, Beijing, China; Center for Geodata and Analysis, Beijing Normal University, Beijing, China; Faculty of Geographical Science, Beijing Normal University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","8 Mar 2022","2022","60","","1","13","High spatial and temporal resolution normalized difference vegetation index (NDVI) time series are indispensable for monitoring land surfaces dynamics in spatiotemporally heterogeneous areas. Spatiotemporal fusion (STF) is one of the most common methods used for producing such data. These methods require the use of one or two pairs of fine images (with fine spatial but rough temporal resolution, such as Landsat images) and coarse images [with fine temporal but rough spatial resolution, such as Moderate Resolution Imaging Spectroradiometer (MODIS)]. A coarse image at the prediction date is also required to predict the corresponding missing fine image in the time series. Recently, the proposed deep learning (DL)-based STF methods have achieved promising fusion performance but are challenged in areas with frequent cloud contamination and landcover change prediction, while they also suffer from unstable fusion performance. Moreover, current STF methods lack a quality assessment process for the fusion results. To address these limitations, in this article, we propose a multitask DL framework for STF of NDVI time series, which integrates two types of DL-based STF methods. Four experiments in two study sites were conducted to test the effectiveness of the proposed method, and the results indicate that it achieves accurate and stable fusion capable of predicting landcover changes even when image pairs obtained at long intervals are used. In addition, a fusion uncertainty estimation method is proposed, which has the potential to be used as a quality assessment metric.","1558-0644","","10.1109/TGRS.2021.3140144","Tibetan Plateau Scientific Expedition and Research Program (STEP)(grant numbers:2019QZKK0608); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667458","Frequent cloud contamination;landcover change;multitask learning;normalized difference vegetation index (NDVI);spatiotemporal fusion (STF)","Spatial resolution;Feature extraction;Time series analysis;Image resolution;Spatiotemporal phenomena;Earth;Contamination","geophysical image processing;geophysical techniques;image fusion;image resolution;learning (artificial intelligence);remote sensing;terrain mapping;time series;vegetation;vegetation mapping","spatiotemporal fusion;land surfaces dynamics;spatiotemporally heterogeneous areas;rough temporal resolution;Landsat images;coarse image;Moderate Resolution Imaging Spectroradiometer;deep learning-based STF methods;frequent cloud contamination;landcover change prediction;unstable fusion performance;current STF methods;NDVI time series;DL-based STF methods;fusion uncertainty estimation method;multitask deep learning framework","","","","45","IEEE","4 Jan 2022","","","IEEE","IEEE Journals"
"Unmixing-Based PAN-Guided Fusion Network for Hyperspectral Imagery","S. Li; Y. Tian; H. Xia; Q. Liu","School of Geography and Information Engineering, China University of Geosciences, Wuhan, China; School of Geography and Information Engineering, China University of Geosciences, Wuhan, China; School of Geography and Information Engineering, China University of Geosciences, Wuhan, China; School of Geography and Information Engineering, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","10 Mar 2022","2022","60","","1","17","The hyperspectral image (HSI) has been widely used in many applications due to its fruitful spectral information. However, the limitation of imaging sensors has reduced its spatial resolution that causes detail loss. One solution is to fuse the low spatial resolution HSI (LR-HSI) and the panchromatic (PAN) image with inverse features to get the high-resolution HSI (HR-HSI). Most of the existing fusion methods just focus on small fusion ratios like 4 or 6, which might be impractical for some large ratios’ HSI and PAN image pairs. Moreover, the ill-posedness of restoring detail information in HSI with hundreds of bands from PAN image with only one band has not been solved effectively, especially under large fusion ratios. Therefore, a lightweight unmixing-based PAN-guided fusion network (Pgnet) is proposed to mitigate this ill-posedness and improve the fusion performance significantly. Note that the fusion process of the proposed network is under the projected low-dimensional abundance subspace with an extremely large fusion ratio of 16. Furthermore, based on the linear and nonlinear relationships between the PAN intensity and abundance, an interpretable PAN detail inject network (PDIN) is designed to inject the PAN details into the abundance feature efficiently. Comprehensive experiments on simulated and real datasets demonstrate the superiority and generality of our method over several state-of-the-art (SOTA) methods qualitatively and quantitatively (the codes in pytorch and paddle versions and dataset could be available at https://github.com/rs-lsl/Pgnet).","1558-0644","","10.1109/TGRS.2022.3141765","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676675","Detail injection;extreme hyperpansharpening;hyperspectral unmixing;multi-scale fusion;panchromatic (PAN)-guided network","Task analysis;Spatial resolution;Hyperspectral imaging;Pansharpening;Image restoration;Image resolution;Convolution","geophysical image processing;geophysical signal processing;hyperspectral imaging;image fusion;image resolution;image sensors;remote sensing;sensor fusion","hyperspectral imagery;hyperspectral image;fruitful spectral information;low spatial resolution HSI;LR-HSI;panchromatic image;inverse features;high-resolution HSI;existing fusion methods;fusion ratios;PAN image pairs;lightweight unmixing-based PAN-guided fusion network;fusion performance;fusion process;low-dimensional abundance subspace;extremely large fusion ratio;interpretable PAN detail inject network;PAN details;abundance feature","","","","61","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"D2TNet: A ConvLSTM Network With Dual-Direction Transfer for Pan-Sharpening","M. Gong; J. Ma; H. Xu; X. Tian; X. -P. Zhang","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Department of Electrical, Computer, and Biomedical Engineering, Ryerson University, Toronto, ON, Canada","IEEE Transactions on Geoscience and Remote Sensing","3 May 2022","2022","60","","1","14","In this article, we propose an efficient convolutional long short-term memory (ConvLSTM) network with dual-direction transfer for pan-sharpening, termed D2TNet. We design a specially structured ConvLSTM network that allows for dual-directional communication, including multiscale information and multilevel information. On the one hand, due to the sensitivity of spatial information to scales and the sensitivity of spectral information to levels, multiscale and multilevel information is extracted to facilitate the fuller use of source images. On the other hand, ConvLSTM is employed to capture the strong dependencies between multiscale information and multilevel information. Besides, we introduce a multiscale loss to enable different scales contributing to each other to generate high-resolution multispectral images that are closer to the ground truth. Extensive experiments, including qualitative evaluation, quantitative evaluation, and efficiency comparison, are implemented to verify that our D2TNet outperforms state-of-the-art methods indeed.","1558-0644","","10.1109/TGRS.2022.3169134","Natural Science Fund of Hubei Province(grant numbers:2019CFA037); Key Research and Development Program of Hubei Province(grant numbers:2020BAB113); Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2020-04661); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761261","Convolutional long short-term memory (ConvLSTM);dual-direction transfer;multiscale loss;pan-sharpening","Feature extraction;Logic gates;Fuses;Image sensors;Deep learning;Convolutional neural networks;Spatial resolution","geophysical image processing;image classification;image fusion;image resolution;image texture;remote sensing","dual-direction transfer;pan-sharpening;short-term memory network;termed D2TNet;specially structured ConvLSTM network;dual-directional communication;multiscale information;multilevel information;spatial information;spectral information;multiscale loss;D2TNet outperforms state-of-the-art methods","","","","41","IEEE","21 Apr 2022","","","IEEE","IEEE Journals"
"Temporal Attention Networks for Multitemporal Multisensor Crop Classification","Z. Li; G. Chen; T. Zhang","School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China; Wireless Technology Laboratory, Ericsson Research, Beijing, China; National Key Laboratory of Science and Technology on Multi-spectral Information Processing, Wuhan, China","IEEE Access","26 Sep 2019","2019","7","","134677","134690","Crop classification based on multitemporal multisensor remote sensing imagery has a great significance. As more and more Earth observation satellites are launched, it becomes easier to obtain increasingly dense multitemporal data, and the category patterns hidden in multitemporal data can be more and more finely mined and expressed. However, the traditional classification methods treat the features of different temporal periods consistently, so the classification performances are not well enough, especially for the categories with subtle phenological differences. In this paper, we propose a temporal-attention CNN-GRU (Convolutional Neural Networks and Gated Recurrent Unit Networks) approach to distinguish subtle crop differences, and the temporal attention mechanism introduced in the model can achieve the effect of enhancing phenological differences and suppressing phenological similarity. Firstly, we use the GRU networks to model the temporal correlation of the multitemporal data. And then, the temporal attention layer utilize a query module to retrieve “what is the important information” over the whole temporal sequence, so we can obtain the attention weights for each temporal period. In the experiments, multitemporal samples are collected from Sentinel-2A/B and Landsat-8. Due to the different spatial resolutions of multiband images, transposed convolution which can extract the raw spatial structure information is used for multiband features fusion. Experimental results on multitemporal data show that the proposed approach achieve the best performance compared with conventional methods, especially for the categories with similar phenological laws.","2169-3536","","10.1109/ACCESS.2019.2939152","National Natural Science Foundation of China(grant numbers:61227007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822931","CNN-GRU;crop classification;multiresolution fusion;multitemporal multisensor;temporal attention","Agriculture;Remote sensing;Earth;Artificial satellites;Spatial resolution;Satellites;Feature extraction","convolutional neural nets;crops;feature extraction;geophysical image processing;image classification;image fusion;recurrent neural nets;remote sensing;vegetation mapping","multitemporal multisensor crop classification;multitemporal multisensor remote sensing imagery;Earth observation satellites;temporal-attention CNN-GRU;Gated Recurrent Unit Networks;GRU networks;classification methods;convolutional neural networks;gated recurrent unit networks;Sentinel-2A/B;Landsat-8;multiband features fusion;temporal attention networks","","10","","38","CCBY","3 Sep 2019","","","IEEE","IEEE Journals"
"Wildland Fire Detection and Monitoring Using a Drone-Collected RGB/IR Image Dataset","X. Chen; B. Hopkins; H. Wang; L. O’Neill; F. Afghah; A. Razi; P. Fulé; J. Coen; E. Rowell; A. Watts","School of Computing, Clemson University, Clemson, SC, USA; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; School of Computing, Clemson University, Clemson, SC, USA; School of Forestry, Northern Arizona University, Flagstaff, AZ, USA; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA; School of Computing, Clemson University, Clemson, SC, USA; School of Forestry, Northern Arizona University, Flagstaff, AZ, USA; National Center for Atmospheric Research, Boulder, CO, USA; Desert Research Institute, Reno, NV, USA; U.S. Forest Service, Pacific Wildland Fire Science Laboratory, Seattle, WA, USA","IEEE Access","24 Nov 2022","2022","10","","121301","121317","Current forest monitoring technologies including satellite remote sensing, manned/piloted aircraft, and observation towers leave uncertainties about a wildfire’s extent, behavior, and conditions in the fire’s near environment, particularly during its early growth. Rapid mapping and real-time fire monitoring can inform in-time intervention or management solutions to maximize beneficial fire outcomes. Drone systems’ unique features of 3D mobility, low flight altitude, and fast and easy deployment make them a valuable tool for early detection and assessment of wildland fires, especially in remote forests that are not easily accessible by ground vehicles. In addition, the lack of abundant, well-annotated aerial datasets – in part due to unmanned aerial vehicles’ (UAVs’) flight restrictions during prescribed burns and wildfires – has limited research advances in reliable data-driven fire detection and modeling techniques. While existing wildland fire datasets often include either color or thermal fire images, here we present (1) a multi-modal UAV-collected dataset of dual-feed side-by-side videos including both RGB and thermal images of a prescribed fire in an open canopy pine forest in Northern Arizona and (2) a deep learning-based methodology for detecting fire and smoke pixels at accuracy much higher than the usual single-channel video feeds. The collected images are labeled to “fire” or “no-fire” frames by two human experts using side-by-side RGB and thermal images to determine the label. To provide context to the main dataset’s aerial imagery, the included supplementary dataset provides a georeferenced pre-burn point cloud, an RGB orthomosaic, weather information, a burn plan, and other burn information. By using and expanding on this guide dataset, research can develop new data-driven fire detection, fire segmentation, and fire modeling techniques.","2169-3536","","10.1109/ACCESS.2022.3222805","Air Force Office of Scientific Research(grant numbers:FA9550-20-1-0090); National Science Foundation(grant numbers:CNS-2232048,CNS-2204445,CNS-2038741,CNS-2038759); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9953997","Data-driven fire detection;prescribed fire;fire modeling;fire data;unmanned aerial vehicle (UAV);deep learning","Data models;Fires;Autonomous aerial vehicles;Deep learning;Forests;Monitoring;Satellite navigation systems;Remote sensing","autonomous aerial vehicles;deep learning (artificial intelligence);forestry;geophysical image processing;image colour analysis;image fusion;infrared imaging;object detection;remote sensing;robot vision;video signal processing;wildfires","3D mobility;beneficial fire outcomes;burn information;deep learning-based methodology;drone systems;drone-collected RGB-IR image dataset;dual-feed side-by-side videos;fire modeling techniques;fire segmentation;forest monitoring technologies;georeferenced pre-burn point cloud;ground vehicles;guide dataset;in-time intervention;low flight altitude;multimodal UAV-collected dataset;no-fire frames;Northern Arizona;observation towers;open canopy pine forest;prescribed burns;prescribed fire;real-time fire monitoring;reliable data-driven fire detection;remote forests;RGB orthomosaic;satellite remote sensing;smoke pixels;thermal images;unmanned aerial vehicles;well-annotated aerial datasets;wildland fire datasets;wildland fire detection","","","","84","CCBY","17 Nov 2022","","","IEEE","IEEE Journals"
"A Unified Pansharpening Method With Structure Tensor Driven Spatial Consistency and Deep Plug-and-Play Priors","P. Liu; J. Liu; L. Xiao","Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing, China; Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing, China; Jiangsu Key Laboratory of Spectral Imaging and Intelligent Sense, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","12 Dec 2022","2022","60","","1","14","Pansharpening is to generate a high-resolution multispectral (HRMS) image by preserving the spectral information from a low-resolution multispectral (LRMS) image and the spatial content from a panchromatic (PAN) image. This article proposes a unified pansharpening method with structure tensor driven spatial consistency and deep plug-and-play priors. First, the spectral fidelity constraint between HRMS and LRMS is imposed for preserving spectral information. Second, the structure tensor is applied to characterize the spatial geometric information of HRMS and PAN images, thus the structure tensor driven spatial consistency prior between HRMS and PAN is particularly exploited for preserving spatial content. Moreover, by generalizing the convolution neural network (CNN) fusion method into a unified variational framework, a novel CNN-based deep plug-and-play prior between the HRMS and CNN-based fused multispectral (MS) images is also proposed to generate more image characteristics for further preserving spectral information and spatial content. Besides, the proposed model is solved by the alternating direction method of multipliers (ADMMs) algorithm. Finally, extensive experiments on both reduced and full resolution by comparing with various representative approaches exhibit the excellent performance of the proposed method.","1558-0644","","10.1109/TGRS.2022.3225563","China Postdoctoral Science Foundation(grant numbers:2022M711692); National Natural Science Foundation of China(grant numbers:61802202); Fundamental Research Funds for the Central Universities(grant numbers:JSGP202204); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9966820","Convolution neural network (CNN);deep plug-and-play prior;pansharpening;structure tensor;variational models","Pansharpening;Tensors;Spatial resolution;Plugs;Optimization;Adaptation models;Sensors","geophysical image processing;image fusion;image resolution;neural nets;remote sensing","-play priors;alternating direction method;convolution neural network fusion method;deep plug;full resolution;high-resolution multispectral image;HRMS;image characteristics;low-resolution multispectral image;multispectral images;PAN;panchromatic image;preserving spectral information;reduced resolution;spatial content;spatial geometric information;spectral fidelity constraint;structure tensor driven spatial consistency;unified pansharpening method;unified variational framework","","","","54","IEEE","30 Nov 2022","","","IEEE","IEEE Journals"
"A Flexible Object-Level Processing Strategy to Enhance the Weight Function-Based Spatiotemporal Fusion Method","D. Guo; W. Shi; H. Zhang; M. Hao","Jiangsu Key Laboratory of Resources and Environmental Information Engineering, China University of Mining and Technology, Xuzhou, China; Department of Land Surveying and Geo-Informatics, Smart Cities Research Institute, The Hong Kong Polytechnic University, Hong Kong, China; Jiangsu Key Laboratory of Resources and Environmental Information Engineering, China University of Mining and Technology, Xuzhou, China; Jiangsu Key Laboratory of Resources and Environmental Information Engineering, China University of Mining and Technology, Xuzhou, China","IEEE Transactions on Geoscience and Remote Sensing","26 Oct 2022","2022","60","","1","11","Spatiotemporal fusion technique provides a cost-efficient way to achieve dense time series observation. Among all categories of spatiotemporal fusion methods, the weight function-based method attracted considerable attention. However, this kind of method selects similar pixels in a regular window without considering the distribution of features, which will weaken its ability to preserve the structure information. Besides, the weight function-based method carries out pixel-by-pixel fusion computation, which leads to computational inefficiency. To solve the aforementioned issues, a flexible object-level (OL) processing strategy is proposed in this article. Three popular spatiotemporal fusion methods include the spatial and temporal adaptive reflectance fusion model (STARFM), the enhance STARFM (ESTARFM) and the three-step method (Fit-FC) were selected as examples to analyze and validate the effectiveness of the OL processing strategy. Four study sites with different surface landscapes and change patterns were adopted for experiments. Experimental results indicated that the OL fusion versions of STARFM, ESTARFM, and Fit-FC can better preserve the structural information, and were 102.89–113.71, 92.77–115.73, and 30.51–36.15 times faster than their original methods. Remarkably, the OL fusion versions of Fit-FC outperform all competing methods in one-pair case fusion experiments, especially in Poyang lake wetland (PY) area (root mean square error (RMSE) is 0.0343 versus 0.0380, correlation coefficient ( $r$ ) is 0.7469 versus 0.6986 compare with Fit-FC). Additionally, the OL processing strategy can also be adopted to enhance other methods which use the principle of combining similar adjacent information. The program and test data are available at https://github.com/Andy-cumt.","1558-0644","","10.1109/TGRS.2022.3212474","Otto Poon Charitable Foundation Smart Cities Research Institute, The Hong Kong Polytechnic University(grant numbers:CD03); Ministry of Science and Technology of the People’s Republic of China(grant numbers:2019YFB2103102); Urban Informatics for Smart Cities, The Hong Kong Polytechnic University(grant numbers:1-ZVN6); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9912409","Enhance spatial and temporal adaptive reflectance fusion model (ESTARFM);Fit-FC;object-level (OL) processing;spatiotemporal fusion;spatial and temporal adaptive reflectance fusion model (STARFM);weight function-based method","Spatiotemporal phenomena;Image segmentation;Computational modeling;Satellites;Time series analysis;Graphics processing units;Spatial resolution","geophysical image processing;image fusion;image resolution;lakes;mean square error methods;remote sensing;spatiotemporal phenomena;time series;wetlands","dense time series observation;weight function-based method;structure information;pixel-by-pixel fusion computation;flexible object-level processing strategy;spatiotemporal fusion methods;spatial reflectance fusion model;temporal adaptive reflectance fusion model;STARFM;three-step method;OL processing strategy;OL fusion versions;structural information;one-pair case fusion experiments;weight function-based spatiotemporal fusion method;spatiotemporal fusion technique;Poyang lake wetland","","","","46","IEEE","5 Oct 2022","","","IEEE","IEEE Journals"
"Two-Stream Deep Architecture for Hyperspectral Image Classification","S. Hao; W. Wang; Y. Ye; T. Nie; L. Bruzzone","College of Communication and Electronic Engineering, Qingdao University of Technology, Qingdao, China; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; College of Communication and Electronic Engineering, Qingdao University of Technology, Qingdao, China; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Geoscience and Remote Sensing","23 Mar 2018","2018","56","4","2349","2361","Most traditional approaches classify hyperspectral image (HSI) pixels relying only on the spectral values of the input channels. However, the spatial context around a pixel is also very important and can enhance the classification performance. In order to effectively exploit and fuse both the spatial context and spectral structure, we propose a novel two-stream deep architecture for HSI classification. The proposed method consists of a two-stream architecture and a novel fusion scheme. In the two-stream architecture, one stream employs the stacked denoising autoencoder to encode the spectral values of each input pixel, and the other stream takes as input the corresponding image patch and deep convolutional neural networks are employed to process the image patch. In the fusion scheme, the prediction probabilities from two streams are fused by adaptive class-specific weights, which can be obtained by a fully connected layer. Finally, a weight regularizer is added to the loss function to alleviate the overfitting of the class-specific fusion weights. Experimental results on real HSIs demonstrate that the proposed two-stream deep architecture can achieve competitive performance compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2017.2778343","National Natural Science Foundation of China(grant numbers:61572269,61701272); Fundamental Research Funds for the Central Universities(grant numbers:2682016CX083); Natural Science Foundation of Shandong Province(grant numbers:ZR2017PF004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240968","Class-specific fusion;convolutional neural networks (CNNs);deep learning;hyperspectral image (HSI) classification;remote sensing;stacked denoising autoencoder (SdAE);two-stream architecture","Feature extraction;Machine learning;Hyperspectral imaging;Training","hyperspectral imaging;image classification;image denoising;image fusion;learning (artificial intelligence);neural net architecture;probability","spectral structure;two-stream deep architecture;HSI classification;two-stream architecture;fusion scheme;deep convolutional neural networks;hyperspectral image classification;classification performance enhancement;HSI pixels;stacked denoising autoencoder;spectral value encoding;image patch processing;prediction probabilities;adaptive class-specific weights;fully connected layer;weight regularizer;class-specific fusion weight overfitting","","79","","58","IEEE","27 Dec 2017","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and Multispectral Images Using Spectral Unmixing and Sparse Coding","Z. H. Nezhad; A. Karami; R. Heylen; P. Scheunders","Department of Electrical Engineering, Shahid Bahonar University of Kerman, Kerman, Iran; Visionlab, University of Antwerp, Antwerp, Belgium; Visionlab, University of Antwerp, Antwerp, Belgium; Visionlab, University of Antwerp, Antwerp, Belgium","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","6","2377","2389","Unlike multispectral (MSI) and panchromatic (PAN) images, generally the spatial resolution of hyperspectral images (HSI) is limited, due to sensor limitations. In many applications, HSI with a high spectral as well as spatial resolution are required. In this paper, a new method for spatial resolution enhancement of a HSI using spectral unmixing and sparse coding (SUSC) is introduced. The proposed method fuses high spectral resolution features from the HSI with high spatial resolution features from an MSI of the same scene. Endmembers are extracted from the HSI by spectral unmixing, and the exact location of the endmembers is obtained from the MSI. This fusion process by using spectral unmixing is formulated as an ill-posed inverse problem which requires a regularization term in order to convert it into a well-posed inverse problem. As a regularizer, we employ sparse coding (SC), for which a dictionary is constructed using high spatial resolution MSI or PAN images from unrelated scenes. The proposed algorithm is applied to real Hyperion and ROSIS datasets. Compared with other state-of-the-art algorithms based on pansharpening, spectral unmixing, and SC methods, the proposed method is shown to significantly increase the spatial resolution while perserving the spectral content of the HSI.","2151-1535","","10.1109/JSTARS.2016.2528339","Flemish fund for scientific research (FWO - Vlaanderen); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7425150","Fusion;hyperspectral images (HSI);multispectral images (MSI);sparse coding (SC);spectral unmixing","Spatial resolution;Dictionaries;Inverse problems;Hyperspectral sensors;Distortion;Earth","geophysical image processing;geophysical techniques;hyperspectral imaging;image coding;image fusion;image resolution;inverse problems;remote sensing","hyperspectral image sensor;multispectral image;spectral unmixing;sparse coding;panchromatic image;spatial resolution;HSI;image fusion;ill-posed inverse problem;MSI;spatial resolution MSI image;high spatial PAN image;state-of-the-art algorithm;spectral unmixing;Hyperion dataset;ROSIS dataset","","48","","41","IEEE","3 Mar 2016","","","IEEE","IEEE Journals"
"Generative Adversarial Network for Pansharpening With Spectral and Spatial Discriminators","A. Gastineau; J. -F. Aujol; Y. Berthoumieu; C. Germain","CNRS, UMR 5251, Bordeaux INP, Institut de Mathématique de Bordeaux (IMB), University of Bordeaux, Talence, France; CNRS, UMR 5251, Bordeaux INP, Institut de Mathématique de Bordeaux (IMB), University of Bordeaux, Talence, France; Laboratoire de Intégration du Matériau au Systéme (IMS), CNRS, UMR 5218, Bordeaux INP, University of Bordeaux, Talence, France; Laboratoire de Intégration du Matériau au Systéme (IMS), CNRS, UMR 5218, Bordeaux INP, University of Bordeaux, Talence, France","IEEE Transactions on Geoscience and Remote Sensing","8 Dec 2021","2022","60","","1","11","The pansharpening problem amounts to fusing a high-resolution panchromatic image with a low-resolution multispectral image so as to obtain a high-resolution multispectral image. Therefore, the preservation of the spatial resolution of the panchromatic image and the spectral resolution of the multispectral image is of key importance for the pansharpening problem. To cope with it, we propose a new method based on a bidiscriminator in a generative adversarial network (GAN) framework. The first discriminator is optimized to preserve textures of images by taking as input the luminance and the near-infrared band of images, and the second discriminator preserves the color by comparing the chroma components Cb and Cr. Thus, this method allows to train two discriminators, each one with a different and complementary task. Moreover, to enhance these aspects, the proposed method based on bidiscriminator, and called MDSSC-GAN SAM, considers a spatial and a spectral constraint in the loss function of the generator. We show the advantages of this new method on experiments carried out on Pléiades and World View 3 satellite images.","1558-0644","","10.1109/TGRS.2021.3060958","French State through the French National Research Agency (ANR) in the Framework of the Investments for the Future Programme IdEx Bordeaux—SysNum(grant numbers:ANR-10-IDEX-03-02); ANR(grant numbers:ANR-18-CE92-0050 SUPREMATIM); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9371303","Bidiscriminator;deep learning;generative adversarial network (GAN);pansharpening;remote sensing","Spatial resolution;Pansharpening;Gallium nitride;Generative adversarial networks;Vegetation mapping;Satellites;Generators","image colour analysis;image fusion;image resolution;image texture;neural nets;spectral analysis","pansharpening;high-resolution panchromatic image;low-resolution multispectral image;high-resolution multispectral image;spatial resolution;spectral resolution;generative adversarial network;spectral discriminator;spatial discriminator;loss function;image texture","","11","","29","IEEE","5 Mar 2021","","","IEEE","IEEE Journals"
"A Graph-Based Approach for Data Fusion and Segmentation of Multimodal Images","G. Iyer; J. Chanussot; A. L. Bertozzi","Department of Mathematics, University of California at Los Angeles, Los Angeles, CA, USA; CNRS, Grenoble INP, GIPSA-lab, Univ. Grenoble Alpes, Grenoble, France; Department of Mathematics, University of California at Los Angeles, Los Angeles, CA, USA","IEEE Transactions on Geoscience and Remote Sensing","21 Apr 2021","2021","59","5","4419","4429","In the past few years, graph-based methods have proven to be a useful tool in a wide variety of energy minimization problems. In this article, we propose a graph-based algorithm for feature extraction and segmentation of multimodal images. By defining a notion of similarity that integrates information from each modality, we create a fused graph that merges the different data sources. The graph Laplacian then allows us to perform feature extraction and segmentation on the fused data set. We apply this method in a practical example, namely, the segmentation of optical and LiDAR images. The results obtained confirm the potential of the proposed method.","1558-0644","","10.1109/TGRS.2020.2971395","NSF(grant numbers:DMS-1118971); Office of Naval Research(grant numbers:N00014-16-1-2119); NSF(grant numbers:DMS-1417674); European Research Council (CHESS Project)(grant numbers:320684); Centre National de la Recherche Scientifique(grant numbers:PICS-USA 263484); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206144","Graphs;manifold learning;multimodal remote sensing;segmentation","Image segmentation;Laser radar;Data integration;Laplace equations;Optical imaging;Optical sensors","feature extraction;graph theory;image fusion;image segmentation;optical radar;sensor fusion","graph-based approach;data fusion;multimodal images;graph-based methods;energy minimization problems;graph-based algorithm;feature extraction;integrates information;fused graph;graph Laplacian;fused data;optical LiDAR images","","8","","39","IEEE","25 Sep 2020","","","IEEE","IEEE Journals"
"Explore Better Network Framework for High-Resolution Optical and SAR Image Matching","H. Zhang; L. Lei; W. Ni; T. Tang; J. Wu; D. Xiang; G. Kuang","Northwest Institute of Nuclear Technology, Xi’an, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; Northwest Institute of Nuclear Technology, Xi’an, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; Northwest Institute of Nuclear Technology, Xi’an, China; Beijing Advanced Innovation Center for Soft Matter Science and Engineering and the Interdisciplinary Research Center for Artificial Intelligence, Beijing University of Chemical Technology, Beijing, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","15 Feb 2022","2022","60","","1","18","To fully explore the complementary information from optical and synthetic aperture radar (SAR) imageries, they need first to be coregistered with high accuracy. Due to the vast radiometric and geometric disparity, the problem to match high-resolution optical and SAR images is quite challenging. The present deep learning-based methods have shown advantages over the traditional approaches, but the performance increment is not significant. In this article, we explore a better network framework for high-resolution optical and SAR image matching from three aspects. First, we propose an effective multilevel feature fusion method, which helps to take advantage of both the low-level fine-grained features for precious feature location and the high-level semantic features for better discriminative ability. Second, a feature channel excitation procedure is conducted using a novel multifrequency channel attention module, which is able to make image features of different types and multiple levels effectively collaborate with each other and produce image matching features with high diversity. Third, the self-adaptive weighting loss is introduced, with which, each sample is assigned with an adaptive weighting factor, and therefore, information buried in all nearby samples can be better exploited. Under a pseudo-Siamese architecture, the proposed optical and SAR image matching network (OSMNet) is trained and tested on a large and diverse high-resolution optical and SAR dataset. Extensive experiments demonstrate that each component of the proposed deep framework helps to improve the matching accuracy. Also, the OSMNet shows overwhelming superior to the state-of-the-art handcrafted approaches on imageries of different land-cover types.","1558-0644","","10.1109/TGRS.2021.3126939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609993","Convolutional neural networks (CNNs);feature fusion;high resolution;image matching;multi-frequency channel attention;optical;remote sensing;self-adaptive weighting loss (SAW);synthetic aperture radar (SAR)","Optical imaging;Adaptive optics;Radar polarimetry;Optical sensors;Nonlinear optics;Image matching;Spatial resolution","feature extraction;image fusion;image matching;learning (artificial intelligence);radar imaging;synthetic aperture radar","network framework;vast radiometric disparity;geometric disparity;deep learning-based methods;effective multilevel feature fusion method;low-level fine-grained features;precious feature location;high-level semantic features;feature channel excitation procedure;image features;image matching features;proposed optical SAR image;large resolution optical;diverse high-resolution optical;SAR dataset;matching accuracy","","4","","54","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"MegaStitch: Robust Large-Scale Image Stitching","A. Zarei; E. Gonzalez; N. Merchant; D. Pauli; E. Lyons; K. Barnard","Department of Computer Science, The University of Arizona, Tucson, AZ, USA; School of Plant Sciences, The University of Arizona, Tucson, AZ, USA; Data Science Institute, The University of Arizona, Tucson, AZ, USA; School of Plant Sciences, The University of Arizona, Tucson, AZ, USA; School of Plant Sciences, The University of Arizona, Tucson, AZ, USA; Department of Computer Science, The University of Arizona, Tucson, AZ, USA","IEEE Transactions on Geoscience and Remote Sensing","10 Mar 2022","2022","60","","1","9","We address fast image stitching for large image collections while being robust to drift due to chaining transformations and minimal overlap between images. We focus on scientific applications where ground-truth accuracy is far more important than visual appearance or projection error, which can be misleading. For common large-scale image stitching use cases, transformations between images are often restricted to similarity or translation. When homography is used in these cases, the odds of being trapped in a poor local minimum and producing unnatural results increases. Thus, for transformations up to affine, we cast stitching as minimizing reprojection error globally using linear least-squares with a few, simple constraints. For homography, we observe that the global affine solution provides better initialization for bundle adjustment compared to an alternative that initializes with a homography-based scaffolding and at lower computational cost. We evaluate our methods on a very large translation dataset with limited overlap as well as four drone datasets. We show that our approach is better compared to alternative methods such as MGRAPH in terms of computational cost, scaling to large numbers of images, and robustness to drift. We also contribute ground-truth datasets for this endeavor.","1558-0644","","10.1109/TGRS.2022.3141907","Department of Energy Advanced Research Projects Agency–Energy(grant numbers:DE-AR0000594,DE-AR0001101); Department of Energy Biological and Environmental Research(grant numbers:DE-SC0020401); Bill & Melinda Gates Foundation(grant numbers:OPP1129603); Cotton Incorporated(grant numbers:20-720); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676637","Bundle adjustment;image geocorrection;image stitching;linear least squares;remote sensing","Bundle adjustment;Image stitching;Drones;Optimization;Cameras;Global Positioning System;Feature extraction","cameras;feature extraction;image fusion;image matching;image reconstruction;image registration;image segmentation;motion estimation;pose estimation","chaining transformations;scientific applications;ground-truth accuracy;visual appearance;common large-scale image stitching;poor local minimum;producing unnatural results increases;reprojection error;global affine solution;homography-based scaffolding;translation dataset;ground-truth datasets;fast image stitching;image collections","","1","","45","CCBYNCND","11 Jan 2022","","","IEEE","IEEE Journals"
"Blind Quality Assessment of Fused WorldView-3 Images by Using the Combinations of Pansharpening and Hypersharpening Paradigms","C. Kwan; B. Budavari; A. C. Bovik; G. Marchisio","Applied Research LLC., Rockville, MD, USA; Applied Research LLC., Rockville, MD, USA; University of Texas, Austin, TX, USA; Digital Globe, Inc., Westminster, CO, USA","IEEE Geoscience and Remote Sensing Letters","25 Sep 2017","2017","14","10","1835","1839","WorldView 3 (WV-3) is the first commercially deployed super-spectral, very high-resolution (HR) satellite. However, the resolution of the short-wave infrared (SWIR) bands is much lower than that of the other bands. In this letter, we describe four different approaches, which are combinations of pansharpening and hypersharpening methods, to generate HR SWIR images. Since there are no ground truth HR SWIR images, we also propose a new picture quality predictor to assess hypersharpening performance, without the need for reference images. We describe extensive experiments using actual WV-3 images that demonstrate that some approaches can yield better performance than others, as measured by the proposed blind image quality assessment model of hypersharpened SWIR images.","1558-0571","","10.1109/LGRS.2017.2737820","Defense Advanced Research Projects Agency(grant numbers:D17PC00025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8016358","Hypersharpening;image quality;multispectral;pansharpening;short-wave infrared (SWIR);superspectral;very high resolution (VHR);visible near infrared (VNIR);WorldView 3 (WV-3)","Image resolution;Distortion;Distortion measurement;Image quality;Fuses;Training","geophysical image processing;image fusion","blind quality assessment;fused WorldView-3 images;pansharpening combinations;hypersharpening paradigms;SWIR images","","80","","22","IEEE","24 Aug 2017","","","IEEE","IEEE Journals"
"Hyperspectral Anomaly Detection With Multiscale Attribute and Edge-Preserving Filters","S. Li; K. Zhang; Q. Hao; P. Duan; X. Kang","Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Changsha, China","IEEE Geoscience and Remote Sensing Letters","26 Sep 2018","2018","15","10","1605","1609","In this letter, a novel anomaly detection method is proposed, which can effectively fuse the multiscale information extracted by attribute and edge-preserving filters. The proposed method consists of the following steps. First, multiscale attribute and edge-preserving filters are utilized to obtain multiscale anomaly detection maps. Then, the multiscale detection maps are fused via an averaging approach, and the training samples of the anomalies and background are selected from the fused detection map. Next, the support vector machine classification is performed on the hyperspectral image to obtain an anomaly probability map. Finally, the detection result is obtained by multiplying the fused detection map and the anomaly probability map, followed by an edge-preserving filtering-based postprocessing. Experiments performed on four real hyperspectral data sets demonstrate that the proposed method shows a better detection performance with respect to several state-of-the-art hyperspectral anomaly detection methods.","1558-0571","","10.1109/LGRS.2018.2853705","National Natural Science Fund of China for International Cooperation and Exchanges(grant numbers:61520106001); National Natural Science Foundation of China(grant numbers:61601179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8421593","Anomaly detection;edge-preserving filter;hyperspectral image;multiscale information fusion","Anomaly detection;Hyperspectral imaging;Image edge detection;Support vector machines;Training;Kernel","edge detection;hyperspectral imaging;image classification;image filtering;image fusion;image sampling;probability;support vector machines","multiscale attribute;edge-preserving filters;multiscale information;multiscale anomaly detection maps;multiscale detection maps;fused detection map;anomaly probability map;anomaly detection;edge-preserving filtering;hyperspectral anomaly detection;support vector machine classification","","48","","17","IEEE","27 Jul 2018","","","IEEE","IEEE Journals"
"Fast Color Blending for Seamless Image Stitching","F. Fang; T. Wang; Y. Fang; G. Zhang","Department of Computer Science and Technology, East China Normal University, Shanghai, China; Department of Computer Science and Technology, East China Normal University, Shanghai, China; Department of Mathematics, Hong Kong Baptist University, Hong Kong; Department of Computer Science and Technology, East China Normal University, Shanghai, China","IEEE Geoscience and Remote Sensing Letters","24 Jun 2019","2019","16","7","1115","1119","In this letter, we propose a fast and robust method for stitching overlapped images captured by the unmanned aerial vehicle. First, we apply the shape-preserving half-projective method to precisely and stably align a pair of partially overlapped input images. Then, an optimal stitching line is searched to remove ghosts caused by the moving objects in the overlapped area. We subsequently propose a color blending method to eliminate all the color inconsistencies in the prealigned image. In accordance with the color differences of the pixels on the optimal stitching seam, we utilize weighted value coordinate interpolation algorithms to compute accurate color changes for all the pixels in the target image. The calculated color changes are then added to the target image to remove the color inconsistency. Furthermore, we introduce the superpixel segmentation to divide the target image into a reduced number of superpixels, and we assign each superpixel the same color change value. Such a superpixel level operation can greatly reduce the computational complexity. Experiments show that our method is promising to achieve effective and efficient stitching results.","1558-0571","","10.1109/LGRS.2019.2893210","Key Project of the National Natural Science Foundation of China(grant numbers:61731009); National Natural Science Foundation of China(grant numbers:61871185); Shanghai Municipal Education Commission(grant numbers:17CG25); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676030","Color blending;image stitching;superpixels;unmanned aerial vehicle (UAV) aerial images","Image color analysis;Unmanned aerial vehicles;Feature extraction;Image segmentation;Interpolation;Lighting;Computational complexity","feature extraction;image capture;image colour analysis;image fusion;image registration;image resolution;image segmentation;interpolation","fast color blending;seamless image stitching;unmanned aerial vehicle;shape-preserving half-projective method;partially overlapped input images;optimal stitching line;overlapped area;color blending method;color inconsistency;prealigned image;color differences;pixels;optimal stitching seam;accurate color changes;target image;calculated color changes;color change value;computational complexity","","16","","26","IEEE","28 Mar 2019","","","IEEE","IEEE Journals"
"Restoration of Pansharpened Images by Conditional Filtering in the PCA Domain","J. Duran; A. Buades","Department of Mathematics and Computer Science, Institute of Applied Computing and Community Code, University of the Balearic Islands, Palma, Spain; Department of Mathematics and Computer Science, Institute of Applied Computing and Community Code, University of the Balearic Islands, Palma, Spain","IEEE Geoscience and Remote Sensing Letters","26 Feb 2019","2019","16","3","442","446","Pansharpening techniques aim at fusing a low-spatial resolution multispectral (MS) image with a higher spatial resolution panchromatic (PAN) image to produce an MS image at high spatial resolution. Despite significant progress in the field, spectral and spatial distortions might still compromise the quality of the results. We introduce a restoration strategy to mitigate artifacts of fused products. After applying the principal component analysis transform to a pansharpened image, the chromatic components are filtered conditionally to the geometry of PAN. The structural component is then replaced by the locally histogram-matched PAN for spatial enhancement. Experimental results illustrate the efficiency of the proposed restoration chain.","1558-0571","","10.1109/LGRS.2018.2873654","Ministerio de Ciencia e Innovación(grant numbers:TIN2014-53772-R,TIN2017-85572-P); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8527530","Aliasing;image restoration;nonlocal filtering;pansharpening;principal component analysis (PCA);satellite imagery","Image restoration;Principal component analysis;Transforms;Spatial resolution;Reactive power;Geometry;Distortion","geophysical image processing;geophysical techniques;image fusion;image resolution;principal component analysis","PCA domain;pansharpening techniques;low-spatial resolution multispectral image;higher spatial resolution panchromatic image;MS image;high spatial resolution;spectral distortions;spatial distortions;restoration strategy;fused products;principal component analysis;pansharpened image;locally histogram-matched PAN;spatial enhancement;restoration chain;conditional filtering;chromatic components;structural component","","12","","22","IEEE","8 Nov 2018","","","IEEE","IEEE Journals"
"Infrared Small UAV Target Detection Based on Residual Image Prediction via Global and Local Dilated Residual Networks","H. Fang; M. Xia; G. Zhou; Y. Chang; L. Yan","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Computer Science and Technology, Xidian University, Xi’an, China; State Key Laboratory of Material Processing and Die and Mold Technology, Huazhong University of Science and Technology, Wuhan, China; Artificial Intelligence Research Center, Pengcheng Laboratory, Shenzhen, China; School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Thermal infrared imaging possesses the ability to monitor unmanned aerial vehicles (UAVs) in both day and night conditions. However, long-range detection of the infrared UAVs often suffers from small/dim targets, heavy clutter, and noise in the complex background. The conventional local prior-based and the nonlocal prior-based methods commonly have a high false alarm rate and low detection accuracy. In this letter, we propose a model that converts small UAV detection into a problem of predicting the residual image (i.e., background, clutter, and noise). Such novel reformulation allows us to directly learn a mapping from the input infrared image to the residual image. The constructed image-to-image network integrates the global and the local dilated residual convolution blocks into the U-Net, which can capture local and contextual structure information well and fuse the features at different scales both for image reconstruction. Additionally, subpixel convolution is utilized to upscale the image and avoid image distortion during upsampling. Finally, the small UAV target image is obtained by subtracting the residual image from the input infrared image. The comparative experiments demonstrate that the proposed method outperforms state-of-the-art ones in detecting real-world infrared images with heavy clutter and dim targets.","1558-0571","","10.1109/LGRS.2021.3085495","National Natural Science Foundation of China(grant numbers:41501371,61971460); Natural Science Foundation of Shaanxi Province(grant numbers:2018JM4036); Open Research Fund of the National Key Laboratory of Science and Technology on Multispectral Information Processing(grant numbers:6142113190103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452107","Convolutional neural network (CNN);infrared small unmanned aerial vehicle (UAV) target;residual learning;target detection","Convolution;Unmanned aerial vehicles;Object detection;Training;Feature extraction;Clutter;Image reconstruction","autonomous aerial vehicles;convolutional neural nets;feature extraction;image fusion;image reconstruction;image sampling;infrared imaging;object detection","residual image prediction;local dilated residual networks;long-range detection;infrared UAVs;heavy clutter;nonlocal prior-based methods;low detection accuracy;UAV detection;image-to-image network;local dilated residual convolution blocks;local structure information;contextual structure information;image reconstruction;image distortion;UAV target image;real-world infrared images;infrared small UAV target detection;global dilated residual networks;thermal infrared imaging;unmanned aerial vehicles;subpixel convolution","","9","","13","IEEE","11 Jun 2021","","","IEEE","IEEE Journals"
"Multiscale Random Convolution Broad Learning System for Hyperspectral Image Classification","Y. Ma; Z. Liu; C. L. P. Chen","School of Automation, Guangdong University of Technology, Guangzhou, China; Guangdong-HongKong-Macao Joint Laboratory for Smart Discrete Manufacturing, Guangzhou, China; Faculty of Computer Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Deep learning is a powerful technique for image processing. Convolution neural network (CNN) is one of the widely used approaches for hyperspectral image (HSI) classification. These methods mostly need a time-consuming pretraining process to obtain deep features. Random patches networks (RPNets) provide a novel approach that the convolution kernel can be the original image without any pretraining process. In this letter, we propose a novel HSI classification method, multiscale random convolution broad learning system (MRC-BLS), which takes the spatial feature learning by an adaptive weighted mean filter as the convolution kernel to extract local spatial feature in the first layer. Different sizes of random convolution kernels can obtain a multiscale feature map. The weighted fusion of multiscale spatial features extracted by different sizes kernels can get better performance in HSI classification. A broad learning system (BLS) is an efficient classifier to classify images by the multiscale random feature. Experiments in three HSI data sets fully testify to the efficiency and satisfactory performance of the proposed method.","1558-0571","","10.1109/LGRS.2021.3060876","National Natural Science Foundation of China(grant numbers:61573108,61803090); Natural Science Foundation of Guangdong Province(grant numbers:2016A030313715); Guangdong Province Universities and Colleges Pearl River Scholar Funded Scheme; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9429865","Broad learning system (BLS);deep learning;hyperspectral image (HSI);multiscale random convolution","Feature extraction;Convolution;Kernel;Learning systems;Deep learning;Training;Principal component analysis","convolutional neural nets;deep learning (artificial intelligence);feature extraction;hyperspectral imaging;image classification;image filtering;image fusion;learning (artificial intelligence);random processes","multiscale random convolution broad learning system;hyperspectral image classification;deep learning;convolution neural network;random patches networks;convolution kernel;random convolution kernels;multiscale feature map;multiscale spatial features;multiscale random feature;CNN;RPNets;HSI classification method;MRC-BLS;adaptive weighted mean filter;local spatial feature extraction;spatial feature learning;weighted fusion","","2","","21","IEEE","12 May 2021","","","IEEE","IEEE Journals"
"Object Detection Deployed on UAVs for Oblique Images by Fusing IMU Information","H. Shen; D. Lin; T. Song","Beijing Key Laboratory of Unmanned Aerial Vehicle (UAV) Autonomous Control, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Unmanned Aerial Vehicle (UAV) Autonomous Control, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Unmanned Aerial Vehicle (UAV) Autonomous Control, Beijing Institute of Technology, Beijing, China","IEEE Geoscience and Remote Sensing Letters","24 Jan 2022","2022","19","","1","5","Object detection is a very challenging task due to the serious object scale diversity. There is an obvious scale distribution in the oblique images captured by the unmanned aerial vehicles (UAVs): the objects at the top of the image are smaller in scale, while the objects at the bottom of the image are larger in scale. Based on this prior information, we propose an object detector with the divide-and-conquer strategy. First, we estimate the object scale using inertial measurement unit (IMU) information. Then the small objects at the top of image are detected by shallow networks with small receptive field and the big objects at the bottom of image are detected by deep networks with big receptive field. Compared with YOLOv5, our method improves the accuracy by 4.6% on mean of average precision (mAP) metric and improves the speed by 79%. Our method can also be performed in real-time on an NVIDIA XAVIER NX with about 30 frames/s. The code is made available on GitHub (https://github.com/bitshenwenxiao/UAVYOLO).","1558-0571","","10.1109/LGRS.2022.3141109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9673756","Inertial measurement unit (IMU);object detection;oblique image;unmanned aerial vehicle (UAV)","Detectors;Decoding;Feature extraction;Cameras;Object detection;Task analysis;Automobiles","autonomous aerial vehicles;image fusion;image sensors;object detection;robot vision","object detection;UAV;oblique images;IMU information fusion;obvious scale distribution;unmanned aerial vehicles;object detector;divide-and-conquer strategy;inertial measurement unit information;big objects;big receptive field;IMU information;shallow network;small receptive field;deep networks;mean of average precision metric;mAP metric;NVIDIA XAVIER NX","","2","","28","IEEE","7 Jan 2022","","","IEEE","IEEE Journals"
"Multilevel Information Fusion-Based Change Detection for Multiangle PolSAR Images","B. Zou; H. Li; L. Zhang","Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","17 Dec 2021","2022","19","","1","5","Change detection is a key technology in the field of polarimetric synthetic aperture radar (PolSAR) image processing. The current research on the change detection mainly focuses on studying PolSAR images with the same angle or small angle difference, and the angle problem is not considered. However, when the angle difference occurs, especially a large angle difference, some pixels might be falsely detected because the angle difference can affect the polarimetric characteristics. In this letter, we propose a multilevel information fusion-based (MIFB) method, which is suitable for extracting change information from PolSAR images with angle difference. In particular, the proposed method first adopts data resolution correction, then applies an improved feature-based registration algorithm, and finally, incorporates weighted graph theory with the superpixel segmentation algorithm to extract and merge pixel-based and object-based change areas to eliminate false alarms. Experimental results for multitemporal and multiangle PolSAR images reveal that the MIFB method can effectively eliminate false detection caused by angle differences and improve the detection accuracy.","1558-0571","","10.1109/LGRS.2020.3041307","National Natural Science Foundation of China(grant numbers:61401124,61871158); Scientific Research Foundation for the Returned Overseas Scholars of Heilongjiang Province(grant numbers:LC2018029); Aeronautical Science Foundation of China(grant numbers:20182077008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288684","Change detection;image registration;multiangle;polarimetric synthetic aperture radar (PolSAR)","Scattering;Data mining;Feature extraction;TV;Change detection algorithms;Signal processing algorithms;Image segmentation","feature extraction;graph theory;image fusion;image registration;image segmentation;radar imaging;radar polarimetry;synthetic aperture radar","multilevel information fusion-based change detection;multiangle PolSAR images;polarimetric synthetic aperture radar image processing;multilevel information fusion-based method;change information;small angle difference;improved feature-based registration algorithm;object-based change areas;data resolution correction","","1","","13","IEEE","9 Dec 2020","","","IEEE","IEEE Journals"
"Fusion of United Sparse Principal Component Analysis Dictionary Based on Linear Unmixing Image Technique","X. Sun","Department of Ocean, Minjiang University, Fuzhou, China","IEEE Geoscience and Remote Sensing Letters","27 Dec 2018","2019","16","1","120","124","Based on the linear unmixing images of different surface objects, online dictionary learning algorithm was utilized to compute the sparse dictionaries for multispectral linear unmixing images and panchromatic images. Principal component analysis (PCA) was then utilized to generate united sparse PCA dictionaries through the extraction of the first principal components of panchromatic images and unmixing image dictionaries. The number of dictionaries is determined to be 480 after taking into consideration of the limitation in computing power and root-mean-square error of restructured images. Based on these dictionaries, orthogonal matching pursuit method was utilized to calculate the sparse coefficients of multispectral and panchromatic images, separately, while nonnegative matrix factorization fusion algorithm was utilized to calculate multispectral and panchromatic sparse coefficients to obtain sparse coefficient of the fusional image on all bands, with the resulted matrix having a size of 480 × 255 025. These united sparse PCA dictionaries and fusion sparse coefficients were then used to reconstruct the fusional image. Through the analysis of five quantitative indices of fusion assessment, the proposed fusion algorithm has retained the multispectral information of images and enhanced the detailed information in image texture.","1558-0571","","10.1109/LGRS.2018.2867006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454792","Linear unmixing;nonnegative matrix factorization (NMF) fusion;online dictionary learning (ODL) algorithm;orthogonal matching pursuit (OMP) algorithm;principal component analysis (PCA) dictionary","Dictionaries;Principal component analysis;Sparse matrices;Image reconstruction;Matching pursuit algorithms;Matrix decomposition;Machine learning","image fusion;image reconstruction;image texture;iterative methods;learning (artificial intelligence);matrix decomposition;mean square error methods;principal component analysis","surface objects;united sparse PCA dictionary;online dictionary learning algorithm;root-mean-square error;orthogonal matching pursuit method;image reconstruction;united sparse principal component analysis dictionary;image texture;fusional image;nonnegative matrix factorization fusion algorithm;panchromatic images;multispectral linear unmixing images","","","","22","IEEE","5 Sep 2018","","","IEEE","IEEE Journals"
"Hyperspectral and LiDAR Data Classification Using Kernel Collaborative Representation Based Residual Fusion","C. Ge; Q. Du; W. Li; Y. Li; W. Sun","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Jul 2019","2019","12","6","1963","1973","A new framework is proposed for the fusion of hyperspectral and light detection and ranging (LiDAR) data based on the extinction profiles (EPs), local binary pattern (LBP), and kernel collaborative representation classification. Specifically, EP and LBP features are extracted from both sources. Then, the derived features of each source are classified by collaborative representation-based classifier with Tikhonov regularization (KCRT). Reconstruction residuals are fused to produce the final label assignment. Experimental results demonstrate that the proposed method outperforms the existing methods in hyperspectral and LiDAR data fusion.","2151-1535","","10.1109/JSTARS.2019.2913206","National Key R&D Program of China(grant numbers:2018YFB0505000); China Scholarship Council(grant numbers:201706960055); National Natural Science Foundation of China(grant numbers:41671342,61871177,61571345,91538101,61501346,61502367,61701360); 111 project(grant numbers:B08038); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2016JQ6023); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR19D010001); State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing(grant numbers:18R05); Wuhan University(grant numbers:18R05); Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718389","Collaborative representation-based classification with Tikhonov regularization (KCRT);extinction profile (EP);hyperspectral;image fusion;light detection and ranging (LiDAR);local binary pattern (LBP)","Laser radar;Kernel;Feature extraction;Hyperspectral imaging;Collaboration","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image reconstruction;image representation;optical radar","residual fusion;hyperspectral detection;local binary pattern;kernel collaborative representation classification;collaborative representation-based classifier;reconstruction residuals;light detection and ranging data;LBP feature extraction;LiDAR data classification;extinction profiles;EP feature extraction;Tikhonov regularization;KCRT;label assignment;hyperspectral data fusion;LiDAR data fusion","","16","","41","IEEE","20 May 2019","","","IEEE","IEEE Journals"
"Deep Residual Network-Based Fusion Framework for Hyperspectral and LiDAR Data","C. Ge; Q. Du; W. Sun; K. Wang; J. Li; Y. Li","School of Information Science and Engineering, Shandong Normal University, Jinan, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, Shaanxi, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, Shaanxi, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, Shaanxi, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Feb 2021","2021","14","","2458","2472","This article presents a deep residual network-based fusion framework for hyperspectral and LiDAR data. In this framework, three new fusion methods are proposed, which are the residual network-based deep feature fusion (RNDFF), the residual network-based probability reconstruction fusion (RNPRF) and the residual network-based probability multiplication fusion (RNPMF). The three methods use extinction profile (EP), local binary pattern (LBP), and deep residual network. Specifically, EP and LBP features are extracted from two sources and stacked as spatial features. For RNDFF, the deep features of each source are extracted by a deep residual network, and then the deep features are stacked to create the fusion features which are classified by softmax classifier. For RNPRF, the deep features of each source are input to the softmax classifier to obtain the probability matrices, and then the probability matrices are fused by weighted addition to producing the final label assignment. For RNPMF, the probability matrices are fused by array multiplication. Experimental results demonstrate that the classification performance of the proposed methods significantly outperform existing methods in hyperspectral and LiDAR data fusion.","2151-1535","","10.1109/JSTARS.2021.3054392","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB0505000); National Natural Science Foundation of China(grant numbers:61901343,41971296,61871177,61571345,91538101,61501346,61502367,61701360); Higher Education Discipline Innovation Project(grant numbers:B08038); Natural Science Foundation of Shaanxi Province(grant numbers:2016JQ6023); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; Natural Science Foundation of Zhejiang Province(grant numbers:LR19D010001); Open Fund of State Laboratory of Information Engineering in Surveying; Mapping and Remote Sensing; Wuhan University(grant numbers:18R05); Science and Technology on Space Intelligent Control Laboratory(grant numbers:ZDSYS-2019-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336235","Deep residual network;extinction profile;Goddard 's LiDAR;hyperspectral;hyperspectral and thermal (G-LiHT) data;image fusion;local binary pattern (LBP);probability fusion;light detection and ranging (LiDAR)","Laser radar;Feature extraction;Hyperspectral imaging;Residual neural networks;Stacking;Data mining;Training","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image reconstruction;matrix multiplication;optical radar;probability","classification performance;array multiplication;label assignment;extinction profile;RNPMF;RNDFF;hyperspectral data fusion;RNPRF;LiDAR data fusion;probability matrices;softmax classifier;deep features;spatial features;LBP features;local binary pattern;residual network-based probability multiplication fusion;residual network-based probability reconstruction fusion;residual network-based deep feature fusion;fusion methods;deep residual network-based fusion framework","","12","","54","CCBY","26 Jan 2021","","","IEEE","IEEE Journals"
"Multilevel Spatial-Channel Feature Fusion Network for Urban Village Classification by Fusing Satellite and Streetview Images","R. Fan; J. Li; F. Li; W. Han; L. Wang","Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Faculty of Science and Technology, University of Macau, Macau, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","3 Oct 2022","2022","60","","1","13","Urban villages (UVs) refer to areas of urban informal settlements lagging behind the rapid urbanization process. Recent studies focus on using satellite images to classify UV. However, satellite images only capture objects from a bird-eye perspective, thus cannot obtain complex spatial relationships between objects. In UV areas, buildings and objects are usually dense, small in size, and obscure each other. Therefore, it is challenging to classify UV accurately using only satellite images with bird-eye perspectives. In this article, to solve this problem, we proposed a novel method that uses satellite images combined with streetview images to classify UV. Specifically, we propose a novel multilevel spatial-channel feature fusion network, namely FusionMixer, that integrates CNN-based feature extraction modules and a multilevel spatial-channel feature fusing layer to make an optimal UV classification. Experiments were conducted in Shenzhen City (the RsSt-ShenzhenUV dataset) and a public UV dataset (the  $S^{2}\text {UV}$  dataset). The proposed FusionMixer achieved an increase of overall accuracy (OA) by 8.83% and 8.84%, and improves Kappa by 0.1765 and 0.1770 in the validation set and testing set, compared to the second-best fusion models in RsSt-ShenzhenUV dataset. Experiments in the  $S^{2}\text {UV}$  dataset show that the proposed FusionMixer improves OA by 1.82% and Kappa by 0.04 compared to other methods. We also added a set of experiments on a public dataset (Houston dataset) and compare our method with the current state-of-the-art multimodal fusion methods to prove the generalization of the proposed FusionMixer in fusing other multimodality data. These experiments confirmed the superior performance of the proposed FusionMixer.","1558-0644","","10.1109/TGRS.2022.3208166","National Natural Science Foundation of China(grant numbers:41925007,U21A2013); Special Fund of Hubei Luojia Laboratory(grant numbers:220100035); Hubei Natural Science Foundation of China(grant numbers:2019CFA023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9895454","Deep learning;multimodal;remote sensing;streetview;urban village (UV)","Task analysis;Satellites;Feature extraction;Data models;Urban areas;Neural networks;Deep learning","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;town and country planning","RsSt-ShenzhenUV dataset;public UV dataset;FusionMixer;multimodal fusion;urban village classification;streetview images;urban informal settlements;satellite images;urbanization;bird-eye perspective;complex spatial relationships;UV areas;CNN-based feature extraction;multilevel spatial-channel feature fusion network;UV classification;Shenzhen City;S2 UV dataset","","","","59","IEEE","20 Sep 2022","","","IEEE","IEEE Journals"
"Extinction Profiles Fusion for Hyperspectral Images Classification","L. Fang; N. He; S. Li; P. Ghamisi; J. A. Benediktsson","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Technische Universität München, Signal Processing in Earth Observation, Münich, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavk, Iceland","IEEE Transactions on Geoscience and Remote Sensing","27 Feb 2018","2018","56","3","1803","1815","An extinction profile (EP) is an effective spatial-spectral feature extraction method for hyperspectral images (HSIs), which has recently drawn much attention. However, the existing methods utilize the EPs in a stacking way, which is hard to fully explore the information in EPs for HSI classification. In this paper, a novel fusion framework termed EPs-fusion (EPs-F) is proposed to exploit the information within and among EPs for HSI classification. In general, EPs-F includes the following two stages. In the first stage, by extracting the EPs from three independent components of an HSI, three complementary groups of EPs can be constructed. For each EP, an adaptive superpixel-based composite kernel strategy is proposed to explore the spatial information within an EP. The weights to create the composite kernel and the number of superpixels are automatically determined based on the spatial information of each EP. In the second stage, since the different EPs contain highly complementary information, a simple yet effective decision fusion method is further applied to obtain the final classification result. Experiments on three real HSI data sets verify the qualitative and quantitative superiority of the proposed EPs-F method over several state-of-the-art HSI classifiers.","1558-0644","","10.1109/TGRS.2017.2768479","National Natural Science Fund of China for Distinguished Young Scholars(grant numbers:61325007); National Natural Science Fund of China for International Cooperation and Exchanges(grant numbers:61520106001); National Natural Science Foundation(grant numbers:61771192); Fund of Hunan Province for Science and Technology Plan Project(grant numbers:2017RS3024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8118159","Classification;composite kernel;decision fusion;extinction profile (EP);hyperspectral image (HSI)","Kernel;Feature extraction;Support vector machines;Integrated circuits;Fuses;Hyperspectral imaging","feature extraction;hyperspectral imaging;image classification;image fusion","HSI classification;EPs-F method;spatial-spectral feature extraction;extinction profiles fusion;adaptive superpixel-based composite kernel strategy","","90","","47","IEEE","22 Nov 2017","","","IEEE","IEEE Journals"
"HOG-ShipCLSNet: A Novel Deep Learning Network With HOG Feature Fusion for SAR Ship Classification","T. Zhang; X. Zhang; X. Ke; C. Liu; X. Xu; X. Zhan; C. Wang; I. Ahmad; Y. Zhou; D. Pan; J. Li; H. Su; J. Shi; S. Wei","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; 3rd Graduate Student Team, Naval Aviation University, Yantai, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2022","2022","60","","1","22","Ship classification in synthetic aperture radar (SAR) images is a fundamental and significant step in ocean surveillance. Recently, with the rise of deep learning (DL), modern abstract features from convolutional neural networks (CNNs) have hugely improved SAR ship classification accuracy. However, most existing CNN-based SAR ship classifiers overly rely on abstract features, but uncritically abandon traditional mature hand-crafted features, which may incur some challenges for further improving accuracy. Hence, this article proposes a novel DL network with histogram of oriented gradient (HOG) feature fusion (HOG-ShipCLSNet) for preferable SAR ship classification. In HOG-ShipCLSNet, four mechanisms are proposed to ensure superior classification accuracy, that is, 1) a multiscale classification mechanism (MS-CLS-Mechanism); 2) a global self-attention mechanism (GS-ATT-Mechanism); 3) a fully connected balance mechanism (FC-BAL-Mechanism); and 4) an HOG feature fusion mechanism (HOG-FF-Mechanism). We perform sufficient ablation studies to confirm the effectiveness of these four mechanisms. Finally, our experimental results on two open SAR ship datasets (OpenSARShip and FUSAR-Ship) jointly reveal that HOG-ShipCLSNet dramatically outperforms both modern CNN-based methods and traditional hand-crafted feature methods.","1558-0644","","10.1109/TGRS.2021.3082759","National Natural Science Foundation of China(grant numbers:61571099,61501098,61671113); National Key Research and Development Program of China(grant numbers:2017YFB0502700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9445223","Convolutional neural networks (CNNs);deep learning (DL);feature fusion;histogram of oriented gradient (HOG);ship classification;synthetic aperture radar (SAR)","Marine vehicles;Synthetic aperture radar;Feature extraction;Radar polarimetry;Artificial intelligence;Support vector machines;Oceans","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image classification;image fusion;object detection;radar computing;radar imaging;ships;synthetic aperture radar","novel DL network;histogram of oriented gradient feature fusion;HOG-FF-Mechanism;HOG feature fusion mechanism;FC-BAL-Mechanism;GS-ATT-Mechanism;MS-CLS-Mechanism;multiscale classification mechanism;preferable SAR ship classification;traditional mature hand-crafted features;abstract features;existing CNN-based SAR ship classifiers;SAR ship classification accuracy;convolutional neural networks;modern features;ocean surveillance;synthetic aperture radar images;novel deep learning network;HOG-ShipCLSNet","","35","","132","IEEE","2 Jun 2021","","","IEEE","IEEE Journals"
"Can We Track Targets From Space? A Hybrid Kernel Correlation Filter Tracker for Satellite Video","J. Shao; B. Du; C. Wu; L. Zhang","School of Computer Science, National Engineering Research Center for Multimedia Software, Wuhan University, Wuhan, China; School of Computer Science, National Engineering Research Center for Multimedia Software, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, School of Computer, Wuhan University, Wuhan, China; School of Computer, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","30 Oct 2019","2019","57","11","8719","8731","Despite the great success of correlation filter-based trackers in visual tracking, it is questionable whether they can still perform on the satellite video data, acquired by a satellite or space station very high above the earth. The difficulty lies in that the targets usually occupy only a few pixels compared with the image size of over one million pixels and almost melt into the similar background. Since correlation filter models strongly depend on the quality of features and the spatial layout of the tracked object, they would probably fail on satellite video tracking tasks. In this paper, we propose a hybrid kernel correlation filter (HKCF) tracker employing two complementary features adaptively in a ridge regression framework. One feature is the optical flow that can detect variation pixels of the target. The other one is the histogram of oriented gradient that can capture the contour and texture information in the target, and an adaptive fusion strategy is proposed to employ the strengths of both features in different satellite videos. Quantitative evaluations are performed on six real satellite video data sets. The results show that our approach outperforms state-of-the-art tracking methods while running at more than 100 frames/s.","1558-0644","","10.1109/TGRS.2019.2922648","National Natural Science Foundation of China(grant numbers:41431175,61822113,41871243); Natural Science Foundation of Hubei Province(grant numbers:2018CFA050); National Key Research and Development Program of China(grant numbers:2018YFA0605501); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789388","Histogram of oriented gradient (HOG);kernelized correlation filter (KCF);optical flow (OF);ridge regression;satellite video tracking","Target tracking;Satellites;Correlation;Feature extraction;Streaming media;Kernel","correlation methods;geophysical image processing;image filtering;image fusion;image sequences;object tracking;regression analysis;target tracking;video signal processing","correlation filter models;satellite video tracking tasks;hybrid kernel correlation filter tracker;ridge regression framework;variation pixels;satellite videos;satellite video data sets;target tracking;correlation filter-based trackers;visual tracking;optical flow;histogram of oriented gradient;adaptive fusion strategy","","33","","51","IEEE","6 Aug 2019","","","IEEE","IEEE Journals"
"Pansharpening: Context-Based Generalized Laplacian Pyramids by Robust Regression","G. Vivone; S. Marano; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; LJK, CNRS, Grenoble INP, Inria, Université Grenoble Alpes, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2020","2020","58","9","6152","6167","Pansharpening refers to the combination of panchromatic (PAN) and multispectral (MS) images, designed to obtain a fused product retaining the fine spatial resolution of the former and the high spectral content of the latter. One of the most popular and successful approaches to pansharpening is the method known as context-based generalized Laplacian pyramid, which requires as a key ingredient for the estimation of the so-called injection coefficients. In this article, we propose the adoption of robust techniques for the estimation of the injection coefficients and detection strategies to select the clusters for which robust regression is needed, providing a suitable balancing between fusion performance and computational burden. Experimental results conducted on five real data sets acquired by the sensors QuickBird, WorldView-3, and WorldView-4, show the superiority of the proposed method with respect to current state-of-the-art pansharpening techniques.","1558-0644","","10.1109/TGRS.2020.2974806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9024138","Clustering;image fusion;multiresolution analysis (MRA);pansharpening;remote sensing;robust regression","Robustness;Spatial resolution;Estimation;Multiresolution analysis;Discrete wavelet transforms","geophysical image processing;geophysical techniques;image resolution","PAN;fused product;fine spatial resolution;high spectral content;popular approaches;context-based generalized Laplacian pyramid;injection coefficients;current state-of-the-art pansharpening techniques;robust regression;robust techniques","","37","","58","IEEE","4 Mar 2020","","","IEEE","IEEE Journals"
"Hyperspectral Sharpening Approaches Using Satellite Multiplatform Data","R. Restaino; G. Vivone; P. Addesso; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Institute of Methodologies for Environmental Analysis, CNR-IMAA, Tito Scalo, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Université Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","578","596","The use of hyperspectral (HS) data is growing over the years, thanks to the very high spectral resolution. However, HS data are still characterized by a spatial resolution that is too low for several applications, thus motivating the design of fusion techniques aimed to sharpen HS images with high spatial resolution data. To reach a significant resolution enhancement, high-resolution images should be acquired by different satellite platforms. In this article, we highlight the pros and cons of employing real multiplatform data, using the EO-1 satellite as an exemplary case. The spatial resolution of the HS data collected by the Hyperion sensor is improved by exploiting both the ALI panchromatic image collected from the same platform and acquisitions from the WorldView-3 and the QuickBird satellites. Furthermore, we tackle the problem of assessing the final quality of the fused product at the nominal resolution, which presents further difficulties in this general environment. Useful indications for the design of an effective sharpening method in this case are finally outlined.","1558-0644","","10.1109/TGRS.2020.3000267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119819","Hyperion data;hyperspectral (HS) sharpening;image fusion;pansharpening;remote sensing","Spatial resolution;Sensors;Satellites;Image sensors;Hyperspectral sensors","geophysical techniques","hyperspectral sharpening approaches;satellite multiplatform data;hyperspectral data;high spectral resolution;HS data;fusion techniques;HS images;high spatial resolution data;resolution enhancement;high-resolution images;satellite platforms;EO-1 satellite;ALI panchromatic image;QuickBird satellites;nominal resolution;effective sharpening method;Hyperion sensor;WorldView-3 satellite;QuickBird satellite","","5","","84","IEEE","17 Jun 2020","","","IEEE","IEEE Journals"
"Multifeature Transformation and Fusion-Based Ship Detection With Small Targets and Complex Backgrounds","M. Zha; W. Qian; W. Yang; Y. Xu","School of Software, Jiangxi Agricultural University, Nanchang, China; School of Software, Jiangxi Agricultural University, Nanchang, China; School of Software, Jiangxi Agricultural University, Nanchang, China; School of Software, Jiangxi Agricultural University, Nanchang, China","IEEE Geoscience and Remote Sensing Letters","29 Jul 2022","2022","19","","1","5","With the development of deep learning, synthetic aperture radar (SAR) image ship detection based on the convolutional neural network has made significant progress. However, there are two problems: 1) the false alarm detection rate is high due to complex background and coherent speckle noise interference and 2) for smaller ship targets, missed detection is prone to occur. In this letter, a novel ship detection model based on multifeature transformation and fusion (MFTF-Net) is proposed to address the issues. First, to avoid the randomness of initial point selection and the influence of outlier points, the anchor frame clustering approach based on the  $K$ -medians++ algorithm is presented to cluster the object candidate frames. Second, the low-level feature information is passed to the high level by constructing a local enhancement network; then, an improved transformer structure is introduced to replace the last convolutional block of the backbone network to obtain rich contextual information. Finally, a four-scale residual feature fusion network is designed, which fully fuses the object’s detailed and semantic information. In addition, improved convolutional block attention module (CBAM) and squeeze and excitation (SE) attention mechanisms are applied in the lower two layers and upper two layers of the network output to reduce the interference of confusing information, respectively. The experimental results demonstrate that the proposed method is superior to the state-of-the-art 13 baseline models on SAR ship detection dataset (SSDD), high-resolution SAR images dataset (HRSID), and SAR-ship-dataset public datasets in terms of the mean average precision (mAP), recall, accuracy, and  $F1$  metrics.","1558-0571","","10.1109/LGRS.2022.3192559","National Key Research and Development Program of China(grant numbers:2020YFD1100605); National Natural Science Foundation of China(grant numbers:61966016,62166020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833507","Feature fusion;multifeature;ship detection;synthetic aperture radar (SAR) image;transformer","Feature extraction;Marine vehicles;Clustering algorithms;Transformers;Training;Interference;Synthetic aperture radar","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image enhancement;image fusion;image resolution;object detection;radar imaging;ships;synthetic aperture radar","network output;convolutional block attention module;four-scale residual feature fusion network;rich contextual information;backbone network;transformer structure;local enhancement network;low-level feature information;object candidate frames;anchor frame;outlier points;initial point selection;multifeature transformation;ship detection model;missed detection;smaller ship targets;coherent speckle noise interference;false alarm detection rate;convolutional neural network;synthetic aperture radar image ship detection;deep learning;complex background;SAR-ship-dataset public datasets","","1","","29","IEEE","20 Jul 2022","","","IEEE","IEEE Journals"
"Pan-Sharpening Based on Transformer With Redundancy Reduction","K. Zhang; Z. Li; F. Zhang; W. Wan; J. Sun","School of Information Science and Engineering, Shandong Normal University, Ji’nan, China; School of Information Science and Engineering, Shandong Normal University, Ji’nan, China; School of Information Science and Engineering, Shandong Normal University, Ji’nan, China; School of Information Science and Engineering, Shandong Normal University, Ji’nan, China; School of Information Science and Engineering, Shandong Normal University, Ji’nan, China","IEEE Geoscience and Remote Sensing Letters","12 Jul 2022","2022","19","","1","5","Pan-sharpening methods based on deep neural network (DNN) have produced the state-of-the-art results. However, the common information in the panchromatic (PAN) image and the low spatial resolution multispectral (LRMS) image is not sufficiently explored. As PAN and LRMS images are collected from the same scene, there exists some common information among them, in addition to their respective unique information. The direct concatenation of extracted features leads to some redundancy in the feature space. To reduce the redundancy among features and exploit the global information in source images, we proposed a novel pan-sharpening method by combining the convolution neural network and transformer. Specifically, PAN and LRMS images are encoded as unique features and common features by the subnetworks consisting of convolution blocks and transformer blocks. Then, the common features are averaged and combined with unique features from source images for the reconstruction of the fused image. To extract accurate common features, the equality constraint is imposed on them. Experimental results show that the proposed method outperforms the state-of-the-art methods on both reduced-scale and full-scale datasets. The source code is available at https://github.com/RSMagneto/TRRNet.","1558-0571","","10.1109/LGRS.2022.3186985","Natural Science Foundation of China(grant numbers:61901246); China Postdoctoral Science Foundation Grant(grant numbers:2019TQ0190,2019M662432); Scientific Research Leader Studio of Ji’nan(grant numbers:2021GXRC081); Ningxia Natural Science Foundation(grant numbers:2021AAC03045); Joint Project for Smart Computing of Shandong Natural Science Foundation(grant numbers:ZR2020LZH015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810289","Common information;pan-sharpening;redundancy reduction;transformer;unique information","Transformers;Feature extraction;Convolution;Spatial resolution;Redundancy;Image reconstruction;Termination of employment","feature extraction;geophysical image processing;image fusion;image resolution;neural nets","feature space;global information;source images;pan-sharpening method;convolution neural network;LRMS images;transformer blocks;fused image;accurate common features;redundancy reduction;deep neural network;common information;panchromatic image;low spatial resolution multispectral image;respective unique information","","","","17","IEEE","29 Jun 2022","","","IEEE","IEEE Journals"
"Seismic Random Noise Suppression Based on MIRNet With Dense Feature Fusion","Z. Zhang; G. Li; L. Wang","College of Physical and Electronic Engineering, Shanxi University, Taiyuan, China; College of Physical and Electronic Engineering, Shanxi University, Taiyuan, China; College of Physical and Electronic Engineering, Shanxi University, Taiyuan, China","IEEE Geoscience and Remote Sensing Letters","9 Jun 2022","2022","19","","1","5","Random noise suppression is crucial for seismic signal processing. In recent years, machine learning has developed rapidly and attracted broad attention. MIRNet has achieved great success in image denoising, superresolution, and image enhancement. It utilizes information interactions between parallel multiresolution convolutional streams to combine low-resolution contextual information while still retaining high-resolution spatial details to learn rich features. However, as the depth of the network increases, MIRNet would gradually lead to the loss of shallow information, which is not fully utilized. In this letter, an MIRNet with dense feature fusion (DFF-MIRNet) is proposed. Dense connection based on an attention mechanism is used, so that the network can adaptively learn more effective features from previous and current local features, while a global feature fusion mechanism is introduced, so that the network can learn global-level features holistically and adaptively. DFF-MIRNet can fully combine the low-level, mid-level, and high-level features for training and avoid the loss of shallow feature information. Through the compared experiments, it is shown that DFF-MIRNet achieves better performance than time–frequency peak filtering (TFPF),  $f{\text {-}}k$  filtering, and MIRNet.","1558-0571","","10.1109/LGRS.2022.3177231","Natural Science Foundation of Shanxi Province(grant numbers:202103021224012); Higher Education Innovation Foundation of Shanxi Province(grant numbers:2019L0053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780124","Attention mechanism;dense feature fusion;MIRNet;random noise suppression;seismic exploration","Feature extraction;Noise reduction;Convolution;Training;Noise measurement;Filtering;Data mining","feature extraction;filtering theory;geophysical signal processing;geophysical techniques;image denoising;image enhancement;image fusion;image resolution;learning (artificial intelligence);random noise;seismology;time-frequency analysis","seismic random noise suppression;seismic signal processing;machine learning;broad attention;image denoising;image enhancement;information interactions;parallel multiresolution convolutional streams;low-resolution contextual information;high-resolution spatial details;rich features;network increases;shallow information;dense feature fusion;DFF-MIRNet;dense connection;attention mechanism;previous features;current local features;global feature fusion mechanism;global-level features;high-level features;shallow feature information","","","","14","IEEE","23 May 2022","","","IEEE","IEEE Journals"
"Fusion of Satellite Images and Weather Data With Transformer Networks for Downy Mildew Disease Detection","W. Maillet; M. Ouhami; A. Hafiane","PRISME Laboratory, INSA CVL, University of Orléans, Bourges, France; PRISME Laboratory, INSA CVL, University of Orléans, Bourges, France; PRISME Laboratory, INSA CVL, University of Orléans, Bourges, France","IEEE Access","20 Jan 2023","2023","11","","5406","5416","Crop diseases significantly affect the quantity and quality of agricultural production. In a context where the goal of precision agriculture is to minimize or even avoid the use of pesticides, weather and remote sensing data with deep learning can play a pivotal role in detecting crop diseases, allowing localized treatment of crops. However, combining heterogeneous data such as weather and images remains a hot topic and challenging task. Recent developments in transformer architectures have shown the possibility of fusion of data from different domains, such as text-image. The current trend is to custom only one transformer to create a multimodal fusion model. Conversely, we propose a new approach to realize data fusion using three transformers. In this paper, we first solved the missing satellite images problem, by interpolating them with a ConvLSTM model. Then, we proposed a multimodal fusion architecture that jointly learns to process visual and weather information. The architecture is built from three main components, a Vision Transformer and two transformer-encoders, allowing to fuse both image and weather modalities. The results of the proposed method are promising achieving an overall accuracy of 97%.","2169-3536","","10.1109/ACCESS.2023.3237082","European Consortium ERA-NET ICT-AGRI-FOOD; French National Research Agency (ANR)(grant numbers:ANR-21-ICAF-0002-01); European Project MERIAVINO; ICT-AGRIFOOD ERA-NET; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10017285","Remote sensing;image processing;deep learning;data fusion;vegetation indices;crop monitoring;agriculture","Transformers;Meteorology;Satellites;Feature extraction;Diseases;Crops;Computer architecture;Vegetation mapping;Indexes;Deep learning;Remote sensing","deep learning (artificial intelligence);diseases;geophysical image processing;image fusion;interpolation;recurrent neural nets","agricultural production;ConvLSTM model;crop diseases;deep learning;downy mildew disease detection;missing satellite image problem;multimodal data fusion model;multimodal fusion architecture;remote sensing data;text-imaging;transformer-encoder network architectures;vision transformer networks;weather data modalities","","","","49","CCBY","16 Jan 2023","","","IEEE","IEEE Journals"
"Deep Learning for Multiple-Image Super-Resolution","M. Kawulok; P. Benecki; S. Piechaczek; K. Hrynczenko; D. Kostrzewa; J. Nalepa","Faculty of Automatic Control, Electronics and Computer Science, Silesian University of Technology, Gliwice, Poland; Faculty of Automatic Control, Electronics and Computer Science, Silesian University of Technology, Gliwice, Poland; Faculty of Automatic Control, Electronics and Computer Science, Silesian University of Technology, Gliwice, Poland; Faculty of Automatic Control, Electronics and Computer Science, Silesian University of Technology, Gliwice, Poland; Faculty of Automatic Control, Electronics and Computer Science, Silesian University of Technology, Gliwice, Poland; Faculty of Automatic Control, Electronics and Computer Science, Silesian University of Technology, Gliwice, Poland","IEEE Geoscience and Remote Sensing Letters","21 May 2020","2020","17","6","1062","1066","Super-resolution (SR) reconstruction is a process aimed at enhancing the spatial resolution of images, either from a single observation, based on the learned relation between low and high resolution, or from multiple images presenting the same scene. SR is particularly important, if it is not feasible to acquire images at the desired resolution, while there are single or many observations available at lower resolution - this is inherent to a variety of remote sensing scenarios. Recently, we have witnessed substantial improvement in single-image SR attributed to the use of deep neural networks for learning the relation between low and high resolution. Importantly, deep learning has not been widely exploited for multiple-image super-resolution, which benefits from information fusion and in general allows for achieving higher reconstruction accuracy. In this letter, we introduce a new approach to combine the advantages of multiple-image fusion with learning the low-to-high resolution mapping using deep networks. The results of our extensive experiments indicate that the proposed framework outperforms the state-of-the-art SR methods.","1558-0571","","10.1109/LGRS.2019.2940483","European Space Agency (SuperDeep Project, realized by Future Processing); Statutory Research Funds of Institute of Informatics, Silesian University of Technology, Poland(grant numbers:02/020/BK_18/0128); Silesian University of Technology, Poland(grant numbers:BKM-556/RAU2/2018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8884136","Convolutional neural networks (CNNs);deep learning;image processing;super resolution (SR)","Image reconstruction;Spatial resolution;Satellites;Deep learning;Imaging;Gallium nitride","image fusion;image reconstruction;image resolution;learning (artificial intelligence);neural nets","single-image SR;deep neural networks;deep learning;multiple-image super-resolution;multiple-image fusion;low-to-high resolution mapping;super-resolution reconstruction;spatial resolution;single observation;information fusion","","44","","22","IEEE","28 Oct 2019","","","IEEE","IEEE Journals"
"Deep Joint Estimation Network for Satellite Video Super-Resolution With Multiple Degradations","H. Liu; Y. Gu","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","19 Apr 2022","2022","60","","1","15","Super-resolution (SR) for satellite video data has been a hot research topic in the field of remote sensing video analysis. The existing satellite video SR methods assume that the blur kernel in the imaging degradation model is known. However, the blur kernel in real satellite videos is usually unknown, which inevitably results in poor performance when the true blur kernel is not consistent with a predefined blur kernel. To address this issue, this article proposes a deep joint estimation network for satellite video SR (JENSVSR), which jointly estimates blur kernels and SR frames. Specifically, JENSVSR is composed of a video SR subnetwork and a blur kernel estimation subnetwork. On one hand, the video SR subnetwork makes use of multiple video frames to generate super-resolved satellite frames. To effectively fuse information from adjacent frames, an alignment and fusion module is proposed in the feature space. On the other hand, the blur estimation subnetwork is also proposed to predict blur kernels. The two subnetworks are coupled by cross-task feature fusion modules (CTFFMs) to achieve joint estimation rather than two-step independent estimation. The performance of our proposed method is evaluated on synthetic and real satellite videos. The experimental results show that our proposed method is superior to the current state-of-the-art SR methods.","1558-0644","","10.1109/TGRS.2022.3163790","National Natural Science Foundation of Key International Cooperation(grant numbers:61720106002); National Science Fund for Excellent Young Scholars(grant numbers:61522107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745539","Blur kernel estimation;multiple degradations;satellite video;super-resolution (SR)","Kernel;Satellites;Estimation;Degradation;Image reconstruction;Spatial resolution;Convolution","image fusion;image reconstruction;image resolution;image restoration;video signal processing","deep joint estimation network;satellite video super-resolution;satellite video data;remote sensing video analysis;existing satellite video SR methods;satellite videos;predefined blur kernel;SR frames;video SR subnetwork;blur kernel estimation subnetwork;multiple video frames;super-resolved satellite frames;blur estimation subnetwork;current state-of-the-art SR methods","","","","57","IEEE","31 Mar 2022","","","IEEE","IEEE Journals"
"Uncertainty Estimation of Local-Scale Land Surface Temperature Products Over Urban Areas Using Monte Carlo Simulations","Z. Mitraka; G. Doxani; F. Del Frate; N. Chrysoulakis","University of Rome Tor Vergata, Foundation for Research and Technology Hellas, Heraklion, Greece; European Space Agency, Frascati, Italy; University of Rome Tor Vergata, Rome, Italy; Foundation for Research and Technology Hellas, Heraklion, Greece","IEEE Geoscience and Remote Sensing Letters","10 Jun 2016","2016","13","7","917","921","Detailed, frequent, and accurate land surface temperature (LST) estimates from satellites may support various applications related to the urban climate. When satellite-retrieved LST is used in modeling, the level of uncertainty is important to account for. In this letter, an uncertainty estimation scheme based on Monte Carlo simulations is proposed for local-scale LST products derived from image fusion. The downscaling algorithm combines frequent low-resolution thermal measurements with surface cover information from high spatial resolution imagery. The uncertainty is estimated for all the intermediate products, allowing the analysis of individual uncertainties and their contribution to the final LST product. Uncertainties of less than 2 K was found for most part of the test area. The uncertainty estimation method, although demanding in terms of computations, can be useful for the uncertainty analysis of other satellite products.","1558-0571","","10.1109/LGRS.2016.2553367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7463541","Land surface temperature (LST);Landsat;Moderate Resolution Imaging Spectroradiometer (MODIS);Monte Carlo methods;uncertainty;urban areas;Land surface temperature (LST);Landsat;Moderate Resolution Imaging Spectroradiometer (MODIS);Monte Carlo methods;uncertainty;urban areas","Uncertainty;Estimation;Land surface temperature;Monte Carlo methods;MODIS;Spatial resolution;Satellites","land surface temperature;Monte Carlo methods","local-scale land surface temperature;urban areas;Monte Carlo simulations;satellite-retrieved LST;image fusion;high spatial resolution imagery;uncertainty analysis","","5","","20","IEEE","2 May 2016","","","IEEE","IEEE Journals"
"A Variational Pansharpening Approach Based on Reproducible Kernel Hilbert Space and Heaviside Function","L. -J. Deng; G. Vivone; W. Guo; M. Dalla Mura; J. Chanussot","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Department of Information Engineering Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Case Western Reserve University, Cleveland, OH, USA; Grenoble Images Speech Signals and Automatics Laboratory, Grenoble Institute of Technology, Institute of Engineering, University Grenoble Alpes, CNRS, Grenoble, France; Grenoble Images Speech Signals and Automatics Laboratory, Grenoble Institute of Technology, Institute of Engineering, University Grenoble Alpes, CNRS, Grenoble, France","IEEE Transactions on Image Processing","1 Jun 2018","2018","27","9","4330","4344","Pansharpening is an important application in remote sensing image processing. It can increase the spatial-resolution of a multispectral image by fusing it with a high spatial-resolution panchromatic image in the same scene, which brings great favor for subsequent processing such as recognition, detection, etc. In this paper, we propose a continuous modeling and sparse optimization based method for the fusion of a panchromatic image and a multispectral image. The proposed model is mainly based on reproducing kernel Hilbert space (RKHS) and approximated Heaviside function (AHF). In addition, we also propose a Toeplitz sparse term for representing the correlation of adjacent bands. The model is convex and solved by the alternating direction method of multipliers which guarantees the convergence of the proposed method. Extensive experiments on many real datasets collected by different sensors demonstrate the effectiveness of the proposed technique as compared with several state-of-the-art pansharpening approaches.","1941-0042","","10.1109/TIP.2018.2839531","NSFC(grant numbers:61702083,61772003); Fundamental Research Funds for the Central Universities(grant numbers:ZYGX2016KYQD142); U.S. NIH(grant numbers:1R21EB016535-01); NSF(grant numbers:DMS-1521582); CNRS(grant numbers:PICS 263484); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8361903","Pansharpening;remote sensing image;sparse model;RKHS;Heaviside;Toeplitz sparsity;alternating direction method of multipliers","Spatial resolution;Image edge detection;Sensors;Two dimensional displays;Electronic mail;Multiresolution analysis","geophysical image processing;Hilbert spaces;image fusion;image resolution;optimisation;remote sensing;variational techniques","approximated Heaviside function;reproducible kernel Hilbert space;variational pansharpening approach;state-of-the-art pansharpening approaches;Toeplitz sparse term;sparse optimization based method;spatial-resolution panchromatic image;multispectral image;remote sensing image processing","","53","","71","IEEE","21 May 2018","","","IEEE","IEEE Journals"
"HPGAN: Hyperspectral Pansharpening Using 3-D Generative Adversarial Networks","W. Xie; Y. Cui; Y. Li; J. Lei; Q. Du; J. Li","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","463","477","Hyperspectral (HS) pansharpening, as a special case of the superresolution (SR) problem, is to obtain a high-resolution (HR) image from the fusion of an HR panchromatic (PAN) image and a low-resolution (LR) HS image. Though HS pansharpening based on deep learning has gained rapid development in recent years, it is still a challenging task because of the following requirements: 1) a unique model with the goal of fusing two images with different dimensions should enhance spatial resolution while preserving spectral information; 2) all the parameters should be adaptively trained without manual adjustment; and 3) a model with good generalization should overcome the sensitivity to different sensor data in reasonable computational complexity. To meet such requirements, we propose a unique HS pansharpening framework based on a 3-D generative adversarial network (HPGAN) in this article. The HPGAN induces the 3-D spectral-spatial generator network to reconstruct the HR HS image from the newly constructed 3-D PAN cube and the LR HS image. It searches for an optimal HR HS image by successive adversarial learning to fool the introduced PAN discriminator network. The loss function is specifically designed to comprehensively consider global constraint, spectral constraint, and spatial constraint. Besides, the proposed 3-D training in the high-frequency domain reduces the sensitivity to different sensor data and extends the generalization of HPGAN. Experimental results on data sets captured by different sensors illustrate that the proposed method can successfully enhance spatial resolution and preserve spectral information.","1558-0644","","10.1109/TGRS.2020.2994238","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Young Talent fund of University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097446","Generative adversarial networks;global constraint;hyperspectral pansharpening;spectral–spatial constraints;3-D high-frequency block","Generators;Generative adversarial networks;Spatial resolution;Gallium nitride;Bayes methods;Data models","computational complexity;deep learning (artificial intelligence);image fusion;image resolution;spectral analysis","spectral constraint;spatial constraint;sensor data;HPGAN;spatial resolution;spectral information;hyperspectral pansharpening;3D generative adversarial network;superresolution problem;high-resolution image;HR panchromatic image;low-resolution HS image;deep learning;unique HS pansharpening framework;LR HS image;optimal HR HS image;3D PAN cube;3D spectral-spatial generator network;3D training;PAN discriminator network;successive adversarial learning;image fusion;computational complexity","","32","","36","IEEE","20 May 2020","","","IEEE","IEEE Journals"
"An Iterative Regularization Method Based on Tensor Subspace Representation for Hyperspectral Image Super-Resolution","T. Xu; T. -Z. Huang; L. -J. Deng; N. Yokoya","Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan","IEEE Transactions on Geoscience and Remote Sensing","3 Jun 2022","2022","60","","1","16","Hyperspectral image super-resolution (HSI-SR) can be achieved by fusing a paired multispectral image (MSI) and hyperspectral image (HSI), which is a prevalent strategy. But, how to precisely reconstruct the high spatial resolution hyperspectral image (HR-HSI) by fusion technology is a challenging issue. In this article, we propose an iterative regularization method based on tensor subspace representation (IR-TenSR) for MSI-HSI fusion, thus HSI-SR. First, we propose a tensor subspace representation (TenSR)-based regularization model that integrates the global spectral–spatial low-rank and the nonlocal self-similarity priors of HR-HSI. These two priors have been proven effective, but previous HSI-SR works cannot simultaneously exploit them. Subsequently, we design an iterative regularization procedure to utilize the residual information of acquired low-resolution images, which are ignored in other works that produce suboptimal results. Finally, we develop an effective algorithm based on the proximal alternating minimization method to solve the TenSR-regularization model. With that, we obtain the iterative regularization algorithm. Experiments implemented on the simulated and real datasets illustrate the advantages of the proposed IR-TenSR compared with the state-of-the-art fusion approaches. The code is available at https://github.com/liangjiandeng/IR_TenSR.","1558-0644","","10.1109/TGRS.2022.3176266","National Natural Science Foundation of China(grant numbers:12171072,61702083); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216); National Key Research and Development Program of China(grant numbers:2020YFA0714001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777947","Global spatial–spectral low-rank prior;hyperspectral image super-resolution;iterative regularization;nonlocal self-similarity;proximal alternating minimization;tensor subspace","Tensors;Correlation;Spatial resolution;Hyperspectral imaging;Task analysis;Superresolution;Iterative algorithms","geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image representation;image resolution;iterative methods;minimisation;tensors","hyperspectral image super-resolution;high spatial resolution hyperspectral image;HR-HSI;iterative regularization method;IR-TenSR;MSI-HSI fusion;tensor subspace representation-based regularization model;global spectral-spatial low-rank;previous HSI-SR works;iterative regularization procedure;acquired low-resolution images;proximal alternating minimization method;TenSR-regularization model;iterative regularization algorithm;paired multispectral image;fusion technology","","2","","74","IEEE","18 May 2022","","","IEEE","IEEE Journals"
"Deep Hyperspectral and Multispectral Image Fusion via Probabilistic Matrix Factorization","B. Lin; Y. Guo","School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, China; School of Electronics and Communication Engineering, Sun Yat-sen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","24 Feb 2023","2023","61","","1","14","Deep learning methods are popular for hyperspectral and multispectral image (HSI-MSI) fusion to obtain a high-resolution HSI. However, most of them are unsatisfactory due to limited generalization ability and poor interpretability. This article proposes a highly interpretable deep HSI-MSI fusion method based on probabilistic matrix factorization (PMF) under the Bayesian framework. In the proposed method, an HSI is factorized into two matrices, namely, the Gaussian-prior-regularized spectral matrix and the deep-prior-regularized abundance matrix. Then, we split the optimization process into two meaningful iterative updating steps: updating the spectral matrix based on least-squares estimation and updating the abundance matrix based on a convolutional neural network (CNN)-based Gaussian denoiser for 2-D gray images. To improve the generalization ability, we provide solutions for selections of hyperparameters, CNN-based denoiser architecture, and training strategy. Using the given solutions, the proposed fusion method can be trained with 2-D images once and then used to fuse different types of HSI and MSI excellently. Experiments on three datasets demonstrate that the proposed fusion method has good fusion performance and high generalization ability compared with other state-of-the-art methods. The source code will be available at https://github.com/KevinBHLin/.","1558-0644","","10.1109/TGRS.2023.3244992","Natural Science Foundation of Guangdong Province (China)(grant numbers:2022A1515010493); Fundamental Research Funds for the Central Universities (Sun Yat-sen University)(grant numbers:2021qntd11); Science Technology and Innovation Commission of Shenzhen Municipality(grant numbers:2021A03); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2022B1515020103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10044135","Convolutional neural network (CNN)-based Gaussian denoiser;deep prior regularization (DPR);hyperspectral image (HSI) fusion;inertial Proximal Alternating Linearized Minimization (iPALM);probabilistic matrix factorization (PMF)","Bayes methods;Training;Spatial resolution;Optimization;Deep learning;Imaging;Probabilistic logic","","","","","","41","IEEE","14 Feb 2023","","","IEEE","IEEE Journals"
"A Latent Feature-Based Multimodality Fusion Method for Theme Classification on Web Map Service","Z. Yang; Z. Gui; H. Wu; W. Li","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Geographical Sciences and Urban Planning, Arizona State University, Tempe, USA","IEEE Access","7 Feb 2020","2020","8","","25299","25309","Massive maps have been shared as Web Map Service (WMS) from various providers, which could be used to facilitate people's daily lives and support space analysis and management. The theme classification of maps could help users efficiently find maps and support theme-related applications. Traditionally, metadata is usually used in analyzing maps content, few papers use maps, especially legends. In fact, people usually considers metadata, maps and legends together to understand what maps tell, however, no study has tried to exploit how to combine them. This paper proposes a method to fuse them with the purpose of classifying map themes, named latent feature based multimodality fusion for theme classification (LFMF-TC). Firstly, a multimodal dataset is created that supports the supervised classification on map themes. Secondly, textual and visual features are designed for metadata, maps, and legends using some advanced techniques. Thirdly, a latent feature based fusion method is proposed to fuse the multimodal features on the feature level. Finally, a neural network classifier is implemented using supervised learning on the multimodal dataset. In addition, a web-based collaboration platform is developed to facilitate users in labeling multimodal samples through an interactive Graphical User Interface (GUI). Extensive experiments are designed and implemented, whose results prove that LFMF-TC could significantly improve the classification accuracy. In theory, the LFMF-TC could be used for other applications with few modifications.","2169-3536","","10.1109/ACCESS.2019.2954851","National Natural Science Foundation of China(grant numbers:41930107,41971349); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908799","Cartography;machine learning;multimodality fusion;theme classification;web map service","Metadata;Feature extraction;Visualization;Fuses;Image color analysis;Geospatial analysis;Semantics","cartography;feature extraction;graphical user interfaces;groupware;image classification;image fusion;Internet;learning (artificial intelligence);meta data;pattern classification","GUI;graphical user interface;classification accuracy;labeling multimodal samples;web-based collaboration platform;multimodal features;latent feature based fusion method;visual features;textual features;supervised classification;multimodal dataset;LFMF-TC;latent feature;maps content;metadata;theme-related applications;Web map service;theme classification;latent feature-based multimodality fusion method","","4","","30","CCBY","21 Nov 2019","","","IEEE","IEEE Journals"
"Unsupervised Balanced Hash Codes Learning With Multichannel Feature Fusion","Y. Chen; D. Zhao; X. Lu; S. Xiong; H. Wang","Wuhan University of Technology Chongqing Research Institute, Chongqing, China; Wuhan University of Technology Chongqing Research Institute, Chongqing, China; Wuhan University of Technology Chongqing Research Institute, Chongqing, China; Wuhan University of Technology Chongqing Research Institute, Chongqing, China; Key Laboratory of Spectral Imaging Technology CAS, Xian Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Apr 2022","2022","15","","2816","2825","Unsupervised hashingalgorithms are widely used in large-scale remote sensing images (RSIs) retrieval task. However, existing RSI retrieval algorithms fail to capture the multichannel characteristic of multispectral RSIs and the balanced property of hash codes, which lead the poor performance of RSI retrieval. To tackle these issues, we develop an unsupervised hashing algorithm, namely, variational autoencoder balanced hashing (VABH), to leverage multichannel feature fusion and multiscale context information to perform RSI retrieval task. First, multichannel feature fusion module is designed to extract RSI feature information by leveraging the multichannel properties of multispectral RSI. Second, multiscale learning module is developed to learn the multiscale context information of RSIs. Finally, a novel objective function is designed to capture the discrimination and balanced property of hash codes in the hashing learning process. Comprehensive experiments on diverse benchmark have well demonstrated the reasonableness and effectiveness of the proposed VABH algorithm.","2151-1535","","10.1109/JSTARS.2022.3162251","National Natural Science Foundation of China(grant numbers:62176194,62101393); Major project of IoV(grant numbers:2020AAA001); Sanya Science and Education Innovation Park of Wuhan University of Technology(grant numbers:2021KF0031); Fundamental Research Funds for the Central Universities(grant numbers:WUT:2019III051GX,WUT:2021III054JC,WUT:215210002,WUT: 213110001,WUT:223110001); National Natural Science Foundation of China(grant numbers:cstc2021jcyj-msxmX1148); Open Project of Wuhan University of Technology Chongqing Research Institute(grant numbers:ZL2021-6); MindSpore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9743294","Deep hash codes;multichannel feature fusion;multiscale context information;unsupervised hashing learning","Feature extraction;Codes;Data mining;Convolution;Linear programming;Approximation algorithms;Semantics","feature extraction;file organisation;geophysical image processing;image classification;image fusion;image retrieval;information retrieval;learning (artificial intelligence);remote sensing","multiscale context information;RSI retrieval task;fusion module;multichannel properties;multispectral RSI;multiscale learning module;balanced property;hashing learning process;VABH algorithm;unsupervised balanced hash codes learning;multichannel feature fusion;unsupervised hashingalgorithms;large-scale remote sensing images retrieval task;existing RSI retrieval algorithms;multichannel characteristic;unsupervised hashing algorithm;variational autoencoder balanced hashing;leverage multichannel","","2","","50","CCBY","25 Mar 2022","","","IEEE","IEEE Journals"
"DCT-Based Local Descriptor for Robust Matching and Feature Tracking in Wide Area Motion Imagery","K. Gao; H. Aliakbarpour; G. Seetharaman; K. Palaniappan","Department of EECS, CIVA Lab, University of Missouri, Columbia, MO, USA; Department of EECS, CIVA Lab, University of Missouri, Columbia, MO, USA; Advanced Computing Concepts, U.S. Naval Research Laboratory, Washington, DC, USA; Department of EECS, CIVA Lab, University of Missouri, Columbia, MO, USA","IEEE Geoscience and Remote Sensing Letters","21 Jul 2021","2021","18","8","1441","1445","We introduce a novel discrete cosine transform-based feature (DCTF) descriptor designed for both robustly matching features in aerial video and tracking features across wide-baseline oblique views in aerial wide area motion imagery (WAMI). Our DCTF descriptor preserves local structure more compactly in the frequency domain by utilizing the mathematical properties of the discrete cosine transform (DCT) and outperforms widely used the spatial-domain feature extraction methods, such as speeded up robust features (SURF) and scale-invariant feature transform (SIFT). The DCTF descriptor can be used in combination with other feature detectors, such as SURF and features from accelerated segment test (FAST), for which we provide experimental results. The performance of DCTF for image matching and feature tracking is evaluated on two city-scale aerial WAMI data sets (ABQ-215 and LA-351) and a synthetic aerial drone video data set digital imaging and remote sensing image generation (Rochester Institute of Technology (RIT)-DIRSIG). DCTF is a compact 120-D descriptor that is less than half the dimensionality of state-of-the-art deep learning-based approaches, such as SuperPoint, LF-Net, and DeepCompare, which requires no learning and is domain-independent. Despite its small size, the DCTF descriptor surprisingly produces the highest image matching accuracies ( F1 = 0.76 and ABQ-215), the longest maximum and average feature track lengths, and the lowest tracking error (0.3 pixel, LA-351) compared with both handcrafted and deep learning features.","1558-0571","","10.1109/LGRS.2020.3000762","U.S. Army Research Laboratory(grant numbers:W911NF-1820285); Army Research Office(grant numbers:DURIP W911NF-1910181); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9126851","3-D stereo;aerial video;convolutional neural network (CNN);DCT;feature descriptor;matching;point correspondences;spatial frequency","Feature extraction;Discrete cosine transforms;Tracking;Agriculture;Detectors;Deep learning","discrete cosine transforms;feature extraction;image fusion;image matching;image sensors;learning (artificial intelligence);object detection;object tracking;remote sensing;transforms","DCT-based local descriptor;robust matching;feature tracking;novel discrete cosine;feature descriptor;robustly matching features;aerial video;tracking features;wide-baseline oblique views;aerial wide area motion imagery;DCTF descriptor;local structure;frequency domain;spatial-domain feature extraction methods;robust features;SURF;feature detectors;city-scale aerial WAMI data sets;synthetic aerial drone video data;state-of-the-art deep learning-based approaches;highest image matching accuracies;longest maximum;average feature track lengths;lowest tracking error;handcrafted learning features;deep learning features","","3","","18","IEEE","26 Jun 2020","","","IEEE","IEEE Journals"
"Hyperspectral Pansharpening via Local Intensity Component and Local Injection Gain Estimation","W. Dong; Y. Yang; J. Qu; S. Xiao; Q. Du","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; Beijing Electronic Science and Technology Institute, Beijing, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Hyperspectral (HS) pansharpening is an attractive topic in the field of remote sensing, which has attracted the attention of many researchers. Component substitution (CS)-based HS pansharpening algorithms are of great interest due to their simplicity and high spatial quality, and they mainly consist of two phases: detail extraction and detail injection. Detail extraction is performed by estimating the intensity component, whereas detail injection depends on the definition of injection gain. In the classic CS-based pansharpening methods, the intensity component is estimated through a global synthesis scheme, and injection gains can be obtained by a context-adaptive or a global approach. In this letter, we propose an improved CS-based HS pansharpening method in which the intensity component and the injection gain are estimated locally achieved by the binary partition tree (BPT) image segmentation algorithm. The proposed method is applied to two credible CS-based HS pansharpening algorithms, including the Gram–Schmidt adaptive (GSA) and the Brovey transform (Brovey). The experimental results show that the proposed method improves the performance of GSA and Brovey and creates promising results perceptually and quantitatively.","1558-0571","","10.1109/LGRS.2021.3094216","National Defense Pre-Research Foundation; Higher Education Discipline Innovation Project(grant numbers:B08038); National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2021JQ-194,2021JQ-197); Fundamental Research Funds for the Central Universities(grant numbers:XJS210108,XJS210104); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110856); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9486865","Detail extraction;detail injection;injection gain;intensity component;pansharpening","Pansharpening;Image segmentation;Spatial resolution;Estimation;Merging;Bayes methods;Sparse matrices","geophysical image processing;image fusion;image resolution;image segmentation;remote sensing","credible CS-based HS pansharpening algorithms;hyperspectral pansharpening;local intensity component;local injection gain estimation;component substitution-based HS pansharpening algorithms;high spatial quality;classic CS-based pansharpening methods;global synthesis scheme;binary partition tree image segmentation algorithm;CS-based HS pansharpening method","","","","24","IEEE","15 Jul 2021","","","IEEE","IEEE Journals"
"Fusion of Sun-Synchronous and Geostationary Images for Coastal and Ocean Color Survey Application to OLCI (Sentinel-3) and FCI (MTG)","C. Peschoud; A. Minghelli; S. Mathieu; M. Lei; I. Pairaud; C. Pinazo","Université de Toulon, CNRS, LSIS UMR 7296, Toulon Cedex 9, France; Université de Toulon, CNRS, LSIS UMR 7296, Toulon Cedex 9, France; Thales Alenia Space, Cannes la Bocca, France; Université de Toulon, CNRS, LSIS UMR 7296, Toulon Cedex 9, France; LERPAC Ifremer, Centre Méditerranée, Zone Portuaire de Brégaillon, La Seyne-sur-Mer Cedex, France; Université deToulon, CNRS/INSU, IRD, Mediterranean Institute of Oceanography (MIO), UM 110, La Garde, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 May 2017","2017","10","1","45","56","Open ocean and coastal area monitoring requires multispectral satellite images with a middle spatial resolution (~300 m) and a high temporal repeatability (~1 h). As no current satellite sensors have such features, the aim of this study is to propose a fusion method to merge images delivered by a low earth orbit (LEO) sensor with images delivered by a geostationary earth orbit (GEO) sensor. This fusion method, called spatial spectral temporal fusion (SSTF), is applied to the future sensors- Ocean and Land Color Instrument (OLCI) (on Sentinel-3) and Flexible Combined Imager (FCI) (on Meteosat Third Generation) whose images were simulated. The OLCI bands, acquired at t0, are divided by the oversampled corresponding FCI band acquired at t0 and multiplied by the FCI bands acquired at t1. The fusion product is used for the next fusion at t1 and so on. The high temporal resolution of FCI allows its signal-to-noise ratio (SNR) to be enhanced by the means of temporal filtering. The fusion quality indicator ERGAS computed between SSTF fusion products and reference images is around 0.75, once the FCI images are filtered from the noise and 1.08 before filtering. We also compared the estimation of chlorophyll (Chl), suspended particulate matter (SPM), and colored dissolved organic matter (CDOM) maps from the fusion products with the input simulation maps. The comparison shows an average relative errors on Chl, SPM, and CDOM, respectively, of 64.6%, 6.2%, and 9.5% with the SSTF method. The SSTF method was also compared with an existing fusion method called the spatial and temporal adaptive reflectance fusion model (STARFM).","2151-1535","","10.1109/JSTARS.2016.2558819","Provence-Alpes-Côte d'Azur; Thales Alenia Space; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487007","Fusion;image simulation;meteosat Third Generation (MTG);ocean color;Ocean and Land Color Instrument","Sensors;Atmospheric modeling;Spatial resolution;Biological system modeling;Image sensors;Sea measurements","geophysical image processing;image fusion;image resolution;ocean composition;oceanographic equipment;underwater optics","sun-synchronous image fusion;geostationary image fusion;open ocean monitoring;coastal area monitoring;multispectral satellite images;spatial resolution;high temporal repeatability;satellite sensors;low earth orbit sensor;geostationary earth orbit sensor;spatial spectral temporal fusion;Ocean and Land Color Instrument;Flexible Combined Imager;Meteosat Third Generation;Flexible Combined Imager band;signal-to-noise ratio;temporal filtering;fusion quality indicator ERGAS;SSTF fusion products;reference images;chlorophyll estimation;suspended particulate matter;colored dissolved organic matter maps;input simulation maps;average relative errors;spatial and temporal adaptive reflectance fusion model","","6","","44","IEEE","8 Jun 2016","","","IEEE","IEEE Journals"
"Multispectral Pansharpening Based on Multisequence Convolutional Recurrent Neural Network","P. Wang; Z. He; Y. Zhang; G. Zhang; H. Liu; H. Leung","Donghai Laboratory, Zhoushan, China; Key Laboratory of Radar Imaging and Microwave Photonics, Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China; 8511 Research Institute of China Aerospace Science and Industry Corporation, Nanjing, China; Key Laboratory of Radar Imaging and Microwave Photonics, Ministry of Education, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Nanjing Research Institute of Electronics Engineering, Nanjing, China; Department of Electrical and Computer Engineering, University of Calgary, Calgary, Canada","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Nov 2022","2022","15","","9482","9496","Multispectral (MS) pansharpening is defined as the fusion of spatial information in panchromatic (PAN) image and spectral information in MS image. In this work, we propose an MS pansharpening based on multisequence convolutional recurrent neural network (MCRNN). The proposed MCRNN contains two subnetworks (shallow feature extraction subnetwork and deep feature fusion subnetwork). In the shallow feature extraction subnetwork, PAN and MS images are superimposed in the spectral dimension as multisequence data. A convolutional neural network based on residual learning is then used to obtain the feature maps from multisequence data. In the deep feature fusion subnetwork, since MS and PAN images are highly correlated, a convolutional recurrent neural network belonging to recurrent neural network is used to model adjacent and across-band relationships between these feature maps to capture the local and global correlations of the features in different bands. The global average pooling is then performed on the output results to yield the pansharpening result. Several datasets are tested at reduced and full-resolution experiments, the experimental results show that the performance of the proposed MCRNN is superior to the traditional pansharpening methods.","2151-1535","","10.1109/JSTARS.2022.3218367","National Key R&D Program of China(grant numbers:2019YFB2102005); Program of Remote Sensing Intelligent Monitoring and Emergency Services for Regional Security Elements; Hong Kong Scholars Program(grant numbers:XJ2022043); Natural Science Foundation of Jiangsu Province(grant numbers:BK20221478); Science Foundation of Donghai Laboratory(grant numbers:DH-2022KF01011); National Natural Science Foundation of China(grant numbers:61801211); Open Research Fund Program of Key Laboratory of Digital Mapping and Land Information Application, Ministry of Natural Resources(grant numbers:ZRZYBWD202203); Open Fund of Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions (Henan University), Ministry of Education(grant numbers:GTYR202211); Project of Experimental Technology Research and Development; Nanjing University of Aeronautics and Astronautics(grant numbers:SYJS202203Y); Postgraduate Research & Practice Innovation Program of NUAA(grant numbers:xcxjh20220405); Innovation and Entrepreneurship Training Program for College Students of NUAA(grant numbers:2022CX004044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933611","Convolutional neural network (CNN);feature extraction;feature fusion;multispectral (MS) pansharpening;recurrent neural network (RNN)","Feature extraction;Pansharpening;Recurrent neural networks;Convolutional neural networks;Data mining;Correlation;Spatial resolution","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;learning (artificial intelligence);neural nets;recurrent neural nets;remote sensing","MS pansharpening;multisequence convolutional recurrent neural network;MCRNN;subnetworks;shallow feature extraction subnetwork;deep feature fusion subnetwork;PAN;MS image;multisequence data;convolutional neural network;feature maps;pansharpening result;traditional pansharpening methods;multispectral pansharpening;panchromatic image;spectral information","","","","64","CCBY","31 Oct 2022","","","IEEE","IEEE Journals"
"Sparsity Constrained Fusion of Hyperspectral and Multispectral Images","X. Fu; S. Jia; M. Xu; J. Zhou; Q. Li","College of Computer Science and Software Engineering and the Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering and the Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering and the Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia; Key Laboratory for Geo-Environmental Monitoring of Coastal Zone of the Ministry of Natural Resources, Shenzhen University, Shenzhen, China","IEEE Geoscience and Remote Sensing Letters","9 Feb 2022","2022","19","","1","5","Fusing a Hyperspectral image (HSI) and a multispectral image (MSI) from different sensors is an economic and effective approach to get an image with both high spatial and spectral resolution, but localized changes between the multiplatform images can have negative impacts on the fusion. In this letter, we propose a novel sparsity constrained fusion method (SCFus) to fuse multiplatform HSIs and MSIs based on matrix factorization. Specifically, we imposed  $\ell _{1}$  norm on the residual term of the MSI to account for the localized changes between the hyperspectral and MSIs. Furthermore, we plugged a state-of-the-art denoiser, namely block-matching and 3-D filtering (BM3D), as the prior of the subspace coefficients by exploiting the plug-and-play framework. We refer to the proposed method as SCFus for hyperspectral and MSIs. Experimental results suggest that the proposed fusion method is more effective in fusing hyperspectral and MSIs than the competitors.","1558-0571","","10.1109/LGRS.2022.3146248","National Natural Science Foundation of China(grant numbers:41971300,61901278,62001303); Key Project of Department of Education of Guangdong Province(grant numbers:2020ZDZX3045); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011413); China Postdoctoral Science Foundation(grant numbers:2021M692162); Shenzhen Scientific Research and Development Funding Program(grant numbers:20200803152531004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9691365","Hyperspectral image (HSI);image changes;image fusion;multiplatform data","Hyperspectral imaging;Fuses;Lakes;Tensors;Instruments;Indexes;Additives","geophysical image processing;hyperspectral imaging;image colour analysis;image denoising;image fusion;image representation;image resolution;matrix decomposition;sensor fusion","MSI;SCFus;fusion method;sparsity constrained fusion;multispectral image;hyperspectral image;economic approach;high spatial resolution;spectral resolution;localized changes;multiplatform images;multiplatform HSI;block-matching and 3-D filtering;BM3D","","2","","18","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"Navigation and SAR focusing with map aiding","Z. Sjanic; F. Gustafsson","Division of Automatic Control, Linköping University, Linköping, Sweden; Division of Automatic Control, Linköping University, Linköping, Sweden","IEEE Transactions on Aerospace and Electronic Systems","28 Sep 2015","2015","51","3","1652","1663","A method for fusing synthetic aperture radar (SAR) images with optical aerial images is presented. This is done in a navigation framework, in which the absolute position and orientation of the flying platform, as computed from the inertial navigation system, is corrected based on the aerial image coordinates taken as ground truth. The method is suitable for new low-price SAR systems for small unmanned vehicles. The primary application is surveillance, and to some extent it can be applied to remote sensing, where the SAR image provides complementary information by revealing reflectivity to microwave frequencies. The method is based on first applying an edge detection algorithm to the images and then optimising the most important navigation states by matching the two binary images. To get a measure of the estimation uncertainty, we embed the optimisation in a least squares framework, in which an explicit method to estimate the (relative) size of the errors is presented. The performance is demonstrated on real SAR and aerial images, leading to an error of only a few pixels (around 4 m in our case), which is a quite satisfactory performance for applications like surveillance and navigation.","1557-9603","","10.1109/TAES.2015.130397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272820","","Synthetic aperture radar;Navigation;Optical imaging;Image edge detection;Trajectory;Detectors;Optical sensors","autonomous aerial vehicles;edge detection;geophysical image processing;image fusion;image matching;inertial navigation;least mean squares methods;optimisation;radar imaging;remote sensing by radar;search radar;synthetic aperture radar","synthetic aperture radar;SAR navigation;SAR focusing;SAR image fusion;optical aerial image coordinates;absolute position estimation;orientation estimation;inertial navigation system;small unmanned vehicles;survelliance;remote sensing;edge detection algorithm;navigation state optimisation;binary image matching;least square method;error size estimation","","15","","31","IEEE","28 Sep 2015","","","IEEE","IEEE Journals"
"Band Assignment Approaches for Hyperspectral Sharpening","D. Picone; R. Restaino; G. Vivone; P. Addesso; M. Dalla Mura; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Salerno, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Salerno, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Salerno, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Salerno, Italy; University of Grenoble Alpes, CNRS, GIPSA-lab, Grenoble, France; University of Grenoble Alpes, CNRS, GIPSA-lab, Grenoble, France","IEEE Geoscience and Remote Sensing Letters","24 Apr 2017","2017","14","5","739","743","Classical pansharpening algorithms constitute a class of image fusion methods that have been widely investigated in the literature. They have been developed for combining a single- and a multichannel image (panchromatic (PAN) and multispectral (MS), respectively), but can be adapted to the sharpening of hyperspectral (HS) data, both through companion PAN and MS images. We focus in this letter on the HS/MS fusion, showing that the assignation of the MS channel to each HS band is a key step, and investigate several alternatives to make this choice. The assignment algorithms are tested in conjunction with both component substitution and multiresolution analysis pansharpening methods and assessed on images acquired by the Hyperion and ALI sensors. The numerical evaluation shows that the best results can be obtained by optimizing the spectral angle mapper metric confirming that classical methods represent a reliable basis for the development of novel sharpening algorithms.","1558-0571","","10.1109/LGRS.2017.2677087","Labex OSUG@2020(grant numbers:ANR10 LABX56); Programme National de Télédétection Spatiale(grant numbers:PNTS-2016-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885593","Data fusion;hyperspectral data;image sharpening;multispectral data;pansharpening","Spatial resolution;Indexes;Algorithm design and analysis;Hyperspectral imaging;Data integration","geophysical techniques;hyperspectral imaging;image fusion","band assignment approaches;classical pansharpening algorithms;image fusion methods;multichannel image;single-channel image;panchromatic PAN image;hyperspectral data sharpening;MS channel assignation;multiresolution analysis;pansharpening methods;Hyperion sensor;ALI sensor;numerical evaluation;spectral angle mapper metric;classical methods","","14","","17","IEEE","23 Mar 2017","","","IEEE","IEEE Journals"
"Building Detection From Monocular VHR Images by Integrated Urban Area Knowledge","A. Manno-Kovács; A. O. Ok","Distributed Events Analysis Research Laboratory, Institute for Computer Science and Control, Hungarian Academy of Sciences (MTA SZTAKI), Budapest, Hungary; Department of Geodesy and Photogrammetry, Nevsehir H.B.V. University, Nevsehir, Turkey","IEEE Geoscience and Remote Sensing Letters","7 Aug 2015","2015","12","10","2140","2144","This letter proposes an approach for building detection from single very high resolution optical satellite images by fusing the knowledge of shadow and urban area information. One of the main contributions of this work is in the integration of urban area information: unlike previous studies, we use such information to substantially revise and improve the initial shadow mask. Additionally, we present an effective way to discriminate dark regions from cast shadows, a task that has continuously been reported to be very difficult. In this letter, we benefit from graph cuts to produce a comprehensive solution for automatic building detection: a flexible multilabel partitioning procedure is proposed, in which the number of optimized classes is automatically selected according to the contents of a scene of interest. The results of the evaluation of 14 demanding test patches confirm the technical merit of the proposed approach, as well as its superiority over three recently developed state-of-the-art methods.","1558-0571","","10.1109/LGRS.2015.2452962","European Space Agency DUSIREF Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167681","Automated building detection;flexible multilabel partitioning;graph cuts;satellite images;urban area detection;Automated building detection;flexible multilabel partitioning;graph cuts;satellite images;urban area detection","Buildings;Urban areas;Remote sensing;Feature extraction;Vegetation mapping;Satellites;Image color analysis","buildings (structures);graph theory;image fusion;image resolution;object detection","monocular VHR images;integrated urban area knowledge;high resolution optical satellite images;shadow area information;urban area information;initial shadow mask;graph cuts;automatic building detection;flexible multilabel partitioning procedure;optimized class number;test patches","","34","","18","IEEE","27 Jul 2015","","","IEEE","IEEE Journals"
"Multifeature Fusion-Based Hand Gesture Sensing and Recognition System","Y. Wang; Y. Shu; X. Jia; M. Zhou; L. Xie; L. Guo","School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; Alibaba Group, Hangzhou, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2021","2022","19","","1","5","With the development of the radar sensing technology, hand gesture sensing and recognition has attracted much attention. This letter adopts a frequency-modulated continuous wave (FMCW) radar to achieve short-range hand gesture sensing and recognition. Specifically, the range, Doppler, and angle parameters of hand gestures are measured by fast Fourier transformation (FFT) and multiple signal classification (MUSIC) algorithm, respectively. The mixup (MP) algorithm combined with augmentation (AU) algorithm using a weight factor is applied to expand the hand gesture data. Then, a complementary multidimensional feature fusion network-based hand gesture recognition (CMFF-HGR) is designed to extract the features and achieve HGR. Finally, a series of experiments are carried out to verify the effectiveness of the proposed approach, and the results show that the recognition accuracy is higher than the existing alternatives with low computational complexity.","1558-0571","","10.1109/LGRS.2021.3086136","National Natural Science Foundation of China(grant numbers:61901076,61771083); Fundamental and Frontier Research Project of Chongqing(grant numbers:cstc2020jcyj-msxmX0865); China Postdoctoral Science Foundation(grant numbers:2021M693773); Natural Science Foundation of Chongqing(grant numbers:KJQN201900603,KJZDK202000605,KJQN202000630); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459735","Dataset expansion;hand gesture recognition (HGR);millimeter-wave radar;multidimensional feature fusion;short-range radar sensing","Feature extraction;Radar;Sensors;Doppler effect;Training;Radar remote sensing;Gold","computational complexity;CW radar;Doppler radar;fast Fourier transforms;feature extraction;FM radar;gesture recognition;image fusion;radar imaging;radar receivers;signal classification","multifeature fusion-based hand gesture sensing;radar sensing technology;frequency-modulated continuous wave radar;short-range hand gesture sensing;mixup algorithm;hand gesture data;short-range hand gesture recognition;FMCW radar;multifeature fusion-based hand gesture recognition system;multidimensional feature fusion network-based hand gesture recognition;fast Fourier transformation;FFT;multiple signal classification algorithm;MUSIC algorithm;augmentation algorithm;CMFF-HGR","","14","","19","IEEE","18 Jun 2021","","","IEEE","IEEE Journals"
"Multiorder Interaction Information Embedding-Based Multiview Fusion-Aided Hyperspectral Image Classification","X. Li; W. Cao; K. Zhang; B. Liu; D. Tao; W. Liu","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Yangtze Three Gorges Technology and Economy Development Company Ltd., Beijing, China; School of Petroleum Engineering, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; School of Information Science and Engineering, Yunnan University, Kunming, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China","IEEE Geoscience and Remote Sensing Letters","31 Aug 2022","2022","19","","1","5","Hyperspectral images (HSIs) are obtained from hyperspectral imaging sensors, which capture information in hundreds of spectral bands of objects. However, how to take full advantage of spatial and spectral information from many spectral bands to improve the performance of HSI classification remains an open question. Many HSI classification works have recently been reported by employing multiview learning (MVL) algorithms that can fully use complementary information between different view features and thus have received widespread attention. This letter proposes a multiview fusion network based on multiorder interaction information embedding for HSI classification. First, the correlation matrix between spectral bands is used to divide the original data into multiple subsets as local views. The subset after the segmented-PCA process is used as the global view. Second, the features of different views are extracted separately using a feature extraction network and mapped to the same dimension. Prefusion is achieved by multiorder interaction of various view features. Finally, loss-weighted fusion is applied to each view according to its contribution to the classification task. To evaluate the effectiveness of the proposed method, complete experiments were conducted on three commonly used HSI datasets, namely Pavia University, Houston 2013, and Houston 2018. The experimental results demonstrate that the proposed method improves the classification performance of existing feature extraction networks and is more competitive with other methods in the field.","1558-0571","","10.1109/LGRS.2022.3199607","Open Project Program of the National Laboratory of Pattern Recognition (NLPR)(grant numbers:202000009); Major Scientific and Technological Projects of China National Petroleum Corporation (CNPC)(grant numbers:ZD2019-183-008); National Natural Science Foundation of China(grant numbers:61671480); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858887","Hyperspectral image (HSI) classification;interaction information;loss fusion;multiview learning (MVL)","Feature extraction;Correlation;Petroleum;Principal component analysis;Optimization;Geoscience and remote sensing;Task analysis","feature extraction;hyperspectral imaging;image classification;image fusion;image segmentation;principal component analysis","local views;feature extraction network;loss-weighted fusion;classification task;commonly used HSI datasets;classification performance;interaction information embedding-based multiview fusion-aided hyperspectral image classification;hyperspectral images;hyperspectral imaging sensors;capture information;spectral bands;spatial information;spectral information;HSI classification;classification works;complementary information;different view features;multiview fusion network;multiorder interaction information embedding","","1","","19","IEEE","17 Aug 2022","","","IEEE","IEEE Journals"
"Deep Unsupervised Blind Hyperspectral and Multispectral Data Fusion","J. Li; K. Zheng; J. Yao; L. Gao; D. Hong","College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Computational Optical Imaging Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Computational Optical Imaging Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Computational Optical Imaging Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Computational Optical Imaging Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","7 Mar 2022","2022","19","","1","5","Hyperspectral images (HSIs) usually have finer spectral resolution but coarser spatial resolution than multispectral images (MSIs). To obtain a desired HSI with higher spatial resolution, great research attention has been paid to achieving hyperspectral super-resolution by fusing the observed HSI with an auxiliary MSI of the same scene. However, most of the existing HSI-MSI fusion methods rely either on prior knowledge of the degradation model or on sufficient training data, hindering their practicality and interpretability. In this letter, we propose a novel unsupervised HSI-MSI fusion network with the ability of degradation adaptive learning, namely, UDALN. Specifically, we propose three modules to straightly encode the spatial and spectral transformations across resolutions, i.e., SpaDnet, SpeUnet, and SpeDnet. Through an elaborately designed three-stage unsupervised training strategy, the estimated network parameters can exhibit clear physical meanings of degradation processes and therefore help guarantee a faithful reconstruction of the desired HSI. The experimental results on two widely used hyperspectral datasets demonstrate the effectiveness of our method in comparison to the state-of-the-art HSI-MSI fusion models. (Code available at https://github.com/JiaxinLiCAS/UDALN_GRSL.)","1558-0571","","10.1109/LGRS.2022.3151779","National Natural Science Foundation of China(grant numbers:62161160336,42030111); China Postdoctoral Science Foundation(grant numbers:2021M693234); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714360","Data fusion;deep learning;hyperspectral;multispectral;unsupervised learning","Degradation;Spatial resolution;Training;Hyperspectral imaging;Kernel;Adaptation models;Tensors","geophysical image processing;hyperspectral imaging;image classification;image colour analysis;image fusion;image resolution;image sensors;learning (artificial intelligence);remote sensing;sensor fusion;unsupervised learning","deep unsupervised blind hyperspectral;multispectral data fusion;hyperspectral images;finer spectral resolution;coarser spatial resolution;multispectral images;desired HSI;higher spatial resolution;great research attention;observed HSI;auxiliary MSI;existing HSI-MSI fusion methods;unsupervised HSI-MSI fusion network;degradation adaptive learning;spatial transformations;spectral transformations;three-stage unsupervised training strategy;estimated network parameters;degradation processes;widely used hyperspectral datasets;state-of-the-art HSI-MSI fusion models","","12","","22","IEEE","15 Feb 2022","","","IEEE","IEEE Journals"
"S3: A Spectral-Spatial Structure Loss for Pan-Sharpening Networks","J. -S. Choi; Y. Kim; M. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; Artificial Intelligence Research Division, Korea Aerospace Research Institute, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Geoscience and Remote Sensing Letters","22 Apr 2020","2020","17","5","829","833","Recently, many deep-learning-based pan-sharpening methods have been proposed for generating high-quality pan-sharpened (PS) satellite images. These methods focused on various types of convolutional neural network (CNN) structures, which were trained by simply minimizing a spectral loss between network outputs and the corresponding high-resolution (HR) multi-spectral (MS) target images. However, owing to different sensor characteristics and acquisition times, HR panchromatic (PAN) and low-resolution MS image pairs tend to have large pixel misalignments, especially for moving objects in the images. Conventional CNNs trained with only the spectral loss with these satellite image data sets often produce PS images of low visual quality including double-edge artifacts along strong edges and ghosting artifacts on moving objects. In this letter, we propose a novel loss function, called a spectral-spatial structure (S3) loss, based on the correlation maps between MS targets and PAN inputs. Our proposed S3 loss can be very effectively used for pan-sharpening with various types of CNN structures, resulting in significant visual improvements on PS images with suppressed artifacts.","1558-0571","","10.1109/LGRS.2019.2934493","National Research Foundation of Korea (NRF); Ministry of Science, ICT and Future Planning through the Basic Science Research Program(grant numbers:2017R1A2A2A05001476); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812763","Convolutional neural network (CNN);deep learning;pan colorization;pan-sharpening;satellite imagery;spectral-spatial structure;super-resolution (SR)","Training;Satellites;Correlation;Spatial resolution;Visualization;Convolutional neural networks","convolutional neural nets;geophysical image processing;image classification;image fusion;image resolution;learning (artificial intelligence);remote sensing","spectral-spatial structure loss;MS targets;PAN inputs;CNN structures;PS images;deep-learning-based pan-sharpening methods;high-quality pan-sharpened satellite images;convolutional neural network structures;spectral loss;network outputs;multispectral target images;HR panchromatic;moving objects;satellite image data sets;double-edge artifacts;strong edges;ghosting artifacts;loss function","","11","","32","IEEE","26 Aug 2019","","","IEEE","IEEE Journals"
"Unsupervised Hyperspectral Pansharpening by Ratio Estimation and Residual Attention Network","J. Nie; Q. Xu; J. Pan","State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","1 Mar 2022","2022","19","","1","5","Most deep learning-based hyperspectral pansharpening methods use the hyperspectral images (HSIs) as the ground truth. Training samples are usually obtained by blurring and downsampling the panchromatic image and HSI. However, the blurring and downsampling operation lose much spatial and spectral information. As a result, the model parameters trained by these reduced-resolution samples are unsuitable for fusing full-resolution images. To tackle this problem, we propose an unsupervised hyperspectral pansharpening method via ratio estimation (RE) and residual attention network (RE-RANet). The spatial and spectral information of the fused image are derived from the original panchromatic and HSI rather than reduced-resolution images. At first, we generate the initial ratio image using the ratio enhancement method. The initial ratio image is fine-tuned by the residual attention network (RANet) to generate a multichannel ratio image. Then, we inject the multichannel ratio image that contains spatial detail information into the HSI. Finally, the generated hyperspectral image is constrained by the spatial constraint loss and the spectral constraint loss. Experiments on the EO-1 and Chikusei datasets verify the effectiveness of the proposed method. Compared with other state-of-the-art approaches, our method performs well in qualitative visual effects and quantitative evaluation indicators.","1558-0571","","10.1109/LGRS.2022.3149166","National Natural Science Foundation of China(grant numbers:61972021,61672076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9703344","Deep learning;hyperspectral pansharpening;ratio estimation (RE);residual attention network (RANet)","Hyperspectral imaging;Pansharpening;Spatial resolution;Training;Estimation;Deep learning;Indexes","deep learning (artificial intelligence);geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image enhancement;image fusion;image resolution;image sampling;remote sensing","reduced-resolution samples;full-resolution images;unsupervised hyperspectral pansharpening method;ratio estimation;residual attention network;spatial information;spectral information;fused image;original panchromatic HSI;reduced-resolution images;initial ratio image;ratio enhancement method;multichannel ratio image;generated hyperspectral image;spatial constraint loss;spectral constraint loss;deep learning-based hyperspectral pansharpening methods;hyperspectral images;training samples;panchromatic image;downsampling operation","","5","","28","IEEE","4 Feb 2022","","","IEEE","IEEE Journals"
"Improving Hyperspectral Image Classification by Fusing Spectra and Absorption Features","B. Guo; H. Shen; M. Yang","School of Automation, Hangzhou Dianzi University, Hangzhou, China; Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun Institute of Optics, Changchun, China; Fine Mechanics and Physics, Chinese Academy of Sciences, Changchun Institute of Optics, Changchun, China","IEEE Geoscience and Remote Sensing Letters","20 Jul 2017","2017","14","8","1363","1367","Many features can be extracted to classify hyperspectral imagery. Classification relying on a single feature set may lose some useful information due to the intrinsic limitation of each feature extraction model. To improve classification accuracy, we propose an information fusion approach, in which both the global and the local aspects of hyperspectral data are taken into account and are combined by a decision-level fusion method. The global features are hyperspectral reflectance curves representing the holistic response to the incident light, and the local features are absorption characteristics reflected by materials' individual constituents. The decision-level fusion is carried out by analyzing the entropy of the classification output from the global feature set and modifying this output via the results of a multilabel classification using the local feature set. Simulations of classification performance on 16 classes of vegetation from the AVIRIS 92AV3C and Salinas data set show the effectiveness of the method, which increases the classification accuracy compared to a popular support vector machine-based method and a production-rule-based decision fusion method.","1558-0571","","10.1109/LGRS.2017.2712805","National Natural Science Foundation of China(grant numbers:61375011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961241","Decision-level fusion;hyperspectral image classification;multilabel classification","Absorption;Hyperspectral imaging;Feature extraction;Support vector machines;Training;Entropy","absorption;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;reflectivity;remote sensing;vegetation","hyperspectral image classification;feature extraction;hyperspectral imagery;classification accuracy;information fusion approach;decision-level fusion method;hyperspectral reflectance curve;absorption characteristics;multilabel classification;vegetation;AVIRIS 92AV3C;Salinas data set","","5","","15","IEEE","28 Jun 2017","","","IEEE","IEEE Journals"
"A Two-Branch Network Combined With Robust Principal Component Analysis for Hyperspectral Image Classification","C. Mu; Q. Zeng; Y. Liu; Y. Qu","Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; Newcastle Business School, Northumbria University, Newcastle upon Tyne, U.K.","IEEE Geoscience and Remote Sensing Letters","23 Nov 2021","2021","18","12","2147","2151","Noise in hyperspectral images (HSIs) may degrade the HSI classification result. Robust principal component analysis (RPCA) is an excellent method to obtain low-rank (LR) representation of data and is widely used in image denoising and also in HSI classification. However, data are drawn as a union from multiple subspaces in HSIs, so LR subspace estimation (LRSE) is necessary when using RPCA, which is complicated and time-consuming. To solve this problem, this letter proposes a novel LR-based method for HSI classification called two-branch network combined with RPCA, which combines RPCA with a neural network. Specifically, both the LR component and the sparse component are preserved and used for feature extraction in two independent convolutional branches. This way, we can avoid information loss without using accurate LRSE. A concatenate operation and a pointwise convolution are then adopted to realize the feature fusion. Finally, fused features are constructed to indicate the ground truth of each pixel in the classification process. The proposed method constructs a convenient model for HSI classification by discarding the LRSE and combining denoising, feature extraction, feature fusion, and classification into an end-to-end network. The experimental results on three data sets demonstrate that the proposed method outperforms many state-of-the-art methods including ones based on LR representation and ones based on deep learning. In addition, it maintains good classification performance for the cases of small samples and class imbalance.","1558-0571","","10.1109/LGRS.2020.3013707","National Natural Science Foundation of China(grant numbers:61672405,61876141,U1701267,61772399,61773304,61773300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9167293","Convolutional neural networks (CNNs);deep learning;hyperspectral image (HSI) classification;low-rank (LR) representation;two-branch structure","Feature extraction;Noise reduction;Robustness;Hyperspectral imaging;Sparse matrices;Principal component analysis","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;hyperspectral imaging;image classification;image denoising;image fusion;image representation;principal component analysis;remote sensing","two-branch network;robust principal component analysis;hyperspectral image classification;HSI classification;RPCA;low-rank representation;image denoising;LR subspace estimation;LRSE;neural network;LR component;sparse component;feature extraction;independent convolutional branches;feature fusion;end-to-end network;LR representation;deep learning","","4","","19","IEEE","14 Aug 2020","","","IEEE","IEEE Journals"
"An Efficient Detail Extraction Algorithm for Improving Haze-Corrected CS Pansharpening","W. Wang; H. Liu","School of Automation and Information Engineering, Xi’an University of Technology, Xi’an, China; School of Automation and Information Engineering, Xi’an University of Technology, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","In order to overcome the potential distortion problem, an improved haze-corrected version of multiplicative-based component substitution pansharpening method is proposed. Our improvement is to acquire the optimal spatial detail maps in detail extraction procedure with sparse structural manifold embedding (SSME) and guided filter. First, a major spatial detail image is estimated by using SSME to well preserve the structural details, such as textures, edges, and contours. Second, a detail map is obtained by using the guided filter compensates for the local information loss of the major detail image due to inaccurate recovery for the patches with weaken structural information. Finally, the pansharpened image is acquired by proportionally injecting two detail images into the low-resolution multispectral image. The performance evaluations on degraded and real QuickBird data sets by using quality assessment metrics and visual analysis demonstrate that the proposed method achieves better results as compared with other existing methods.","1558-0571","","10.1109/LGRS.2021.3059777","National Natural Science Foundation of China(grant numbers:61703334,61973248); Project funded by China Postdoctoral Science Foundation(grant numbers:2016M602942XB); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366294","Guided filter;haze;multiplicative-based component substitution;pansharpening;sparse structural manifold embedding (SSME)","Pansharpening;Distortion;Indexes;Manifolds;Training data;Correlation;Atmospheric modeling","geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing","efficient detail extraction algorithm;improving haze-corrected CS pansharpening;potential distortion problem;improved haze-corrected version;multiplicative-based component substitution pansharpening method;optimal spatial detail maps;detail extraction procedure;sparse structural manifold;SSME;spatial detail image;guided filter compensates;local information loss;weaken structural information;pansharpened image;low-resolution multispectral image","","1","","27","IEEE","1 Mar 2021","","","IEEE","IEEE Journals"
"MTFFN: Multimodal Transfer Feature Fusion Network for Hyperspectral Image Classification","H. Yan; E. Zhang; J. Wang; C. Leng; J. Peng","College of Computer Science and Information Engineering, Anyang Institute of Technology, Anyang, China; College of Information Engineering, Northwest A&F University, Xi’an, China; School of Information Science and Technology, Northwest University, Xi’an, China; School of Mathematics, Northwest University, Xi’an, China; School of Information Science and Technology, Northwest University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","30 Mar 2022","2022","19","","1","5","Transfer learning is an effective way to alleviate the problem of insufficient samples in a hyperspectral image (HSI) classification. However, the present transfer learning-based methods usually transfer knowledge from a single source domain, such as the natural image domain. Therefore, these methods cannot simultaneously transfer spectral and spatial knowledge to the target domain in HSIs. Generally, the natural image has rich spatial structure and texture information, while the HSI has abundant spectral information. To better utilize the knowledge learned from natural image datasets and HSI datasets, we proposed a multimodal transfer feature fusion network (MTFFN) for HSI classification. In MTFFN, a dual-branch network structure is designed to transfer the two-modal knowledge from the natural image domain and the source HSI domain to the target domain in two branches, respectively. A multitask learning strategy is adopted to achieve feature fusion. The fused features are used to generate the final classification result. Moreover, a local attention mechanism is designed to extract more meaningful spectral features. Experiments on two public datasets show that the proposed method is effective (https://github.com/HuaipYan/MTFFN).","1558-0571","","10.1109/LGRS.2022.3160882","National Natural Science Foundation of China(grant numbers:62006188,62101446); Xi’an Key Laboratory of Intelligent Perception and Cultural Inheritance(grant numbers:2019219614SYS011CG033); Key Research and Development Program of Shaanxi(grant numbers:2021ZDLGY15-06); Program for Changjiang Scholars and Innovative Research Team in University(grant numbers:IRT_17R87); International Science and Technology Cooperation Research Plan in Shaanxi Province of China(grant numbers:2022KW-08); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738646","Hyperspectral image (HSI) classification;local attention mechanism;multitask learning;transfer learning","Feature extraction;Task analysis;Kernel;Convolution;Knowledge engineering;Data mining;Three-dimensional displays","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image texture;learning (artificial intelligence);remote sensing","multitask learning strategy;final classification result;MTFFN;spectral features extraction;multimodal transfer feature fusion network;hyperspectral image classification;transfer learning-based methods;single source domain;natural image domain;spectral knowledge;spatial knowledge;target domain;rich spatial structure;texture information;abundant spectral information;natural image datasets;HSI datasets;HSI classification;dual-branch network structure;two-modal knowledge;source HSI domain","","1","","16","IEEE","21 Mar 2022","","","IEEE","IEEE Journals"
"Automatic Extraction of Layover From InSAR Imagery Based on Multilayer Feature Fusion Attention Mechanism","X. Cai; L. Chen; J. Xing; X. Xing; R. Luo; S. Tan; J. Wang","School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Traffic and Transportation Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Layover is a kind of geometric distortion in radar systems with side-look imaging, especially in mountainous and dense urban areas. It causes phase distortion and alters target characteristics in the acquired images, which directly hinders the application of radar images. In this letter, the multilayer feature fusion attention mechanism (MF2AM) is proposed to extract layover from interferometric synthetic aperture radar (InSAR) imagery automatically. First, the SAR image, the corresponding coherence map, and interferometric phases are channel-fused to enhance semantic information of layover areas. Then, the fused image is fed into MF2AM to extract the essential features of layover. Finally, the detection results are produced via MF2AM. MF2AM consists of the encoder and the decoder. The encoder contains three parts: the resnet101, attention-based atrous spatial pyramid (AASP), and the semantic embedding branch (SEB). In the decoder, step decoding is used to better fuse high- and low-level features and improve the effect of edge segmentation. To verify the proposed method, the images of millimeter wave InSAR system are used for the experiment, and the performance is compared with DeepLabV3+ and Geospatial Contextual Attention Mechanism (GCAM). The results show that the MF2AM has achieved obvious performance advantages. The average pixel accuracy and average intersection over union (IOU) are 0.9601 and 0.9310, respectively, and the average test time is only 7.97 s.","1558-0571","","10.1109/LGRS.2021.3105722","National Natural Science Foundation of China(grant numbers:42101468,42074033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524811","Attention mechanism;deep learning;interferometric synthetic aperture radar (InSAR);layover;semantic segmentation","Feature extraction;Convolution;Semantics;Decoding;Radar polarimetry;Data mining;Image edge detection","image fusion;radar imaging;radar interferometry;remote sensing by radar;synthetic aperture radar","radar images;multilayer feature fusion attention mechanism;MF2AM;interferometric synthetic aperture radar imagery;SAR image;interferometric phases;layover areas;fused image;attention-based atrous spatial pyramid;low-level features;millimeter wave InSAR system;Geospatial Contextual Attention Mechanism;automatic extraction;InSAR imagery;geometric distortion;radar systems;side-look imaging;mountainous areas;dense urban areas;phase distortion;alters target characteristics;time 7.97 s","","1","","22","IEEE","30 Aug 2021","","","IEEE","IEEE Journals"
"An Efficient Pansharpening Approach Based on Texture Correction and Detail Refinement","H. Lu; Y. Yang; S. Huang; W. Tu","College of Information Engineering, Jinhua Polytechnic, Jinhua, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Computer Science and Technology, Tiangong University, Tianjin, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Pansharpening aims at fusing a multispectral (MS) image and panchromatic (PAN) image to obtain a high spatial resolution multispectral (HRMS) image. To obtain accurate details and reduce spectral distortion, this letter proposes an efficient pansharpening approach based on texture correction (TC) and detail refinement. First, a TC model is constructed based on spatial and spectral fidelity constraints to obtain a texture image that is highly correlated with the MS image. Second, a detail acquisition model is proposed by the consecutive parameter regression to adaptively refine the extracted details. Finally, the extracted details are injected into the up-sampled MS (UPMS) image to obtain the fused HRMS image. Experimental results demonstrate that the proposed method can obtain the high-quality results with high efficiency.","1558-0571","","10.1109/LGRS.2021.3098112","National Natural Science Foundation of China(grant numbers:62072218,61862030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501252","Detail refinement;pansharpening;parameter regression;texture correction (TC)","Pansharpening;Correlation;Sensors;Distortion;Image sensors;Spatial resolution;Space exploration","geophysical image processing;image fusion;image resolution;image texture;regression analysis;remote sensing","detail acquisition model;consecutive parameter regression;extracted details;MS image;fused HRMS image;high-quality results;pansharpening aims;panchromatic image;high spatial resolution multispectral image;accurate details;spectral distortion;efficient pansharpening approach;detail refinement;TC model;spatial fidelity constraints;spectral fidelity constraints;texture image;PAN;HRMS","","1","","23","IEEE","29 Jul 2021","","","IEEE","IEEE Journals"
"MetaBoost: A Novel Heterogeneous DCNNs Ensemble Network With Two-Stage Filtration for SAR Ship Classification","H. Zheng; Z. Hu; J. Liu; Y. Huang; M. Zheng","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China","IEEE Geoscience and Remote Sensing Letters","16 Jun 2022","2022","19","","1","5","Current synthetic aperture radar (SAR) ship classification research mainly focuses on modifying deep convolutional neural networks (DCNNs) and injecting manual features on DCNNs. Yet, the weak robustness of individual models in high-risk scenarios makes it difficult to gain the trust of SAR experts. In this letter, an automated method of heterogeneous DCNNs model ensemble based on two-stage filtration (MetaBoost) is proposed, effectively achieving robustness and high accuracy recognition on SAR ship classification. The principle of MetaBoost is generating a pool of diverse heterogeneous classifiers, selecting a subset of the most diverse and accurate classifiers, and finally fusing meta-features from the optimal subset. MetaBoost is a self-configuring algorithm that automatically determines the optimal type and the number of base classifiers to be combined. Extensive experiments on the OpenSARShip and FUSAR-Ship datasets show that MetaBoost significantly outperforms individual classifiers, traditional ensemble models, and feature injection techniques.","1558-0571","","10.1109/LGRS.2022.3180793","Natural Science Foundation of China(grant numbers:62172442); Youth Science Foundation of Natural Science Foundation of Hunan Province(grant numbers:2020JJ5775); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791346","Deep learning;heterogeneous ensemble;synthetic aperture radar (SAR);SAR ship classification","Synthetic aperture radar;Marine vehicles;Training;Manuals;Diversity reception;Robustness;Fuses","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;pattern classification;radar imaging;remote sensing by radar;ships;synthetic aperture radar","diverse heterogeneous classifiers;diverse classifiers;accurate classifiers;MetaBoost;base classifiers;FUSAR-Ship datasets;traditional ensemble models;novel heterogeneous DCNNs ensemble network;two-stage filtration;SAR Ship classification;current synthetic aperture radar ship classification research;deep convolutional neural networks;manual features;weak robustness;high-risk scenarios;SAR experts;heterogeneous DCNNs model ensemble;high accuracy recognition;SAR ship classification","","","","18","IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"PMACNet: Parallel Multiscale Attention Constraint Network for Pan-Sharpening","Y. Liang; P. Zhang; Y. Mei; T. Wang","School of Optoelectronic Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Optoelectronic Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Optoelectronic Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","10 May 2022","2022","19","","1","5","Pan-sharpening, a task involving information fusion, entails merging panchromatic (PAN) images with high spatial resolution and low-resolution multispectral (LRMS) images in order to obtain high-resolution multispectral (HRMS) images. Due to deep learning’s excellent regression capabilities, it has recently become the dominating technique for this assignment. Meanwhile, the development of the transformer, a novel deep learning architecture for natural language processing, has provided researchers with new insights. In this letter, we seek to extend transformer’s excellent mechanisms to pixel-level fusion challenges. We designed a parallel convolutional neural network structure for learning both the regions of interest from the LRMS images and the residuals required for regression to HRMS images. Then, in our proposed pixelwise attention constraint (PAC) module, the residuals will be changed utilizing the learned region of interest. In addition, we presented a novel multireceptive-field attention block (MRFAB) to frame our network. Experiments on two datasets also show that our work is better than the mainstream algorithms at both indicators and visualization.","1558-0571","","10.1109/LGRS.2022.3170904","National Natural Science Foundation of China(grant numbers:62075031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764690","Attention mechanism;convolutional neural network(CNN);information fusion;pansharpening;remote sensing","Feature extraction;Picture archiving and communication systems;Task analysis;Transformers;Convolution;Spatial resolution;Natural language processing","correlation methods;filtering theory;geophysical image processing;image enhancement;image fusion;image resolution;learning (artificial intelligence);natural language processing;neural nets","LRMS images;HRMS images;pixelwise attention constraint module;novel multireceptive-field attention block;PMACNet;parallel multiscale attention constraint network;pan-sharpening;information fusion;panchromatic images;high spatial resolution;low-resolution multispectral images;high-resolution multispectral images;dominating technique;transformer;deep learning architecture;natural language processing;pixel-level fusion challenges;parallel convolutional neural network structure","","","","22","IEEE","28 Apr 2022","","","IEEE","IEEE Journals"
"Sentinel-2 Sharpening Using a Single Unsupervised Convolutional Neural Network With MTF-Based Degradation Model","H. V. Nguyen; M. O. Ulfarsson; J. R. Sveinsson; M. D. Mura","Department of Electrical and Electronic Engineering, Nha Trang University, Nha Trang, Vietnam; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Institut Universitaire de France, Paris, France","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Jul 2021","2021","14","","6882","6896","The Sentinel-2 (S2) constellation provides multispectral images at 10 m, 20 m, and 60 m resolution bands. Obtaining all bands at 10 m resolution would benefit many applications. Recently, many model-based and deep learning (DL)-based sharpening methods have been proposed. However, the downside of those methods is that the DL-based methods need to be trained separately for the 20 m and the 60 m bands in a supervised manner at reduced resolution, while the model-based methods heavily depend on the hand-crafted image priors. To break the gap, this article proposes a novel unsupervised DL-based S2 sharpening method using a single convolutional neural network (CNN) to sharpen the 20 and 60 m bands at the same time at full resolution. The proposed method replaces the hand-crafted image prior by the deep image prior (DIP) provided by a CNN structure whose parameters are easily optimized using a DL optimizer. We also incorporate the modulation transfer function-based degradation model as a network layer, and add all bands to both network input and output. This setting improves the DIP and exploits the advantage of multitask learning since all S2 bands are highly correlated. Extensive experiments with real S2 data show that our proposed method outperforms competitive methods for reduced-resolution evaluation and yields very high quality sharpened image for full-resolution evaluation.","2151-1535","","10.1109/JSTARS.2021.3092286","Icelandic Research Fund(grant numbers:207233-051); University of Iceland Doctoral(grant numbers:1547-154305); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464640","Convolutional neural networks (CNNs);image fusion;MTF-based degradation;Sentinel-2 image sharpening;super-resolution;unsupervised deep learning","Spatial resolution;Pansharpening;Electronics packaging;Image fusion;Hyperspectral imaging;Degradation;Training","convolution;feature extraction;image classification;image resolution;learning (artificial intelligence);neural nets;optical transfer function;unsupervised learning","Sentinel-2 sharpening;single unsupervised convolutional neural network;MTF-based degradation model;Sentinel-2 constellation;multispectral images;deep learning-based sharpening methods;reduced resolution;model-based methods;hand-crafted image priors;novel unsupervised DL-based S2 sharpening method;single convolutional neural network;deep image;modulation transfer function-based degradation model;network layer;network input;competitive methods;reduced-resolution evaluation;yields very high quality sharpened image;full-resolution evaluation;size 10.0 m;size 20.0 m;size 60.0 m","","9","","58","CCBY","24 Jun 2021","","","IEEE","IEEE Journals"
"Hyperspectral Super-Resolution of Locally Low Rank Images From Complementary Multisource Data","M. A. Veganzones; M. Simões; G. Licciardi; N. Yokoya; J. M. Bioucas-Dias; J. Chanussot","Grenoble Images Parole Signal Automatique Laboratory, CNRS, Saint-Martin-d’Hères, France; Instituto de Telecomunicações and the Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Grenoble Images Parole Signal Automatique Laboratory, Grenoble Institute of Technology, Saint-Martin-d’Hères, France; Department of Advanced Interdisciplinary Studies, The University of Tokyo, Tokyo, Japan; Instituto de Telecomunicações and the Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Image Processing","8 Dec 2015","2016","25","1","274","288","Remote sensing hyperspectral images (HSIs) are quite often low rank, in the sense that the data belong to a low dimensional subspace/manifold. This has been recently exploited for the fusion of low spatial resolution HSI with high spatial resolution multispectral images in order to obtain super-resolution HSI. Most approaches adopt an unmixing or a matrix factorization perspective. The derived methods have led to state-of-the-art results when the spectral information lies in a low-dimensional subspace/manifold. However, if the subspace/manifold dimensionality spanned by the complete data set is large, i.e., larger than the number of multispectral bands, the performance of these methods mainly decreases because the underlying sparse regression problem is severely ill-posed. In this paper, we propose a local approach to cope with this difficulty. Fundamentally, we exploit the fact that real world HSIs are locally low rank, that is, pixels acquired from a given spatial neighborhood span a very low-dimensional subspace/manifold, i.e., lower or equal than the number of multispectral bands. Thus, we propose to partition the image into patches and solve the data fusion problem independently for each patch. This way, in each patch the subspace/manifold dimensionality is low enough, such that the problem is not ill-posed anymore. We propose two alternative approaches to define the hyperspectral super-resolution through local dictionary learning using endmember induction algorithms. We also explore two alternatives to define the local regions, using sliding windows and binary partition trees. The effectiveness of the proposed approaches is illustrated with synthetic and semi real data.","1941-0042","","10.1109/TIP.2015.2496263","European Research Council (Programme FP7/20072013) through the DECODA Project(grant numbers:2012-ERC-AdG-320594); French National Research Agency through the XIMRI Project(grant numbers:ANR-BLAN-SIMI2-LS-101019-6-01); Portuguese Science and Technology Foundation(grant numbers:SFRH/BD/87693/2012); Portuguese Science and Technology Foundation(grant numbers:PTDC/EEIPRO/1470/2012,UID/EEA/50008/2013); European Research Council through the CHESS Project(grant numbers:2012-ERC-AdG-320684); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312998","Hyperspectral imagery;multispectral imagery;super-resolution;data fusion;dictionary learning;spectral unmixing;binary partition tree;Hyperspectral imagery;multispectral imagery;super-resolution;data fusion;dictionary learning;spectral unmixing;binary partition tree","Hyperspectral imaging;Spatial resolution;Sensors;Signal resolution","geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);regression analysis;remote sensing;trees (mathematics)","hyperspectral super-resolution;locally low rank image;complementary multisource data;remote sensing hyperspectral images;remote sensing HSI;low spatial resolution HSI;high spatial resolution multispectral images;spectral information;low-dimensional subspace;low-dimensional manifold;manifold dimensionality;subspace dimensionality;sparse regression problem;data fusion problem;local dictionary learning;endmember induction algorithms;sliding windows;binary partition trees","","122","","32","IEEE","30 Oct 2015","","","IEEE","IEEE Journals"
"Multiscale Superpixel-Level Subspace-Based Support Vector Machines for Hyperspectral Image Classification","H. Yu; L. Gao; W. Liao; B. Zhang; A. Pižurica; W. Philips","College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Department of Telecommunications and Information Processing, IMEC-TELIN-Ghent University, Ghent, Belgium; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China; Department of Telecommunications and Information Processing, IMEC-TELIN-Ghent University, Ghent, Belgium; Department of Telecommunications and Information Processing, IMEC-TELIN-Ghent University, Ghent, Belgium","IEEE Geoscience and Remote Sensing Letters","25 Oct 2017","2017","14","11","2142","2146","This letter introduces a new spectral-spatial classification method for hyperspectral images. A multiscale superpixel segmentation is first used to model the distribution of classes based on spatial information. In this context, the original hyperspectral image is integrated with segmentation maps via a feature fusion process in different scales such that the pixel-level data can be represented by multiscale superpixel-level (MSP) data sets. Then, a subspace-based support vector machine (SVMsub) is adopted to obtain the classification maps with multiscale inputs. Finally, the classification result is achieved via a decision fusion process. The resulting method, called MSP-SVMsub, makes use of the spatial and spectral coherences, and contributes to better feature characterization. Experimental results based on two real hyperspectral data sets indicate that the MSP-SVMsub exhibits good performance compared with other related methods.","1558-0571","","10.1109/LGRS.2017.2755061","National Natural Science Foundation of China(grant numbers:41722108,41325004,91638201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8057975","Hyperspectral image classification;multiscale superpixel segmentation;subspace projection;support vector machines (SVM)","Hyperspectral imaging;Image segmentation;Support vector machines;Training;Principal component analysis","geophysical image processing;hyperspectral imaging;image classification;image fusion;image segmentation;support vector machines","segmentation maps;original hyperspectral image;spatial information;multiscale superpixel segmentation;spectral-spatial classification method;hyperspectral image classification;multiscale superpixel-level subspace;MSP-SVMsub exhibits good performance;hyperspectral data sets;called MSP-SVMsub;decision fusion process;classification result;multiscale inputs;classification maps;support vector machine;multiscale superpixel-level data sets;pixel-level data;feature fusion process","","61","","15","IEEE","4 Oct 2017","","","IEEE","IEEE Journals"
"Spatial–Temporal Gray-Level Co-Occurrence Aware CNN for SAR Image Change Detection","X. Zhang; X. Su; Q. Yuan; Q. Wang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Air Force Research Institute, Beijing, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Deep learning-based synthetic aperture radar (SAR) image change detection has recently achieved remarkable success due to its great potential for extracting abstract features. However, the existing methods still have room for improvement in dealing with the speckle of SAR images. In this letter, a deep spatial–temporal gray-level co-occurrence aware convolutional neural network (STGCNet) is proposed, which can effectively mine the spatial–temporal information of the bitemporal images and obtain the speckle-robust results by introducing the 3-D gray-level co-occurrence matrix (3-D-GLCM) as auxiliary feature. Specifically, representative features are extracted from original image pairs and their corresponding 3-D-GLCM through two-stream network, followed by an adaptive fusion module to balance the contribution of each branch. Then, the final binary change detection results are obtained by a fully connected layer. The training process relies on reliable labels generated by unsupervised models rather than manually annotated data, and therefore, the proposed STGCNet is practical in reality. Experiments on synthesized and real SAR data sets demonstrate the robustness and competitiveness of the proposed method compared with the state-of-the-art algorithms.","1558-0571","","10.1109/LGRS.2021.3110302","Chang’an University, Xi’an, China, through the National Key Research and Development Program of China(grant numbers:2020YFC1512000); Excellent Youth Foundation of Hubei Scientific Committee(grant numbers:2020CFA051); National Natural Science Foundation of China(grant numbers:61801332,41901302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537572","3-D gray-level co-occurrence matrix (3-D-GLCM), change detection;deep learning;synthetic aperture radar (SAR)","Radar polarimetry;Feature extraction;Training;Speckle;Reliability;Synthetic aperture radar;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image fusion;image representation;radar imaging;speckle;synthetic aperture radar","Co-Occurrence Aware CNN;SAR image change detection;deep learning-based synthetic aperture radar image change detection;abstract features;SAR images;deep spatial-temporal gray-level;aware convolutional neural network;spatial-temporal information;bitemporal images;speckle-robust results;co-occurrence matrix;auxiliary feature;representative features;original image pairs;final binary change detection results;synthesized SAR data sets;real SAR data sets","","2","","18","IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"MSSL: Hyperspectral and Panchromatic Images Fusion via Multiresolution Spatial–Spectral Feature Learning Networks","J. Qu; Y. Shi; W. Xie; Y. Li; X. Wu; Q. Du","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","13 Dec 2021","2022","60","","1","13","The fusion of hyperspectral (HS) and panchromatic (PAN) images aims to generate a fused HS image that combines spectral information of the HS image with spatial information of the PAN image. In this article, we propose a multiresolution spatial–spectral feature learning (MSSL) framework for fusing HS and PAN images. The proposed MSSL transforms the existing deep and complex network into several simple and shallow subnetworks to simplify the feature learning process. MSSL upsamples the HS image while downsamples the PAN image and designs multiresolution 3-D convolutional autoencoder (CAEs) networks with a spectral constraint to learn complete spatial–spectral features of the HS image. MSSL designs multiresolution 2-D CAEs with spatial constraint to extract spatial features of the PAN image, with a low computational cost. In order to effectively generate the pansharpened HS image with high spatial and spectral fidelity, a multiresolution residual network is presented to reconstruct the HS image from the extracted spatial–spectral features. Extensive experiments are conducted on three widely used remote sensing data sets in comparison with state-of-the-art HS image fusion methods, demonstrating the superiority of the proposed MSSL method. Code is available at https://github.com/Jiahuiqu/MSSL.","1558-0644","","10.1109/TGRS.2021.3066374","National Natural Science Foundation of China(grant numbers:61571345,91538101,61501346,61502367,61701360); Higher Education Discipline Innovation Project(grant numbers:B08038); Supported by Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program of the Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2016JQ6023); Open Fund of Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions (Henan University), Ministry of Education(grant numbers:GTYR201904); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9386241","Convolutional autoencoder (CAE);hyperspectral (HS) pansharpening;image fusion;multiresolution;spatial–spectral feature","Feature extraction;Spatial resolution;Image resolution;Pansharpening;Image reconstruction;Data mining;Bayes methods","","","","5","","41","IEEE","25 Mar 2021","","","IEEE","IEEE Journals"
"Fusing Hyperspectral and Multispectral Images via Coupled Sparse Tensor Factorization","S. Li; R. Dian; L. Fang; J. M. Bioucas-Dias","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Instituto de Telecomunicacões, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal","IEEE Transactions on Image Processing","24 May 2018","2018","27","8","4118","4130","Fusing a low spatial resolution hyperspectral image (LR-HSI) with a high spatial resolution multispectral image (HR-MSI) to obtain a high spatial resolution hyperspectral image (HR-HSI) has attracted increasing interest in recent years. In this paper, we propose a coupled sparse tensor factorization (CSTF)-based approach for fusing such images. In the proposed CSTF method, we consider an HR-HSI as a 3D tensor and redefine the fusion problem as the estimation of a core tensor and dictionaries of the three modes. The high spatial-spectral correlations in the HR-HSI are modeled by incorporating a regularizer, which promotes sparse core tensors. The estimation of the dictionaries and the core tensor are formulated as a coupled tensor factorization of the LR-HSI and of the HR-MSI. Experiments on two remotely sensed HSIs demonstrate the superiority of the proposed CSTF algorithm over the current state-of-the-art HSI-MSI fusion approaches.","1941-0042","","10.1109/TIP.2018.2836307","National Natural Science Fund of China for Distinguished Young Scholars(grant numbers:61325007); National Natural Science Fund of China for International Cooperation and Exchanges(grant numbers:61520106001); Fund of Hunan Province for Science and Technology Plan Project(grant numbers:2017RS3024); Portuguese Science and Technology Foundation(grant numbers:UID/EEA/50008/2013,ERANETMED/0001/2014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359412","Super-resolution;fusion;hyperspectral imaging;coupled sparse tensor factorization","Tensile stress;Dictionaries;Spatial resolution;Sparse matrices;Hyperspectral imaging;Estimation","hyperspectral imaging;image fusion;image resolution;image sensors;matrix decomposition;remote sensing;tensors","LR-HSI;coupled sparse tensor factorization;low spatial resolution hyperspectral image;high spatial resolution multispectral image;high spatial resolution hyperspectral image;CSTF method;spatial-spectral correlations;sparse core tensors;3D tensor;image fusion;HR-HSI model;remote sensing;HR-MSI","","248","","57","IEEE","15 May 2018","","","IEEE","IEEE Journals"
"Regularizing Subspace Representation for Fusing Hyperspectral and Multispectral Images","Y. Yang; C. Wang; Y. Feng; J. Zhang; Y. Zheng; S. Chen","Engineering Research Center of Learning-Based Intelligent System, Ministry of Education, Tianjin, China; Engineering Research Center of Learning-Based Intelligent System, Ministry of Education, Tianjin, China; College of Science, Zhejiang University of Technology, Hangzhou, China; Engineering Research Center of Learning-Based Intelligent System, Ministry of Education, Tianjin, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; Engineering Research Center of Learning-Based Intelligent System, Ministry of Education, Tianjin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13 Dec 2021","2021","14","","12273","12286","Fusing a low spatial resolution hyperspectral image (LR-HSI) with a high spatial but low spectral resolution multispectral image (HR-MSI) has been regarded as an effective approach to obtain high resolution HSI (HR-HSI). While matrix factorization-based approaches obtained promising performance for HSI-MSI fusion, the mixed noise introduced into the LR-HSIs inevitably influence the accuracy of the estimated endmembers and representation coefficients. Therefore, to effectively fuse the LR-HSI and HR-MSI information, and meanwhile avoid the impact of mixed noise, in this article, we introduce a denoising regularized subspace representation-based HSI-MSI fusion paradigm. To do so, by assuming that the spectral subspaces of the desired HR-HSI and the acquired LR-HSI are similar, we reformulate the fusion problem as the estimation of the low-dimensional subspace and representation coefficients. More precisely, first, a robust subspace is estimated by the residual statistics on the median filtered images, which can detect pixels contaminated by mixed noise. Then, the well-known block-matching and 3-D filtering regularizer is incorporated into the alternating direction method of multipliers algorithm, which serves as a convex surrogate for estimating the coefficients, thereby promoting the self-similarity of images. Experimental results, carried out on simulated and real datasets, demonstrate the effectiveness of the proposed approach in terms of preserving both spatial details and texture. Furthermore, significantly improved image quality is observed when compared to those of other methods that do not consider noise effects.","2151-1535","","10.1109/JSTARS.2021.3130719","National Natural Science Foundation of China(grant numbers:62020106004,92048301,61876167,62102285); National Key R&D Program of China(grant numbers:2018YFB1305202); Natural Science Foundation of Zhejiang Province, China(grant numbers:LY20F030017,LGG21F030011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9633168","Hyperspectral imaging (HSI);image fusion;self-similarity;subspace representation","Noise reduction;Spatial resolution;Tensors;Sparse matrices;Hyperspectral imaging;Matrix decomposition;Degradation","hyperspectral imaging;image classification;image denoising;image filtering;image fusion;image representation;image resolution;image sensors;matrix decomposition;median filters","spatial resolution hyperspectral image;spectral resolution multispectral image;high resolution HSI;matrix factorization-based approaches;representation coefficients;HR-MSI information;denoising regularized subspace representation-based HSI-MSI fusion paradigm;spectral subspaces;HR-HSI;fusion problem;low-dimensional subspace;robust subspace;median filtered images;spatial details;image texture;image quality;LR-HSI","","1","","62","CCBY","2 Dec 2021","","","IEEE","IEEE Journals"
"An Efficient Pan Sharpening via Texture Based Dictionary Learning and Sparse Representation","S. Ayas; E. T. Gormus; M. Ekinci","Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey; Department of Geomatics Engineering, Karadeniz Technical University, Trabzon, Turkey; Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Jul 2018","2018","11","7","2448","2460","Remote sensing image pan sharpening has attracted researchers' interest, since spatial resolution of multispectral (MS) image can be enhanced by injecting spatial details of a panchromatic image to MS image. In this paper, a novel sparse representation based pan sharpening method is proposed to overcome the disadvantages of traditional methods such as color distortion and blurring effect. This learning based method utilizes a compact single dictionary generated from texture information of high-resolution MS images in order to provide more effective and robust pan sharpening. Two data sets acquired from IKONOS and Quickbird satellites are used to evaluate the performance and robustness of the proposed algorithm. The proposed method is compared with nine well-known component substitution and multiresolution analysis methods and a state-of-art method using several quality measurement indices with references. The experimental results demonstrate that the proposed algorithm is competitive or superior to other conventional methods in terms of visual and quantitative analysis, as it preserves spectral information and provides high quality spatial details in the final product image.","2151-1535","","10.1109/JSTARS.2018.2835573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370102","Dictionary learning;multispectral (MS) image;panchromatic (PAN) image;pan sharpening;sparse representation","Dictionaries;Spatial resolution;Image reconstruction;Transforms;Machine learning;Multiresolution analysis","dictionaries;geophysical image processing;image colour analysis;image fusion;image representation;image resolution;image sensors;image texture;learning (artificial intelligence);remote sensing","color distortion;blurring effect;learning based method;compact single dictionary;texture information;effective pan sharpening;robust pan sharpening;component substitution;multiresolution analysis methods;state-of-art method;high quality spatial details;final product image;efficient pan sharpening;texture based dictionary learning;sensing image pan sharpening;spatial resolution;multispectral image;panchromatic image;MS image;novel sparse representation based pan sharpening method","","18","","34","IEEE","31 May 2018","","","IEEE","IEEE Journals"
"Locally Linear Detail Injection for Pansharpening","J. Liu; Y. Hui; P. Zan","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; Department of Electronic Engineering, University of Maryland at College Park, College Park, MD, USA","IEEE Access","19 Jun 2017","2017","5","","9728","9738","This paper presents a novel method, referred to as locally linear detail injection (LLDI), for the pansharpening problem, which is based on the assumption that the spatial details of each multispectral (MS) band can be locally and linearly represented by the spatial details of panchromatic images. LLDI first exploits such assumption through scales using the modulation transfer function (MTF) of the MS instrument and then performs detail injections into the available low-resolution MS images. Visual analysis and quantitative evaluation performed on QuickBird and WorldView-2 data sets at both reduced and full scales show that the proposed LLDI achieves superior improvements over its baselines.","2169-3536","","10.1109/ACCESS.2017.2710226","National Natural Science Foundation of China(grant numbers:2013CB329404,11401465,11401461,61572393,11690011,11601415,11671317); Fundamental Research Funds for the Central Universities(grant numbers:xjj2014010,2015gjhz15); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7941985","Remote sensing;image fusion;pansharpening","Spatial resolution;Principal component analysis;Indexes;Image fusion;Modulation;Transfer functions","geophysical image processing;image resolution;optical transfer function","WorldView-2 data sets;QuickBird data sets;quantitative evaluation;visual analysis;low-resolution MS images;MS instrument;MTF;modulation transfer function;panchromatic image spatial details;MS band;multispectral band;pansharpening problem;locally linear detail injection","","10","","26","OAPA","7 Jun 2017","","","IEEE","IEEE Journals"
"Deep Multiscale Detail Networks for Multiband Spectral Image Sharpening","X. Fu; W. Wang; Y. Huang; X. Ding; J. Paisley","School of Information Science and Technology, University of Science and Technology of China, Hefei, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; Department of Electrical Engineering, Data Science Institute, Columbia University, New York, NY, USA","IEEE Transactions on Neural Networks and Learning Systems","3 May 2021","2021","32","5","2090","2104","We introduce a new deep detail network architecture with grouped multiscale dilated convolutions to sharpen images contain multiband spectral information. Specifically, our end-to-end network directly fuses low-resolution multispectral and panchromatic inputs to produce high-resolution multispectral results, which is the same goal of the pansharpening in remote sensing. The proposed network architecture is designed by utilizing our domain knowledge and considering the two aims of the pansharpening: spectral and spatial preservations. For spectral preservation, the up-sampled multispectral images are directly added to the output for lossless spectral information propagation. For spatial preservation, we train the proposed network in the high-frequency domain instead of the commonly used image domain. Different from conventional network structures, we remove pooling and batch normalization layers to preserve spatial information and improve generalization to new satellites, respectively. To effectively and efficiently obtain multiscale contextual features at a fine-grained level, we propose a grouped multiscale dilated network structure to enlarge the receptive fields for each network layer. This structure allows the network to capture multiscale representations without increasing the parameter burden and network complexity. These representations are finally utilized to reconstruct the residual images which contain spatial details of PAN. Our trained network is able to generalize different satellite images without the need for parameter tuning. Moreover, our model is a general framework, which can be directly used for other kinds of multiband spectral image sharpening, e.g., hyperspectral image sharpening. Experiments show that our model performs favorably against compared methods in terms of both qualitative and quantitative qualities.","2162-2388","","10.1109/TNNLS.2020.2996498","National Natural Science Foundation of China(grant numbers:81671766,81671674,61671309,61901433,U1605252); Fundamental Research Funds for the Central Universities(grant numbers:20720160075,20720180059); CCF-Tencent Open Fund; Natural Science Foundation of Fujian Province of China(grant numbers:2017J01126); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106801","Deep learning;hyperspectral image (HSI) sharpening;image fusion;pansharpening;superresolution","Satellites;Neural networks;Learning systems;Spatial resolution;Task analysis","geophysical image processing;geophysical signal processing;image coding;image colour analysis;image fusion;image resolution;remote sensing;wavelet transforms","deep multiscale detail networks;multiband spectral image sharpening;network architecture;grouped multiscale dilated convolutions;multiband spectral information;end-to-end network;panchromatic inputs;high-resolution multispectral results;pansharpening;domain knowledge;spectral preservations;spatial preservations;spectral preservation;multispectral images;lossless spectral information propagation;spatial preservation;high-frequency domain;commonly used image domain;conventional network structures;batch normalization layers;spatial information;multiscale contextual features;grouped multiscale dilated network structure;network layer;multiscale representations;residual images;trained network;different satellite images;hyperspectral image sharpening","","58","","69","IEEE","2 Jun 2020","","","IEEE","IEEE Journals"
"Noniterative Hyperspectral Image Reconstruction From Compressive Fused Measurements","J. Bacca; C. V. Correa; H. Arguello","Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Apr 2019","2019","12","4","1231","1239","Compressive spectral imaging (CSI) enables the acquisition of spectral and spatial information of a scene using fewer projected measurements than traditional scanning approaches. Recently, research efforts have focused on obtaining high-resolution spectral images via expensive detectors and sophisticated CSI devices. Alternatively, high-resolution spectral images can be obtained using side information or fusion of compressed measurements, without significantly increasing acquisition costs. Indeed, these approaches retrieve improved resolution images applying iterative and computationally expensive algorithms. This paper proposes the fusion of compressed measurements obtained from two state-of-the-art CSI systems, the single-pixel camera (SPC) and the three-dimensional coded aperture snapshot imaging system (3D-CASSI), such that high-resolution images can be obtained by exploiting detailed spectra provided by the SPC and high spatial resolution of the 3D-CASSI. Specifically, a noniterative reconstruction algorithm is proposed, based on the fact that the spatial-spectral data lie in a low-dimensional subspace. In contrast to related works, the proposed approach relies on implementable CSI systems. Simulations and experimental results show the effectiveness of the proposed method compared to similar approaches, both in reconstruction quality and complexity. Specifically, the proposed method is up to 5.6 times faster than its counterparts and provides comparable quality of attained reconstructions.","2151-1535","","10.1109/JSTARS.2019.2902332","Vice-presidency of research and extension of the Universidad Industrial de Santander(grant numbers:1872); Universidad Industrial de Santander through the program to support postdoctoral researchers of the vice-presidency of research and extension; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672159","Compressive spectral imaging (CSI);high-resolution spectral images;image fusion;noniterative reconstructions","Image coding;Sensors;Image reconstruction;Imaging;Encoding;Hyperspectral imaging","cameras;compressed sensing;geophysical image processing;hyperspectral imaging;image coding;image fusion;image reconstruction;image resolution","noniterative hyperspectral image reconstruction;compressive fused measurements;compressive spectral imaging;spectral information;spatial information;high-resolution spectral images;compressed measurements;high-resolution images;high spatial resolution;noniterative reconstruction algorithm;spatial-spectral data;implementable CSI systems;projected measurements;CSI systems;expensive detectors;single-pixel camera;three-dimensional coded aperture snapshot imaging system;3D-CASSI;CSI devices","","23","","33","IEEE","20 Mar 2019","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Data Fusion via Joint Local-Nonlocal Modeling and Truncation Operator","J. Jiang; Y. Feng; H. Xu; G. Shen; J. Zheng","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Aug 2022","2022","15","","5880","5893","Fusion technology has been the core strategy to obtain a high-spatial-spectral resolution hyperspectral image (HSI). In recent years, few fusion models focused on exploiting the underlying manifold structure in the spatial dimension of the high-resolution HSI. We assume that image patches of high-resolution multispectral image (HR-MSI) and high-resolution hyperspectral image (HR-HSI) share similar manifold structures. Through this bridge, a local-nonlocal low-dimensional manifold defined from HR-MSI is built to favor the patch relationship in HR-HSI, which is further utilized to build a set of orthogonal bases. Subsequently, we introduce a truncation operation based on the adaptively constructed orthogonal bases, which can efficiently preserve the low-frequency information of the image and reduce the interference of noise. Finally, we combine the local-nonlocal manifold term and the truncation operation, coined as LNTM, into a variational super-resolution framework to regularize the latent HR-HSI, leading to a strongly convex function regarding the fusion problem. The cost function is solved by a carefully designed variant of alternating direction method of multipliers algorithm. Different experiments on three public benchmarks validate our algorithm outperforms the recent start-of-the-arts concerning both visual quality and numerical metrics.","2151-1535","","10.1109/JSTARS.2022.3190935","National Key Research and Development Program of China(grant numbers:2018YFE0126100); National Natural Science Foundation of China(grant numbers:61602413,62073295); Open Research Projects of Zhejiang Lab(grant numbers:2019KD0AD01/007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829822","Hyperspectral images (HSIs);image fusion;manifold model;patch-based method;superresolution","Manifolds;Tensors;Spatial resolution;Hyperspectral imaging;Dictionaries;Sparse matrices;Matrix decomposition","convex programming;data fusion;geophysical image processing;hyperspectral imaging;image fusion;image resolution;spectral analysis","variational super-resolution framework;multispectral data fusion;alternating direction method of multipliers algorithm;LNTM;convex function;latent HR-HSI;local-nonlocal manifold term;truncation operation;orthogonal bases;low-dimensional manifold;similar manifold structures;HR-MSI;high-resolution multispectral image;image patches;spatial dimension;high-spatial-spectral resolution hyperspectral image","","1","","43","CCBY","14 Jul 2022","","","IEEE","IEEE Journals"
"Three-Dimensional Through-Wall Sensing of Moving Targets Using Passive Multistatic Radars","G. Gennarelli; R. Solimene; F. Soldovieri; M. G. Amin","Institute for Electromagnetic Sensing of the Environment, Italian National Research Council (CNR), Napoli, Italy; Department of Information Engineering, Second University of Naples, Aversa, Italy; Institute for Electromagnetic Sensing of the Environment, Italian National Research Council (CNR), Napoli, Italy; Center for Advanced Communications, Villanova University, Villanova, PA, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","29 Jan 2016","2016","9","1","141","148","This paper deals with the localization of moving targets in a three-dimensional (3-D) space by a passive multistatic through-wall radar system. Receiving sensors are deployed around the surveillance area to gather the electromagnetic signals scattered from targets, which are illuminated by opportunistic sources present in the environment. A single frequency operation of the system is assumed in view of the narrowband nature of sources commonly encountered in practical situations. The localization task is undertaken by using an inverse source-based approach, where the unknown targets are estimated as the supports of current distributions induced over their volume/surface. A multiarray image fusion strategy is applied to mitigate the lack of resolution associated with the single frequency receiver operation. The change detection is used to detect and localize animate targets in the scene. The feasibility and effectiveness of the proposed approach is assessed by numerical tests based on full-wave synthetic data.","2151-1535","","10.1109/JSTARS.2015.2443078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131448","Change detection;moving targets;passive radar;Change detection;moving targets;passive radar","Radar imaging;Arrays;Image resolution;Receivers;Tomography;Sensors","current distribution;electromagnetic wave scattering;image fusion;inverse problems;numerical analysis;passive radar;radar detection;radar imaging;radar receivers;search radar;sensors","three-dimensional through-wall sensing;moving target localization;three-dimensional space;3D space;passive multistatic through-wall radar system;surveillance;electromagnetic signal scattering;inverse source-based approach;current distribution;multiarray image fusion strategy;single frequency receiver operation;numerical testing;full-wave synthetic data","","14","","34","IEEE","23 Jun 2015","","","IEEE","IEEE Journals"
"A Comprehensive Flexible Spatiotemporal DAta Fusion Method (CFSDAF) for Generating High Spatiotemporal Resolution Land Surface Temperature in Urban Area","C. Shi; N. Wang; Q. Zhang; Z. Liu; X. Zhu","Institute of Earth Surface System and Hazards, College of Urban and Environmental Sciences, Northwest University, Xi'an, China; Institute of Tibetan Plateau Research, Chinese Academy of Sciences, Beijing, China; Institute of Earth Surface System and Hazards, College of Urban and Environmental Sciences, Northwest University, Xi'an, China; Xi'an Research Institute of Surveying and Mapping, Xi'an, China; College of Resources and Environment, University of Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","23 Nov 2022","2022","15","","9885","9899","Spatiotemporal fusion of land surface temperature (LST) has a vital significance in studying the temporal and spatial variation of urban heat islands. But most existing LST fusion methods do not consider the highly heterogeneous urban surface and complexity of the spatial layout. In this study, a Comprehensive Flexible Spatiotemporal DAta Fusion (CFSDAF) method was proposed to generate a high spatiotemporal resolution urban LST image, which was an improvement of the Flexible Spatiotemporal DAta Fusion (FSDAF). The CFSDAF first adjusted the differences between coarse-resolution LST and fine-resolution LST. Then, the visible and near-infrared image of a fine resolution was introduced to execute spectral unmixing and to conduct soft classification, which considered the mixed pixel of fine-resolution LST. The inverse distance weighting (IDW) interpolation was used in improving the computational efficiency, and the constrained least square was selected to better distribute the residual. The performance of CFSDAF was compared with the temporal adaptive reflectance fusion model (STARFM) and FSDAF. The results indicate that the predicted images by CFSDAF are better than STARFM and FSDAF from both visual comparison and quantitative assessment in two experiments, and CFSDAF can reserve more spatial details and accurately restore the spatial continuity of urban LST than others. Moreover, CFSDAF has high computational efficiency and can monitor land cover changes the same as FSDAF. Due to the above advantages, the CFSDAF has great potential for studying spatiotemporal changes of LST and UHI in an urban area.","2151-1535","","10.1109/JSTARS.2022.3220897","Strategic Priority Research Program of the Chinese Academy of Sciences(grant numbers:XDA19070302); National Natural Science Foundation of China(grant numbers:42130516); Second Tibetan Plateau Scientific Expedition and Research Program(grant numbers:2019QZKK020102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946977","Comprehensive flexible spatiotemporal data fusion (CFSDAF);flexible spatiotemporal data fusion (FSDAF);landsat;land surface temperature (LST);MODIS;spatiotemporal fusion","Spatial resolution;Spatiotemporal phenomena;Land surface temperature;Urban areas;Earth;Remote sensing;Artificial satellites","atmospheric temperature;geophysical image processing;image fusion;image resolution;infrared imaging;land cover;land surface temperature;terrain mapping","coarse-resolution LST;Comprehensive Flexible Spatiotemporal DAta Fusion method;fine-resolution LST;high computational efficiency;high spatiotemporal resolution urban LST image;highly heterogeneous urban surface;LST fusion methods;spatiotemporal fusion;spatiotemporal resolution land surface temperature;temporal adaptive reflectance fusion model;urban area;urban heat islands","","","","84","CCBYNCND","11 Nov 2022","","","IEEE","IEEE Journals"
"Pansharpening via Detail Injection Based Convolutional Neural Networks","L. He; Y. Rao; J. Li; J. Chanussot; A. Plaza; J. Zhu; B. Li","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province the College of Electrical and Information Engineering, Hunan University, Changsha, China; CNRS, Grenoble INP, GIPSA-lab, University of Grenoble Alpes, Grenoble, France; Hyperspectral Computing Laboratory, University of Extremadura, Cáceres, Spain; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Apr 2019","2019","12","4","1188","1204","Pansharpening aims to fuse a multispectral (MS) image with an associated panchromatic (PAN) image, producing a composite image with the spectral resolution of the former and the spatial resolution of the latter. Traditional pansharpening methods can be ascribed to a unified detail injection context, which views the injected MS details as the integration of PAN details and bandwise injection gains. In this paper, we design a new detail injection based convolutional neural network (DiCNN) framework for pansharpening with the MS details being directly formulated in end-to-end manners, where the first detail injection based CNN (DiCNN1) mines MS details through the PAN image and the MS image, and the second one (DiCNN2) utilizes only the PAN image. The main advantage of the proposed DiCNNs is that they provide explicit physical interpretations and can achieve fast convergence while achieving high pansharpening quality. Furthermore, the effectiveness of the proposed approaches is also analyzed from a relatively theoretical point of view. Our methods are evaluated via experiments on real MS image datasets, achieving excellent performance when compared to other state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2019.2898574","National Natural Science Foundation of China(grant numbers:61571195,61771496,61633010,61836003); Guangdong Provincial Natural Science Foundation(grant numbers:2016A030313254,2016A030313516,2017A030313382); National Key Research and Development Program of China(grant numbers:2017YFB0502900); China Scholarship Council(grant numbers:201706155080); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667040","Convolutional neural networks (CNNs);detail injection;pansharpening","Spatial resolution;Integrated circuits;Task analysis;Transforms;Remote sensing;Convolutional neural networks","convolutional neural nets;geophysical image processing;geophysical techniques;image fusion;image resolution","DiCNN;detail injection based convolutional neural networks;pansharpening quality;panchromatic image;pansharpening methods;composite image;multispectral image;MS image datasets;PAN image;CNN mines;end-to-end manners;convolutional neural network framework;bandwise injection gains;PAN details;injected MS details;unified detail injection context;spatial resolution;spectral resolution","","92","","53","IEEE","13 Mar 2019","","","IEEE","IEEE Journals"
"Pansharpening via Unsupervised Convolutional Neural Networks","S. Luo; S. Zhou; Y. Feng; J. Xie","Key Laboratory of Dependable Service Computing in Cyber Society, Ministry of Education, Chongqing University and College of Computer Science, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Society, Ministry of Education, Chongqing University and College of Computer Science, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Society, Ministry of Education, Chongqing University and College of Computer Science, Chongqing University, Chongqing, China; School of Bioinformatics, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Aug 2020","2020","13","","4295","4310","Pansharpening is normally utilized to take full advantage of all the available spectral and spatial information that are derived from a low-spatial-resolution multispectral (MS) image and its associated high-spatial-resolution (HR) panchromatic (PAN) image, respectively, producing a fused MS image with high spectral and spatial resolutions. Many methods have been recently developed based on convolutional neural networks (CNNs) for the pansharpening task, but most of them still have some drawbacks: 1) The information cannot efficiently flow in their simple stacked convolutional architectures, thereby hindering the representation ability of the networks. 2) They are commonly trained using supervised learning, which does not only require an extra effort to produce the simulated training data, but can also lead to scale-related problems in the fusion results. In this article, we propose a novel unsupervised CNN-based pansharpening method to overcome these limitations. Specifically, we design an iterative network architecture, in which a PAN-guided strategy and a set of skip connections are adopted to continuously extract and fuse the features from the input, thus enhancing the information reuse and transmission. Besides, we propose a new loss function for unsupervised training in which the relationships between the input MS and PAN images and the fused MS image are used to design the spatial constrains and spectral consistency, respectively. The typical quality index with no-reference is also added to this function to further adjust the spectral and spatial qualities. The designed loss function allows the network to be learned only on input images, without any hand-crafted labels (reference HR MS image). We evaluated the effectiveness of our designed network architecture and the combined loss function, and the experiments testify that our unsupervised strategy can also obtain promising results with minor spectral and spatial distortions compared with other traditional and supervised methods.","2151-1535","","10.1109/JSTARS.2020.3008047","National Natural Science Foundation of China(grant numbers:61762025); Natural Science Foundation of Chongqing(grant numbers:cstc2019jcyj-msxmx0657); National Science Foundation of Chongqing(grant numbers:cstc2018jcyjAX0243); Frontier and Application Foundation Research Program of CQ CSTC(grant numbers:cstc2017jcyjAX0340); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9136909","Convolutional neural networks (CNNs);Pansharpening;unsupervised","Feature extraction;Training;Remote sensing;Fuses;Indexes;Distortion","convolutional neural nets;feature extraction;image fusion;image resolution;supervised learning","fused MS image;spectral resolutions;spatial resolutions;stacked convolutional architectures;high-spatial-resolution panchromatic image;low-spatial-resolution multispectral image;spatial information;spectral information;unsupervised convolutional neural networks;spatial distortions;spectral distortions;input images;spatial qualities;spectral qualities;spectral consistency;spatial constrains;unsupervised training;information reuse;PAN-guided strategy;iterative network architecture;unsupervised CNN-based pansharpening","","30","","54","CCBY","8 Jul 2020","","","IEEE","IEEE Journals"
"Synergetic Classification of Long-Wave Infrared Hyperspectral and Visible Images","X. Lu; J. Zhang; T. Li; G. Zhang","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","7","3546","3557","A decision-level-based synergetic classification method has been proposed to conduct land-cover classification for long-wave infrared (LWIR) and high-resolution visible (VIS) images in this paper. The problem of synergic classification is challenging, since we aim for classification map at the spatial resolution of the VIS image under different imaging modes. The proposed method consists of two stages, i.e., stage of classifications for LWIR hyperspectral (HS) and VIS images separately, and stage of decision-level fusion for both classification results. For LWIR HS image, we have proposed a new semisupervised feature extraction method named as semisupervised local discriminant analysis (SLDA) for SVM classification. In parallel, spatial features which have been extracted from the high-resolution VIS image are used to combine with the spectral features for classification. In the second stage, several common decision-level fusion rules have been employed to integrate both classification results. We also present a context-based opinion pools (CBP) strategy to enhance the classification accuracy. Experiments conducted on the dataset of 2014 IEEE GRSS Data Fusion Contest show the advantage of our proposed SLDA method for HS image, and the effect of spatial-spectral features for high-resolution VIS image. Especially, the presented synergic classification strategy for HS and VIS images has higher overall accuracy and better visual effect than those only using single source image and those compared fusion methods in the experiments.","2151-1535","","10.1109/JSTARS.2015.2442594","National Natural Science Foundation of China(grant numbers:61271348); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131467","Feature extraction;long-wave infrared hyperspectral (LWIR);semisupervised;synergic classification;Feature extraction;long-wave infrared hyperspectral (LWIR);semisupervised;synergic classification","Feature extraction;Image color analysis;Support vector machines;Remote sensing;Accuracy;Thermal sensors;Covariance matrices","feature extraction;geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion;land cover","long-wave infrared hyperspectral image;high-resolution visible image;decision-level-based synergetic classification method;land-cover classification;imaging modes;decision-level fusion stage;LWIR HS image;semisupervised feature extraction method;semisupervised local discriminant analysis;SVM classification;common decision-level fusion rules;context-based opinion pools;CBP strategy;AD 2014;IEEE GRSS Data Fusion Contest;synergic classification strategy","","18","","35","IEEE","23 Jun 2015","","","IEEE","IEEE Journals"
"Infrared Small Target Detection Based on Derivative Dissimilarity Measure","X. Cao; C. Rong; X. Bai","Image Processing Center, Beihang University, Beijing, China; Image Processing Center, Beihang University, Beijing, China; Image Processing Center, Beihang University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 Sep 2019","2019","12","8","3101","3116","Robust detection of a small target and effective suppression of a complex background are important issues in infrared searching and tracking systems. To address these issues, in this paper, a derivative dissimilarity measure (DDM) is proposed for infrared small target detection. First, derivatives of several directions from an infrared image are computed based on the facet model. Then, the derivative feature (DF) of a small target is extracted from each direction. Taking the continuity of background and discontinuity of small targets into consideration, a novel dissimilarity measure is proposed based on the model of derived kernel. Combined with DFs, a DDM is constructed and utilized to enhance the small target and suppress the background in different scenes. Finally, a result map is obtained by fusing sub-maps from all directions. Experimental results on six datasets demonstrate that the proposed small target detection method can not only suppress complex background clutters successfully regardless of noise interference, but can also locate weak and dim targets precisely with a high detection ratio and low false-alarm ratio compared to some other methods.","2151-1535","","10.1109/JSTARS.2019.2920327","National Natural Science Foundation of China(grant numbers:U1736217); Fundamental Research Funds for the Central Universities(grant numbers:YWF-19-BJJ-43); Program for New Century Excellent Talents in Universities(grant numbers:NCET-13-0020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8737845","Derivative dissimilarity measure (DDM);infrared image;small target detection","Object detection;Image edge detection;Atmospheric modeling;Earth;Remote sensing;Target tracking;Computational modeling","clutter;image fusion;infrared imaging;object detection;object tracking;radar clutter","derivative dissimilarity measure;DDM;facet model;derivative feature;novel dissimilarity measure;derived kernel;target detection method;complex background clutters;weak targets;dim targets;high detection ratio;robust small target detection;infrared tracking system;infrared small target detection;effective complex background suppression;infrared searching system","","18","","42","IEEE","17 Jun 2019","","","IEEE","IEEE Journals"
"Band Dependent Spatial Details Injection Based on Collaborative Representation for Pansharpening","M. Imani","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4994","5004","This paper proposes an improved version of the generalized band dependent spatial detail pansharpening method. The proposed method called collaborative representation based band dependent spatial detail (CR-BDSD) benefits the advantages of collaborative representation (CR) to increase the difference between the panchromatic image and the approximation image. Therefore, the highlighted detail image is provided. Then, by adding it to the upsampled multispectral image, the pansharpened image is generated. The use of local spectral-spatial information and also removing noise and redundant spatial features are other benefits of CR in the pansharpening product. According to the experimental results, the CR-BDSD method provides superior fusion results in terms of both quantative and qualitative measures.","2151-1535","","10.1109/JSTARS.2018.2851791","Iran National Science Foundation(grant numbers:96001899); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8410943","Band dependent spatial detail (BDSD);collaborative representation (CR);deblurring;pansharpening","Spatial resolution;Transforms;Collaboration;Estimation;Remote sensing;Sensors","image classification;image fusion;image representation","band dependent spatial details injection;band dependent spatial detail pansharpening;CR-BDSD method;pansharpening product;redundant spatial features;spectral-spatial information;pansharpened image;upsampled multispectral image;highlighted detail image;approximation image;panchromatic image;collaborative representation","","17","","49","IEEE","13 Jul 2018","","","IEEE","IEEE Journals"
"A Self-Supervised Decision Fusion Framework for Building Detection","C. Senaras; F. T. Yarman Vural","Institute of Photogrammetry and GeoInformation (IPI), Leibniz Universität Hannover, Hannover, Germany; Department of Computer Engineering, Middle East Technical University, Ankara, Turkey","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Apr 2016","2016","9","5","1780","1791","In this study, a new building detection framework for monocular satellite images, called self-supervised decision fusion (SSDF) is proposed. The model is based on the idea of self-supervision, which aims to generate training data automatically from each individual test image, without human interaction. This approach allows us to use the advantages of the supervised classifiers in a fully automated framework. We combine our previous supervised and unsupervised building detection frameworks to suggest a self-supervised learning architecture. Hence, we borrow the major strength of the unsupervised approach to obtain one of the most important clues, the relation of a building, and its cast shadow. This important information is, then, used in order to satisfy the requirement of training sample selection. Finally, an ensemble learning algorithm, called fuzzy stacked generalization (FSG), fuses a set of supervised classifiers trained on the automatically generated dataset with various shape, color, and texture features. We assessed the building detection performance of the proposed approach over 19 test sites and compare our results with the state of the art algorithms. Our experiments show that the supervised building detection method requires more than 30% of the ground truth (GT) training data to reach the performance of the proposed SSDF method. Furthermore, the SSDF method increases the F-score by 2 percentage points (p.p.) on the average compared to performance of the unsupervised method.","2151-1535","","10.1109/JSTARS.2015.2463118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7185338","Building detection;decision fusion;ensemble learning;self-supervision;Building detection;decision fusion;ensemble learning;self-supervision","Buildings;Training;Vegetation mapping;Feature extraction;Training data;Remote sensing;Shape","geophysical image processing;image fusion","SSDF method;F-score;fuzzy stacked generalization;unsupervised building detection frameworks;self-supervised decision fusion;monocular satellite images;self-supervised decision fusion framework","","15","","26","IEEE","11 Aug 2015","","","IEEE","IEEE Journals"
"Fusion of Urban TanDEM-X Raw DEMs Using Variational Models","H. Bagheri; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4761","4774","Recently, a new global digital elevation model (DEM) with pixel spacing of 0.4 arcsec and relative height accuracy finer than 2 m for flat areas (slopes <; 20%) and better than 4 m for rugged terrain (slopes > 20%) was created through the TanDEM-X mission. One important step of the chain of global DEM generation is to mosaic and fuse multiple raw DEM tiles to reach the target height accuracy. Currently, weighted averaging (WA) is applied as a fast and simple method for TanDEM-X raw DEM fusion, in which the weights are computed from height error maps delivered from the Integrated TanDEM-X Processor (ITP). However, evaluations show that WA is not the perfect DEM fusion method for urban areas, especially in confrontation with edges such as building outlines. The main focus of this paper is to investigate more advanced variational approaches such as TV-L1 and Huber models. Furthermore, we also assess the performance of variational models for fusing raw DEMs produced from data takes with different baseline configurations and height of ambiguities. The results illustrate the high efficiency of variational models for TanDEM-X raw DEM fusion in comparison to WA. Using variational models could improve the DEM quality by up to 2m, particularly in inner city subsets.","2151-1535","","10.1109/JSTARS.2018.2878608","European Research Council (ERC); European Union's Horizon 2020 research and innovation(grant numbers:ERC-2016-StG-714087); Young Investigators Group “SiPEO”(grant numbers:VH-NG-1018); Bavarian Academy of Sciences and Humanities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540396","Data fusion;Huber model; $ L_1$  norm total variation;TanDEM-X DEM;weight map","Urban areas;Synthetic aperture radar;Remote sensing;Data models;Data fusion;Data integration","digital elevation models;image fusion;radar interferometry;terrain mapping","TanDEM-X mission;global DEM generation;multiple raw DEM tiles;target height accuracy;TanDEM-X raw DEM fusion;height error maps;Integrated TanDEM-X Processor;perfect DEM fusion method;urban areas;Huber models;variational models;raw DEMs;DEM quality;urban TanDEM-X;global digital elevation model;relative height accuracy finer","","2","","36","IEEE","18 Nov 2018","","","IEEE","IEEE Journals"
"C-CNN: Contourlet Convolutional Neural Networks","M. Liu; L. Jiao; X. Liu; L. Li; F. Liu; S. Yang","Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","2 Jun 2021","2021","32","6","2636","2649","Extracting effective features is always a challenging problem for texture classification because of the uncertainty of scales and the clutter of textural patterns. For texture classification, spectral analysis is traditionally employed in the frequency domain. Recent studies have shown the potential of convolutional neural networks (CNNs) when dealing with the texture classification task in the spatial domain. In this article, we try combining both approaches in different domains for more abundant information and proposed a novel network architecture named contourlet CNN (C-CNN). The network aims to learn sparse and effective feature representations for images. First, the contourlet transform is applied to get the spectral features from an image. Second, the spatial-spectral feature fusion strategy is designed to incorporate the spectral features into CNN architecture. Third, the statistical features are integrated into the network by the statistical feature fusion. Finally, the results are obtained by classifying the fusion features. We also investigated the behavior of the parameters in contourlet decomposition. Experiments on the widely used three texture data sets (kth-tips2-b, DTD, and CUReT) and five remote sensing data sets (UCM, WHU-RS, AID, RSSCN7, and NWPU-RESISC45) demonstrate that the proposed approach outperforms several well-known classification methods in terms of classification accuracy with fewer trainable parameters.","2162-2388","","10.1109/TNNLS.2020.3007412","State Key Program of National Natural Science of China(grant numbers:61836009); Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103,61801124); National Natural Science Foundation of China(grant numbers:U1701267,61871310,61573267,61906150); Fund for Foreign Scholars in University Research and Teaching Program’s 111 Project(grant numbers:B07048); Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT 15R53); S&T Innovation Project from the Chinese Ministry of Education; National Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ-659); China Postdoctoral Fund(grant numbers:2019M663641); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145825","Contourlet transform;multiscale;remote sensing scene classification;texture classification","Feature extraction;Computational modeling;Wavelet transforms;Spectral analysis;Geometry;Spatial resolution","computer vision;convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;image representation;image texture;learning (artificial intelligence);object recognition;remote sensing;spectral analysis","C-CNN;contourlet CNN;novel network architecture;abundant information;spatial domain;texture classification task;frequency domain;spectral analysis;textural patterns;contourlet convolutional neural networks;classification accuracy;well-known classification methods;widely used three texture data sets;contourlet decomposition;fusion features;statistical feature fusion;statistical features;CNN architecture;spatial-spectral feature fusion strategy;spectral features;effective feature representations;sparse feature representations","","40","","98","IEEE","21 Jul 2020","","","IEEE","IEEE Journals"
"Multimodal Information Fusion for Weather Systems and Clouds Identification From Satellite Images","C. Bai; D. Zhao; M. Zhang; J. Zhang","Key Laboratory of Visual Media Intelligent Processing Technology of Zhejiang Province, Hangzhou, China; Key Laboratory of Visual Media Intelligent Processing Technology of Zhejiang Province, Hangzhou, China; Key Laboratory of Visual Media Intelligent Processing Technology of Zhejiang Province, Hangzhou, China; School of Control Science and Engineering, Shandong University, Jinan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Sep 2022","2022","15","","7333","7345","Seeing the cloud and then understanding the weather is one of the important means for people to forecast weather. There has been a certain progress in the use of deep learning technology for weather forecasting, especially in the automatic understanding of disaster weather from satellite image, which can be seen as the image classification problem. Publicly available satellite image benchmark database tries to link weather directly with satellite images. However, single image modal is far from enough to correctly identify weather systems and clouds. Thus, we integrate images with meteorological elements, in which five kinds of meteorological elements, such as season, month, date stamp, and geographic longitude, and latitude, are labeled. To effectively use such various modalities for clouds and weather systems identification through satellite image classification tasks, we propose a new satellite image classification framework: multimodal auxiliary network (MANET). MANET consists of three parts: image feature extraction module based on convolutional neural network, meteorological information feature extraction module based on perceptron, and layer-level multimodal fusion. MANET successfully integrates the multimodal information, including meteorological elements and satellite images. The experimental results show that MANET can achieve better weather systems and clouds and land cover classification results based on satellite images.","2151-1535","","10.1109/JSTARS.2022.3202246","National Key Research and Development Program of China(grant numbers:2018YFE0126100); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LR21F020002); Key Research and Development Program of Jiangsu Province(grant numbers:BE2021093); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869335","Clouds;image classification;meteorology;multimodal","Meteorology;Image classification;Task analysis;Satellites;Clouds;Cloud computing;Mobile ad hoc networks","atmospheric techniques;clouds;convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;perceptrons;remote sensing;weather forecasting","weather forecasting;disaster weather;single image modal;meteorological elements;satellite image classification tasks;image feature extraction module;satellite image benchmark database;clouds;date stamp;geographic longitude;multimodal auxiliary network;convolutional neural network;meteorological information feature extraction module;perceptron;layer-level multimodal fusion;land cover classification","","","","76","CCBY","29 Aug 2022","","","IEEE","IEEE Journals"
"Improving Pixel-Based Change Detection Accuracy Using an Object-Based Approach in Multitemporal SAR Flood Images","J. Lu; J. Li; G. Chen; L. Zhao; B. Xiong; G. Kuang","School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, ON, Canada; Department of Geography and Earth Sciences, University of North Carolina at Charlotte, Charlotte, NC, USA; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","7","3486","3496","Most of existing change detection methods could be classified into three groups, the traditional pixel-based change detection (PBCD), the object-based change detection (OBCD), and the hybrid change detection (HCD). Nevertheless, both PBCD and OBCD have disadvantages, and classical HCD methods belong to intuitive decision-level fusion schemes of PBCD and OBCD. There is no optimum HCD method as of yet. Analyzing the complementarities of PBCD and OBCD method, we propose a new unsupervised algorithm-level fusion scheme (UAFS-HCD) in this paper to improve the accuracy of PBCD using spatial context information through: 1) getting the preliminary change mask with PBCD at first to estimate some parameters for OBCD; 2) deriving the unchanged area mask to eliminate the areas without changes, reducing error amplification phenomenon of OBCD; and 3) obtaining the final change mask by means of OBCD method. Taking flood detection with multitemporal SAR data as an example, we compared the new scheme with some classical methods, including PBCD, OBCD, and HCD method and supervised manual trial-and-error procedure (MTEP). The experimental results of flood detection showed that the new scheme was efficient and robust, and its accuracy sometimes can even exceed MTEP.","2151-1535","","10.1109/JSTARS.2015.2416635","China Scholarship Council(grant numbers:201304740123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081753","Change detection;object-based;pixel-based;SAR images;unsupervised change detection;Change detection;object-based;pixel-based;SAR images;unsupervised change detection","Change detection algorithms;Synthetic aperture radar;Histograms;Accuracy;Feature extraction;Backscatter;Surface waves","floods;geophysical image processing;hydrological techniques;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","pixel-based change detection method;object-based change detection method;optimum hybrid change detection method;intuitive decision-level fusion schemes;unsupervised algorithm-level fusion scheme;spatial context information;preliminary change mask;unchanged area mask;error amplification phenomenon;multitemporal SAR data;supervised manual trial-and-error procedure;multitemporal SAR flood images","","56","","51","IEEE","8 Apr 2015","","","IEEE","IEEE Journals"
"CNN-Based Multilayer Spatial–Spectral Feature Fusion and Sample Augmentation With Local and Nonlocal Constraints for Hyperspectral Image Classification","J. Feng; J. Chen; L. Liu; X. Cao; X. Zhang; L. Jiao; T. Yu","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi'an, China; Chinese Academy of Sciences, Key Laboratory of Spectral Imaging Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Apr 2019","2019","12","4","1299","1313","The extraction of joint spatial-spectral features has been proved to improve the classification performance of hyperspectral images (HSIs). Recently, utilizing convolutional neural networks (CNNs) to learn joint spatial-spectral features has become of great interest. However, the existing CNN models ignore complementary spatial-spectral information among the shallow and deep layers. Moreover, insufficient training samples in HSIs afflict these CNN models with overfitting problem. In order to address these problems, a novel CNN method for HSI classification is proposed. It considers multilayer spatial-spectral feature fusion and sample augmentation with local and nonlocal constraints, which is abbreviated as MSLN-CNN. In MSLN-CNN, a triple-architecture CNN is constructed to extract spatial-spectral features by cascading spectral features to dual-scale spatial features from shallow to deep layers. Then, multilayer spatial-spectral features are fused to learn complementary information among the shallow layers with detailed information and the deep layers with semantic information. Finally, the multilayer spatial-spectral feature fusion and classification are integrated into a unified network, and MSLN-CNN can be optimized in the end-to-end way. To alleviate the small sample size problem, the unlabeled samples having high confidences on local spatial constraint and nonlocal spectral constraint are selected and prelabeled. The nonlocal spectral constraint considers the structure information with spectrally similar samples in the nonlocal searching, while the local spatial constraint utilizes the contextual information with spatially adjacent samples. Experimental results on several hyperspectral datasets demonstrate that the proposed method achieves more encouraging classification performance than the current state-of-the-art classification methods, especially with the limited training samples.","2151-1535","","10.1109/JSTARS.2019.2900705","National Natural Science Foundation of China(grant numbers:61871306,61772400,61773304); China Postdoctoral Science Foundation(grant numbers:2015M570816,2016T90892); National Natural Science of China(grant numbers:61836009); Open Research Fund of Key Laboratory of Spectral Imaging Technology, Chinese Academy of Sciences(grant numbers:LSIT201803D); Fundamental Research Funds for the Central Universities(grant numbers:JBX181707); Postdoctoral Research Program in Shaanxi Province of China; Joint Fund of the Equipment Research of Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667086","Convolutional neural networks (CNNs);hyperspectral image (HSI) classification;multilayer feature fusion;nonlocal information;spatial–spectral feature extraction","Feature extraction;Nonhomogeneous media;Training;Deep learning;Data mining;Hyperspectral sensors","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);remote sensing","sample augmentation;MSLN-CNN;triple-architecture CNN;dual-scale spatial features;deep layers;local spatial constraint;nonlocal spectral constraint;spectrally similar samples;spatially adjacent samples;CNN-based multilayer spatial-spectral feature fusion;hyperspectral image classification;complementary spatial-spectral information;HSIs classification;joint spatial-spectral features extraction;convolutional neural networks;shallow layers;semantic information","","56","","49","IEEE","13 Mar 2019","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Method Based on 2D–3D CNN and Multibranch Feature Fusion","Z. Ge; G. Cao; X. Li; P. Fu","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Oct 2020","2020","13","","5776","5788","The emergence of a convolutional neural network (CNN) has greatly promoted the development of hyperspectral image (HSI) classification technology. However, the acquisition of HSI is difficult. The lack of training samples is the primary cause of low classification performance. The traditional CNN-based methods mainly use the 2-D CNN for feature extraction, which makes the interband correlations of HSIs underutilized. The 3-D CNN extracts the joint spectral-spatial information representation, but it depends on a more complex model. Also, too deep or too shallow network cannot extract the image features well. To tackle these issues, we propose an HSI classification method based on the 2D-3D CNN and multibranch feature fusion. We first combine 2-D CNN and 3-D CNN to extract image features. Then, by means of the multibranch neural network, three kinds of features from shallow to deep are extracted and fused in the spectral dimension. Finally, the fused features are passed into several fully connected layers and a softmax layer to obtain the classification results. In addition, our network model utilizes the state-of-the-art activation function Mish to further improve the classification performance. Our experimental results, conducted on four widely used HSI datasets, indicate that the proposed method achieves better performance than the existing alternatives.","2151-1535","","10.1109/JSTARS.2020.3024841","Natural Science Foundation of Jiangsu Province(grant numbers:BK2019020735); National Natural Science Foundation of China(grant numbers:61801222); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200676","Activation function;convolutional neural network (CNN);deep learning;feature fusion;hyperspectral image (HSI) classification","Feature extraction;Three-dimensional displays;Two dimensional displays;Hyperspectral imaging;Machine learning;Solid modeling","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);remote sensing","traditional CNN-based methods;feature extraction;joint spectral-spatial information representation;image features;HSI classification method;multibranch feature fusion;multibranch neural network;network model;HSI datasets;convolutional neural network;hyperspectral image classification technology;low classification performance;2D-3D CNN","","48","","61","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"Deep Multilayer Fusion Dense Network for Hyperspectral Image Classification","Z. Li; T. Wang; W. Li; Q. Du; C. Wang; C. Liu; X. Shi","School of Computer Science, Shenyang Aerospace University, Shenyang, China; School of Computer Science, Shenyang Aerospace University, Shenyang, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; School of Computer Science, Shenyang Aerospace University, Shenyang, China; School of Computer Science, Shenyang Aerospace University, Shenyang, China; School of Computer Science, Shenyang Aerospace University, Shenyang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Apr 2020","2020","13","","1258","1270","Deep spectral-spatial features fusion has become a research focus in hyperspectral image (HSI) classification. However, how to extract more robust spectral-spatial features is still a challenging problem. In this article, a novel deep multilayer fusion dense network (MFDN) is proposed to improve the performance of HSI classification. The proposed MFDN simultaneously extracts the spatial and spectral features based on different sample input sizes, which can extract abundant spectral and spatial correlation information. First, the principal component analysis algorithm is performed on hyperspectral data to extract low-dimensional HSI data, and then the spatial features are extracted from the low-dimensional 3-D HSI data through 2-D convolutional, 2-D dense block, and average-pooling layers. Second, the spectral features are extracted directly from the raw 3-D HSI data by means of 3-D convolutional, 3-D dense block, and average-pooling layers. Third, the spatial and spectral features are fused together through 3-D convolutional, 3-D dense block, and average-pooling layers. Finally, the fused spectral-spatial features are sent into two full connection layers to extract high-level abstract features. Furthermore, densely connected structures can help alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and improve the HSI classification accuracy. The proposed fusion network outperforms the other state-of-the-art methods especially with a small number of labeled samples. Experimental results demonstrate that it can achieve outstanding hyperspectral classification performance.","2151-1535","","10.1109/JSTARS.2020.2982614","National Natural Science Foundation of China(grant numbers:61922013,61902339,61703287); Liaoning Provincial Natural Science Foundation of China(grant numbers:20180550337,2019-MS-254,20180550664); Foundation of Liaoning Educational Committee(grant numbers:JYT19029); Shaanxi Key Laboratory of Intelligent Processing for Big Energy Data in Yanan University(grant numbers:IPBED14); Doctoral Starting up Foundation of Yan'an University(grant numbers:YDBK2019-06); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050922","Deep learning;densely connected convolutional neural network;hyperspectral image (HSI) classification;multilayer feature fusion","Feature extraction;Data mining;Convolution;Hyperspectral imaging;Nonhomogeneous media;Kernel","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;principal component analysis;remote sensing","deep multilayer fusion dense network;hyperspectral image classification;spectral-spatial features fusion;robust spectral-spatial features;MFDN;spectral features;different sample input sizes;abundant spectral correlation information;spatial correlation information;hyperspectral data;low-dimensional HSI data;3-D HSI data;2-D dense block;average-pooling layers;3-D dense block;fused spectral-spatial features;high-level abstract features;densely connected structures;feature propagation;feature reuse;HSI classification accuracy;fusion network;outstanding hyperspectral classification performance","","36","","29","CCBY","30 Mar 2020","","","IEEE","IEEE Journals"
"Multimodal Bilinear Fusion Network With Second-Order Attention-Based Channel Selection for Land Cover Classification","X. Li; L. Lei; Y. Sun; M. Li; G. Kuang","State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Mar 2020","2020","13","","1011","1026","As two different tools for earth observation, the optical and synthetic aperture radar (SAR) images can provide complementary information of the same land types for better land cover classification. However, because of the different imaging mechanisms of optical and SAR images, how to efficiently exploit the complementary information becomes an interesting and challenging problem. In this article, we propose a novel multimodal bilinear fusion network (MBFNet), which is used to fuse the optical and SAR features for land cover classification. The MBFNet consists of three components: the feature extractor, the second-order attention-based channel selection module (SACSM), and the bilinear fusion module. First, in order to avoid the network parameters tempting to ingratiate dominant modality, the pseudo-siamese convolutional neural network (CNN) is taken as the feature extractor to extract deep semantic feature maps of optical and SAR images, respectively. Then, the SACSM is embedded into each stream, and the fine channel-attention maps with second-order statistics are obtained by bilinear integrating the global average-pooling and global max-pooling information. The SACSM can not only automatically highlight the important channels of feature maps to improve the representation power of networks, but also uses the channel selection mechanism to reconfigure compact feature maps with better discrimination. Finally, the bilinear pooling is used as the feature-level fusion method, which establishes the second-order association between two compact feature maps of the optical and SAR streams to obtain the low-dimension bilinear fusion features for land cover classification. Experimental results on three broad coregistered optical and SAR datasets demonstrate that our method achieves more effective land cover classification performance than the state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2020.2975252","National Natural Science Foundation of China(grant numbers:61971426); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9019866","Attention mechanism;bilinear pooling model;convolutional neural network (CNN);feature fusion;land cover classification;multimodal learning","Optical imaging;Feature extraction;Synthetic aperture radar;Optical sensors;Machine learning;Adaptive optics;Optical scattering","convolutional neural nets;feature extraction;geophysical image processing;higher order statistics;image classification;image fusion;image representation;land cover;learning (artificial intelligence);optical radar;radar imaging;remote sensing by radar;synthetic aperture radar","pseudosiamese convolutional neural network;feature extractor;SACSM;global max-pooling information;compact feature maps;feature-level fusion method;low-dimension bilinear fusion features;effective land cover classification performance;multimodal bilinear fusion network;complementary information;second-order attention-based channel selection module;bilinear fusion module;network parameters;earth observation;synthetic aperture radar;SAR images;optical radar;optical imaging mechanisms;MBFNet;SAR feature fusion;optical feature fusion;dominant modality;CNN;deep semantic feature map extraction;channel-attention maps;global average-pooling;second-order statistics;network representation power;second-order association;SAR datasets;coregistered optical datasets","","25","","59","CCBY","2 Mar 2020","","","IEEE","IEEE Journals"
"Spectral-Fidelity Convolutional Neural Networks for Hyperspectral Pansharpening","L. He; J. Zhu; J. Li; D. Meng; J. Chanussot; A. Plaza","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; School of Mathematics and Statistics and the Ministry of Education Key Laboratory of Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, China; GIPSA Lab, Grenoble Institute of Technology, Centre National de la Recherche Scientifique, Université Grenoble Alpes, Grenoble, France; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Oct 2020","2020","13","","5898","5914","Hyperspectral (HS) pansharpening aims at fusing a low-resolution HS (LRHS) image with a panchromatic image to obtain a full-resolution HS image. Most of the existing HS pansharpening approaches are usually based on traditional multispectral pansharpening techniques, which are not especially tailored for two inherent challenges of the HS pansharpening, i.e., much wider spectral range gap between the two kinds of images and having to recover details in many continuous spectral bands simultaneously. In this article, we develop new spectral-fidelity convolutional neural networks (called HSpeNets) for HS pansharpening to keep the fidelity of a pansharpened image to its true spectra as much as possible. Our methods particularly focus on the decomposability of HS details, accordingly synthesizing these details progressively, and meanwhile introduce a spectral-fidelity loss. We give theoretical justifications and provide detailed experimental results, showing the superiorities of the proposed HSpeNets with regard to other state-of-the-art pansharpening approaches.","2151-1535","","10.1109/JSTARS.2020.3025040","National Natural Science Foundation of China(grant numbers:62071184,61771496,61571195); Natural Science Foundation of Guangdong Province(grant numbers:2016A030313254,2016A030313516,2017A030313382); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200718","Convolutional neural networks (CNNs);hierarchical detail reconstruction;hyperspectral image;pansharpening;spectral-fidelity loss","Feature extraction;Spatial resolution;Image reconstruction;Hyperspectral imaging;Kernel","convolutional neural nets;geophysical image processing;geophysical techniques;image fusion;image resolution;remote sensing","multispectral pansharpening;HS pansharpening;spectral-fidelity loss;HS details;pansharpened image;continuous spectral bands;spectral range gap;full-resolution HS image;panchromatic image;low-resolution HS image;hyperspectral pansharpening;spectral-fidelity convolutional neural networks","","20","","47","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"Pansharpening Based on Joint-Guided Detail Extraction","Y. Yang; H. Lu; S. Huang; W. Tu","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; College of Information Engineering, Jinhua Polytechnic, Jinhua, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2021","2021","14","","389","401","Pansharpening is the process of fusing low spatial resolution multispectral (MS) images with high spatial resolution panchromatic (PAN) images, so as to obtain high spatial resolution multispectral (HRMS) images. In this article, a new pansharpening method based on a joint-guided detail extraction is proposed to maintain the spectral and spatial fidelity of a pansharpened image. First, to obtain details that are highly correlated with an MS image, a new PAN image is constructed and guided by the intensity component of the MS image through a variational model. The construction of the new PAN image improves the correlation between the PAN and MS images, and thus reduces the spectral distortion. The variational model is rapidly solved using the least-squares method. Second, to obtain accurate details from the new PAN image, the extraction of the details is guided by each band of the MS image through a regression model, which can further reduce the spatial distortion. The regression model is effectively solved using the gradient descend method. Finally, the details are injected into the upsampled MS image to obtain a fused image. Numerous experiments on the proposed approach were conducted and the results were compared with previous state-of-the-art pansharpening methods. The experimental results verify that the proposed method can efficiently achieve high-quality HRMS images.","2151-1535","","10.1109/JSTARS.2020.3032472","National Natural Science Foundation of China(grant numbers:62072218,61662026,61862030); Natural Science Foundation of Jiangxi Province(grant numbers:20182BCB22006,20181BAB202010,20192ACB20002,20192ACBL21008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234606","Detail extraction;joint-guided;pansharpening;regression model;variational model","Adaptation models;Computational modeling;Spatial resolution;Sensors;Correlation;Distortion","geophysical image processing;image fusion;image resolution;regression analysis;remote sensing","high spatial resolution panchromatic images;high spatial resolution multispectral images;pansharpening method;spectral fidelity;spatial fidelity;pansharpened image;PAN image;variational model;regression model;spatial distortion;upsampled MS image;fused image;state-of-the-art pansharpening methods;high-quality HRMS images;joint-guided detail extraction","","11","","74","CCBY","21 Oct 2020","","","IEEE","IEEE Journals"
"Pansharpening Based on Low-Rank Fuzzy Fusion and Detail Supplement","Y. Yang; C. Wan; S. Huang; H. Lu; W. Wan","The School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; The School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China; The School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Sep 2020","2020","13","","5466","5479","Pansharpening is a technique used to reconstruct a high-resolution (HR) multispectral (MS) image by combining an HR panchromatic (PAN) image with a low-resolution MS image. In recent years, the detail-injection model has demonstrated excellent performance in pansharpening, thus receiving wide attention. Obtaining appropriate details is vital for the detail-injection model. Therefore, this article presents a detail optimization approach to obtain more precise high-frequency (HF) details for pansharpening. The proposed method comprises two steps. In the first step, we design a low-rank fuzzy fusion model to fuse the HF details of the PAN and MS images. In this model, the high frequencies of the PAN and upsampled MS images are decomposed into low-rank and sparse components, and the corresponding fusion rules are designed according to their characteristics. Because some details of the PAN image are replaced with those of the MS image, using them directly as injection details may result in redundant information or spatial distortion. To solve this problem and further optimize the details, in the second step, we construct an adaptive detail supplement model. Based on the similarity and correlation between the fused HF and the original HF of the PAN image, the fused details are supplemented to obtain the final injection details. Experimental results on the IKONOS, Pleiades, QuickBird, and WorldView-2 datasets demonstrate that the proposed algorithm is better than the state-of-the-art methods in maintaining spectral information and improving spatial details.","2151-1535","","10.1109/JSTARS.2020.3022857","National Natural Science Foundation of China(grant numbers:61662026,61862030,62072218); Natural Science Foundation of Jiangxi Province(grant numbers:20182BCB22006,20181BAB202010,20192ACB20002,20192ACBL21008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187996","Detail-injection;detail supplementation;fuzzy logic;pansharpening","Hafnium;Fuses;Optimization;Spatial resolution;Matrix decomposition;Sparse matrices;Fuzzy logic","fuzzy set theory;geophysical image processing;image fusion;image resolution;remote sensing","corresponding fusion rules;PAN image;adaptive detail supplement model;fused details;final injection details;spectral information;spatial details;pansharpening;high-resolution multispectral image;panchromatic image;low-resolution MS image;detail-injection model;appropriate details;detail optimization approach;high-frequency details;low-rank fuzzy fusion model;HF details","","7","","68","CCBY","8 Sep 2020","","","IEEE","IEEE Journals"
"Hierarchical Shrinkage Multiscale Network for Hyperspectral Image Classification With Hierarchical Feature Fusion","H. Gao; Z. Chen; C. Li","College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Jun 2021","2021","14","","5760","5772","Recently, deep learning (DL)-based hyperspectral image classification (HSIC) has attracted substantial attention. Many works based on the convolutional neural network (CNN) model have been certificated to be significantly successful for boosting the performance of HSIC. However, most of these methods extract features by using a fixed convolutional kernel and ignore multiscale features of the ground objects of hyperspectral images (HSIs). Although some recent methods have proposed multiscale feature extraction schemes, more computing and storage resources were consumed. Moreover, when using CNN to implement HSI classification, many methods only use the high-level semantic information extracted from the end of the network, ignoring the edge information extracted from shallow layers of the network. To settle the preceding two issues, a novel HSIC method based on hierarchical shrinkage multiscale network and the hierarchical feature fusion is proposed, with which the newly proposed classification framework can fuse features generated by both of multiscale receptive field and multiple levels. Specifically, multidepth and multiscale residual block (MDMSRB) is constructed by superposition dilated convolution to realize multiscale feature extraction. Furthermore, according to the change of feature size in different stages of the neural networks, we design a hierarchical shrinkage multiscale feature extraction network by pruning MDMSRB to reduce the redundancy of network structure. In addition, to make full use of the features extracted in each stage of the network, the proposed network hierarchically integrates low-level edge features and high-level semantic features effectively. Experimental results demonstrate that the proposed method achieves more competitive performance with a limited computational cost than other state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2021.3083283","National Natural Science Foundation of China(grant numbers:62071168); National Key R&D Program of China(grant numbers:2018YFC1508106); Fundamental Research Funds for the Central Universities(grant numbers:B200202183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440750","Convolutional neural network (CNN);hierarchical feature fusion (HFF);hierarchical shrinkage multiscale network (HSMSN);hyperspectral image classification (HSIC);multidepth and multiscale residual block (MDMSRB)","Feature extraction;Convolution;Hyperspectral imaging;Data mining;Computational modeling;Neural networks;Kernel","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;remote sensing","hierarchical shrinkage multiscale network;hierarchical feature fusion;deep learning;convolutional neural network;CNN;fixed convolutional kernel;multiscale features;hyperspectral images;multiscale feature extraction;HSI classification;high-level semantic information;HSIC;neural networks;hierarchical shrinkage multiscale feature extraction network;low-level edge features;high-level semantic features;hyperspectral image classification;multidepth and multiscale residual block;MDMSRB","","4","","60","CCBY","25 May 2021","","","IEEE","IEEE Journals"
"Pansharpening Based on Variational Fractional-Order Geometry Model and Optimized Injection Gains","Y. Yang; H. Lu; S. Huang; W. Wan; L. Li","School of Computer Science and Technology, Tiangong University, Tianjin, China; College of Information Engineering, Jinhua Polytechnic, Jinhua, China; School of Computer Science and Technology, Tiangong University, Tianjin, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","9 Mar 2022","2022","15","","2128","2141","Pansharpening techniques fuse the complementary information from panchromatic (PAN) and multispectral (MS) images to obtain a high-resolution MS image. However, the majority of existing pansharpening techniques suffer from spectral distortion owing to the low correlation between the MS and PAN images, and difficulties in obtaining appropriate injection gains. To address these issues, this article presents a novel pansharpening method based on the variational fractional-order geometry (VFOG) model and optimized injection gains. Specifically, to improve the correlation between the PAN and MS images, the VFOG model is constructed to generate a refined PAN image with a similar spatial structure to the MS image, while maintaining the gradient information of the original PAN image. Furthermore, to obtain accurate injection gains, and considering that the vegetated and nonvegetated regions should be dissimilar, an optimized adaptive injection gain based on the normalized differential vegetation index is designed. The final pansharpened image is obtained by an injection model using the refined PAN image and optimized injection gains. Extensive experiments on various satellite datasets demonstrate that the proposed method offers superior spectral and spatial fidelity compared to existing state-of-the-art algorithms.","2151-1535","","10.1109/JSTARS.2022.3154642","National Natural Science Foundation of China(grant numbers:62072218,61862030); Natural Science Foundation of Jiangxi Province(grant numbers:20192ACB20002,20192ACBL21008); Natural Science Foundation of Zhejiang Province(grant numbers:LY22F020017); Talent Project of Jiangxi Thousand Talents Program(grant numbers:jxsq2019201056); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722977","Detail injection scheme;injection gain;pansharpening;variational fractional-order geometry model","Pansharpening;Distortion;Transforms;Spatial resolution;Satellites;Image edge detection;Geometry","geophysical image processing;geophysical signal processing;image fusion;image resolution;remote sensing;spectral analysis;vegetation","refined PAN image;optimized injection gains;fractional-order geometry model;pansharpening techniques;high-resolution MS image;appropriate injection gains;novel pansharpening method;VFOG model;original PAN image;accurate injection gains;optimized adaptive injection gain;final pansharpened image;injection model","","2","","53","CCBY","28 Feb 2022","","","IEEE","IEEE Journals"
"Low-Rank Tensor Decomposition With Smooth and Sparse Regularization for Hyperspectral and Multispectral Data Fusion","F. Ma; F. Yang; Y. Wang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electrical and Control Engineering, Liaoning Technical University, Huludao, China; School of Electronic and Information Engineering, Liaoning Technical University, Huludao, China","IEEE Access","21 Jul 2020","2020","8","","129842","129856","The fusion of hyperspectral and multispectral images is an effective way to obtain hyperspectral super-resolution images with high spatial resolution. A hyperspectral image is a datacube containing two spatial dimensions and a spectral dimension. The fusion methods based on non-negative matrix factorization need to reshape the three-dimensional data in matrix form, which will result in the loss of data structure information. Owing to the non-uniqueness of tensor rank and noise inference, there is a lot of redundant information in the spatial and spectral subspaces of tensor decomposition. To address the above problems, this article incorporates smooth and sparse regularization into low-rank tensor decomposition to reformulate a fusion method, in which the logarithmic sum function is adopted to eliminate the effect of redundant information and shadows in both spatial and spectral domains. Moreover, the total-variation-based regularizer is employed to vertically smooth the spectral factor matrix to suppress the noise. Then, the alternating direction multiplier method, as well as the conjugate gradient approach, is utilized to design a set of efficient algorithms by complexity reduction. The experimental results demonstrate that the proposed method can yield better performance than the state-of-the-art benchmark algorithms in most cases, which also verifies the effectiveness of incorporated regularizers in low signal-to-noise ratio environments for hyperspectral super-resolution images.","2169-3536","","10.1109/ACCESS.2020.3009263","Scientific Research Project of Colleges from the Liaoning Department of Education, China(grant numbers:LJ2017QL014,LJ2019QL006,LJ2019JL022); National Basic Research Program of China (973 Program)(grant numbers:2018YFB1403303); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139926","Remote sensing;image fusion;low-rank tensor decomposition;total variation;super-resolution","Tensile stress;Spatial resolution;Hyperspectral imaging;Matrix decomposition;Signal resolution","data structures;hyperspectral imaging;image fusion;image resolution;matrix decomposition;tensors","smooth regularization;sparse regularization;low-rank tensor decomposition;fusion method;redundant information;spatial domains;spectral domains;total-variation-based regularizer;spectral factor matrix;alternating direction multiplier method;regularizers;low signal-to-noise ratio environments;super-resolution images;hyperspectral data fusion;multispectral data fusion;hyperspectral images;multispectral images;high spatial resolution;hyperspectral image;spatial dimensions;spectral dimension;nonnegative matrix factorization need;three-dimensional data;matrix form;data structure information;nonuniqueness;tensor rank;spatial subspaces;spectral subspaces","","5","","56","CCBY","14 Jul 2020","","","IEEE","IEEE Journals"
"Hyperspectral Image Super-Resolution via Deep Spatiospectral Attention Convolutional Neural Networks","J. -F. Hu; T. -Z. Huang; L. -J. Deng; T. -X. Jiang; G. Vivone; J. Chanussot","School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; FinTech Innovation Center, Financial Intelligence and Financial Engineering Research Key Laboratory of Sichuan Province, School of Economic Information Engineering, Southwestern University of Finance and Economics, Chengdu, China; National Research Council–Institute of Methodologies for Environmental Analysis, CNR-IMAA, Tito Scalo, Italy; Université Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, Grenoble, France","IEEE Transactions on Neural Networks and Learning Systems","30 Nov 2022","2022","33","12","7251","7265","Hyperspectral images (HSIs) are of crucial importance in order to better understand features from a large number of spectral channels. Restricted by its inner imaging mechanism, the spatial resolution is often limited for HSIs. To alleviate this issue, in this work, we propose a simple and efficient architecture of deep convolutional neural networks to fuse a low-resolution HSI (LR-HSI) and a high-resolution multispectral image (HR-MSI), yielding a high-resolution HSI (HR-HSI). The network is designed to preserve both spatial and spectral information thanks to a new architecture based on: 1) the use of the LR-HSI at the HR-MSI’s scale to get an output with satisfied spectral preservation and 2) the application of the attention and pixelShuffle modules to extract information, aiming to output high-quality spatial details. Finally, a plain mean squared error loss function is used to measure the performance during the training. Extensive experiments demonstrate that the proposed network architecture achieves the best performance (both qualitatively and quantitatively) compared with recent state-of-the-art HSI super-resolution approaches. Moreover, other significant advantages can be pointed out by the use of the proposed approach, such as a better network generalization ability, a limited computational burden, and the robustness with respect to the number of training samples. Please find the source code and pretrained models from https://liangjiandeng.github.io/Projects_Res/HSRnet_2021tnnls.html.","2162-2388","","10.1109/TNNLS.2021.3084682","National Natural Science Foundation of China(grant numbers:61772003,61702083,12001446,61876203); National Key Research and Development Program of China(grant numbers:2020YFA0714001); Key Projects of Applied Basic Research in Sichuan Province(grant numbers:2020YJ0216); Fundamental Research Funds for the Central Universities(grant numbers:JBK2102001); MIAI@Grenoble Alpes(grant numbers:ANR- 19-P3IA-0003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9449622","Attention module (AM);deep convolutional neural network (CNN);hyperspectral image (HSI) super-resolution;image fusion;pixelShuffle (PS)","Superresolution;Tensors;Spatial resolution;Hyperspectral imaging;Computer architecture;Training data;Learning systems","geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image representation;image resolution;learning (artificial intelligence);neural nets;remote sensing","deep convolutional neural networks;deep spatiospectral attention convolutional neural networks;high-resolution HSI;high-resolution multispectral image;HR-MSI's scale;HSIs;hyperspectral image super-resolution;hyperspectral images;inner imaging mechanism;low-resolution HSI;LR-HSI;network architecture;network generalization ability;output high-quality spatial details;plain mean squared error loss function;recent state-of-the-art HSI super-resolution approaches;satisfied spectral preservation;simple architecture;spatial information thanks;spatial resolution;spectral channels;spectral information thanks","","10","","81","IEEE","9 Jun 2021","","","IEEE","IEEE Journals"
"Pansharpening Based on Adaptive High-Frequency Fusion and Injection Coefficients Optimization","Y. Yang; C. Wan; S. Huang; H. Lu; W. Wan","School of Computer Science and Technology, Tiangong University, Tianjin, China; School of Management Science and Engineering, Harbin Engineering University, Harbin, China; School of Software, Tiangong University, Tianjin, China; College of Information Engineering, Jinhua Polytechnic, Jinhua, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jan 2023","2023","16","","799","811","The purpose of pansharpening is to fuse a multispectral (MS) image with a panchromatic (PAN) image to generate a high spatial-resolution multispectral (HRMS) image. However, the traditional pansharpening methods do not adequately take consideration of the information of MS images, resulting in inaccurate detail injection and spectral distortion in the pansharpened results. To solve this problem, a new pansharpening approach based on adaptive high-frequency fusion and injection coefficients optimization is proposed, which can obtain an accurate injected high-frequency component (HFC) and injection coefficients. First, we propose a multi-level sharpening model to enhance the spatial information of the MS image, and then extract the HFCs from the sharpened MS image and PAN image. Next, an adaptive fusion strategy is designed to obtain the accurate injected HFC by calculating the similarity and difference of the extracted HFCs. Regarding the injection coefficients, we propose injection coefficients optimization scheme based on the spatial and spectral relationship between the MS image and PAN image. Finally, the HRMS image is obtained through injecting the fused HFC into the upsampled MS image with the injection coefficients. Experiments with simulated and real data are performed on IKONOS and Pléiades datasets. Both subjective and objective results indicate that our method has better performance than state-of-the-art pansharpening approaches.","2151-1535","","10.1109/JSTARS.2022.3232145","National Natural Science Foundation of China(grant numbers:62072218,61862030,62261025); Natural Science Foundation of Jiangxi Province(grant numbers:20192ACB20002,20192ACBL21008); Talent project of Jiangxi Thousand Talents Program(grant numbers:jxsq2019201056); Postdoctoral Research Projects of Jiangxi Province(grant numbers:2020KY44); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999343","High-frequency fusion;injection coefficients;multilevel sharpening;pansharpening","Pansharpening;Hybrid fiber coaxial cables;Optimization;Adaptation models;Electronic mail;Fuses;Data mining","geophysical image processing;image fusion;image resolution;remote sensing","accurate injected HFC;accurate injected high-frequency component;adaptive fusion strategy;adaptive high-frequency fusion;HRMS image;inaccurate detail injection;injection coefficients optimization scheme;PAN image;panchromatic image;pansharpened results;pansharpening approach;sharpened MS image;spatial information;spatial relationship;spatial-resolution multispectral image;spectral relationship;state-of-the-art pansharpening approaches;traditional pansharpening methods;upsampled MS image","","","","47","CCBY","26 Dec 2022","","","IEEE","IEEE Journals"
"OFFS-Net: Optimal Feature Fusion-Based Spectral Information Network for Airborne Point Cloud Classification","P. He; K. Gao; W. Liu; W. Song; Q. Hu; X. Cheng; S. Li","North China University of Water Resources and Electric Power, Zhengzhou, China; North China University of Water Resources and Electric Power, Zhengzhou, China; North China University of Water Resources and Electric Power, Zhengzhou, China; North China University of Water Resources and Electric Power, Zhengzhou, China; North China University of Water Resources and Electric Power, Zhengzhou, China; North China University of Water Resources and Electric Power, Zhengzhou, China; North China University of Water Resources and Electric Power, Zhengzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Dec 2022","2023","16","","141","152","Airborne laser scanning (ALS) point cloud classification is a necessary step for understanding 3-D scenes and their applications in various industries. However, the classification accuracy and efficiency are low: 1) point cloud classification methods lack effective filtering of the large number of traditional features, 2) significant category imbalance and coordinate scale problems in ALS point cloud classification. To address these problems, this article proposes an airborne LiDAR point cloud classification method based on deep learning network with optimal feature fusion-based spectral information. This method involves the following steps: First, multiscale point cloud features are extracted, and random forest method is used to filter the features, while spectral information is fused to obtain a point cloud feature dataset with less but better data. Second, to adapt to the characteristics of the airborne point cloud, the improved RandLA-Net can simultaneously retain the advantages of random sampling and learn deeper semantic information by fusing the constructed point cloud features with the local feature aggregation module in the network. Third, four fusion models are constructed to verify the effectiveness of the optimal feature fusion-based spectral information network (OFFS-Net) model for airborne point cloud classification. Last, these models are trained and tested on Vaihingen 3-D dataset. The OFFS-Net achieves overall accuracy score of 84.9% and F1-score of 72.3%, which are better than the mainstream methods. This also validates that the proposed OFFS-Net point cloud classification method, based on the advantages of geometric feature and spectral information is excellent.","2151-1535","","10.1109/JSTARS.2022.3223698","National Natural Science Foundation of China(grant numbers:41901285); Funds for Henan Province(grant numbers:2021HYTP009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956888","Deep learning;LiDAR point cloud;optimal feature;refined classification;spectral information","Point cloud compression;Feature extraction;Laser radar;Atmospheric modeling;Three-dimensional displays;Deep learning;Data mining","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);optical radar;remote sensing by laser beam","airborne laser scanning point cloud classification;airborne LiDAR point cloud classification method;airborne point cloud classification;ALS point cloud classification;classification accuracy;constructed point cloud features;geometric feature;local feature aggregation module;multiscale point cloud features;OFFS-Net achieves overall accuracy score;OFFS-Net point cloud classification method;optimal feature fusion-based spectral information network;point cloud feature dataset;random forest method;traditional features","","","","42","CCBY","21 Nov 2022","","","IEEE","IEEE Journals"
"Cross-Domain Association Mining Based Generative Adversarial Network for Pansharpening","L. He; W. Zhang; J. Shi; F. Li","Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China; Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China; School of Automation Science and Engineering, Xi'an Jiaotong University, Xi'an, China; Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Sep 2022","2022","15","","7770","7783","Multispectral (MS) pansharpening can improve the spatial resolution of MS images, which plays an increasingly important role in agriculture and environmental monitoring. Existing neural network-based methods tend to focus on global features of images, without considering the inherent relationships between similar substances in MS images. However, there is a high probability that different substances at the junction mix with each other, which leads to spectral distortion in the final pansharpened image. In this article, we propose a cross-domain association mining-based generative adversarial network for pansharpening, which consists of a spectral fidelity generator and dual discriminators. In our spectral fidelity generator, the cross-region similarity attention module is designed to establish dependencies between similar substances at different positions in the image, thereby leveraging the similar spectral features to generate pansharpened images with better spectral preservation. To mine the potential relationship between the MS image domain and the panchromatic image domain, we pretrain a spatial information extraction network. The network is then transferred to the dual-discriminator architecture to obtain the spatial information of the pansharpened images more accurately and prevent the loss of spatial details. The experimental results show that our method outperforms several state-of-the-art pansharpening methods in both quantitative and qualitative evaluations.","2151-1535","","10.1109/JSTARS.2022.3204824","National Natural Science Foundation of China(grant numbers:U1903213); Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology(grant numbers:2021SYS-04); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0966); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880485","Deep learning;dual discriminators;image association;multispectral (MS) pansharpening","Pansharpening;Generators;Feature extraction;Spatial resolution;Junctions;Generative adversarial networks;Superresolution","data mining;geophysical image processing;geophysical signal processing;image fusion;image resolution;neural nets;remote sensing","spectral fidelity generator;cross-region similarity attention module;similar substances;similar spectral features;pansharpened images;spectral preservation;MS image domain;panchromatic image domain;spatial information extraction network;state-of-the-art pansharpening methods;multispectral pansharpening;spatial resolution;MS images;neural network-based methods;inherent relationships;different substances;spectral distortion;final pansharpened image;cross-domain association mining-based generative adversarial network","","","","57","CCBY","7 Sep 2022","","","IEEE","IEEE Journals"
"A Deep-Shallow Fusion Network With Multidetail Extractor and Spectral Attention for Hyperspectral Pansharpening","Y. -W. Zhuo; T. -J. Zhang; J. -F. Hu; H. -X. Dou; T. -Z. Huang; L. -J. Deng","Yingcai Honors College, University of Electronic Science and Technology of China, Chengdu, China; Yingcai Honors College, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Science, Xihua University, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13 Sep 2022","2022","15","","7539","7555","Hyperspectral (HS) pansharpening aims at fusing a low-resolution HS image with a high-resolution panchromatic (PAN) image to obtain a HS image with both higher spectral and spatial resolutions. However, existing HS pansharpening algorithms are mainly based on multispectral pansharpening approaches, which cannot perfectly restore much spectral information in the continuous spectral bands and much broader spectral range, leading to spectral distortion and spatial blur. In this paper, we develop a new hyperspectral pansharpening network architecture (called Hyper-DSNet) to fully preserve latent spatial details and spectral fidelity via a deep-shallow fusion structure with multi-detail extractor and spectral attention. First, to solve the problem of spatial ambiguity, five types of high-pass filter templates are used to fully extract the spatial details of the PAN image, constructing a so-called multi-detail extractor. Then, a multi-scale convolution module and a deep-shallow fusion structure, which reduces parameters by decreasing the number of output channels as the network goes deeper, is utilized sequentially. In final, a spectral attention module is conducted to preserve the spectrum for a wealth of spectral information of HS images. Visual and quantitative experiments on three commonly used simulated datasets and one full-resolution dataset demonstrate the effectiveness and robustness of the proposed Hyper-DSNet against the recent state-of-the-art hyperspectral pansharpening techniques. Ablation studies and discussions further verify our contributions, e.g., better spectral preservation and spatial detail recovery.","2151-1535","","10.1109/JSTARS.2022.3202866","Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0501); National Natural Science Foundation of China(grant numbers:12171072); Sichuan Science and Technology Project(grant numbers:2021ZYD0021,2022NSFSC0507); National Key Research and Development Program of China(grant numbers:2020YFA0714001); Southwest Minzu University Research Startup Funds(grant numbers:RQD2021066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870551","Convolutional neural network (CNN);deep- shallow architecture;hyperspectral (HS) pansharpening;multi- detail extractor (MDE);spectral attention (SA)","Pansharpening;Hyperspectral imaging;Bayes methods;Spatial resolution;Feature extraction;Distortion;Image reconstruction","geophysical image processing;image enhancement;image fusion;image representation;image resolution;remote sensing","deep-shallow fusion network;multidetail extractor;low-resolution HS image;high-resolution panchromatic image;multispectral pansharpening approaches;spectral information;continuous spectral bands;spectral distortion;spatial blur;hyperspectral pansharpening network architecture;Hyper-DSNet;latent spatial details;spectral fidelity;deep-shallow fusion structure;spatial ambiguity;high-pass filter templates;PAN image;multiscale convolution module;spectral attention module;full-resolution dataset;hyperspectral pansharpening techniques;spectral preservation;spatial detail recovery;HS pansharpening algorithms","","","","84","CCBY","30 Aug 2022","","","IEEE","IEEE Journals"
"PolSAR Target Detection via Reflection Symmetry and a Wishart Classifier","M. Gu; H. Liu; Y. Wang; D. Yang","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Access","9 Jun 2020","2020","8","","103317","103326","Detection of man-made targets using polarimetric synthetic aperture radar (PolSAR) data has become a promising research area. The reflection symmetry is gradually being applied to man-made target detection algorithms as a physical property that can distinguish between man-made targets and natural clutter. However, the two terms related to the reflection symmetry property in the polarimetric coherency matrix, namely, the C12 and C23 terms, are not fully exploited by the traditional methods. To fully exploit the polarization information of the two terms, an image fusion strategy based on the position and scale information of the scale-invariant feature transform (SIFT) key points is proposed in this paper. Then, a new Wishart classifier based on the patch-level Wishart distance is used to realize automatic target detection of the fused image. The experimental results on measured data show that the proposed method can enhance the contrast between targets and clutter. In addition, the detection performance of the proposed method under different target-to-clutter ratios (TCRs) are verified on the synthetic data and measured data.","2169-3536","","10.1109/ACCESS.2020.2999472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106405","Target detection;reflection symmetry;polarimetric SAR;complex Wishart distribution","Object detection;Radar polarimetry;Synthetic aperture radar;Marine vehicles;Scattering;Detectors;Clutter","feature extraction;geophysical image processing;image classification;image fusion;object detection;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","PolSAR target detection;Wishart classifier;man-made targets;polarimetric synthetic aperture radar data;target detection algorithms;physical property;natural clutter;reflection symmetry property;polarimetric coherency matrix;polarization information;image fusion strategy;scale information;patch-level Wishart distance;automatic target detection;detection performance;target-to-clutter ratios;synthetic data","","2","","29","CCBY","2 Jun 2020","","","IEEE","IEEE Journals"
"Reliability of Infrared Thermography in Detecting Leaks in Buried Water Reticulation Pipes","P. M. Bach; J. K. Kodikara","Monash Infrastructure Research Institute, Department of Civil Engineering, Monash University, Clayton, V.I.C., Australia; Monash Infrastructure Research Institute, Department of Civil Engineering, Monash University, Clayton, V.I.C., Australia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Sep 2017","2017","10","9","4210","4224","The failure of pipelines in water distribution networks results in a significant volume of pristine quality water being lost. Failing infrastructure is often identified through different condition monitoring methods and then maintained. Increasing pressures on a rapidly growing water supply network call for more efficient leak detection techniques. Infrared (IR) thermography has received little attention in the literature, with little effort spent on understanding and assessing its feasibility, pertaining to the detection of buried pipe leaks. The present study develops and tests a passive leak detection methodology using IR thermography (informed by scientific literature) for small diameter buried reticulation pipes. The method was assessed in the field at 27 sites comprising water leaks beneath soil and grass nature strips. Results showed that the passive leak detection method by means of IR thermography was able to clearly identify 59% of leaks, with an additional 22% of sites having notable, yet inconclusive, thermal signatures. Based on image interpretation of field data, the authors propose a set of four types of surface thermal signatures likely to be detected from leaking water reticulation pipes, and a conceptual representation of subsurface processes that would produce these signatures. The study creates opportunities for improving the practicality of IR thermography as a leak detection tool and its combination with other techniques for better automation and data fusion.","2151-1535","","10.1109/JSTARS.2017.2708817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7946278","Engineering;leak detection;pipelines;remote sensing;water resources","Leak detection;Pipelines;Water resources;Water conservation;Temperature measurement;Temperature sensors","civil engineering computing;condition monitoring;image fusion;infrared imaging;leak detection;pipelines;water supply","infrared thermography reliability;IR thermography;leak detection;buried water reticulation pipes;pipeline failure;water distribution networks;water loss;infrastructure failure;condition monitoring;water supply network;image interpretation;surface thermal signatures;data fusion;engineering;public water supply","","27","","52","IEEE","12 Jun 2017","","","IEEE","IEEE Journals"
"A Center Location Algorithm for Tropical Cyclone in Satellite Infrared Images","P. Wang; P. Wang; C. Wang; Y. Yuan; D. Wang","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Jun 2020","2020","13","","2161","2172","The precise location of the tropical cyclone (TC) center is critical for intensity estimation and trajectory prediction. Due to the variability of TC morphology and structure, there are still some challenges in locating its center automatically. The ability of the deep convolutional network to capture multilevel structural features of the images is exploited. Furthermore, a two-step scheme for locating the TC center is proposed, which contains the object detection for TCs with deep learning and the comprehensive decision for TC centers. In the object detection, considering the statistical scale distribution of TCs, the global and local features extracted by the network are combined to form the fusion feature maps through the upsampling and concatenation. The changes in the TC scale are accommodated by two different scale outputs. A high detection rate and a low false alarm rate are obtained with the object detection, which provides an initial position for the TC center. Within the scope of the TCs, the final position of the center is obtained through segmentation, edge detection, circle fitting, and comprehensive decision. The experimental results show that the average latitude and longitude error of the proposed method is about 0.237$^\circ$. For the TC in the initial phase or dissipation stage, the location results are usually superior to the results of the comparison algorithms.","2151-1535","","10.1109/JSTARS.2020.2995158","General Program of National Natural Science Foundation of China(grant numbers:61972282); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9095212","Image processing (IP);neural networks;object detection;remote sensing;satellite applications","Feature extraction;Tropical cyclones;Clouds;Object detection;Satellites;Satellite broadcasting","atmospheric techniques;edge detection;feature extraction;geophysical image processing;image fusion;infrared imaging;learning (artificial intelligence);object detection;storms;weather forecasting","center location algorithm;satellite infrared images;tropical cyclone center;intensity estimation;trajectory prediction;deep convolutional network;multilevel structural features;TC center;object detection;global features;local features;fusion feature maps;TC scale;statistical scale distribution;edge detection;circle fitting","","14","","44","CCBY","18 May 2020","","","IEEE","IEEE Journals"
"Hyperspectral Image Superresolution Using Unidirectional Total Variation With Tucker Decomposition","T. Xu; T. -Z. Huang; L. -J. Deng; X. -L. Zhao; J. Huang","Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China; Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","13 Aug 2020","2020","13","","4381","4398","The hyperspectral image superresolution (HSI-SR) problem aims to improve the spatial quality of a low spatial resolution HSI by fusing the LR-HSI with the corresponding high spatial resolution multispectral image. The generated HSI with high spatial quality, i.e., the target high spatial resolution hyperspectral image (HR-HSI), generally has some fundamental latent properties, e.g., the sparsity, and the piecewise smoothness along with the three modes (i.e., width, height, and spectral mode). However, limited works consider both properties in the HSI-SR problem. In this work, a novel unidirectional total variation (TV) based approach is been proposed. On the one hand, we consider that the target HR-HSI exhibits both the sparsity and the piecewise smoothness on the three modes, and they can be depicted well by the ℓ1-norm and TV, respectively. On the other hand, we utilize the classical Tucker decomposition to decompose the target HR-HSI (a three-mode tensor) as a sparse core tensor multiplied by the dictionary matrices along with the three modes. Especially, we impose the ℓ1-norm on core tensor to characterize the sparsity and the unidirectional TV on three dictionaries to characterize the piecewise smoothness. The proximal alternating optimization scheme and the alternating direction method of multipliers are used to iteratively solve the proposed model. Experiments on three common datasets illustrate that the proposed approach has better performance than some current state-of-the-art HSI-SR methods. Please find source code from: https://liangjiandeng.github.io/.","2151-1535","","10.1109/JSTARS.2020.3012566","National Natural Science Foundation of China(grant numbers:61772003,61702083,61876203); Applied Basic Research Program of Sichuan Province(grant numbers:2020YJ0216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151315","Hyperspectral image superresolution (HSI-SR);image fusion;piecewise smoothness;sparsity;unidirectional total variation (TV)","Tensile stress;Spatial resolution;Dictionaries;Matrix decomposition;Hyperspectral imaging","geophysical image processing;hyperspectral imaging;image resolution;matrix algebra;optimisation;remote sensing;tensors","piecewise smoothness;spectral mode;norm TV;classical Tucker decomposition;sparse core tensor;unidirectional TV;hyperspectral image superresolution problem;low spatial resolution HSI;LR-HSI;high spatial resolution multispectral image;high spatial quality;high spatial resolution hyperspectral image;HSI-SR methods;unidirectional total variation based approach","","32","","50","CCBY","28 Jul 2020","","","IEEE","IEEE Journals"
"Two-Stage Pansharpening Based on Multi-Level Detail Injection Network","J. Hu; C. Du; S. Fan","Key Laboratory of Electric Power Robot of Hunan Province, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; Key Laboratory of Electric Power Robot of Hunan Province, Changsha University of Science and Technology, Changsha, China","IEEE Access","2 Sep 2020","2020","8","","156442","156455","Pansharpening is an effective technology to obtain high resolution multispectral (HRMS) images by fusing low resolution multispectral (LRMS) images and high resolution panchromatic (PAN) images. With the rapid development of deep learning, some pansharpening methods based on deep learning have been proposed. Although fused images are greatly improved, there are still some areas for improvement. For example, the spectral preservation is not good enough and the details of fused images are not rich enough. To address the above problems, a two-stage pansharpening method based on convolutional neural network (CNN) is proposed. In the first stage, image super-resolution technology with residual block is used to enhance LRMS. In order to preserve spectra, inspired by the SAM (spectral angle mapper) index, a new spectral loss function is proposed. The second stage is the fusion stage. Detail injection block is proposed by combining detail injection and CNN in this stage. Experiments on WorldView2 and GeoEye1 images demonstrate that our fused images present more spatial details and better spectra by comparing with existing methods.","2169-3536","","10.1109/ACCESS.2020.3019201","National Natural Science Foundation of China(grant numbers:61601061,61971071); Scientific Research Fund of Hunan Provincial Education Department(grant numbers:14B006); Open Research Fund of Key Laboratory of Electric Power Robot of Hunan Province(grant numbers:PROF1902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9174972","Pansharpening;detail injection block;residual learning;convolutional neural network","Spatial resolution;Remote sensing;Indexes;Transforms;Distortion;Convolutional neural networks","convolutional neural nets;geophysical image processing;image fusion;image resolution;remote sensing","deep learning;spectral preservation;two-stage pansharpening method;convolutional neural network;image super-resolution technology;LRMS;SAM index;fusion stage;detail injection block;GeoEye1 images;multilevel detail injection network;high resolution multispectral images;low resolution multispectral images;high resolution panchromatic images;pansharpening methods;CNN;spectral angle mapper index","","7","","58","CCBY","24 Aug 2020","","","IEEE","IEEE Journals"
"A New Approach for Surface Urban Heat Island Monitoring Based on Machine Learning Algorithm and Spatiotemporal Fusion Model","Y. Yao; C. Chang; F. Ndayisaba; S. Wang","Key Laboratory of Pattern Recognition and Intelligent Information Processing, Institutions of Higher Education of Sichuan Province, Chengdu University, Chengdu, China; State Key Laboratory of Desert and Oasis Ecology, Xinjiang Institute of Ecology and Geography, Chinese Academy of Science, Urumqi, China; Department of Geography, Protestant Institute of Arts and Social Sciences, Huye, Rwanda; School of Management, Xinjiang Agricultural University, Urumqi, China","IEEE Access","17 Sep 2020","2020","8","","164268","164281","Land surface temperature (LST) is an important indicator for assessing the surface urban heat island (SUHI) effect. This paper presents a novel approach to derive LST estimates by integrating machine learning algorithm and spatiotemporal fusion model at high spatial and temporal resolution. The spatial resolutions of Landsat TM and Landsat 8 LST data were first downscaled using random forest (RF) algorithm from 120 m and 100 m, respectively, to 30 m. The resultant LST data were fused with MODerate-resolution Imaging Spectroradiometer (MODIS) LST data, by means of the Flexible Spatiotemporal Data Fusion method (FSDAF), in order to generate high spatiotemporal resolution summer daytime LST data covering the center of Chengdu city in China. The proposed new method was used to estimate the spatiotemporal variations of the summer daytime SUHI from 2009 to 2018 over Chengdu city. Results show that: (1) RF performs way better than the classical downscaling algorithm-thermal sharpening algorithm (TsHARP) for LST, and produces higher accuracy for different land covers; (2) the fused high spatiotemporal resolution summer daytime LST values were evaluated with in situ LST obtained from Chengdu Meteorological Office and the final validation results indicated that the proposed method, in generating LST dataset, can provide more details of urban thermal environment and produce higher accuracy than the traditional FSDAF; (3) significantly increasing trends of summer daytime SUHI intensity (SUHII) in the study area were observed. SUHII increased from 2.78 °C in 2009 to 4.04 °C in 2018. The highest and lowest summer daytime LST estimates were recorded over impervious surface area (ISA) and waters, respectively.","2169-3536","","10.1109/ACCESS.2020.3022047","National Basic Research Program of China (973 Program)(grant numbers:2016YFC0500201-02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9187236","Machine learning algorithm;land surface temperature;spatiotemporal fusion;surface urban heat island (SUHI)","Remote sensing;Earth;Artificial satellites;Spatial resolution;Land surface temperature;Urban areas;Spatiotemporal phenomena","atmospheric boundary layer;atmospheric temperature;geophysical image processing;image fusion;infrared imaging;land surface temperature;learning (artificial intelligence);remote sensing;spatiotemporal phenomena","surface urban heat island monitoring;machine learning algorithm;spatiotemporal fusion model;land surface temperature;surface urban heat island effect;high spatial resolution;temporal resolution;Landsat TM;Landsat 8 LST data;random forest algorithm;MODerate-resolution Imaging Spectroradiometer LST data;Flexible Spatiotemporal Data Fusion method;high spatiotemporal resolution summer daytime LST data;Chengdu city;spatiotemporal variations;classical downscaling algorithm-thermal sharpening algorithm;land covers;fused high spatiotemporal resolution summer daytime LST values;LST dataset;urban thermal environment;summer daytime SUHI intensity;highest summer daytime LST estimates;lowest summer daytime LST estimates","","5","","46","CCBY","7 Sep 2020","","","IEEE","IEEE Journals"
"Deep Hierarchical Vision Transformer for Hyperspectral and LiDAR Data Classification","Z. Xue; X. Tan; X. Yu; B. Liu; A. Yu; P. Zhang","PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China","IEEE Transactions on Image Processing","15 Apr 2022","2022","31","","3095","3110","In this study, we develop a novel deep hierarchical vision transformer (DHViT) architecture for hyperspectral and light detection and ranging (LiDAR) data joint classification. Current classification methods have limitations in heterogeneous feature representation and information fusion of multi-modality remote sensing data (e.g., hyperspectral and LiDAR data), these shortcomings restrict the collaborative classification accuracy of remote sensing data. The proposed deep hierarchical vision transformer architecture utilizes both the powerful modeling capability of long-range dependencies and strong generalization ability across different domains of the transformer network, which is based exclusively on the self-attention mechanism. Specifically, the spectral sequence transformer is exploited to handle the long-range dependencies along the spectral dimension from hyperspectral images, because all diagnostic spectral bands contribute to the land cover classification. Thereafter, we utilize the spatial hierarchical transformer structure to extract hierarchical spatial features from hyperspectral and LiDAR data, which are also crucial for classification. Furthermore, the cross attention (CA) feature fusion pattern could adaptively and dynamically fuse heterogeneous features from multi-modality data, and this contextual aware fusion mode further improves the collaborative classification performance. Comparative experiments and ablation studies are conducted on three benchmark hyperspectral and LiDAR datasets, and the DHViT model could yield an average overall classification accuracy of 99.58%, 99.55%, and 96.40% on three datasets, respectively, which sufficiently certify the effectiveness and superior performance of the proposed method.","1941-0042","","10.1109/TIP.2022.3162964","National Natural Science Foundation of China(grant numbers:42101458,41801388,42130112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755059","Hyperspectral image;light detection and ranging;joint classification;vision transformer;convolutional vision transformer;cross attention fusion","Feature extraction;Transformers;Hyperspectral imaging;Laser radar;Data mining;Collaboration;Data models","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image registration;optical radar;pattern classification;remote sensing;sensor fusion","deep hierarchical vision transformer architecture;light detection;current classification methods;heterogeneous feature representation;information fusion;multimodality remote sensing data;LiDAR data;collaborative classification accuracy;powerful modeling capability;long-range dependencies;strong generalization ability;transformer network;spectral sequence transformer;spectral dimension;hyperspectral images;diagnostic spectral bands;land cover classification;spatial hierarchical transformer structure;hierarchical spatial features;heterogeneous features;multimodality data;contextual aware fusion mode;collaborative classification performance;LiDAR datasets;DHViT model","","11","","48","IEEE","11 Apr 2022","","","IEEE","IEEE Journals"
"Triplet Hybrid Discriminative Fields Model Based on Bayesian Fusion for Unsupervised Nonstationary SAR Image Multiclass Segmentation","P. Zhang; M. Li; W. Song; Y. Wu; L. An","National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; Remote Sensing Image Processing and Fusion Group, School of Electronics Engineering, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4848","4861","The discriminative random fields (DRF) model is suitable for analyzing images with complex textural structures and has achieved promising results in image segmentation. However, the DRF model does not consider the nonstationarity of synthetic aperture radar (SAR) images and lacks the ability to model SAR scattering statistics in nonstationary SAR image segmentation. In this paper, we propose a triplet hybrid discriminative random fields (THDF) model based on Bayesian fusion. According to its semantic structure, the THDF model belongs to hybrid discriminative models, and it provides the following promising contributions to nonstationary SAR image segmentation while inheriting the advantages of the discriminative models: first, it takes the nonstationarity of SAR images into account from the perspective of their texton appearances, and thus regulates the local label interaction patterns and considers the distribution differences of the congeneric image features in different stationary parts; and second, for nonstationary SAR images, it performs a fusion-type treatment of the nonstationary textural features and the SAR scattering statistics based on Bayesian fusion and, thus, captures the nonstationary information from SAR data in a more complete manner. The effectiveness of the proposed model is demonstrated through applications to both synthetic images and real SAR image segmentations.","2151-1535","","10.1109/JSTARS.2018.2875935","National Natural Science Foundation of China(grant numbers:61871312,61772390); Aeronautical Science Foundation of China(grant numbers:2016081011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516305","Bayesian fusion;nonstationary property;synthetic aperture radar (SAR) image multiclass segmentation;triplet hybrid discriminative random fields (THDF)","Synthetic aperture radar;Image segmentation;Hidden Markov models;Data models;Bayes methods;Scattering;Analytical models","Bayes methods;image fusion;image segmentation;image texture;radar imaging;synthetic aperture radar","SAR scattering statistics;texton appearances;local label interaction patterns;synthetic images;nonstationary textural features;nonstationary SAR images;congeneric image features;hybrid discriminative models;THDF model;triplet hybrid discriminative random fields;nonstationary SAR image segmentation;synthetic aperture radar images;DRF model;complex textural structures;discriminative random fields model;unsupervised nonstationary SAR image multiclass segmentation;Bayesian fusion","","5","","36","IEEE","31 Oct 2018","","","IEEE","IEEE Journals"
"Development of LR-PCA Based Fusion Approach to Detect the Changes in Mango Fruit Crop by Using Landsat 8 OLI Images","H. C. Verma; T. Ahmed; S. Rajan; M. K. Hasan; A. Khan; H. Gohel; A. Adam","ICAR-Central Institute for Subtropical Horticulture, Lucknow, Uttar Pradesh, India; Department of Computer Application, Integral University, Lucknow, Uttar Pradesh, India; ICAR-Central Institute for Subtropical Horticulture, Lucknow, Uttar Pradesh, India; Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Department of Computer Application, Integral University, Lucknow, Uttar Pradesh, India; Department of Computer Science, University of Houston–Victoria, Victoria, TX, USA; Faculty of Information Science and Technology, Universiti Kebangsaan Malaysia, Bangi, Malaysia","IEEE Access","19 Aug 2022","2022","10","","85764","85776","Change detection (CD) is the process of detecting changes from multi-temporal satellite images that have undergone spatial changes due to natural phenomena and/or human-induced activities. Mango is a major fruit crop in India, but detection of changes in mango crops remains a challenging task because the reason that many perennial crops have similar reflectance profiles. Therefore, a potent change detection technique is required for different applications such as the rate of deforestation, urban developments, damage evaluation, and resource monitoring. Compared to annual and seasonal crops, relatively few studies have been conducted on change detection in perennial fruit crops. In this study, a novel log ratio change detection technique (i.e., LR-PCA) is developed using the fusion of log-ratio (LR) and principal component analysis (PCA) images derived from bi-temporal soil adjusted vegetation index (SAVI) images to extract meaningful information and detect temporal changes in the mango fruit crop areas with high change detection accuracy. The proposed approach comprised two steps: (1) SAVI images from 2015 and 2019 were used to retrieve the log-ratio (LR) and principal component (PC) images, respectively, and both the images were fused by applying the pixel-by-pixel fusion approach. (2) Fused images were classified into three classes: “positive change”, “no change”, and “negative change” using a derived threshold value. The results show that the LR-PCA method of change detection yields a high change detection accuracy of 92% in comparison with the other change detection methods viz. vegetation image differencing, image ratioing, PCA, and log-ratio. To validate the adaptability of the proposed algorithm, experiments with two sets of bi-temporal SAVI indices images belonging to the Sitapur district of Uttar Pradesh State determine that the proposed change detection method performs well as compared to the existing individual methods for detection of changes in mango fruit crop. The proposed method is expected to be useful for detecting changes in the area of perennial crops. In the future, an accurate and efficient change detection analysis may be helpful for developing a real-time mango fruit crop monitoring system at the national level.","2169-3536","","10.1109/ACCESS.2022.3194000","University Kembangan Malaysia(grant numbers:FRGS/1/2020/ICT03/UKM/02/6); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9840352","Landsat 8 OLI;change detection;log ratio;principal component analysis;mango crop","Crops;Earth;Principal component analysis;Remote sensing;Artificial satellites;Vegetation mapping;Satellite broadcasting","crops;geophysical image processing;geophysical techniques;image fusion;principal component analysis;remote sensing;soil","LR-PCA method;image ratioing;log-ratio;real-time mango fruit crop monitoring system;Landsat 8 OLI images;multitemporal satellite images;annual crops;seasonal crops;mango fruit crop areas;bitemporal soil adjusted vegetation index images;pixel-by-pixel fusion;Sitapur district;Uttar Pradesh State;India;AD 2015;AD 2019","","1","","43","CCBY","26 Jul 2022","","","IEEE","IEEE Journals"
"Multiview Attention CNN-LSTM Network for SAR Automatic Target Recognition","C. Wang; X. Liu; J. Pei; Y. Huang; Y. Zhang; J. Yang","Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Dec 2021","2021","14","","12504","12513","Synthetic aperture radar (SAR) is a microwave remote sensing system. It has a broad scope of applications in both military and civilian fields. Benefited from the latest advances in deep learning, SAR automatic target recognition technology has made an excellent breakthrough However, most existing methods ignore the large variation of scattering characteristics of SAR target images with different azimuths, which limits the performance and practical application. The SAR images under different azimuths contain distinct feature information, and the images under adjacent azimuths are correlated in terms of features. Therefore, extracting the feature information of images under adjacent azimuths and leveraging their correlation can improve the recognition performance. In this article, we proposed a multiview attention convolutional neural network with long short-term memory (LSTM) network to extract and fuse the features from images with adjacent azimuths. It adopts multiple convolutional modules to extract deep features from each single-view SAR image and spatial attention module to locate the information of the target and suppress the useless noise. Then, the LSTM module performs feature fusion based on the correlation of features obtained from adjacent azimuths. Finally, based on these multiview images, deep features are extracted and fused to obtain precise recognition results. Experiments are performed on the moving and stationary target acquisition and recognition dataset, and the results have verified the effectiveness of the proposed method.","2151-1535","","10.1109/JSTARS.2021.3130582","National Natural Science Foundation of China(grant numbers:61901091,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627549","Convolutional neural network (CNN);deep learning;long short-term memory (LSTM);multiview;synthetic aperture radar automatic target recognition (SAR ATR)","Radar polarimetry;Feature extraction;Azimuth;Synthetic aperture radar;Target recognition;Image recognition;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image fusion;image representation;radar computing;radar imaging;radar target recognition;recurrent neural nets;synthetic aperture radar","recognition dataset;stationary target acquisition;multiview images;LSTM module performs;spatial attention module;single-view SAR image;deep features;multiple convolutional modules;short-term memory network;multiview attention convolutional neural network;recognition performance;adjacent azimuths;distinct feature information;SAR images;SAR target images;SAR automatic target recognition technology;deep learning;civilian fields;military fields;microwave remote sensing system;synthetic aperture radar;multiview attention CNN-LSTM network","","4","","20","CCBY","25 Nov 2021","","","IEEE","IEEE Journals"
"DPFL-Nets: Deep Pyramid Feature Learning Networks for Multiscale Change Detection","M. Yang; L. Jiao; F. Liu; B. Hou; S. Yang; M. Jian","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","IEEE Transactions on Neural Networks and Learning Systems","27 Oct 2022","2022","33","11","6402","6416","Due to the complementary properties of different types of sensors, change detection between heterogeneous images receives increasing attention from researchers. However, change detection cannot be handled by directly comparing two heterogeneous images since they demonstrate different image appearances and statistics. In this article, we propose a deep pyramid feature learning network (DPFL-Net) for change detection, especially between heterogeneous images. DPFL-Net can learn a series of hierarchical features in an unsupervised fashion, containing both spatial details and multiscale contextual information. The learned pyramid features from two input images make unchanged pixels matched exactly and changed ones dissimilar and after transformed into the same space for each scale successively. We further propose fusion blocks to aggregate multiscale difference images (DIs), generating an enhanced DI with strong separability. Based on the enhanced DI, unchanged areas are predicted and used to train DPFL-Net in the next iteration. In this article, pyramid features and unchanged areas are updated alternately, leading to an unsupervised change detection method. In the feature transformation process, local consistency is introduced to constrain the learned pyramid features, modeling the correlations between the neighboring pixels and reducing the false alarms. Experimental results demonstrate that the proposed approach achieves superior or at least comparable results to the existing state-of-the-art change detection methods in both homogeneous and heterogeneous cases.","2162-2388","","10.1109/TNNLS.2021.3079627","National Natural Science Foundation of China(grant numbers:U1701267); State Key Program of National Natural Science of China(grant numbers:61836009); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201); Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439971","Change detection;deep neural networks;feature transformation;heterogeneous images;local consistency;pyramid features;remote sensing","Feature extraction;Remote sensing;Sensors;Radar polarimetry;Correlation;Task analysis;Semantics","Bayes methods;expectation-maximisation algorithm;feature extraction;geophysical image processing;image classification;image fusion;image matching;image representation;image segmentation;learning (artificial intelligence);radar imaging;unsupervised learning","deep pyramid feature learning network;different image appearances;DPFL-Net;DPFL-nets;enhanced DI;existing state-of-the-art change detection methods;feature transformation process;heterogeneous images;hierarchical features;input images;learned pyramid features;multiscale change detection;multiscale contextual information;multiscale difference images;statistics;unchanged areas;unsupervised change detection method","","6","","60","IEEE","24 May 2021","","","IEEE","IEEE Journals"
"Hyperspectral and Multispectral Image Fusion Based on Low Rank Constrained Gaussian Mixture Model","B. Lin; X. Tao; Y. Duan; J. Lu","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Access","9 Apr 2018","2018","6","","16901","16910","This paper attempts to fuse a multispectral image and an auxiliary hyperspectral image (HSI) with no requirement of image registration. Most previous studies solve this problem with sparsity-based methods. However, in this paper, a novel fusion framework is developed based on a Gaussian mixture model (GMM): First, the GMM is adopted to extract the spectral information from the input HSI. Low-rank constraints are imposed on the covariance matrices of the model to solve the computational problem in the expectation-maximization approach. Second, considering the spatial self-similarity, a structure-similarity regularization term is designed to further enhance the quality of the reconstructed image. To that end, a forward–backward splitting method is adopted to cut down the computational complexity of the optimization. The proposed method does not require two well-aligned images, thus, it will not be influenced by the registration errors between two fusing images. Experimental results of a simulated data set and an actual satellite (EO-1/Hyperion/ALI) data set show that the proposed method displays a stable performance and outperforms many state-of-the-art methods with acceptable computational complexity, when registration errors are taken into consideration.","2169-3536","","10.1109/ACCESS.2018.2817071","National Basic Research Project of China (973)(grant numbers:2013CB329006); National Natural Science Foundation of China(grant numbers:61622110,61471220,91538107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319415","Hyperspectral image fusion;Gaussian mixture model;low rank constraint;local and nonlocal similarity;registration errors","Spatial resolution;Covariance matrices;Optimization;Feature extraction;Gaussian mixture model","","","","6","","40","OAPA","19 Mar 2018","","","IEEE","IEEE Journals"
"A Critical Comparison Among Pansharpening Algorithms","G. Vivone; L. Alparone; J. Chanussot; M. Dalla Mura; A. Garzelli; G. A. Licciardi; R. Restaino; L. Wald","Department of Information Engineering, Electrical Engineering and Applied Mathematics, North Atlantic Treaty Organization (NATO) Science and Technology Organization (STO) Center for Maritime Research and Experimentation, La Spezia, Italy; Department of Information Engineering, University of Florence, Florence, Italy; Grenoble Images Speech Signals and Automatics Laboratory (GIPSA-Lab), University of Iceland, Reykjavík, Iceland; Grenoble Images Speech Signals and Automatics Laboratory (GIPSA-Lab), Grenoble Institute of Technology, Grenoble, France; Department of Information Engineering and Mathematical Sciences, University of Siena, Siena, Italy; Grenoble Images Speech Signals and Automatics Laboratory (GIPSA-Lab), Grenoble Institute of Technology, Grenoble, France; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Center Observation, Impacts, Energy, MINES ParisTech, Sophia Antipolis, France","IEEE Transactions on Geoscience and Remote Sensing","7 Jan 2015","2015","53","5","2565","2586","Pansharpening aims at fusing a multispectral and a panchromatic image, featuring the result of the processing with the spectral resolution of the former and the spatial resolution of the latter. In the last decades, many algorithms addressing this task have been presented in the literature. However, the lack of universally recognized evaluation criteria, available image data sets for benchmarking, and standardized implementations of the algorithms makes a thorough evaluation and comparison of the different pansharpening techniques difficult to achieve. In this paper, the authors attempt to fill this gap by providing a critical description and extensive comparisons of some of the main state-of-the-art pansharpening methods. In greater details, several pansharpening algorithms belonging to the component substitution or multiresolution analysis families are considered. Such techniques are evaluated through the two main protocols for the assessment of pansharpening results, i.e., based on the full- and reduced-resolution validations. Five data sets acquired by different satellites allow for a detailed comparison of the algorithms, characterization of their performances with respect to the different instruments, and consistency of the two validation procedures. In addition, the implementation of all the pansharpening techniques considered in this paper and the framework used for running the simulations, comprising the two validation procedures and the main assessment indexes, are collected in a MATLAB toolbox that is made available to the community.","1558-0644","","10.1109/TGRS.2014.2361734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6998089","Benchmarking;component substitution (CS);multiresolution analysis (MRA);multispectral (MS) pansharpening;quality assessment;very high-resolution optical images;Benchmarking;component substitution (CS);multiresolution analysis (MRA);multispectral (MS) pansharpening;quality assessment;very high-resolution optical images","Vectors;Spatial resolution;Principal component analysis;Indexes;Transforms;Algorithm design and analysis","image fusion;image resolution","pansharpening algorithms;multispectral image;panchromatic image;component substitution;multiresolution analysis;Matlab toolbox","","778","1","77","IEEE","24 Dec 2014","","","IEEE","IEEE Journals"
"Local Binary Patterns and Extreme Learning Machine for Hyperspectral Imagery Classification","W. Li; C. Chen; H. Su; Q. Du","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Department of Electrical Engineering, University of Texas at Dallas, Richardson, TX, USA; School of Earth Sciences and Engineering, Hohai University, Nanjing, China; Department of Electrical and Computer Engineering, Mississippi State University, Mississippi State, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","9 Mar 2015","2015","53","7","3681","3693","It is of great interest in exploiting texture information for classification of hyperspectral imagery (HSI) at high spatial resolution. In this paper, a classification paradigm to exploit rich texture information of HSI is proposed. The proposed framework employs local binary patterns (LBPs) to extract local image features, such as edges, corners, and spots. Two levels of fusion (i.e., feature-level fusion and decision-level fusion) are applied to the extracted LBP features along with global Gabor features and original spectral features, where feature-level fusion involves concatenation of multiple features before the pattern classification process while decision-level fusion performs on probability outputs of each individual classification pipeline and soft-decision fusion rule is adopted to merge results from the classifier ensemble. Moreover, the efficient extreme learning machine with a very simple structure is employed as the classifier. Experimental results on several HSI data sets demonstrate that the proposed framework is superior to some traditional alternatives.","1558-0644","","10.1109/TGRS.2014.2381602","National Natural Science Foundation of China(grant numbers:NSFC-61302164,41201341); Fundamental Research Funds for the Central Universities(grant numbers:YS1404); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010879","Decision fusion;extreme learning machine (ELM);Gabor filter;hyperspectral imagery (HSI);local binary patterns (LBPs);pattern classification;Decision fusion;extreme learning machine (ELM);Gabor filter;hyperspectral imagery (HSI);local binary patterns (LBPs);pattern classification","Feature extraction;Support vector machines;Kernel;Hyperspectral imaging;Vectors;Educational institutions;Principal component analysis","decision theory;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;image texture;learning (artificial intelligence)","more hyperspectral imagery classification;spatial resolution;texture information;HSI;local binary pattern;LBP;local image feature extraction;feature level fusion;decision level fusion;global Gabor features;spectral feature extraction;pattern classification process;soft-decision fusion rule;extreme learning machine;HSI data sets","","479","2","42","IEEE","15 Jan 2015","","","IEEE","IEEE Journals"
"Deep Fully Convolutional Network-Based Spatial Distribution Prediction for Hyperspectral Image Classification","L. Jiao; M. Liang; H. Chen; S. Yang; H. Liu; X. Cao","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education of China, International Research Center of Intelligent Perception and Computation, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","25 Sep 2017","2017","55","10","5585","5599","Most of the existing spatial-spectral-based hyperspectral image classification (HSIC) methods mainly extract the spatial-spectral information by combining the pixels in a small neighborhood or aggregating the statistical and morphological characteristics. However, those strategies can only generate shallow appearance features with limited representative ability for classes with high interclass similarity and spatial diversity and therefore reduce the classification accuracy. To this end, we present a novel HSIC framework, named deep multiscale spatial-spectral feature extraction algorithm, which focuses on learning effective discriminant features for HSIC. First, the well pretrained deep fully convolutional network based on VGG-verydeep-16 is introduced to excavate the potential deep multiscale spatial structural information in the proposed hyperspectral imaging framework. Then, the spectral feature and the deep multiscale spatial feature are fused by adopting the weighted fusion method. Finally, the fusion feature is put into a generic classifier to obtain the pixelwise classification. Compared with the existing spectral-spatial-based classification techniques, the proposed method provides the state-of-the-art performance and is much more effective, especially for images with high nonlinear distribution and spatial diversity.","1558-0644","","10.1109/TGRS.2017.2710079","Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438103,91438201); National Basic Research Program (973 Program) of China(grant numbers:2013CB329402); Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT_15R53); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967742","Deep multiscale feature;feature fusion;fully convolutional network (FCN);hyperspectral image classification (HSIC);spatial distribution prediction","Feature extraction;Kernel;Graphical models;Distribution functions;Hyperspectral imaging;Training;Convolution","convolution;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;neural nets;statistics","deep fully convolutional network;spatial distribution prediction;hyperspectral image classification;HSIC;statistical characteristics;morphological characteristics;deep multiscale spatial-spectral feature extraction;weighted fusion","","144","","42","IEEE","3 Jul 2017","","","IEEE","IEEE Journals"
"Pansharpening of Multispectral Images Based on Nonlocal Parameter Optimization","A. Garzelli","Department of Information Engineering and Mathematical Sciences, University of Siena, Siena, Italy","IEEE Transactions on Geoscience and Remote Sensing","9 Oct 2014","2015","53","4","2096","2107","High-quality pansharpened multispectral (MS) images are rarely obtained from fast, efficient, and robust algorithms. In most cases, effective pansharpening methods have huge computational complexity, as in the case of variational methods, or algorithms based on sparse representations. Moreover, injection models are often application dependent, not sufficiently general to be applied to different scenarios, and the resulting algorithm implementations cannot process large-size images. The proposed pansharpening method is accurate and fast and can be successfully applied to huge images. It also solves the problem of contextadaptive schemes that tune the spatial injection parameters on local statistics: Instabilities and blocky artifacts can be generated by pansharpening methods whose parameters are computed on local windows. The proposed method is an extension of the classical component-substitution algorithms: An optimal detail image (in the mmse sense) extracted from the panchromatic band is calculated for each MS band by evaluating band-dependent generalized intensities. It overcomes window-based local estimation of parameters by applying a nonlocal parameter optimization through K-means clustering. Very high quality scores, both at degraded and full scale, and excellent visual quality of the fused images demonstrate the validity of the method.","1558-0644","","10.1109/TGRS.2014.2354471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906274","Multispectral (MS) images;optimization;pansharpening;quality assessment;Multispectral (MS) images;optimization;pansharpening;quality assessment","Spatial resolution;Parameter estimation;Clustering algorithms;Estimation;Optimization;Indexes","geophysical image processing;geophysical techniques;image fusion","multispectral image pansharpening;nonlocal parameter optimization;robust algorithms;variational methods;pansharpening method;spatial injection parameters;local statistics;classical component-substitution algorithms;panchromatic band;optimal detail image;K-means clustering;fused image visual quality","","82","","35","IEEE","22 Sep 2014","","","IEEE","IEEE Journals"
"Fusion of Multiple Edge-Preserving Operations for Hyperspectral Image Classification","P. Duan; X. Kang; S. Li; P. Ghamisi; J. A. Benediktsson","Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","IEEE Transactions on Geoscience and Remote Sensing","22 Nov 2019","2019","57","12","10336","10349","In this article, a novel hyperspectral image (HSI) classification method based on fusing multiple edge-preserving operations (EPOs) is proposed, which consists of the following steps. First, the edge-preserving features are obtained by performing different types of EPOs, i.e., local edge-preserving filtering and global edge-preserving smoothing on the dimension-reduced HSI. Then, with the assistance of a superpixel segmentation method, the edge-preserving features are further improved by considering the inter and intra spectral properties of superpixels. Finally, the spectral and edge-preserving features are fused to form one composite kernel, which is fed into the support vector machine (SVM) followed by a majority voting fusion scheme. Experimental results on three data sets demonstrate the superiority of the proposed method over several state-of-the-art classification approaches, especially when the training sample size is limited. Furthermore, 21 well-known methods, including mathematical morphology-based approaches, sparse representation models, and deep learning-based classifiers, are adopted to be compared with the proposed method on Houston data set with standard sets of training and test samples released during 2013 Data Fusion Contest, which also shows the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2019.2933588","National Natural Science Foundation of China(grant numbers:61890962); National Natural Science Foundation of China(grant numbers:61601179,6187119); National Natural Science Foundation of China(grant numbers:61520106001); Fund of the Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province(grant numbers:2018TP1013); Fund of Hunan Province for the Science and Technology Plan Project(grant numbers:2017RS3024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821552","Decision fusion;edge-preserving operation (EPO);feature extraction;hyperspectral image (HSI);image classification","Image edge detection;Smoothing methods;Feature extraction;Support vector machines;Transforms;Hyperspectral imaging","edge detection;feature extraction;hyperspectral imaging;image classification;image fusion;image representation;image segmentation;learning (artificial intelligence);mathematical morphology;support vector machines","EPOs;local edge-preserving filtering;global edge-preserving smoothing;dimension-reduced HSI;superpixel segmentation method;edge-preserving features;multiple edge-preserving operations;hyperspectral image classification method;support vector machine;SVM;voting fusion scheme;mathematical morphology-based approaches;sparse representation models;deep learning-based classifiers;Houston data set;data fusion contest;spectral-preserving features","","72","","62","IEEE","30 Aug 2019","","","IEEE","IEEE Journals"
"Probabilistic Fusion of Pixel-Level and Superpixel-Level Hyperspectral Image Classification","S. Li; T. Lu; L. Fang; X. Jia; J. A. Benediktsson","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; School of Engineering and Information Technology, University of New South Wales, Canberra, BC, Australia; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavk, Iceland","IEEE Transactions on Geoscience and Remote Sensing","30 Sep 2016","2016","54","12","7416","7430","A novel hyperspectral image (HSI) classification method by the probabilistic fusion of pixel-level and superpixel-level classifiers is proposed. Generally, pixel-level classifiers based on spectral information only may generate “salt and pepper” result in the classification map since spatial correlation is not considered. By incorporating spatial information in homogeneous regions, the superpixel-level classifiers can effectively eliminate the noisy appearance. However, the classification accuracy will be deteriorated if undersegmentation cannot be fully avoided in superpixel-based approaches. Therefore, it is proposed to adaptively combine both the pixel-level and superpixel-level classifiers, to improve the classification performance in both homogenous and structural areas. In the proposed method, a support vector machine classifier is first applied to estimate the pixel-level class probabilities. Then, superpixel-level class probabilities are estimated based on a joint sparse representation. Finally, the two levels of class probabilities are adaptively combined in a maximum a posteriori estimation model, and the classification map is obtained by solving the maximum optimization problem. Experimental results on real HSI images demonstrate the superiority of the proposed method over several well-known classification approaches in terms of classification accuracy.","1558-0644","","10.1109/TGRS.2016.2603190","National Natural Science Fund of China for Distinguished Young Scholars(grant numbers:61325007); National Natural Science Fund of China for International Cooperation and Exchanges(grant numbers:61520106001); Science and Technology Plan Project Fund of Hunan Province(grant numbers:2015WK3001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7565589","Hyperspectral image (HSI);probabilistic fusion;spectral–spatial classification;superpixel segmentation","Support vector machines;Probabilistic logic;Training;Adaptation models;Approximation algorithms;Optimization;Estimation","geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image fusion","superpixel-level hyperspectral image classification;hyperspectral image classification method;spectral information;support vector machine classifier;superpixel-level class probabilities;maximum optimization problem","","66","","51","IEEE","13 Sep 2016","","","IEEE","IEEE Journals"
"Deep Feature Fusion via Two-Stream Convolutional Neural Network for Hyperspectral Image Classification","X. Li; M. Ding; A. Pižurica","Department of Telecommunications and Information Processing, UGent-GAIM, Ghent University, Ghent, Belgium; School of Instrumentation Science and Engineering, Harbin Institute of Technology, Harbin, China; Department of Telecommunications and Information Processing, imec-UGent-GAIM, Ghent University, Ghent, Belgium","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2020","2020","58","4","2615","2629","The representation power of convolutional neural network (CNN) models for hyperspectral image (HSI) analysis is in practice limited by the available amount of the labeled samples, which is often insufficient to sustain deep networks with many parameters. We propose a novel approach to boost the network representation power with a two-stream 2-D CNN architecture. The proposed method extracts simultaneously, the spectral features and local spatial and global spatial features, with two 2-D CNN networks and makes use of channel correlations to identify the most informative features. Moreover, we propose a layer-specific regularization and a smooth normalization fusion scheme to adaptively learn the fusion weights for the spectral-spatial features from the two parallel streams. An important asset of our model is the simultaneous training of the feature extraction, fusion, and classification processes with the same cost function. Experimental results on several hyperspectral data sets demonstrate the efficacy of the proposed method compared with the state-of-the-art methods in the field.","1558-0644","","10.1109/TGRS.2019.2952758","China Scholarship Council; Fonds Wetenschappelijk Onderzoek(grant numbers:G.OA26.17N); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920212","Convolutional neural networks (CNNs);feature fusion;hyperspectral image (HSI) classification;squeeze-and-excitation (SE)","Feature extraction;Training;Streaming media;Machine learning;Hyperspectral imaging;Convolutional neural networks","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;learning (artificial intelligence);neural net architecture","feature fusion;two-stream convolutional neural network;hyperspectral image classification;convolutional neural network models;hyperspectral image analysis;labeled samples;deep networks;network representation power;CNN architecture;spectral features;local spatial;global spatial features;2-D CNN networks;informative features;layer-specific regularization;smooth normalization fusion scheme;fusion weights;spectral-spatial features;parallel streams;feature extraction;classification processes;hyperspectral data sets","","64","","63","IEEE","3 Dec 2019","","","IEEE","IEEE Journals"
"A Convex Optimization-Based Coupled Nonnegative Matrix Factorization Algorithm for Hyperspectral and Multispectral Data Fusion","C. -H. Lin; F. Ma; C. -Y. Chi; C. -H. Hsieh","Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan; School of Electronic and Information Engineering, Liaoning Technical University, Huludao, China; Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan; Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Geoscience and Remote Sensing","27 Feb 2018","2018","56","3","1652","1667","Fusing a low-spatial-resolution hyperspectral data with a high-spatial-resolution (HSR) multispectral data has been recognized as an economical approach for obtaining HSR hyperspectral data, which is important to accurate identification and classification of the underlying materials. A natural and promising fusion criterion, called coupled nonnegative matrix factorization (CNMF), has been reported that can yield high-quality fused data. However, the CNMF criterion amounts to an ill-posed inverse problem, and hence, advisable regularization can be considered for further upgrading its fusion performance. Besides the commonly used sparsity-promoting regularization, we also incorporate the well-known sum-of-squared-distances regularizer, which serves as a convex surrogate of the volume of the simplex of materials’ spectral signature vectors (i.e., endmembers), into the CNMF criterion, thereby leading to a convex formulation of the fusion problem. Then, thanks to the biconvexity of the problem nature, we decouple it into two convex subproblems, which are then, respectively, solved by two carefully designed alternating direction method of multipliers (ADMM) algorithms. Closed-form expressions for all the ADMM iterates are derived via convex optimization theories (e.g., Karush–Kuhn–Tucker conditions), and furthermore, some matrix structures are employed to obtain alternative expressions with much lower computational complexities, thus suitable for practical applications. Some experimental results are provided to demonstrate the superior fusion performance of the proposed algorithm over state-of-the-art methods.","1558-0644","","10.1109/TGRS.2017.2766080","Ministry of Science and Techonlogy, R.O.C.(grant numbers:MOST 104-2221-E-007-069-MY3); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107710","Alternating direction method of multipliers (ADMM);convex optimization;coupled nonnegative matrix factorization (CNMF);data fusion;hyperspectral data","Hyperspectral imaging;Convex functions;Data models;Data integration;Inverse problems;Spatial resolution","computational complexity;image fusion;image resolution;inverse problems;iterative methods;matrix decomposition;optimisation;spectral analysis","multispectral data fusion;low-spatial-resolution hyperspectral data;high-spatial-resolution multispectral data;economical approach;HSR hyperspectral data;natural fusion criterion;high-quality fused data;CNMF criterion;convex subproblems;convex optimization theories;alternating direction method of multipliers;sparsity-promoting regularization;sum-of-squared-distances regularizer;coupled nonnegative matrix factorization;hyperspectral data fusion;ll-posed inverse problem;materials spectral signature vectors;Karush-Kuhn-Tucker conditions;computational complexity","","63","","66","IEEE","14 Nov 2017","","","IEEE","IEEE Journals"
"Collaborative Representation-Based Multiscale Superpixel Fusion for Hyperspectral Image Classification","S. Jia; X. Deng; J. Zhu; M. Xu; J. Zhou; X. Jia","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Guangdong Key Laboratory of Urban Informatics, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Information and Communication Technology, Griffith University, Nathan, QLD, Australia; School of Engineering and Information Technology, University of New South Wales, Canberra, ACT, Australia","IEEE Transactions on Geoscience and Remote Sensing","25 Sep 2019","2019","57","10","7770","7784","In virtue of the spatial structural characteristic of surface materials, the performance of the hyperspectral image classification can be boosted by incorporating texture information. Normally, the spatial structure can be extracted by predefined operators, including the popular extended multiattribute profiles (EMAPs) and the Gabor filters. Recently, superpixel segmentation, which reflects the homogeneous regularity of objects, has drawn much attention in the field. In this paper, a collaborative representation-based multiscale superpixel fusion (CRMSF) approach has been proposed for the hyperspectral image classification. First, after obtaining the EMAPs from the raw hyperspectral image, a group of predesigned 3-D Gabor wavelet filters is convolved with the EMAP features, and the EMAP-Gabor features can, thus, be achieved. Second, the collaborative representation-based classification (CRC) is employed to fully and efficiently make use of the huge amount of extracted EMAP-Gabor features. Third, multiscale superpixel maps are generated from the EMAP features that are utilized to regularize the classification map obtained by CRC. A heuristic strategy has been specially devised to automatically decide the number of extracted superpixels in multiple scales, which can be perfectly compatible with hyperspectral images having various spatial sizes and spatial resolutions. This is the most important contribution of the developed CRMSF approach. Finally, the classification task is accomplished by fusing the multiple regularized classification maps. The CRMSF approach has been evaluated on four popular hyperspectral image data sets, and the experimental results show the advantages of CRMSF, particularly for a hyperspectral image with high spatial resolution.","1558-0644","","10.1109/TGRS.2019.2916329","National Natural Science Foundation of China(grant numbers:61671307,41871329); Guangdong Special Support Program of Top-Notch Young Professionals(grant numbers:2015TQ01X238); Shenzhen Scientific Research and Development Funding Program(grant numbers:JCYJ20180305124802421,JCYJ20180305125902403,JCYJ20170818092931604); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730490","Feature extraction;hyperspectral image;superpixel segmentation","Hyperspectral imaging;Feature extraction;Collaboration;Image segmentation;Spatial resolution;Wavelet transforms","feature extraction;Gabor filters;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;image resolution;image segmentation;image texture;wavelet transforms","hyperspectral image classification;spatial structural characteristic;spatial structure;EMAPs;Gabor filters;superpixel segmentation;collaborative representation-based multiscale superpixel fusion approach;raw hyperspectral image;predesigned 3-D Gabor wavelet filters;EMAP features;collaborative representation-based classification;extracted EMAP-Gabor features;multiscale superpixel maps;classification map;extracted superpixels;classification task;multiple regularized classification maps;extended multiattribute profiles;hyperspectral image data sets;texture information;EMAP-Gabor features;high spatial resolution","","58","","55","IEEE","4 Jun 2019","","","IEEE","IEEE Journals"
"Hyperspectral Pansharpening Using Deep Prior and Dual Attention Residual Network","Y. Zheng; J. Li; Y. Li; J. Guo; X. Wu; J. Chanussot","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; CAS Key Laboratory of Spectral Imaging Technology, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, Grenoble, France","IEEE Transactions on Geoscience and Remote Sensing","26 Oct 2020","2020","58","11","8059","8076","Convolutional neural networks (CNNs) have recently achieved impressive improvements on hyperspectral (HS) pansharpening. However, most of the CNN-based HS pansharpening approaches would have to first upsample the low-resolution hyperspectral image (LR-HSI) using bicubic interpolation or data-driven training strategy, which inevitably lose some details or greatly rely on the learning process. In addition, most previous methods regard the pansharpening as a black-box problem and treat diverse features equally, thus hindering the discriminative ability of CNNs. To conquer these issues, a novel HS pansharpening method using deep hyperspectral prior (DHP) and dual-attention residual network (DARN) is proposed in this article. Specifically, we first upsample the LR-HSI to the scale of the panchromatic (PAN) image through the DHP algorithm, which can better preserve spatial and spectral information without learning from large data sets. The upsampled result is then concatenated with the PAN image to form the input of the DARN, where several channel-spatial attention residual blocks (CSA ResBlocks) are stacked to map the residual HSI between the reference HSI and the upsampled HSI. In each CSA ResBlock, two complementary attention modules, i.e., channel attention and spatial attention modules, are designed to adaptively learn more informative features of spectral channels and spatial locations simultaneously, which can effectively boost the fusion accuracy. Finally, the fused HSI is obtained by the summation of the upsampled HSI and the reconstructed residual HSI. The experimental results of both simulated and real HS data sets demonstrate that the performance of our DHP-DARN method is superior over the state-of-the-art HS pansharpening approaches.","1558-0644","","10.1109/TGRS.2020.2986313","National Nature Science Foundation of China(grant numbers:61901343,61571345,61701360,61671383,91538101,61501346,61502367); China Postdoctoral Science Foundation(grant numbers:2017M623124); China Postdoctoral Science Special Foundation(grant numbers:2018T111019); Open Research Fund of the CAS Key Laboratory of Spectral Imaging Technology(grant numbers:LSIT201924W); Fundamental Research Funds for the Central Universities(grant numbers:JB190107); Higher Education Discipline Innovation Project(grant numbers:B08038); Key Research and Development Program of Shaanxi province(grant numbers:2017KJXX-50); Key Lab of China(grant numbers:6142411184413); Innovation Fund of Xidian University(grant numbers:10221150004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076645","Deep hyperspectral prior (DHP);dual-attention residual network (DARN);hyperspectral (HS) pansharpening","Spatial resolution;Hyperspectral imaging;Bayes methods","convolutional neural nets;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;image sampling;interpolation;learning (artificial intelligence)","hyperspectral pansharpening;dual attention residual network;convolutional neural networks;CNN-based HS pansharpening approaches;data-driven training strategy;learning process;black-box problem;panchromatic imaging;spatial information;spectral information;channel-spatial attention residual blocks;CSA ResBlock;upsampled HSI;complementary attention modules;spatial attention modules;DHP-DARN method;HS data set simulation;residual HSI reconstruction;PAN imaging;LR-HSI fusion;low-resolution hyperspectral imaging;bicubic interpolation;deep hyperspectral prior","","50","","53","IEEE","23 Apr 2020","","","IEEE","IEEE Journals"
"Saliency-Guided Single Shot Multibox Detector for Target Detection in SAR Images","L. Du; L. Li; D. Wei; J. Mao","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2020","2020","58","5","3366","3376","The single shot multibox detector (SSD), a proposal-free method based on convolutional neural network (CNN), has recently been proposed for target detection and has found applications in synthetic aperture radar (SAR) images. Moreover, the saliency information reflected in the saliency map can highlight the target of interest while suppressing clutter, which is beneficial for better scene understanding. Therefore, in this article, we propose a saliency-guided SSD (S-SSD) for target detection in SAR images, in which we effectively integrate the saliency into the SSD network not only to suggest where to focus on but also to improve the representation capability in complex scenes. The proposed S-SSD contains two separated convolutional backbone subnetwork architectures, one with the original SAR image as input to extract features, and the other with the corresponding saliency map obtained from the modified Itti's method as input to acquire refined saliency information under supervision. In addition, the dense connection structure, instead of the plain structure used in original SSD, is applied in the two convolutional backbone architectures to utilize multiscale information with fewer parameters. Then, for integrating saliency information to guide the network to emphasize informative regions, multilevel fusion modules are utilized to merge the two streams into a unified framework, thereby making the whole network end-to-end jointly trained. Finally, the convolutional predictors are used to predict targets. The experimental results on the miniSAR real data demonstrate that the proposed S-SSD can achieve better detection performance than state-of-the-art methods.","1558-0644","","10.1109/TGRS.2019.2953936","National Science Foundation of China(grant numbers:61771362,U1833203,61671354); Higher Education Discipline Innovation Project(grant numbers:B18039); Shaanxi Innovation Team Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930938","Convolutional neural network (CNN);deep learning;saliency detection;single shot multibox detector (SSD);synthetic aperture radar (SAR);target detection","Object detection;Feature extraction;Radar polarimetry;Clutter;Synthetic aperture radar;Detectors;Convolution","convolutional neural nets;feature extraction;image fusion;image representation;interference suppression;neural net architecture;object detection;radar clutter;radar computing;radar detection;radar imaging;synthetic aperture radar","multilevel fusion modules;plain structure;dense connection structure;feature extraction;representation capability;clutter suppression;convolutional backbone subnetwork architectures;detection performance;convolutional predictors;informative regions;multiscale information;original SSD;refined saliency information;modified Itti method;original SAR image;complex scenes;SSD network;S-SSD;saliency-guided SSD;scene understanding;synthetic aperture radar images;convolutional neural network;SAR images;target detection;saliency-guided single shot multibox detector","","48","","27","IEEE","11 Dec 2019","","","IEEE","IEEE Journals"
"Hyperspectral Image Super-Resolution Based on Spatial and Spectral Correlation Fusion","C. Yi; Y. -Q. Zhao; J. C. -W. Chan","Key Laboratory of Information Fusion Technology, Ministry of Education of China, School of Automation, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Information Fusion Technology, Ministry of Education of China, School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Electronics and Informatics, Vrije Universiteit Brussel, Brussel, Belgium","IEEE Transactions on Geoscience and Remote Sensing","22 Jun 2018","2018","56","7","4165","4177","Super-resolution image reconstruction has been utilized to overcome the problem of spatial resolution limitation in hyperspectral (HS) imaging. To improve the spatial resolution of HS image, this paper proposes an HS-multispectral (MS) fusion method, which exploits spatial and spectral correlations and proper regularization. High spatial correlation between MS image and the desired high-resolution HS image is conserved via an over-completed dictionary, and the spectral degradation between them projected onto the space of sparsity is applied as the spectral constraint. The high spectral correlation between high-spatial- and low-spatial-resolution HS image is preserved through linear spectral unmixing. The idea of an interactive feedback proposed in our previous work is also used when dealing with spatial reconstruction and unmixing. Low-rank property is introduced in this paper to regularize the sparse coefficients of the HS patch matrix, which is utilized as the spatial constraint. Experiments on both simulated and real data sets demonstrate that the proposed fusion algorithm achieves lower spectral distortions and the super-resolution results are superior to those of other state-of-the-art methods.","1558-0644","","10.1109/TGRS.2018.2828042","National Natural Science Foundation of China(grant numbers:61771391,61371152); National Natural Science Foundation of China(grant numbers:61511140292); Fundamental Research Funds for the Central Universities(grant numbers:3102015ZY045); China Scholarship Council(grant numbers:201706290150); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8358017","Hyperspectral (HS) image;low rank;spatial–spectral correlation;super-resolution enhancement","Correlation;Spatial resolution;Sparse matrices;Dictionaries;Degradation;Image reconstruction","geophysical image processing;image fusion;image reconstruction;image representation;image resolution","hyperspectral image super-resolution;spatial correlation fusion;spectral correlation fusion;super-resolution image reconstruction;spatial resolution limitation;hyperspectral imaging;HS-multispectral fusion method;spatial correlations;spectral correlations;high spatial correlation;MS image;high-resolution HS image;spectral degradation;spectral constraint;high spectral correlation;low-spatial-resolution HS image;linear spectral unmixing;spatial reconstruction;HS patch matrix;spatial constraint;fusion algorithm achieves;lower spectral distortions;super-resolution results","","48","","45","IEEE","11 May 2018","","","IEEE","IEEE Journals"
"Unsupervised Recurrent Hyperspectral Imagery Super-Resolution Using Pixel-Aware Refinement","W. Wei; J. Nie; L. Zhang; Y. Zhang","Research and Development Institute, Northwestern Polytechnical University in Shenzhen, Shenzhen, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","2 Dec 2021","2022","60","","1","15","Unsupervised fusion-based hyperspectral imagery (HSI) super-resolution (SR) is an essential task of HSI processing, which aims to reconstruct a high-resolution (HR) HSI using only an observed low-resolution HSI and a conventional HR image. Although a large number of unsupervised HSI SR methods have been proposed, the heuristic handcrafted image priors adopted by the majority of these methods restrict their capacity to capture specific characteristics of the HSI, as well as their ability to generalize to noisy observation images. In this study, we investigate a fusion-based HSI SR framework with the deep image prior, in which the deep neural network (rather than a heuristic handcrafted image prior) is exploited to capture plenty of image statistics. Within this framework, we further propose an unsupervised recurrence-based HSI SR method using pixel-aware refinement, which utilizes the intermediate reconstruction results to self-supervise unsupervised learning. Due to containing the information of the image-specific characteristic, the proposed method achieves better performance, in terms of both accuracy and robustness to noise, compared with the existing methods. Extensive experiments on four HSI data sets demonstrate the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2020.3039534","National Natural Science Foundation of China(grant numbers:61671385,62071387,U19B2037); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20190806160210899); Seed Foundation of Innovation and Creation for Graduate Students in Northwestern Polytechnical University(grant numbers:CX2020025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9292466","Hyperspectral image super-resolution (SR);pixel-aware refinement;unsupervised deep learning","Image reconstruction;Spatial resolution;Optimization;Computer science;Training;Spectral analysis;Sparse matrices","deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image classification;image fusion;image reconstruction;image representation;image resolution;statistics;unsupervised learning","image statistics;unsupervised recurrence-based HSI SR method;pixel-aware refinement;self-supervise unsupervised learning;image-specific characteristic;low-resolution HSI;heuristic handcrafted image prior;noisy observation images;fusion-based HSI SR framework;deep image prior;unsupervised recurrent hyperspectral imagery superresolution;unsupervised fusion-based hyperspectral imagery superresolution;high-resolution HSI reconstruction;deep neural network","","37","","45","IEEE","11 Dec 2020","","","IEEE","IEEE Journals"
"Contributions to Automatic Target Recognition Systems for Underwater Mine Classification","T. Fei; D. Kraus; A. M. Zoubir","Hella KGaA Hueck & Co., Lippstadt, Germany, Germany; Hella KGaA Hueck & Co., Lippstadt, Germany, Germany; Institute of Wateracoustics, Sonar Engineering and Signal Theory (IWSS), Hochschule Bremen, Bremen, Germany","IEEE Transactions on Geoscience and Remote Sensing","4 Aug 2014","2015","53","1","505","518","This paper deals with several original contributions to an automatic target recognition (ATR) system, which is applied to underwater mine classification. The contributions concentrate on feature selection and object classification. First, a sophisticated filter method is designed for the feature selection. This filter method utilizes a novel feature relevance measure, the composite relevance measure (CRM). Feature relevance measures in the literature (e.g., mutual information and relief weight) evaluate the features only with respect to certain aspects. The CRM is a combination of several measures so that it is able to provide a more comprehensive assessment of the features. Both linear and nonlinear combinations of these measures are taken into account. A wide range of classifiers is able to provide satisfactory classification results by using the features selected according to the CRM. Second, in the step of object classification, an ensemble learning scheme in the framework of the Dempster–Shafer theory is introduced to fuse the results obtained by different classifiers. This fusion can improve the classification performance. We propose a reasonable construction of the basic belief assignment (BBA). The BBA considers both the reliability of the classifiers and the support of individual classifiers provided to the hypotheses about the types of test objects. Finally, this ATR system is applied to real synthetic aperture sonar imagery to evaluate its performance.","1558-0644","","10.1109/TGRS.2014.2324971","ATLAS ELEKTRONIK GmbH Bremen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825823","Automatic target recognition (ATR);Dempster–Shafer theory (DST);ensemble learning;filter method for feature selection;synthetic aperture sonar (SAS);Automatic target recognition (ATR);Dempster–Shafer theory (DST);ensemble learning;filter method for feature selection;synthetic aperture sonar (SAS)","Customer relationship management;Feature extraction;Redundancy;Synthetic aperture sonar;Vectors;Image segmentation;Topology","feature selection;filtering theory;geophysical image processing;image classification;image fusion;inference mechanisms;learning (artificial intelligence);oceanographic equipment;reliability;sonar imaging;synthetic aperture sonar;uncertainty handling","automatic target recognition system;underwater mine classification;ATR system;feature selection;object classification;sophisticated filter method;composite relevance measure;CRM;mutual information;ensemble learning scheme;Dempster-Shafer theory;basic belief assignment;BBA;reliability;synthetic aperture sonar imagery","","35","","65","IEEE","4 Jun 2014","","","IEEE","IEEE Journals"
"Multiple 3-D Feature Fusion Framework for Hyperspectral Image Classification","J. Zhu; J. Hu; S. Jia; X. Jia; Q. Li","Shenzhen Key Laboratory of Spatial Information Smarting Sensing and Services, Shenzhen University, Shenzhen, China; Computer Vision Research Institute, College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Computer Vision Research Institute, College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Engineering and Information Technology, University of New South Wales at Canberra, Canberra, ACT, Australia; Shenzhen Key Laboratory of Spatial Information Smarting Sensing and Services, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","23 Mar 2018","2018","56","4","1873","1886","Due to the 3-D nature of hyperspectral images, as well as the spatial properties (such as regularity and continuity) of land covers, many 3-D feature extraction operators have been designed to fully exploit the joint spatial-spectral information. However, the large amount of obtained features can suffer from the “curse of dimensionality” problem, especially for the small training sample set. Moreover, various spatial-spectral features can represent the characteristics of the hyperspectral image from different aspects. In this paper, a multiple 3-D feature fusion framework (M3DF3) has been proposed for hyperspectral image classification. First, we extend the 2-D Gabor surface feature into 3-D (3DSF) domains to comply with the spatial-spectral structure of the hyperspectral image, which is directly applied on the original hyperspectral image instead of the Gabor features. Second, three 3-D feature extraction methods, including the 3-D morphological profile, the 3-D local binary pattern, and the proposed 3DSF, that, respectively, characterize the hyperspectral image from three different angles, i.e., morphology, local dependence, and shape smoothness, are fused under a multitask sparse representation framework to take full advantage of the multiple 3-D features together. The proposed M3DF3 approach was fully tested on three real-world hyperspectral image data, i.e., the widely used Indian Pines, Pavia University, and Houston University. The results show that our method can achieve as high as 68.22%, 79.44%, and 72.84% accuracies, respectively, even when only few samples, i.e., three samples per class, are used for training.","1558-0644","","10.1109/TGRS.2017.2769113","National Natural Science Foundation of China(grant numbers:61671307); Guangdong Special Support Program of Top-notch Young Professionals(grant numbers:2015TQ01X238); Open Fund of Key Laboratory of Urban Land Resources Monitoring and Simulation, Ministry of Land and Resources(grant numbers:KF-2016-02-002); Shenzhen Future Industry Development Funding Program(grant numbers:201607281039561400); Shenzhen Scientific Research and Development Funding Program(grant numbers:JCYJ20160422093647889); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8244247","Feature fusion;hyperspectral image classification;sparse representation","Hyperspectral imaging;Feature extraction;Shape;Wavelet transforms;Surface morphology","feature extraction;Gabor filters;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;land cover","spatial-spectral features;spatial-spectral structure;multiple 3D feature fusion framework;land covers;joint spatial-spectral information;curse-of-dimensionality problem;M3DF3;3D feature extraction methods;3D morphological profile;3D local binary pattern;Indian Pines;Pavia University;Houston University;real-world hyperspectral image data;Gabor features;original hyperspectral image;3DSF;2-D Gabor surface feature;3-D feature extraction operators;spatial properties;hyperspectral image classification","","33","","67","IEEE","1 Jan 2018","","","IEEE","IEEE Journals"
"Robust Pol-ISAR Target Recognition Based on ST-MC-DCNN","X. Bai; X. Zhou; F. Zhang; L. Wang; R. Xue; F. Zhou","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","22 Nov 2019","2019","57","12","9912","9927","Although the deep convolutional neural network (DCNN) has been successfully applied to automatic target recognition (ATR) of ground vehicles based on synthetic aperture radar (SAR), most of the available techniques are not suitable for inverse synthetic aperture radar (ISAR) because they cannot tackle the inherent unknown deformation (e.g., translation, scaling, and rotation) among the training and test samples. To achieve robust polarimetric-ISAR (Pol-ISAR) ATR, this paper proposes the spatial transformer-multi-channel-deep convolutional neural network, i.e., ST-MC-DCNN. In this structure, we adopt the double-layer spatial transformer network (STN) module to adjust the image deformation of each polarimetric channel and then perform a robust hierarchical feature extraction by MC-DCNN. Finally, we carry out feature fusion in the concatenation layer and output the recognition result by the softmax classifier. The proposed network is end-to-end trainable and could learn the optimal deformation parameters automatically from training samples. For the fully Pol-ISAR image database generated from electromagnetic (EM) echoes of four satellites, the proposed structure achieves higher recognition accuracy than traditional DCNN and MC-DCNN. Additionally, it has shown robustness to image scaling, rotation, and combined deformation.","1558-0644","","10.1109/TGRS.2019.2930112","National Natural Science Foundation of China(grant numbers:61522114,61631019); Foundation for the Author of National Excellent Doctoral Dissertation of the People's Republic of China(grant numbers:201448); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B18039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804365","Automatic target recognition (ATR);deep convolutional neural network (DCNN);image deformation;inverse synthetic aperture radar (ISAR)","Feature extraction;Scattering;Strain;Target recognition;Shape;Azimuth;Image recognition","convolutional neural nets;feature extraction;image classification;image fusion;image recognition;radar imaging;radar polarimetry;radar target recognition;synthetic aperture radar;telecommunication computing","robust Pol-ISAR target recognition;ST-MC-DCNN;automatic target recognition;inverse synthetic aperture radar;inherent unknown deformation;polarimetric-ISAR ATR;double-layer spatial transformer network module;image deformation;polarimetric channel;robust hierarchical feature extraction;concatenation layer;recognition result;optimal deformation parameters;training samples;Pol-ISAR image database;image scaling;electromagnetic echoes;feature fusion;spatial transformer-multichannel-deep convolutional neural network","","32","","62","IEEE","16 Aug 2019","","","IEEE","IEEE Journals"
"Low-Rank Decomposition and Total Variation Regularization of Hyperspectral Video Sequences","Y. Xu; Z. Wu; J. Chanussot; M. Dalla Mura; A. L. Bertozzi; Z. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Grenoble Images Parole Signal Automatique Laboratory, Centre National de la Recherche Scientifique, Grenoble Institute of Technology, Grenoble, France; Grenoble Images Parole Signal Automatique Laboratory, Centre National de la Recherche Scientifique, Grenoble Institute of Technology, Grenoble, France; Department of Mathematics, University of California at Los Angeles, Los Angeles, CA, USA; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","27 Feb 2018","2018","56","3","1680","1694","Hyperspectral video sequences (HVSs) are well suited for gas plume detection (GPD). The high spectral resolution allows the detection of chemical clouds even when they are optically thin. Processing this new type of video sequences is challenging and requires advanced image and video analysis algorithms. In this paper, we propose a novel method for GPD recorded in HVSs. Based on the assumption that the background is stationary and the gas plume is moving, the proposed method separates the background from the gas plume via a low-rank and sparse decomposition. Furthermore, taking into consideration that the gas plume is continuous in both spatial and temporal dimensions, we include total variation regularization in the constrained minimization problem, which we solve using the augmented Lagrangian multiplier method. After applying the above process to each extracted feature, a novel fusion strategy is proposed to combine the information into a final detection result. Experimental results using real data sets indicate that the proposed method achieves very promising GPD performance.","1558-0644","","10.1109/TGRS.2017.2766094","National Natural Science Foundation of China(grant numbers:61471199,61772274,61701238,91538108,11431015); ANR ASTRID Program(grant numbers:ANR-16-ASTR-0027-01-APHYPIS); CNRS(grant numbers:PICS-USA 263484); Jiangsu Provincial Natural Science Foundation of China(grant numbers:BK20170858); Fundamental Research Funds for the Central Universities(grant numbers:30917015104); Research Fund of Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks(grant numbers:WSNLBKF201507); National Science Foundation(grant numbers:DMS-1118971); NSF(grant numbers:DMS-1417674); ONR(grant numbers:N00014-16-1-2119); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8126244","Detection;hyperspectral video sequences (HVSs);low-rank;sparse;total variation (LRSTV)","Sparse matrices;Hyperspectral imaging;TV;Video sequences;Feature extraction;Chemicals","chemistry computing;feature extraction;hyperspectral imaging;image fusion;image resolution;image sequences;minimisation;spectrochemical analysis;video signal processing","HVS;spectral resolution;image analysis;fusion strategy;augmented Lagrangian multiplier method;sparse decomposition;video analysis;chemical clouds;GPD;gas plume detection;hyperspectral video sequences;total variation regularization;low-rank decomposition","","31","","50","IEEE","4 Dec 2017","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Based on Multiscale Spatial Information Fusion","H. Li; Y. Song; C. L. P. Chen","School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China; School of Mathematics and Statistics, Huazhong University of Science and Technology, Wuhan, China; Faculty of Science and Technology, University of Macau, Macau, China","IEEE Transactions on Geoscience and Remote Sensing","25 Aug 2017","2017","55","9","5302","5312","In hyperspectral image (HSI) classification, the combination of spectral information and spatial information can be applied to enhance the classification performance. In order to better characterize the variability of spatial features at different scales, we propose a new framework called multiscale spatial information fusion (MSIF). The MSIF consists of three parts: multiscale spatial information extraction, local 1-D embedding (L1-DE), and information fusion. First, spatial filter with different scales is used to extract multiscale spatial information. Then, L1-DE is utilized to map the spectral information and spatial information at different scales into 1-D space, respectively. Finally, the obtained 1-D coordinates are used to label the unlabeled spatial neighbors of the labeled samples. The proposed MSIF captures intrinsic spatial information contained in homogeneous regions of different sizes by multiscale strategy. Since the spatial information at different scales is processed separately in MSIF, the variance of spatial information at different scales can be reflected. The use of L1-DE reduces computational cost by mapping high-dimensional samples into 1-D space. In MSIF, the L1-DE and information fusion are used iteratively, and the iterative process terminates in a finite number of steps. The algorithm analysis demonstrates the effectiveness of the proposed method. The experimental results on four widely used HSI data sets show that the proposed method achieved higher classification accuracies compared with other state-of-the-art spectral-spatial classification methods.","1558-0644","","10.1109/TGRS.2017.2705176","National Natural Science Foundation of China(grant numbers:61472155,91330118,61572540); Macau Science and Technology Development Fund(grant numbers:019/2015/A); UM Multiyear Research Funds; Postdoctoral Science Foundation of China(grant numbers:2015M582223); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944687","Hyperspectral image (HSI) classification;local 1-D embedding (L1-DE);multiscale spatial information","Hyperspectral imaging;Kernel;Support vector machines;Dictionaries;Feature extraction","hyperspectral imaging;image classification;image fusion","hyperspectral image classification;multiscale spatial information fusion;HSI classification;spectral information;spatial information;MSIF;local 1D embedding;L1-DE;multiscale strategy;spectral-spatial classification method","","29","","34","IEEE","8 Jun 2017","","","IEEE","IEEE Journals"
"Efficient Multiple Feature Fusion With Hashing for Hyperspectral Imagery Classification: A Comparative Study","Z. Zhong; B. Fan; K. Ding; H. Li; S. Xiang; C. Pan","National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","1 Jun 2016","2016","54","8","4461","4478","Due to the complementary properties of different features, multiple feature fusion has a large potential for hyperspectral imagery classification. At the meantime, hashing is promising in representing a high-dimensional float-type feature with extremely low bit binary codes while maintaining the performance. In this paper, we study the possibility of using hashing to fuse multiple features for hyperspectral imagery classification. For this purpose, we propose a multiple feature fusion framework to evaluate the performance of using different hashing methods. For comparison and completeness, we also have an extensive comparison to five subspace-based dimension reduction methods and six fusion-based methods which are popular solutions to deal with multiple features in hyperspectral image classification. Experimental results on four benchmark hyperspectral data sets demonstrate that using hashing to fuse multiple features can achieve comparable or better performance with the traditional subspace-based dimension reduction methods and fusion-based methods. Moreover, the binary features obtained by using hashing need much less storage and are faster to compute distances with the help of machine instructions.","1558-0644","","10.1109/TGRS.2016.2542342","National Natural Science Foundation of China(grant numbers:61573352,61472119,91338202,91438105); Beijing Natural Science Foundation(grant numbers:4142057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447770","Binary codes;classification;feature fusion;hashing;hyperspectral images;Binary codes;classification;feature fusion;hashing;hyperspectral images","Feature extraction;Hyperspectral imaging;Binary codes;Kernel;Support vector machines","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion","land-cover classification;fusion-based methods;subspace-based dimension reduction methods;multiple feature fusion framework;high-dimensional float-type feature;hyperspectral imagery classification;efficient multiple feature fusion","","25","","61","IEEE","5 Apr 2016","","","IEEE","IEEE Journals"
"A CNN-Based Spatial Feature Fusion Algorithm for Hyperspectral Imagery Classification","A. J. X. Guo; F. Zhu","Center for Applied Mathematics, Tianjin University, Tianjin, China; Center for Applied Mathematics, Tianjin University, Tianjin, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","7170","7181","The shortage of training samples remains one of the main obstacles in applying the neural networks to the hyperspectral images classification. To fuse the spatial and spectral information, pixel patches are often utilized to train a model, which may further aggregate this problem. In the existing works, an artificial neural network (ANN) model supervised by centerloss (ANNC) was introduced. Training merely with spectral information, the ANNC yields discriminative spectral features suitable for the subsequent classification tasks. In this paper, we propose a novel convolutional neural network (CNN)-based spatial feature fusion (CSFF) algorithm, which allows a smart integration of spatial information to the spectral features extracted by ANNC. As a critical part of CSFF, a CNN-based discriminant model is introduced to estimate whether two pixels belong to the same class. At the testing stage, by applying the discriminant model to the pixel pairs generated by a test pixel and each of its neighbors, the local structure is estimated and represented as a customized convolutional kernel. The spectral-spatial feature is generated by a convolutional operation between the estimated kernel and the corresponding spectral features within a local region. The final label is determined by classifying the resulting spectral-spatial feature. Without increasing the number of training samples or involving pixel patches at the training stage, the CSFF framework achieves the state of the art by declining 20%-50% classification failures in experiments on three well-known hyperspectral images.","1558-0644","","10.1109/TGRS.2019.2911993","National Natural Science Foundation of China(grant numbers:61701337); Natural Science Foundation of Tianjin City(grant numbers:18JCQNJC01600); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709967","Convolutional neural networks;deep learning;feature extraction;hyperspectral image classification","Feature extraction;Training;Hyperspectral imaging;Testing;Training data;Adaptation models","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);pattern classification","hyperspectral imagery classification;artificial neural network model;CNN-based discriminant model;ANNC;CSFF;CNN-based spatial feature fusion;convolutional neural network-based spatial feature fusion;feature extraction","","24","","50","IEEE","8 May 2019","","","IEEE","IEEE Journals"
"NAS-Guided Lightweight Multiscale Attention Fusion Network for Hyperspectral Image Classification","J. Wang; R. Huang; S. Guo; L. Li; M. Zhu; S. Yang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","24 Sep 2021","2021","59","10","8754","8767","Deep learning (DL) has become a hot topic in the research field of hyperspectral image (HSI) classification. However, with increasing depth and size of deep learning methods, its application in mobile and embedded vision applications has brought great challenges. In this article, we address a network architecture search (NAS)-guided lightweight spectral–spatial attention feature fusion network (LMAFN) for HSI classification. The overall architecture of the proposed network is guided by several conclusions of NAS, which achieves fewer parameters and lower computation cost with deeper network structure by exploiting multiscale Ghost grouped with efficient channel attention (ECA) module for adaptively adjusting the weights of different channels. It helps fully extract spectral–spatial discriminant features to avoid information loss of the dimension reduction operation. Specifically, a multilayer feature fusion method is proposed to extract the fusion information of the spectral–spatial features of each layer by considering complementary information of different hierarchical structures. Therefore, high-lever spectral–spatial attributes are gradually exploited along with the increase in layers and the fusion of layers. The experimental verification on three real HSI data sets demonstrates that the proposed framework presents more satisfying classification performance and efficiency with deeper network structure and lower parameter size.","1558-0644","","10.1109/TGRS.2021.3049377","State Key Program of National Natural Science of China(grant numbers:61836009); National Natural Science Foundation of China(grant numbers:61801353,61876221); Project Supported by the Fundamental Research Funds for the Central Universities(grant numbers:JB191907); Project Supported by Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ-065); The Project Supported by the China Postdoctoral Science Foundation(grant numbers:2018M633474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9330782","Attention mechanism;hyperspectral image (HSI) classification;lightweight structure;multilayer feature fusion;multiscale convolution","Convolution;Feature extraction;Kernel;Standards;Computational modeling;Training;Neural networks","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence)","NAS-guided lightweight multiscale attention fusion network;hyperspectral image classification;increasing depth;deep learning methods;mobile embedded vision applications;network architecture search-guided lightweight spectral-spatial attention;HSI classification;deeper network structure;multiscale Ghost;efficient channel attention module;spectral-spatial discriminant features;multilayer feature fusion method;fusion information;spectral-spatial features;different hierarchical structures;high-lever spectral-spatial attributes;HSI data sets;satisfying classification performance;lower parameter size","","20","","47","IEEE","21 Jan 2021","","","IEEE","IEEE Journals"
"Nonnegative and Nonlocal Sparse Tensor Factorization-Based Hyperspectral Image Super-Resolution","W. Wan; W. Guo; H. Huang; J. Liu","Laboratory of Mathematics and Complex Systems (Ministry of Education of China), School of Mathematical Sciences, Beijing Normal University, Beijing, China; Department of Mathematics, Case Western Reserve University, Cleveland, OH, USA; Laboratory of Mathematics and Complex Systems (Ministry of Education of China), School of Mathematical Sciences, Beijing Normal University, Beijing, China; Laboratory of Mathematics and Complex Systems (Ministry of Education of China), School of Mathematical Sciences, Beijing Normal University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","24 Nov 2020","2020","58","12","8384","8394","Hyperspectral image (HSI) super-resolution refers to enhancing the spatial resolution of a 3-D image with many spectral bands (slices). It is a seriously ill-posed problem when the low-resolution (LR) HSI is the only input. It is better solved by fusing the LR HSI with a high-resolution (HR) multispectral image (MSI) for a 3-D image with both high spectral and spatial resolution. In this article, we propose a novel nonnegative and nonlocal 4-D tensor dictionary learning-based HSI super-resolution model using group-block sparsity. By grouping similar 3-D image cubes into clusters and then conduct super-resolution cluster by cluster using 4-D tensor structure, we not only preserve the structure but also achieve sparsity within the cluster due to the collection of similar cubes. We use 4-D tensor Tucker decomposition and impose nonnegative constraints on the dictionaries and group-block sparsity. Numerous experiments demonstrate that the proposed model outperforms many state-of-the-art HSI super-resolution methods.","1558-0644","","10.1109/TGRS.2020.2987530","National Key Research and Development Program of China(grant numbers:2017YFA0604903); China Scholarship Council(grant numbers:201706040141); National Science Foundation(grant numbers:DMS-1521582); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9082892","Hyperspectral imaging (HSI);nonlocal sparse tensor factorization (NLSTF);nonnegative tensor dictionary learning;super-resolution","Tensors;Dictionaries;Sparse matrices;Noise reduction;Spatial resolution;Correlation","geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image representation;image resolution;matrix decomposition;stereo image processing;tensors","nonnegative tensor factorization;spatial resolution;low-resolution HSI;high-resolution multispectral image;group-block sparsity;3D image cubes;super-resolution cluster;HSI super-resolution;nonlocal sparse tensor factorization;hyperspectral image super resolution;4D tensor tucker decomposition;nonlocal 4D tensor dictionary learning;nonnegative 4D tensor dictionary learning;LR HSI fusion","","19","","33","IEEE","30 Apr 2020","","","IEEE","IEEE Journals"
"PAN-Guided Cross-Resolution Projection for Local Adaptive Sparse Representation- Based Pansharpening","H. Yin","College of Automation, College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","24 Jun 2019","2019","57","7","4938","4950","Sparse representation (SR)-based methods solve pansharpening as an image superresolution problem and receive great popularity. Conventional approaches assume that the high- and low-resolution images have the same sparse coefficients. However, the identity mapping is not universal and also limits the performance. To overcome this limitation, this paper proposes a PAN-guided cross-resolution projection-based pan-sharpening (PGCP-PS) which incorporates the SR image superresolution and details injection pansharpening scheme into a framework. The basic idea of PGCP-PS is to inject a possible offset into the SR superresolution reconstructed part. In addition, the same sparse coefficients assumption across different resolutions is relaxed as the same sparse support with a local adaptive cross-resolution projection. By exploiting the similarity between panchromatic (PAN) and multispectral (MS) images, the cross-resolution projection and offset for sharpening the MS image are estimated from a simulated PAN image superresolution scenario. The high- and low-resolution dictionaries used in the stage of SR image superresolution are learned from PAN image and its degraded version. A series of experimental results on the reduced-scale and full-scale data sets demonstrates that the PGCP-PS outperforms some advanced methods and existing SR-based methods.","1558-0644","","10.1109/TGRS.2019.2894702","National Natural Science Foundation of China(grant numbers:61501255,61876092); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8643394","Cross-resolution projection;offset;pansharpening;sparse representation","Spatial resolution;Image reconstruction;Dictionaries;Signal resolution;Image sensors;Sensors","image fusion;image reconstruction;image representation;image resolution;learning (artificial intelligence)","local adaptive sparse representation;sparse representation-based methods;image superresolution problem;low-resolution images;PAN-guided cross-resolution projection-based pan-sharpening;PGCP-PS;SR image superresolution;details injection pansharpening scheme;SR superresolution;sparse coefficients assumption;local adaptive cross-resolution projection;MS image;simulated PAN image superresolution scenario;low-resolution dictionaries;high-resolution images;high-resolution dictionaries","","18","","49","IEEE","17 Feb 2019","","","IEEE","IEEE Journals"
"CRIM-FCHO: SAR Image Two-Stage Segmentation With Multifeature Ensemble","H. Yu; L. Jiao; F. Liu","School of Aerospace Science and Technology, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China","IEEE Transactions on Geoscience and Remote Sensing","10 Mar 2016","2016","54","4","2400","2423","This paper investigates the synthetic aperture radar (SAR) image segmentation in terms of feature analysis and fusion and develops a new algorithm based on multifeature ensemble accordingly. This paper is characterized by two aspects. First, multiple heterogeneous features are extracted to accurately describe the objects in SAR images. These features are then integrated in the feature level and the similarity level, respectively, to avoid the mutual influences between different kinds of features and maximize the discriminability of the similarity measure between objects. Second, a two-stage algorithm consisting of a coarse merging stage and a fine classification stage is proposed. In the coarse merging stage, a context-based region iterative merging algorithm is designed to merge most of the unambiguous superpixels in image domain at a high speed. In the fine classification stage, a fuzzy clustering algorithm incorporating hybrid optimization is developed to balance the efficiency and the robustness of the algorithm by simultaneously searching heuristically in the complete high-dimension feature space and searching along the direction of the gradient steepest descent in each feature subspace. The effectiveness of the proposed method has been successfully validated on synthetic and real SAR images.","1558-0644","","10.1109/TGRS.2015.2501162","National Natural Science Foundation of China(grant numbers:61203202,61201298,61473228,61501352); China Postdoctoral Science Foundation(grant numbers:2014M562376); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:S2015YFJQ0573); Fundamental Research Funds for the Central Universities(grant numbers:JB141304,JB151308); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368911","Fuzzy clustering;image segmentation;multifeature ensemble;region merging;similarity measures;synthetic aperture radar (SAR) image;Fuzzy clustering;image segmentation;multifeature ensemble;region merging;similarity measures;synthetic aperture radar (SAR) image","Synthetic aperture radar;Image segmentation;Feature extraction;Brightness;Merging;Algorithm design and analysis;Clustering algorithms","feature extraction;gradient methods;image fusion;image segmentation;optimisation;radar imaging;synthetic aperture radar","gradient steepest descent;high-dimension feature space;hybrid optimization;fuzzy clustering algorithm;image domain;context-based region iterative merging algorithm;fine classification stage;coarse merging stage;similarity measure discriminability;heterogeneous feature extraction;feature fusion;synthetic aperture radar image segmentation;multifeature ensemble;SAR image two-stage segmentation;CRIM-FCHO","","17","","90","IEEE","30 Dec 2015","","","IEEE","IEEE Journals"
"A Novel Framework to Automatically Fuse Multiplatform LiDAR Data in Forest Environments Based on Tree Locations","H. Guan; Y. Su; T. Hu; R. Wang; Q. Ma; Q. Yang; X. Sun; Y. Li; S. Jin; J. Zhang; Q. Ma; M. Liu; F. Wu; Q. Guo","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Department of Forestry, Mississippi State University, Mississippi State, USA; China National Forestry Economics and Development Research Center, National Forestry and Grassland Administration, Beijing, China; Academy of Inventory and Planning, National Forestry and Grassland Administration, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","26 Feb 2020","2020","58","3","2165","2177","The emerging near-surface light detection and ranging (LiDAR) platforms [e.g., terrestrial, backpack, mobile, and unmanned aerial vehicle (UAV)] have shown great potential for forest inventory. However, different LiDAR platforms have limitations either in data coverage or in capturing undercanopy information. The fusion of multiplatform LiDAR data is a potential solution to this problem. Because of the complexity and irregularity of forests and the inaccurate positioning information under forest canopies, current multiplatform data fusion still involves substantial manual efforts. In this article, we proposed an automatic multiplatform LiDAR data registration framework based on the assumption that each forest has a unique tree distribution pattern. Five steps are included in the proposed framework, i.e., individual tree segmentation, triangulated irregular network (TIN) generation, TIN matching, coarse registration, and fine registration. TIN matching, as the essential step to find the corresponding tree pairs from multiplatform LiDAR data, uses a voting strategy based on the similarity of triangles composed of individual tree locations. The proposed framework was validated by fusing backpack and UAV LiDAR data and fusing multiscan terrestrial LiDAR data in coniferous forests. The results showed that both registration experiments could reach a satisfying data registration accuracy (horizontal root-mean-square error (RMSE) <; 30 cm and vertical RMSE <; 20 cm). Moreover, the proposed framework was insensitive to individual tree segmentation errors, when the individual tree segmentation accuracy was higher than 80%. We believe that the proposed framework has the potential to increase the efficiency of accurately registering multiplatform LiDAR data in forest environments.","1558-0644","","10.1109/TGRS.2019.2953654","National Basic Research Program of China (973 Program)(grant numbers:2016YFC0500202); Chinese Academy of Sciences(grant numbers:KFZD-SW-319-06); National Natural Science Foundation of China(grant numbers:41871332); Study on Monitoring of the Program for Targeted Improvement of Forest Quality Project(grant numbers:0011107); Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930045","Forest;multiplatform light detection and ranging (LiDAR);registration;tree location","Vegetation;Laser radar;Forestry;Tin;Three-dimensional displays;Unmanned aerial vehicles;Registers","autonomous aerial vehicles;forestry;geophysical image processing;image fusion;image matching;image registration;image segmentation;optical radar;vegetation;vegetation mapping","voting strategy;near-surface light detection and ranging platform;multiscan terrestrial LiDAR data fusion;data registration accuracy;automatic multiplatform LiDAR data fusion;forest environments;individual tree segmentation accuracy;individual tree segmentation errors;coniferous forests;UAV LiDAR data;individual tree locations;TIN matching;triangulated irregular network generation;unique tree distribution pattern;automatic multiplatform LiDAR data registration framework;forest canopies;data coverage;forest inventory","","17","","60","IEEE","9 Dec 2019","","","IEEE","IEEE Journals"
"Polar-Spatial Feature Fusion Learning With Variational Generative-Discriminative Network for PolSAR Classification","Z. Wen; Q. Wu; Z. Liu; Q. Pan","Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","30 Oct 2019","2019","57","11","8914","8927","Feature learning-based polarimetric synthetic aperture radar (PolSAR) classification model will generally suffer from the challenge of deficient labeled pixels. In this paper, we propose a novel generative-discriminative network for PolSAR polar-spatial feature fusion learning and classification, which comprises of a deep generative network and a discriminative network with their bottom layers shared. With this architecture, it enables to make use of both labeled and unlabeled pixels in a PolSAR image for model learning in a semisupervised way. Moreover, the proposed network imposes a Gaussian random field prior and a conditional random field posterior on the learned fusion features and the output label configuration, respectively. Without the need of the complicated recurrent iterations, our network can still efficiently produce the structured fusion feature as well as a smoothed classification map by involving some auxiliary variables, and it is specifically optimized via variational inference within an alternating direction method of multipliers iteration scheme. Extensive experiments on different benchmark PolSAR imageries demonstrate the effectiveness and superiority of the proposed network. Compared with other state-of-the-art algorithms of PolSAR feature learning and classification, our model can achieve a much better performance in terms of the visual quality of the label map and overall classification accuracy, facilitating the much less labeling pixels.","1558-0644","","10.1109/TGRS.2019.2923738","National Natural Science Foundation of China(grant numbers:61806165,61790552); Northwestern Polytechnical University(grant numbers:31020180QD040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765386","Deep learning;discriminative model;feature fusion;generative model;polarimetric synthetic aperture radar (PolSAR) classification;variational inference","Feature extraction;Data models;Scattering;Task analysis;Covariance matrices;Adaptation models;Deep learning","image classification;image fusion;iterative methods;neural nets;radar computing;radar imaging;radar polarimetry;synthetic aperture radar","variational generative-discriminative network;PolSAR classification;deficient labeled pixels;PolSAR polar-spatial feature fusion learning;deep generative network;unlabeled pixels;PolSAR image;Gaussian random field;conditional random field;learned fusion features;output label configuration;structured fusion feature;smoothed classification map;benchmark PolSAR imagery;feature learning;polarimetric synthetic aperture radar;complicated recurrent iterations;alternating direction method of multipliers iteration","","16","","47","IEEE","17 Jul 2019","","","IEEE","IEEE Journals"
"Cross Fusion Net: A Fast Semantic Segmentation Network for Small-Scale Semantic Information Capturing in Aerial Scenes","C. Peng; K. Zhang; Y. Ma; J. Ma","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","13","Capturing accurate multiscale semantic information from the images is of great importance for high-quality semantic segmentation. Over the past years, a large number of methods attempt to improve the multiscale information capturing ability of the networks via various means. However, these methods always suffer unsatisfactory efficiency (e.g., speed or accuracy) on the images that include a large number of small-scale objects, for example, aerial images. In this article, we propose a new network named cross fusion net (CF-Net) for fast and effective extraction of the multiscale semantic information, especially for small-scale semantic information. In particular, the proposed CF-Net can capture more accurate small-scale semantic information from two aspects. On the one hand, we develop a channel attention refinement block to select the informative features. On the other hand, we propose a cross fusion block to enlarge the receptive field of the low-level feature maps. As a result, the network can encode more accurate semantic information from the small-scale objects, and the segmentation accuracy of the small-scale objects is improved accordingly. We have compared the proposed CF-Net with several state-of-the-art semantic segmentation methods on two popular aerial image segmentation data sets. Experimental results reveal that the average  $F_{1}$  score gain brought by our CF-Net is about 0.43% and the  $F_{1}$  score gain of the small-scale objects (e.g., cars) is about 2.61%. In addition, our CF-Net has the fastest inference speed, which proves its superiority in the aerial scenes. Our code will be released at: https://github.com/pcl111/CF-Net.","1558-0644","","10.1109/TGRS.2021.3053062","National Natural Science Foundation of China(grant numbers:62061160370,61773295); Key Research and Development Program of Hubei Province(grant numbers:2020BAB113); Natural Science Foundation of Hubei Province(grant numbers:2019CFA037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340601","Aerial image;deep learning;multiscale;semantic segmentation;small scale","Semantics;Image segmentation;Feature extraction;Data mining;Convolution;Deep learning;Learning systems","aerospace computing;image capture;image fusion;image segmentation;semantic networks","cross fusion net;fast semantic segmentation network;small-scale semantic information capturing;aerial scenes;multiscale semantic information;high-quality semantic segmentation;multiscale information capturing ability;small-scale objects;informative features;cross fusion block;aerial image segmentation data sets","","16","","52","IEEE","29 Jan 2021","","","IEEE","IEEE Journals"
"Reconstruction From Multispectral to Hyperspectral Image Using Spectral Library-Based Dictionary Learning","X. Han; J. Yu; J. Luo; W. Sun","State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; School of Optoelectronics, Beijing Institute of Technology, Beijing, China; State Key Laboratory of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","25 Feb 2019","2019","57","3","1325","1335","High-spatial hyperspectral (HH) image reconstruction using both high-spatial multispectral (HM) image and low-spatial hyperspectral (LH) image over the same scene is widely used in many real applications. Nevertheless, the pair of HM image and LH image over the same scene is hard to obtain. To solve this problem, a new HH image reconstruction method using spectral library-based dictionary learning (named as HIRSL) is proposed in this paper, only from one HM image. The above reconstruction problem is formulated in the framework of sparse representation, as an estimation of the band matching matrix, the spectral dictionary, and the sparse coefficients. More specifically, a band matching method is proposed for mapping the common spectral library to a specific spectral library corresponding to the reconstructed HH image in spectral domain. Then, an efficient spectral dictionary learning method is proposed for the construction of spectral dictionary using the matched specific spectral library, which avoids the dependence of the LH image over the same scene. Finally, the sparse coefficients of the HM image with respect to the learned spectral dictionary are estimated using the alternating direction method of multipliers without nonnegative constraint. Comparison results on simulated and real data sets with the relative state-of-the-art methods demonstrate that even only using one HM image, our proposed method achieves a comparable reconstruction quality of high-spatial hyperspectral image both in spatial and spectral domains.","1558-0644","","10.1109/TGRS.2018.2866054","National Natural Science Foundation of China(grant numbers:61501008); Natural Science Foundation of Beijing Municipality(grant numbers:4172002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8465699","Hyperspectral image reconstruction;sparse representation;spectral dictionary learning;spectral library","Image reconstruction;Hyperspectral imaging;Dictionaries;Libraries;Spatial resolution;Machine learning","geophysical image processing;hyperspectral imaging;image classification;image fusion;image matching;image reconstruction;image representation;learning (artificial intelligence);matrix algebra","spectral library-based dictionary learning;high-spatial hyperspectral image reconstruction;high-spatial multispectral image;low-spatial hyperspectral;LH image;HH image reconstruction method;one HM image;reconstruction problem;band matching matrix;sparse coefficients;band matching method;common spectral library;reconstructed HH image;spectral domain;matched specific spectral library;comparable reconstruction quality;spatial domains;spectral domains;spectral dictionary learning;HIRSL","","15","","52","IEEE","14 Sep 2018","","","IEEE","IEEE Journals"
"Hyperspectral Restoration and Fusion With Multispectral Imagery via Low-Rank Tensor-Approximation","N. Liu; L. Li; W. Li; R. Tao; J. E. Fowler; J. Chanussot","Université Grenoble Alpes, Grenoble, France; School of Automation, Beijing Information Science and Technology University, Beijing, China; Beijing Key Laboratory of Fractional Signals and Systems, Beijing, China; Beijing Key Laboratory of Fractional Signals and Systems, Beijing, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2021","2021","59","9","7817","7830","Tensor-based fusion that couples the high spatial resolution of a multispectral image (MSI) to the high spectral resolution of a hyperspectral image (HSI) is considered. The fusion problem is first formulated mathematically as a convex optimization of a tensor trace norm imposing low-rank spatially as well as spectrally, with an alternating-directions optimization featuring linearization providing the solution. Although prior tensor-based fusion approaches typically resort to tensor decomposition, the proposed algorithm exploits ideas from the field of tensor completion to directly impose a low-rank property spatially and spectrally while avoiding the computationally complex patch clustering and dictionary learning common to competing fusion techniques. Additionally, small modifications to the basic optimization permit a fusion process robust to missing hyperspectral values such as those that can result from dead stripes in real hyperspectral sensors. The experimental evaluations on both synthetic imagery as well as real imagery demonstrate that the resulting low-rank tensor-approximation (LRTA) fusion algorithm preserves both spatial details and texture, yielding significantly improved image quality when compared to other state-of-the-art fusion methods as well as effective restoration under conditions of missing stripes within the HSI.","1558-0644","","10.1109/TGRS.2020.3049014","Beijing Natural Science Foundation(grant numbers:JQ20021); National Natural Science Foundation of China(grant numbers:61922013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328229","Data fusion;hyperspectral imagery (HSI);low-rank tensor","Tensors;Spatial resolution;Degradation;Hyperspectral imaging;Image restoration;Interpolation;Clustering algorithms","computational complexity;geophysical image processing;image fusion;image representation;image restoration;iterative methods;medical image processing;optimisation;sensor fusion;tensors","high spectral resolution;hyperspectral image;HSI;fusion problem;convex optimization;tensor trace;alternating-directions optimization;prior tensor-based fusion approaches;tensor completion;low-rank property;computationally complex patch clustering;dictionary learning;fusion techniques;basic optimization;fusion process;hyperspectral values;hyperspectral sensors;synthetic imagery;resulting low-rank tensor-approximation fusion algorithm;spatial details;texture;improved image quality;state-of-the-art fusion methods;hyperspectral restoration;multispectral imagery;high spatial resolution;multispectral image","","15","","40","IEEE","18 Jan 2021","","","IEEE","IEEE Journals"
"Game Theory-Based Hyperspectral Anomaly Detection","Z. Huang; X. Kang; S. Li; Q. Hao","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2020","2020","58","4","2965","2976","In this article, a novel game theory-based approach is proposed for anomaly detection in hyperspectral images (HSIs) via effectively exploring multiple spectral and spatial characteristics of anomalies. This approach comprises three main steps. First, spectral, extend morphological profiles (EMPs), and Gabor texture features are captured from an input HSI. Then, we define the anomaly detection problem as an anomaly game model, in which image regions (superpixels) of different features are modeled as players who select to be “anomaly” or “background” as their strategies. Three initial detection results are produced based on each player's strategy in the Nash equilibrium of the anomaly game. Last, a saliency-based decision fusion technique is used to combine the complementary information in different features, so as to obtain a fused detection map. The performance of the proposed anomaly detection technique is evaluated on four real-scene HSIs. Experimental results validate that our approach can outperform some state-of-the-art anomaly detection methods.","1558-0644","","10.1109/TGRS.2019.2958359","National Natural Science Foundation of China(grant numbers:61890962); National Natural Science Foundation of China(grant numbers:61520106001); National Natural Science Foundation of China(grant numbers:61871179); Fund of Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province(grant numbers:2018TP1013); Fund of Hunan Province for Science and Technology Plan Project(grant numbers:2017RS3024); Hunan Provincial Innovation Foundation for Postgraduate(grant numbers:CX20190300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945176","Anomaly detection;decision fusion;game theory;hyperspectral images (HSIs);Nash equilibrium;spectral–spatial information","Games;Feature extraction;Anomaly detection;Hyperspectral imaging;Nash equilibrium","feature extraction;game theory;hyperspectral imaging;image fusion;image texture;object detection","game theory;hyperspectral images;Gabor texture features;anomaly detection problem;anomaly game model;image regions;saliency-based decision fusion technique;fused detection map;hyperspectral anomaly detection;extend morphological profiles;Nash equilibrium","","13","","66","IEEE","30 Dec 2019","","","IEEE","IEEE Journals"
"A Hyperspectral Anomaly Detection Method Based on Low-Rank and Sparse Decomposition With Density Peak Guided Collaborative Representation","S. Feng; S. Tang; C. Zhao; Y. Cui","Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","13","The low-rank and sparse decomposition model (LSDM) has been widely studied by researchers and has successfully solved the problem of hyperspectral image (HSI) anomaly detection (AD). The traditional LSDM usually ignores the information of the low-rank matrix, which only detects the anomalous targets by using the sparse component. To utilize both the sparse component and the low-rank component comprehensively, an anomaly detector for HSIs based on LSDM with density peak guided collaborative representation (LSDDPCRD) is proposed in this article. First, the LSDM technique with the mixture of Gaussian model is used to decompose the original HSI, which can also alleviate the background noise contamination problem. Then, the low-rank matrix is detected by the density peak guided collaborative representation detection algorithm, while the sparse matrix is calculated according to the Manhattan distance. In addition, an entropy-based adaptive fusing method is designed to combine the results obtained from the low-rank matrix and the sparse component. It could choose the fusing weights adaptively according to the characteristics of an HSI. The experimental results indicate that the LSDDPCRD performs better than eight classical and state-of-the-art AD algorithms (GRX, LRX, SRX-Segmented, CRD, RPCA-RX, LSMAD, LRASR, and LSDM-MoG) on four real HSIs.","1558-0644","","10.1109/TGRS.2021.3054736","National Natural Science Foundation of China(grant numbers:62002083,61971153,61801142,62071136); Heilongjiang Postdoctoral Foundation(grant numbers:LBH-Z20051); Heilongjiang Postdoctoral Funds for Scientific Research Initiation(grant numbers:LBH-Q20085); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9349473","Collaborative representation;density peak;hyperspectral anomaly detection (AD);hyperspectral image (HSI);low-rank and sparse decomposition","Sparse matrices;Hyperspectral imaging;Detectors;Matrix decomposition;Covariance matrices;Collaboration;Task analysis","entropy;feature extraction;Gaussian processes;hyperspectral imaging;image fusion;image representation;image resolution;image segmentation;object detection;sparse matrices","density peak guided collaborative representation;hyperspectral image anomaly detection;low-rank matrix;sparse component;low-rank component;anomaly detector;LSDM technique;sparse matrix;low-rank and sparse decomposition model;entropy-based adaptive fusing method;Gaussian model","","13","","50","IEEE","8 Feb 2021","","","IEEE","IEEE Journals"
"Feature Fusion With Predictive Weighting for Spectral Image Classification and Segmentation","Y. Zhang; C. P. Huynh; K. N. Ngan","Electronic Engineering Department, The Chinese University of Hong Kong, Hong Kong; Amazon Lab126, Sunnyvale, CA, USA; University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","6792","6807","In this paper, we propose a spatial-spectral feature fusion model with a predictive feature weighting mechanism and demonstrate its applications to the problems of hyperspectral image classification and segmentation. To address these problems, we learn a set of 1-D convolutional local spectral filters and 2-D spatial-spectral filters that feed features into a fusion module, in an end-to-end fashion. We propose a lightweight predictive feature weighting component embedded in the fusion model and consider four design fusion options, i.e., by adding or concatenating features with equal or predicted weights. For the pixel classification task, the training input consists of image patches with labeled central pixels, whereas for the spatial segmentation task, it includes the label maps of image regions. The proposed networks have been evaluated on the Indian Pines, Pavia University, and Houston University for the classification problem and the SpaceNet data set for the spatial segmentation problem. The quantitative results favor the proposed approach over the state-of-the-art methods across all the four data sets.","1558-0644","","10.1109/TGRS.2019.2908679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698434","Convolutional neural networks (CNNs);fusion models;hyperspectral image classification;hyperspectral image segmentation;predictive feature weighting;semantic segmentation","Feature extraction;Image segmentation;Hyperspectral imaging;Task analysis;Training;Convolution","feature extraction;filtering theory;geophysical image processing;hyperspectral imaging;image classification;image fusion;image segmentation;learning (artificial intelligence)","Houston University;Pavia University;Indian Pines;concatenating features;image patches;pixel classification task;equal predicted weights;design fusion options;lightweight predictive feature;fusion module;feed features;2-D spatial-spectral filters;1-D convolutional local spectral filters;hyperspectral image classification;predictive feature weighting mechanism;spatial-spectral feature fusion model;spectral image classification;predictive weighting;spatial segmentation problem;classification problem;image regions;spatial segmentation task;labeled central pixels","","11","","58","IEEE","24 Apr 2019","","","IEEE","IEEE Journals"
"Multilevel Superpixel Structured Graph U-Nets for Hyperspectral Image Classification","Q. Liu; L. Xiao; J. Yang; Z. Wei","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2022","2022","60","","1","15","Limited by the shape-fixed kernels, convolutional neural networks (CNNs) are usually difficult to model difform land covers in hyperspectral images (HSIs), leading to inadequate land use. Recently, benefiting from the ability to conduct shape-adaptive convolutions and model complex patterns in graph-structured data, graph convolutional networks (GCNs) have been applied to HSI classification. However, due to the massive computation in GCNs, HSI is usually pretreated into a graph based on a specific superpixel segmentation, which limits the modeling of spatial topologies to the same scale. To break this limitation, we propose a multilevel superpixel structured graph U-Net (MSSGU) to learn multiscale features on multilevel graphs. Specifically, we construct several hierarchical segmentations from fine to coarse by progressively merging adjacent superpixels and then convert them into multilevel graphs. Meanwhile, based on the merging relations between hierarchical superpixels, we establish the pooling and unpooling functions to transfer features from one graph to another, thereby enabling different-level graphs to collaborate in a single network. Different from concatenating different-scale features straightforwardly in the feature fusion stage, MSSGU fuses them in a coarse-to-fine progressive manner, which can generate subtler fusion features adaptive to the pixelwise classification task. Moreover, we use a CNN instead of GCN to extract and fuse the pixel-level features, which greatly reduces the computation. Such a hybrid U-Net can exploit features of HSIs from a multiscale hierarchical perspective, and its performance has been proven competitive with other deep-learning-based methods by extensive experiments on three benchmark datasets.","1558-0644","","10.1109/TGRS.2021.3112586","National Natural Science Foundation of China(grant numbers:61871226,61571230,62001226); Jiangsu Provincial Social Developing Project(grant numbers:BE2018727); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200465); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547387","Convolutional neural network (CNN);graph convolutional network (GCN);graph U-Net;hierarchical superpixel segmentation;hyperspectral image (HSI) classification","Feature extraction;Convolutional neural networks;Topology;Merging;Fuses;Vegetation;Kernel","convolutional neural nets;feature extraction;geophysical image processing;graph theory;image classification;image colour analysis;image fusion;image representation;image resolution;image segmentation;land cover;learning (artificial intelligence)","GCNs;HSI classification;specific superpixel segmentation;multilevel superpixel structured graph;multiscale features;multilevel graphs;hierarchical segmentations;adjacent superpixels;hierarchical superpixels;different-level graphs;different-scale features;feature fusion stage;coarse-to-fine progressive manner;subtler fusion features;pixelwise classification task;pixel-level features;hybrid U-Net;HSI;multiscale hierarchical perspective;U-nets;hyperspectral image classification;shape-fixed kernels;convolutional neural networks;difform land covers;hyperspectral images;inadequate land use;shape-adaptive convolutions;model complex patterns;graph-structured data;graph convolutional networks","","10","","46","IEEE","24 Sep 2021","","","IEEE","IEEE Journals"
"A Multichannel Data Fusion Method to Enhance the Spatial Resolution of Microwave Radiometer Measurements","M. Alparone; F. Nunziata; C. Estatico; M. Migliaccio","Dipartimento di Ingegneria, Università degli Studi di Napoli “Parthenope,”, Naples, Italy; Dipartimento di Ingegneria, Università degli Studi di Napoli “Parthenope,”, Naples, Italy; Dipartimento di Matematica, Università degli Studi di Genoa, Genoa, Italy; Dipartimento di Ingegneria, Università degli Studi di Napoli “Parthenope,”, Naples, Italy","IEEE Transactions on Geoscience and Remote Sensing","24 Feb 2021","2021","59","3","2213","2221","In this study, a method to improve the reconstruction performance of antenna-pattern deconvolution based on the gradient iterative regularization scheme is proposed. The method exploits microwave measurements acquired by a multichannel radiometer to enhance their native spatial resolution. The proposed rationale consists of using the information carried on a high-frequency (finer spatial resolution) channel to ameliorate the spatial resolution of the lowest resolution radiometer channel. Experiments performed using both synthetic and real special sensor microwave/imager (SSM/I) radiometer data demonstrate that an enhanced spatial resolution 19.35-GHz channel can be obtained by ingesting in the algorithm information coming from 37.0-GHz channel. This multichannel spatial resolution method is also shown to outperform the conventional gradient-like regularization scheme in terms of both observation of smaller targets and reduction of ringings and fluctuations.","1558-0644","","10.1109/TGRS.2020.3005204","GNCS-INDAM, Italy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134973","Inverse problem;microwave radiometer (MWR);multichannel data fusion;resolution enhancement","Spatial resolution;Microwave radiometry;Microwave measurement;Frequency measurement;Kernel;Microwave imaging;Microwave theory and techniques","data fusion;deconvolution;gradient methods;image fusion;image reconstruction;image resolution;image sensors;iterative methods;microwave antennas;microwave detectors;microwave imaging;microwave measurement;millimetre wave antennas;millimetre wave detectors;millimetre wave imaging;millimetre wave measurement;radiometers","SSM-I radiometer data;radiometer data channel;enhanced spatial resolution channel;multichannel spatial resolution method;multichannel radiometer;microwave measurements;gradient iterative regularization scheme;antenna-pattern deconvolution;microwave radiometer measurements;multichannel data fusion method;frequency 19.35 GHz;frequency 37.0 GHz","","10","","23","IEEE","7 Jul 2020","","","IEEE","IEEE Journals"
"Boosting Hyperspectral Image Classification With Unsupervised Feature Learning","W. Wei; S. Xu; L. Zhang; J. Zhang; Y. Zhang","Research and Development Institute, Northwestern Polytechnical University Shenzhen Zhongnan Network Learning Center, Shenzhen, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","6 Dec 2021","2022","60","","1","15","The deep learning-based method has shown promising competence in image classification. Its success can be attributed to the ability to learn discriminative feature representation given plenty of labeled data. However, in real-hyperspectral image (HSI) classification applications, since pixel labeling is difficult and costly, the labels we can obtain within an HSI are always limited and noisy (i.e., inaccurate), which consequently causes overfitting of the deep learning-based method. To address this problem, we propose a novel unified deep learning network to employ both labeled and unlabeled data for training, with which the unsupervised structure knowledge, e.g., intracluster similarity and intercluster dissimilarity, inherently contained in those unlabeled data can be exploited to boost the conventional supervised classification. Specifically, we first explore the unsupervised structure knowledge in unlabeled data via a clustering method and formulate a supervised clustering task on those data with the obtained cluster labels. Then, we propose a multitask network to jointly address both the conventional classification task and the formulated supervised clustering task. With a shared feature extraction module and a high-level feature fusion module, the unsupervised structure knowledge contained in unlabeled data can be effectively introduced into the classification task, which is beneficial to learn a more discriminative feature representation and, thus, well mitigates the overfitting problem and improves the classification results. Experimental results on three data sets demonstrate the proposed method can effectively label the unlabeled data within an HSI, especially when the training labels are limited and noisy.","1558-0644","","10.1109/TGRS.2021.3054037","National Natural Science Foundation of China(grant numbers:62071387,61671385,U19B2037); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20190806160210899); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369394","Feature fusion;hyperspectral image (HIS) classification;multitask method;semisupervised learning","Feature extraction;Task analysis;Noise measurement;Data mining;Training;Support vector machines;Hyperspectral imaging","deep learning (artificial intelligence);feature extraction;geophysical image processing;geophysics computing;hyperspectral imaging;image classification;image fusion;image representation;pattern clustering;supervised learning;unsupervised learning","unsupervised feature learning;deep learning;discriminative feature representation;hyperspectral image classification;pixel labeling;supervised classification;supervised clustering;feature extraction;feature fusion module;multitask network;Indian Pines data set","","8","","37","IEEE","4 Mar 2021","","","IEEE","IEEE Journals"
"Ship Size Extraction for Sentinel-1 Images Based on Dual-Polarization Fusion and Nonlinear Regression: Push Error Under One Pixel","B. Li; B. Liu; W. Guo; Z. Zhang; W. Yu","Shanghai Key Laboratory of Intelligent Sensing and Recognition, Shanghai Jiao Tong University, Shanghai, China; College of Marine Sciences, Shanghai Ocean University, Shanghai, China; Shanghai Key Laboratory of Intelligent Sensing and Recognition, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Intelligent Sensing and Recognition, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Intelligent Sensing and Recognition, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","20 Jul 2018","2018","56","8","4887","4905","In this paper, we present a method of ship size extraction for Sentinel-1 synthetic aperture radar (SAR) images, which is composed of the image processing stage and the regression stage. In order to achieve extraction with high accuracy, considering the data characteristics of Sentinel-1 images, we propose to use the dual-polarization fusion and the nonlinear regression with the gradient boosting. The experiments and analyses on a relatively large data set show that: 1) compared with the existing and related studies, the proposed method achieves an improved performance. The extraction errors are pushed under one pixel, and they are 4.66% (8.80 m) and 7.01% (2.17 m) for length and width, respectively; 2) the dual-polarization information fusion does improve the size extraction accuracy; and 3) the nonlinear regression does exploit the relationship between the influential factors and the size parameters and provide a better performance than the linear regression. The experimental results verify that the proposed design is suitable for ship size extraction in Sentinel-1 SAR images.","1558-0644","","10.1109/TGRS.2018.2841882","National Natural Science Foundation of China(grant numbers:61331015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392506","Dual-polarization fusion;nonlinear regression;Sentinel-1;ship size extraction;synthetic aperture radar (SAR) image","Marine vehicles;Synthetic aperture radar;Image resolution;Scattering;Data mining;Surveillance;Azimuth","gradient methods;image fusion;marine radar;radar imaging;regression analysis;ships;synthetic aperture radar","ship size extraction;Sentinel-1 SAR images;dual-polarization fusion;nonlinear regression;Sentinel-1 synthetic aperture radar images;image processing stage;regression stage;dual-polarization information fusion;size extraction accuracy;size parameters;linear regression","","7","","49","IEEE","21 Jun 2018","","","IEEE","IEEE Journals"
"AFSar: An Anchor-Free SAR Target Detection Algorithm Based on Multiscale Enhancement Representation Learning","H. Wan; J. Chen; Z. Huang; R. Xia; B. Wu; L. Sun; B. Yao; X. Liu; M. Xing","Key Laboratory of Intelligent Computing and Signal Processing, Ministry of Education, School of Electronics and Information Engineering, Anhui University, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing, Ministry of Education, School of Electronics and Information Engineering, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing, Ministry of Education, School of Electronics and Information Engineering, Anhui University, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","3 Mar 2022","2022","60","","1","14","Unlike optical images, synthetic aperture radar (SAR) images have unique characteristics, such as few samples, strong scattering, sparseness, multiple scales, complex interference and background, and inconspicuous target edge contour information. Current SAR target detection algorithms have difficulty in balancing accuracy and speed, and the performance of these algorithms is relatively limited, thus making it difficult to deploy practical applications. To this end, this article proposes AFSar, an innovative anchor-free SAR target detection algorithm based on multiscale enhancement representation learning. First, we introduce the latest anchor-free architecture YOLOX as the basic framework. Second, to reduce the computational complexity of the model and to improve the ability of multiscale feature extraction, we redesigned the lightweight backbone, namely, MobileNetV2S. Furthermore, we propose an attention enhancement PAN module, called CSEMPAN, which highlights the unique strong scattering characteristics of SAR targets by integrating channel and spatial attention mechanisms. Finally, in view of the multiscale and strong sparse characteristics of SAR targets, we propose a new target detection head, namely, ESPHead. ESPHead extracts the features of targets with different scales by using dilated convolution with different dilated rates, so as to enhance the detection ability of the model for targets with different scales. The results of ablation experiments on the SSDD dataset show that the mAP of our algorithm reaches 0.977, while the Flops is only 9.86 G, achieving state of the art.","1558-0644","","10.1109/TGRS.2021.3137817","National Natural Science Foundation of China(grant numbers:62001003); Natural Science Foundation of Anhui Province(grant numbers:2008085QF284); China Postdoctoral Science Foundation(grant numbers:2020M671851); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661358","Anchor-free;deep learning;enhancement representation learning;multiscale learning;synthetic aperture radar (SAR) target detection","Feature extraction;Object detection;Radar polarimetry;Synthetic aperture radar;Detection algorithms;Optical imaging;Prediction algorithms","feature extraction;image fusion;object detection;radar imaging;synthetic aperture radar","innovative anchor-free SAR target detection algorithm;multiscale enhancement representation learning;latest anchor-free architecture;multiscale feature extraction;attention enhancement PAN module;unique strong scattering characteristics;SAR targets;multiscale characteristics;strong sparse characteristics;target detection head;AFSar;optical images;synthetic aperture radar images;complex interference;inconspicuous target edge contour information;current SAR target detection algorithms","","6","","47","IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"Fusion of Sparse Model Based on Randomly Erased Image for SAR Occluded Target Recognition","Z. He; H. Xiao; C. Gao; Z. Tian; S. -W. Chen","National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","26 Oct 2020","2020","58","11","7829","7844","The recognition of partially occluded targets is a difficult problem in the field of synthetic aperture radar (SAR) target recognition. To eliminate the effect of occlusion, the intuitive idea is to determine the exact location and the size of the occluded area. However, this is very difficult, even impossible in practice. In order to avoid this difficulty and to improve the recognition performance for the partially occluded target, a fusion strategy of the sparse representation (SR) model based on randomly erased images is proposed to recognize the partially occluded target. The proposed method randomly erases some areas many times in both the test samples and the training samples. The erased training samples in each erasure are used to sparsely represent the corresponding erased test sample. Finally, all the SR results are fused to recognize the test sample. The proposed method utilizes random erasure to eliminate the possible occluded region. In addition, this method uses the fusion strategy to overcome under-erasing of the occluded region and erroneous erasure of the unoccluded region. The key parameter of the proposed method is the erasure ratio only. Although the erasure is random, the recognition performance of the method is relatively stable. Therefore, the method can eliminate the influence of occlusion without determining the details of occlusion. The experimental results show that the proposed method is significantly better than the state-of-the-art methods in the case of occlusion. Additionally, the recognition performance of the proposed method is similar to some comparison methods in the case of no occlusion.","1558-0644","","10.1109/TGRS.2020.2984577","National Natural Science Foundation of China(grant numbers:61771480); National Pre-Research Foundation(grant numbers:61404150104,61404160109); Science and Technology Planning Project of Hunan Province(grant numbers:2019RS2025); Project of National University of Defense Technology(grant numbers:ZK18-02-14); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9067033","Automatic target recognition (ATR);fusion strategy;occluded target;random erasure;sparse representation (SR);synthetic aperture radar (SAR)","Target recognition;Mathematical model;Synthetic aperture radar;Image reconstruction;Training;Electromagnetic scattering","image fusion;image recognition;image representation;learning (artificial intelligence);radar imaging;radar target recognition;synthetic aperture radar","sparse model;randomly erased image;SAR;partially occluded target;training samples;occluded target recognition;synthetic aperture radar;sparse representation model;randomly erased images;erroneous erasure;erasure ratio","","6","","39","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"Multiresolution Compressive Feature Fusion for Spectral Image Classification","J. M. Ramirez; H. Arguello","Department of Electrical Engineering, Universidad de Los Andes, Mérida, Venezuela; Department of Computer Science, Universidad Industrial de Santander, Bucaramanga, Colombia","IEEE Transactions on Geoscience and Remote Sensing","22 Nov 2019","2019","57","12","9900","9911","Compressive spectral imaging (CSI) has emerged as an alternative acquisition framework that simultaneously senses and compresses spectral images. In this context, the spectral image classification from CSI compressive measurements has become a challenging task since the feature extraction stage usually requires reconstructing the spectral image. Moreover, most approaches do not consider multi-sensor compressive measurements. In this paper, an approach for fusing features obtained from multi-sensor compressive measurements is proposed for spectral image classification. To this end, linear models describing low-resolution features as degraded versions of the high-resolution features are developed. Furthermore, an inverse problem is formulated aiming at estimating high-resolution features including both a sparsity-inducing term and a total variation (TV) regularization term to exploit the correlation between neighboring pixels of the spectral image, and therefore, to improve the performance of pixel-based classifiers. An algorithm based on the alternating direction method of multipliers (ADMM) is described for solving the fusion problem. The proposed feature fusion approach is tested for two CSI architectures: three-dimensional coded aperture snapshot spectral imaging (3D-CASSI) and colored CASSI (C-CASSI). Extensive simulations on various spectral image data sets show that the proposed approach outperforms other classification approaches under different performance criteria.","1558-0644","","10.1109/TGRS.2019.2930093","Universidad Industrial de Santander(grant numbers:8914); Vicerrectoría de Investigación y Extensión of the Universidad Industrial de Santander through the Postdoctoral Internship Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809355","Compressive spectral imaging (CSI);feature fusion;fusion algorithms;multi-sensor;spectral image classification","Feature extraction;Image coding;Apertures;Sensors;Optical imaging;Image resolution","data compression;feature extraction;geophysical image processing;image classification;image coding;image colour analysis;image fusion;image reconstruction;image resolution","multiresolution compressive feature fusion;spectral image classification;compressive spectral imaging;senses;compresses spectral images;CSI compressive measurements;feature extraction stage;multisensor compressive measurements;low-resolution features;high-resolution features;feature fusion approach;spectral image data sets","","6","","43","IEEE","21 Aug 2019","","","IEEE","IEEE Journals"
"Wavenumber-Domain Multiband Signal Fusion With Matrix-Pencil Approach for High-Resolution Imaging","J. Wang; P. Aubry; A. Yarovoy","Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands; Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands","IEEE Transactions on Geoscience and Remote Sensing","22 Jun 2018","2018","56","7","4037","4049","In this paper, a wavenumber-domain matrix-pencil-based multiband signal fusion approach was proposed for multiband microwave imaging. The approach proposed is based on the Born approximation of the field scattered from a target resulting in the fact that in a given scattering direction, the scattered field can be represented over the whole frequency band as a sum of the same number of contributions. Exploiting the measured multiband data and taking advantage of the parametric modeling for the signals in a radial direction, a unified signal model can be estimated for a large bandwidth in the wavenumber domain. It can be used to fuse the signals at different subbands by extrapolating the missing data in the frequency gaps between them or coherently integrating the overlaps between the adjacent subbands, thus synthesizing an equivalent wideband signal spectrum. Taking an inverse Fourier transform, the synthesized spectrum results in a focused image with improved resolution. Compared with the space–time domain fusion methods, the proposed approach is applicable for radar imaging with the signals collected by either collocated or noncollocated arrays in different frequency bands. Its effectiveness and accuracy are demonstrated through both numerical simulations and experimental imaging results.","1558-0644","","10.1109/TGRS.2018.2821001","European Commission within the FP-7 Framework Program through NeTTUN Project(grant numbers:280712); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8340833","Matrix-pencil approach (MPA);microwave imaging;multiband signal;signal fusion;wavenumber domain","Radar imaging;Bandwidth;Antenna arrays;Scattering;Microwave imaging","approximation theory;electromagnetic wave scattering;extrapolation;Fourier transforms;image fusion;image resolution;inverse transforms;matrix algebra;microwave imaging;radar imaging","scattering direction;inverse Fourier transform;radar imaging;focused image;synthesized spectrum results;equivalent wideband signal spectrum;frequency gaps;missing data;unified signal model;radial direction;parametric modeling;measured multiband data;frequency band;scattered field;Born approximation;multiband microwave imaging;multiband signal fusion approach;wavenumber-domain matrix-pencil;high-resolution imaging;matrix-pencil approach;wavenumber-domain multiband signal fusion","","6","","30","IEEE","18 Apr 2018","","","IEEE","IEEE Journals"
"Multiscale Deep Learning Network With Self-Calibrated Convolution for Hyperspectral and LiDAR Data Collaborative Classification","Z. Xue; X. Yu; X. Tan; B. Liu; A. Yu; X. Wei","PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China; PLA Strategic Support Force Information Engineering University, Zhengzhou, China","IEEE Transactions on Geoscience and Remote Sensing","27 Jan 2022","2022","60","","1","16","In this article, we propose a novel multiscale deep learning network with self-calibrated convolution (MSNetSC) for hyperspectral and light detection and ranging (LiDAR) data collaborative classification. Conventional deep learning methods have limitations in extracting multiscale features at a granular level from multimodality data and fusing these features in a context-awareness way, which will severely restrict the performance of hyperspectral and LiDAR data joint classification. The proposed multiscale deep learning network utilizes a hierarchical residual structure combined with self-calibrated convolution to extract features with different receptive fields, and this can enhance the model’s capability to represent the multimodality data. Besides, we employ spectral and spatial self-attention modules to adaptively calibrate weights of features with different scales, thereby enhancing the discriminative ability of extracted multiscale features. Furthermore, the attentional feature fusion module can dynamically and adaptively fuse the features from multimodality data in a contextual scale-aware way, and this attention-based feature fusion method will further improve the collaborative classification performance of hyperspectral and LiDAR data. Four benchmark multimodality data (i.e., hyperspectral and LiDAR data) sets collected by different sensors and at different acquisition times are employed for joint classification experiments. These comparative classification results and ablation study sufficiently certify the superiority of the proposed model in terms of collaborative classification accuracy when compared with other state-of-the-art methods.","1558-0644","","10.1109/TGRS.2021.3106025","National Natural Science Foundation of China(grant numbers:41801388,42101458); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526908","Attentional feature fusion (AFF);collaborative classification;hierarchical residual network;hyperspectral image (HSI);light detection and ranging (LiDAR);self-calibrated convolution","Feature extraction;Laser radar;Hyperspectral imaging;Data mining;Convolution;Collaboration;Fuses","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);optical radar;sensor fusion","multiscale deep learning network;self-calibrated convolution;light detection;conventional deep learning methods;fusing these features;LiDAR data joint classification;extracted multiscale features;attentional feature fusion module;collaborative classification performance;benchmark multimodality data;joint classification experiments;collaborative classification accuracy","","5","","43","IEEE","1 Sep 2021","","","IEEE","IEEE Journals"
"Comprehensive Sample Augmentation by Fully Considering SSS Imaging Mechanism and Environment for Shipwreck Detection Under Zero Real Samples","C. Huang; J. Zhao; Y. Yu; H. Zhang","Institute of Marine Science and Technology, Wuhan University, Wuhan, China; Institute of Marine Science and Technology, Wuhan University, Wuhan, China; Institute of Marine Science and Technology, Wuhan University, Wuhan, China; Department of Automation, School of Power and Mechanical Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","14","To solve the shortage of training samples when using deep learning to detect shipwrecks, a comprehensive sample augmentation method is proposed. The method fully considers the imaging mechanism and environment of side-scan sonar (SSS), such as acoustic emission and reception, waterbody, target reflection, and seafloor background, and generates diverse and representative shipwreck samples from five aspects of target diversity, target texture, imaging resolution, equipment and environmental noises, and background through a series of novel sample augmentation methods. Under the condition of zero real SSS samples, a detection model of YOLOv5s was established with these amplified samples and achieved a mean average precision (MAP) better than 96% for real SSS data detection.","1558-0644","","10.1109/TGRS.2021.3116671","National Natural Science Foundation of China(grant numbers:42176186); Key Research and Development Program of New Energy Engineering Company, Ltd., of China Communications Construction Company Third Harbor Engineering Company, Ltd.(grant numbers:2019-ZJKJ-ZDZX-01-0349); Class-A Project of New Energy Engineering Company, Ltd., of China Communications Construction Company Third Harbor Engineering Company, Ltd.(grant numbers:2020-04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565214","Imaging environment;imaging mechanism;sample augmentation;shipwreck detection;side-scan sonar (SSS);style transfer","Marine vehicles;Optical imaging;Target recognition;Optical sensors;Sonar;Companies;Shape","feature extraction;geophysical image processing;image classification;image fusion;image representation;image resolution;image texture;learning (artificial intelligence);object detection;oceanographic techniques;sonar;sonar detection;sonar imaging","SSS imaging mechanism;shipwreck detection;deep learning;shipwrecks;comprehensive sample augmentation method;side-scan sonar;target reflection;seafloor background;diverse shipwreck samples;representative shipwreck samples;target diversity;target texture;environmental noises;sample augmentation methods;detection model;amplified samples;SSS data detection","","4","","44","IEEE","8 Oct 2021","","","IEEE","IEEE Journals"
"Structure–Color Preserving Network for Hyperspectral Image Super-Resolution","B. Pan; Q. Qu; X. Xu; Z. Shi","Science and Technology on Special System Simulation Laboratory, Beijing Simulation Center, Beijing, China; School of Statistics and Data Science and the Key Laboratory of Pure Mathematics and Combinatorics of Ministry of Education, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; Image Processing Center, School of Astronautics, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","1 Mar 2022","2022","60","","1","12","Fusion-based hyperspectral super-resolution (HSR) algorithms usually utilize a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (MSI) to generate a high-resolution hyperspectral image (HR-HSI), which have attracted increasing attention in recent years. However, how to deal with the abundant spectral information of hyperspectral images and complex structure characteristics of MSIs has always been the focus and difficulty of fusion-based HSR. In this article, we propose a new structure–color preserving network (SCPNet) for HSR, which is developed under the basis of the joint attention mechanism. The SCPNet mainly includes three modules: structure-preserving module (SPM), color-preserving module (CPM), and cross-fusion module. The SPM is constructed based on the spatial attention, which aims to capture and enhance the significant structure information from the high-resolution MSI. Meanwhile, the CPM is constructed based on the channel attention, where the spectral characteristics in the LR-HSI are preserved during the reconstruction process. Finally, we propose a cross attention-based cross-fusion strategy to integrate the features from the two branches and reconstruct the final HR-HSI. The major contribution of SCPNet is that the structure and color information is described and preserved via the joint attention mechanism. Experimental results indicate that the proposed SCPNet has presented advantages on three benchmark datasets when compared with some state-of-the-art HSR methods.","1558-0644","","10.1109/TGRS.2021.3135028","National Key Research and Development Program of China(grant numbers:2019YFC1510905); National Natural Science Foundation of China(grant numbers:62001251,62001252); China Postdoctoral Science Foundation(grant numbers:2020M670631); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648196","Attention mechanism;hyperspectral super-resolution (HSR);structure–color preserving","Hyperspectral imaging;Spatial resolution;Feature extraction;Image reconstruction;Image color analysis;Tensors;Correlation","geophysical image processing;hyperspectral imaging;image colour analysis;image fusion;image reconstruction;image resolution;sensor fusion","structure-color preserving network;hyperspectral image super-resolution;super-resolution algorithms;low-resolution hyperspectral image;high-resolution multispectral image;high-resolution hyperspectral image;abundant spectral information;hyperspectral images;fusion-based HSR;SCPNet;joint attention mechanism;SPM;color-preserving module;CPM;cross-fusion module;spatial attention;significant structure information;high-resolution MSI;channel attention;spectral characteristics;LR-HSI;cross attention-based cross-fusion strategy;final HR-HSI;color information;state-of-the-art HSR methods","","3","","55","IEEE","13 Dec 2021","","","IEEE","IEEE Journals"
"When Pansharpening Meets Graph Convolution Network and Knowledge Distillation","K. Yan; M. Zhou; L. Liu; C. Xie; D. Hong","Science Island Branch, University of Science and Technology of China, Hefei, China; Science Island Branch, University of Science and Technology of China, Hefei, China; Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China; Institute of Intelligent Machines, Hefei Institutes of Physical Science, Chinese Academy of Sciences, Hefei, China; Key Laboratory of Computational Optical Imaging Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","29 Apr 2022","2022","60","","1","15","In this article, we propose a novel graph convolutional network (GCN) for pansharpening, defined as GCPNet, which consists of three main modules: the spatial GCN module (SGCN), the spectral band GCN module (BGCN), and the atrous spatial pyramid module (ASPM). Specifically, due to the nature of GCN, the proposed SGCN and BGCN are capable of exploring the long-range relationship between the object and the global state in the spatial and spectral aspects, which benefits pansharpened results and has not been fully investigated before. In addition, the designed ASPM is equipped with multiscale atrous convolutions and learns richer local feature information, so as to cover the objects of different sizes in satellite images. To further enhance the representation of our proposed GCPNet, asynchronous knowledge distillation is introduced to provide compact features by heterogeneous task imitation in a teacher–student paradigm. In the paradigm, the teacher network acts as a variational autoencoder to extract compact features of the ground-truth MS images. The student network, devised for pansharpening, is trained with the assistance of the teacher network to transfer the important information of the expected ground-truth MS images. Extensive experimental results on different satellite datasets demonstrate that our proposed network outperforms the state-of-the-art methods both visually and quantitatively. The source code is released at https://github.com/Keyu-Yan/GCPNet.","1558-0644","","10.1109/TGRS.2022.3168192","National Natural Science Foundation of China(grant numbers:32171888); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9758796","Asynchronous knowledge distillation;atrous convolution;graph convolutional network (GCN);pansharpening","Pansharpening;Convolution;Satellites;Feature extraction;Task analysis;Knowledge engineering;Spatial resolution","feature extraction;geophysical image processing;graph theory;image classification;image fusion;image resolution;learning (artificial intelligence);wavelet transforms","satellite images;asynchronous knowledge distillation;compact features;heterogeneous task imitation;teacher-student paradigm;teacher network;student network;expected ground-truth MS images;different satellite datasets;pansharpening meets graph convolution network;novel graph convolutional network;spatial GCN module;SGCN;spectral band GCN module;BGCN;atrous spatial pyramid module;long-range relationship;global state;spatial aspects;spectral aspects;pansharpened results;designed ASPM;multiscale atrous convolutions;richer local feature information","","3","","67","IEEE","18 Apr 2022","","","IEEE","IEEE Journals"
"An Enhanced SiamMask Network for Coastal Ship Tracking","X. Yang; Y. Wang; N. Wang; X. Gao","State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, School of Telecommunications Engineering, Xidian University, Xi’an, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China","IEEE Transactions on Geoscience and Remote Sensing","8 Feb 2022","2022","60","","1","11","Coastal ship tracking is significant for cargo transportation and route determination. However, there are few effective tracking methods for specialized ship tracking. Although Siamese networks have been commonly used for object tracking in the field of deep learning, the results for the ship are not accurate due to the lack of contour and edge information. In addition, scale variation and seawater cause unstable ship movements, which aggravates the reduction in tracking accuracy. Therefore, we propose an enhanced SiamMask network for coastal ship tracking. Compared to the previous Siamese network, our algorithm has the following three advantages. First, we apply the unity of visual object tracking and semisupervised object segmentation to the ship tracking task, which completes target tracking while outputting edge shape information. Second, we propose a refined feature pyramid network that utilizes a feature fusion module and enhanced residual module (ERM) to solve the problems of scale variation in datasets. Third, we propose an attentionwise cross correlation with a multidimension attention module (MDAM) to focus more on ship targets and suppress nontargets through autonomous learning at width, height, and channel levels, which creates a tradeoff between the accuracy and the robustness of tracking algorithms. The experimental results show that our method achieves leading performance in LMD-TShips, outperforming most of the state-of-the-art trackers. Code is available at https://gitee.com/EnhancedSiamShipTracking/code.","1558-0644","","10.1109/TGRS.2021.3122330","National Natural Science Foundation of China(grant numbers:61976166,62036007,62176195,61922066,61876142); Key Research and Development Program of Shaanxi(grant numbers:2021GY-030); Innovation Capacity Support Plan of Shaanxi Province(grant numbers:2020KJXX-027); Fundamental Research Funds for the Central Universities(grant numbers:JB210115); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9584864","Coastal ship tracking;enhanced residual module (ERM);multidimension attention module (MDAM);SiamMask network","Marine vehicles;Target tracking;Task analysis;Correlation;Feature extraction;Sea measurements;Object tracking","edge detection;feature extraction;image fusion;image motion analysis;image representation;image segmentation;learning (artificial intelligence);marine engineering computing;object detection;object tracking;ships;target tracking;video signal processing","enhanced SiamMask network;coastal ship tracking;tracking methods;specialized ship tracking;Siamese networks;tracking accuracy;visual object tracking;ship tracking task;target tracking;refined feature pyramid network;ship targets;unstable ship movements;attentionwise cross correlation","","3","","42","IEEE","22 Oct 2021","","","IEEE","IEEE Journals"
"In Situ Volumetric SAR","C. F. Barnes; S. Prasad","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Geoscience and Remote Sensing","26 Sep 2018","2018","56","10","6082","6100","Volumetric synthetic aperture radar (VolSAR) analysis techniques and image formation algorithms suitable for short ranges and large scenes are presented. From a diffractive wave field inversion perspective, ultrawide beamwidth and near range SAR imaging scenarios can be viewed as a form of in situ SAR. Novel Huygens–Fresnel processing methods are introduced that empower in situ volumetric imaging with 2-D and 3-D aperture synthesis. These methods support coherent fusion across multiple separated frequency bands and also support spatial subaperture fusion of data from sparse sensor swarms. A novel signal analysis tool we call a chirp couplet is developed and shown to be useful in the expression and exploitation of temporal and spatial SAR chirp signals. Chirp couplets provide a physically motivated and unifying tool for both temporal and spatial elements in SAR signal analysis. In the temporal domain, chirp couplets provide a generalized formulation of the coupling of time and frequency that exists, for example, in linear frequency modulated waveforms. In the spatial domain, chirp couplets describe the coupling of sensor position and sensed wavenumber. Formulations of chirp couplets suitable for ray tomographic SAR algorithms (i.e., polar format algorithms) are contrasted with chirp couplets capable of supporting diffractive tomographic SAR algorithms (i.e., Stolt format algorithms). Scenarios in which diffraction limited resolution can be achieved with in situ VolSAR and finite length synthetic apertures are explored. VolSAR imaging methods are shown to escape the approximations that constrain the application and performance of range-Doppler and polar format methods.","1558-0644","","10.1109/TGRS.2018.2831644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368088","3D radar imaging;Huygens–Fresnel theory;radar signal processing;radar theory;synthetic aperture radar (SAR);volumetric radar imaging","Apertures;Chirp;Synthetic aperture radar;Radar imaging;Tomography;Image resolution","image fusion;image sensors;radar imaging;signal synthesis;synthetic aperture radar","multiple separated frequency bands;signal analysis tool;chirp couplet;temporal SAR chirp signals;spatial SAR chirp signals;temporal elements;spatial elements;SAR signal analysis;ray tomographic SAR algorithms;polar format algorithms;diffractive tomographic SAR algorithms;Stolt format algorithms;VolSAR imaging methods;volumetric synthetic aperture radar analysis techniques;image formation algorithms;diffractive wave field inversion perspective;ultrawide beamwidth;Novel Huygens-Fresnel processing methods;in situ volumetric SAR;in situ VolSAR;in situ volumetric imaging;near range SAR imaging scenarios;3D aperture synthesis;2D aperture synthesis;coherent fusion;sparse sensor swarms;temporal SAR chirp signals;generalized formulation;linear frequency modulated waveforms;sensor position coupling;sensed wavenumber;finite length synthetic apertures;polar format methods;range-Doppler format methods","","3","","80","OAPA","28 May 2018","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Using Feature Fusion Hypergraph Convolution Neural Network","Z. Ma; Z. Jiang; H. Zhang","Key Laboratory of Spacecraft Design Optimization and Dynamic Simulation Technologies, Ministry of Education, Beihang University, Beijing, China; Key Laboratory of Spacecraft Design Optimization and Dynamic Simulation Technologies, Ministry of Education, Beihang University, Beijing, China; Key Laboratory of Spacecraft Design Optimization and Dynamic Simulation Technologies, Ministry of Education, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","14 Feb 2022","2022","60","","1","14","Convolution neural networks (CNNs) and graph representation learning are two common methods for hyperspectral image (HSI) classification. Recently, graph convolutional neural networks, a combination of CNN and graph representation learning, have shown great potential in the HSI classification problem. However, the existing graph convolution network (GCN)-based methods have many problems, such as overdependence on the adjacency matrix, usage of a single modal feature, and lower accuracy than the mature CNN method. In this article, we propose a feature fusion hypergraph neural network (F2HNN) for HSI classification. F2HNN first generates hyperedges from features of different modalities to construct a hypergraph representing multimodal features in HSI. Then, the HSI and the extracted hypergraph are input into the hypergraph convolutional neural network for learning. In addition, we propose three feature fusion strategies. The first strategy is the most basic spatial and spectral feature fusion. The second strategy fuses the spectral features extracted by a pretrained multilayer perceptron (MLP) with the spatial features to reduce the redundant information of the original spectral features. The third strategy uses the fusion of CNN features, spectral features, and spatial features to explore the capabilities of F2HNN. Sufficient experiments on four datasets have proved the effectiveness of F2HNN.","1558-0644","","10.1109/TGRS.2021.3123423","National Key Research and Development Program of China(grant numbers:2019YFC1510905); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9590574","Deep learning;feature fusion;graph convolution networks (GCNs);hypergraph learning;hyperspectral image (HSI) classification","Feature extraction;Convolution;Task analysis;Deep learning;Convolutional neural networks;Hyperspectral imaging;Data mining","feature extraction;geophysical image processing;graph theory;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);multilayer perceptrons;neural nets","2 HNN;hyperspectral image classification;feature fusion hypergraph convolution neural network;convolution neural networks;representation learning;convolutional neural networks;HSI classification problem;existing graph convolution network-based methods;single modal feature;mature CNN method;feature fusion hypergraph neural network;multimodal features;extracted hypergraph;hypergraph convolutional neural network;fusion strategies;spectral feature fusion;spatial features;original spectral features;CNN features","","2","","49","IEEE","27 Oct 2021","","","IEEE","IEEE Journals"
"Joint Estimation of Absolute Attitude and Size for Satellite Targets Based on Multi-Feature Fusion of Single ISAR Image","J. Wang; Y. Li; M. Song; M. Xing","Academy of Advanced Interdisciplinary Research, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Beijing Institute of Space Long March Vehicle, Beijing, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","12 Apr 2022","2022","60","","1","20","It is challenging to estimate satellite targets’ absolute attitude and size with limited observational data. This article proposes an innovative way to jointly estimate satellite targets’ absolute attitude and size in the 3-D stable coordinates based on inverse synthetic aperture radar (ISAR) image interpretation with only one image. By taking advantage of the rectangular solar panels commonly equipped on satellites, this article extracts solar panel’s principal components, line features, and phase features of single ISAR imagery with principal component analysis (PCA), radon transform (RT), and minimum-entropy (ME)-based autofocus method, respectively. The projection relationship between these features and the absolute attitude and size of the satellite are established separately. Through multi-features fusion, a joint parameter estimation optimization function is established. This optimization is solved iteratively by the quasi-Newton method. The attitude and size parameters can be estimated simultaneously and rapidly, realizing the satellite state estimation under limited observation data. The excellent performance of the proposed algorithm is verified through different experiments.","1558-0644","","10.1109/TGRS.2022.3159345","National Key Research and Development Program of China(grant numbers:2018YFB2202500); National Natural Science Foundation of China(grant numbers:62171337,62101396); Key Research and Development Program of Shaanxi Province(grant numbers:2017KW-ZD-12); Shaanxi Province Funds for Distinguished Young Youths(grant numbers:S2020-JC-JQ-0056); Fundamental Research Funds for the Central Universities(grant numbers:XJS212205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733900","Attitude estimation;inverse synthetic aperture radar (ISAR) image interpretation;multi-features fusion;satellite targets;size estimation","Satellites;Estimation;Radar imaging;Radar;Spaceborne radar;Space vehicles;Three-dimensional displays","estimation theory;image fusion;minimum entropy methods;Newton method;optimisation;parameter estimation;principal component analysis;radar imaging;Radon transforms;state estimation;synthetic aperture radar","absolute attitude estimation;satellite targets;multifeature fusion;inverse synthetic aperture radar image interpretation;rectangular solar panels;solar panel;single ISAR imagery;Radon transform;joint parameter estimation optimization function;satellite state estimation;RT;ME-based autofocus method;minimum-entropy-based autofocus method;quasiNewton method;PCA;principal component analysis","","1","","38","IEEE","14 Mar 2022","","","IEEE","IEEE Journals"
"A Stronger Baseline for Seismic Facies Classification With Less Data","X. Chen; Q. Zou; X. Xu; N. Wang","Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","16 May 2022","2022","60","","1","10","With the great success of deep learning in computer vision, the application of convolution neural network (CNN) in seismic facies classification is growing rapidly. However, most of the previous works based on pure state-of-the-art CNN architectures still suffer from coarse segmentation results. In this article, we study the challenges of seismic facies classification and propose a stronger baseline. More specifically, we propose a simple yet effective unsupervised approach named spatial pyramid sampling (SPS) to choose representative samples for training to reduce the labeling costs. Next, we propose a multimodal fusion (M2F) module to extract and fuse the edge and frequency information from selected seismic images to build a stable multimodal representation. Finally, we propose a local-to-global (L2G) module, which improves the recognition power by capturing the local relationship between pixels and enhancing the global context representation. Experimental results demonstrate that the proposed method achieves a superior performance with less labeled training data, especially for small categories.","1558-0644","","10.1109/TGRS.2022.3171694","National Natural Science Foundation of China(grant numbers:61906013,62106017); Beijing Natural Science Foundation(grant numbers:M22022,L211015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9766041","Context aggregation;local awareness;multimodal knowledge;seismic facies classification;semantic segmentation","Feature extraction;Training;Semantics;Data mining;Image edge detection;Convolution;Fuses","computer vision;convolutional neural nets;deep learning (artificial intelligence);edge detection;feature extraction;geology;geophysical image processing;image classification;image fusion;image segmentation;unsupervised learning","seismic facies classification;convolution neural network;CNN architectures;multimodal fusion module;seismic images;deep learning;computer vision;unsupervised SPS;spatial pyramid sampling;edge extraction;edge fusion;frequency information;multimodal representation;local-to-global module","","1","","48","IEEE","2 May 2022","","","IEEE","IEEE Journals"
"Fusion Before Imaging Method for Heterogeneous Borehole Radar Subsurface Surveys","S. Yi; H. Yang; N. Li; T. Li; Y. Fan; Q. H. Liu","School of Electronic Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; School of Electronic Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; School of Electronic Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; School of Electronic Science and Engineering, University of Electronic Science and Technology of China (UESTC), Chengdu, China; Department of Electrical and Computer Engineering, Duke University, Durham, NC, USA","IEEE Transactions on Geoscience and Remote Sensing","6 Dec 2021","2022","60","","1","14","In this article, an efficient radar fusion-before-imaging (RFBI) method for heterogeneous borehole radar systems is proposed. Different from conventional fusion-after-imaging processes, the sample sets collected from heterogeneous borehole radar systems (monostatic, bistatic, or multiple-input multiple-output) are first merged into one data set before the imaging process in RFBI, and a single imaging operation is demanded to obtain the target space image with high precision. Specifically, the diversity in heterogeneous borehole radar sample sets is taken into consideration, and the radar sample sets are inserted and fused into a high-dimensional sample set before imaging. The target space spectrum is generated according to the echo space–frequency constraint relationship, and the target space is extracted from one imaging process. The influence of clutters in RFBI results is reduced, and the imaging accuracy is satisfactory. Meanwhile, due to the fusion process ahead, the computational time of RFBI hardly increases with the number of radar sample sets. The synthetic and field experiment results show that RFBI demonstrates comparable accuracy as Kirchhoff migration and higher efficiency in processing large amounts of data sets at the cost of large memory requirement, which is suitable for the joint imaging of heterogeneous radar systems.","1558-0644","","10.1109/TGRS.2021.3054786","National Natural Science Foundation of China(grant numbers:U19A2056,61701088,61701093); Fundamental Research Funds for the Central Universities(grant numbers:ZYGX2020J002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354452","Borehole radar;fusion-before-imaging;heterogeneous radar system;radar imaging","Radar imaging;Radar;Imaging;Transceivers;Spaceborne radar;Transmitters;Receivers","feature extraction;ground penetrating radar;image fusion;image sampling;MIMO radar;radar clutter;radar imaging","heterogeneous borehole radar subsurface surveys;radar fusion-before-imaging method;heterogeneous borehole radar systems;fusion-after-imaging processes;single imaging operation;target space image;heterogeneous borehole radar sample sets;high-dimensional sample set;target space spectrum;fusion process;joint imaging;heterogeneous radar systems;RFBI method;echo space-frequency constraint;Kirchhoff migration","","1","","31","IEEE","15 Feb 2021","","","IEEE","IEEE Journals"
"A Novel Multiband Fusion Method Based on a Modified RELAX Algorithm for High-Resolution and Anti-Non-Gaussian Colored Clutter Microwave Imaging","W. Jiang; J. Liu; J. Yang; X. Zhang; W. Li","National Key Laboratory of Microwave Imaging Technology, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronics, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","2 Feb 2022","2022","60","","1","12","A novel multiband fusion method based on a modified RELAX algorithm (MRA) is proposed. In the MRA, a maximum difference criterion is applied to the singular values of echoes’ Hankel matrix to improve the accuracy of calculating the number of scattering centers. In addition, a transformation process is put forward to merge the frequency-dependent factor (FDF) term, which is in the geometrical theory of diffraction (GTD) model, into the phase term of the echo to simplify the model. Moreover, a matching relationship for the correspondence between the FDF values and the cost function is established to effectively increase the estimated precision of the FDF. Therefore, based on the MRA, exact estimations of the incoherent errors among subband echoes are achieved, and then a fused full-band echo (FBE) with high estimation accuracy is produced. Using all the fused FBEes, high-resolution, anti-non-Gaussian colored clutter microwave imaging is realized. The effectiveness of the proposed method is validated with the simulated and real-data experimental results in a simulated non-Gaussian colored clutter environment.","1558-0644","","10.1109/TGRS.2021.3109724","National Key Research and Development Program of China(grant numbers:2018YFA0701900,2018YFA0701901); National Natural Science Foundation of China(grant numbers:61690191); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9538392","Geometrical theory of diffraction (GTD) model;microwave imaging;modified RELAX algorithm (MRA);multiband fusion;non-Gaussian colored clutter","Clutter;Radar;Frequency estimation;Scattering;Microwave imaging;Bandwidth;Radar imaging","filtering theory;Gaussian noise;geometrical theory of diffraction;Hankel matrices;image fusion;microwave imaging;radar clutter;radar imaging;singular value decomposition","multiband fusion method;modified RELAX algorithm;anti-nonGaussian colored clutter microwave imaging;MRA;maximum difference criterion;singular values;frequency-dependent factor term;geometrical theory of diffraction model;phase term;FDF values;subband echoes;full-band echo;high estimation accuracy;nonGaussian colored clutter environment","","1","","44","IEEE","15 Sep 2021","","","IEEE","IEEE Journals"
"Multi-Scale Progressive Fusion Attention Network Based on Small Sample Training for DAS Noise Suppression","N. Wu; T. Xing; Y. Li","Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China; Department of Information, College of Communication Engineering, Jilin University, Changchun, China","IEEE Transactions on Geoscience and Remote Sensing","15 Mar 2022","2022","60","","1","12","Distributed acoustic sensing (DAS), increasingly mature technology for signal acquisition, has been gradually applied in the field of environmental monitoring and seismic exploration. However, the usually strong noise in DAS data and the huge amplitude contrasts among direct waves, reflected waves, and converted waves significantly complicate subsequent data processing and interpretation, which would further limit the wide application and rapid development of DAS in the seismic exploration field. Taking the high-accuracy processing requirements of DAS data into consideration, we propose a multiscale progressive fusion attention network (MPFAN) with pyramidal structure trained by small samples, which explores the collaborative representation of complementary information between DAS noise and its multiscale versions in a multiscale direction to realize the fine modeling of DAS noise and then achieve the suppression of DAS noise. At each scale, MPFAN uses a recurrent calculation to explore the potential correlation of complex DAS noise and learn the global texture, and reassigns the weights of feature maps on each channel through an attention mechanism to focus on detailed information. At each hierarchy, the information flow converges from the bottom to the top layer of the network to finally achieve an accurate estimation of DAS noise. Experiments show that, after training on a forward DAS dataset, which is composed of synthetic noise-free DAS signals and some DAS noises from both synthetic and field data, MPFAN performs better in high degree than some classical methods—not only more noises are obviously suppressed but also more reflected signals are better preserved.","1558-0644","","10.1109/TGRS.2022.3142805","National Natural Science Foundation of China(grant numbers:41974143,42174153); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680682","Attention mechanism;distributed optical fiber acoustic sensing (DAS);multiscale progressive fusion;noise suppression;seismic exploration","Feature extraction;Noise reduction;Noise measurement;Data mining;Convolution;Signal to noise ratio;Sensors","distributed sensors;feature extraction;filtering theory;geophysical signal processing;geophysical techniques;image fusion;image representation;image texture;learning (artificial intelligence);object detection;pattern recognition;seismology;sensor fusion","multiscale progressive fusion attention network;DAS noise suppression;DAS data;seismic exploration field;complex DAS noise;forward DAS dataset;synthetic noise-free DAS signals","","","","49","IEEE","13 Jan 2022","","","IEEE","IEEE Journals"
"SDCAFNet: A Deep Convolutional Neural Network for Land-Cover Semantic Segmentation With the Fusion of PolSAR and Optical Images","B. Chu; J. Chen; J. Chen; X. Pei; W. Yang; F. Gao; S. Wang","Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China; Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China; Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Oct 2022","2022","15","","8928","8942","Due to the different imaging mechanisms between optical and polarimetric synthetic aperture radar (PolSAR) images, determining how to effectively use such complementary information has become an interesting and challenging problem. Convolutional neural networks (CNNs) and other deep neural networks have achieved good experimental results in remote sensing land-cover semantic segmentation. However, the CNN convolution structure can extract only the features within the receptive field in the spatial dimension without focusing on the relationship between multiple channels; therefore, it is impossible to realize fusion and complementarity between multiple channels. In this article, we propose a novel spatial dense channel attention fusion network (SDCAFNet), which takes optical and PolSAR images as different inputs and completes feature fusion and semantic segmentation within a neural network. First, SDCAFNet uses a two-stream siamese CNN network to realize the preliminary feature coding of optical and PolSAR images. Then, a spatial dense channel attention module (SDCAM) is proposed. The channel activation values obtained at different positions are combined in the spatial dense matrix, which can describe the attention in the feature fusion process. Finally, we introduce the fused features into the symmetric skip-connection decoder composed of multiple symmetric decoder blocks to realize end-to-end land-cover semantic segmentation. Experimental results show that SDCAFNet can effectively learn the correlation between optical and PolSAR channels and has a better segmentation accuracy than other methods.","2151-1535","","10.1109/JSTARS.2022.3213601","S&T Program of Hebei(grant numbers:21340302D); Foundation of the CETC Key Laboratory of Aerospace Information Applications(grant numbers:SCX20629T007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9916086","Channel attention;feature fusion;land-cover semantic segmentation;optical image;polarimetric synthetic aperture radar (PoLSAR)","Optical imaging;Feature extraction;Optical sensors;Adaptive optics;Task analysis;Decoding;Optical reflection","feature extraction;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;object detection;radar imaging;radar polarimetry;synthetic aperture radar","SDCAFNet;deep convolutional neural network;different imaging mechanisms;interesting problem;convolutional neural networks;deep neural networks;good experimental results;remote sensing land-cover semantic segmentation;CNN convolution structure;spatial dimension;multiple channels;novel spatial dense channel attention fusion network;optical images;completes;two-stream siamese CNN network;preliminary feature coding;optical PolSAR images;spatial dense channel attention module;channel activation values;spatial dense matrix;feature fusion process;fused features;multiple symmetric decoder blocks;end-to-end land-cover semantic segmentation;optical PolSAR channels;segmentation accuracy","","","","48","CCBYNCND","11 Oct 2022","","","IEEE","IEEE Journals"
"Change Detection on Multi-Spectral Images Based on Feature-level U-Net","W. Wiratama; J. Lee; D. Sim","Department of Computer Engineering, Kwangwoon University, Seoul, South Korea; Department of Computer Engineering, Kwangwoon University, Seoul, South Korea; Department of Computer Engineering, Kwangwoon University, Seoul, South Korea","IEEE Access","22 Jan 2020","2020","8","","12279","12289","This paper proposes a change detection algorithm on multi-spectral images based on feature-level U-Net. A low-complexity pan-sharpening method is proposed to employ not only panchromatic images, but also multi-spectral images for enhancing the performance of the deep neural network. The high-resolution multi-spectral (HRMS) images are then fed into the proposed feature-level U-Net. The proposed feature-level U-Net consists of two-stages: a feature-level subtracting network and U-Net. The feature-level subtracting network is used to extract dynamic difference images (DI) for the use of low-level and high-level features. By employing this network, the performance of change detection algorithms can be improved with a smaller number of layers for U-Net with a low computational complexity. Furthermore, the proposed algorithm detects small changes by taking benefits of both geometrical and spectral resolution enhancement and adopting an intensity-hue-saturation (IHS) pan-sharpening method. A modified of IHS pan-sharpening algorithm is introduced to solve spectral distortion problem by applying mean filtering in high frequency. We found that the proposed change detection on HRMS images gives a better performance compared to existing change detection algorithms by achieving an average F-1 score of 0.62, a percentage correct classification (PCC) of 98.78%, and a kappa of 61.60 for test datasets.","2169-3536","","10.1109/ACCESS.2020.2964798","MSIT (Ministry of Science and ICT), South Korea, through the ITRC (Information Technology Research Center) support program(grant numbers:IITP-2019-2016-0-00288); Institute for Information and communications Technology Promotion; National Research Foundation of Korea; Ministry of Science, ICT and Future Planning(grant numbers:NRF-2018R1A2B2008238); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952681","Convolutional neural network;deep learning;remote sensing;satellite images;change detection","Feature extraction;Image segmentation;Spatial resolution;Change detection algorithms;Distortion;Detection algorithms","feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;neural nets;remote sensing","dynamic difference images;high-level features;change detection algorithm;geometrical resolution enhancement;spectral resolution enhancement;intensity-hue-saturation pan-sharpening method;IHS pan-sharpening algorithm;spectral distortion problem;HRMS images;existing change detection algorithms;low-complexity pan-sharpening method;panchromatic images;high-resolution multispectral images;U-Net;feature-level subtracting network","","9","","41","CCBY","8 Jan 2020","","","IEEE","IEEE Journals"
"Optimization-Based Hyperspectral Spatiotemporal Super-Resolution","P. -C. Chang; J. -T. Lin; C. -H. Lin; P. -W. Tang; Y. Liu","Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Miin Wu School of Computing, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan; Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan","IEEE Access","12 Apr 2022","2022","10","","37477","37494","Due to hardware limitations and financial considerations, it is challenging to acquire fine spatial and temporal resolution (FSFT) images, which leads to the interest in spatiotemporal fusion problems. Instead of directly obtaining costly FSFT images, an alternative is to fuse fine spatial, coarse temporal resolution (FSCT) images with coarse spatial, fine temporal resolution (CSFT) images. Unlike existing spatiotemporal methods which are only designed for multispectral images, this paper first proposes a new fusion framework for hyperspectral spatiotemporal super-resolution, termed HSTSR. In this paper, we first deal with the coarse temporal resolution issue by adopting the fast iterative shrinkage-thresholding algorithm (FISTA) to estimate the missing images at the intermediate time series. Then, we fuse the hyperspectral image and the multispectral image in each time series via coupled nonnegative matrix factorization (CNMF) to get FSFT hyperspectral images. Importantly, we can automatically estimate the associated spatial blurring and spectral downsampling matrices without prior satellite hardware information. Compared with other extended multispectral spatiotemporal methods, our method not only attains satisfying qualities significantly faster, but also requires much less input data.","2169-3536","","10.1109/ACCESS.2022.3163266","Einstein Program (Young Scholar Fellowship Program) of the Ministry of Science and Technology (MOST), Taiwan(grant numbers:MOST 110-2636-E-006-026); Higher Education Sprout Project of Ministry of Education (MOE) to the Headquarters of University Advancement at National Cheng Kung University (NCKU); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745133","Gradient descent;ADMM;FISTA;convex optimization;hyperspectral;multispectral;spatiotemporal;super-resolution;image fusion","Spatial resolution;Spatiotemporal phenomena;Satellites;Hyperspectral imaging;Superresolution;Sensor fusion;Image sensors","geophysical image processing;hyperspectral imaging;image fusion;image resolution","optimization-based hyperspectral spatiotemporal super-resolution;hardware limitations;spatiotemporal fusion problems;costly FSFT images;fine spatial, coarse temporal resolution images;coarse spatial, fine temporal resolution images;multispectral image;fusion framework;coarse temporal resolution issue;fast iterative shrinkage-thresholding algorithm;missing images;FSFT hyperspectral images;spatial blurring;extended multispectral spatiotemporal methods","","2","","52","CCBY","30 Mar 2022","","","IEEE","IEEE Journals"
"Region-Level SAR Image Segmentation Based on Edge Feature and Label Assistance","R. Shang; M. Liu; L. Jiao; J. Feng; Y. Li; R. Stolkin","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Extreme Robotics Laboratory, University of Birmingham, Birmingham, U.K","IEEE Transactions on Geoscience and Remote Sensing","8 Nov 2022","2022","60","","1","16","This article proposes a novel segmentation algorithm for synthetic aperture radar (SAR) images. The algorithm performs region-level segmentation based on edge feature and label assistance. It demonstrates improved performance in terms of segmentation accuracy while better preserving image edges. First, an edge detection scheme is implemented, which fuses information from two advanced edge detection methods, thereby obtaining a more precise edge strength map (ESM). Second, a Canny algorithm is performed to divide the SAR image into edge regions and homogeneous regions, and different smoothing templates are selected according to pixel positions. Therefore, an anisotropic smoothing on the SAR image can be achieved, aiming at suppressing the noise within targets while also accurately maintaining the target boundaries. Third, the K-means clustering is applied to the smoothed result to generate an initial set of labels. Using ESM and the initial labels as inputs, a watershed transformation and a majority voting strategy are employed to realize an initial segmentation at the region level. Finally, a label-aided region merging (LaRM) strategy is used to correctly segment the wrongly labeled regions to give the final segmentation result. The LaRM, with merging rules based on the label rather than gray characteristics, can avoid the need for calculating a large number of complex formulas, thus accelerating the region merging. Results are presented of experiments on both simulated and real SAR images, in which the proposed region-level SAR image segmentation algorithm based on edge feature and label assistance (REFLA) method is compared against six state-of-the-art algorithms from the literature. REFLA achieves higher accuracy while better retaining the image edges.","1558-0644","","10.1109/TGRS.2022.3217053","National Natural Science Foundation of China(grant numbers:62176200,61773304,61871306); Natural Science Basic Research Program of Shaanxi(grant numbers:2022JC-45,2022JQ-616); Open Research Projects of Zhejiang Laboratory(grant numbers:2021KG0AB03); Higher Education Discipline Innovation Project; National Key Research and Development Program of China; Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110686); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930138","Anisotropic smoothing;edge feature;label-aided region merging (LaRM);synthetic aperture radar (SAR) image;unsupervised segmentation","Image edge detection;Image segmentation;Radar polarimetry;Merging;Smoothing methods;Speckle;Feature extraction","edge detection;feature extraction;image denoising;image fusion;image segmentation;radar imaging;smoothing methods;synthetic aperture radar","edge feature;advanced edge detection methods;precise edge strength map;ESM;Canny algorithm;anisotropic smoothing;region-level SAR image segmentation;label assistance;synthetic aperture radar;image edges;pixel positions;noise supression;watershed transformation;majority voting strategy;label-aided region merging;REFLA method","","1","","40","IEEE","25 Oct 2022","","","IEEE","IEEE Journals"
"Variable Subpixel Convolution Based Arbitrary-Resolution Hyperspectral Pansharpening","L. He; J. Xie; J. Li; A. Plaza; J. Chanussot; J. Zhu","School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and the Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hyperspectral Computing Laboratory, University of Extremadura, Cáceres, Spain; Jean Kuntzmann Laboratory (LJK), National Centre for Scientific Research (CNRS), National Institute for Research in Digital Science and Technology (Inria), Grenoble Institute of Technology (Grenoble INP), Université Grenoble Alpes, Grenoble, France; School of Automation Science and Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","29 Jul 2022","2022","60","","1","19","Standard hyperspectral (HS) pansharpening relies on fusion to enhance low-resolution HS (LRHS) images to the resolution of their matching panchromatic (PAN) images, whose practical implementation is normally under a stipulation of scale invariance of the model across the training phase and the pansharpening phase. By contrast, arbitrary resolution HS (ARHS) pansharpening seeks to pansharpen LRHS images to any user-customized resolutions. For such a new HS pansharpening task, it is not feasible to train and store convolution neural network (CNN) models for all possible candidate scales, which implies that the single model acquired from the training phase should be capable of being generalized to yield HS images with any resolutions in the pansharpening phase. To address the challenge, a novel variable subpixel convolution (VSPC)-based CNN (VSPC-CNN) method following our arbitrary upsampling CNN (AU-CNN) framework is developed for ARHS pansharpening. The VSPC-CNN method comprises a two-stage elevating thread. The first stage is to improve the spatial resolution of the input HS image to that of the PAN image through a prepansharpening module, and then, a VSPC-encapsulated arbitrary scale attention upsampling (ASAU) module is cascaded for arbitrary resolution adjustment. After training with given scales, it can be generalized to pansharpen HS image to arbitrary scales under the spatial patterns invariance across the training and pansharpening phases. Experimental results from several specific VSPC-CNNs on both simulated and real HS datasets show the superiority of the proposed method.","1558-0644","","10.1109/TGRS.2022.3189624","National Natural Science Foundation of China(grant numbers:62071184,61571195,42030111,61836003); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515011615); Guangzhou Science and Technology Program(grant numbers:202002030395); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826457","Arbitrary resolution;convolution neural network (CNN);hyperspectral (HS) pansharpening;spatial pattern;variable subpixel convolution (VSPC)","Pansharpening;Spatial resolution;Training;Standards;Task analysis;Convolution;Optimized production technology","convolutional neural nets;geophysical image processing;hyperspectral imaging;image enhancement;image fusion;image resolution","low-resolution HS;matching panchromatic images;training phase;arbitrary resolution HS pansharpening;user-customized resolutions;convolution neural network models;variable subpixel convolution-based CNN method;arbitrary upsampling CNN framework;ARHS pansharpening;VSPC-CNN method;spatial resolution;HS image;PAN image;VSPC-encapsulated arbitrary scale attention upsampling module;arbitrary resolution adjustment;arbitrary scales;simulated HS datasets;LRHS images;variable subpixel convolution based arbitrary-resolution hyperspectral pansharpening","","1","","53","IEEE","11 Jul 2022","","","IEEE","IEEE Journals"
"MD Loss: Efficient Training of 3-D Seismic Fault Segmentation Network Under Sparse Labels by Weakening Anomaly Annotation","Y. Dou; K. Li; J. Zhu; T. Li; S. Tan; Z. Huang","College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China; Shengli Oilfield Company, SINOPEC, Dongying, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Shengli Oilfield Company, SINOPEC, Dongying, China; College of Computer Science and Technology, China University of Petroleum (East China), Qingdao, China","IEEE Transactions on Geoscience and Remote Sensing","17 Aug 2022","2022","60","","1","14","Data-driven fault detection has been regarded as a 3-D image segmentation task. The models trained from synthetic data are difficult to generalize in some surveys. Recently, training 3-D fault segmentation using sparse manual 2-D slices is thought to yield promising results, but manual labeling has many false negative labels (FNLs) (abnormal annotations), which is detrimental to training and consequently to detection performance. Motivated to train 3-D fault segmentation networks under sparse 2-D labels while suppressing FNLs, we analyze the training process gradient and propose the mask dice (MD) loss. Moreover, the fault is an edge feature, and current encoder–decoder architectures widely used for fault detection (e.g., U-shape network) are not conducive to edge representation. Consequently, fault-net is proposed, which is designed for the characteristics of faults, employs high-resolution propagation features, and embeds multiscale compression fusion block to fuse multiscale information, which allows the edge information to be fully preserved during propagation and fusion, thus enabling advanced performance via few computational resources. The experiment demonstrates that MD loss supports the inclusion of human experience in training and suppresses FNLs therein, enabling baseline models to improve performance and generalize to more surveys. Fault-Net is capable of providing a more stable and reliable interpretation of faults, and it uses extremely low computational resources and inference is significantly faster than other models. Our method indicates optimal performance in comparison with several mainstream methods.","1558-0644","","10.1109/TGRS.2022.3196810","National Natural Science Foundation of China, Major Program(grant numbers:51991365); Natural Science Foundation of Shandong Province of China(grant numbers:ZR2021MF082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851460","3-D image segmentation;interpretation;seismic attributes;seismic fault detection","Three-dimensional displays;Training;Labeling;Image edge detection;Image segmentation;Fault detection;Task analysis","edge detection;expectation-maximisation algorithm;fault diagnosis;feature extraction;image fusion;image segmentation;learning (artificial intelligence)","sparse manual 2D slices;3D seismic fault segmentation network;3D image segmentation task;suppresses FNLs;edge information;embeds multiscale compression fusion block;high-resolution propagation features;current encoder-decoder architectures;edge feature;mask dice loss;training process gradient;suppressing FNLs;abnormal annotations;false negative labels;manual labeling;synthetic data;data-driven fault detection;anomaly annotation;sparse labels;MD loss;fault-net","","1","","40","IEEE","5 Aug 2022","","","IEEE","IEEE Journals"
"Lightweight Multiresolution Feature Fusion Network for Spectral Super-Resolution","S. Mei; G. Zhang; N. Wang; B. Wu; M. Ma; Y. Zhang; Y. Feng","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2023","2023","61","","1","14","Spectral super-resolution (SR), which reconstructs high spatial-resolution hyperspectral images (HSIs) from RGB inputs, has been demonstrated to be one of the effective computational imaging techniques to acquire HSIs. Though deep neural networks have shown their superiority in such a complex mapping problem, existing networks generally involve a very complex structure with huge amounts of parameters, resulting in giant memory occupation. In this article, a lightweight multiresolution feature fusion network (MRFN) is proposed, which adopts a multiresolution feature extraction and fusion framework to fully explore RGB inputs in different scales of resolution. Specifically, a lightweight feature extraction module (LFEM), which adopts cheap convolution and attention mechanisms, is constructed to explore different scales of features under a lightweight structure. Moreover, a hybrid loss function is proposed by encountering not only pixel-value level reconstruction error but also spectral continuity and fidelity. Experiments over three benchmark datasets, i.e., CAVE, Interdisciplinary Computational Vision Laboratory (ICVL), and NTIRE2022 datasets, have demonstrated that the proposed MRFN can reconstruct HSIs from RGB inputs in higher quality with fewer parameters and computational floating-point operations (FLOPs) compared with several state-of-the-art networks.","1558-0644","","10.1109/TGRS.2023.3234124","National Natural Science Foundation of China(grant numbers:62171381); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005275","Convolutional neural network (CNN);hyperspectral image (HSI);lightweight network;spectral super-resolution (SR)","Image reconstruction;Feature extraction;Spatial resolution;Hyperspectral imaging;Convolution;Neural networks;Task analysis","cartography;deep learning (artificial intelligence);feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image motion analysis;image resolution","attention mechanisms;complex mapping problem;computational imaging techniques;deep neural networks;fusion framework;HSI;LFEM;lightweight feature extraction module;lightweight multiresolution feature fusion network;MRFN;multiresolution feature extraction;RGB inputs;spatial-resolution hyperspectral images;spectral continuity;spectral super-resolution","","","","53","IEEE","4 Jan 2023","","","IEEE","IEEE Journals"
"GIFM: An Image Restoration Method With Generalized Image Formation Model for Poor Visible Conditions","Z. Liang; W. Zhang; R. Ruan; P. Zhuang; C. Li","School of Internet, Anhui University, Hefei, China; School of Information Engineering, Henan Institute of Science and Technology, Xinxiang, China; School of Internet, Anhui University, Hefei, China; Key Laboratory of Knowledge Automation for Industrial Processes, Ministry of Education, School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Jurong West, Singapore","IEEE Transactions on Geoscience and Remote Sensing","20 Dec 2022","2022","60","","1","16","Recently, image restoration has attracted considerable attention from researchers, and these methods generally restore degraded images based on the atmospheric scattering model (ATSM) and retinex model (RM). The two models only take into the single attenuation process during imaging, thereby introducing undesirable results. To deal with this issue, we propose an image restoration method based on a generalized image formation model (GIFM). First, unlike the existing image restoration methods, we rebuild a novel image formation model, which describes the light attenuation process that includes the light source-scene path and scene-sensor path. Second, we construct an objective optimization function to decompose a degraded image into a color distorted component and color corrected component, and an augmented Lagrange multiplier-based alternating direction minimization algorithm is provided to solve the optimization problem. Finally, we fully consider the advantages of the small-scale neighborhood and large-scale neighborhood in image restoration, and an image itself brightness-based weighted fusion strategy is proposed to balance brightness enhancement and contrast improvement. Extensive experiments on three image enhancement datasets show that our GIFM achieves better results than state-of-the-art methods. Experiments further suggest that our GIFM performs well for image restoration of extreme scenes, keypoint detection, object detection, and image segmentation.","1558-0644","","10.1109/TGRS.2022.3227548","China Postdoctoral Science Foundation(grant numbers:2019M660438); National Natural Science Foundation of China(grant numbers:62171252,62071272,61701247,62001158,62273001); Postdoctoral Science Foundation of China(grant numbers:2021M701903); National Key Research and Development Program of China(grant numbers:2020AAA0130000); MindSpore, CANN, and Ascend AI Processor; CAAI-Huawei MindSpore Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9975803","Image formation model;image restoration;optimization function;weighted fusion","Image restoration;Atmospheric modeling;Image color analysis;Attenuation;Optimization;Imaging;Brightness","image colour analysis;image enhancement;image fusion;image restoration;image segmentation;minimisation","alternating direction minimization algorithm;atmospheric scattering model;augmented Lagrange multiplier;brightness enhancement;brightness-based weighted fusion;color corrected component;color distorted component;contrast improvement;degraded image;generalized image formation model;GIFM;image restoration;image segmentation;light source-scene path;objective optimization function;retinex model","","","","82","IEEE","8 Dec 2022","","","IEEE","IEEE Journals"
"Fused Adaptive Receptive Field Mechanism and Dynamic Multiscale Dilated Convolution for Side-Scan Sonar Image Segmentation","Z. Wang; S. Zhang; L. Gross; C. Zhang; B. Wang","School of Information Engineering, Xijing University, Xi’an, China; School of Information Engineering, Xijing University, Xi’an, China; School of Earth and Environmental Sciences, The University of Queensland, Brisbane, St Lucia, QLD, Australia; College of Artificial Intelligence, Tianjin University of Science and Technology, Tianjin, China; School of Information and Navigation, Air Force Engineering University, Xian, China","IEEE Transactions on Geoscience and Remote Sensing","12 Sep 2022","2022","60","","1","17","Side-scan sonar (SSS) is a vital sensor for marine survey, which is widely used in military and civilian fields. The accurate segmentation of SSS images is critical in sonar image intelligent interpretation. Existing SSS image segmentation methods have several limitations, such as insufficient feature extraction, relatively worse segmentation results for tiny target categories, and serious interference by seabed reverberation noise and bright shadow region. To overcome these issues, we propose a novel encoder–decoder architecture SSS image segmentation method based on convolution neural network (CNN). First, we extract the multiscale feature information contained in target region using the dynamic multiscale dilated convolution (DMDC_Conv). Second, to further obtain the global and detail feature information, we construct the adaptive receptive field mechanism block (ARFM_Block). Third, we design a feature fusion attention mechanism block (FFAM_Block) to fuse high-level and low-level feature information with different scales and suppress background information interference. Final, we construct a tree structure optimization module (TSOM) to solve the problem of pixel misclassification and obtain refine SSS image segmentation results. Extensive experiments are carried out on the constructed real scene SSS image dataset. The experimental results show that the proposed method achieves 93.24% and 90.82% of mean pixel accuracy (MPA) and mean intersection over union (MIoU), respectively, which outperforms the other state-of-the-art methods and has a substantial advantage in inference speed and calculation parameters.","1558-0644","","10.1109/TGRS.2022.3201248","Neural Science Foundation of Shanxi Province(grant numbers:2021JQ-879); Jiangsu Innovation and Entrepreneurship Program(grant numbers:184080H10827); Shaanxi Key Laboratory of Integrated and Intelligent Navigation open fund(grant numbers:SKLIIN-20180211); Shaanxi Province Key Research and Development Program(grant numbers:2021GY-341,2022GY-118); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9866038","Attention mechanism;convolution neural network (CNN);image segmentation;multiscale feature information;side-scan sonar (SSS)","Image segmentation;Feature extraction;Convolution;Convolutional neural networks;Sonar;Decoding;Interference","convolutional neural nets;feature extraction;image classification;image filtering;image fusion;image segmentation;object detection;optimisation;sonar imaging;trees (mathematics)","seabed reverberation noise;bright shadow region;novel encoder-decoder architecture SSS image segmentation method;convolution neural network;multiscale feature information;target region;dynamic multiscale dilated convolution;global detail feature information;adaptive receptive field mechanism block;ARFM_Block;feature fusion attention mechanism block;FFAM_Block;low-level feature information;suppress background information interference;refine SSS image segmentation results;constructed real scene SSS image dataset;side-scan sonar image segmentation;vital sensor;marine survey;military fields;civilian fields;SSS images;sonar image intelligent interpretation;SSS image segmentation methods;insufficient feature extraction;relatively worse segmentation results;tiny target categories;serious interference","","","","50","IEEE","24 Aug 2022","","","IEEE","IEEE Journals"
"TFTN: A Transformer-Based Fusion Tracking Framework of Hyperspectral and RGB","C. Zhao; H. Liu; N. Su; Y. Yan","Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Marine Communication and Information Technology, Ministry of Industry and Information Technology, Harbin Engineering University, Harbin, China","IEEE Transactions on Geoscience and Remote Sensing","11 Nov 2022","2022","60","","1","15","Although the red, green, and blue (RGB) image has a high spatial resolution, it only depicts color intensities in RGB channels, which easily leads to the failure of the tracker based on RGB modality in some challenging scenarios, for example, when the color of the object and background is similar. The hyperspectral image with rich spectral information is more robust in these difficult situations, so it is essential to explore how to effectively apply hyperspectral features to supplement RGB information in object tracking. However, there is no fusion tracking algorithm based on hyperspectral and RGB data. Based on this, we propose a novel fusion tracking framework of hyperspectral and RGB in this article, termed as transformer-based fusion tracking network (TFTN), to enhance the performance of object tracking. Within the framework, we construct a dual-branch structure based on the Siamese network to obtain the modality-specific representations of different modality images. Besides, the framework is generic, which is suitable for the Siamese series of tracking algorithms. In addition, we design a Siamese 3-D convolutional neural network as the specific branch of hyperspectral modality for synchronous extraction of the spatial and spectral features of hyperspectral data, to give full play to the role of hyperspectral data in improving network tracking performance. Particularly, inspired by the structure of Transformer, we design a transformer-based fusion module to capture the potential interaction of intramodality and intermodality features of different modalities. This is the first work that combines the information of hyperspectral and RGB modalities to improve tracking performance. At the same time, it is also the first time that employs the self-attention module of Transformer to combine the information of different modalities for multimodality fusion tracking. Experimental results on the dataset composed of hyperspectral and RGB image sequences show that the proposed TFTN tracker is superior to the state-of-the-art trackers, demonstrating the effectiveness of this method.","1558-0644","","10.1109/TGRS.2022.3215816","National Natural Science Foundation of China(grant numbers:62071136,62271159,62002083,61971153); Heilongjiang Outstanding Youth Foundation(grant numbers:YQ2022F002); Heilongjiang Postdoctoral Foundation(grant numbers:LBH-Q20085 LBH-Z20051); Fundamental Research Funds for the Central Universities(grant numbers:3072022QBZ0805,3072021CFT0801,3072022CF0808); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9924197","3-D convolutional neural network (CNN);attention;fusion tracking;hyperspectral images (HSIs);transformer","Hyperspectral imaging;Transformers;Object tracking;Visualization;Task analysis;Feature extraction;Three-dimensional displays","feature extraction;image classification;image colour analysis;image fusion;image representation;image sequences;learning (artificial intelligence);neural nets;object detection;object tracking;sensor fusion;target tracking;video signal processing","color intensities;different modality images;dual-branch structure;fusion tracking algorithm;high spatial resolution;hyperspectral data;hyperspectral features;hyperspectral image;hyperspectral modality;hyperspectral RGB data;hyperspectral RGB image sequences;hyperspectral RGB modalities;modality-specific representations;multimodality fusion tracking;network tracking performance;object tracking;RGB channels;RGB information;RGB modality;rich spectral information;Siamese 3-D convolutional neural network;Siamese network;spatial features;spectral features;TFTN tracker;tracking algorithms;transformer-based fusion module;Transformer-based fusion tracking framework;transformer-based fusion tracking network","","","","53","IEEE","19 Oct 2022","","","IEEE","IEEE Journals"
"Spectral Variation Augmented Representation for Hyperspectral Imagery Classification With Few Labeled Samples","B. Xie; Y. Zhang; S. Mei; G. Zhang; Y. Feng; Q. Du","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","21 Nov 2022","2022","60","","1","12","Due to variations in imaging conditions, spectra of the same type of ground objects usually exhibit certain discrepancies, leading to intraclass spectral distance increase and interclass distance decrease. As a result, classification accuracy is greatly affected, especially in cases with few labeled samples. For representation-based classifiers, the spectral variability within limited training samples is far from sufficient to represent diverse variations within testing ones. To handle this problem, a spectral variation augmented representation for hyperspectral imagery classification (SVARC) with few labeled samples is proposed in this article. First, a novel class-independent and -dependent components-based linear representation model (CICD-LRM) is proposed to emphasize the representation of spectral variation. Second, depending on spatial and spectral correlation, the CICD-LRM-guided global and local spectral variation extraction schemes are designed, and a fused spectral variation dictionary is constructed by concatenation. Finally, a classifier for hyperspectral images based on the CICD-LRM and spectral variation dictionary is proposed, and specifically, three different spectral variation reconstruction strategies are designed. Similar to most representation-based classifiers, a residual-driven decision is also employed in the proposed classifier. Comparative experiments are conducted with eight classical and state-of-the-art methods using two benchmark datasets. The experimental results demonstrate that the proposed SVARC method significantly outperforms the compared ones in cases with few labeled samples.","1558-0644","","10.1109/TGRS.2022.3220579","National Natural Science Foundation of China(grant numbers:62171381); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9941154","Augmented representation;hyperspectral classification;limited training samples;spectral variation","Training;Dictionaries;Testing;Iron;Feature extraction;Imaging;Hyperspectral imaging","feature extraction;geophysical image processing;geophysical techniques;image classification;image fusion;image representation;learning (artificial intelligence)","CICD-LRM;classification accuracy;dependent components-based;diverse variations;fused spectral variation dictionary;global spectral variation extraction scheme;hyperspectral imagery classification;hyperspectral images;imaging conditions;interclass distance decrease;labeled samples;local spectral variation extraction scheme;representation model;representation-based classifiers;spatial correlation;spectral correlation;spectral distance increase;spectral variability;spectral variation augmented representation;spectral variation reconstruction strategies;training samples","","","","46","IEEE","7 Nov 2022","","","IEEE","IEEE Journals"
"Global–Local Transformer Network for HSI and LiDAR Data Joint Classification","K. Ding; T. Lu; W. Fu; S. Li; F. Ma","Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","8 Nov 2022","2022","60","","1","13","Hyperspectral images (HSIs) contain rich spatial and spectral detail information, while light detection and ranging (LiDAR) data can provide the elevation information. Thus, the fusion of HSI and LiDAR data can help in more accurate image classification, which becomes a hot research topic. However, it is difficult to capture complex local and global spatial–spectral associations; meanwhile, how to build an effective interaction between multimodal data is another important issue. To this end, a novel global–local transformer network (GLT-Net) is proposed for the joint classification of HSI and LiDAR data, in this article. The main idea is to fully exploit the advantage of the convolution operator in characterizing locally correlated features and the promising capability of transformer architecture in learning long-range dependencies. Moreover, multiscale feature fusion and probabilistic decision fusion strategies are also designed in one framework, to further improve the classification performance. Here, the proposed GLT-Net mainly consists of multiscale local spatial feature learning, global spectral feature learning, and global–local feature fusion classification. In specific, multimodal image cubes of different sizes are first extracted and sent into convolutional neural networks (CNNs) to learn local spatial features, which is followed by multimodal information propagation and spatial-attention-guided multiscale feature fusion. Afterward, by considering spectral feature channels from a sequential perspective, vision transformers are introduced to model the global spectral dependencies. Finally, multiple class estimations based on local and global features are integrated via a probabilistic decision fusion strategy. In this way, complementary information of multimodal data and local/global spectral–spatial information can be fully mined and jointly used. Extensive experiments on three popular HSI and LiDAR datasets demonstrate that the proposed method performs superiority over the state-of-the-art methods. The source code of the proposed method will be made publicly available at https://github.com/Ding-Kexin/GLT-Net.","1558-0644","","10.1109/TGRS.2022.3216319","National Key Research and Development Program of China(grant numbers:2021YFA0715203); National Natural Science Foundation of China(grant numbers:62271196,62001160); Hunan Provincial Natural Science Foundation(grant numbers:2022JJ30164,2022JJ40089); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926173","Convolutional neural networks (CNNs);hyperspectral images (HSIs);image classification;light detection and ranging (LiDAR) data;transformers","Feature extraction;Laser radar;Transformers;Representation learning;Task analysis;Data mining;Hyperspectral imaging","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;object detection;optical radar;probability","hyperspectral images;light detection and ranging;image classification;global-local transformer network;joint classification;transformer architecture;multiscale feature fusion;probabilistic decision fusion strategy;multiscale local spatial feature learning;global spectral feature learning;global-local feature fusion classification;specific image cubes;multimodal image cubes;convolutional neural networks;multimodal information propagation;spectral feature channels;global spectral dependencies;popular HSI;LiDAR datasets;GLT-Net;multiple class estimations","","","","47","IEEE","21 Oct 2022","","","IEEE","IEEE Journals"
"SRCANet: Stacked Residual Coordinate Attention Network for Infrared Ship Detection","P. Wu; H. Huang; H. Qian; S. Su; B. Sun; Z. Zuo","College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China; College of Intelligence Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","18 Nov 2022","2022","60","","1","14","The inability of conventional algorithms to detect infrared (IR) ship targets in complex scenes led to the development of detection methods based on convolutional neural networks (CNNs). In this study, we propose a CNN-based stacked residual coordinate attention network (SRCANet) for detecting IR ship targets. Three-directional stacked interaction modules and a full-scale skip connection feature fusion scheme are introduced. The proposed network maintains and integrates sufficient contextual information of IR ship targets and obtains clear target boundary information. A cascaded residual coordinate attention module (CRCAM) is designed as the basic node in the SRCANet. In addition, a residual coordinate attention module (RCAM) is introduced, which combines a 2-D convolution layer with batch normalization and rectified linear unit (CBR), a coordination attention module, and a residual connection. The RCAM enhances the input feature map and improves the representability of objects of interest. The CRCAM comprises several cascading RCAMs that deepen the feature extraction layers. Furthermore, because there is no publicly available IR ship target dataset for segmentation, pixel-level annotations are performed on a set of IR ship target images and released as a single-frame IR ship detection (SISD) dataset. Extensive experiments were conducted on the SISD dataset and the widely used single-frame IR small target (SIRST) dataset to demonstrate the superiority of the proposed method. The results indicate that the SRCANet outperforms the state-of-the-art models, and it is more robust when target texture information is lacking. The SISD dataset is available at https://github.com/echo-sky/SISD.","1558-0644","","10.1109/TGRS.2022.3218563","National Natural Science Foundation of China(grant numbers:52101377); Natural Science Foundation of Hainan Province(grant numbers:2020JJ5672); Hunan Provincial Innovation Foundation for Postgraduate(grant numbers:CX20210020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933804","Cascaded residual coordinate attention module (CRCAM);infrared (IR) ship target detection;single-frame IR ship detection dataset (SISD);stacked residual coordinate attention network (SRCANET)","Marine vehicles;Feature extraction;Image segmentation;Object detection;Clutter;Semantics;Shape","feature extraction;image fusion;image recognition;image segmentation;image texture;infrared imaging;learning (artificial intelligence);neural nets;object detection;ships","2-D convolution layer;cascaded residual;cascading RCAMs;CNN-based stacked residual;conventional algorithms;convolutional neural networks;coordination attention module;CRCAM;detecting IR ship targets;detection methods;feature extraction layers;full-scale skip connection;infrared ship detection;input feature map;IR ship target images;obtains clear target boundary information;publicly available IR ship target dataset;RCAM;residual connection;single-frame IR ship detection dataset;single-frame IR small target dataset;SISD dataset;SRCANet;stacked residual coordinate attention network;sufficient contextual information;target texture information;three-directional stacked interaction modules","","","","73","IEEE","1 Nov 2022","","","IEEE","IEEE Journals"
"Simultaneous Seismic Data Interpolation and Denoising Based on Nonsubsampled Contourlet Transform Integrating With Two-Step Iterative Log Thresholding Algorithm","C. Li; X. Wen; X. Liu; S. Zu","Key Laboratory of Earth Exploration and Information Technology of Ministry of Education, College of Geophysics, Chengdu University of Technology, Chengdu, China; Key Laboratory of Earth Exploration and Information Technology of Ministry of Education, College of Geophysics, Chengdu University of Technology, Chengdu, China; Key Laboratory of Earth Exploration and Information Technology of Ministry of Education, College of Geophysics, Chengdu University of Technology, Chengdu, China; Key Laboratory of Earth Exploration and Information Technology of Ministry of Education, College of Geophysics, Chengdu University of Technology, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","3 Aug 2022","2022","60","","1","10","Seismic data interpolation and denoising play vital roles in obtaining complete and clean data in seismic data processing. Seismic data usually miss along various spatial axes and always mix with random noise. To obtain complete and clean seismic data, reconstruction technology can interpolate missing data and attenuate random noise. A nonsubsampled contourlet transform (NSCT) is an effective transform to obtain multiscale and multidirection sparse domain data for compression sensing interpolation and denoising. However, conventional iterative shrinkage/thresholding (IT) cannot handle ill-posed and ill-conditioned equations for solving linear inverse problem. We present a two-step iterative log thresholding (TwILT) method to overcome ill-posed and ill-conditioned problems and improve the convergence rate and solution accuracy, which can interpolate and denoise seismic data simultaneously in the NSCT framework. First, we use the NSCT to convert the seismic missing data with random noise to sparse domain. Then, we apply the TwILT algorithm to interpolate and denoise data in sparse domain. The result of each iteration is based on the results of the previous two iterations, which can accelerate convergence rate. In addition, log thresholding can further improve convergence rate and solution accuracy. Finally, we use inverse NSCT to obtain the interpolated and denoised seismic data. The new method can reconstruct the irregularly missing data and attenuate random noise to obtain complete and clean seismic data with high accuracy, which is crucial for seismic imaging and inversion. We demonstrate the applicability and effectiveness of this simultaneous interpolation and denoising technique with successful applications to both synthetic and field data examples.","1558-0644","","10.1109/TGRS.2022.3192531","National Natural Science Foundation of China(grant numbers:41774142); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833541","Log thresholding;multiscale and multidirection;nonsubsampled contourlet transform (NSCT);simultaneous interpolation and denoising;two-step iterative","Transforms;Interpolation;Noise reduction;Filter banks;Thresholding (Imaging);Low-pass filters;Image reconstruction","geophysical signal processing;geophysical techniques;image denoising;image fusion;interpolation;inverse problems;iterative methods;random noise;seismology;signal denoising;signal reconstruction;transforms;wavelet transforms","clean data;seismic data processing;attenuate random noise;multiscale domain data;multidirection sparse domain data;compression sensing interpolation;two-step iterative log thresholding method;convergence rate;interpolated denoised seismic data;irregularly missing data;seismic imaging;simultaneous interpolation;synthetic field data examples;simultaneous seismic data interpolation;nonsubsampled contourlet transform integrating;two-step iterative log thresholding algorithm;denoising play vital roles;complete data","","","","57","IEEE","20 Jul 2022","","","IEEE","IEEE Journals"
"Deep Learning Vertical Resolution Enhancement Considering Features of Seismic Data","Y. Gao; D. Zhao; T. Li; G. Li; S. Guo","State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; Dagang Branch of Research Institute, BGP, Tianjin, China; State Key Laboratory of Petroleum Resources and Prospecting, China University of Petroleum, Beijing, China; Research Institute of Exploration and Development, Dagang Oil Field, Petrochina, Tianjin, China","IEEE Transactions on Geoscience and Remote Sensing","13 Jan 2023","2023","61","","1","13","The resolution of seismic data determines the ability to characterize individual geological structures in a seismic image. Sparse spike inversion (SSI) is an effective approach for improving the resolution of seismic data. However, the basic assumption of SSI is that the strong reflectivity of the formation is sparse, which may not be a reasonable fit for weak thin-layer reflections. In this study, we propose a deep learning-based method to reconstruct high-resolution seismic data by combining information from the longitudinal reflectivity distribution and lateral geological structure features in the field data. A U-shaped network that fuses residual block and attention mechanism is used to learn the relationship between low resolution and high resolution. In addition, we use a hybrid loss function that combines  ${\ell _{1}}$  loss and structural similarity (SSIM) loss to optimize the network parameters for better distinguishing the geometrical features characterized by structural amplitude changes. To train the network, we adopt a workflow to automatically generate numerous 2-D low-resolution data and their corresponding high-resolution data. In this workflow, the prior information, such as statistical reflectivity distribution of well logs and structural features of the data are considered. The synthetic data and field data tests show that our method can work well compared to the traditional method even though only a few well logs are available. Especially in the field data example, our method recovers thin layers better and yields laterally more consistent high-resolution results.","1558-0644","","10.1109/TGRS.2023.3234617","Fundamental Research Project of CNPC Geophysical Key Laboratory(grant numbers:2022DQ0604-4); National Natural Science Foundation of China(grant numbers:42074141); Strategic Cooperation Technology Projects of China National Petroleum Corporation and China University of Petroleum-Beijing(grant numbers:ZLZX2020-03); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007857","Data processing;deep learning;seismic resolution enhancement;spatial continuity","Feature extraction;Neural networks;Training;Training data;Deep learning;Deconvolution;Data mining","deep learning (artificial intelligence);feature extraction;geophysical image processing;geophysical techniques;image fusion;image resolution;seismology","2D low-resolution data;deep learning vertical resolution enhancement;geometrical features;high-resolution data;high-resolution seismic data reconstruction;hybrid loss function;individual geological structures;lateral geological structure features;longitudinal reflectivity distribution;residual block-attention mechanism fusion;seismic data features;seismic image;sparse spike inversion;SSI;SSIM;SSIM loss;statistical reflectivity distribution;structural amplitude changes;structural features;structural similarity loss;thin-layer reflections;U-shaped network","","","","61","IEEE","5 Jan 2023","","","IEEE","IEEE Journals"
"Sal²RN: A Spatial–Spectral Salient Reinforcement Network for Hyperspectral and LiDAR Data Fusion Classification","J. Li; Y. Liu; R. Song; Y. Li; K. Han; Q. Du","CAS Key Laboratory of Spectral Imaging Technology, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Key Laboratory of intelligent Infrared perception, Shanghai Institute of Technical Physics, Chinese Academy of Sciences, Shanghai, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Transactions on Geoscience and Remote Sensing","5 Jan 2023","2023","61","","1","14","Hyperspectral image (HSI) and light detection and ranging (LiDAR) data fusion have been widely employed in HSI classification to promote interpreting performance. In the existing deep learning methods based on spatial–spectral features, the features extracted from different layers are treated fairly in the learning process. In reality, features extracted from the continuous layers contribute differentially to the final classification, such as large tracts of woodland and agriculture typically count on shallow contour features, whereas deep semantic spectral features have meaningful constraints for small entities like vehicles. Furthermore, the majority of existing classification algorithms employ a patch input scheme, which has a high probability to introduce pixels of different categories at the boundary. To acquire more accurate classification results, we propose a spatial–spectral saliency reinforcement network (Sal2RN) in this article. In spatial dimension, a novel cross-layer interaction module (CIM) is presented to adaptively alter the significance of features between various layers and integrate these diversified features. Moreover, a customized center spectrum correction module (CSCM) integrates neighborhood information and adaptively modifies the center spectrum to reduce intraclass variance and further improve the classification accuracy of the network. Finally, a statistically based feature weighted combination module is constructed to effectively fuse spatial, spectral, and LiDAR features. Compared with traditional and advanced classification methods, the Sal2RN achieves the state-of-the-art classification performance on three open benchmark datasets.","1558-0644","","10.1109/TGRS.2022.3231930","National Key Research and Development Program of China(grant numbers:2018AAA0102702); National Nature Science Foundation of China(grant numbers:61901343,61571345,61671383,91538101,61501346,61502367); State Key Laboratory of Geo-Information Engineering(grant numbers:SKLGIE2020-M-3-1); Science and Technology on Space Intelligent Control Laboratory(grant numbers:ZDSYS-2019-03); China Postdoctoral Science Foundation(grant numbers:2017M623124); China Postdoctoral Science Foundation(grant numbers:2018T111019); Open Research Fund of CAS Key Laboratory of Spectral Imaging Technology(grant numbers:LSIT201924W); Government-Business-University-Research Cooperation Foundation between Wuhu and Xidian(grant numbers:XWYCXY-012021002-HT); Higher Education Discipline Innovation Project(grant numbers:B08038); Innovation Fund of Xidian University(grant numbers:10221150004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998520","Fusion classification;hyperspectral image (HSI);light detection and ranging (LiDAR);spatial–spectral feature","Feature extraction;Hyperspectral imaging;Transformers;Fuses;Laser radar;Convolutional neural networks;Data integration","deep learning (artificial intelligence);feature extraction;hyperspectral imaging;image classification;image fusion;optical radar;radar computing;reinforcement learning","CIM;classification algorithms;cross-layer interaction module;CSCM;customized center spectrum correction module;deep learning methods;deep semantic spectral features;feature weighted combination module;HSI classification;hyperspectral image;LiDAR data fusion classification;light detection and ranging;patch input scheme;Sal2RN;shallow contour features;spatial dimension;spatial-spectral features;spatial-spectral saliency reinforcement network;spatial-spectral salient reinforcement network","","","","55","IEEE","23 Dec 2022","","","IEEE","IEEE Journals"
"SCL-Net: An End-to-End Supervised Contrastive Learning Network for Hyperspectral Image Classification","T. Lu; Y. Hu; W. Fu; K. Ding; B. Bai; L. Fang","Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China; Key Laboratory of Visual Perception and Artificial Intelligence of Hunan Province, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","7 Dec 2022","2022","60","","1","12","In recent years, deep learning (DL) presents a promising performance in hyperspectral image (HSI) classification, due to the powerful capability of automatically learning deep semantic characteristics of images. However, it is still difficult to learn highly discriminative features when limited samples are available for training a deep network. Focused on this issue, a novel end-to-end supervised contrastive learning network (SCL-Net) for spectral–spatial classification is proposed, in this article. Instead of learning features of the individual sample, the supervised contrastive learning is introduced to capture the similarity and dissimilarity distribution properties of sample pairs in a feature representation space. In this way, the need for plenty of training samples will be alleviated while an effective network training mechanism is provided for learning highly separative features. Here, SCL-Net mainly consists of one pairwise contrastive learning (PCL) subnetwork and one multilevel spectral–spatial information fusion (MLSIF) subnetwork. For the PCL subnetwork, spectral vectors are projected into deep spectral features based on convolutional operators, which are then followed by distance evaluation between “positive” pairs of similar samples and “negative” pairs of dissimilar ones. Then, a spectral distance matrix is constructed to push the network to gradually learn better features of higher intraclass compactness and interclass dispersion. For the MLSIF subnetwork, a hybrid feature-decision fusion strategy is designed, where spatial and spectral features are jointly exploited to further boost the classification performance. In specific, feature fusion is conducted by connecting low/mid/high-level spectral and spatial features via weighting, while multiple class estimations based on multilevel fusion features are adaptively integrated via probabilistic decision fusion. Overall, these two subnetworks are collaboratively trained in one framework, by optimizing a defined joint loss function consisting of a contrastive loss and a cross-entropy loss. Compared with several state-of-the-art methods, the proposed method yields a superior classification performance in terms of both objective metrics and visual performance.","1558-0644","","10.1109/TGRS.2022.3223664","National Natural Science Foundation of China(grant numbers:62271196,62001160); Hunan Provincial Natural Science Foundation(grant numbers:2022JJ30164,2022JJ40089,2020GK2038); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9963913","Contrastive learning;convolutional neural networks (CNNs);hyperspectral image (HSI);spectral–spatial classification","Feature extraction;Hyperspectral imaging;Training;Deep learning;Task analysis;Data mining;Convolutional neural networks","entropy;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence)","deep learning;deep network;deep spectral features;dissimilarity distribution properties;DL;effective network training mechanism;end-to-end supervised contrastive learning network;feature fusion;feature representation space;highly discriminative features;highly separative features;hybrid feature-decision fusion strategy;hyperspectral image classification;MLSIF subnetwork;multilevel fusion features;pairwise contrastive learning subnetwork;PCL subnetwork;SCL-Net;spatial features;spectral distance matrix;spectral vectors;spectral-spatial classification;spectral-spatial information fusion subnetwork;superior classification performance;training samples","","","","46","IEEE","24 Nov 2022","","","IEEE","IEEE Journals"
"Deep Shearlet Network for Change Detection in SAR Images","H. Dong; L. Jiao; W. Ma; F. Liu; X. Liu; L. Li; S. Yang","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China","IEEE Transactions on Geoscience and Remote Sensing","28 Dec 2022","2022","60","","1","15","Convolutional neural networks (CNNs) can extract shift-invariant features and have been widely applied in the change detection task. However, common CNN lacks noise robustness and needs supervised data to alleviate these problems; in this article, we propose a novel deep shearlet convolutional neural network (ShearNet) for change detection in synthetic aperture radar (SAR) images. In the network, a shearlet denoising layer (SDL) is designed to enhance the representation ability of common CNN. In SDL, feature maps are decomposed into subband coefficients by shearlet transform (ST). Due to the optimal sparse representation property and high direction sensitivity of ST, the network can capture important geometric information. Then, hard-threshold shrinkage is applied to high-frequency subbands to drop small coefficients that are most likely to be noise so that reducing the effect of noise. Finally, ShearNet is trained by introducing a noise-robust loss with noisy labels. The noisy labels are obtained by deep clustering that shows more robustness than existing preclassification methods. This fine-tuning process novelly follows the paradigm of learning from noisy labels to aside the difficulty of precisely labeling samples. Our experimental results on multiple real SAR datasets show that ShearNet can boost accuracy and have better applicability for change detection in SAR images. The source code is available at https://github.com/yizhilanmaodhh/ShearNet.","1558-0644","","10.1109/TGRS.2022.3228776","Key Scientific Technological Innovation Research Project by Ministry of Education; National Natural Science Foundation of China Innovation Research Group Fund(grant numbers:61621005); State Key Program and the Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61836009); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103,91838303); National Natural Science Foundation of China(grant numbers:U1701267,62076192,62006177,61902298,61573267,61906150,62276199); Higher Education Discipline Innovation Project; Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT 15R53); ST Innovation Project from the Chinese Ministry of Education; Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-06); National Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ-659,2022JQ-607); China Postdoctoral Fund(grant numbers:2022T150506); Scientific Research Project of Education Department in Shaanxi Province of China(grant numbers:20JY023); Fundamental Research Funds for the Central Universities(grant numbers:XJS201901,XJS201903,JBF201905,JB211908); CAAI-Huawei MindSpore Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982693","Convolutional neural networks (CNNs);deep clustering;noise-robustness loss;shearlet transform (ST);synthetic aperture radar (SAR) image change detection","Feature extraction;Convolutional neural networks;Radar polarimetry;Task analysis;Synthetic aperture radar;Noise measurement;Speckle","feature extraction;image classification;image denoising;image enhancement;image fusion;image representation;learning (artificial intelligence);neural nets;radar imaging;synthetic aperture radar;transforms;wavelet transforms","change detection task;common CNN;convolutional neural networks;deep clustering;deep shearlet convolutional neural network;deep shearlet network;feature maps;high direction sensitivity;high-frequency subbands;noise robustness;noise-robust loss;noisy labels;optimal sparse representation property;representation ability;SAR images;SDL;ShearNet;shift-invariant features;ST;subband coefficients;supervised data;synthetic aperture radar images","","","","62","IEEE","12 Dec 2022","","","IEEE","IEEE Journals"
"Continuous Human Activity Recognition With Distributed Radar Sensor Networks and CNN–RNN Architectures","S. Zhu; R. G. Guendel; A. Yarovoy; F. Fioranelli","Department of Microelectronics, Delft University of Technology, Delft, The Netherlands; Department of Microelectronics, Delft University of Technology, Delft, The Netherlands; Department of Microelectronics, Delft University of Technology, Delft, The Netherlands; Department of Microelectronics, Delft University of Technology, Delft, The Netherlands","IEEE Transactions on Geoscience and Remote Sensing","15 Jul 2022","2022","60","","1","15","Unconstrained human activities recognition with a radar network is considered. A hybrid classifier combining both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for spatial–temporal pattern extraction is proposed. The 2-D CNNs (2D-CNNs) are first applied to the radar data to perform spatial feature extraction on the input spectrograms. Subsequently, gated recurrent units with bidirectional implementations are used to capture the long- and short-term temporal dependencies in the feature maps generated by the 2D-CNNs. Three NN-based data fusion methods were explored and compared with utilize the rich information provided by the different radar nodes. The performance of the proposed classifier was validated rigorously using the K-fold cross-validation (CV) and leave-one-person-out (L1PO) methods. Unlike competitive research, the dataset with continuous human activities with seamless interactivity transitions that can occur at any time and unconstrained moving trajectories of the participants has been collected and used for evaluation purposes. Classification accuracy of about 90.8% is achieved for nine-class human activity recognition (HAR) by the proposed classifier with the halfway fusion method.","1558-0644","","10.1109/TGRS.2022.3189746","Dutch Research Council NWO under project RAD-ART, Radar-aware Activity Recognition with Innovative Temporal Networks; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825631","Deep learning (DL);distributed radar;human activity recognition (HAR);micro-Doppler signatures;radar sensor network (RSN)","Radar;Spectrogram;Radar tracking;Feature extraction;Ultra wideband radar;Particle measurements;Atmospheric measurements","convolutional neural nets;feature extraction;image classification;image fusion;image motion analysis;radar computing;radar imaging;recurrent neural nets","continuous human activity recognition;distributed radar sensor networks;CNN-RNN architectures;unconstrained human activities recognition;radar network;hybrid classifier;convolutional neural networks;recurrent neural networks;spatial-temporal pattern extraction;2D-CNN;radar data;spatial feature extraction;input spectrograms;gated recurrent units;bidirectional implementations;long- term temporal dependencies;short-term temporal dependencies;feature maps;data fusion methods;radar nodes;K-fold cross-validation method;continuous human activities;unconstrained moving trajectories;nine-class human activity recognition;halfway fusion method;leave-one-person-out method","","","","83","IEEE","11 Jul 2022","","","IEEE","IEEE Journals"
"Improving the Spatial Resolution of FY-3 Microwave Radiation Imager via Fusion With FY-3/MERSI","H. Song; G. Wang; A. Cao; Q. Liu; B. Huang","Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Collaborative Innovation Center on Forecast and Evaluation of Meteorological Disasters, Nanjing University of Information Science and Technology, Nanjing, China; Shanghai Institute of Satellite Engineering, Shanghai, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center on Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Chinese University of Hong Kong, Hong Kong","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Aug 2017","2017","10","7","3055","3063","This paper proposes an effective approach to improve the spatial resolution of FengYun-3 (FY-3) microwave radiation imager (MWRI) data via fusing with FY-3 medium-resolution imager (MERSI) data. Located onboard the same satellite FY-3, the complementary properties of MWRI and MERSI, such as cloud penetrating ability and high spatial resolution, are explored to extend the applications of MWRI data to mesoscale or microscale. To this end, we make efforts to improve the spatial resolution of MWRI data by combining the spatial information and spectral information from MERSI and MWRI, respectively. The proposed fusion procedure includes two stages. In the first stage, the MERSI and MWRI images are jointly spectrally unmixed via learning a spectral dictionary pair, and then the spatial information from MERSI is transferred into MWRI by sparse coding, resulting in a spatial resolution enhanced MWRI image. In the second stage, the spectral information of the spatial resolution enhanced MWRI image is enhanced through guided filtering. To form a unified fusion framework for both the cloud-free and cloud-contaminated cases in MERSI images, we propose to leverage a learning-based single image super-resolution method in the cloud-contaminated case, which learns a spatial dictionary pair from the MWRI image pairs of low and high spatial resolutions obtained in the cloud-free case. To the best of our knowledge, the proposed method is the first fusion based one for spatial resolution enhancement of microwave radiometer images. Finally, to assess the performance of the proposed fusion framework, we choose three MERSI-MWRI image pairs to evaluate in both the cloud-free and cloud-contaminated cases. Both visual comparisons and quantitative evaluations validate the effectiveness of the proposed fusion method in preserving the spatial information of MERSI and the spectral information of MWRI. Comparisons with a state-of-the-art method that does not resort to optical data, demonstrate the superiority of the pro-posed method with better overall measurements for experimental results.","2151-1535","","10.1109/JSTARS.2017.2665524","Natural Science Foundation of China(grant numbers:41501377,41561124014,41375099,41371417); Foundation of Jiangsu Province of China(grant numbers:Grant BK20150906,Grant 15KJB170012,Grant 15KJA520001); BDAT Nanjing University of Information Science and Technology(grant numbers:KXK1406); HKRGC General Research Fund(grant numbers:14606315); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872436","FY-3 microwave radiation imager (MWRI);FY-3/medium-resolution imager (MERSI);guided filtering;spatial resolution (SR);spectral unmixing","Microwave radiometry;Spatial resolution;Microwave imaging;Microwave theory and techniques;Clouds;Optical imaging;Adaptive optics","atmospheric techniques;clouds;geophysical image processing;image fusion;image resolution;weather forecasting","optical data;visual comparisons;learning-based single image super-resolution method;cloud-free evaluation;MERSI-MWRI image fusion;microwave radiometer image;spatial resolution enhancement;cloud-free case;cloud-contaminated case;spectral information;spatial information;cloud-penetrating ability;MWRI data;FY-3 microwave radiation imager","","11","","32","IEEE","6 Mar 2017","","","IEEE","IEEE Journals"
"Multiview Imaging for Low-Signature Target Detection in Rough-Surface Clutter Environment","D. Comite; F. Ahmad; D. Liao; T. Dogaru; M. G. Amin","Radar Imaging Laboratory, Center for Advanced Communications, Villanova University, Villanova, PA, USA; Electrical and Computer Engineering Department, Temple University, Philadelphia, PA, USA; U.S. Army Research Laboratory, Adelphi, MD, USA; U.S. Army Research Laboratory, Adelphi, MD, USA; Radar Imaging Laboratory, Center for Advanced Communications, Villanova University, Villanova, PA, USA","IEEE Transactions on Geoscience and Remote Sensing","25 Aug 2017","2017","55","9","5220","5229","Forward-looking ground-penetrating radar (FL-GPR) permits standoff sensing of shallow in-road threats. A major challenge facing this radar technology is the high rate of false alarms stemming from the vulnerability of the target responses to interference scattering arising from interface roughness and subsurface clutter. In this paper, we present a multiview approach for target detection in FL-GPR. Various images corresponding to the different views are generated using a tomographic algorithm, which considers the near-field nature of the sensing problem. Furthermore, for reducing clutter and maintaining high cross-range resolution over the imaged area, each image is computed in a segmentwise fashion using coherent integration over a suitable set of measurements from multiple platform positions. We employ two fusion approaches based on likelihood ratio tests detector to combine the multiview images for enhanced target detection. The superior performance of the multiview approach over single-view imaging is demonstrated using electromagnetic modeling data.","1558-0644","","10.1109/TGRS.2017.2703820","U.S. ARO and ARL(grant numbers:W911NF-11-1-0536); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942118","Forward-looking ground-penetrating radar (FL-GPR);image fusion;microwave tomography;multiview imaging;target detection;terrain clutter","Radar imaging;Imaging;Clutter;Landmine detection;Sensors;Image segmentation","ground penetrating radar;object detection;radar detection;radar imaging","multiview imaging;low-signature target detection;rough-surface clutter environment;forward-looking ground-penetrating radar;FL-GPR;tomographic algorithm;coherent integration;likelihood ratio tests detector;electromagnetic modeling data","","29","","29","IEEE","7 Jun 2017","","","IEEE","IEEE Journals"
"Unsupervised Cycle-Consistent Generative Adversarial Networks for Pan Sharpening","H. Zhou; Q. Liu; D. Weng; Y. Wang","State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China; School of Biomedical Engineering, Capital Medical University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","26 Apr 2022","2022","60","","1","14","Deep learning-based pan sharpening has received significant research interest in recent years. Most of the existing methods fall into the supervised learning framework in which they downsample the multispectral (MS) and panchromatic (PAN) images and regard the original MS images as ground truths to form training samples based on Wald’s protocol. Although impressive performance could be achieved, they have difficulties when generalizing to the original full-scale images due to the scale gap, which makes them lack of practicability. In this article, we propose an unsupervised generative adversarial framework that learns from the full-scale images without the ground truths to alleviate this problem. We first extract the modality-specific features from the PAN and MS images with a two-stream generator, perform fusion in the feature domain, and then reconstruct the pan-sharpened images. Furthermore, we introduce a novel hybrid loss based on the cycle-consistency and adversarial scheme to improve the performance. Comparison experiments with the state-of-the-art methods are conducted on GaoFen-2 (GF-2) and WorldView-3 satellites. Results demonstrate that the proposed method can greatly improve the pan-sharpening performance on the full-scale images, which clearly shows its practical value. Codes are available at https://github.com/zhysora/UCGAN.","1558-0644","","10.1109/TGRS.2022.3166528","NSFC(grant numbers:62176017,41871283,U1804157); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755137","Cycle consistency;generative adversarial network (GAN);image fusion;pan sharpening;unsupervised learning","Feature extraction;Deep learning;Training;Task analysis;Generators;Generative adversarial networks;Spatial resolution","","","","3","","53","IEEE","11 Apr 2022","","","IEEE","IEEE Journals"
"Full Scale Regression-Based Injection Coefficients for Panchromatic Sharpening","G. Vivone; R. Restaino; J. Chanussot","Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Department of Information Engineering, Electrical Engineering and Applied Mathematics, University of Salerno, Fisciano, Italy; Université Grenoble Alpes, CNRS, Grenoble INP, GIPSA-lab, Grenoble, France","IEEE Transactions on Image Processing","12 Apr 2018","2018","27","7","3418","3431","Pansharpening is usually related to the fusion of a high spatial resolution but low spectral resolution (panchromatic) image with a high spectral resolution but low spatial resolution (multispectral) image. The calculation of injection coefficients through regression is a very popular and powerful approach. These coefficients are usually estimated at reduced resolution. In this paper, the estimation of the injection coefficients at full resolution for regression-based pansharpening approaches is proposed. To this aim, an iterative algorithm is proposed and studied. Its convergence, whatever the initial guess, is demonstrated in all the practical cases and the reached asymptotic value is analytically calculated. The performance is assessed both at reduced resolution and at full resolution on four data sets acquired by the IKONOS sensor and the WorldView-3 sensor. The proposed full scale approach always shows the best performance with respect to the benchmark consisting of state-of-the-art pansharpening methods.","1941-0042","","10.1109/TIP.2018.2819501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325487","Iterative methods;full scale estimation;pansharpening;data fusion;remote sensing","Spatial resolution;Estimation;Multiresolution analysis;Iterative methods;Satellites;Closed-form solutions","image fusion;image registration;image resolution;iterative methods;regression analysis;remote sensing","panchromatic sharpening;low spectral resolution image;high spectral resolution;low spatial resolution image;regression-based pansharpening approaches;full-scale regression;panchromatic image;WorldView-3 sensor;IKONOS sensor;iterative algorithm","","97","","62","IEEE","26 Mar 2018","","","IEEE","IEEE Journals"
"Real-Time One-Stream Semantic-Guided Refinement Network for RGB-Thermal Salient Object Detection","F. Huo; X. Zhu; Q. Zhang; Z. Liu; W. Yu","State Key Laboratory of Power Transmission Equipment and System Security, Chongqing University, Chongqing, China; State Key Laboratory of Power Transmission Equipment and System Security, Chongqing University, Chongqing, China; State Key Laboratory of Power Transmission Equipment and System Security, Chongqing University, Chongqing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; State Key Laboratory of Power Transmission Equipment and System Security, Chongqing University, Chongqing, China","IEEE Transactions on Instrumentation and Measurement","8 Jul 2022","2022","71","","1","12","Salient object detection (SOD) has been widely used in practical applications, such as multisensor image fusion, remote sensing, and defect detection. Recently, SOD from RGB and thermal (T) has been rapidly developed due to its robustness to extreme situations, such as low illumination and occlusion. However, existing methods all utilize a dual-stream encoder, which significantly increases the computation burdens and hinders real-world deployment. To this end, we propose a real-time One-stream Semantic-guided Refinement Network (OSRNet) for RGB-T SOD. Specifically, we first fuse the RGB and T via concatenation, addition, and multiplication operations to dig the complementary information between each modality. The efficient early fusion not only facilitates the information exchange between each modality but also avoids the cumbersome dual-stream encoder structure. Then, the lightweight decoder is proposed, making the high-level semantic information filter the low-level noisy features and gradually refine the final prediction. Also, we apply deep supervision to make the training procedure more stable and fast. Due to the early fusion strategy, OSRNet can run at a real-time speed (53–60 fps) on a single GPU. Extensive quantitative and qualitative experiments show that our network outperforms 11 state-of-the-art methods in terms of seven evaluation metrics. Our codes have been released at https://github.com/huofushuo/OSRNet.","1557-9662","","10.1109/TIM.2022.3185323","Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJZD-K202100102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803225","Multisensor fusion;RGB-thermal;salient object detection (SOD)","Feature extraction;Semantics;Lighting;Decoding;Fuses;Visualization;Real-time systems","decoding;graphics processing units;image colour analysis;image fusion;information filtering;object detection;sensor fusion","RGB-thermal salient object detection;SOD;multisensor image fusion;remote sensing;defect detection;low illumination;computation burdens;hinders real-world deployment;OSRNet;complementary information;dual-stream encoder structure;high-level semantic information;low-level noisy features;early fusion strategy;real-time speed;real-time one-stream semantic-guided refinement network;cumbersome dual-stream encoder structure;RGB-T SOD;single GPU","","7","","66","IEEE","22 Jun 2022","","","IEEE","IEEE Journals"
"High-Performance SAR Automatic Target Recognition Under Limited Data Condition Based on a Deep Feature Fusion Network","Q. Yu; H. Hu; X. Geng; Y. Jiang; J. An","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; College of Ocean and Earth, Xiamen University, Xiamen, China; Electronic Information School, Wuhan University, Wuhan, China; Chinese Antarctic Center of Surveying and Mapping, Wuhan University, Wuhan, China","IEEE Access","19 Nov 2019","2019","7","","165646","165658","Deep learning algorithms have been widely applied to the recognition of remote-sensing images due to their excellent performance on various recognition problems with sufficient data. However, limited data on synthetic aperture radar (SAR) images degrade the performance of neural networks for SAR automatic target recognition (ATR). To address this problem, this paper presents a new deep feature fusion framework by combining the Gabor features and information of raw SAR images and fusing the feature vectors extracted from different layers in our proposed neural network. Gabor features improve the richness of SAR image features. The number of free parameters of neural networks is largely reduced by utilizing large-scale convolutional kernel factorization and global average pooling. Moreover, the fusion of feature vectors from different layers helps improve the recognition performance of neural networks. Experimental results on the MSTAR dataset demonstrate the effectiveness of our proposed method. The proposed neural network can achieve an average accuracy of 99% on the classification of ten-class targets and even achieve a high recognition accuracy on limited data and noisy data.","2169-3536","","10.1109/ACCESS.2019.2952928","National Defense Innovation Science Foundation; National Natural Science Foundation of China(grant numbers:61174196); Shanghai Academy of Spaceflight Technology(grant numbers:SAST2017-080); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8895967","Automatic target recognition;deep convolutional networks;synthetic aperture radar;deep feature fusion;Gabor features","Feature extraction;Radar polarimetry;Neural networks;Synthetic aperture radar;Target recognition;Silicon;Computational modeling","convolutional neural nets;feature extraction;image classification;image fusion;learning (artificial intelligence);radar computing;radar imaging;radar target recognition;remote sensing by radar;synthetic aperture radar","SAR automatic target recognition;deep feature fusion network;deep learning algorithms;remote-sensing images;synthetic aperture radar images;neural network;deep feature fusion framework;Gabor features;feature vectors;SAR image features;noisy data;ATR;convolutional kernel factorization;global average pooling;MSTAR dataset","","19","","52","CCBY","11 Nov 2019","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and Multispectral Images With Sparse and Proximal Regularization","F. Yang; Z. Ping; F. Ma; Y. Wang","School of Electrical and Control Engineering, Liaoning Technical University, Huludao, China; School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China; School of Electronic and Information Engineering, Liaoning Technical University, Huludao, China; School of Electronic and Information Engineering, Liaoning Technical University, Huludao, China","IEEE Access","31 Dec 2019","2019","7","","186352","186363","Fusion of hyperspectral and multispectral imagery data is utilized to reconstruct a super-resolution image with high spectral and spatial resolution, which plays a significant role in remote sensing image processing. Conversely, hyperspectral and multispectral data can be modeled as two low-dimensional subspaces by respectively spatially and spectrally degrading the desired image. A representative method is called coupled non-negative matrix factorization (CNMF) based on a Gaussian observation model, but it is an ill-posed inverse problem. In addition, from the perspective of matrix factorization, the matrixing process of hyperspectral and multispectral cube data generally results in the loss of structural information and performance degradation. To address these issues, this article proposes a proximal minimum-volume expression to regularize the convex simplex, enclosing all reconstructed image pixels instead of low-dimensional subspace data. Then, we incorporate sparse and proximal regularizers into the original CNMF to reformulate the fusion problem as a dynamical system via proximal alternating optimization. Finally, the alternating direction method of multipliers is adopted to split the variables for the closed-form solutions that are further reduced in computational complexity. The experimental results show that the proposed algorithm in this paper performs better than the state-of-the-art fusion methods in most cases, which verifies the effectiveness and efficiency of this proposed algorithm in yielding high-fidelity reconstructed images.","2169-3536","","10.1109/ACCESS.2019.2961240","Scientific Research Project of Colleges from Liaoning Department of Education (P.R.C)(grant numbers:LJ2019QL006,LJ2017QL014,LJ2019JL022); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937525","Proximal regularization;coupled non-negative matrix factorization;data fusion;minimum volume;alternating optimization","Hyperspectral imaging;Tensors;Matrix decomposition;Image reconstruction;Spatial resolution;Data integration","computational complexity;geophysical image processing;image classification;image fusion;image reconstruction;image resolution;matrix decomposition;optimisation;remote sensing","multispectral images;proximal regularization;super-resolution image;high spectral resolution;spatial resolution;remote sensing image processing;hyperspectral data;multispectral data;low-dimensional subspaces;desired image;representative method;nonnegative matrix factorization;Gaussian observation model;inverse problem;matrixing process;hyperspectral cube data;multispectral cube data;structural information;performance degradation;minimum-volume expression;reconstructed image pixels;low-dimensional subspace data;regularizers;CNMF;fusion problem;proximal alternating optimization;high-fidelity reconstructed images","","3","","52","CCBY","20 Dec 2019","","","IEEE","IEEE Journals"
"Change Detection in Multitemporal Monitoring Images Under Low Illumination","Y. Zhu; Z. Jia; J. Yang; N. K. Kasabov","Key Laboratory of Signal Detection and Processing, Xinjiang Uygur Autonomous Region, Xinjiang University, Ürümqi, China; Key Laboratory of Signal Detection and Processing, Xinjiang Uygur Autonomous Region, Xinjiang University, Ürümqi, China; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Knowledge Engineering and Discovery Research Institute, Auckland University of Technology, Auckland, New Zealand","IEEE Access","20 Jul 2020","2020","8","","126700","126712","Video surveillance may involve the simultaneous monitoring of a large number of areas. Real-time automatic change detection of a monitoring area (such as involving the movement of people or vehicles) can reduce risks incurred in negligent manual observation. However, the low signal-to-noise ratio (SNR) of dark environments can significantly corrupt camera images, making it difficult for machine learning surveillance systems to detect small changes in monitored images. In addition, in the absence of changes between two multitemporal monitoring images, sensor noise can lead to false alarms. The objective of this paper is to reduce the effect of sensor noise on change detection of monitored images and the run time of change detection algorithms. For these purposes, we proposed a novel multitemporal monitoring image change detection algorithm based on morphological structure filtering and normalized fusion difference image. First, the random noise in two surveillance images was removed using a multidirectional weighted multiscale series of a morphological filter. Next, two difference images were obtained by using the compression log-ratio operator and the mean ratio operator, and a fusion difference image was obtained by equal-weight fusion of the two difference images. Then, the sigmoid function was used to compress the fusion difference map to obtain a normalized fusion difference image, and a median filter was used to obtain a final difference image. Finally, the k-means clustering algorithm was utilized to obtain the change detection results. The experimental results demonstrate that the proposed method can accurately detect changes in a night monitoring scene in real time. Subjective and objective evaluation of the experimental results demonstrate that the proposed method is superior to reference algorithms in terms of change detection accuracy, time and robustness.","2169-3536","","10.1109/ACCESS.2020.3008262","National Science Foundation of China(grant numbers:U1803261); International Science and Technology Cooperation Project of the Ministry of Education of China(grant numbers:DICE 2016–2196); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137696","Change detection;morphological structure filtering;normalized fusion difference map;low illumination monitoring image","Monitoring;Detection algorithms;Lighting;Remote sensing;Cameras;Morphology;Change detection algorithms","feature extraction;image colour analysis;image enhancement;image fusion;image motion analysis;image segmentation;learning (artificial intelligence);median filters;object detection;pattern clustering;video signal processing;video surveillance","k-means clustering algorithm;multitemporal monitoring image change detection algorithm;low signal-to-noise ratio;monitoring area;real-time automatic change detection;simultaneous monitoring;video surveillance;change detection accuracy;night monitoring scene;change detection results;final difference image;fusion difference map;compression log-ratio operator;difference images;surveillance images;random noise;normalized fusion difference image;change detection algorithms;sensor noise;multitemporal monitoring images;monitored images;machine learning surveillance systems;camera images","","2","","39","CCBY","9 Jul 2020","","","IEEE","IEEE Journals"
"Multispectral Transmission Map Fusion Method and Architecture for Image Dehazing","R. Kumar; B. K. Kaushik; R. Balasubramanian","Department of Electronics and Communication Engineering, IIT Roorkee, Roorkee, India; Department of Electronics and Communication Engineering, IIT Roorkee, Roorkee, India; Department of Computer Science and Engineering, IIT Roorkee, Roorkee, Uttarakhand, India","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","23 Oct 2019","2019","27","11","2693","2697","Image dehazing is an essential preprocessing stage for several applications such as surveillance, long-range imaging, automatic driver-assistance system (ADAS), and remote sensing. Haze particles degrade visible-band images more severely than the infrared images due to scattering phenomena. Therefore, infrared data can be effectively utilized to enhance the visibility of color images captured in a hazy environment. In this brief, for the first time, a multispectral transmission map fusion approach is presented, wherein a haze-aware weight generation scheme is used to improve transmission map in specific regions. This achieves significant quantitative improvement as compared with the existing methods. In addition, the method is extremely suitable for hardware implementation. Therefore, a low-cost very large-scale integration (VLSI) architecture is also presented. The application-specified integrated circuit (ASIC) and field-programmable gate array (FPGA) implementations dissipate 3.45- and 57-mW power at 150 MHz, respectively. Both ASIC and FPGA achieve a high throughput of 450 and 308 Mpixels/s, respectively. The design does not require any external memory such as dynamic random access memory, thus making it suitable for on-chip processing that can be closely integrated with an image sensor.","1557-9999","","10.1109/TVLSI.2019.2932033","Defence Research and Development Organisation(grant numbers:12901T51617R067G4341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802277","Hardware implementation;image defogging;image dehazing;RGB–near-infrared (NIR) fusion;transmission map fusion","Hafnium;Image color analysis;Hardware;Very large scale integration;Color;Image edge detection;Estimation","application specific integrated circuits;field programmable gate arrays;image fusion;radiofrequency integrated circuits;VLSI","dynamic random access memory;FPGA;field-programmable gate array;ASIC;VLSI architecture;ADAS;long-range image dehazing architecture;automatic driver-assistance system;multispectral transmission map fusion method;image sensor;application-specified integrated circuit;large-scale integration architecture;haze-aware weight generation scheme;multispectral transmission map fusion approach;image color analysis;scattering phenomena;infrared imaging;remote sensing;frequency 150.0 MHz;power 3.45 mW;power 57 mW","","7","","17","IEEE","15 Aug 2019","","","IEEE","IEEE Journals"
"MVF-CNN: Fusion of Multilevel Features for Large-Scale Point Cloud Classification","Y. Li; G. Tong; X. Li; L. Zhang; H. Peng","College of Information Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; College of Information Science and Engineering, Northeastern University, Shenyang, China","IEEE Access","15 Apr 2019","2019","7","","46522","46537","Due to the disorder, sparsity, and irregularity of point clouds, the accurate classification of large-scale point clouds is a challenging problem. Voxelization-based deep learning methods have been applied to point cloud classification and have achieved good performances. However, there are some problems in the methods, such as voxels lacking color information, single receptive fields being considered only at the same voxel scale, and only the global features of voxels being considered in the classification. This paper proposes a deep learning-based algorithm for large-scale point cloud classification through the fusion of multiscale voxels and features (MVF-CNN). First, the point cloud is transformed into voxels of two different sizes. Then, the voxels are input into the 3D convolutional neural network (3D CNN) with three different scale receptive fields for the feature extraction. Next, the output features of the 3D CNN are input into the proposed global and local feature fusion classification network (GLNet), which fuses the global features of the voxels at the main branch and the local features of each voxel at the auxiliary branch. Finally, the multiscale features of the main branch are fused, and its classification results are obtained. We have conducted the experiments on six-point cloud scenes. The experimental results show that the proposed algorithm accurately classifies large-scale point clouds. Compared with several semisupervised/supervised learning methods, the proposed algorithm obtains better classification results. In addition, the experimental results also demonstrate that the proposed algorithm has a strong generalization ability and obviously has a better classification performance than the compared algorithms.","2169-3536","","10.1109/ACCESS.2019.2908983","National Natural Science Foundation of China(grant numbers:41371324,61175031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8679988","Point cloud;semantic segmentation;multiscale voxels;3D convolutional neural network (3D CNN)","Three-dimensional displays;Feature extraction;Semantics;Image segmentation;Image color analysis;Deep learning;Classification algorithms","convolutional neural nets;feature extraction;image classification;image colour analysis;image fusion;learning (artificial intelligence)","MVF-CNN;multilevel features;large-scale point cloud classification;voxelization-based deep learning methods;deep learning-based algorithm;multiscale voxels;feature extraction;3D CNN;feature fusion classification network","","12","","38","OAPA","2 Apr 2019","","","IEEE","IEEE Journals"
"Multisensor Time Synchronization Error Modeling and Compensation Method for Distributed POS","J. Li; L. Jia; G. Liu","Science and Technology on Inertial Laboratory and the Key Laboratory of Fundamental Science for National Defense-Novel Inertial Instrument and Navigation System Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China; Science and Technology on Inertial Laboratory and the Key Laboratory of Fundamental Science for National Defense-Novel Inertial Instrument and Navigation System Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China; Science and Technology on Inertial Laboratory and the Key Laboratory of Fundamental Science for National Defense-Novel Inertial Instrument and Navigation System Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China","IEEE Transactions on Instrumentation and Measurement","7 Oct 2016","2016","65","11","2637","2645","An airborne distributed position and orientation system (POS) is high-precision measurement equipment that can accurately provide multinode time-spatial reference for novel remote sensing system as multitask imaging sensors and array antenna synthetic aperture radar. However, it is difficult for multisensor to precisely acquire information at the same moment and result in data fusion error. Thus, the measurement precision is severely degraded. To solve the problem, a multisensor time synchronization error modeling and compensation method is proposed. Based on the component and operation principles of distributed POS, the time synchronization mechanism is analyzed. Multisensor time synchronization error models that include time delay error, random error, and time-varying error are established. A time synchronization error compensation method of the distributed POS is proposed. The experiment results show that the proposed method can accurately calibrate and compensate for the time synchronization error, and improve the measurement precision of the distributed POS. It verified the validity of the proposed method.","1557-9662","","10.1109/TIM.2016.2598020","National Natural Science Foundation of China(grant numbers:61571030); National High Technology Research and Development Program of China (863 Program)(grant numbers:2015AA124001); Beijing Youth Elite Project; CALT Innovation Foundation(grant numbers:CALT201505); Basic Scientific Research(grant numbers:YWF-15-YQGD-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7547391","Distributed position and orientation system (POS);modeling and compensation;multisensor;time synchronization error","Synchronization;Clocks;Delay effects;Sensors;Global Positioning System;Imaging;Antenna arrays","compensation;distributed sensors;image fusion;image sensors;position measurement;radar antennas;remote sensing;sensor arrays;synchronisation;synthetic aperture radar;time measurement","multisensor time synchronization error modeling;compensation method;distributed POS;airborne distributed position and orientation system;high-precision measurement equipment;multinode time-spatial reference;remote sensing system;multitask imaging sensor;array antenna synthetic aperture radar;data fusion error;time delay error;random error;time-varying error;calibration","","14","","21","IEEE","18 Aug 2016","","","IEEE","IEEE Journals"
"Point Cloud Saliency Detection by Local and Global Feature Fusion","X. Ding; W. Lin; Z. Chen; X. Zhang","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","22 Aug 2019","2019","28","11","5379","5393","Inspired by the characteristics of the human visual system, a novel method is proposed for detecting the visually salient regions on 3D point clouds. First, the local distinctness of each point is evaluated based on the difference with its local surroundings. Then, the point cloud is decomposed into small clusters, and the initial global rarity value of each cluster is calculated; a random walk ranking method is then used to introduce cluster-level global rarity refinement to each point in all the clusters. Finally, an optimization framework is proposed to integrate both the local distinctness and the global rarity values to obtain the final saliency detection result of the point cloud. We compare the proposed method with several relevant algorithms and apply it to some computer graphics applications, such as interest point detection, viewpoint selection, and mesh simplification. The experimental results demonstrate the superior performance of the proposed method.","1941-0042","","10.1109/TIP.2019.2918735","Singapore Ministry of Education Tier-2 Fund(grant numbers:MOE2016-T2-2-057(S)); National Natural Science Foundation of China(grant numbers:61771348); China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726371","3D point cloud;visual perception;saliency","Three-dimensional displays;Saliency detection;Two dimensional displays;Visualization;Videos;Feature extraction;Visual systems","feature extraction;image fusion;pattern clustering;random processes;solid modelling","point cloud saliency detection;random walk ranking method;cluster-level global rarity refinement;feature fusion;visually salient region detection;3D point clouds;3D models","Algorithms;Databases, Factual;Humans;Imaging, Three-Dimensional;Visual Perception","16","","83","IEEE","30 May 2019","","","IEEE","IEEE Journals"
"Infrared and Visible Image Registration Based on Scale-Invariant PIIFD Feature and Locality Preserving Matching","Q. Du; A. Fan; Y. Ma; F. Fan; J. Huang; X. Mei","Air Force Early Warning Academy, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Access","20 Nov 2018","2018","6","","64107","64121","Registration of multi-sensor data is a prerequisite for multimodal image analysis such as image fusion. This paper focuses on the problem of infrared and visible image registration, which has played an important role for the purpose of enhancing visual perception. Existing methods based on multimodal feature descriptor such as partial intensity invariant feature descriptor (PIIFD) usually fail in correctly aligning infrared and visible image pairs, due to their significant differences in resolution and appearance. In this paper, we propose a scale-invariant PIIFD (SI-PIIFD) feature and a robust feature matching method to address this problem. Specifically, we first extract corner points as control point candidates since they are usually sufficient and uniformly distributed across the image domain. Then, the SI-PIIFDs are calculated for all corner points and matched according to the descriptor similarity together with a locality preserving geometric constraint. Subsequently, we model the spatial transformation between an infrared and visible image pair with an affine function and introduce a robust Bayesian framework to estimate it from the SI-PIIFD feature matches even if they contaminated by false matches. Finally, the backward approach is chosen for image transformation to avoid holes and overlaps in the output image. Extensive experiments on a challenging dataset with comparisons to other state-of-the-arts demonstrate the effectiveness of the proposed method, both in terms of accuracy and efficiency.","2169-3536","","10.1109/ACCESS.2018.2877642","National Natural Science Foundation of China(grant numbers:61605146); Fundamental Research Funds for the Central Universities of China(grant numbers:2042018kf0018,2042017kf0009); Beijing Advanced Innovation Center for Intelligent Robots and Systems(grant numbers:2016IRS15); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502747","Infrared;image registration;feature matching;multimodal descriptor;robust estimation","Feature extraction;Image registration;Estimation;Fans;Image resolution;Image analysis;Image fusion","affine transforms;Bayes methods;computational geometry;feature extraction;image fusion;image matching;image registration;image resolution;infrared imaging","scale-invariant PIIFD feature;multimodal image analysis;infrared image registration;visible image registration;multimodal feature descriptor;partial intensity invariant feature descriptor;image transformation;multisensor data registration;locality preserving matching;feature matching;Bayesian framework;affine function","","10","","55","OAPA","23 Oct 2018","","","IEEE","IEEE Journals"
"Fusion Similarity-Based Reranking for SAR Image Retrieval","X. Tang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","2","242","246","A new reranking method, fusion similarity-based reranking, is proposed in this letter to improve the performance of synthetic aperture radar (SAR) image retrieval. First, the top ranked SAR images within the initial retrieval results are picked for reranking. Considering the negative influence of the speckle noise, three SAR-oriented visual features are selected to represent them. In addition, the different relevance scores corresponding to an SAR image are estimated in various modalities (i.e., different feature spaces). Second, a fusion similarity is defined under the relevance score space to measure the resemblance between two SAR images. This fusion similarity is calculated using the modal-image matrix, which is construed by the estimated scores to integrate the contributions of all modalities. Finally, an existing reranking function is adopted to rerank the SAR images with the help of the estimated scores and calculated fusion similarities. The positive experimental results demonstrate that our reranking method is effective and efficient.","1558-0571","","10.1109/LGRS.2016.2636819","National Basic Research Program (973 Program) of China(grant numbers:2013CB329402); National Natural Science Foundation of China(grant numbers:61573267,61473215,61571342,61572383,61501353,61502369,61271302,61272282,61202176); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103); Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT 15R53); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801895","Fusion similarity;reranking;synthetic aperture radar (SAR) image retrieval","Synthetic aperture radar;Feature extraction;Image retrieval;Visualization;Speckle;Estimation;Semantics","image fusion;image retrieval;matrix algebra;radar imaging;synthetic aperture radar","modal-image matrix;relevance score space;SAR-oriented visual features;speckle noise;SAR image retrieval;synthetic aperture radar image retrieval;fusion similarity-based reranking;reranking method","","31","","18","IEEE","29 Dec 2016","","","IEEE","IEEE Journals"
"Hand-Held 3-D Reconstruction of Large-Scale Scene With Kinect Sensors Based on Surfel and Video Sequences","H. Xu; L. Yu; S. Fei","School of Mechanical and Electric Engineering, Soochow University, Suzhou, China; School of Mechanical and Electric Engineering, Soochow University, Suzhou, China; School of Automation, Southeast University, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","10 Dec 2018","2018","15","12","1842","1846","This letter presents a hand-held complex large-scale scene reconstruction method with Kinect sensors based on surfel and video sequences. The feature point method simultaneous localization and mapping (SLAM) is employed to estimate the pose of the camera, and then bundle adjustment by combining 2-D and 3-D feature points is used to optimize camera pose. Also, the surfel model is employed to construct deformation maps for the fusion and optimization of point clouds, and finally, an accurate precise 3-D map can be obtained. The main contribution of this letter is that: 1) by using the SLAM method to obtain camera pose as the initial value of optimization, the problem of insufficient memory and low efficiency of the structure form motion method can be well solved; 2) sparsely textured regions can be reconstructed better by using bundle adjustment by combining 2-D and 3-D feature points; and 3) dense 3-D reconstruction of large scenes can be achieved, and the reconstructed 3-D models are more elaborate. Finally, experimental results show that this proposed method can be applied to a variety of complex large-scale scenes, and can obtain accurate precise 3-D model. This presented 3-D reconstruction method can be widely used in the fields of human-computer interaction, consumer electronics, and virtual reality.","1558-0571","","10.1109/LGRS.2018.2866280","National Natural Science Foundation of China(grant numbers:61873176); Anhui University(grant numbers:KFKT201806); Key Technology Program of Suzhou, China(grant numbers:SYG201639); Natural Science Fund for Colleges and Universities in Jiangsu Province(grant numbers:16KJB120005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8458208","3-D reconstruction;bundle adjustment;Kinect sensors;surfel","Cameras;Solid modeling;Bundle adjustment;Mathematical model;Simultaneous localization and mapping;Optimization","cameras;image fusion;image motion analysis;image reconstruction;image sequences;image texture;pose estimation;robot vision;SLAM (robots);stereo image processing;video signal processing","video sequences;Kinect sensors;3-D feature points;SLAM method;structure form motion method;3-D reconstruction method;hand-held 3-D reconstruction;surfel sequences;scene reconstruction method;feature point method simultaneous localization and mapping","","19","","17","IEEE","11 Sep 2018","","","IEEE","IEEE Journals"
"Unsupervised Domain Adaptation for Micro-Doppler Human Motion Classification via Feature Fusion","Y. Lang; Q. Wang; Y. Yang; C. Hou; D. Huang; W. Xiang","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; College of Science and Engineering, James Cook University, Cairns, QLD, Australia","IEEE Geoscience and Remote Sensing Letters","26 Feb 2019","2019","16","3","392","396","Micro-Doppler-based human motion classification has become a topical area of research recently. However, the current research is limited by the lack of labeled training data. Domain adaptation, namely, the ability to take advantage of knowledge from an available source data set and apply it to an unlabeled target data set, is useful in this situation. A typical strategy for this transfer learning technique is to extract domain-invariant feature representations. In this letter, an unsupervised domain adaptation method for micro-Doppler classification is proposed. Given no available measurement training samples, we creatively utilize the motion capture database as an auxiliary and adapt its interior knowledge to the measurement data set. To achieve domain-invariant features, three types of features are extracted and fused including low-level deep features from the convolutional neural network, empirical features, and statistical features. After feature fusion, a k-nearest neighbor classifier is applied to the measurement data to classify seven human activities. Experimental results show that our approach outperforms several state-of-the-art unsupervised domain adaptation methods. The impact of the output from different convolution layers is further investigated, and ablation studies of the efficacy of each feature are also carried out in this letter.","1558-0571","","10.1109/LGRS.2018.2873776","National Natural Science Foundation of China(grant numbers:61520106002,61731003,61871282); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8497048","Domain adaptation;human motion classification;micro-Doppler","Feature extraction;Spectrogram;Radar;Data models;Motion measurement;Convolution;Doppler effect","convolutional neural nets;Doppler radar;feature extraction;image classification;image fusion;image motion analysis;learning (artificial intelligence);pattern classification;radar imaging","feature fusion;labeled training data;transfer learning technique;domain-invariant feature representations;unsupervised domain adaptation method;motion capture database;measurement data;domain-invariant features;empirical features;statistical features;unsupervised domain adaptation methods;low-level deep features;microdoppler-based human motion classification;convolutional neural network;feature extraction;k-nearest neighbor classifier","","18","","21","IEEE","18 Oct 2018","","","IEEE","IEEE Journals"
"Squeeze-and-Excitation Laplacian Pyramid Network With Dual-Polarization Feature Fusion for Ship Classification in SAR Images","T. Zhang; X. Zhang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","7 Jan 2022","2022","19","","1","5","This letter proposes a squeeze-and-excitation Laplacian pyramid network with dual-polarization feature fusion (SE-LPN-DPFF) for ship classification in synthetic aperture radar (SAR) images. SE-LPN-DPFF offers three contributions: 1) dual-polarization (VV and VH) feature fusion (DPFF); 2) channel modeling by the squeeze-and-excitation (SE) to balance each polarization feature’s contribution; and 3) Laplacian pyramid network (LPN) to achieve multiresolution analysis (MRA). Extensive ablation studies can confirm the effectiveness of each contribution. Results on the three- and six-category OpenSARShip datasets reveal the state-of-the-art SAR ship classification performance.","1558-0571","","10.1109/LGRS.2021.3119875","National Natural Science Foundation of China(grant numbers:61571099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568903","Dual-polarization feature fusion (DPFF);Laplacian pyramid network (LPN);ship classification;squeeze-and-excitation (SE);synthetic aperture radar (SAR)","Marine vehicles;Synthetic aperture radar;Training;Feature extraction;Laplace equations;Radar polarimetry;Tensors","electromagnetic wave polarisation;image classification;image fusion;image resolution;radar imaging;radar resolution;ships;synthetic aperture radar","dual-polarization feature fusion;synthetic aperture radar images;SAR ship classification performance;SE-LPN-DPFF;squeeze-and-excitation Laplacian pyramid network;SAR images;channel modeling;multiresolution analysis;MRA","","16","","43","IEEE","13 Oct 2021","","","IEEE","IEEE Journals"
"Sentinel-2 and Sentinel-3 Intersensor Vegetation Estimation via Constrained Topic Modeling","R. Fernandez-Beltran; F. Pla; A. Plaza","Institute of New Imaging Technologies, University Jaume I, Castelló de la Plana, Spain; Institute of New Imaging Technologies, University Jaume I, Castelló de la Plana, Spain; Hyperspectral Computing Laboratory, University of Extremadura, Cáceres, Spain","IEEE Geoscience and Remote Sensing Letters","25 Sep 2019","2019","16","10","1531","1535","This letter presents a novel intersensor vegetation estimation framework, which aims at combining Sentinel-2 (S2) spatial resolution with Sentinel-3 (S3) spectral characteristics in order to generate fused vegetation maps. On the one hand, the multispectral instrument (MSI), carried by S2, provides high spatial resolution images. On the other hand, the Ocean and Land Color Instrument (OLCI), one of the instruments of S3, captures the Earth's surface at a substantially coarser spatial resolution but using smaller spectral bandwidths, which makes the OLCI data more convenient to highlight specific spectral features and motivates the development of synergetic fusion products. In this scenario, the approach presented here takes advantage of the proposed constrained probabilistic latent semantic analysis (CpLSA) model to produce intersensor vegetation estimations, which aim at synergically exploiting MSI's spatial resolution and OLCI's spectral characteristics. Initially, CpLSA is used to uncover the MSI reflectance patterns, which are able to represent the OLCI-derived vegetation. Then, the original MSI data are projected onto this higher abstraction-level representation space in order to generate a high-resolution version of the vegetation captured in the OLCI domain. Our experimental comparison, conducted using four data sets, three different regression algorithms, and two vegetation indices, reveals that the proposed framework is able to provide a competitive advantage in terms of quantitative and qualitative vegetation estimation results.","1558-0571","","10.1109/LGRS.2019.2903231","Generalitat Valenciana(grant numbers:APOSTD/2017/007); Ministerio de Economía y Competitividad(grant numbers:ESP2016-79503-C2-2-P,TIN2015-63646-C5-5-R); Junta de Extremadura(grant numbers:GR18060); European Union under the H2020 EOXPOSURE project(grant numbers:734541); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674589","Constrained probabilistic latent semantic analysis (CpLSA);Sentinel-2;Sentinel-3;topic models;vegetation estimation","Vegetation mapping;Spatial resolution;Estimation;Analytical models;Instruments;Probabilistic logic;Semantics","feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;probability;regression analysis;vegetation;vegetation mapping","multispectral instrument;high spatial resolution images;MSI reflectance patterns;OLCI-derived vegetation;Sentinel-3 intersensor vegetation estimation;constrained topic modeling;fused vegetation maps;Sentinel-2 intersensor vegetation estimation;ocean and land color instrument;spectral features;constrained probabilistic latent semantic analysis;regression algorithms","","15","","21","IEEE","26 Mar 2019","","","IEEE","IEEE Journals"
"Fusion of Multifeature Low-Rank Representation for Synthetic Aperture Radar Target Configuration Recognition","X. Zhang; Y. Wang; D. Li; Z. Tan; S. Liu","College of Communication Engineering, Chongqing University, Chongqing, China; College of Communication Engineering, Chongqing University, Chongqing, China; Guangxi Key Laboratory of Wireless Wideband Communication and Signal Processing, Guilin, China; College of Communication Engineering, Chongqing University, Chongqing, China; College of Communication Engineering, Chongqing University, Chongqing, China","IEEE Geoscience and Remote Sensing Letters","26 Aug 2018","2018","15","9","1402","1406","In this letter, we propose a synthetic aperture radar (SAR) target configuration recognition algorithm based on the fusion of multifeature low-rank representations (LRRs). First, Gabor, principal component analysis, and wavelet features are extracted for the SAR training set and test set, respectively. Second, with the LRR model, each feature of the test samples is represented by those of the training set, leading to the corresponding coefficient matrix. Then, the preliminary prediction labels of all features of the test sample are obtained according to the LRR coefficients. Third, in order to further improve the confidence of recognition and reduce the instability of the algorithm, a two-stage decision fusion strategy is adopted to obtain the final prediction labels. The first stage utilizes a vote fusion for the recognition results of multiaspect neighborhood test samples for each feature pattern, which exploits the strong correlation of these neighborhood samples. Furthermore, the second stage fuses the three results obtained in the first stage through Bayesian inference. Bayesian inference is widely used in decision fusion, which can improve the confidence of results by about 3%. Experiments on the moving and stationary target acquisition and recognition data set demonstrate the effectiveness and superiority of the proposed algorithm.","1558-0571","","10.1109/LGRS.2018.2842068","National Natural Science Foundation of China(grant numbers:61301224,61501068); Basic and Advanced Research Project in Chongqing(grant numbers:cstc2017jcyjA1378,cstc2016jcyjA0134); Guangxi Key Laboratory of Wireless Wideband Communication and Signal Processing, 2017 the Fund Project of Director(grant numbers:GXKL06170202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388228","Decision fusion;low-rank representation (LRR);synthetic aperture radar (SAR);target configuration recognition","Synthetic aperture radar;Feature extraction;Training;Target recognition;Principal component analysis;Signal processing algorithms;Bayes methods","Bayes methods;feature extraction;image fusion;image representation;pattern clustering;principal component analysis;radar imaging;radar target recognition;synthetic aperture radar","wavelet features;SAR training set;test set;LRR model;test sample;corresponding coefficient matrix;preliminary prediction labels;LRR coefficients;two-stage decision fusion strategy;final prediction labels;vote fusion;recognition results;multiaspect neighborhood test samples;feature pattern;neighborhood samples;stationary target acquisition;recognition data;synthetic aperture radar target configuration recognition algorithm;principal component analysis;multifeature low-rank representation fusion","","14","","18","IEEE","19 Jun 2018","","","IEEE","IEEE Journals"
"Modified Tensor Distance-Based Multiview Spectral Embedding for PolSAR Land Cover Classification","B. Ren; B. Hou; J. Chanussot; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xian, China; LJK, CNRS, Grenoble INP, Inria, Université Grenoble Alpes, Grenoble, France; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xian, China","IEEE Geoscience and Remote Sensing Letters","24 Nov 2020","2020","17","12","2095","2099","This letter proposes a novel method for combining multiview features in polarimetric synthetic aperture radar (PolSAR) for land cover classification. It is well-known that feature extraction and classifier design are two significant steps in machine learning methods for PolSAR data interpretation. Each PolSAR pixel can be represented in different feature spaces, such as polarimetric data scattering, or the polarimetric target decomposition spaces. In this letter, a tensor-based multiview embedding algorithm is proposed to fuse those features from different spaces in order to obtain a distinctive set of features for the subsequent classification. Based on the pixel-based classification tasks, a modified tensor distance (MTD) is designed to accurately calculate the distance between tensors. It emphasizes the importance of the central pixel, and decreases the influence of the neighbors in the feature patch when calculating tensor distance. Furthermore, the complementary properties of different views are exploited by an MTD measured tensor multiview spectral embedding method, so as to obtain relevant low-dimensional features. Compared with state-of-the-art methods, the validation and effectiveness of the proposed method is demonstrated on two real PolSAR data sets.","1558-0571","","10.1109/LGRS.2019.2962185","National Natural Science Foundation of China(grant numbers:61671350,61836009); Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-05); China Postdoctoral Science Foundation Funded Project(grant numbers:2018M633468); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957317","Land cover classification;modified tensor distance (MTD);multiview spectral embedding (MSE);polarimetric synthetic aperture radar (PolSAR)","Tensors;Synthetic aperture radar;Feature extraction;Task analysis;Backscatter;Speckle;Measurement","feature extraction;geophysical image processing;image classification;image fusion;image resolution;land cover;radar imaging;radar polarimetry;synthetic aperture radar;tensors","PolSAR data sets;low-dimensional features;spectral embedding method;feature patch;central pixel;pixel-based classification tasks;subsequent classification;tensor-based multiview;polarimetric target decomposition spaces;polarimetric data scattering;feature spaces;PolSAR pixel;PolSAR data interpretation;classifier design;feature extraction;polarimetric synthetic aperture radar;multiview features;PolSAR land cover classification;modified tensor distance-based multiview spectral embedding","","9","","16","IEEE","13 Jan 2020","","","IEEE","IEEE Journals"
"Hyperspectral Image Super-Resolution Based on Multiscale Mixed Attention Network Fusion","J. Hu; Y. Tang; Y. Liu; S. Fan","School of Electrical and Information Engineering and the Key Laboratory of Electric Power Robot of Hunan Province, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering and the Key Laboratory of Electric Power Robot of Hunan Province, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering and the Key Laboratory of Electric Power Robot of Hunan Province, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering and the Key Laboratory of Electric Power Robot of Hunan Province, Changsha University of Science and Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","11 Jan 2022","2022","19","","1","5","Hyperspectral images (HSIs) contain rich spectral information and have great application value. However, due to various hardware limitations, the spatial resolution of HSIs acquired by the sensor is low. HSI super-resolution (SR) attracts much attention to improve spatial quality. In this letter, a single HSI SR method based on network fusion is proposed. Our method includes the SR network part and fusion part. In the SR network part, we construct 3-D multiscale mixed attention networks (3-D-MSMANs) by cascading 3-D multiscale mixed attention block (3-D-MSMAB) to restore high-resolution HSIs. 3-D-MSMAB consists of the 3-D Res2net module and the mixed attention module. 3-D Res2net module is a simple and effective multiscale method. The mixed attention module is proposed by combining the first- and second-order statistics of features. In addition, we use the mutual learning loss between 3-D-MSMAN so that they can learn from each other. In the fusion part, the fusion module is designed to merge the output of each 3-D-MSMAN. Our method can achieve good results in both simulated and real SR experiments. Code is available at https://github.com/LYT-max/Mixed-Attention-for-HSI-SR.","1558-0571","","10.1109/LGRS.2021.3124974","National Natural Science Foundation of China(grant numbers:61601061); Natural Science Foundation of Hunan Province, China(grant numbers:2021JJ40609); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9598826","Hyperspectral image (HSI);image super-resolution (SR);mixed attention;mutual learning","Three-dimensional displays;Superresolution;Convolution;Spatial resolution;Hyperspectral imaging;Image reconstruction;Feature extraction","hyperspectral imaging;image fusion;image reconstruction;image resolution;learning (artificial intelligence)","single HSI SR method;SR network part;fusion part;high-resolution HSIs;mixed attention module;effective multiscale method;fusion module;hyperspectral image super-resolution;hyperspectral images;spectral information;spatial resolution;HSI super-resolution;spatial quality;multiscale mixed attention network fusion;3D Res2net module;3D-MSMAB;3D multiscale mixed attention block;3D-MSMAN;3D multiscale mixed attention networks","","4","","17","IEEE","2 Nov 2021","","","IEEE","IEEE Journals"
"Cross-Connected Bidirectional Pyramid Network for Infrared Small-Dim Target Detection","Y. Bai; R. Li; S. Gou; C. Zhang; Y. Chen; Z. Zheng","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; Academy of Advanced Interdisciplinary Research, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi’an, China; School of Maritime Economics and Management, Dalian Maritime University, Dalian, China; Chinese Academy of Sciences (CAS), Xi’an Institute of Optics and Precision Mechanics, Xi’an, China; Beijing Aerospace Automatic Control Institute, Beijing, China","IEEE Geoscience and Remote Sensing Letters","17 Feb 2022","2022","19","","1","5","Infrared small-dim target detection is an important technology in the fields of infrared guidance, anti-missile, and tracking system. Due to the small size of targets, no obvious structure information, and low image signal-to-noise ratio (SNR), infrared small-dim target detection is still a challenging task. In this letter, a cross-connected bidirectional pyramid network (CBP-Net) is proposed for infrared small-dim target detection. The main body of the CBP-Net is to embed a bottom-up pyramid in the feature pyramid network (FPN), which is designed to provide more comprehensive target information by connecting with the original multi-scale features and the top-down pyramid. The bottom-up pyramid together with the top-down pyramid forms the proposed bidirectional pyramid structure. Then, an region of interest (ROI) feature augment module (RFA) composed of deformable ROI pooling and position attention is designed to fuse multi-scale ROI features and enhance the spatial information of the small-dim target. Besides, a regular constraint loss (RCL) is introduced to restrict multi-scale feature fusion to learn more precise target location information. Experimental results on two challenging datasets show that the performance of the proposed CBP-Net is superior to the state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3145577","National Natural Science Foundation of China(grant numbers:61801345,61806154,62102296); Fundamental Research Funds for the Central Universities(grant numbers:JC2102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690173","Cross-connected bidirectional pyramid network (CBP-Net);infrared small-dim target detection;regular constraint loss (RCL);region of interest (ROI) feature augment","Feature extraction;Object detection;Proposals;Convolution;Loss measurement;Information filters;Signal to noise ratio","feature extraction;image fusion;image segmentation;infrared imaging;object detection","CBP-Net;feature pyramid network;comprehensive target information;bidirectional pyramid structure;precise target location information;cross-connected bidirectional pyramid network;infrared small-dim target detection;low image signal-to-noise ratio;bottom-up pyramid;top-down pyramid;region of interest;feature augment module;deformable ROI pooling;multiscale ROI features;regular constraint loss;multiscale feature fusion","","4","","22","IEEE","21 Jan 2022","","","IEEE","IEEE Journals"
"End-to-End Multilevel Hybrid Attention Framework for Hyperspectral Image Classification","J. Xiang; C. Wei; M. Wang; L. Teng","Key Laboratory of Advanced Ship Communication and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Ship Communication and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Ship Communication and Information Technology, Harbin Engineering University, Harbin, China; Key Laboratory of Advanced Ship Communication and Information Technology, Harbin Engineering University, Harbin, China","IEEE Geoscience and Remote Sensing Letters","11 Jan 2022","2022","19","","1","5","HSI has abundant spectral–spatial information. Using this information to improve the accuracy of HSI classification is a hot issue in the industry. This letter proposes an end-to-end multilevel hybrid attention network (DMCN). It is composed of a dense 3-D convolutional neural network (3D-CNN), grouped residual 2D-CNN, and coordinate attention that can perceive categories. In the case of a small number of training samples, DMCN can still extract spectral–spatial fusion information and learn spatial features more deeply for classification. Experiments are conducted on three well-known hyperspectral datasets, i.e., Indian Pines (IP), University of Pavia (UP), and Salinas (SA). The results show that DMCN achieved 92.39%, 97.28%, and 98.40% classification accuracy in IP, UP, and SA.","1558-0571","","10.1109/LGRS.2021.3126125","National Major Research and Development Project of China(grant numbers:2018YFE0206500); National Natural Science Foundation of China(grant numbers:62071140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606739","Classification;dense 3-D convolutional neural network (3D-CNN);grouped residual 2D-CNN;hybrid attention network;hyperspectral image (HSI)","Feature extraction;Convolution;Hyperspectral imaging;Convolutional neural networks;Complexity theory;Kernel;IP networks","convolutional neural nets;feature extraction;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence)","end-to-end multilevel hybrid attention framework;hyperspectral image classification;HSI classification;hybrid attention network;DMCN;3D convolutional neural network;grouped residual 2D-CNN;spatial feature learning;hyperspectral datasets;3D-CNN;coordinate attention;spectral-spatial fusion information extraction;training samples","","2","","15","IEEE","8 Nov 2021","","","IEEE","IEEE Journals"
"A Multiview Spectral–Spatial Feature Extraction and Fusion Framework for Hyperspectral Image Classification","J. Feng; J. Zhang; Y. Zhang","Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Hyperspectral images (HSIs) can provide abundant and diverse features which are helpful for classification, such as spectral, texture, and shape features. Combining these features can enhance the ability to describe the characteristics of different classes of land covers. However, most of the existing methods usually stack features from multiple views to construct high-dimensional data and then learn information from it which may waste information inherent in different feature spaces. In this letter, we proposed a multiview spectral–spatial feature extraction (FE) and fusion framework for the analysis and classification of the HSIs. First, different and complementary spatial features extracted by extended multiattribute profiles (EMAPs), gray-level cooccurrence matrix (GLCM), and Gabor from the original HSI are, respectively, stacked with the spectral bands to construct multiview data set for one single scene of HSI. Thus, each sample can be represented in different spectral–spatial domains. Then, a semisupervised FE method, which combines local fisher discriminant analysis (LFDA) that explores discriminative information from limited labeled samples and the improved neighborhood preserving embedding (NPE) that aims at maintaining the local neighborhood structure from a global perspective, is applied on the multiview data set to eliminate redundant information and obtain multiview spectral–spatial features. Note that the improved NPE which adds spatial interpixel correlations to similarity measure between samples is applied on all the samples rather than unlabeled samples, and hence, the best spatial nearby neighbors for each sample could be found from the whole data. Finally, we integrate these multiview features with diversity and complementarity to construct the intact feature representation for each sample. The experimental results show that the proposed method outperforms the state-of-the-art multiview FE methods for HSI classification.","1558-0571","","10.1109/LGRS.2021.3066613","National Natural Science Foundation of China(grant numbers:61871150); Fundamental Research Funds for the Local Colleges and Universities of Heilong Jiang Province(grant numbers:KYYWF10236180211); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9386215","Feature extraction (FE);feature fusion;hyperspectral image (HSI) classification;multiview feature","Feature extraction;Iron;Data mining;Stacking;Hyperspectral imaging;Shape;Correlation","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;matrix algebra;semi-supervised learning (artificial intelligence)","high-dimensional data;feature spaces;multiview spectral-spatial feature extraction;fusion framework;HSIs;complementary spatial features;gray-level cooccurrence matrix;spectral bands;multiview data set;semisupervised FE method;discriminative information;spatial interpixel correlations;spatial nearby neighbors;HSI classification;hyperspectral image classification;shape features;stack features;multiview FE methods;local Fisher discriminant analysis;extended multiattribute profiles;EMAPs;GLCM;LFDA;neighborhood preserving embedding;NPE;local neighborhood structure;similarity measure;feature representation","","2","","18","IEEE","25 Mar 2021","","","IEEE","IEEE Journals"
"Hyperspectral Image Stitching via Optimal Seamline Detection","Z. Peng; Y. Ma; X. Mei; J. Huang; F. Fan","Electronic Information School, Wuhan University, Wuhan, China; Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China; Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China; Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China; Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","30 Dec 2021","2022","19","","1","5","Hyperspectral images (HSIs) with both spatial and spectral information have found broad applications. Since most cameras have narrow viewing angle, generating panoramic images is essential to show a large-range view of the environment. So far, there are few studies on HSI stitching, and the stitching result still suffers from some problems, such as blurring and ghosting, geometric misalignment, visible seam, and spectral distortion. Hence, to address the above disadvantages, we propose a novel HSI stitching strategy using optimal seamline detection approach in this letter. First, we use a fast and robust seam estimation method to determine the seamline in each single band of HSI. This method works in RGB images, and we have modified it to be used in a single-band gray-scale image of HSI. Then, to guarantee the integrity of spatial and spectral information of hundreds of bands of HSI, we propose to apply the structural similarity (SSIM) index to select the optimal one among all band candidate seamlines and use the selected optimal seamline to stitch all the remaining bands. The experimental results demonstrate that our proposed approach outperforms traditional HSI stitching approach in both spatial and spectral performances.","1558-0571","","10.1109/LGRS.2021.3097394","National Natural Science Foundation of China(grant numbers:61903279,62075169,62003247,62061160370); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497352","Hyperspectral image (HSI) stitching;optimal seamline detection;spectral performance;structural similarity (SSIM) index","Indexes;Gray-scale;Distortion;Hyperspectral imaging;Image stitching;Estimation;Urban areas","cameras;geophysical image processing;hyperspectral imaging;image classification;image colour analysis;image fusion;image segmentation;image sensors;object detection","fast seam estimation method;robust seam estimation method;RGB images;spatial information;spectral information;structural similarity index;band candidate seamlines;selected optimal seamline;remaining bands;traditional HSI stitching approach;spatial performances;spectral performances;hyperspectral image stitching;hyperspectral images;HSIs;narrow viewing angle;panoramic images;large-range view;stitching result;geometric misalignment;visible seam;spectral distortion;novel HSI stitching strategy;optimal seamline detection approach","","2","","22","IEEE","27 Jul 2021","","","IEEE","IEEE Journals"
"A Multilayer Fusion Network With Rotation- Invariant and Dynamic Feature Representation for Multiview Low-Altitude Image Registration","Y. Liu; X. Gong; Y. Yang","Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University, Kunming, China; Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University, Kunming, China; Laboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University, Kunming, China","IEEE Geoscience and Remote Sensing Letters","21 May 2021","2021","18","6","1019","1023","Due to human and natural factors, when the small unmanned aerial vehicles (UAVs) are monitoring the ground, multiview transformation problems such as image distortion and low overlap will occur, which will inhibit the accuracy of low-altitude image registration and limit the subsequent application. In this letter, we propose a mismatch removal method based on the Siamese architecture to solve the issues of multiview images. A dynamic neighbor-guided patch representation is designed to enhance the representation of each feature point. Meanwhile, a multilayer fusion is used to obtain more comprehensive information on feature points, and whether a pair of points correspond depends on the similarity of its descriptors. The network is trained by adding a rotation-invariant layer to solve the inevitable rotation and image distortion in multiview scenarios. The experimental results prove that our method can deal with the scenarios of the horizontal rotation, vertical rotation, mixture, scaling, and extreme, and is better than the other five state-of-the-art methods in most scenarios.","1558-0571","","10.1109/LGRS.2020.2992816","National Natural Science Foundation of China(grant numbers:41971392,41661080); Yunnan Ten-Thousand Talents Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9094586","Feature matching;multilayer fusion (MLF);multiview;neighbor-guided;rotation-invariant","Feature extraction;Training;Unmanned aerial vehicles;Nonhomogeneous media;Image registration;Monitoring;Robustness","autonomous aerial vehicles;feature extraction;image fusion;image registration;image representation;neural net architecture;robot vision","mismatch removal method;Siamese architecture;dynamic neighbor-guided patch representation;feature point;image distortion;multilayer fusion network;dynamic feature representation;small unmanned aerial vehicles;multiview low altitude image registration;rotation-invariant feature representation","","2","","15","IEEE","18 May 2020","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and Multispectral Images by Convolutional Sparse Representation","C. Xing; Y. Cong; Z. Wang; M. Wang","Shenzhen Research Institute, Nanjing University of Aeronautics and Astronautics, Shenzhen, China; Shenzhen Research Institute, Nanjing University of Aeronautics and Astronautics, Shenzhen, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","18 Mar 2022","2022","19","","1","5","Sparse representation (SR)-based methods have achieved numerous successes in the fusion of hyperspectral and multispectral images (HSIs and MSIs). However, in many SR-based fusion methods, due to patch dividing, it is hard to make pixel values across boundaries of contiguous patches be exactly consistent, which limits the ability to preserve scene details. To remedy such deficiency, a fusion framework is proposed for HSIs and MSIs by using convolutional sparse representation (FCS). This novel fusion method consists of three stages: 1) the spectral dictionary is trained by the convolutional sparse dictionary learning algorithm to extract spectral information from HSIs; 2) hyperspectral and multispectral transferring matrices are estimated to map HSIs and MSIs onto the space of high-resolution hyperspectral images (HR-HSIs); and 3) we construct the convolutional sparse fusion model for HR-HSIs. Different from those traditional patch-based SR fusion methods, the FCS method focuses on the whole images instead of dividing patches, which can suppress the limitation of scene detail preservation caused by sparse coding on independent patches. Also, it belongs to a kind of online learning without lots of training samples. The Pavia dataset and the Paris dataset are used to evaluate the performance of our method. Experimental results indicate that the FCS method achieves much fusion performance compared with commonly used and state-of-the-art algorithms.","1558-0571","","10.1109/LGRS.2022.3155595","National Natural Science Foundation of China(grant numbers:62101247,62106104,61903189,61973158); Special Fund for Guiding Local Scientific and Technological Development of the Central Government in Shenzhen(grant numbers:2021Szvup063); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723078","Convolutional sparse representation;fusion;hyperspectral and multispectral images (HSIs and MSIs);sparse representation (SR)","Hyperspectral imaging;Convolutional codes;Sparse matrices;Dictionaries;Encoding;Image reconstruction;Machine learning","geophysical image processing;hyperspectral imaging;image classification;image fusion;image representation;image resolution;learning (artificial intelligence);matrix algebra","sparse coding;FCS method;multispectral images;convolutional sparse representation;sparse representation-based methods;SR-based fusion methods;patch dividing;contiguous patches;MSIs;spectral dictionary;convolutional sparse dictionary;map HSIs;high-resolution hyperspectral images;HR-HSIs;convolutional sparse fusion model;patch-based SR fusion methods;Paris dataset;online learning;Pavia dataset;multispectral transferring matrices","","1","","26","IEEE","28 Feb 2022","","","IEEE","IEEE Journals"
"Fast Ship Detection With Spatial-Frequency Analysis and ANOVA-Based Feature Fusion","W. Zhang; Q. M. J. Wu; Y. Yang; T. Akilan; W. G. W. Zhao; Q. Li; J. Niu","Department of Computer Science, Lakehead University, Thunder Bay, ON, Canada; Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada; Vector Institute for Artificial Intelligence, Toronto, ON, Canada; Department of Software Engineering, Lakehead University, Thunder Bay, ON, Canada; Faculty of Business Administration, Lakehead University, Thunder Bay, ON, Canada; Department of Engineering, Ocean University of China, Qingdao, China; Department of Engineering, Ocean University of China, Qingdao, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2021","2022","19","","1","5","High-frequency surface wave radar (HFSWR) can be effectively used to detect ships in the exclusive economic zone. However, the ship signal is concealed and interfered with various clutter and background noise in the Doppler spectrum. In this letter, a range-Doppler (RD) image-based novel ship detection algorithm is proposed by exploiting spatial-frequency information and a unique feature fusion based on the analysis of variance. The algorithm subsumes three successive stages: Stage I—the plausible region of interest is captured, Stage II—the features from different sources are fused into one generalized feature space, and Stage III—an extreme learning machine-based classifier is utilized to localize the ships. Experimental results on challenging HFSWR-RD datasets demonstrate that the proposed algorithm has a competitive performance over other ship detection algorithms.","1558-0571","","10.1109/LGRS.2021.3076661","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425544","ANOVA;extreme learning machine;high-frequency surface wave radar;range-Doppler (RD) image","Marine vehicles;Clutter;Analysis of variance;Feature extraction;Frequency-domain analysis;Noise measurement;Kernel","Doppler radar;feedforward neural nets;image capture;image classification;image denoising;image fusion;radar clutter;radar detection;radar imaging;ships;statistical analysis","fast ship detection;spatial-frequency analysis;ANOVA-based feature fusion;high-frequency surface wave radar;background noise;Doppler spectrum;range-Doppler image-based novel ship detection algorithm;spatial-frequency information;feature fusion;extreme learning machine-based classifier;ship detection algorithms;HFSWR-RD datasets;analysis of variance;clutter noise","","1","","15","IEEE","7 May 2021","","","IEEE","IEEE Journals"
"SAR Image Change Detection Based on Semisupervised Learning and Two-Step Training","C. Wang; W. Su; H. Gu","School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","20 Dec 2021","2022","19","","1","5","Change detection, as an essential part of synthetic aperture radar (SAR) automatic target recognition (ATR) systems, remains a challenging problem. The lack of labeled data seriously impedes the development of deep learning-based methods in SAR applications. In this letter, we propose a patch-based semisupervised method to detect changed pixels from limited training data in the field of SAR. The complete approach includes the unsupervised pretraining stage and the iterative discrimination stage. The states of pixels are determined by comparing the corresponding image patches. First, to reduce the impact of insufficient data, we train a trapezium U-Net-like structure to extract representative and generalized features by unsupervised training on all pixels. Feature maps contain detailed and semantic information due to repetitive feature fusions. A designed feature activation module is utilized to recalibrate fused features. Finally, the discrimination is finished by a simple classification convolutional neural network (CNN) following a two-step training strategy. Ablation studies indicate the function provided by the proposed modifications. Experiments on real SAR images demonstrate that the proposed method can improve the detection accuracy by more than 1.2% compared with other state-of-the-art deep learning-based methods.","1558-0571","","10.1109/LGRS.2021.3050746","National Natural Science Foundation of China(grant numbers:61671246,61801221,62001229); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9332238","Change detection;channel-attention;feature fusion;pseudolabels;semisupervised;synthetic aperture radar (SAR)","Feature extraction;Radar polarimetry;Training;Shape;Synthetic aperture radar;Convolution;Training data","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image classification;image fusion;image recognition;object detection;radar computing;radar imaging;synthetic aperture radar;unsupervised learning","representative features;generalized features;unsupervised training;feature maps;detailed information;semantic information;feature fusions;designed feature activation module;fused features;simple classification convolutional neural network;two-step training strategy;SAR images;detection accuracy;state-of-the-art deep learning-based methods;SAR image change detection;synthetic aperture radar automatic target recognition systems;SAR applications;patch-based semisupervised method;changed pixels;training data;unsupervised pretraining stage;iterative discrimination stage;corresponding image patches;trapezium U-Net-like structure","","1","","16","IEEE","21 Jan 2021","","","IEEE","IEEE Journals"
"Automatic Registration Method for TLS LiDAR Data and Image-Based Reconstructed Data","L. Xu; J. Feng; X. Li; C. Liu","State Key Laboratory of Inertial Science and Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China; State Key Laboratory of Inertial Science and Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China; State Key Laboratory of Inertial Science and Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China; State Key Laboratory of Inertial Science and Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","26 Feb 2019","2019","16","3","482","486","Point clouds registration is an important research topic in the field of data fusion from camera and light detection and ranging (LiDAR). In this letter, a new registration method, fast multiscale registration (FMSR), takes the scale factor into account and is proposed for the registration of two point clouds obtained from camera and LiDAR. An adaptive-scale keypoint quality algorithm was used to detect and match keypoints, which were input to the coarse registration process to improve the coarse registration accuracy. A new heuristic criterion was also proposed for fine registration, which avoids falling into the local minima. Furthermore, to increase efficiency of fine registration, the k-nearest neighbors algorithm was selected to directly search the optimal matching from the raw point clouds without triangulating point clouds into mesh. The FMSR method is highly precise, insensitive to outliers, and relatively efficient. Experimental results showed that the root-mean-square error of the registration was approximately 0.2 m when the size of the object was about 20.3 m × 7.85 m × 26.56 m, the total number of matched points was 12 789, and the execution time was approximately 2.1 s, indicating that the proposed method resulted in improved accuracy and efficiency of registration.","1558-0571","","10.1109/LGRS.2018.2875178","National Natural Science Foundation of China(grant numbers:61671038,61721091); Program for Changjiang Scholars and Innovative Research Team in University(grant numbers:IRT16R02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8513844","Fast multiscale registration (FMSR);keypoints detection and matching;point clouds;scale factor","Three-dimensional displays;Laser radar;Cameras;Feature extraction;Image reconstruction;Covariance matrices;Principal component analysis","image fusion;image reconstruction;image registration;optical radar","TLS LiDAR data;image-based reconstructed data;point clouds registration;data fusion;light detection;fast multiscale registration;scale factor;adaptive-scale keypoint quality algorithm;coarse registration process;coarse registration accuracy;fine registration;k-nearest neighbors algorithm;optimal matching;raw point clouds;FMSR method;matched points;automatic registration method","","1","","20","IEEE","28 Oct 2018","","","IEEE","IEEE Journals"
"GCN-Enhanced Multidomain Fusion Network for Through-Wall Human Activity Recognition","X. Wang; S. Guo; J. Chen; P. Chen; G. Cui","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","26 May 2022","2022","19","","1","5","This letter considers the problem of human activity recognition (HAR) behind the walls using ultrawideband (UWB) radar. The graph convolutional network (GCN)-enhanced multidomain fusion network (GMFN) is proposed to improve the recognition performance by utilizing the complementarity of the multidomain features. Specifically, first, a multibranch convolutional neural network (CNN) is proposed to extract the multidomain features from the range, time-frequency (TF), and range-Doppler (RD) domain. Then the multidomain features are constructed as a graph, and the GCN is employed to fuse the multidomain features on the graph. Finally, HAR is implemented in the form of graph classification. The experimental results on the real data show that the proposed GMFN achieves better performance than the state-of-the-art multidomain fusion HAR methods.","1558-0571","","10.1109/LGRS.2022.3176117","National Natural Science Foundation of China(grant numbers:61871080,62001091,U19B2017); Changjiang Scholar Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777687","Graph neural network;human activity recognition (HAR);multidomain feature fusion;ultrawideband~(UWB) radar","Feature extraction;Spectrogram;Radar;Discrete Fourier transforms;Fuses;Time-frequency analysis;Radar scattering","convolutional neural nets;feature extraction;graph theory;image classification;image fusion;image motion analysis;image recognition;ultra wideband radar","ultrawideband radar.;recognition performance;multidomain features;multibranch convolutional neural network;state-of-the-art multidomain fusion HAR methods;GCN-enhanced multidomain fusion network;through-wall human activity recognition;UWB;CNN;TF;RD;time-frequency domain;range-Doppler domain;GMFN","","1","","17","IEEE","18 May 2022","","","IEEE","IEEE Journals"
"A Multi-Domain Fusion Human Motion Recognition Method Based on Lightweight Network","P. Chen; Q. Jian; P. Wu; S. Guo; G. Cui; C. Jiang; L. Kong","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","7 Jan 2022","2022","19","","1","5","Through-wall human motion recognition is suffered from the problems of too few samples and too large model parameters. In this letter, we propose a multi-domain fusion through-the-wall radar (TWR) human motion recognition model based on lightweight network and transfer learning. Specifically, in order to make full use of the target information, a multiple parallel feature pyramid network (FPN) is first proposed to extract the detailed feature information from the time–frequency map and range profile. After that, a lightweight network based on the MobileNetV3 network and transfer learning is proposed. The MobileNetV3 model is pre-trained on the public ImageNet database. To ensure the performance of transfer learning, a heterogeneous migration learning algorithm is used to cross-domain transform the obtained time–frequency map and range profile. Experimental results show that the proposed model has a better performance in accuracy, model size, training time, and robustness compared with the existing methods. It also has the potential to embed portable radar, which has important research value for the application of radar in real life.","1558-0571","","10.1109/LGRS.2021.3132692","National Natural Science Foundation of China(grant numbers:61871080,62001091,61701088); Initial Scientific Research Foundation of University of Electronic Science and Technology of China (UESTC)(grant numbers:Y030202059018051); Sichuan Science and Technology Program(grant numbers:2019JDJQ0014); ChangJiang Scholar Program; National Defense Science and Technology Special Innovation Zone Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635774","Feature pyramid network (FPN);human motion recognition;lightweight network;transfer learning","Feature extraction;Time-frequency analysis;Radar;Target recognition;Transfer learning;Doppler effect;Semantics","feature extraction;image fusion;image motion analysis;image recognition;image representation;learning (artificial intelligence);radar imaging","multidomain fusion human motion recognition method;lightweight network;through-wall human motion recognition;model parameters;multidomain fusion through-the-wall radar;transfer learning;target information;multiple parallel feature pyramid network;detailed feature information;time-frequency map;range profile;MobileNetV3 network;MobileNetV3 model;heterogeneous migration learning algorithm","","","","15","IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"DGCNN Network Architecture With Densely Connected Point Pairs in Multiscale Local Regions for ALS Point Cloud Classification","Y. Chen; Y. Xu; Y. Xing; G. Liu","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Earth Sciences and Engineering, Hohai University, Nanjing, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","28 Dec 2021","2022","19","","1","5","Deep learning in 3-D classification tasks focuses on the designs of comprehensive local aggregation operators. Airborne laser scanning (ALS) point clouds have their own characteristics: 1) the object overlaps in the vertical direction; 2) the spatial density is uneven; and 3) the objects present large-scale variations between different categories. However, the dynamic graph convolutional neural network (DGCNN) ignores the inherent properties of ALS point clouds. This study modifies the DGCNN network and proposes a new neural network module called Webconv, which densely connects point pairs in multiscale local regions to learn contextual information. One modified cross entropy loss function with variable weight is proposed to solve the problem of uneven category distributions in ALS points. Because preprocessing such as block partition ignores the context information of the whole region, the conditional random field is used to refine the point cloud. Our approach achieves state-of-the-art performance on the dataset of the 2019 IEEE GRSS Data Fusion Contest 3-D Point Cloud Classification Challenge and can be widely used for ALS point classification.","1558-0571","","10.1109/LGRS.2021.3102599","National Natural Science Foundation of China(grant numbers:41904170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519928","Airborne laser scanning (ALS) point cloud classification;contextual information learning;dynamic graph convolutional neural network (DGCNN);local feature aggregation;multiscale representation","Three-dimensional displays;Semantics;Feature extraction;Training;Task analysis;Laser radar;Labeling","entropy;feature extraction;geophysical image processing;geophysical signal processing;graph theory;image classification;image fusion;learning (artificial intelligence);neural nets;sensor fusion","DGCNN network architecture;multiscale local regions;ALS Point Cloud Classification;deep learning;3-D classification tasks;comprehensive local aggregation operators;airborne laser scanning;vertical direction;spatial density;dynamic graph convolutional neural network;neural network module;modified cross entropy loss function;uneven category distributions;ALS points;2019 IEEE GRSS Data Fusion Contest 3-D Point Cloud Classification Challenge","","","","18","IEEE","20 Aug 2021","","","IEEE","IEEE Journals"
"RGB-D Salient Object Detection via Minimum Barrier Distance Transform and Saliency Fusion","A. Wang; M. Wang","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China","IEEE Signal Processing Letters","4 Apr 2017","2017","24","5","663","667","Automatic detection of salient objects in images has gained its popularity in computer vision field for its usage in numerous vision tasks in recent years. Depth information plays an important role in the human vision system while it is underutilized in most existing two-dimensional (2-D) saliency detection methods. In this letter, a multistage salient object detection framework via minimum barrier distance transform and saliency fusion based on multilayer cellular automata (MCA) is proposed. First, we independently generate the 3-D spatial prior, depth bias, and RGB-produced and depth-induced saliency maps. Next, the two saliency maps are weighted by depth bias to obtain two initial maps. Then, we adopt a saliency optimization step to generate more precise depth-induced saliency map. Moreover, the initial RGB-produced and the optimized depth-induced maps are further fused with 3-D spatial prior. Finally, we utilize MCA to fuse all saliency maps generated previously and obtain the final saliency result with complete salient object. The proposed method is evaluated on the publicly available benchmark dataset, RGBD1000. Compared to several state-of-the-art 2-D and depth-aware approaches, the experimental results demonstrate the effectiveness and superiority of our method, which can accurately detect the salient objects from RGB-D images, and has the most satisfactory overall performance.","1558-2361","","10.1109/LSP.2017.2688136","National Science Foundation of China(grant numbers:61025005); National Key Research and Development Program of China(grant numbers:2016YFB0700802,2016YFB0800600); The Innovative Youth Projects of Ocean Remote Sensing Engineering Technology Research Center of State Oceanic Administration of China(grant numbers:2015001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887660","Minimum barrier distance (MBD);RGB-D image;saliency fusion;saliency map;salient object detection","Object detection;Two dimensional displays;Three-dimensional displays;Optimization;Transforms;Signal processing algorithms;Image color analysis","cellular automata;computer vision;image colour analysis;image fusion;object detection;optimisation;transforms","RGBD1000 benchmark dataset;saliency optimization;depth-induced saliency maps;RGB-produced saliency maps;depth bias;3D spatial prior generation;MCA;multilayer cellular automata;multistage salient object detection framework;2D saliency detection methods;two-dimensional saliency detection methods;human vision system;depth information;computer vision;automatic salient object detection;saliency fusion;minimum barrier distance transform;RGB-D salient object detection","","47","","37","IEEE","27 Mar 2017","","","IEEE","IEEE Journals"
"Multispectral Fusion Transformer Network for RGB-Thermal Urban Scene Semantic Segmentation","H. Zhou; C. Tian; Z. Zhang; Q. Huo; Y. Xie; Z. Li","PLA Academy of Military Science (AMS), Institute of Systems Engineering, Beijing, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; PLA Academy of Military Science (AMS), Institute of Systems Engineering, Beijing, China; PLA Academy of Military Science (AMS), Institute of Systems Engineering, Beijing, China; PLA Academy of Military Science (AMS), Institute of Systems Engineering, Beijing, China","IEEE Geoscience and Remote Sensing Letters","10 Jun 2022","2022","19","","1","5","Semantic segmentation plays a vital role in autonomous vehicles. Fusing the rich details of RGB image and the illumination robustness of thermal image has great potential to improve the performance of RGB-T semantic segmentation. In multispectral feature fusion, the current main methods are less effective in the characterization of correlations and complementarities of RGB-T. In order to generate robust cross-spectral fusion features, we propose a multispectral fusion transformer network (MFTNet). Specifically, we first design an MFT module to handle the intraspectra correlation and the interspectra complementarity of RGB-T in the multispectral fusion encoder. MFT effectively enhances the RGB-T feature representation under various challenges. Then, an optimization strategy with progressive deep supervision (PDS) loss is proposed to directly supervise the upper and lower layers of the decoder. This strategy can guide the decoder to achieve precise segmentation in a coarse-to-fine manner. Finally, plenty of experimental results prove the effectiveness of our method. On the MFNet dataset, MFNet achieved 74.7 mAcc and 57.3 mIoU, outperforming the state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3179721","National Natural Science Foundation of China(grant numbers:62173265); Fundamental Research Funds for the Central Universities; Innovation Fund of Xidian University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9789430","Deep learning;multispectral fusion;RGB-T images;semantic segmentation;transformer","Decoding;Semantics;Transformers;Image segmentation;Feature extraction;Correlation;Convolution","feature extraction;geophysical image processing;image colour analysis;image fusion;image segmentation;infrared imaging","multispectral fusion encoder;RGB-T feature representation;precise segmentation;multispectral fusion transformer network;RGB-thermal urban scene semantic segmentation;RGB image;illumination robustness;thermal image;multispectral feature fusion;current main methods;complementarities;robust cross-spectral fusion features","","4","","22","IEEE","7 Jun 2022","","","IEEE","IEEE Journals"
"HLF-Net: Pansharpening Based on High- and Low-Frequency Fusion Networks","W. Diao; F. Zhang; H. Wang; W. Wan; J. Sun; K. Zhang","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2022","2022","19","","1","5","Many deep neural networks (DNNs) have been constructed for the pansharpening task. However, the differences between the high and low frequencies in images are not considered in some DNN-based pansharpening methods. As high and low frequencies have different information of images, it is difficult for the same network to learn and reconcile the two kinds of frequencies. Considering the aforementioned differences, we propose a new pansharpening network to fuse the high and low frequencies in low spatial resolution multispectral (LRMS) and panchromatic (PAN) images separately. Specifically, a high- and low-frequency fusion network (HLF-Net) is constructed, which is composed of a high-frequency fusion network (HF-Net) and a low-frequency fusion network (LF-Net). In the HF-Net, skip attention is introduced into U-Net to better retain the high frequencies in feature maps. The LF-Net uses the involution to capture the dependency among the channels of feature maps. Experiments on the GeoEye-1 dataset reveal that the proposed network outperforms some state-of-the-art methods. The code can be accessed at https://github.com/RSMagneto/HLF-Net","1558-0571","","10.1109/LGRS.2022.3225974","Natural Science Foundation of China(grant numbers:61901246); China Postdoctoral Science Foundation Grant(grant numbers:2019TQ0190,2019M662432); China Scholarship Council(grant numbers:202008370035); Scientific Research Leader Studio of Jinan(grant numbers:2021GXRC081); Ningxia Natural Science Foundation(grant numbers:2021AAC03045); Joint Project for Smart Computing of Shandong Natural Science Foundation(grant numbers:ZR2020LZH015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9967980","High-frequency fusion network (HF-Net);low-frequency fusion network (LF-Net);multispectral (MS) image;panchromatic (PAN) image;pansharpening","High frequency;Pansharpening;Spatial resolution;Convolution;Feature extraction;Sun;Distortion","deep learning (artificial intelligence);hyperspectral imaging;image fusion;image resolution","deep neural networks;DNN-based pansharpening methods;feature maps;GeoEye-1 dataset;high-frequency fusion network;HLF-Net;LF-Net;low spatial resolution multispectral images;low-frequency fusion network;LRMS;PAN images;panchromatic images;pansharpening network;pansharpening task;skip attention;U-Net","","1","","27","IEEE","1 Dec 2022","","","IEEE","IEEE Journals"
"Intelligent On-Site Lithology Identification Based on Deep Learning of Rock Images and Elemental Data","Z. Xu; H. Shi; P. Lin; W. Ma","Geotechnical and Structural Engineering Research Center and the School of Qilu Transportation, Shandong University, Jinan, China; Geotechnical and Structural Engineering Research Center and the School of Civil Engineering, Shandong University, Jinan, China; Geotechnical and Structural Engineering Research Center and the School of Qilu Transportation, Shandong University, Jinan, China; Geotechnical and Structural Engineering Research Center and the School of Qilu Transportation, Shandong University, Jinan, China","IEEE Geoscience and Remote Sensing Letters","14 Jun 2022","2022","19","","1","5","To reduce the disadvantages of factors such as limited expression of 2-D images, weathering, or man-made pollution on image-based methods and realize rapid and accurate on-site lithology identification, we propose an intelligent lithology identification method based on the deep learning of rock images and element information. The initial lithology probabilities and locations are first determined from the images using an object detection model. Then, the initial lithology probabilities and element data are used as an input to correct the lithology probabilities using self-constructed fusion model. Finally, all lithology detection results are integrated and redundant results are removed. The verification experiment shows that the accuracy of the proposed method is 13.73% higher than that of the image-based method. The proposed method improves the performance of lithology identification by improving the classification of lithology and influencing the choice of bounding box. The proposed fusion identification method is a real-time and accurate method for the identification of complex lithology and provides a new idea for the rapid and intelligent identification of on-site lithology.","1558-0571","","10.1109/LGRS.2022.3179623","National Science Fund for Excellent Young Scholars(grant numbers:52022053); National Natural Science Foundation of China(grant numbers:52009073); Science Fund for Distinguished Young Scholars of Shandong Province(grant numbers:ZR201910270116); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785980","Deep learning;elemental data;lithology identification;object detection;rock image","Rocks;Data models;Feature extraction;Object detection;Training;Object recognition;Data mining","backpropagation;deep learning (artificial intelligence);geophysics computing;image classification;image fusion;object detection;probability;rocks","deep learning;rock images;elemental data;image-based method;intelligent lithology identification method;element information;initial lithology probabilities;object detection model;element data;self-constructed fusion model;fusion identification method;rapid identification;intelligent identification;intelligent on-site lithology identification;2D images;lithology detection","","1","","16","IEEE","1 Jun 2022","","","IEEE","IEEE Journals"
"A Deep Learning-Based GPR Forward Solver for Predicting B-Scans of Subsurface Objects","Q. Dai; Y. H. Lee; H. -H. Sun; J. Qian; G. Ow; M. L. M. Yusof; A. C. Yucel","School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore; National Parks Board, Cluny Road, Singapore; National Parks Board, Cluny Road, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Jurong West, Singapore","IEEE Geoscience and Remote Sensing Letters","26 Jul 2022","2022","19","","1","5","The forward full-wave modeling of ground-penetrating radar (GPR) facilitates the understanding and interpretation of GPR data. Traditional forward solvers require excessive computational resources, especially when their repetitive executions are needed in signal processing and/or machine learning algorithms for GPR data inversion. To alleviate the computational burden, a deep learning-based 2-D GPR forward solver is proposed to predict the GPR B-scans of subsurface objects buried in the heterogeneous soil. The proposed solver is constructed as a bimodal encoder–decoder neural network. Two encoders followed by an adaptive feature fusion module are designed to extract informative features from the subsurface permittivity and conductivity maps. The decoder subsequently constructs the B-scans from the fused feature representations. To enhance the network’s generalization capability, transfer learning is employed to fine-tune the network for new scenarios vastly different from those in training set. Numerical results show that the proposed solver achieves a mean relative error of 1.28%. For predicting the B-scan of one subsurface object, the proposed solver requires 12 ms, which is  $22\,\,500\times $  less than the time required by a classical physics-based solver.","1558-0571","","10.1109/LGRS.2022.3192003","Ministry of National Development Research Fund, National Parks Board, Singapore; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832628","Deep learning;ground-penetrating radar (GPR) forward solver;heterogeneous soil;transfer learning","Feature extraction;Decoding;Transfer learning;Permittivity;Conductivity;Training;Time-domain analysis","decoding;deep learning (artificial intelligence);feature extraction;geophysical image processing;ground penetrating radar;image fusion;radar computing;radar imaging","subsurface object;full-wave modeling;ground-penetrating radar;excessive computational resources;signal processing;machine learning algorithms;GPR data inversion;B-scan;bimodal encoder-decoder neural network;adaptive feature fusion module;subsurface permittivity;conductivity maps;fused feature representations;transfer learning;classical physics-based solver;deep learning-based 2D GPR forward solver;B-scans prediction;time 12.0 ms","","1","","14","IEEE","18 Jul 2022","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Based on Semisupervised Self-Learning and Multiview Information Fusion","J. Feng; J. Zhang; Y. Zhang","Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","12 Jul 2022","2022","19","","1","5","Hyperspectral images (HSIs) have been utilized in various fields due to abundant information, whose application is to a great extent limited by the number of labeled samples. Hence, how to exploit the diverse and complementary characteristics of multiple information from different views inherent in the HSIs is very critical for improving the classification performance under the condition of small-sized training set. This letter presents a novel method to tackle the small labeled sample size problem with semisupervised self-learning (S3L) and multiview information fusion. First, a sample augmentation scheme based on S3L and high-reliable neighborhood structure is designed for realizing training set enlargement. Thus, the pseudo-labeled samples with high quality would be automatically picked out from the unlabeled data by exploiting complementary information from multiple views, i.e., semantic information, spectral bands, texture, and geospatial information. Then, we retrain the classifier by the enlarged training set and generate an intermediate classification map. Finally, we simultaneously utilize the location and gradient information of the samples to adaptively realize the refined land-cover classification. Experimental results on three widely used datasets compared with several representative methods for small-sized HSI classification validate the effectiveness of the proposed method.","1558-0571","","10.1109/LGRS.2022.3186680","National Natural Science Foundation of China(grant numbers:61871150); Fundamental Research Funds for the Local Colleges; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9808343","Hyperspectral image (HSI) classification;multiview information;sample augmentation;semisupervised self-learning (S³L)","Training;Testing;Iron;Semantics;Hyperspectral imaging;Classification algorithms;Measurement","feature extraction;hyperspectral imaging;image classification;image fusion;image texture;unsupervised learning","hyperspectral image classification;semisupervised self-learning;multiview information fusion;HSIs;labeled samples;complementary characteristics;classification performance;small-sized training set;labeled sample size problem;sample augmentation scheme;high-reliable neighborhood structure;training set enlargement;pseudolabeled samples;complementary information;semantic information;geospatial information;intermediate classification map;gradient information;refined land-cover classification;small-sized HSI classification","","","","13","IEEE","27 Jun 2022","","","IEEE","IEEE Journals"
"Low-Light Image Enhancement for UAVs With Multi-Feature Fusion Deep Neural Networks","A. Singh; A. Chougule; P. Narang; V. Chamola; F. R. Yu","Department of Computer Science and Information Systems, Birla Institute of Technology and Science (BITS)-Pilani, Pilani, India; Department of Electrical and Electronics Engineering, Anuradha and Prashanth Palakurthi Centre for Artificial Intelligence Research (APPCAIR), Birla Institute of Technology and Science (BITS)-Pilani, Pilani, India; Department of Computer Science and Information Systems, Birla Institute of Technology and Science (BITS)-Pilani, Pilani, India; Department of Electrical and Electronics Engineering, Anuradha and Prashanth Palakurthi Centre for Artificial Intelligence Research (APPCAIR), Birla Institute of Technology and Science (BITS)-Pilani, Pilani, India; Department of Systems and Computer Engineering, Carleton University, Ottawa, Canada","IEEE Geoscience and Remote Sensing Letters","16 Sep 2022","2022","19","","1","5","Object detection in low-light aerial images is a challenging problem due to considerable variation in brightness and varying contrast. Deep learning-based approaches have recently demonstrated great promise in image enhancement. Many existing neural networks used for image quality enhancement first encode the input into low-resolution representations and then decode these representations back to a higher resolution for the contextual information. However, this method leads to the loss of semantic content. Recent research has demonstrated the advantage of maintaining high-resolution information along with lower resolution representations, which maintains image features throughout the network. In this letter, we propose a novel architecture named RNet for low-light image enhancement of aerial images. The proposed network contains multiresolution branches for better understanding of different levels of local and global context through different streams. The performance of RNet is evaluated on a recent synthetic dataset. We also present a comprehensive evaluation with a representative set of state-of-the-art enhancement techniques and neural net architectures.","1558-0571","","10.1109/LGRS.2022.3181106","National Rural Infrastructure Development Agency (NRIDA) Research Grant funding under the Project titled “AI-enabled drone-based remote health assessment of PMGSY roads”(grant numbers:NRRDA-P017(23)/2021-Dir (P-II)); ARTPARK Student Innovation Grant funding under the Project titled “Enhancing drone-based surveillance in low visibility conditions”(grant numbers:UG-07); Shastri Indo Canadian Institute (SICI) Shastri Institutional Collaborative Research Grant (SICRG) through the Artificial Intelligence Enabled Security Provisioning and Vehicular Vision Innovations for Autonomous Vehicles Project; Shastri Indo Canadian Institute (SICI) Shastri Institutional Collaborative Research Grant (SICRG) through the Artificial Intelligence Enabled Security Provisioning and Vehicular Vision Innovations for Autonomous Vehicles Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791380","Deep learning;image enhancement;low-light vision;unmanned aerial vehicle (UAV)","Image resolution;Image enhancement;Streaming media;Training;Lighting;Feature extraction;Measurement","autonomous aerial vehicles;deep learning (artificial intelligence);feature extraction;image enhancement;image fusion;image representation;image resolution;object detection;robot vision","multifeature fusion deep neural networks;low-light aerial images;brightness;varying contrast;deep learning-based approaches;image quality enhancement;low-resolution representations;high-resolution information;image features;low-light image enhancement;object detection;RNet;UAV","","","","24","IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"Hyperspectral Fusion Using Weighted Nonlocal Vector Total Variation","U. V. S.; P. Nair; K. N. Chaudhury","Department of Electrical Engineering, Indian Institute of Science, Bengaluru, India; Department of Electrical Engineering, Indian Institute of Science, Bengaluru, India; Department of Electrical Engineering, Indian Institute of Science, Bengaluru, India","IEEE Geoscience and Remote Sensing Letters","2 Nov 2022","2022","19","","1","5","Hyperspectral (HS) images have high spectral but low spatial resolutions, while multispectral (MS) images, on the other hand, have low spectral but high spatial resolutions. In HS–MS fusion, the HS and MS images are combined to obtain a single image with high spatial and spectral resolutions. Such images are typically textured and exhibit repetitive structures. To exploit this prior, we propose a nonlocal weighted total-variation regularizer. The novelty of the design is that the following hold: 1) we use a weighted norm, where the weights are derived from the MS image and 2) pixel variations over nonlocal neighborhoods are considered. We incorporate the regularizer into a standard convex optimization framework involving quadratic data-fidelity terms. We develop an efficient ADMM algorithm for solving this optimization problem—the novelty in this regard is that we use a variable splitting technique that results in the closed-form solutions of the ADMM subproblems. We report results on standard datasets demonstrating that the proposed regularizer can recover fine textures (as opposed to local pixel-based methods) and outperform the state-of-the-art methods.","1558-0571","","10.1109/LGRS.2022.3215281","SERB(grant numbers:CRG/2020/000527); Government of India and ISRO-IISc Space Technology Cell(grant numbers:ISTC/EEE/KNC/440); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9921218","ADMM;hyperspectral (HS) fusion;nonlocal regularization;total variation","Spatial resolution;Standards;Noise measurement;Noise reduction;Deep learning;Closed-form solutions;Bayes methods","convex programming;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;image texture;variational techniques","low spatial resolutions;high spatial resolutions;multispectral images;high spectral resolutions;hyperspectral images;weighted nonlocal vector total variation;hyperspectral fusion;quadratic data-fidelity terms;standard convex optimization framework;nonlocal neighborhoods;MS image;weighted norm;nonlocal weighted total-variation regularizer;repetitive structures;spectral resolutions;single image;HS-MS fusion;low spectral resolutions","","","","32","IEEE","17 Oct 2022","","","IEEE","IEEE Journals"
"Convolutional Sparse Representation of Injected Details for Pansharpening","R. Fei; J. Zhang; J. Liu; F. Du; P. Chang; J. Hu","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","25 Sep 2019","2019","16","10","1595","1599","In this letter, we address the pansharpening problem, which focuses on constructing a high-resolution (HR) multispectral (MS) image from a low-resolution (LR) MS and an HR panchromatic (Pan) image. The accuracy of pansharpening method based on sparse representation (SR) mainly depends on the construction of dictionary and the learning of sparse coefficients, while the details injection (DI)-based pansharpening method sharpens the MS bands by adding the proper spatial details from Pan. The combination of SR and DI has been put forward as the pansharpening method based on SR of injected details (SR-D). However, limited to the patch-based manner, pansharpening with traditional SR model faces two disadvantages, i.e., limited ability in detail preservation and high sensitivity to misregistration. In this letter, we replace the traditional SR model with convolutional SR (CSR) as a global SR model in the SR-D method and propose a new pansharpening method called CSR of injected details (CSR-D) to overcome the above-mentioned two drawbacks. Experimental results on the IKONOS and WorldView2 data sets show that the proposed method can achieve remarkable spectral and spatial quality on both reduced scale and full scale.","1558-0571","","10.1109/LGRS.2019.2904526","National Natural Science Foundation of China(grant numbers:61877049,61572393,11671317,11601415); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703107","Convolutional sparse representation (CSR);image fusion;panchromatic (Pan) data;pansharpening","Dictionaries;Sparse matrices;Bayes methods;Spatial resolution;Convolution;Mathematical model;Multiresolution analysis","hyperspectral imaging;image representation;image resolution","convolutional sparse representation;high-resolution multispectral image;HR panchromatic image;sparse coefficients;MS bands;global SR model;low-resolution MS image;WorldView2 data sets;IKONOS data sets;dictionary construction;pan sharpening problem","","28","","24","IEEE","30 Apr 2019","","","IEEE","IEEE Journals"
"Large-Scale Semantic 3-D Reconstruction: Outcome of the 2019 IEEE GRSS Data Fusion Contest—Part B","Y. Lian; T. Feng; J. Zhou; M. Jia; A. Li; Z. Wu; L. Jiao; M. Brown; G. Hager; N. Yokoya; R. Hänsch; B. L. Saux","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi’an, China; Johns Hopkins University Applied Physics Laboratory, Laurel, MD, USA; Johns Hopkins University, Baltimore, MD, USA; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; German Aerospace Center (DLR), Weßling, Germany; ESA/ESRIN, φ-Lab, Frascati, (RM), Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","1158","1170","We present the scientific outcomes of the 2019 Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The contest included challenges with large-scale datasets for semantic 3-D reconstruction from satellite images and also semantic 3-D point cloud classification from airborne LiDAR. 3-D reconstruction results are discussed separately in Part-A. In this Part-B, we report the results of the two best-performing approaches for 3-D point cloud classification. Both are deep learning methods that improve upon the PointSIFT model with mechanisms to combine multiscale features and task-specific postprocessing to refine model outputs.","2151-1535","","10.1109/JSTARS.2020.3035274","National Natural Science Foundation of China(grant numbers:61836009); National Natural Science Foundation of China(grant numbers:U1701267); National Natural Science Foundation of China(grant numbers:91438201); Intelligence Advanced Research Projects Activity(grant numbers:2017-17032700004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246669","Classification;convolutional neural networks;data fusion contest (DFC);deep learning;image analysis and data fusion;light detection and ranging (LiDAR);point cloud;semantic labeling;semantic mapping","Three-dimensional displays;Semantics;Laser radar;Training;Earth;Data integration;Satellites","geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);optical radar;terrain mapping","3-D point cloud classification;2019 IEEE GRSS data fusion contest;scientific outcomes;2019 data fusion contest;image analysis;large-scale datasets;satellite images;3-D reconstruction results;image analysis","","12","","41","CCBY","2 Nov 2020","","","IEEE","IEEE Journals"
"A MAP-Based Approach for Hyperspectral Imagery Super-Resolution","H. Irmak; G. B. Akar; S. E. Yuksel","Radar and Electronic Warfare Systems Business Sector (REHIS), ASELSAN Inc., Ankara, Turkey; Department of Electrical and Electronics Engineering, Middle East Techical University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Hacettepe University, Ankara, Turkey","IEEE Transactions on Image Processing","27 Mar 2018","2018","27","6","2942","2951","In this paper, we propose a novel single image Bayesian super-resolution (SR) algorithm where the hyperspectral image (HSI) is the only source of information. The main contribution of the proposed approach is to convert the ill-posed SR reconstruction problem in the spectral domain to a quadratic optimization problem in the abundance map domain. In order to do so, Markov random field based energy minimization approach is proposed and proved that the solution is quadratic. The proposed approach consists of five main steps. First, the number of endmembers in the scene is determined using virtual dimensionality. Second, the endmembers and their low resolution abundance maps are computed using simplex identification via the splitted augmented Lagrangian and fully constrained least squares algorithms. Third, high resolution (HR) abundance maps are obtained using our proposed maximum a posteriori based energy function. This energy function is minimized subject to smoothness, unity, and boundary constraints. Fourth, the HR abundance maps are further enhanced with texture preserving methods. Finally, HR HSI is reconstructed using the extracted endmembers and the enhanced abundance maps. The proposed method is tested on three real HSI data sets; namely the Cave, Harvard, and Hyperspectral Remote Sensing Scenes and compared with state-of-the-art alternative methods using peak signal to noise ratio, structural similarity, spectral angle mapper, and relative dimensionless global error in synthesis metrics. It is shown that the proposed method outperforms the state of the art methods in terms of quality while preserving the spectral consistency.","1941-0042","","10.1109/TIP.2018.2814210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8310636","Hyperspectral image;super-resolution reconstruction;MAP Framework;quadratic programming","Hyperspectral imaging;Image reconstruction;Minimization;Spatial resolution;Bayes methods;Dictionaries","Bayes methods;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;image resolution;Markov processes;maximum likelihood estimation;minimisation;remote sensing","abundance map domain;Markov random field;energy minimization approach;virtual dimensionality;low resolution abundance maps;splitted augmented Lagrangian constrained least squares algorithms;fully constrained least squares algorithms;high resolution abundance maps;maximum a posteriori based energy function;HR abundance maps;texture preserving methods;HR HSI;extracted endmembers;enhanced abundance maps;HSI data sets;spectral angle mapper;spectral consistency;Hyperspectral imagery super-resolution;novel single image Bayesian super-resolution algorithm;hyperspectral image;SR reconstruction problem;spectral domain;quadratic optimization problem;HSI","","37","","57","IEEE","9 Mar 2018","","","IEEE","IEEE Journals"
"Vehicle Tracking and Speed Estimation From Roadside Lidar","J. Zhang; W. Xiao; B. Coifman; J. P. Mills","School of Engineering, Newcastle University, Newcastle Upon Tyne, U.K.; School of Engineering, Newcastle University, Newcastle Upon Tyne, U.K.; Department of Civil, Environmental and Geodetic Engineering, Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA; School of Engineering, Newcastle University, Newcastle Upon Tyne, U.K.","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Oct 2020","2020","13","","5597","5608","Vehicle speed is a key variable for the calibration, validation, and improvement of traffic emission and air quality models. Lidar technologies have significant potential in vehicle tracking by scanning the surroundings in 3-D frequently, hence can be used as traffic flow monitoring sensors for accurate vehicle counting and speed estimation. However, the characteristics of lidar-based vehicle tracking and speed estimation, such as attainable accuracy, remain as open questions. This research therefore proposes a tracking framework from roadside lidar to detect and track vehicles with the aim of accurate vehicle speed estimation. Within this framework, on-road vehicles are first detected from the observed point clouds, after which a centroid-based tracking flow is implemented to obtain initial vehicle transformations. A tracker, utilizing the unscented Kalman Filter and joint probabilistic data association filter, is adopted in the tracking flow. Finally, vehicle tracking is refined through an image matching process to improve the accuracy of estimated vehicle speeds. The effectiveness of the proposed approach has been evaluated using lidar data obtained from two different panoramic 3-D lidar sensors, a RoboSense RS-LiDAR-32 and a Velodyne VLP-16, at a traffic light and a road intersection, respectively, in order to account for real-world scenarios. Validation against reference data obtained by a test vehicle equipped with accurate positioning systems shows that more than 94% of vehicles could be detected and tracked, with a mean speed accuracy of 0.22 m/s.","2151-1535","","10.1109/JSTARS.2020.3024921","UKCRIC - UK Collaboratorium for Research in Infrastructure & Cities: Newcastle Laboratories(grant numbers:EP/R010102/1); China Scholarship Council(grant numbers:201706370243); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200682","Smart city;3-D lidar;traffic monitoring;urban sensing;vehicle detection","Laser radar;Three-dimensional displays;Sensors;Vehicle detection;Estimation;Atmospheric modeling;Feature extraction","image fusion;image matching;Kalman filters;object detection;object tracking;optical radar;road traffic;road vehicles;target tracking;traffic engineering computing","roadside lidar;traffic emission;air quality models;lidar technologies;accurate vehicle counting;lidar-based vehicle tracking;accurate vehicle speed estimation;on-road vehicles;centroid-based tracking flow;initial vehicle transformations;estimated vehicle speeds;lidar data;different panoramic 3-D lidar sensors;RoboSense RS-LiDAR-32;mean speed accuracy","","28","","32","CCBY","18 Sep 2020","","","IEEE","IEEE Journals"
"Gaussian Pyramid Based Multiscale Feature Fusion for Hyperspectral Image Classification","S. Li; Q. Hao; X. Kang; J. A. Benediktsson","Hunan University, Changsha, Hunan, CN; Hunan University, Changsha, Hunan, CN; Hunan University, Changsha, Hunan, CN; Haskoli Islands, Reykjavik, IS","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Sep 2018","2018","11","9","3312","3324","In this paper, we propose a segmented principal component analysis (SPCA) and Gaussian pyramid decomposition based multiscale feature fusion method for the classification of hyperspectral images. First, considering the band-to-band cross correlations of objects, the SPCA method is utilized for the spectral dimension reduction of the hyperspectral image. Then, the dimension-reduced image is decomposed into several Gaussian pyramids to extract the multiscale features. Next, the SPCA method is performed again to compute the fused SPCA based Gaussian pyramid features (SPCA-GPs). Finally, the performance of the SPCA-GPs is evaluated using the support vector machine classifier. Experiments performed on three widely used hyperspectral images show that the proposed SPCA-GPs method outperforms several compared classification methods in terms of classification accuracies and computational cost.","2151-1535","","10.1109/JSTARS.2018.2856741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424168","Feature fusion;Gaussian pyramid;hyperspectral image classification;principal component analysis (PCA)","Hyperspectral imaging;Principal component analysis;Feature extraction;Dimensionality reduction;Covariance matrices","feature extraction;Gaussian processes;image classification;image fusion;image segmentation;principal component analysis;support vector machines","segmented principal component analysis;Gaussian pyramid decomposition;band-to-band cross correlations;hyperspectral image;multiscale feature extraction;support vector machine classifier;SPCA-GPs method;fused SPCA based Gaussian pyramid features;multiscale features;dimension-reduced image;spectral dimension reduction;SPCA method;decomposition based multiscale feature fusion method;hyperspectral image classification","","50","","52","IEEE","1 Aug 2018","","","IEEE","IEEE Journals"
"Deep Collaborative Attention Network for Hyperspectral Image Classification by Combining 2-D CNN and 3-D CNN","H. Guo; J. Liu; J. Yang; Z. Xiao; Z. Wu","Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; Jiangsu Provincial Engineering Laboratory for Pattern Recognition and Computational Intelligence, Jiangnan University, Wuxi, China; School of Computer Science, Nanjing University of Science and Technology, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","28 Aug 2020","2020","13","","4789","4802","Deep learning-based methods based on convolutional neural networks (CNNs) have demonstrated remarkable performance in hyperspectral image (HSI) classification. Most of these approaches are only based on 2-D CNN or 3-D CNN. It is dramatic from the literature that using just 2-D CNN may result in missing channel relationship information, and using just 3-D CNN may make the model very complex. Moreover, the existing network models do not pay enough attention to extracting spectral-spatial correlation information. To address these issues, we propose a deep collaborative attention network for HSI classification by combining 2-D CNN, and 3-D CNN (CACNN). Specifically, we first extract spectral-spatial features by using 2-D CNN, and 3-D CNN, respectively, and then use a “NonLocalBlock” to combine these two kinds of features. This block serves as a typical spatial attention mechanism, and makes salient features be emphasized. Then, we propose a “Conv_Block” that is similar to the lightweight dense block to extract correlation information contained in the feature maps. Finally, we consider a deep multilayer feature fusion strategy, and thereby combine the features of different hierarchical layers to extract the strong correlated spectral-spatial information among them. To test the performance of CACNN approach, several experiments are performed on four well-known HSIs. The results are compared with the state-of-the-art approaches, and satisfactory performance is obtained by our proposed method. The code of CACNN method is available on Dr. J. Liu's GitHub.","2151-1535","","10.1109/JSTARS.2020.3016739","National Natural Science Foundation of China(grant numbers:61601201); Natural Science Foundation of Jiangsu Province(grant numbers:BK20160188); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9167434","Convolutional neural network (CNN);feature extraction;hyperspectral image classification;multilayer feature fusion;spatial attention mechanism","Feature extraction;Data mining;Correlation;Hyperspectral imaging;Solid modeling;Principal component analysis","convolutional neural nets;correlation methods;feature extraction;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);solid modelling;spectral analysis","deep multilayer feature fusion strategy;Conv_Block;HSI classification;spectral-spatial correlation information;convolutional neural networks;2-D CNN;hyperspectral image classification;deep learning-based methods;deep collaborative attention network;3-D CNN","","28","","64","CCBY","14 Aug 2020","","","IEEE","IEEE Journals"
"Robust Human Targets Tracking for MIMO Through-Wall Radar via Multi-Algorithm Fusion","H. Li; G. Cui; L. Kong; G. Chen; M. Wang; S. Guo","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Apr 2019","2019","12","4","1154","1164","The detection and tracking of human targets behind the wall is of great importance in urban sensing. The random and high maneuvering behaviors of moving human targets and the clutter diversity lead to high missed detection and false-alarm probability. In this paper, we consider two-dimensional human-target tracking problem, and a multi-algorithm fusion (MAF) framework exploiting the mean-shift algorithm and the Kalman filter is proposed. Compared with the mean-shift algorithm, the proposed MAF framework has robust tracking performance, especially in the presence of multiple targets. Finally, the proposed MAF framework is evaluated by simulations and real data.","2151-1535","","10.1109/JSTARS.2019.2901262","National Natural Science Foundation of China(grant numbers:61871080,61771109); ChangJiang Scholar Program; 111 Project(grant numbers:B17008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8671685","Moving human-target tracking;multi-algorithm fusion (MAF);through-wall radar","Target tracking;Radar tracking;Trajectory;Kalman filters;MIMO communication;Radar imaging","image fusion;Kalman filters;MIMO radar;probability;radar imaging;radar tracking;target tracking","robust human targets tracking;MIMO through-wall radar;urban sensing;random maneuvering behaviors;high maneuvering behaviors;high missed detection;false-alarm probability;human-target tracking problem;multialgorithm fusion framework;MAF framework;robust tracking performance;multiple targets;human target detection;clutter diversity;Kalman filter","","24","1","34","IEEE","20 Mar 2019","","","IEEE","IEEE Journals"
"Information Fusion for Urban Road Extraction From VHR Optical Satellite Images","Z. Miao; W. Shi; A. Samat; G. Lisini; P. Gamba","Department of Land Surveying and Geo-Informatics, Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Land Surveying and Geo-Informatics, Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Geographic Information Science, Nanjing University, Nanjing, China; Department of Industrial and Information Engineering, University of Pavia, Pavia, Italy; Department of Industrial and Information Engineering, University of Pavia, Pavia, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","5","1817","1829","This paper presents a novel method exploiting fusion at the information level for urban road extraction from very high resolution (VHR) optical satellite images. Given a satellite image, we explore spectral and shape features computed at the pixel level, and use them to select road segments using two different methods (i.e., expectation maximization clustering and linearness filtering). A road centerline extraction method, which is relying on the outlier robust regression, is subsequently applied to extract accurate centerlines from road segments. After that, three different sets of information fusion rules are applied to jointly exploit results from these methods, which offer ways to address their own limitations. Two VHR optical satellite images are used to validate the proposed method. Quantitative results prove that information fusion following centerline extraction by multiple techniques is able to produce the best accuracy values for automatic urban road extraction from VHR optical satellite images.","2151-1535","","10.1109/JSTARS.2015.2498663","National Natural Science Foundation of China(grant numbers:41201451,40901214); Ministry of Science and Technology of China(grant numbers:2012BAJ15B04,2012AA12A305); National Administration of Surveying, Mapping, and Geoinformation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7337372","Centerline;expectation maximization (EM);information fusion;linearness filter;RANdom SAmple Consensus (RANSAC);Centerline;expectation maximization (EM);information fusion;linearness filter;RANdom SAmple Consensus (RANSAC)","Roads;Feature extraction;Satellites;Data mining;Optical imaging;Image segmentation;Active contours","feature extraction;geophysical image processing;geophysical techniques;image fusion","information fusion;VHR optical satellite images;pixel level;road segments;road centerline extraction method;outlier robust regression;automatic urban road extraction","","18","","75","IEEE","25 Nov 2015","","","IEEE","IEEE Journals"
"Densely Connected Multiscale Attention Network for Hyperspectral Image Classification","H. Gao; Y. Miao; X. Cao; C. Li","College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Feb 2021","2021","14","","2563","2576","Hyperspectral images (HSIs) are characterized by high spatial resolution and are rich in spectral information. In the process of HSI classification, the extraction of spectral-spatial features directly influences the classification results. In recent years, the hyperspectral classification method based on convolutional neural networks has demonstrated excellent performance. However, as the network structure deepens, degradation occurs, and the features learned from the fixed-scale convolutional kernels are usually specific, which is not conducive to feature learning and thus impairs the classification accuracy. To solve the problem of difficult extraction of features and underutilization of information from HSI data, a densely connected multiscale attention network based on 3-D convolution is proposed for HSI classification. First, to reduce the spectral redundancy of the HSIs, the principal component analysis algorithm is performed on the raw HSI data; then, several multiscale blocks comprised of parallel factorized spatial-spectral convolution modules of different sizes are adopted to extract the enriched spectral-spatial features from HSIs; furthermore, dense connections are introduced to further fuse features obtained from blocks of different depths, thereby enhancing feature reuse and propagation and helping to alleviate the problem of vanishing gradients. Besides, the channel-spectral-spatial attention block is put forward to spontaneously reweight the fused features to emphasize the features that are more relevant to the classification results while weakening the less relevant ones. The experimental results show that the proposed method is effective in extracting discriminative features of the target and outperforms the other state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2021.3056124","National Natural Science Foundation of China(grant numbers:62071168,61701166); National Key Research and Development Program of China(grant numbers:2018YFC1508106); Fundamental Research Funds for the Central Universities(grant numbers:B200202183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345335","Attention mechanism;convolutional neural network (CNN);dense connectivity;hyperspectral image (HSI) classification;multiscale features","Feature extraction;Data mining;Convolution;Task analysis;Hyperspectral imaging;Three-dimensional displays;Computational modeling","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);principal component analysis","fixed-scale convolutional kernels;densely connected multiscale attention network;spectral redundancy;raw HSI data;multiscale blocks;parallel factorized spatial-spectral convolution modules;feature reuse;channel-spectral-spatial attention block;fused features;discriminative features;hyperspectral image classification;high spatial resolution;spectral information;convolutional neural networks;principal component analysis algorithm","","14","","55","CCBY","2 Feb 2021","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and LiDAR Data Using Discriminant Correlation Analysis for Land Cover Classification","F. Jahan; J. Zhou; M. Awrangjeb; Y. Gao","Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia; Institute of Integrated and Intelligent Systems, Griffith University, Nathan, QLD, Australia","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Oct 2018","2018","11","10","3905","3917","It is evident that using complementary features from different sensors is effective for land cover classification. Therefore, combining complementary information from hyperspectral (HS) and light detection and ranging (LiDAR) data can greatly assist in such applications. In this paper, we propose a model for land cover classification, which extracts effective features representing different characteristics (e.g., spectral, geometrical/structural) of objects of interest from these two complementary data sources (e.g., HS and LiDAR) and fuse them effectively by incorporating dimensionality reduction technique. The HS bands are first grouped based on their joint entropy and structural similarity for group-wise spatial feature extraction. The spectral and spatial features from HS are then fused in parallel via discriminant correlation analysis (DCA) method for each band group. This is followed by a multisource fusion step between the spatial features extracted from HS and LiDAR data using DCA. The resultant features from both band-group fusion and multisource fusion steps are concatenated with several other features extracted from HS and LiDAR data. In the proposed model, DCA fusion produces discriminative features by eliminating between-class correlations and confining within-class correlations. We compare the performance of our feature extraction and fusion scheme using random forest and support vector machine classifiers. We also compare our approach with several state-of-the-art approaches on two benchmark land cover datasets and show that our approach outperforms the alternatives by a large margin.","2151-1535","","10.1109/JSTARS.2018.2868142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8464243","discriminant correlation analysis (DCA);fusion;hyperspectral (HS);light detection and ranging (LiDAR);multisource","Feature extraction;Laser radar;Data mining;Correlation;Manganese;Entropy;Hyperspectral sensors","entropy;feature extraction;image classification;image fusion;optical radar;pattern classification;support vector machines","LiDAR data;land cover classification;complementary information;light detection;group-wise spatial feature extraction;discriminant correlation analysis method;multisource fusion step;band-group fusion;DCA fusion;fusion scheme;benchmark land cover datasets","","13","","28","IEEE","13 Sep 2018","","","IEEE","IEEE Journals"
"Hyperspectral Image Super Resolution Based on Multiscale Feature Fusion and Aggregation Network With 3-D Convolution","J. Hu; Y. Tang; S. Fan","School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; Key Laboratory of Electric Power Robot of Hunan Province, Changsha University of Science and Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Sep 2020","2020","13","","5180","5193","The spectral resolution of hyperspectral images (HSIs) is very high. Nevertheless, their spatial resolution is low due to various hardware limitations. Therefore, it is important to study HSI super resolution to improve their spatial resolution. In this article, for hyperspectral single-image super resolution, we propose a multiscale feature fusion and aggregation network with 3-D convolution (MFFA-3D) by cascading the MFFA-3D block. The MFFA-3D block includes a group multiscale feature fusion part and a multiscale feature aggregation part. In group multiscale feature fusion part, a novel group multiscale feature fusion method is proposed. Group feature fusion module and two-step multiscale module are proposed in multiscale feature aggregation part. In order to prevent spectral distortion, a spectral gradient loss function is proposed and combined with the mean square error loss function to form the final loss function. Since the proposed super-resolution (SR) network is a full 3-D convolutional network, our method can perform direct super-resolution transfer even if the number of the bands of test images is different from that of the training images. The experiments over simulated and real HSIs demonstrate the superiority of the proposed method in terms of qualitative and quantitative evaluation.","2151-1535","","10.1109/JSTARS.2020.3020890","National Natural Science Foundation of China(grant numbers:61601061,61971071); Scientific Research Fund of Hunan Provincial Education Department(grant numbers:14B006); Open Research Fund of Key Laboratory of Electric Power Robot of Hunan Province(grant numbers:PROF1902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184151","3-D convolutional neural network;group;hyperspectral image (HSI);multiscale feature fusion;super resolution (SR)","Three-dimensional displays;Spatial resolution;Convolution;Interpolation;Image reconstruction;Hyperspectral sensors","convolutional neural nets;feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);mean square error methods","mean square error loss function;multiscale feature aggregation part;group multiscale feature fusion part;MFFA-3D block;hyperspectral single-image super resolution;HSI super resolution;spatial resolution;spectral resolution;aggregation network;direct super-resolution transfer;full 3-D convolutional network;spectral gradient loss function;two-step multiscale module","","10","","33","CCBY","1 Sep 2020","","","IEEE","IEEE Journals"
"Refined Pan-Sharpening With NSCT and Hierarchical Sparse Autoencoder","H. Li; F. Liu; S. Yang; K. Zhang; X. Su; L. Jiao","School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","12","5715","5725","Most of available pan-sharpening technologies suffer from spectral and spatial distortions, for the coarse extraction from Panchromatic (Pan) image and brute injection of details to multispectral (MS) images. In this paper, in order to reduce the color distortion and enhance the spatial information of fused images, we propose a refined pan-sharpening (RPS) method using geometric multiscale analysis (GMA) and hierarchical sparse autoencoder (HSAE). First, a GMA tool, nonsubsampled contourlet transform (NSCT), is used to capture directional details of the Pan image at multiple scales. Then at each scale, HSAE is developed to gradually filter out the refined spatial details, via sparsely coding details under spatial self-dictionaries. The refined details are then injected into MS images to alleviate spectral distortions. By exploring the spatial structure in images and refining the spatial details injection via HSAE, RPS can reduce distortions to present fidelity colors and sharp appearance. Some experiments are taken on several datasets collected by QuickBird, Geoeye, and IKONOS satellites, and the experimental results show that RPS can reduce distortions in both the spectral and spatial domains, and outperform some related methods in terms of both visual results and numerical guidelines.","2151-1535","","10.1109/JSTARS.2016.2584142","National Basic Research Program (973 Program) of China(grant numbers:2013CB329402); National Natural Science Foundation of China(grant numbers:61573267,61173090); Fund for Foreign Scholars in University Research and Teaching Programs(grant numbers:B07048); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103); Fundamental Research Funds for the Central Universities(grant numbers:JB140317,BDY021429); Science Basic Research Program in the Shaanxi Province of China(grant numbers:16JK1823); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523328","Hierarchical sparse autoencoder (HSAE);nonsubsampled contourlet transform (NSCT);refined details;refined pan-sharpening (RPS);spectral distortion","Distortion;Image restoration;Multispectral imaging;Spatial resolution","image coding;image colour analysis;image enhancement;image filtering;image fusion;image sampling;numerical analysis;transforms","image filtering;IKONOS satellite;Geoeye satellite;QuickBird satellite;nonsubsampled contourlet transform;HSAE;hierarchical sparse autoencoder;GMA;geometric multiscale analysis;RPS method;refined pan-sharpening method;fused imaging;color distortion reduction;MS imaging;multispectral imaging;Pan imaging;coarse extraction;NSCT","","10","","39","IEEE","27 Jul 2016","","","IEEE","IEEE Journals"
"Attention Multisource Fusion-Based Deep Few-Shot Learning for Hyperspectral Image Classification","X. Liang; Y. Zhang; J. Zhang","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","17 Sep 2021","2021","14","","8773","8788","Recently, deep learning-based methods outperform others in hyperspectral image (HSI) classification. However, the deep learning methods require sufficient labeled samples to improve performance, which is unfeasible in practice. The training labels are usually limited in HSIs that need to be classified (namely target domain), while other available labels in multisource HSIs (namely source domain) are not utilized effectively. To mitigate these issues, an attention multisource fusion method of few-shot learning (AMF-FSL) is proposed for small-sized HSI classification. AMF-FSL is an implementation of few-shot learning (FSL) in the meta-learning field, which can transfer the learned ability of classification from multiple source data to target data. The process of learning to classify in AMF-FSL is not restricted by the traditional requirement of the same distribution between the source and target domains, which can learn from the source domain and apply it to a different distribution in the target domain. Moreover, the multisource domain adaption in AMF-FSL has the capacity of extracting features from fused homogeneous and heterogeneous data in the source domain, which can improve the generalization of the classification model in the cross domains. Specifically, the multisource domain adaption contains three modules, namely the target-based class alignment, domain attention assignment, and multisource data fusion, which are responsible for aligning the class space, paying band-level attention, and merging the distributions of homogeneous and heterogeneous data in the source domain. The experimental results demonstrate the effectiveness of the multisource domain adaption and the superiority of AMF-FSL over other state-of-the-art methods in small-sized HSI classification.","2151-1535","","10.1109/JSTARS.2021.3109012","National Natural Science Foundation of China(grant numbers:61871150,61771170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527083","Domain band attention;few-shot learning;hyperspectral image (HSI) classification;multisource fusion","Feature extraction;Adaptation models;Data models;Classification algorithms;Training;Data mining;Data integration","deep learning (artificial intelligence);feature extraction;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence)","attention multisource fusion method;few-shot learning;AMF-FSL;small-sized HSI classification;meta-learning field;multisource domain adaption;fused homogeneous data;heterogeneous data;classification model;target-based class alignment;domain attention assignment;multisource data fusion;hyperspectral image classification;deep learning-based methods;deep learning methods;multisource HSIs;source domain","","10","","56","CCBYNCND","1 Sep 2021","","","IEEE","IEEE Journals"
"Hyperspectral Classification via Global-Local Hierarchical Weighting Fusion Network","B. Tu; W. He; W. He; X. Ou; A. Plaza","School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politecnica, University of Extremadura, Caceres, Spain","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Dec 2021","2022","15","","184","200","The fusion of spectral–spatial features based on deep learning has become the focus of research in hyperspectral image (HSI) classification. However, previous deep frameworks based on spectral–spatial fusion usually performed feature aggregation only at the branch ends. Furthermore, only first-order statistical features are considered in the fusion process, which is not conducive to improving the discrimination of spectral–spatial features. This article proposes a global–local hierarchical weighted fusion end-to-end classification architecture. The architecture includes two subnetworks for spectral classification and spatial classification. For the spectral subnetwork, two band-grouping strategies are designed, and bidirectional long short-term memory is used to capture spectral context information from global to local perspectives. For the spatial subnetwork, a pooling strategy based on local attention is combined to construct a global–local pooling fusion module to enhance the discriminability of spatial features learned by a convolutional neural network. For the fusion stage, a hierarchical weighting fusion mechanism is developed to obtain the nonlinear relationship between both spectral and spatial features. The experimental results on four real HSI datasets and a GF-5 satellite dataset demonstrate that the method proposed is more competitive in terms of accuracy and generalization.","2151-1535","","10.1109/JSTARS.2021.3133009","National Natural Science Foundation of China(grant numbers:61977022); Science Foundation for Distinguished Young Scholars of Hunan Province(grant numbers:2020JJ2017); Key Research and Development Program of Hunan Province(grant numbers:2019SK2012); Foundation of Department of Water Resources of Hunan Province(grant numbers:XSKJ2021000-12,XSKJ2021000-13); Natural Science Foundation of Hunan Province(grant numbers:2020JJ4340,2021JJ40226); Foundation of Education Bureau of Hunan Province(grant numbers:20B257,20B266); Scientific Research Fund of Education Department of Hunan Province(grant numbers:19A200); Open Fund of Education Department of Hunan Province(grant numbers:20K062); Postgraduate Scientific Research Innovation Project of Hunan Province(grant numbers:QL20210254); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645230","Band grouping;deep learning (DL);features fusion;global–local;hyperspectral image (HSI)","Feature extraction;Logic gates;Hyperspectral imaging;Task analysis;Convolutional neural networks;Training;Earth","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);neural nets","fusion process;spectral-spatial features;global-local hierarchical weighted fusion end-to-end classification architecture;spectral classification;spatial classification;spectral subnetwork;spectral context information;local perspectives;spatial subnetwork;local attention;global-local pooling fusion module;fusion stage;hierarchical weighting fusion mechanism;hyperspectral classification;global-local hierarchical weighting fusion network;deep learning;hyperspectral image classification;previous deep frameworks;spectral-spatial fusion;feature aggregation;first-order statistical features","","10","","56","CCBY","9 Dec 2021","","","IEEE","IEEE Journals"
"Poissonian Hyperspectral Image Superresolution Using Alternating Direction Optimization","C. Zou; Y. Xia","Center for Discrete Mathematics and Theoretical Computer Science, College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","20 May 2017","2016","9","9","4464","4479","The reconstruction of Poissonian image is an active research area in recent years. This paper proposes a novel method for Poissonian hyperspectral image superresolution by fusing a low-spatial-resolution hyperspectral image and a low-spectral-resolution multispectral image. The fusion scheme is designed as an optimization problem, whose cost function consists of the two data-fidelity terms about Poisson distribution, the sparse representation term, and the nonlocal regularization term. The two data-fidelity terms can capture statistical information of Poisson noise. The sparse representation term is used for enhancing the quality of sparsity-based signal reconstruction, and the nonlocal regularization term exploits the spatial similarity of hyperspectral image. As a result, the hyperspectral image and multispectral image are well fused. Finally, the designed optimization problem is effectively solved by an alternating direction optimization algorithm. Simulation results illustrate that the proposed method has a better performance than several well-known methods both in terms of quality indexes and reconstruction visual effect.","2151-1535","","10.1109/JSTARS.2016.2585158","National Natural Science Foundation of China(grant numbers:61473330,61401098); Fujian Provincial Department of Education(grant numbers:JA15078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7513429","Alternating direction optimization;hyperspectral (HS) image;multispectral (MS) image;nonlocal regularization;poisson noise;sparse representation","Spatial resolution;Optimization;Hyperspectral imaging;Image reconstruction;Signal resolution","geophysical image processing;hyperspectral imaging;image fusion;image reconstruction","reconstruction visual effect;quality indexes;sparsity-based signal reconstruction quality;Poisson noise statistical information;data-fidelity terms;nonlocal regularization term;Poisson distribution;low-spectral-resolution multispectral image;low-spatial-resolution hyperspectral image;Poissonian hyperspectral image superresolution;active research area;Poissonian image reconstruction;alternating direction optimization;poissonian hyperspectral image superresolution","","10","","62","IEEE","14 Jul 2016","","","IEEE","IEEE Journals"
"Forward-Looking Geometric Configuration Optimization Design for Spaceborne-Airborne Multistatic Synthetic Aperture Radar","D. Mao; Y. Zhang; J. Pei; W. Huo; Y. Zhang; Y. Huang; J. Yang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Yangtze Delta Region, Quzhou, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Aug 2021","2021","14","","8033","8047","Spaceborne-airborne multistatic synthetic aperture radar (SA-MuSAR) has the ability to provide high-resolution forward-looking imagery for receivers, but it relies on careful design of the geometric configuration (GC). In this article, a forward-looking GC optimization design method is proposed to obtain a high-quality fused image with limited observation time. First, the relationship between the spatial resolution and GC is illustrated by the wavenumber spectrum distribution of SA-MuSAR. Second, GC evaluators depending on the distribution of multiple wavenumber spectrum data are proposed. The GC design problem of coherent SA-MuSAR is transformed into a constrained multiobjective optimization problem. An intelligent evolutionary algorithm is adopted to optimize the wavenumber spectrum distribution. With the proposed method, high-quality forward-looking imagery can be obtained with a short observation time. Numerical simulations are carried out to verify the effectiveness of the proposed method.","2151-1535","","10.1109/JSTARS.2021.3103802","National Natural Science Foundation of China(grant numbers:61901092,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511248","Forward-looking geometric configuration (GC) design;spaceborne-airborne multistatic synthetic aperture radar (SA-MuSAR);wavenumber spectrum distribution","Receivers;Spatial resolution;Transmitters;Synthetic aperture radar;Spaceborne radar;Brain modeling;Optimization methods","coherence;evolutionary computation;image fusion;image resolution;optimisation;radar imaging;spaceborne radar;synthetic aperture radar","forward-looking geometric configuration optimization design;spaceborne-airborne multistatic synthetic aperture radar;SA-MuSAR;GC optimization design method;high-quality fused image;wavenumber spectrum distribution;GC design problem;constrained multiobjective optimization problem","","8","","53","CCBY","10 Aug 2021","","","IEEE","IEEE Journals"
"EMFNet: Enhanced Multisource Fusion Network for Land Cover Classification","C. Li; R. Hang; B. Rasti","Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology, Nanjing University of Information Science and Technology, Nanjing, China; Helmholtz-Zentrum Dresden-Rossendorf, Helmholtz Institute Freiberg for Resource Technology, Freiberg, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 May 2021","2021","14","","4381","4389","Feature extraction and fusion are two critical issues for the task of multisource classification. In this article, we propose an enhanced multisource fusion network (EMFNet) to address them in an end-to-end framework. Specifically, two convolutional neural networks are employed to extract features from two different sources. Each network is mainly comprised of three convolutional layers. For each convolutional layer, feature tuning modules are designed to enhance the extracted feature of one source by taking advantage of the other source. After getting the features of two sources, a weighted summation method is used to fuse them. Considering that fusion weights should vary for different inputs, a feature fusion module is designed to achieve this goal. In order to test the performance of our proposed EMFNet, we compare it with state-of-the-art fusion models, including the traditional models and the deep-learning-based models, on two real datasets. Experimental results show that the EMFNet can achieve competitive classification results in comparison with them.","2151-1535","","10.1109/JSTARS.2021.3073719","National Natural Science Foundation of China(grant numbers:61906096,61802199); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180786); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406339","Convolutional neural network (CNN);feature fusion module;feature tuning module;multisource fusion","Feature extraction;Tuning;Hyperspectral imaging;Laser radar;Data mining;Training;Roads","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;land cover;learning (artificial intelligence);neural nets","EMFNet;enhanced multisource fusion network;land cover classification;multisource classification;end-to-end framework;convolutional neural networks;convolutional layer;feature tuning modules;fusion weights;feature fusion module","","7","","33","CCBY","16 Apr 2021","","","IEEE","IEEE Journals"
"SAR Ship Detection Based on End-to-End Morphological Feature Pyramid Network","C. Zhao; X. Fu; J. Dong; R. Qin; J. Chang; P. Lang","School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China; School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China; Shijiazhuang Campus, Army Engineering University, Qadirabad, Bangladesh; School of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Nan'An, China; School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China; School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Jun 2022","2022","15","","4599","4611","Intelligent ship detection based on high-precision synthetic aperture radar (SAR) images plays a vital role in ocean monitoring and maritime management. Denoising is an effective preprocessing step for target detection. Morphological network-based denoising can effectively remove speckle noise, while the smoothing effect of which blurs the edges of the image and reduces the detection accuracy. The fusion of edge extraction and morphological network can improve detection accuracy by compensating for the lack of edge information caused by smoothing. This article proposes an end-to-end lightweight network called morphological feature-pyramid Yolo v4-tiny for SAR ship detection. First, a morphological network is introduced to preprocess the SAR images for speckle noise suppression and edge enhancement, providing spatial high-frequency information for target detection. Then, the original and preprocessed images are combined into the multichannel as an input for the convolution layer of the network. The feature pyramid fusion structure is used to extract the high-level semantic features and shallow detailed features from the image, improving the performance of multiscale target detection. Experiments on the public SAR ship detection dataset and AIR SARShip-1.0 show that the proposed method performs better than the other convolution neural network-based methods.","2151-1535","","10.1109/JSTARS.2022.3150910","111 Project of China(grant numbers:B14010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716855","Convolution neural network (CNN);feature pyramid fusion;morphological network;synthetic aperture radar (SAR) target detection","Feature extraction;Marine vehicles;Image edge detection;Object detection;Kernel;Radar polarimetry;Synthetic aperture radar","edge detection;feature extraction;geophysical image processing;image denoising;image fusion;neural nets;object detection;radar imaging;ships;speckle;synthetic aperture radar","effective preprocessing step;morphological network-based denoising;smoothing effect;detection accuracy;edge extraction;edge information;end-to-end lightweight network;morphological feature-pyramid Yolo v;SAR images;speckle noise suppression;edge enhancement;high-frequency information;original images;preprocessed images;feature pyramid fusion structure;high-level semantic features;shallow detailed features;multiscale target detection;public SAR ship detection dataset;convolution neural network-based methods;end-to-end morphological feature pyramid network;intelligent ship detection;high-precision synthetic aperture radar images","","5","","45","CCBY","18 Feb 2022","","","IEEE","IEEE Journals"
"Feature Extraction Using Multidimensional Spectral Regression Whitening for Hyperspectral Image Classification","B. Tu; Q. Ren; C. Zhou; S. Chen; W. He","Guangxi Key Laboratory of Cryptography and Information Security, Guilin University of Electronic Technology, Guilin, China; School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China; School of Information Science and Technology, Hunan Institute of Science and Technology, Yueyang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Sep 2021","2021","14","","8326","8340","Hyperspectral images (HSIs) consist of hundreds of spectral bands, which can be used to precisely characterize different land cover types. However, an HSI has redundant information and is prone to the “dimensionality curse.” Therefore, it is necessary to reduce redundant information through dimensionality reduction (DR), given that different dimensions contain unique primary feature information, and the feature information is complementary. Accordingly, a new feature extraction method based on multidimensional spectral regression whitening (M-SRW) is proposed, which reduces HSI to different dimensions and reconstructs it for feature extraction. The proposed method consists of the following steps: First, the original HSI is superpixel segmented by the entropy rate segmentation algorithm. Second, SRW is performed in each superpixel block to reduce the dimension of each superpixel block to a different dimension. Third, superpixel blocks of the same dimension are combined to obtain the reconstructed HSI. Finally, the support vector machine is utilized to classify the reconstructed HSI of different dimensions, and majority voting decision fusion is used to obtain the final classification result map. Experiments on three public hyperspectral data sets demonstrated that the proposed M-SRW method is superior to several state-of-the-art feature extraction approaches in terms of classification accuracy.","2151-1535","","10.1109/JSTARS.2021.3104153","National Natural Science Foundation of China(grant numbers:61977022); Science Foundation for Distinguished Young Scholars of Hunan Province(grant numbers:2020JJ2017); Natural Science Foundation of Hunan Province(grant numbers:2019JJ50211,2019JJ50212,2020JJ4340,2021JJ40226); Key Research and Development Program of Hunan Province(grant numbers:2019SK2012); Foundation of Education Bureau of Hunan Province(grant numbers:19B245,19B237,20B257); Engineering Research Center on 3-D Reconstruction; Intelligent Application Technology of Hunan Emergency Communication Engineering Technology Research Center(grant numbers:2019-430602-73-03-006049); Hunan Province Emergency Communication Engineering Technology Research Center(grant numbers:2018TP2022); Guangxi Key Laboratory of Cryptography and Information Security(grant numbers:GCIS201911); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511804","Hyperspectral image (HSI) classification;manifold learning;multidimensional spectral regression whitening (M-SRW);unsupervised dimensionality reduction (DR)","Feature extraction;Support vector machines;Image segmentation;Principal component analysis;Manifolds;Hyperspectral imaging;Erbium","feature extraction;hyperspectral imaging;image classification;image fusion;image segmentation;regression analysis;support vector machines","multidimensional spectral regression whitening;hyperspectral image classification;spectral bands;land cover types;redundant information;dimensionality curse;dimensionality reduction;feature extraction method;original HSI;entropy rate segmentation algorithm;superpixel block;reconstructed HSI;public hyperspectral data sets;primary feature information","","4","","52","CCBY","11 Aug 2021","","","IEEE","IEEE Journals"
"Hyperspectral Image Superresolution Using Global Gradient Sparse and Nonlocal Low-Rank Tensor Decomposition With Hyper-Laplacian Prior","Y. Peng; W. Li; X. Luo; J. Du","Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China; Chongqing Engineering Research Center for Spatial Big Data Intelligent Technology, Chongqing Institute of Meteorological Science, Chongqing, China; School of Computer Science, and Cyber Engineering, Guangzhou University, Guangzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","10 Jun 2021","2021","14","","5453","5469","This article presents a novel global gradient sparse and nonlocal low-rank tensor decomposition model with a hyper-Laplacian prior for hyperspectral image (HSI) superresolution to produce a high-resolution HSI (HR-HSI) by fusing a low-resolution HSI (LR-HSI) with an HR multispectral image (HR-MSI). Inspired by the investigated hyper-Laplacian distribution of the gradients of the difference images between the upsampled LR-HSI and latent HR-HSI, we formulate the relationship between these two datasets as a lp (0 <; p <; 1)-norm term to enforce spectral preservation. Then, the relationship between the HR-MSI and latent HR-HSI is built using a tensor-based fidelity term to recover the spatial details. To effectively capture the high spatio-spectral-nonlocal similarities of the latent HR-HSI, we design a novel nonlocal low-rank Tucker decomposition to model the 3-D regular tensors constructed from the grouped nonlocal similar HR-HSI cubes. The global spatial-spectral total variation regularization is then adopted to ensure the global spatial piecewise smoothness and spectral consistency of the reconstructed HR-HSI from nonlocal low-rank cubes. Finally, an alternating direction method of multipliers-based algorithm is designed to efficiently solve the optimization problem. Experiments on both the synthetic and real datasets collected by different sensors show the effectiveness of the proposed method, from visual and quantitative assessments.","2151-1535","","10.1109/JSTARS.2021.3076170","Chongqing Graduate Student Scientific Research Innovation Project(grant numbers:CYB19174); Chongqing University of Posts and Telecommunications(grant numbers:BYJS201810); National Natural Science Foundation of China(grant numbers:61972060,U1713213,62027827,41871226); Natural Science Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0025,cstc2019cxcyljrc-td0270); National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2019YFE0110800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9417623","Global gradient sparse;hyper-Laplacian;hyperspectral image;nonlocal low-rank;superresolution;total variation;tucker decomposition","Tensors;Superresolution;Sparse matrices;Spatial resolution;Pansharpening;Telecommunications;Hyperspectral imaging","geophysical image processing;gradient methods;hyperspectral imaging;image classification;image fusion;image reconstruction;image representation;image resolution;iterative methods;optimisation;tensors","hyperspectral image superresolution;global gradient sparse;nonlocal low-rank tensor decomposition model;high-resolution HSI;low-resolution HSI;HR multispectral image;HR-MSI;hyper-Laplacian distribution;difference images;HR-HSI;tensor-based fidelity term;high spatio-spectral-nonlocal similarities;3-D regular tensors;grouped nonlocal similar HR-HSI cubes;spatial-spectral total variation regularization;global spatial piecewise smoothness;low-rank cubes;LR-HSI;nonlocal low-rank Tucker decomposition;optimization","","4","","66","CCBY","28 Apr 2021","","","IEEE","IEEE Journals"
"Small Vessel Detection Based on Adaptive Dual-Polarimetric Feature Fusion and Sea–Land Segmentation in SAR Images","Y. Zhou; F. Zhang; F. Ma; D. Xiang; F. Zhang","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Interdisciplinary Research Center for Artificial Intelligence, Beijing University of Chemical Technology, Beijing, China; Interdisciplinary Research Center for Artificial Intelligence, Beijing University of Chemical Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Apr 2022","2022","15","","2519","2534","Detection of small sea vessels in synthetic aperture radar (SAR) images has received much attention in recent years because the small vessels have weak scattering intensity and few image pixels. The existing detection network structures are not well adapted to small-scale targets, the polarimetric data are not properly utilized, and the sea–land segmentation process to remove land false alarms is time-consuming. Regarding these problems, first, a single low-level path aggregation network is designed specifically for small targets. The structure reduces false alarms at the feature level by finding suitable single-scale feature maps for detection and adding a semantic enhancement module. Second, adaptive dual-polarimetric feature fusion is proposed to filter the multichannel features acquired by dual-polarimetric decomposition to reduce feature redundancy. Third, a segmentation layer is added to the network to shield the land from false alarms. The detection and segmentation layers share the feature extraction and feature fusion modules and are jointly trained by a joint loss. Finally, polarimetric SAR detection and segmentation dataset containing small vessel detection and sea–land segmentation labels is created with reference to the LS-SSDDv1.0 dataset, and experimental results on this dataset verify the improvement of this proposed method over other typical methods.","2151-1535","","10.1109/JSTARS.2022.3158807","National Natural Science Foundation of China(grant numbers:62171016,61871413); Fundamental Research Funds for the Central Universities(grant numbers:buctrc202001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9735422","Feature fusion;polarimetric synthetic aperture radar (SAR);sea–land segmentation;small vessel detection","Feature extraction;Radar polarimetry;Image segmentation;Semantics;Clutter;Synthetic aperture radar;Optical imaging","feature extraction;image fusion;image segmentation;marine vehicles;object detection;radar detection;radar imaging;radar polarimetry;synthetic aperture radar","polarimetric data;sea-land segmentation process;land false alarms;feature level;single-scale feature maps;adaptive dual-polarimetric feature fusion;multichannel features;dual-polarimetric decomposition;feature redundancy;segmentation layer;feature extraction;feature fusion modules;polarimetric SAR detection;segmentation dataset;sea-land segmentation labels;sea vessels;synthetic aperture radar images;scattering intensity;image pixels;small-scale targets;small vessel detection;SAR images;detection network structures;semantic enhancement module","","3","","61","CCBY","15 Mar 2022","","","IEEE","IEEE Journals"
"Change Detection in SAR Images Based on Improved Non-Subsampled Shearlet Transform and Multi-Scale Feature Fusion CNN","F. Shen; Y. Wang; C. Liu","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","8 Dec 2021","2021","14","","12174","12186","Traditional methods for change detection in synthetic aperture radar images have difficulty in obtaining results from the generated differential image (DI) owing to speckle noise. In recent years, many deep learning-based methods have emerged because of their outstanding anti-noise and self-learning ability. However, they are limited by the requirement of abundant high-precision labels. Therefore, in this article, we propose a novel unsupervised method based on improved non-subsampled shearlet transform (NSST) and multi-scale feature fusion convolutional neural network for change detection. First, this method improves the traditional NSST algorithm and proposes a novel pseudo-label generator to obtain more pseudo-labels with higher confidence. It is noteworthy that the more accurate the pseudo-labels are, the better the change detection results will be. Second, this method designs a multi-scale feature fusion block in the network to make the feature images contain more complete information and reduces the number of pooling layers to avoid losing feature image details. The main idea of this method is to eliminate the step of generating the DI and directly obtain results from the original images. The theoretical analysis and final results conducted on three real datasets prove its validity. Furthermore, to verify the generality and potential of the proposed method, we apply it to the cross-region change detection and compare it with the supervised method, which achieve satisfactory results.","2151-1535","","10.1109/JSTARS.2021.3126839","Ministry of Science and Technology of the People's Republic of China; National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFB0503001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610984","Change detection;convolutional neural network (CNN);non-subsampled shearlet transform (NSST);synthetic aperture radar (SAR)","Feature extraction;Synthetic aperture radar;Speckle;Radar polarimetry;Convolutional neural networks;Generators;Clustering algorithms","feature extraction;geophysical image processing;image denoising;image fusion;image segmentation;learning (artificial intelligence);neural nets;radar imaging;synthetic aperture radar;transforms","SAR images;multiscale feature fusion CNN;synthetic aperture radar images;generated differential image;DI;deep learning-based methods;outstanding anti-noise;high-precision labels;novel unsupervised method;improved nonsubsampled shearlet transform;multiscale feature fusion convolutional neural network;traditional NSST algorithm;novel pseudolabel generator;pseudolabels;change detection results;multiscale feature fusion block;feature images;feature image details;original images;theoretical analysis;cross-region change detection;supervised method","","3","","40","CCBY","10 Nov 2021","","","IEEE","IEEE Journals"
"Vegetation Segmentation for Sensor Fusion of Omnidirectional Far-Infrared and Visual Stream","D. L. Stone; G. Shah; Y. Motai; A. J. Aved","Department of Electrical and Computer Engineering, Virginia Commonwealth University, Richmond, VA, USA; Bon Secours Health Systems, Richmond, VA, USA; Virginia Commonwealth University, Richmond, VA, USA; REID, Air Force Research Laboratory, Rome, NY, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Mar 2019","2019","12","2","614","626","In the context of vegetation detection, the fusion of omnidirectional (O-D) infrared (IR) and color vision sensors may increase the level of vegetation perception for unmanned robotic platforms. Current approaches are primarily focused on O-D color vision for localization, mapping, and tracking. A literature search found no significant research in our area of interest. The fusion of O-D IR and O-D color vision sensors for the extraction of feature material type has not been adequately addressed. We will look at augmenting indices-based spectral decomposition with IR region-based spectral decomposition to address the number of false detects inherent in indices-based spectral decomposition alone. Our work shows that the fusion of the normalized difference vegetation index (NDVI) from the O-D color camera fused with the IR thresholded signature region associated with the vegetation region minimizes the number of false detects seen with NDVI alone. The contribution of this paper is the demonstration of a new technique, thresholded region fusion technique for the fusion of O-D IR and O-D color. We also look at the Kinect vision sensor fused with the O-D IR camera. Our experimental validation demonstrates a 64% reduction in false detects in our method compared to classical indices-based detection.","2151-1535","","10.1109/JSTARS.2019.2891518","Office of Naval Research; Naval Surface Warfare Center Dahlgren Division; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8636196","Camera;image processing;measurement false positive (FP);omnidirectional (O-D) far-infrared (IR);robot sensing systems;sensor fusion;vegetation detection;vision;visualization","Cameras;Robot sensing systems;Vegetation mapping;Image color analysis;Visualization;Sensor fusion;Feature extraction","cameras;computer vision;feature extraction;image colour analysis;image fusion;image segmentation;image sensors;infrared imaging;mobile robots;robot vision;vegetation","feature material type;IR region-based spectral decomposition;false detects;normalized difference vegetation index;NDVI;O-D color camera;signature region;vegetation region;thresholded region fusion technique;Kinect vision sensor;classical indices-based detection;vegetation segmentation;sensor fusion;omnidirectional far-infrared stream;visual stream;vegetation detection;vegetation perception;unmanned robotic platforms;O-D color vision sensors;omnidirectional color vision sensor;O-D IR color vision sensor;omnidirectional infrared color vision sensor;indices-based spectral decomposition","","3","","53","IEEE","6 Feb 2019","","","IEEE","IEEE Journals"
"CSF-Net: Color Spectrum Fusion Network for Semantic Labeling of Airborne Laser Scanning Point Cloud","J. Li; W. Zhang; W. Diao; Y. Feng; X. Sun; K. Fu","School of Electronic, Electrical, and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical, and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical, and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical, and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","30 Dec 2021","2022","15","","339","352","Airborne laser scanning point cloud semantic labeling, which aims to identify the category of each point, plays a significant role in many applications, such as forest observing, powerline extraction, etc. Under the guidance of deep learning technology, the interpretation thought of point clouds has also greatly changed. However, owing to the irregular and unordered natures of point clouds, it is relatively difficult for classification model to distinguish some objects with similar geometry by single-modal data only. Fortunately, additional gain information, e.g., color spectrum which can be complementary to geometric information, is able to effectively promote the classification effect. Therefore, the design of fusion strategy is a critical part in model construction. In this article, aiming to capture more abstract semantic information for color spectrum data, we elaborate a color spectrum fusion (CSF) module. It can be flexibly integrated into a classification pipeline with just negligible parameters. Then, we expand data fusion thoughts for point clouds and color spectrum and investigate three possible fusion strategies. Accordingly, we develop three architectures to construct CSF-Nets. Ultimately, by taking a weighted cross entropy loss, we can train our CSF-Nets in an end-to-end manner. Experiments on two extensively used datasets: Vaihingen 3D and LASDU show that the presented three fusion approaches all can improve the performance, while the earlier fusion strategy performs the best. Besides, compared with other well-performed methods, CSF-Net is still able to achieve satisfactory performance on overall accuracy and m$F_{1}$-score indicator. This also validates the effectiveness of our multimodal fusion network.","2151-1535","","10.1109/JSTARS.2021.3133602","National Natural Science Foundation of China(grant numbers:61725105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645363","Airborne laser scanning (ALS) point cloud;color spectrum information;deep learning;multimodal fusion;semantic labeling","Image color analysis;Point cloud compression;Three-dimensional displays;Semantics;Feature extraction;Labeling;Deep learning","deep learning (artificial intelligence);feature extraction;image classification;image colour analysis;image fusion","CSF-Net;color spectrum fusion network;airborne laser scanning point cloud semantic labeling;classification effect;semantic information;color spectrum data;color spectrum fusion module;data fusion;deep learning","","1","","67","CCBY","9 Dec 2021","","","IEEE","IEEE Journals"
"High-Precision Pixelwise SAR–Optical Image Registration via Flow Fusion Estimation Based on an Attention Mechanism","Q. Yu; Y. Jiang; W. Zhao; T. Sun","School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 May 2022","2022","15","","3958","3971","Due to the severe speckle noise and complex local deformation in synthetic aperture radar (SAR) images, the problem of high-precision pixelwise registration (dense registration) between SAR and optical images remains far from resolved. In this article, an attention mechanism based optical flow fusion algorithm is proposed to achieve high-precision dense SAR–optical image registration. First, two descriptors, the scale-invariant feature transform (SIFT) and a descriptor based on phase congruency (PC), are used to describe SAR and optical images to eliminate their intensity differences. Then, a salient feature map is extracted as a query matrix to weight the optical flow energy function. When extracting the salient feature map, the Contour Robuste d’Ordre Non Entier detector and the ratio of exponentially weighted averages operator are used to eliminate additive and multiplicative noise in the optical and SAR images, respectively. Finally, the optical flow fields based on SIFT and the PC-based descriptor are fused to compensate for registration ambiguity. Experimental results show that our method is feasible, effective, and robust to noise, and it enables high-precision registration under local deformation.","2151-1535","","10.1109/JSTARS.2022.3172449","Open Project Program Foundation of the Key Laboratory of Opto-Electronics Information Processing; Chinese Academy of Sciences(grant numbers:OEIP-O-202009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9769870","Optical flow;pixelwise registration (dense registration);SAR and optical;synthetic aperture radar (SAR) image","Optical imaging;Optical sensors;Synthetic aperture radar;Adaptive optics;Radar polarimetry;Optical distortion;Image registration","feature extraction;image fusion;image matching;image registration;image resolution;image segmentation;image sequences;optical images;radar imaging;sensor fusion;speckle;synthetic aperture radar;transforms","high-precision pixelwise SAR-optical image registration;flow fusion estimation;attention mechanism;severe speckle noise;complex local deformation;synthetic aperture radar images;high-precision pixelwise registration;dense registration;optical images;optical flow fusion algorithm;high-precision dense SAR-optical image registration;salient feature map;optical flow energy function;optical flow fields;PC-based descriptor;registration ambiguity;high-precision registration","","1","","53","CCBY","5 May 2022","","","IEEE","IEEE Journals"
"FTC-Net: Fusion of Transformer and CNN Features for Infrared Small Target Detection","M. Qi; L. Liu; S. Zhuang; Y. Liu; K. Li; Y. Yang; X. Li","School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Physics, Hefei University of Technology, Hefei, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","14 Oct 2022","2022","15","","8613","8623","Single-frame infrared small target detection is still a challenging task due to the complex background and unobvious structural characteristics of small targets. Recently, convolutional neural networks (CNN) began to appear in the field of infrared small target detection and have been widely used for excellent performance. However, existing CNN-based methods mainly focus on local spatial features while ignoring the long-range contextual dependencies between small targets and backgrounds. To capture the global context-aware information, we propose fusion network architecture of transformer and CNN (FTC-Net), which consists of two branches. The CNN-based branch uses a U-Net with skip connections to obtain low-level local details of small targets. The transformer-based branch applies hierarchical self-attention mechanisms to learn long-range contextual dependencies. Specifically, the transformer branch can suppress background interferences and enhance target features. To obtain local and global feature representation, we design a feature fusion module to realize the feature concentration of two branches. We implement ablation and comparative experiments on a publicly accessed SIRST dataset. Experimental results show that the transformer-based branch is effective and suggest the superiority of the proposed FTC-Net compared with other state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3210707","National Natural Science Foundation of China(grant numbers:6227072073,61771180); Hefei Municipal Natural Science Foundation(grant numbers:2021050); Fundamental Research Funds for the Central Universities(grant numbers:JZ2021HGQA0222); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9912644","Deep learning;feature fusion;hierarchical transformer;infrared small target detection","Transformers;Object detection;Feature extraction;Task analysis;Convolutional neural networks;Image segmentation;Computer architecture","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image fusion;image representation;object detection","FTC-Net;CNN feature fusion;single-frame infrared small target detection;convolutional neural network feature fusion;global context-aware information;transformer branch;local feature representation;global feature representation;feature fusion module;SIRST dataset;network architecture fusion;transformer feature fusion;U-Net;skip connections","","1","","45","CCBY","6 Oct 2022","","","IEEE","IEEE Journals"
"Weighted Residual Dynamic Ensemble Learning for Hyperspectral Image Classification","H. Lu; H. Su; P. Zheng; Y. Gao; Q. Du","School of Earth Sciences and Engineering, Hohai University, Nanjing, China; School of Earth Sciences and Engineering, Hohai University, Nanjing, China; School of Earth Sciences and Engineering, Hohai University, Nanjing, China; School of Earth Sciences and Engineering, Hohai University, Nanjing, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","31 Aug 2022","2022","15","","6912","6927","Recently, collaborative representation classifiers have been extensively studied as an essential method for the hyperspectral image. However, how to comprehensively utilize the classification advantages of multiple collaborative classifiers has not been well investigated. In this article, two new dynamic ensemble learning methods using local weighted residual (LWR-DEL) and double-weighted residual (DWR-DEL) of multicollaborative representation classifiers are proposed. First, the dynamic ensemble learning method based on clustering is utilized to introduce prior knowledge for the collaborative representation classifier. Then, with prior knowledge, the local weights of each classifier for a different region of competence are obtained. To consider the global information of hyperspectral data, the K-nearest neighbor algorithm is adopted to achieve validation samples with global information. The global weights for each classifier can be obtained and then used to constrain the locally weighted residuals. Similar to LWR-DEL, the global information is also used to constrain residual, and then double-weighted constrained residual fusion obtains the final classifier result. The effectiveness of the proposed methods is validated using three hyperspectral data sets. The experimental results show that both LWR-DEL and DWR-DEL outperform their single-classifier counterparts. In particular, the proposed methods provide superior performance compared with the state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3200042","National Natural Science Foundation of China(grant numbers:42122008,41871220); Jiangsu Province Graduate Research and Innovation Program(grant numbers:KYCX220662); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9863643","Dynamic ensemble learning;hyperspectral imagery;multicollaborative representation;weighted residual ensemble","Hyperspectral imaging;Testing;Collaboration;Behavioral sciences;Image classification;Classification algorithms;Representation learning","geophysical image processing;groupware;hyperspectral imaging;image classification;image fusion;nearest neighbour methods","double-weighted constrained residual fusion;LWR-DEL;DWR-DEL;weighted residual dynamic ensemble learning;hyperspectral image classification;local weighted residual;multicollaborative representation classifiers;clustering;K-nearest neighbor algorithm","","","","53","CCBY","19 Aug 2022","","","IEEE","IEEE Journals"
"SAR Image Change Detection Based on Joint Dictionary Learning With Iterative Adaptive Threshold Optimization","Q. Yu; M. Zhang; L. Yu; R. Wang; J. Xiao","School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Jul 2022","2022","15","","5234","5249","Synthetic aperture radar (SAR) image change detection is still a challenge due to inherent speckle noise and scarce datasets. This article proposes a joint-related dictionary learning algorithm based on the k-singular value decomposition (K-SVD) algorithm called JR-KSVD and an iterative adaptive threshold optimization (IATO) algorithm for unsupervised change detection. The JR-KSVD algorithm adds dictionary correlation learning to the K-SVD algorithm to generate a uniform initial dictionary for dual-temporal SAR images, thereby reducing the instability of sparse representations due to atomic correlations and enhancing the extraction of image edges and details. The IATO approach employs thresholds obtained by the “difference-log ratio” fusion image for indefinite residual energy minimization iterations to gradually shrink the threshold variation range and finally generate the change images, which have a high degree of adaptivity and strong real-time performance. Finally, experiments on six real datasets demonstrate that the proposed algorithm exhibits superior detection performance and robustness against some state-of-the-art algorithms.","2151-1535","","10.1109/JSTARS.2022.3187108","Open Project Program Foundation of the Key Laboratory of Opto-Electronics Information Processing; Chinese Academy of Sciences(grant numbers:OEIP-O-202009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810349","Change detection;difference-log ratio image;iterative adaptive threshold;joint-related dictionary learning;synthetic aperture radar (SAR) image","Radar polarimetry;Dictionaries;Machine learning;Image reconstruction;Change detection algorithms;Synthetic aperture radar;Matching pursuit algorithms","feature extraction;image fusion;image representation;iterative methods;optimisation;radar imaging;singular value decomposition;speckle;synthetic aperture radar","related dictionary learning algorithm;dual temporal SAR images;difference log ratio fusion image;K-SVD algorithm;JR-KSVD algorithm;unsupervised change detection;iterative adaptive threshold optimization algorithm;k-singular value decomposition algorithm;scarce datasets;speckle noise;synthetic aperture radar image change detection;SAR image change detection;detection performance;real-time performance;threshold variation range;indefinite residual energy minimization iterations;IATO approach;image edge extraction;atomic correlations","","","","47","CCBY","29 Jun 2022","","","IEEE","IEEE Journals"
"Multiscale Spatial Fusion and Regularization Induced Unsupervised Auxiliary Task CNN Model for Deep Super-Resolution of Hyperspectral Images","V. K. Ha; J. Ren; Z. Wang; G. Sun; H. Zhao; S. Marshall","Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, U.K.; National Subsea Centre, Robert Gordon University, Aberdeen, U.K.; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; School of Computer Sciences, Guangdong Polytechnic Normal University, Guangzhou, China; Department of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, U.K.","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Jun 2022","2022","15","","4583","4598","Hyperspectral images (HSI) feature rich spectral information in many narrow bands but at a cost of a relatively low spatial resolution. As such, various methods have been developed for enhancing the spatial resolution of the low-resolution HSI (Lr-HSI) by fusing it with high-resolution multispectral images (Hr-MSI). The difference in spectrum range and spatial dimensions between the Lr-HSI and Hr-MSI has been fundamental but challenging for multispectral/hyperspectral (MS/HS) fusion. In this article, a multiscale spatial fusion and regularization induced auxiliary task based convolutional neural network model is proposed for deep super-resolution of HSI, where an Lr-HSI is fused with an Hr-MSI to reconstruct a high-resolution HSI (Hr-HSI) counterpart. The multiscale fusion is used to efficiently address the discrepancy in spatial resolutions between the two inputs. Based on the general assumption that the acquired Hr-MSI and the reconstructed Hr-HSI share similar underlying characteristics, the auxiliary task is proposed to learn a representation for improved generality of the model and reduced overfitting. Experimental results on five public datasets have validated the effectiveness of our approach in comparison with several state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3176969","Natural Environment Research Council(grant numbers:NE/S002545/1); Dazhi Scholarship of GPNU; National Natural Science Foundation of China(grant numbers:41971292,62072122); Scientific and Technological Planning Projects of Guangdong Province(grant numbers:2021A0505030074); Scientific Research Capability Improvement Project of Guangdong Key Construction(grant numbers:2021ZDJS025); ETF Scholarship from the Faculty of Engineering; University of Strathclyde; Government Scholarship of Vietnam; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779940","Auxiliary task;convolutional neural networks (CNN);hyperspectral image (HSI);super-resolution (SR);multiscale spatial fusion","Task analysis;Image reconstruction;Tensors;Superresolution;Spatial resolution;Bayes methods;Training","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image classification;image fusion;image resolution;spectral analysis;unsupervised learning","deep super-resolution;hyperspectral images;spatial resolution;low-resolution HSI;Lr-HSI;high-resolution multispectral images;multiscale spatial fusion;convolutional neural network model;high-resolution HSI;multiscale fusion;unsupervised auxiliary task CNN model;spectral information;Hr-MSI;multispectral-hyperspectral fusion;Hr-HSI","","","","59","CCBY","23 May 2022","","","IEEE","IEEE Journals"
"Enhanced Channel Attention Network With Cross-Layer Feature Fusion for Spectral Reconstruction in the Presence of Gaussian Noise","C. Zou; C. Zhang; M. Wei; C. Zou","College of Mathematics and Statistic, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Nov 2022","2022","15","","9497","9508","Spectral reconstruction from RGB images has made significant progress. Previous works usually utilized the noise-free RGB images as input to reconstruct the corresponding hyperspectral images (HSIs). However, due to instrumental limitation or atmospheric interference, it is inevitable to suffer from noise (e.g., Gaussian noise) in the actual image acquisition process, which further increases the difficulty of spectral reconstruction. In this article, we propose an enhanced channel attention network (ECANet) to learn a nonlinear mapping from noisy RGB images to clean HSIs. The backbone of our proposed ECANet is stacked with multiple enhanced channel attention (ECA) blocks. The ECA block is the dual residual version of the channel attention block, which makes the network focus on key auxiliary information and features that are more conducive to spectral reconstruction. For the case that the input RGB images are disturbed by Gaussian noise, cross-layer feature fusion unit is used to concatenate the multiple feature maps at different depths for more powerful feature representations. In addition, we design a novel combined loss function as the constraint of the ECANet to achieve more accurate reconstruction result. Experimental results on two HSI benchmarks, CAVE and NTIRE 2020, demonstrate that the effectiveness of our method in terms of both visual and quantitative over other state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2022.3218820","National Natural Science Foundation of China(grant numbers:62076065); Natural Science Foundation of Fujian Province(grant numbers:2021J01611); Development Program of Fuzhou University(grant numbers:GXRC-21007,XRC-18080); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935112","Cross-layer feature fusion (CLFF);combined loss function;enhanced channel attention (ECA);Gaussian noise;spectral reconstruction","Image reconstruction;Hyperspectral imaging;Gaussian noise;Feature extraction;Cross layer design;Training;Convolution","feature extraction;Gaussian noise;geophysical image processing;image colour analysis;image denoising;image enhancement;image fusion;image reconstruction;image representation;learning (artificial intelligence);sensor fusion;spectral analysis","accurate reconstruction result;actual image acquisition process;channel attention block;corresponding hyperspectral images;cross-layer feature fusion unit;ECANet;enhanced channel attention network;Gaussian noise;key auxiliary information;multiple enhanced channel attention blocks;multiple feature maps;noise-free RGB images;noisy RGB images;spectral reconstruction","","","","42","CCBY","2 Nov 2022","","","IEEE","IEEE Journals"
"Spectral Gradient Fidelity and Spatial Hessian Hyper-Laplacian Sparsity Constraints for Variational Pansharpening","P. Liu; Y. Li","Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing, China; Jiangsu Key Laboratory of Big Data Security and Intelligent Processing, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Aug 2022","2022","15","","6086","6098","In this article, an effectively variational pansharpening method with spectral gradient fidelity and spatial Hessian hyper-Laplacian sparsity constraints (PSGFSHHS) was proposed to fuse the low resolution multispectral (LRMS) and panchromatic (Pan) images to the high resolution multispectral (HRMS) image. First, the spectral feature correlation prior between LRMS and HRMS was modeled by the spectral gradient fidelity constraint. Second, the spatial correlation prior between Pan and HRMS was particularly modeled by the spatial Hessian hyper-Laplacian sparsity constraint from the statistical perspective, which clearly held strong novelty for pansharpening recently by the spatial Hessian hyper-Laplacian sparsity modeling. Third, by combining the spectral gradient fidelity constraint and the spatial Hessian hyper-Laplacian sparsity constraint, the PSGFSHHS model was formed and the alternating direction method of multipliers method was utilized for optimization. Finally, the experimental fusion examples clearly illustrated the effectiveness and capability of PSGFSHHS.","2151-1535","","10.1109/JSTARS.2022.3193182","National Natural Science Foundation of China(grant numbers:61802202); China Postdoctoral Science Foundation(grant numbers:2022M711692); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837408","Spatial hessian hyper-Laplacian sparsity;spectral gradient fidelity constraint;variational pansharpening method","Pansharpening;Satellites;Spatial resolution;Optimization;Multiresolution analysis;Laplace equations;Correlation","geophysical image processing;gradient methods;image fusion;image resolution;optimisation","spectral gradient fidelity constraint;spatial Hessian hyper Laplacian sparsity modeling;spatial Hessian hyper Laplacian sparsity constraint;PSGFSHHS;high resolution multispectra;statistical perspective;multipliers method","","","","45","CCBY","22 Jul 2022","","","IEEE","IEEE Journals"
"Multi-Spectral Image Change Detection Based on Band Selection and Single-Band Iterative Weighting","L. Ma; Z. Jia; Y. Yu; J. Yang; N. K. Kasabov","College of Information Science and Engineering, Xinjiang University, Ürümqi, China; College of Information Science and Engineering, Xinjiang University, Ürümqi, China; College of Information Science and Engineering, Xinjiang University, Ürümqi, China; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University, Shanghai, China; Knowledge Engineering and Discovery Research Institute, Auckland University of Technology, Auckland, New Zealand","IEEE Access","13 Mar 2019","2019","7","","27948","27956","Iteratively reweighted multivariate alteration detection algorithm has the phenomena of broken patches, much noise, and small change area that are difficult to detect, and the overall detection rate is low. In order to solve this problem, this paper proposes a multi-spectral image change detection algorithm based on band selection and single-band iterative weighting. Because the change information of the multi-spectral image is concentrated in some bands, the background and noise information of the rest bands are more, which may have a negative effect on the final result. Therefore, the band with more change information is selected first, and the iterative weighting of a single band can better suppress the noise and background information, so as to obtain a higher band correlation and facilitate the extraction of change information. This method is used to obtain the characteristic difference graph of the selected band with more change information. After Gaussian denoising of each characteristic difference graph, the Euclidean distance formula is used to fuse the difference graph of each band into a change intensity graph. Finally, the unsupervised $k$ -means clustering algorithm is used to perform binary-valued clustering on the fused difference graph to obtain the change detection results. As a practical application, the superior performance of our proposed method was demonstrated through a large number of comparative tests.","2169-3536","","10.1109/ACCESS.2019.2901286","National Natural Science Foundation of China(grant numbers:61665012,U1803261); Ministry of Education of the People's Republic of China(grant numbers:DICE 2016–2196); Natural Science Foundation of Xinjiang Province(grant numbers:2015211C288); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658068","Multi-spectral change detection;IR-MAD;band selection","Change detection algorithms;Correlation;Remote sensing;Time complexity;Correlation coefficient;Feature extraction;Iterative algorithms","geophysical image processing;image denoising;image fusion;iterative methods;object detection;pattern clustering;terrain mapping","multispectral image change detection algorithm;detection rate;change area;multivariate alteration detection algorithm;band selection;change detection results;change intensity graph;characteristic difference graph;higher band correlation;background information;rest bands;change information;single-band iterative weighting","","5","","30","OAPA","4 Mar 2019","","","IEEE","IEEE Journals"
"Graph-Based Logarithmic Low-Rank Tensor Decomposition for the Fusion of Remotely Sensed Images","F. Ma; S. Huo; F. Yang","School of Electronic and Information Engineering, Liaoning Technical University, Huludao, China; School of Electronic and Information Engineering, Liaoning Technical University, Huludao, China; School of Electrical and Control Engineering, Liaoning Technical University, Huludao, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Nov 2021","2021","14","","11271","11286","Hyperspectral images with high spatial resolution play an important role in material classification, change detection, and others. However, owing to the limitation of imaging sensors, it is difficult to directly acquire images with both high spatial resolution and high spectral resolution. Therefore, the fusion of remotely sensed images is an effective way to obtain high-resolution desired data, which is usually an ill-posed inverse problem and susceptible to noise corruption. To address these issues, a low-rank model based on tensor decomposition is proposed to fuse hyperspectral and multispectral images by incorporating graph regularization, in which the logarithmic low-rank function is utilized to suppress the small components for denoising. Furthermore, this article takes advantage of the local spatial similarity of remotely sensed images to enhance the reconstruction performance by constructing spatial graphs, and also promotes signature smoothing between adjacent endmember spectra using the neighborhood-based spectral graph regularization. Finally, a set of efficient solvers is carefully designed via alternating optimization for closed-from solutions and computational reduction, in which vector-matrix operators are adapted to solve the 3-D core tensor. Experimental tests on several real datasets illustrate that the proposed fusion method yields better reconstruction performance than the current state-of-the-art methods, and can significantly suppress noise at the same time.","2151-1535","","10.1109/JSTARS.2021.3123466","Scientific Research Project of Colleges from Liaoning Department of Education(grant numbers:LJKZ0357,LJ2019QL006,LJ2019JL022); NSFC General Project(grant numbers:61971210); Discipline Innovation Team of Liaoning Technical University(grant numbers:LNTU20TD-20,LNTU20TD-25); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591494","Graph regularization;hyperspectral image (HSI) super-resolution;image fusion;low rank;tensor decomposition","Tensors;Spatial resolution;Image reconstruction;Hyperspectral imaging;Matrix decomposition;Superresolution;Sensors","","","","1","","56","CCBY","27 Oct 2021","","","IEEE","IEEE Journals"
"Small Object Detection in Unmanned Aerial Vehicle Images Using Feature Fusion and Scaling-Based Single Shot Detector With Spatial Context Analysis","X. Liang; J. Zhang; L. Zhuo; Y. Li; Q. Tian","Beijing Key Laboratory of Computational Intelligence and Intelligent System, Faculty of Information Technology, Beijing University of Technology, Beijing, China; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Faculty of Information Technology, Beijing University of Technology, Beijing, China; Collaborative Innovation Center of Electric Vehicles in Beijing, Beijing, China; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Faculty of Information Technology, Beijing University of Technology, Beijing, China; Noah???s Ark Laboratory, Huawei Technologies, Shenzhen, China","IEEE Transactions on Circuits and Systems for Video Technology","4 Jun 2020","2020","30","6","1758","1770","Objects in unmanned aerial vehicle (UAV) images are generally small due to the high-photography altitude. Although many efforts have been made in object detection, how to accurately and quickly detect small objects is still one of the remaining open challenges. In this paper, we propose a feature fusion and scaling-based single shot detector (FS-SSD) for small object detection in the UAV images. The FS-SSD is an enhancement based on FSSD, a variety of the original single shot multibox detector (SSD). We add an extra scaling branch of the deconvolution module with an average pooling operation to form a feature pyramid. The original feature fusion branch is adjusted to be better suited to the small object detection task. The two feature pyramids generated by the deconvolution module and feature fusion module are utilized to make predictions together. In addition to the deep features learned by the FS-SSD, to further improve the detection accuracy, spatial context analysis is proposed to incorporate the object spatial relationships into object redetection. The interclass and intraclass distances between different object instances are computed as a spatial context, which proves effective for multiclass small object detection. Six experiments are conducted on the PASCAL VOC dataset and the two UAV image datasets. The experimental results demonstrate that the proposed method can achieve a comparable detection speed but an accuracy superior to those of the six state-of-the-art methods.","1558-2205","","10.1109/TCSVT.2019.2905881","National Natural Science Foundation of China(grant numbers:61531006,61602018,61701011); Beijing Municipal Natural Science Foundation Cooperation Beijing Education Committee(grant numbers:KZ 201810005002,KZ 201910005007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8672115","Unmanned aerial vehicle (UAV) image;small object detection;feature fusion;feature scaling;single shot detector;spatial context analysis","Object detection;Feature extraction;Detectors;Unmanned aerial vehicles;Deconvolution;Photography;Remote sensing","autonomous aerial vehicles;image fusion;image motion analysis;learning (artificial intelligence);object detection;object tracking","FS-SSD;spatial context analysis;object spatial relationships;object redetection;UAV image datasets;unmanned aerial vehicle images;single shot detector;UAV images;original single shot multibox detector;deconvolution module;object detection task;feature fusion module;small object detection","","62","","43","IEEE","20 Mar 2019","","","IEEE","IEEE Journals"
"Pansharpening via Subpixel Convolutional Residual Network","C. Li; Y. Zheng; B. Jeon","Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing, China; Engineering Research Center of Digital Forensics, Ministry of Education, Nanjing, China; School of Electronic and Electrical Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Oct 2021","2021","14","","10303","10313","In this article, we propose a new pansharpening architecture called subpixel convolutional residual network to obtain high-resolution multispectral (MS) images. Different from previous works, we extract features from MS images in a low-resolution space and pay more attention to the balance of spectral and spatial information. Our architecture consists of two branches: the feature extraction branch and the residual branch. The former adopts a four-layer convolutional network to extract features, and then upsamples the feature maps using a subpixel convolution layer. For the latter, we combine the nearest neighbor interpolation and guided filter to yield a preliminary image with fundamental spectral and spatial information. With the outputs of the two branches, we can merge them and yield a pansharpened image. The proposed method was compared with several representative methods. The experimental results demonstrate that our method achieves high fusion accuracy while maintaining a good balance between the spectral and the spatial resolution.","2151-1535","","10.1109/JSTARS.2021.3117944","National Natural Science Foundation of China(grant numbers:U20B2065,61972206,62011540407); Natural Science Foundation of Jiangsu Province(grant numbers:BK20211539); Six Talent Peaks Project in Jiangsu Province(grant numbers:RJFW-015); Qinglan Project of Jiangsu Province of China; National Research Foundation of Korea(grant numbers:NRF-2020K2A9A2A06036255,FY2020); Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560086","Convolutional neural network (CNN);data fusion;guided filter;pansharpening;remote sensing;subpixel","Pansharpening;Feature extraction;Convolutional neural networks;Computer architecture;Interpolation;Spatial resolution;Task analysis","feature extraction;geophysical image processing;image filtering;image resolution;image sampling;interpolation;nearest neighbour methods","subpixel convolutional residual network;high-resolution multispectral imaging;MS imaging;low-resolution space;spatial information;feature extraction branch;four-layer convolutional network;feature mapping;subpixel convolution layer;spatial resolution;pansharpened imaging architecture;nearest neighbor interpolation","","3","","43","CCBY","6 Oct 2021","","","IEEE","IEEE Journals"
"Weather Visibility Prediction Based on Multimodal Fusion","C. Zhang; M. Wu; J. Chen; K. Chen; C. Zhang; C. Xie; B. Huang; Z. He","Pattern Recognition and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Pattern Recognition and Intelligent System Lab, Beijing University of Posts and Telecommunications, Beijing, China; Division of Energy Processes, KTH Royal Institute of Technology, Stockholm, Sweden; National Meteorological Center, Beijing, China; National Meteorological Center, Beijing, China; Department of Statistics and Computer Science, McGill University, Montreal, QC, Canada","IEEE Access","17 Jun 2019","2019","7","","74776","74786","Visibility affects all forms of traffic: roads, sailing, and aviation. Visibility prediction is meaningful in guiding production and life. Different from weather prediction, which relies solely on atmosphere factors, the factors that affect meteorological visibility are more complicated, such as the air pollution caused by factory exhaust emission. However, the current prediction of visibility is mostly based on the numerical prediction method similar to the weather prediction. We proposed a method using multimodal fusion to build a weather visibility prediction system in this paper. An advanced numerical prediction model and a method for emission detection were used to build a multimodal fusion visibility prediction system. We used the most advanced regression algorithm, XGBoost, and LightGBM, to train the fusion model for numerical prediction. Through the estimation of factory emission by the traditional detector in the satellite image, we propose to add the result of estimation based on Landsat-8 satellite images to assist the prediction. By testing our numerical model in atmosphere data of various meteorological observation stations in Beijing-Tianjin-Hebei region from 2002 to 2018, our numerical prediction model turns out to be more accurate than other existing methods, and after fusing with emission detection method, the accuracy of our visibility prediction system has been further improved.","2169-3536","","10.1109/ACCESS.2019.2920865","National Natural Science Fund(grant numbers:61773071); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8731972","Visibility prediction;emission estimation;numerical prediction;satellite image","Satellites;Numerical models;Predictive models;Production facilities;Estimation;Prediction algorithms;Forecasting","air pollution;atmospheric techniques;geophysical image processing;image fusion;regression analysis;remote sensing;weather forecasting","multimodal fusion visibility prediction system;fusion model;emission detection method;weather prediction;atmosphere factors;meteorological visibility;factory exhaust emission;weather visibility prediction system;advanced numerical prediction model;Beijing-Tianjin-Hebei region;AD 2002 to 2018","","17","","41","OAPA","5 Jun 2019","","","IEEE","IEEE Journals"
"Multi-Scale Aerial Target Detection Based on Densely Connected Inception ResNet","M. Zhang; K. Pang; C. Gao; M. Xin","Institute of Data and Knowledge Engineering, Henan University, Kaifeng, China; Henan Key Laboratory of Big Data Analysis and Processing, Henan University, Kaifeng, China; Henan Key Laboratory of Big Data Analysis and Processing, Henan University, Kaifeng, China; School of Computer Science and Engineering, Beihang University, Beijing, China","IEEE Access","15 May 2020","2020","8","","84867","84878","With the rapid development of unmanned aerial vehicles (UAVs), aerial targets detection has attracted extensive attention from researchers. The difficulty of aerial image detection lies in the small proportion of ground targets in aerial images and the wide variety of target sizes. After multiple down-sampling, the features of small targets are almost not available on the feature maps. To address these drawbacks, a densely connected Inception ResNet (RIDNet) is proposed. RIDNet is a lightweight multi-scale fusion detection network constructed with two residual inception units (RI): the RI-Dense model and the RI-Deconv model. The RI-Dense model consists of densely connected layers and shortcut connections. Each convolutional layer in RI-Dense has access to all the subsequent layers and passes on the information that needs to be preserved. The RI-Deconv fuses the global feature in a residual and hierarchical way, which continuously deconvolutes the output of RI-Dense and concatenates the result with the original output to get fusion layers. The fused layers absorb semantic information and detailed information from deep layers and shallow layers, respectively. Extensive experiments show the effectiveness of the proposed RIDNet. Ablation experiments also demonstrate that the RI-Dense model and RI-Deconv model can improve the mAP by 7.8% and 6.8%, respectively.","2169-3536","","10.1109/ACCESS.2020.2992647","National Natural Science Foundation of China(grant numbers:61802111); Foundation of Henan Education Department(grant numbers:19A520002); Fund of Henan Province Young Key Teacher(grant numbers:2017GGJS019); Postdoctoral Science Fund of China(grant numbers:2015M582182); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086446","Object detection;convolutional network;unmanned aerial vehicle","Feature extraction;Object detection;Convolution;Kernel;Unmanned aerial vehicles;Semantics;Detectors","autonomous aerial vehicles;deconvolution;feature extraction;geophysical image processing;image fusion;object detection;remote sensing","densely connected inception;RI-deconv model;RI-dense model;fusion layers;densely connected layers;residual inception units;multiscale fusion detection network;RIDNet;ground targets;aerial image detection;unmanned aerial vehicles;multiscale aerial target detection","","9","","39","CCBY","5 May 2020","","","IEEE","IEEE Journals"
"Super-Resolution Based Deep Learning Techniques for Panchromatic Satellite Images in Application to Pansharpening","G. Rohith; L. S. Kumar","National Institute of Technology Puducherry at Karaikal, Puducherry, India; National Institute of Technology Puducherry at Karaikal, Puducherry, India","IEEE Access","14 Sep 2020","2020","8","","162099","162121","Pansharpening is a technique that fuses the coarser resolution of multispectral imagery (MS) with high spatial resolution panchromatic (PAN) imagery. Pansharpening is prone to spectral distortions based on the nature of the panchromatic band. If the spatial features are unclear in the panchromatic image, the pan-sharpened image will not be able to produce clear images. Super-Resolution (SR) is a technique that enhances minute details of the features in the image, thereby improving spatial information in the image. By fusing the Multispectral image with the super-resolved panchromatic image, there is a chance for producing high-quality multispectral imagery (pan-sharpened image). In this paper, ten state-of-the-art super-resolution based on deep learning techniques are tested and analyzed using ten different publicly available panchromatic datasets. On analysis, a feedback network for image super-resolution (SRFBN) technique outperforms the other algorithms in terms of sharp edges and pattern clarity, which are not visible in the input image. The proposed method is the fusion of SR applied PAN image with the MS image using a benchmarked Band Depended Spatial Detail (BDSD) pansharpening algorithm. The proposed method experiments with six datasets from different sensors. On analysis, the proposed technique outperforms the other counterpart pansharpening algorithms in terms of enhanced spatial information in addition to sharp edges and pattern clarity at reduced spectral distortion. Hence, the super-resolution based pansharpening algorithm is recommended for high spatial image applications.","2169-3536","","10.1109/ACCESS.2020.3020978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184034","Deep Learning (DL);convolution neural networks (CNN);pansharpening applications","Spatial resolution;Satellites;Training;Machine learning;Signal resolution;Sensors","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing","deep learning techniques;panchromatic satellite images;coarser resolution;high spatial resolution panchromatic imagery;panchromatic band;pan-sharpened image;Multispectral image;super-resolved panchromatic image;high-quality multispectral imagery;state-of-the-art super-resolution;image super-resolution technique;sharp edges;pattern clarity;input image;PAN image;MS image;benchmarked Band Depended Spatial Detail pansharpening algorithm;counterpart pansharpening algorithms;enhanced spatial information;high spatial image applications;panchromatic datasets","","5","","45","CCBYNCND","1 Sep 2020","","","IEEE","IEEE Journals"
"Airborne LiDAR and Photogrammetric Point Cloud Fusion for Extraction of Urban Tree Metrics According to Street Network Segmentation","W. Yang; Y. Liu; H. He; H. Lin; G. Qiu; L. Guo","Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China","IEEE Access","15 Jul 2021","2021","9","","97834","97842","This paper provides a practical procedure for fusing LiDAR and photogrammetric point clouds for the extraction of tree metrics. Aerial photogrammetric point clouds are first generated using the structure-from-motion and dense-matching methods. Registration of the LiDAR and photogrammetric point clouds is then performed using an onboard global positioning system and inertial measurement unit. However, due to systematic deviations, it is impossible to directly merge the two types of point cloud. Therefore, an urban street network obtained from the OpenStreetMap digital mapping system is utilized for point cloud segmentation. After segmentation, each chunk is finely registered and merged based on the iterative closest point algorithm, allowing the two types of data to be accurately co-registered and a fused point cloud obtained. Finally, we conducted experiments to extract stand and individual tree metrics from fused point clouds created for two study plots. The height distributions of the fused point clouds were highly consistent with LiDAR data, with the 5%, 10%, 25%, 50%, 75%, 90%, and 95% height percentiles showing acceptable similarities. The height distribution of individual trees was also consistent with that of field measurements. Furthermore, the fused point clouds contain a high point density and RGB color information, which allow shape delineation and estimation of tree health status. This comprehensive analysis demonstrates that this procedure provides a practical way to inventory tree stands and individuals in urban areas.","2169-3536","","10.1109/ACCESS.2021.3094307","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101130009); Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning(grant numbers:2020B121202019); Smart Guangzhou Spatio-temporal Information Cloud Platform Construction(grant numbers:GZIT2016-A5-147); Gao Fen Project of China(grant numbers:30-Y20A34-9010-15/17); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9471876","Photogrammetry;LiDAR;point clouds;urban trees;height","Three-dimensional displays;Vegetation;Laser radar;Measurement;Cameras;Urban areas;Forestry","computer vision;feature extraction;geophysical image processing;Global Positioning System;image colour analysis;image fusion;image matching;image registration;image segmentation;iterative methods;optical radar;photogrammetry;remote sensing by laser beam","photogrammetric point cloud fusion;urban tree metrics;LiDAR;aerial photogrammetric point clouds;point cloud segmentation;iterative closest point algorithm;fused point cloud;high point density","","1","","39","CCBY","2 Jul 2021","","","IEEE","IEEE Journals"
"Learn to Optimize Panchromatic Imagery for Accurate Building Extraction","Y. Wang; G. Wu; C. Feng; R. Shibasaki","Center for Spatial Information Science, The University of Tokyo, Kashiwa, Japan; Center for Spatial Information Science, The University of Tokyo, Kashiwa, Japan; Center for Spatial Information Science, The University of Tokyo, Kashiwa, Japan; Center for Spatial Information Science, The University of Tokyo, Kashiwa, Japan","IEEE Access","31 May 2021","2021","9","","77067","77078","Due to the limited training data, current data-driven algorithms, including deep convolutional networks (DCNs), are susceptible to training data that cannot be applied to new data directly. Unlike existing methods that are trying to improve model generation capability using limited data, we introduce a learning-based image translation method to generate data that share the same characteristics of target data. The low-resolution panchromatic satellite images are converted into high-resolution color images through interpolation and colorization with the proposed symmetric colorization network (SCN). Experiments on a very-high-resolution (VHR) dataset show that images generated by our SCN are with both quantitatively and qualitatively high color fidelity. Furthermore, we also demonstrate that high extraction accuracy is retained during the model transferring from aerial to satellite images. For pre-trained feature pyramid network (FPN), compared to the performance on raw panchromatic images, the interpolated and colorized images increase 305.7% of recall (0.929 vs. 0.229), 78.2% of overall accuracy (0.768 vs. 0.431), 132.5% of f1-score (0.851 vs. 0.366), and 230.8% of Jaccard index (0.741 vs. 0.224), respectively.","2169-3536","","10.1109/ACCESS.2021.3076933","Japan Science and Technology Agency (JST) aXis(grant numbers:JPMJAS2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420111","Panchromatic image colorization;deep learning;building extraction;image-to-image translation;model transfer","Satellites;Training;Buildings;Image color analysis;Testing;Image resolution;Data mining","feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","pre-trained feature pyramid network;raw panchromatic images;interpolated images increase 305;colorized images increase 305;panchromatic imagery;accurate building extraction;training data;current data-driven algorithms;deep convolutional networks;model generation capability;learning-based image translation method;low-resolution panchromatic satellite images;high-resolution color images;interpolation;symmetric colorization network;SCN;very-high-resolution dataset;quantitatively color fidelity;qualitatively high color fidelity;high extraction accuracy","","1","","50","CCBY","30 Apr 2021","","","IEEE","IEEE Journals"
"Pansharpening With Joint Local Low Rank Decomposition and Hierarchical Geometric Filtering","Y. Gao; C. Song; C. Yang; M. Wang; S. Yang","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of Artificial Intelligence, Xidian University, Xi’an, China; Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Access","20 Sep 2019","2019","7","","130578","130589","Extracting matched details of the PANchromatic (PAN) image and injecting them into the MultiSpectral (MS) images, is very crucial in pansharpening. In this paper, a new pansharpening method based on Joint Local Low Rank Decomposition (JLLRD) and Hierarchical Geometric Filtering (HGF) is proposed. First, a cascaded geometric filtering is performed on the PAN and MS images, to extract their multi-scale directional details. Then a joint local low rank decomposition is developed to deduce low-rank and sparse components for injection. Finally, an adaptive injection rule based on spectral correlation coefficient, is designed to further reduce spectral distortion of the fused images. Several experiments are taken to investigate the performance of the proposed JLLRD-HGF method, and the results show that it can extract more accurate injection details and produce less spectral and spatial distortions than its counterparts.","2169-3536","","10.1109/ACCESS.2019.2940482","National Natural Science Foundation of China(grant numbers:91438103,61771376,61771380); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834853","Pansharpening;joint local low-rank decomposition;hierarchical geometric filtering;spectral correlation coefficient","Filtering;Matrix decomposition;Sparse matrices;Distortion;Discrete wavelet transforms;Optimization","geophysical image processing;image fusion;image resolution;remote sensing","joint local low rank decomposition;Hierarchical Geometric Filtering;cascaded geometric filtering;PANchromatic image;MultiSpectral images","","1","","30","CCBY","12 Sep 2019","","","IEEE","IEEE Journals"
"Hyperspectral Pansharpening With Deep Priors","W. Xie; J. Lei; Y. Cui; Y. Li; Q. Du","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, USA","IEEE Transactions on Neural Networks and Learning Systems","1 May 2020","2020","31","5","1529","1543","Hyperspectral (HS) image can describe subtle differences in the spectral signatures of materials, but it has low spatial resolution limited by the existing technical and budget constraints. In this paper, we propose a promising HS pansharpening method with deep priors (HPDP) to fuse a low-resolution (LR) HS image with a high-resolution (HR) panchromatic (PAN) image. Different from the existing methods, we redefine the spectral response function (SRF) based on the larger eigenvalue of structure tensor (ST) matrix for the first time that is more in line with the characteristics of HS imaging. Then, we introduce HFNet to capture deep residual mapping of high frequency across the upsampled HS image and the PAN image in a band-by-band manner. Specifically, the learned residual mapping of high frequency is injected into the structural transformed HS images, which are the extracted deep priors served as additional constraint in a Sylvester equation to estimate the final HR HS image. Comparative analyses validate that the proposed HPDP method presents the superior pansharpening performance by ensuring higher quality both in spatial and spectral domains for all types of data sets. In addition, the HFNet is trained in the high-frequency domain based on multispectral (MS) images, which overcomes the sensitivity of deep neural network (DNN) to data sets acquired by different sensors and the difficulty of insufficient training samples for HS pansharpening.","2162-2388","","10.1109/TNNLS.2019.2920857","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Young Talent fund of University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8750899","Deep priors;high frequency;hyperspectral (HS) pansharpening;structure tensor (ST);sylvester equation","Bayes methods;Hyperspectral sensors;High frequency;Spatial resolution;Fuses;Imaging","eigenvalues and eigenfunctions;geophysical image processing;image fusion;image resolution;image sampling;learning (artificial intelligence);neural nets;remote sensing","hyperspectral pansharpening;hyperspectral image;low-resolution HS image;high-resolution panchromatic image;spectral response function;structure tensor matrix;HS imaging;deep residual mapping;PAN image;band-by-band manner;learned residual mapping;structural transformed HS images;HPDP method;spatial domains;spectral domains;high-frequency domain;multispectral images;deep neural network;HR HS image;HS pansharpening method","","56","","48","IEEE","28 Jun 2019","","","IEEE","IEEE Journals"
"Visual Saliency Modeling for River Detection in High-Resolution SAR Imagery","F. Gao; F. Ma; J. Wang; J. Sun; E. Yang; H. Zhou","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Space Mechatronic Systems Technology Laboratory, Manufacture and Engineering Management, Strathclyde Space Institute, University of Strathclyde, Glasgow, U.K.; School of Electronic, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.","IEEE Access","14 Feb 2018","2018","6","","1000","1014","Accurate detection of rivers plays a significant role in water conservancy construction and ecological protection, where airborne synthetic aperture radar (SAR) data have already become one of the main sources. However, extracting river information from radar data efficiently and accurately still remains an open problem. The existing methods for detecting rivers are typically based on rivers’ edges, which are easily mixed with those of artificial buildings or farmland. In addition, pixel-based image processing approaches cannot meet the requirement of real-time processing. Inspired by the feature integration and target recognition capabilities of biological vision systems, in this paper, we present a hierarchical method for automated detection of river networks in the high-resolution SAR data using biologically visual saliency modeling. For effective saliency detection, the original image is first over-segmented into a set of primitive superpixels. A visual feature set is designed to extract a regional feature histogram, which is then quantized based on the optimal parameters learned from the labeled SAR images. Afterward, three saliency measurements based on the specificity of the rivers in the SAR images are proposed to generate a single layer saliency map, i.e., local region contrast, boundary connectivity, and edge density. Finally, by exploiting belief propagation, we propose a multi-layer saliency fusion approach to derive a high-quality saliency map. Extensive experimental results on three airborne SAR image data sets with the ground truth demonstrate that the proposed saliency model consistently outperforms the existing saliency target detection models.","2169-3536","","10.1109/ACCESS.2017.2777444","National Natural Science Foundation of China(grant numbers:61771027,61071139,61471019,61501011,61171122); Aeronautical Science Foundation of China(grant numbers:20142051022); Pre-Research Project(grant numbers:9140A07040515HK01009); RSE-NNSFC Joint Project (2017–2019) with the China University of Petroleum (Huadong)(grant numbers:6161101383); U.K. EPSRC(grant numbers:EP/N508664/1,EP/R007187/1,EP/N011074/1); Royal Society-Newton Advanced Fellowship(grant numbers:NA160342); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119784","Synthetic aperture radar (SAR);remote sensing;rivers;object detection;biological system modeling","Feature extraction;Synthetic aperture radar;Rivers;Merging;Visualization;Filtering algorithms;Dogs","airborne radar;feature extraction;geophysical image processing;geophysics computing;hydrological techniques;image colour analysis;image fusion;image resolution;image segmentation;object detection;radar imaging;rivers;synthetic aperture radar","river networks;biologically visual saliency modeling;visual feature set;regional feature histogram;labeled SAR images;saliency measurements;single layer saliency map;local region contrast;edge density;multilayer saliency fusion approach;high-quality saliency map;airborne SAR image data sets;river detection;high-resolution SAR imagery;water conservancy construction;ecological protection;airborne synthetic aperture radar data;pixel-based image processing approaches;biological vision systems;hierarchical method;automated detection;saliency detection;saliency target detection models;belief propagation","","47","","54","OAPA","24 Nov 2017","","","IEEE","IEEE Journals"
"A Unified Pansharpening Model Based on Band-Adaptive Gradient and Detail Correction","H. Lu; Y. Yang; S. Huang; W. Tu; W. Wan","College of Information Engineering, Jinhua Polytechnic, Jinhua, China; School of Computer Science and Technology, Tiangong University, Tianjin, China; School of Computer Science and Technology, Tiangong University, Tianjin, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Transactions on Image Processing","6 Jan 2022","2022","31","","918","933","Pansharpening is used to fuse a panchromatic (PAN) image with a multispectral (MS) image to obtain a high-spatial-resolution multispectral (HRMS) image. Traditional pansharpening methods face difficulties in obtaining accurate details and have low computational efficiency. In this study, a unified pansharpening model based on the band-adaptive gradient and detail correction is proposed. First, a spectral fidelity constraint is designed by keeping each band of the HRMS image consistent with that of the MS image. Then, a band-adaptive gradient correction model is constructed by exploring the gradient relationship between a PAN image and each band of the MS image, so as to adaptively obtain an accurate spatial structure for the estimated HRMS image. To refine the spatial details, a detail correction constraint is defined based on the parameter transfer by designing a reduced-scale parameter acquisition model. Finally, a unified model is constructed based on the gradient and detail corrections, which is then solved by an alternating direction multiplier method. Both reduced-scale and full-scale experiments are conducted on several datasets. Compared with state-of-the-art pansharpening methods, the proposed method can achieve the best results in terms of fusion quality and has high efficiency. Specifically, our method improves the SAM and ERGAS metrics by 17.6% and 21.2% respectively compared to the traditional approach with the best average values, and improves these two metrics by 4.3% and 10.3% respectively compared to the learning-based approach with the best average values.","1941-0042","","10.1109/TIP.2021.3137020","National Natural Science Foundation of China(grant numbers:62072218,61862030); Natural Science Foundation of Zhejiang Province(grant numbers:LY22F020017); Talent Project of Jiangxi Thousand Talents Program(grant numbers:jxsq2019201056); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664491","Pansharpening;band-adaptive;gradient correction;detail correction;parameter transfer","Pansharpening;Adaptation models;Wavelet transforms;Distortion;Spatial resolution;Satellites;Optimization","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing","band-adaptive gradient correction model;gradient relationship;PAN image;MS image;accurate spatial structure;estimated HRMS image;spatial details;detail correction constraint;reduced-scale parameter acquisition model;unified model;detail corrections;full-scale experiments;state-of-the-art pansharpening methods;unified pansharpening model;panchromatic image;high-spatial-resolution multispectral image;traditional pansharpening methods","","10","","45","IEEE","28 Dec 2021","","","IEEE","IEEE Journals"
"Terrain Matching by Fusing HOG With Zernike Moments","K. Wang; H. Wang; J. Wang","Beihang University, Beijing, China; Beihang University, Beijing, China; University of New South Wales Australia, Sydney, Australia","IEEE Transactions on Aerospace and Electronic Systems","10 Apr 2020","2020","56","2","1290","1300","A new matching algorithm is proposed by fusing the histogram of oriented gradients' and the Zernike moments' descriptors extracted from the real-time synthetic aperture radar (SAR) images and the real-time elevation maps (REMs), respectively. Both the SAR images and the REMs are acquired by an on-board interferometric SAR simultaneously. The two reciprocal descriptors are fused by weighting their Canberra distances. The numerical experiments demonstrate the advantage of the proposed matching algorithm preliminarily.","1557-9603","","10.1109/TAES.2019.2930016","National Natural Science Foundation of China(grant numbers:41774014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770163","Histogram of oriented gradient (HOG);scene matching;terrain-aided navigation (TAN);terrain matching;Zernike moments (ZMs).","Feature extraction;Histograms;Radar polarimetry;Synthetic aperture radar;Real-time systems;Classification algorithms;Detectors","feature extraction;image fusion;image matching;radar imaging;radar interferometry;remote sensing by radar;synthetic aperture radar;terrain mapping","Canberra distances;histogram of oriented gradient fusion;HOG fusion;SAR images;real-time elevation maps;real-time synthetic aperture radar images;Zernike moment descriptor;terrain matching;matching algorithm;reciprocal descriptors;on-board interferometric SAR;REMs","","4","","36","IEEE","23 Jul 2019","","","IEEE","IEEE Journals"
"Unsupervised Hyperspectral and Multispectral Images Fusion Based on Nonlinear Variational Probabilistic Generative Model","Z. Wang; B. Chen; H. Zhang; H. Liu","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Transactions on Neural Networks and Learning Systems","3 Feb 2022","2022","33","2","721","735","Due to hardware limitations, it is challenging for sensors to acquire images of high resolution in both spatial and spectral domains, which arouses a trend that utilizing a low-resolution hyperspectral image (LR-HSI) and a high-resolution multispectral image (HR-MSI) to fuse an HR-HSI in an unsupervised manner. Considering the fact that most existing methods are restricted by using linear spectral unmixing, we propose a nonlinear variational probabilistic generative model (NVPGM) for the unsupervised fusion task based on nonlinear unmixing. We model the joint full likelihood of the observed pixels in an LR-HSI and an HR-MSI, both of which are assumed to be generated from the corresponding latent representations, i.e., the abundance vectors. The sufficient statistics of the generative conditional distributions are nonlinear functions with respect to the latent variable, realized by neural networks, which results in a nonlinear spectral mixture model. For scalability and efficiency, we construct two recognition models to infer the latent representations, which are parameterized by neural networks as well. Simultaneously inferring the latent representations and optimizing the parameters are achieved using stochastic gradient variational inference, after which the target HR-HSI is retrieved via feedforward mapping. Though without supervised information about the HR-HSI, NVPGM still can be trained based on extra LR-HSI and HR-MSI data sets in advance unsupervisedly and processes the images at the test phase in real time. Three commonly used data sets are used to evaluate the effectiveness and efficiency of NVPGM, illustrating the outperformance of NVPGM in the unsupervised LR-HSI and HR-MSI fusion task.","2162-2388","","10.1109/TNNLS.2020.3028772","Program for Oversea Talent by the Chinese Central Government; Higher Education Discipline Innovation Project(grant numbers:B18039); NSFC(grant numbers:61771361); NSFC for Distinguished Young Scholars(grant numbers:61525105); Shaanxi Innovation Team Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246732","Hyperspectral image (HSI);multispectral image (MSI);nonlinear fusion;probabilistic generative model;super-resolution","Hyperspectral imaging;Spatial resolution;Probabilistic logic;Task analysis;Sensors","feature extraction;geophysical image processing;hyperspectral imaging;image colour analysis;image fusion;image resolution;image sensors;remote sensing;spectral analysis;unsupervised learning","nonlinear variational probabilistic generative model;spatial domains;spectral domains;low-resolution hyperspectral image;high-resolution multispectral image;linear spectral unmixing;NVPGM;unsupervised fusion task;nonlinear unmixing;generative conditional distributions;nonlinear functions;neural networks;nonlinear spectral mixture model;recognition models;stochastic gradient variational inference;target HR-HSI;extra LR-HSI;unsupervised LR-HSI;HR-MSI fusion task","","3","","52","IEEE","2 Nov 2020","","","IEEE","IEEE Journals"
"Residual Dense Network for Pan-Sharpening Satellite Data","D. S. Vinothini; B. S. Bama","Department of Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Electronics and Communication Engineering, Thiagarajar College of Engineering, Madurai, India","IEEE Sensors Journal","4 Dec 2019","2019","19","24","12279","12285","Pan-sharpening is a multi-sensor fusion task that aims to enhance the spatial resolution of spectral data using panchromatic data of the same scene. This work proposes a deep Residual Dense Model (RDM) for Pan-Sharpening (PS) of satellite data which learns hierarchical features that can efficiently represent the local complex structures from panchromatic data. This work addresses the two general problems emphasized in pan-sharpening application viz., spectral and spatial preservation. The proposed Residual Dense Model for Pan-Sharpening network (RDMPSnet), preserves the spectral information by spectral mapping of Low-Resolution Multi-Spectral data (LRMS) while the spatial preservation is achieved by learning the hierarchical structural features from High-Resolution Panchromatic data (HRP). To extract this structural feature RDMPSnet is trained end to end with Low Resolution (LR) panchromatic patches and High Resolution (HR) residue patches to learn a non-linear mapping. The trained non-linear mapping network is capable to generate structural feature for any LRMS data which is injected into the mapped spectral data. The network is experimentally evaluated with Worldview2 and IKONOS2 satellite data and shows that the proposed RDMPS achieves favorable performance both visually and quantitatively against state-of-the-art methods.","1558-1748","","10.1109/JSEN.2019.2939844","University Grants Commission(grant numbers:F117.1/201516/MANF201517TAM-55205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826277","Satellite data;pan-sharpening;multi-spectral image;panchromatic image;deep learning","Training;Image reconstruction;Spatial resolution;Feature extraction;Satellites;Convolution;Sensors","feature extraction;geophysical image processing;image classification;image fusion;image reconstruction;image resolution;learning (artificial intelligence);remote sensing;sensor fusion;terrain mapping","pan-sharpening application viz;spatial preservation;Pan-Sharpening network;spectral information;spectral mapping;Low-Resolution MultiSpectral data;hierarchical structural features;High-Resolution Panchromatic data;structural feature RDMPSnet;Low Resolution panchromatic patches;High Resolution residue patches;nonlinear mapping network;LRMS data;mapped spectral data;IKONOS2 satellite data;Residual Dense network;Pan-Sharpening satellite data;multisensor fusion task;spatial resolution;deep Residual Dense Model;hierarchical features;local complex structures","","1","","20","IEEE","6 Sep 2019","","","IEEE","IEEE Journals"
"Capitalizing on RGB-FIR Hybrid Imaging for Road Detection","Y. Zhang; J. Xie; J. M. Álvarez; C. -Z. Xu; J. Yang; H. Kong","Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; NVIDIA Corporation, Santa Clara, CA, USA; Department of Computer and Information Science, State Key Laboratory of Internet of Things for Smart City (SKL-IOTSC), University of Macau, Macau; Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Electromechanical Engineering (EME), State Key Laboratory of Internet of Things for Smart City (SKL-IOTSC), University of Macau, Macau","IEEE Transactions on Intelligent Transportation Systems","10 Aug 2022","2022","23","8","13819","13834","Traditionally, road detection approaches mostly capitalize on RGB images, 3D LiDAR point cloud or their fusion. However, RGB camera is sensitive to light conditions, while LiDAR point cloud is sparse compared with dense image pixels. In this work, a new hybrid image dataset is provided for the task of road detection based on cameras. In this dataset, the hybrid images are acquired by an optically aligned hybrid imaging device, consisting of a far-infrared (FIR) imager and an RGB camera to output pixel-wise registration of thermal and RGB frames. Then we investigate on three methods based on fully convolutional neural network (F-CNN) to demonstrate the advantages by fusing RGB-FIR images in road detection. First, a middle-fusion based model is built, where the output feature maps of encoder branches from RGB and FIR images are directly concatenated into a single-fusion branch as the decoder. Next, the originally discarded layers after fusion operation for both RGB and FIR branches are recovered as the mimic branches to imitate the distributions of the fusion outputs, which constitutes an extended cross model (ECM). Moreover, the outputs of mimic branches at different scales are also used to imitate the corresponding outputs in the fusion branch, called a hierarchical cross model (HCM). The experimental results demonstrate the effectiveness and efficiency of our fusion strategies.","1558-0016","","10.1109/TITS.2021.3129692","National Natural Science Foundation of China(grant numbers:U1713208); Program for Changjiang Scholars; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629380","RGB-FIR fusion;road detection;CNN;extended cross model;hierarchical cross model","Roads;Sensors;Finite impulse response filters;Laser radar;Cameras;Feature extraction;Three-dimensional displays","cameras;convolutional neural nets;feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;image registration;image resolution;image segmentation;image sensors;object detection;optical radar;remote sensing;traffic engineering computing","RGB-FIR hybrid imaging;road detection;3D LiDAR point cloud;RGB camera;dense image pixels;hybrid image dataset;optically aligned hybrid imaging device;far-infrared imager;output pixel-wise registration;thermal RGB frames;middle-fusion based model;single-fusion branch;fusion operation;fusion outputs;fully convolutional neural network;F-CNN;extended cross model;ECM;hierarchical cross model;HCM","","","","65","IEEE","30 Nov 2021","","","IEEE","IEEE Journals"
"Building Damage Detection via Superpixel-Based Belief Fusion of Space-Borne SAR and Optical Images","X. Jiang; Y. He; G. Li; Y. Liu; X. -P. Zhang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Institute of Information Fusion, Naval Aeronautical University, Yantai, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada","IEEE Sensors Journal","24 Jan 2020","2020","20","4","2008","2022","Space-borne synthetic aperture radar (SAR) and optical sensors are important tools for building damage detection. Fusion of SAR and optical images improves detection performance. However, when the resolutions of the two different kinds of images differ, the performance of the existing pixel-level fusion methods deteriorates significantly due to interpolation-induced distortion. To solve this problem, this paper presents a new superpixel-based belief fusion (SBBF) model for building damage detection. The superpixels on the SAR and optical images are identified by the segmentation on the pre-earthquake optical image to perform the fusion on the superpixel-level instead of the pixel-level in existing methods. Then in the fusion stage, different from the commonly used direct fusion methods that do not consider the reliability in the fusion process, a novel belief fusion method that employs a basic belief assignment (BBA) to incorporate different reliabilities of superpixels is proposed to improve the accuracy of building damage detection. For each superpixel, the BBA is assigned based on the influence of noise and resolutions. The United Nations Operational Satellite Applications Programme (UNOSAT) datasets corresponding to the 2010 Haiti earthquake and the 2011 Tōhoku earthquake, are used to evaluate the performance of the proposed method. The experimental results show that the proposed method achieves significantly better performance than existing separate SAR or optical images based methods, and the existing pixel-level fusion methods.","1558-1748","","10.1109/JSEN.2019.2948582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8878121","Building damage detection;synthetic aperture radar (SAR);optical remote sensing;superpixel;fusion","Optical distortion;Optical sensors;Optical imaging;Synthetic aperture radar;Image resolution;Buildings;Optical reflection","buildings (structures);earthquakes;geophysical image processing;geophysical techniques;image fusion;image resolution;image segmentation;optical images;radar imaging;spaceborne radar;synthetic aperture radar","AD 2011;Haiti earthquake;AD 2010;United Nations Operational Satellite Applications Programme datasets;basic belief assignment;direct fusion methods;space-borne synthetic aperture radar;novel belief fusion method;fusion process;fusion stage;superpixel-level;pre-earthquake optical image;superpixel-based belief fusion model;pixel-level fusion methods;detection performance;optical sensors;space-borne SAR;building damage detection","","16","","42","IEEE","21 Oct 2019","","","IEEE","IEEE Journals"
"Adaptive Resource Optimized Edge Federated Learning in Real-Time Image Sensing Classifications","P. Tam; S. Math; C. Nam; S. Kim","Department of Software Convergence, Soonchunhyang University, Asan, South Korea; Department of Software Convergence, Soonchunhyang University, Asan, South Korea; Department of Software Convergence, Soonchunhyang University, Asan, South Korea; Department of Computer Software Engineering, Soonchunhyang University, Asan, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","10 Nov 2021","2021","14","","10929","10940","With the exponential growth of the Internet of things (IoT) in remote sensing image applications, network resource orchestration and data privacy are significant aspects to handle in bigdata cellular networks. The image data sharing procedure toward central cloud servers in order to perform real-time classifications has leaked client personalization and heavily burdened the communication networks. Thus, the deployment of IoT image sensors in privacy-constrained sectors requires an optimized federated learning (FL) scheme to efficiently consider both aspects of securing data privacy and maximizing the model accuracy with sufficient communication and computation resources. In this article, an adaptive model communication scheme with virtual resource optimization for edge FL is proposed by converging a deep q-learning algorithm to enforce a self-learning agent interacting with network functions virtualization orchestrator and software-defined networking based architecture. The agent targets to optimize the resource control policy of virtual multi-access edge computing entities in virtualized infrastructure manager. The proposed scheme trains the learning model and weighs the optimal actions for particular network states by using an epsilon-greedy strategy. In the exploitation phase, the scheme considers multiple spatial-resolution sensing conditions and allocates computation offloading resources for global multiconvolutional neural networks model aggregation based on the congestion states. In the simulation results, the quality of service and global collaborative model performance metrics were evaluated in terms of delay, packet drop ratios, packet delivery ratios, loss values, and overall accuracy.","2151-1535","","10.1109/JSTARS.2021.3120724","Bio and Medical Technology Development Program; National Research Foundation; Korean government(grant numbers:NRF-2019M3E5D1A02069073); Ministry of Science, ICT; National Program for Excellence in SW; Institute of Information & Communications Technology Planning & Evaluation(grant numbers:2021-0-01399); Soonchunhyang University Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580651","Convolutional neural networks (CNN);deep q-learning (DQL);federated learning (FL);quality of service (QoS);real-time image classifications","Computational modeling;Sensors;Data models;Servers;Real-time systems;Adaptation models;Resource management","Big Data;cellular radio;cloud computing;computer network management;computer network security;convolutional neural nets;data privacy;deep learning (artificial intelligence);greedy algorithms;image classification;Internet of Things;multi-access systems;resource allocation;software defined networking;virtualisation","adaptive resource optimized edge federated learning;real-time image sensing classifications;remote sensing image applications;network resource orchestration;bigdata cellular networks;image data;central cloud servers;communication networks;IoT image sensors;privacy-constrained sectors;federated learning scheme;securing data privacy;adaptive model communication scheme;virtual resource optimization;deep q-learning algorithm;self-learning agent;network functions virtualization orchestrator;resource control policy;virtualized infrastructure manager;learning model;multiple spatial-resolution sensing conditions;computation offloading resources;global multiconvolutional neural networks model aggregation;global collaborative model performance metrics;virtual multiaccess edge computing;epsilon-greedy strategy;software-defined networking based architecture","","7","","47","CCBY","19 Oct 2021","","","IEEE","IEEE Journals"
"Using POI Data and Baidu Migration Big Data to Modify Nighttime Light Data to Identify Urban and Rural Area","Y. Chen; A. Deng","School of CML Engineering Architecture, Zhejiang Guangsha Vocational and Technical University of Construction, Dongyang, China; Architecture and Planning, Sudan University of Science & Technology, Khartoum, Sudan","IEEE Access","13 Sep 2022","2022","10","","93513","93524","The spatial difference between urban and rural areas is the direct result of urban-rural relations. Accurate identification of urban-rural area is helpful to judge the urban-rural mechanism and promote the integration development of urban-rural area. Previous studies only used single nighttime light (NTL) data to identify urban and rural areas, which is likely to have an impact on the identification results due to the large brightness difference of lights. Therefore, based on NTL data and combine with data level fusion algorithm, this study separately fuses point of interest (POI) data that representing the quantity distribution of urban infrastructure and Baidu migration big (BM)data that representing the change relationship of regional population mobility to identify urban and rural areas by using deep learning method. The results show that the highest accuracy of urban-rural spatial identification with single NTL data is 84.32% and kappa is 0.6952, while the highest accuracy identified by data fusion is 95.02% and kappa is 0.8259. It can be seen that the differences caused by light brightness are effectively corrected after data fusion, which greatly improves the accuracy of urban and rural spatial identification. By comparing the results of NTL data modified by different big data, this study analyzes and identifies the accuracy of urban and rural area by using deep learning method, which not only enriches the study of data fusion in urban area, but also provides a basis for analyzing regional urban-rural relations and urban-rural development. Therefore, this study is believed to have important practical value for the coordinated development of urban and rural areas.","2169-3536","","10.1109/ACCESS.2022.3203433","Colonel-Level Project-Establishment of Zhejiang Guangsha Vocational and Technical University of Construction in 2022: Study on Extraction of Built-Up Areas in Zhejiang City Group Based on the Fusion of Multi-Source Remote-Sensing Big Data(grant numbers:2022QNPY007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874787","Urban-rural difference;POI;night light;BM~data;Zhengzhou","Urban areas;Statistics;Sociology;Spatial resolution;Image fusion;Web and internet services;Data integration","Big Data;deep learning (artificial intelligence);sensor fusion;town and country planning","POI data;modify nighttime light data;urban-rural relations;urban-rural area;urban-rural mechanism;single nighttime light data;data level fusion algorithm;urban infrastructure;urban-rural spatial identification;single NTL data;data fusion;urban-rural development;Baidu migration Big Data;deep learning method;urban-and-rural spatial identification;point-of-interest data","","1","","83","CCBY","1 Sep 2022","","","IEEE","IEEE Journals"
"Multi-Temporal Ultra Dense Memory Network for Video Super-Resolution","P. Yi; Z. Wang; K. Jiang; Z. Shao; J. Ma","National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; National Engineering Research Center for Multimedia Software, School of Computer Science, Wuhan University, Wuhan, China; State Key Laboratory for Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Circuits and Systems for Video Technology","3 Aug 2020","2020","30","8","2503","2516","Video super-resolution (SR) aims to reconstruct the corresponding high-resolution (HR) frames from consecutive low-resolution (LR) frames. It is crucial for video SR to harness both inter-frame temporal correlations and intra-frame spatial correlations among frames. Previous video SR methods based on convolutional neural network (CNN) mostly adopt a single-channel structure and a single memory module, so they are unable to fully exploit inter-frame temporal correlations specific for video. To this end, this paper proposes a multi-temporal ultra-dense memory (MTUDM) network for video super-resolution. Particularly, we embed convolutional long-short-term memory (ConvLSTM) into ultra-dense residual block (UDRB) to construct an ultra-dense memory block (UDMB) for extracting and retaining spatio-temporal correlations. This design also reduces the layer depth by expanding the width, thus avoiding training difficulties, such as gradient exploding and vanishing under a large model. We further adopt multi-temporal information fusion (MTIF) strategy to merge the extracted temporal feature maps in consecutive frames, improving the accuracy without requiring much extra computational cost. The experimental results on extensive public datasets demonstrate that our method outperforms the state-of-the-art methods by a large margin.","1558-2205","","10.1109/TCSVT.2019.2925844","National Key Research and Development Project(grant numbers:2016YFE0202300); National Natural Science Foundation of China(grant numbers:61671332,U1736206,61773295); Hubei Technological Innovation Special Fund(grant numbers:2017AAA123); Natural Science Fund of Hubei Province(grant numbers:2018CFA024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8752034","Convolutional neural network (CNN);video super-resolution;ultra dense memory block (UDRB);multi-temporal fusion","Correlation;Feature extraction;Image reconstruction;Neural networks;Motion estimation","convolutional neural nets;feature extraction;image fusion;image reconstruction;image resolution;learning (artificial intelligence);motion estimation;recurrent neural nets;spatiotemporal phenomena;video signal processing","convolutional neural network;single memory module;inter-frame temporal correlations;multitemporal ultra-dense memory network;video super-resolution;ultra-dense residual block;ultra-dense memory block;spatio-temporal correlations;multitemporal information fusion strategy;extracted temporal feature maps;consecutive frames;multitemporal ultra dense memory network;high-resolution frames;low-resolution frames;intra-frame spatial correlations;video SR methods;UDMB;convolutional long-short-term memory;ConvLSTM;MTUDM;CNN;MTIF","","76","","54","IEEE","1 Jul 2019","","","IEEE","IEEE Journals"
"Weighted Feature Fusion of Convolutional Neural Network and Graph Attention Network for Hyperspectral Image Classification","Y. Dong; Q. Liu; B. Du; L. Zhang","Hubei Subsurface Multi-scale Imaging Key Laboratory, Institute of Geophysics and Geomatics, China University of Geosciences, Wuhan, China; Hubei Subsurface Multi-scale Imaging Key Laboratory, Institute of Geophysics and Geomatics, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Image Processing","1 Feb 2022","2022","31","","1559","1572","Convolutional Neural Networks (CNN) and Graph Neural Networks (GNN), such as Graph Attention Networks (GAT), are two classic neural network models, which are applied to the processing of grid data and graph data respectively. They have achieved outstanding performance in hyperspectral images (HSIs) classification field, which have attracted great interest. However, CNN has been facing the problem of small samples and GNN has to pay a huge computational cost, which restrict the performance of the two models. In this paper, we propose Weighted Feature Fusion of Convolutional Neural Network and Graph Attention Network (WFCG) for HSI classification, by using the characteristics of superpixel-based GAT and pixel-based CNN, which proved to be complementary. We first establish GAT with the help of superpixel-based encoder and decoder modules. Then we combined the attention mechanism to construct CNN. Finally, the features are weighted fusion with the characteristics of two neural network models. Rigorous experiments on three real-world HSI data sets show WFCG can fully explore the high-dimensional feature of HSI, and obtain competitive results compared to other state-of-the art methods.","1941-0042","","10.1109/TIP.2022.3144017","National Natural Science Foundation of China(grant numbers:62171417,41871243); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)(grant numbers:2019AEA170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693311","Hyperspectral image classification;convolutional neural network;graph attention network;weighted feature fusion;attention mechanism","Feature extraction;Convolutional neural networks;Training;Hyperspectral imaging;Data mining;Decoding;Data models","convolutional neural nets;geophysical image processing;graph theory;hyperspectral imaging;image classification;image fusion","CNN;graph neural networks;graph attention network;graph data;weighted feature fusion;convolutional neural network;hyperspectral image classification;grid data;WFCG;HSI classification;superpixel-based GAT;pixel-based CNN;superpixel-based encoder modules;superpixel-based decoder modules","Neural Networks, Computer","33","","55","IEEE","25 Jan 2022","","","IEEE","IEEE Journals"
"HRSiam: High-Resolution Siamese Network, Towards Space-Borne Satellite Video Tracking","J. Shao; B. Du; C. Wu; M. Gong; T. Liu","Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; School of Mathematics and Statistics, The University of Melbourne, Melbourne, VIC, Australia; School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW, Australia","IEEE Transactions on Image Processing","24 Feb 2021","2021","30","","3056","3068","Tracking moving objects from space-borne satellite videos is a new and challenging task. The main difficulty stems from the extremely small size of the target of interest. First, because the target usually occupies only a few pixels, it is hard to obtain discriminative appearance features. Second, the small object can easily suffer from occlusion and illumination variation, making the features of objects less distinguishable from features in surrounding regions. Current state-of-the-art tracking approaches mainly consider high-level deep features of a single frame with low spatial resolution, and hardly benefit from inter-frame motion information inherent in videos. Thus, they fail to accurately locate such small objects and handle challenging scenarios in satellite videos. In this article, we successfully design a lightweight parallel network with a high spatial resolution to locate the small objects in satellite videos. This architecture guarantees real-time and precise localization when applied to the Siamese Trackers. Moreover, a pixel-level refining model based on online moving object detection and adaptive fusion is proposed to enhance the tracking robustness in satellite videos. It models the video sequence in time to detect the moving targets in pixels and has ability to take full advantage of tracking and detecting. We conduct quantitative experiments on real satellite video datasets, and the results show the proposed HIGH-RESOLUTION SIAMESE NETWORK (HRSiam) achieves state-of-the-art tracking performance while running at over 30 FPS.","1941-0042","","10.1109/TIP.2020.3045634","National Natural Science Foundation of China(grant numbers:61822113,41871243,61971317); Natural Science Foundation of Hubei Province(grant numbers:2018CFA050,2020CFB594); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)(grant numbers:2019AEA170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9350236","Satellite videos;object tracking;siamese network;high spatial-resolution representation;gaussian mixture model","Satellites;Target tracking;Spatial resolution;Object tracking;Tracking;Robustness;Adaptation models","artificial satellites;computer vision;feature extraction;image fusion;image motion analysis;image resolution;image sequences;neural nets;object detection;object tracking;real-time systems;target tracking;video signal processing","moving object detection;moving object tracking;video sequence;moving target detection;HRSiam;high resolution siamese network;discriminative appearance features;spatial resolution;interframe motion information;space borne satellite video tracking;pixel level refining model;real time localization;adaptive fusion;Siamese Trackers","","23","","54","IEEE","8 Feb 2021","","","IEEE","IEEE Journals"
"Line-Based 2-D–3-D Registration and Camera Localization in Structured Environments","H. Yu; W. Zhen; W. Yang; S. Scherer","Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing (LIESMARS), Wuhan University, Wuhan, China; Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA","IEEE Transactions on Instrumentation and Measurement","8 Oct 2020","2020","69","11","8962","8972","Accurate registration of 2-D imagery with point clouds is a key technology for image-Light Detection and Ranging (LiDAR) point cloud fusion, camera to laser scanner calibration, and camera localization. Despite continuous improvements, automatic registration of 2-D and 3-D data without using additional textured information still faces great challenges. In this article, we propose a new 2-D-3-D registration method to estimate 2-D-3-D line feature correspondences and the camera pose in untextured point clouds of structured environments. Specifically, we first use geometric constraints between vanishing points and 3-D parallel lines to compute all feasible camera rotations. Then, we utilize a hypothesis testing strategy to estimate the 2-D-3-D line correspondences and the translation vector. By checking the consistency with computed correspondences, the best rotation matrix can be found. Finally, the camera pose is further refined using nonlinear optimization with all the 2-D-3-D line correspondences. The experimental results demonstrate the effectiveness of the proposed method on the synthetic and real data set (outdoors and indoors) with repeated structures and rapid depth changes.","1557-9662","","10.1109/TIM.2020.2999137","Project for Innovative Research Groups of the Natural Science Foundation of Hubei Province, China(grant numbers:2018CFA006); Shimizu Institute of Technology, Japan(grant numbers:Japan); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106407","2-D line detection;2-D–3-D line correspondences;2-D-3-D registration;camera localization;camera-Light Detection and Ranging (LiDAR) fusion;vanishing point matching","Three-dimensional displays;Cameras;Two dimensional displays;Feature extraction;Laser radar;Image segmentation;Visualization","calibration;cameras;image fusion;image registration;image sensors;image texture;optical radar;optimisation","line-based 2-D-3-D registration;camera localization;structured environments;automatic registration;2-D-3-D registration method;2-D-3-D line feature correspondences;untextured point clouds;3-D parallel lines;feasible camera rotations;2-D-3-D line correspondences","","20","","48","IEEE","2 Jun 2020","","","IEEE","IEEE Journals"
"A Multifeature-Assisted Road and Vehicle Detection Method Based on Monocular Depth Estimation and Refined U-V Disparity Mapping","W. Ma; S. Zhu","School of Computer Science and Information Engineering, Hubei University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Intelligent Transportation Systems","14 Sep 2022","2022","23","9","16763","16772","Usually, the detection process of traffic objects, such as vehicles, finds the visual input lacks the necessary depth information, so it is difficult to directly and quickly obtain the results. Typically, to separate the objects from the complex background, it is necessary to utilize a complex model or prior knowledge, which can be computationally expensive or simply infeasible. To battle this issue, depth visual information is used in this paper to accurately segment roads and vehicles, so it doesn’t need to use complex models to detect objects in the visual input. First, an unsupervised deep learning-based monocular depth estimation method is used to obtain the stereo disparity map. Then a non-parametric, refined U-V disparity mapping method is used to obtain the road region of interest. Next, this paper uses the road parallel scanning to determine the source and vanishing points and uses the adjacent disparity similarity algorithm to complement and extract the target region to detect roads and vehicles. This algorithm uses multi-feature fusion such as height-width ratio, perspective ratio, and area ratio to accurately segment the target region, and the effectiveness of the proposed method is tested on a public dataset. The experimental results show that the proposed model can accurately and efficiently detect roads and vehicles in a variety of scenarios.","1558-0016","","10.1109/TITS.2022.3195297","National Key Research and Development Program of China(grant numbers:2017YFB1302400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857629","Deep learning;U-V disparity mapping;depth estimation;road and vehicle detection","Roads;Estimation;Vehicle detection;Feature extraction;Cameras;Detection algorithms;Computational modeling","computer vision;feature extraction;image fusion;image segmentation;learning (artificial intelligence);object detection;roads;robot vision;stereo image processing;traffic engineering computing;unsupervised learning","vehicle detection method;detection process;traffic objects;visual input;necessary depth information;complex background;complex model;depth visual information;accurately segment roads;complex models;unsupervised deep learning-based monocular depth estimation method;stereo disparity map;refined U-V disparity mapping method;adjacent disparity similarity algorithm;target region;multifeature-assisted road","","","","37","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Target Three-Dimensional Reconstruction From the Multi-View Radar Image Sequence","Y. Zhou; L. Zhang; C. Xing; P. Xie; Y. Cao","Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China","IEEE Access","1 Apr 2019","2019","7","","36722","36735","Target three-dimensional (3D) reconstruction is a hot topic and also a challenge in remote sensing applications. In this paper, a new reconstruction algorithm is proposed to reconstruct the 3D surface of the stable attitude target from its multi-view radar image sequence. Uniform explicit expression of the radar and optical imaging geometries is derived to bridge the 3D target structure and these two sorts of observation images. In this way, the visual hull of the target is reconstructed by exploiting the multi-view stereo techniques to the silhouette information extracted from the radar image sequence. Meanwhile, the target absolute attitude is also determined. Furthermore, we analyze the primary difficulty of the method induced from the limited radar observation view in a typical application, the 3D reconstruction of an in-orbit satellite. Then, an extended algorithm is proposed with the feature fusion of the radar and optical images to achieve dramatic performance enhancement of the reconstruction in this condition. The feasibility of the proposed algorithm is confirmed in the experiment part, and some conclusions are drawn to guide the future work about extended applications of the proposed algorithm as well.","2169-3536","","10.1109/ACCESS.2019.2905130","National Natural Science Foundation of China(grant numbers:61771372,61771367); National Science Foundation of Shanghai(grant numbers:1428700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667102","Radar imaging;geometrical projection;three-dimensional reconstruction;feature fusion of the radar and optical images","Radar imaging;Spaceborne radar;Three-dimensional displays;Laser radar;Optical sensors;Imaging","feature extraction;image enhancement;image fusion;image motion analysis;image reconstruction;image sequences;optical images;optical information processing;radar imaging;stereo image processing;surface reconstruction","optical images;multiview radar image sequence;remote sensing applications;reconstruction algorithm;stable attitude target;optical imaging geometries;observation images;multiview stereo techniques;target absolute attitude;radar observation view;target three-dimensional reconstruction;3D surface reconstruction;uniform explicit expression;visual hull;silhouette information extraction;in-orbit satellite;feature fusion;performance enhancement","","11","","44","OAPA","14 Mar 2019","","","IEEE","IEEE Journals"
"Fast Fusion-Based Dehazing With Histogram Modification and Improved Atmospheric Illumination Prior","F. Huo; X. Zhu; H. Zeng; Q. Liu; J. Qiu","State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China; State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China; Southwest UAV Science and Technology Company Ltd., Luzhou, China; State Key Laboratory of Power Transmission Equipment and System Security and New Technology, Chongqing University, Chongqing, China; Chongqing Power Transmission and Transformation Engineering Company Ltd., Chongqing, China","IEEE Sensors Journal","18 Jan 2021","2021","21","4","5259","5270","Haze can seriously affect the visible and visual quality of outdoor optical sensor systems, e.g., driving assistance, remote sensing, and video surveillance. Single image dehazing is an intractable problem due to its ill-posed nature. The main idea of the paper is combining multi-scale fusion strategy and prior knowledge, thereby presenting balanced image contrast enhancement and intrinsic color preservation, efficiently. The atmospheric illumination prior (AIP) has been proved that haze mainly degrades the contrast of the luminance channel rather than chrominance channels in YCrCb colorspace. To this end, we firstly identify and remove the color veil (unbalanced color channel) with the white balance algorithm, to reduce the influence of unbalanced color channels neglected by the AIP. Considering the new observation that hazy regions exhibit low contrast with high-intensity pixels, the dense and mild haze are enhanced by a set of histogram modification techniques, respectively. Then, with the derived inputs, multi-scale fusion based on Laplacian decomposition strategy is proposed to blend visual contrast only in the luminance channel. Without relying on complex enhancement algorithms and only dealing with one channel, the proposed method is attractive for real-time applications. Moreover, The proposed method can be directly applied to the video sequences frame by frame, alleviating visual artifacts. The simulation results show that our method is comparative to and even better than the more complex state-of-the-art techniques.","1558-1748","","10.1109/JSEN.2020.3033713","Chongqing Talents Plan for Young Talents(grant numbers:CQYC201905019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9239312","Image/video dehazing;histogram modification;multi-scale fusion;white balance","Image color analysis;Atmospheric modeling;Lighting;Visualization;Sensors;Histograms;Scattering","image colour analysis;image enhancement;image fusion;image restoration;image sequences;video signal processing","visible quality;visual quality;remote sensing;video surveillance;single image dehazing;multiscale fusion;balanced image contrast enhancement;intrinsic color preservation;AIP;luminance channel;chrominance channels;color veil;unbalanced color channel;white balance algorithm;high-intensity pixels;dense haze;mild haze;histogram modification;visual contrast;video sequences frame;visual artifacts;Laplacian decomposition;fusion-based dehazing;outdoor optical sensor systems;atmospheric illumination prior;YCrCb colorspace;hazy regions","","10","","54","IEEE","26 Oct 2020","","","IEEE","IEEE Journals"
"Visible/Infrared Combined 3D Reconstruction Scheme Based on Nonrigid Registration of Multi-Modality Images With Mixed Features","Y. Ma; Y. Wang; X. Mei; C. Liu; X. Dai; F. Fan; J. Huang","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Air Force Early Warning Academy, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Access","21 Feb 2019","2019","7","","19199","19211","When traditional 3D reconstruction techniques were used to reconstruct a scene with hidden and disguised heat source targets, the reconstructed scene could not contain these targets; thus, we could not recognize them. The disadvantages of such problems are particularly acute in the military and remote-sensing areas. For this application problem, the authors proposed a visible/infrared combined 3D reconstruction scheme. The 3D scene containing thermal radiation information could be reconstructed by fusing the data from RGB optical and infrared images combined with computer vision passive optical 3D scene reconstruction technique. Meanwhile, the authors proposed the nonrigid registration of multi-modality images with mixed features to solve the problem that the registration algorithm, which is widely used in traditional passive reconstruction technology, cannot accurately match visible and infrared images. Registration accuracy was improved by approximately 40%, experimental results showed that the visible/infrared combined 3D reconstruction scenes retain the visual reality of the traditional 3D reconstruction, and the hidden targets are highlighted in the scenes, which is conducive to the detection and recognition of interesting targets.","2169-3536","","10.1109/ACCESS.2019.2895905","National Natural Science Foundation of China(grant numbers:61605146); Chinese Postdoctoral Science Foundation(grant numbers:2017M612504); Fundamental Research Funds for the Central Universities of China(grant numbers:2042017kf0009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8628979","3D reconstruction;mixed features;nonrigid registration;visible/infrared combined","Three-dimensional displays;Image reconstruction;Cameras;Feature extraction;Mathematical model;Solid modeling;Surface reconstruction","computer vision;image fusion;image reconstruction;image registration;image sensors;infrared imaging","3D reconstruction scheme;nonrigid registration;multimodality images;mixed features;traditional 3D reconstruction techniques;hidden heat source targets;disguised heat source targets;reconstructed scene;military areas;remote-sensing areas;computer vision passive optical 3D;registration algorithm;traditional passive reconstruction technology;visible images;infrared images;3D reconstruction scenes;hidden targets","","8","","37","OAPA","29 Jan 2019","","","IEEE","IEEE Journals"
"Wheat Yellow Rust Severity Detection by Efficient DF-UNet and UAV Multispectral Imagery","T. Zhang; Z. Yang; Z. Xu; J. Li","Shunde Graduate School, University of Science and Technology Beijing, Foshan, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Shunde Graduate School, University of Science and Technology Beijing, Foshan, China","IEEE Sensors Journal","2 May 2022","2022","22","9","9057","9068","Crop disease seriously affects production because of its highly destructive property. Wheat under different levels of disease infection should be treated by various chemical strategies to enable a precision plant protection. Therefore, a fast and robust algorithm for wheat yellow rust disease severity determination is highly desirable for its sustainable management. The recent use of remote sensing and deep learning is drawing increasing research interests in wheat yellow rust severity detection at leaf level. However, little reviews take field-scale rust severity detection into account by using UAV multispectral images and deep learning networks. As a result, by the means of UAV multispectral images, a real-time yellow rust detection algorithm named Efficient Dual Flow UNet (DF-UNet) to detect different levels of yellow rust is designed and proposed in this paper to meet practical requirements. First, pruning strategy is utilized to realize a lightweight structure. Second, the Sparse Channel Attention (SCA) Module is designed to increase the receptive field of the network and enhance the ability to distinguish each category. Third, by fusing SCA, a novel dual flow branch model with segmentation and ranking branch based on UNet is proposed to accomplish yellow rust severity determination at field scale. The comparative results show that the proposed method reduces more than half computation load and achieves the highest overall accuracy score among other state-of-the-art deep learning models. It is convinced that the proposed DF-UNet can pave the way for automated yellow rust severity detection at farmland scales in a robust way.","1558-1748","","10.1109/JSEN.2022.3156097","Fundamental Research Funds for the China Central Universities of the University of Science and Technology Beijing (USTB)(grant numbers:FRF-DF-19-002); Scientific and Technological Innovation Foundation of Shunde Graduate School, USTB(grant numbers:BK20BE014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733357","Precision agriculture;yellow rust severity;deep learning;DF-UNet;UAV multispectral image","Diseases;Sensors;Labeling;Cameras;Deep learning;Monitoring;Data collection","agriculture;autonomous aerial vehicles;computerised instrumentation;deep learning (artificial intelligence);image fusion;image segmentation;image sensors","UAV multispectral imagery;wheat yellow rust disease severity determination;field-scale rust severity detection;deep learning networks;yellow rust severity determination;automated yellow rust severity detection;DF-UNet;wheat yellow rust severity detection algorithm;dual flow UNet;sparse channel attention module;SCA module;lightweight structure;dual flow branch model","","4","","30","IEEE","11 Mar 2022","","","IEEE","IEEE Journals"
"Robust Image Reconstruction With Misaligned Structural Information","L. Bungert; M. J. Ehrhardt","Department of Mathematics, University of Erlangen, Erlangen, Germany; Institute for Mathematical Innovation, University of Bath, Bath, U.K.","IEEE Access","21 Dec 2020","2020","8","","222944","222955","Multi-modality (or multi-channel) imaging is becoming increasingly important and more widely available, e.g. hyperspectral imaging in remote sensing, spectral CT in material sciences as well as multi-contrast MRI and PET-MR in medicine. Research in the last decades resulted in a plethora of mathematical methods to combine data from several modalities. State-of-the-art methods, often formulated as variational regularization, have shown to significantly improve image reconstruction both quantitatively and qualitatively. Almost all of these models rely on the assumption that the modalities are perfectly registered, which is not the case in most real world applications. We propose a variational framework which jointly performs reconstruction and registration, thereby overcoming this hurdle. Our approach is the first to achieve this for different modalities and outranks established approaches in terms of accuracy of both reconstruction and registration. Numerical results on simulated and real data show the potential of the proposed strategy for various applications in multi-contrast MRI, PET-MR, and hyperspectral imaging: typical misalignments between modalities such as rotations, translations, zooms can be effectively corrected during the reconstruction process. Therefore the proposed framework allows the robust exploitation of shared information across multiple modalities under real conditions.","2169-3536","","10.1109/ACCESS.2020.3043638","European Union’s Horizon 2020 Research and Innovation Programme through the Marie Skłodowska-Curie Grant Agreement(grant numbers:777826 (NoMADS)); Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/S026045/1,EP/T026693/1); Faraday Institution(grant numbers:EP/T007745/1); Leverhulme Trust(grant numbers:ECF-2019-478); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288664","Image fusion;image reconstruction;image registration;multi-modality imaging;nonconvex and nonsmooth optimization;variational regularization","Image reconstruction;Magnetic resonance imaging;Strain;Hyperspectral imaging;Computed tomography;Mathematical model;Optimization","biomedical MRI;biomedical optical imaging;image reconstruction;image registration;medical image processing;positron emission tomography","variational framework;multicontrast MRI;PET-MR;hyperspectral imaging;robust image reconstruction;misaligned structural information;image registration","","9","","60","CCBY","9 Dec 2020","","","IEEE","IEEE Journals"
"A Multiscale Dual-Branch Feature Fusion and Attention Network for Hyperspectral Images Classification","H. Gao; Y. Zhang; Z. Chen; C. Li","College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","31 Aug 2021","2021","14","","8180","8192","Recently, hyperspectral image classification based on deep learning has achieved considerable attention. Many convolutional neural network classification methods have emerged and exhibited superior classification performance. However, most methods focus on extracting features by using fixed convolution kernels and layer-wise representation, resulting in feature extraction singleness. Additionally, the feature fusion process is rough and simple. Numerous methods get accustomed to fusing different levels of features by stacking modules hierarchically, which ignore the combination of shallow and deep spectral-spatial features. In order to overcome the preceding issues, a novel multiscale dual-branch feature fusion and attention network is proposed. Specifically, we design a multiscale feature extraction (MSFE) module to extract spatial-spectral features at a granular level and expand the range of receptive fields, thereby enhancing the MSFE ability. Subsequently, we develop a dual-branch feature fusion interactive module that integrates the residual connection's feature reuse property and the dense connection's feature exploration capability, obtaining more discriminative features in both spatial and spectral branches. Additionally, we introduce a novel shuffle attention mechanism that allows for adaptive weighting of spatial and spectral features, further improving classification performance. Experimental results on three benchmark datasets demonstrate that our model outperforms other state-of-the-art methods while incurring the lower computational cost.","2151-1535","","10.1109/JSTARS.2021.3103176","National Natural Science Foundation of China(grant numbers:62071168); National Key R&D Program of China(grant numbers:2018YFC1508106); Fundamental Research Funds for the Central Universities(grant numbers:B200202183); China Postdoctoral Science Foundation(grant numbers:2021M690885); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9512469","Convolutional neural network (CNN);dual-branch feature fusion (DBFM);hyperspectral image (HSI) classification;multiscale feature extraction (MSFE) module;shuffle attention block","Feature extraction;Image classification;Hyperspectral imaging;Convolutional neural networks;Training;Kernel;Deep learning","convolutional neural nets;feature extraction;geophysical image processing;image classification;image representation;learning (artificial intelligence);pattern classification","dual-branch feature fusion interactive module;residual connection;dense connection;discriminative features;spatial branches;spectral branches;multiscale dual-branch feature fusion;attention network;hyperspectral images classification;deep learning;convolutional neural network classification methods;superior classification performance;convolution kernels;layer-wise representation;feature extraction singleness;feature fusion process;fusing different levels;spectral-spatial features;multiscale feature extraction module;spatial-spectral features;shuffle attention mechanism","","11","","56","CCBY","12 Aug 2021","","","IEEE","IEEE Journals"
"Dynamic Data Augmentation Method for Hyperspectral Image Classification Based on Siamese Structure","H. Gao; J. Zhang; X. Cao; Z. Chen; Y. Zhang; C. Li","College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China; College of Computer and Information, Hohai University, Nanjing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Aug 2021","2021","14","","8063","8076","At present, deep learning classification researches of hyperspectral usually focus on optimizing the classification model. In essence, most of them did not take special measures for the characteristics of the small sample and imbalanced category distribution of hyperspectral itself. Aiming at the problems of small samples and imbalanced category distribution, we propose a dynamic data selection algorithm. For one thing, this algorithm can dynamically select the samples that need data augmentation most. For another, it can be nested in Stochastic gradient descent (SGD) and can be easily implemented. Furthermore, there will be differences between the original and the transformed sample because of data augmentation transformation, which obstructs trained models' performance. Aiming at the difference between the augmented sample and the original sample, we define the similarity score and introduce the Siamese training structure to obtain the similarity score by which we reduce the difference through the SGD algorithm. Experiments show that the method proposed in this article improves the classification results of the backbone training model when using data augmentation for training.","2151-1535","","10.1109/JSTARS.2021.3102610","National Natural Science Foundation of China(grant numbers:62071168); National Key R&D Program of China(grant numbers:2018YFC1508106); Fundamental Research Funds for the Central Universities(grant numbers:B200202183); China Postdoctoral Science Foundation(grant numbers:2021M690885); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9508179","Convolutional neural network (CNN);data augmentation (DA);hyperspectral (HSI) classification;Siamese structure","Training;Data models;Feature extraction;Heuristic algorithms;Deep learning;Hyperspectral imaging;Classification algorithms","deep learning (artificial intelligence);gradient methods;image classification;stochastic processes","dynamic data augmentation;hyperspectral image classification;Siamese structure;deep learning classification;classification model;special measures;imbalanced category distribution;dynamic data selection algorithm;stochastic gradient descent;data augmentation transformation;similarity score;Siamese training structure;SGD algorithm;backbone training model","","4","","34","CCBY","5 Aug 2021","","","IEEE","IEEE Journals"
"An Investigation of Interpolation Techniques to Generate 2D Intensity Image From LIDAR Data","I. Ashraf; S. Hur; Y. Park","Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea; Department of Information and Communication Engineering, Yeungnam University, Gyeongsan, South Korea","IEEE Access","2 Jun 2017","2017","5","","8250","8260","Light detection and ranging (LIDAR) has become a part and parcel of ongoing research in autonomous vehicles. LIDAR efficiently captures data during day and night alike; yet, data accuracy is affected in altered weather conditions. LIDAR data fusion with sensors, such as color camera, hyperspectral camera, and RADAR, proves to be a viable solution to improve the quality of data and add spectral information. LIDAR 3-D point cloud containing intensity data are transformed to 2-D intensity images for the said purpose. LIDAR produces large point cloud, but, while generating images for limited field of view, data sparsity results in poor quality images. Moreover, 3-D to 2-D data transformation also involves data reduction, which further deteriorates the quality of images. This paper focuses on generating intensity images from LIDAR data using interpolation techniques, including bi-linear, natural neighbor, bi-cubic, kriging, inverse distance, and weighted and nearest neighbor interpolation. The main focus is to test the suitability of interpolation methods for 2-D image generation, and analyze the quality of the generated 2-D image. Image similarity metrics, such as root mean square error, normalized least square error, peak signal-to-noise ratio, correlation, difference entropy, mutual information, and structural similarity index measurement, are utilized for camera and LIDAR image matching, and their ability to compare images from heterogeneous sensors is also analyzed. Generated images can further be used for data fusion purpose. Images generated using LIDAR points have a relevant distance matrix as well, which can be used to find the distance of any given pixel from the image. In addiiton, the accuracy of interpolated distance data is evaluated as well by comparing it with the original distance values of traffic cones placed in front of vehicle. Results show that the inverse distance weighted interpolation outperforms other selected methods in 2-D image quality, and images from nearest neighbor appear brighter subjectively.","2169-3536","","10.1109/ACCESS.2017.2699686","Ministry of Education and the National Foundation of Korea (NRF) through the Human Resource Training Project for Regional Innovation(grant numbers:2013H1B8A2031879); Basic Science Research Program through the Ministry of Education(grant numbers:NRF-2014R1A1A2055988); Ministry of Science, ICT and Future Planning, Korea, under the Information Technology Research Center Support Program supervised by the Institute for Information & communications Technology Promotion (IITP)(grant numbers:IITP-2017-2016-0-00313); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915679","Image generation;intelligent vehicles;interpolation;sensor fusion","Laser radar;Interpolation;Three-dimensional displays;Two dimensional displays;Cameras;Sensors;Roads","image fusion;image matching;interpolation;mean square error methods;optical radar;radar imaging;road vehicle radar","structural similarity index measurement;LIDAR image matching;heterogeneous sensors;interpolated distance data;traffic cones;mutual information;peak signal-to-noise ratio;normalized least square error;root mean square error;image similarity metrics;2D image generation;nearest neighbor interpolation;inverse distance weighted interpolation;kriging interpolation;bicubic interpolation;natural neighbor interpolation;bilinear interpolation;data reduction;data sparsity;LIDAR 3D point cloud;spectral information;hyperspectral camera;color camera;data accuracy;autonomous vehicles;LIDAR data fusion;light detection and ranging;2D intensity image;interpolation techniques","","37","1","34","OAPA","30 Apr 2017","","","IEEE","IEEE Journals"
"A Novel Salient Feature Fusion Method for Ship Detection in Synthetic Aperture Radar Images","G. Zhang; Z. Li; X. Li; C. Yin; Z. Shi","Department of Aerospace Science and Technology, Space Engineering University, Beijing, China; Department of Aerospace Science and Technology, Space Engineering University, Beijing, China; Department of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Department of Aerospace Science and Technology, Space Engineering University, Beijing, China; Department of Aerospace Science and Technology, Space Engineering University, Beijing, China","IEEE Access","8 Dec 2020","2020","8","","215904","215914","Ship detection of synthetic aperture radar (SAR) images is one of the research hotspots in the field of marine surveillance. Fusing salient features to detection network can effectively improve the precision of ship detection. However, how to effectively fuse the salient features of SAR images is still a difficult task. In this paper, to improve the ship detection precision, we design a novel one-stage ship detection network to fuse salient features and deep convolutional neural network (CNN) features. Firstly, a saliency map extraction algorithm is proposed. The algorithm is applied to generate saliency map by using multi-scale pyramid features and frequency domain features. Secondly, the backbone of the ship detection network contains a two-stream network. The upper-stream network uses the original SAR image as input to extract multi-scale deep CNN features. The lower-stream network uses the corresponding saliency map as input to acquire multi-scale salient features. Thirdly, for integrating the salient features to deep CNN features, a novel salient feature fusion method is designed. Finally, an improved bi-directional feature pyramid network is applied to the ship detection network for reducing the computational complexity and network parameters. The proposed methods are evaluated on the public ship detection dataset and the experimental results shows that it can make a significant improvement in the precision of SAR image ship detection.","2169-3536","","10.1109/ACCESS.2020.3041372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273001","Ship detection;synthetic aperture radar images;feature fusion;saliency map;deep convolutional neural network","Marine vehicles;Feature extraction;Radar polarimetry;Proposals;Synthetic aperture radar;Surveillance;Fuses","convolutional neural nets;feature extraction;frequency-domain analysis;image fusion;image resolution;marine radar;object detection;radar computing;radar detection;radar imaging;search radar;ships;synthetic aperture radar","network parameters;public ship detection dataset;SAR image ship detection;synthetic aperture radar images;one-stage ship detection network;deep convolutional neural network features;saliency map extraction algorithm;multiscale pyramid features;frequency domain features;two-stream network;upper-stream network;lower-stream network;multiscale salient features;novel salient feature fusion method;improved bi-directional feature pyramid network;computational complexity;marine surveillance;multiscale deep CNN feature extraction","","13","","53","CCBY","30 Nov 2020","","","IEEE","IEEE Journals"
"Prediction of Microbial Spoilage and Shelf-Life of Bakery Products Through Hyperspectral Imaging","Z. Saleem; M. H. Khan; M. Ahmad; A. Sohaib; H. Ayaz; M. Mazzara","Department of Computer Engineering, Advance Image Processing Research Laboratory (AIPRL), Khwaja Freed University of Engineering and Technology (KFUEIT), Rahim Yar Khan, Pakistan; Department of Computer Engineering, Advance Image Processing Research Laboratory (AIPRL), Khwaja Freed University of Engineering and Technology (KFUEIT), Rahim Yar Khan, Pakistan; Department of Computer Science, National University of Computer and Emerging Sciences, Chiniot-Faisalabad Campus, Chiniot, Pakistan; Department of Computer Engineering, Advance Image Processing Research Laboratory (AIPRL), Khwaja Freed University of Engineering and Technology (KFUEIT), Rahim Yar Khan, Pakistan; Department of Computer Engineering, Advance Image Processing Research Laboratory (AIPRL), Khwaja Freed University of Engineering and Technology (KFUEIT), Rahim Yar Khan, Pakistan; Institute of Software Development and Engineering, Innopolis University, Innopolis, Russia","IEEE Access","6 Oct 2020","2020","8","","176986","176996","The shelf life of bakery products highly depends on the environment and it may get spoiled earlier than its expiry which results in food-borne diseases and may affect human health or may get wasted beforehand. The traditional spoilage detection methods are time-consuming and destructive in nature due to the time taken to get microbiological results. To the best of the author's knowledge, this work presents a novel method to automatically predict the microbial spoilage and detect its spatial location in baked items using Hyperspectral Imaging (HSI) range from $395-1000\,\,nm$ . A spectral preserve fusion technique has been proposed to spatially enhance the HSI images while preserving the spectral information. Furthermore, to automatically detect the spoilage, Principal Component Analysis (PCA) followed by K-means and SVM has been used. The proposed approach can detect the spoilage almost 24 hours before it started appearing or visible to a naked eye with 98.13% accuracy on test data. Furthermore, the trained model has been validated through external dataset and detected the spoilage almost a day before it started appearing visually.","2169-3536","","10.1109/ACCESS.2020.3026925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9205804","Shelf life of bakery products;fungus detection and prediction;PCA;K-means;SVM;hyper sharpening","Microorganisms;Hyperspectral imaging;Digital images;Diseases;Principal component analysis;Spectroscopy;Cameras","bakeries;diseases;food preservation;food processing industry;food products;food safety;health and safety;hyperspectral imaging;image fusion;microorganisms;principal component analysis;production engineering computing;support vector machines","human health;microbiological results;microbial spoilage detection methods;PCA;SVM;K-means method;food-borne diseases;shelf life;bakery products;principal component analysis;spectral information;HSI images;spectral preserve fusion technique;Hyperspectral Imaging range;time 24.0 hour","","12","","51","CCBY","25 Sep 2020","","","IEEE","IEEE Journals"
"Fractional-Order Integration Based Fusion Model for Piecewise Gamma Correction Along With Textural Improvement for Satellite Images","H. Singh; A. Kumar; L. K. Balyan; H. -N. Lee","Indian Institute of Information Technology, Design and Manufacturing Jabalpur, Jabalpur, India; Indian Institute of Information Technology, Design and Manufacturing Jabalpur, Jabalpur, India; Indian Institute of Information Technology, Design and Manufacturing Jabalpur, Jabalpur, India; Gwangju Institute of Science and Technology, Gwangju, South Korea","IEEE Access","1 Apr 2019","2019","7","","37192","37210","Fractional-order integration (FOI) and its beauty of optimally ordered adaptive filtering for image quality enhancement are latently too valuable to be casually dismissed. With this motivation, a new Riemann-Liouville fractional-order calculus-based spatial-masking methodology is proposed in this paper in association with counterbalanced piecewise gamma correction (PGC). A generalized FOI-based mask is also suggested. This mask is negatively augmented with the original image for harvesting texture-based benefits. PGC is just employed through a constructive association of both kinds of reciprocally dual gamma values (γ1 = γ and γ2 = 1/γ, ∀γ > 1), which leads to optimally desired enhancement when applied in a weighted counter-correction manner. Efficiently improved and recently proposed opposition-based learning inspired sine-cosine algorithm is employed in this paper, along with a newly framed fitness function. This fitness function is devised in a novel manner by taking care of textural as well as non-textural details of the images. In this paper, especially for dark images, 130% increment is achieved over the input contrast along with the simultaneous 147% increment in the discrete entropy level and 22.8% increment in the sharpness content. Also, brightness and colorfulness are reported with 130% and 196.4% increased with respect to the input indices, respectively. In addition, the textural improvement is advocated in terms of desired comparative reduction of gray-level co-occurrence matrix-based metrics, namely, correlation, energy, and homogeneity, which are suppressed by 25.6%, 72.5%, and 21.8%, respectively. This performance evaluation underlines the excellence and robustness for imparting proper texture as well as edge preserved (or efficiently restored) image quality improvement.","2169-3536","","10.1109/ACCESS.2019.2901292","National Research Foundation of Korea(grant numbers:NRF-2018R1A2A1A19018665); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674462","Fractional-order (FO) masking filter;fractional-order integration (FOI);Riemann–Liouville (RL) definition;sine cosine algorithm (SCA);opposition-based learning (OBL);gray-level co-occurrence matrix (GLCM);quality enhancement;optimal mask designing;two-dimensional (2-D) adaptive filtering;piecewise-gamma correction (PGC)","Histograms;Filtering;Optimization;Image enhancement;Calculus;Image edge detection;Image quality","adaptive filters;approximation theory;entropy;feature extraction;geophysical image processing;image colour analysis;image enhancement;image fusion;image texture;learning (artificial intelligence);matrix algebra;optimisation","texture-based benefits;PGC;constructive association;reciprocally dual gamma values;optimally desired enhancement;weighted counter-correction manner;newly framed fitness function;nontextural details;dark images;textural improvement;co-occurrence matrix-based metrics;image quality improvement;order integration based fusion model;satellite images;fractional-order integration;image quality enhancement;Riemann-Liouville fractional-order calculus;counterbalanced piecewise gamma correction;generalized FOI-based mask","","12","","20","OAPA","26 Mar 2019","","","IEEE","IEEE Journals"
"Adversarial Deep Domain Adaptation for Multi-Band SAR Images Classification","W. Zhang; Y. Zhu; Q. Fu","National Key Laboratory of Science and Technology on ATR, Institution of Electronic Science, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, Institution of Electronic Science, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, Institution of Electronic Science, National University of Defense Technology, Changsha, China","IEEE Access","25 Jun 2019","2019","7","","78571","78583","Deep convolutional neural networks (CNNs) have made a breakthrough on supervised SAR images classification. However, SAR imaging is considerably affected by the frequency band. That means a neural network trained on a SAR image set of one band is not suitable for the classification of another band images. As manually labeling the training samples of each band is always time-consuming, we propose an unsupervised multi-level domain adaptation method based on adversarial learning to solve the problem of multi-band SAR images classification. First, we train a discriminative CNN using samples of one frequency band data set that contains labels to map the data to a latent feature space. Then, we adjust the trained CNN to map the unlabeled samples of another frequency band data set to the same feature space through alternately optimizing two adversarial loss functions. Thus, the features of these two band images are fused and can be classified by the same classifier. We checked the performance of our method using both simulated data and measured data. Our method made a breakthrough in the classification of multi-band images with accuracies of 99% on both data sets. The results are even very close to the supervised CNN trained using a large number of labeled samples.","2169-3536","","10.1109/ACCESS.2019.2922844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736260","Convolutional neural network(CNN);domain adaptation;multi-band SAR images classification;adversarial learning","Radar polarimetry;Neural networks;Training;Adaptation models;Feature extraction;Deep learning;Optical imaging","convolutional neural nets;image classification;image fusion;optimisation;radar imaging;synthetic aperture radar","adversarial deep domain adaptation;frequency band data;deep convolutional neural networks;unsupervised multilevel domain adaptation method;supervised SAR image classification;multiband SAR image classification;discriminative CNN","","12","","37","OAPA","13 Jun 2019","","","IEEE","IEEE Journals"
"Learning Deconvolutional Network for Object Tracking","X. Lu; H. Huo; T. Fang; H. Zhang","Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Zhengzhou University of Light Industry, Zhangzhou, China","IEEE Access","16 Apr 2018","2018","6","","18032","18041","Object tracking can be tackled by learning a model of tracking the target's appearance sequentially. Therefore, robust appearance representation is a critical step in visual tracking. Recently, deep convolution network has demonstrated remarkable ability in visual tracking via leveraging robust high-level features. To obtain these high-level features, convolution and pooling operations are executed alternatively in deep convolution network. However, these operations lead to low spatial resolution feature maps which degrade the localization precision in tracking. While low level features have sufficient spatial resolution, their representation ability is insufficient. To mitigate this issue, we exploited deconvolution network in visual tracking. This deconvolution network works as a learnable upsampling layer which takes low-resolution high-level feature maps as input and outputs enlarged feature maps. Meanwhile, the low level feature maps are fused with these high level feature maps via a summarization operation to better represent target appearance. We formulate the network training as a regression issue and train this network end to end. Extensive experiments on two tracking benchmarks demonstrate the effectiveness of our method.","2169-3536","","10.1109/ACCESS.2018.2820004","National Natural Science Foundation of China(grant numbers:41571402); Science Fund for Creative Research Groups of the National Natural Science Foundation of China(grant numbers:61221003X); National Natural Science Foundation of China(grant numbers:61503173); Key Science and Technology Program of Henan Province(grant numbers:172102210062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326476","Object tracking;deep learning;deconvolution neural network;regression network","Deconvolution;Target tracking;Convolution;Feature extraction;Object tracking;Correlation;Visualization","image fusion;image resolution;learning (artificial intelligence);neural nets;object tracking;regression analysis","object tracking;robust appearance representation;visual tracking;deep convolution network;pooling operations;low spatial resolution feature maps;sufficient spatial resolution;representation ability;deconvolution network;network training;tracking benchmarks;low-resolution high-level feature maps;low level feature map fusion;regression issue;summarization operation;upsampling layer learning","","9","","56","OAPA","27 Mar 2018","","","IEEE","IEEE Journals"
"Runway Detection in SAR Images Based on Fusion Sparse Representation and Semantic Spatial Matching","W. Lv; K. Dai; L. Wu; X. Yang; W. Xu","Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN; Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN; Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN; Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN; Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN","IEEE Access","13 Jun 2018","2018","6","","27984","27992","In this paper, a novel algorithm is presented for runway detection in synthetic aperture radar images. It involves two steps: runway assessment and confirmation. In the first step, the primary runway (PR) and the auxiliary runway (AR) of the airport are assessed by using a sparse representation fusion frame. A set of residuals, for each feature from PR or AR, is first generated by performing sparse reconstructions over two training dictionaries constructed by a set of discriminative features. Based on all residuals for all types of features, two residual sequences for PR and AR are, respectively, built. To improve the assessment performance, these two residual sequences are normalized and further linearly fused. An assessment criterion is applied to the fusion result to infer an optimal target estimate. In the second step, the histogram of oriented gradient feature descriptors of PR, AR, and the entire runway region constructed by PR, AR, and taxiways are first generated. Afterward, two semantic spatial rules are developed to verify each candidate region of interest. If a perfect match is achieved, the candidate target can be confirmed. Since the PR and AR are selected based on the residual fusion related with the concatenation of multiple features, the presented algorithm has a good representation ability to the runway. By introducing semantic spatial relationships into the confirmation scheme, this algorithm can well discriminate other runway-like targets. The test results using real scene data demonstrate that the presented method has superiority to some state-of-the-art alternatives.","2169-3536","","10.1109/ACCESS.2018.2839025","National Natural Science Foundation of China(grant numbers:61601410,61374020); Natural Science Foundation of Zhejiang Province(grant numbers:LY16F010018,LY16F010017,LR15F010002,LQ12F01009); Science Foundation of Zhejiang Sci-Tech University(grant numbers:15032085-Y); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8361411","Runway detection;sparse representation;multifeature fusion;histogram of oriented gradient;semantic spatial matching","Semantics;Feature extraction;Airports;Histograms;Synthetic aperture radar;Dictionaries;Training","image fusion;image matching;image representation;image sequences;object detection;radar imaging;synthetic aperture radar","runway detection;SAR images;semantic spatial matching;synthetic aperture radar images;runway assessment;primary runway;PR;auxiliary runway;sparse representation fusion frame;sparse reconstructions;discriminative features;residual sequences;assessment performance;oriented gradient feature descriptors;semantic spatial rules;residual fusion;multiple features;semantic spatial relationships;runway-like targets;real scene data;runway confirmation","","8","","28","OAPA","21 May 2018","","","IEEE","IEEE Journals"
"Dual-Polarization SAR Ship Target Recognition Based on Mini Hourglass Region Extraction and Dual-Channel Efficient Fusion Network","G. Xiong; Y. Xi; D. Chen; W. Yu","School of Electronic Information and Electrical Engineering, Institute of Sensing and Navigation, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Institute of Sensing and Navigation, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Institute of Sensing and Navigation, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Institute of Sensing and Navigation, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","22 Feb 2021","2021","9","","29078","29089","A novel Dual-polarization SAR ship target recognition method based on feature and loss fusion deep network is proposed in this paper, to improve the generalization ability, the recognition accuracy of imbalance samples, and the real-time performance of the general deep learning network. The proposed combined network is composed of two parts. The first part is the target region extraction network based on lightweight mini Hourglass network, to eliminate the impact of data imbalance and background noise on the identification accuracy. The second part is a two-channel feature/loss function fusion network based on Efficient B2 backbone network, aiming to solve the problem of dual-polarization image feature fusion and iterative convergence acceleration. The proposed method is tested on SAR image ship slices from the OpenSAR data sets. The experimental results indicates that, the proposed method achieves a recognition rate of 110FPS with recognition accuracy of 87.72%, and exceeds the SOTA by recognition accuracy 3.72% with convergence speed improved by 75.68%. The proposed method can be applied to SAR target recognition with dual polarization, imbalance samples and low-resolution condition for reference.","2169-3536","","10.1109/ACCESS.2021.3058188","National Natural Science Foundation of China (NSFC)(grant numbers:62071293,61571294); Equipment Pre-research Area Fund Project(grant numbers:61404130221); Key Laboratory Fund Project(grant numbers:61421060105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9350622","Dual-polarization SAR;ship target recognition;dual-channel efficient network;fusion loss function","Target recognition;Marine vehicles;Synthetic aperture radar;Feature extraction;Radar polarimetry;Image recognition;Training","deep learning (artificial intelligence);feature extraction;image fusion;image recognition;radar imaging;ships;synthetic aperture radar","mini Hourglass region extraction;Dual-channel Efficient fusion network;novel Dual-polarization SAR ship target recognition method;loss fusion deep network;generalization ability;recognition accuracy;imbalance samples;general deep learning network;target region extraction network;lightweight mini Hourglass network;data imbalance;background noise;identification accuracy;Efficient B2 backbone network;dual-polarization image;iterative convergence acceleration;SAR image ship slices;recognition rate;SAR target recognition;dual polarization","","7","","29","CCBY","9 Feb 2021","","","IEEE","IEEE Journals"
"Selection of Multi-Level Deep Features via Spearman Rank Correlation for Synthetic Aperture Radar Target Recognition Using Decision Fusion","L. Zhu","College of Mechanical and Electrical Engineering, Jiaxing University, Jiaxing, China","IEEE Access","28 Jul 2020","2020","8","","133914","133927","Convolutional neural networks (CNN) now become one of the most popular methods in synthetic aperture radar (SAR) target recognition. To fully exploit the deep features learned by CNN, this paper considers all the feature maps from different convolution layers. At each layer, the Spearman rank correlation is employed to evaluate the similarities between the feature maps and original SAR image. A certain proportion of feature maps with high similarities are selected and jointly represented based on the joint sparse representation (JSR) model. For the reconstruction error vectors from different layers, they are combined based on linear weighting using a random weight matrix. The fused reconstruction errors are analyzed to form a decision value for target recognition. The feature selection chooses the robust features and JSR considers the inner correlations between the feature maps from the same layer. In addition, the linear weighting using the random weight matrix could statistically reveal the correlations between the test sample and a certain training class. Therefore, the overall effectiveness and robustness of the proposed method can be enhanced. By performing experiments on the moving and stationary target acquisition and recognition (MSTAR) dataset, the proposed method could achieve a very high average recognition rate of 99.32% for ten classes of ground targets under the standard operating condition (SOC). Furthermore, under the extended operating conditions (EOCs) like configuration differences, depression angle differences, noise corruption, and partial occlusion, the proposed could also achieve superior robustness over some state-of-the-art SAR target recognition methods.","2169-3536","","10.1109/ACCESS.2020.3010969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145552","Synthetic aperture radar (SAR);target recognition;deep features;Spearman rank correlation;joint sparse representation (JSR);random weight matrix","Synthetic aperture radar;Target recognition;Convolution;Correlation;Robustness;Training;Machine learning","convolutional neural nets;feature extraction;image fusion;image representation;learning (artificial intelligence);matrix algebra;radar computing;radar imaging;radar target recognition;synthetic aperture radar","feature selection;robust features;feature maps;linear weighting;random weight matrix;stationary target acquisition;high average recognition rate;multilevel deep features;Spearman rank correlation;synthetic aperture radar target recognition;convolutional neural networks;CNN;convolution layers;joint sparse representation model;SAR target recognition;decision fusion;extended operating conditions;configuration differences;depression angle differences;noise corruption;partial occlusion","","6","","68","CCBY","21 Jul 2020","","","IEEE","IEEE Journals"
"A Novel Generalized Intensity-Hue-Saturation (GIHS) Based Pan-Sharpening Method With Variational Hessian Transferring","P. Liu; L. Xiao","School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Access","6 Sep 2018","2018","6","","46751","46761","The popular intensity-hue-saturation (IHS) pan-sharpening method can provide high spatial quality while suffering from some spectral distortion, mainly due to the fact that it cannot estimate an accurate intensity image to substitute the original intensity image in the IHS space. To overcome this drawback, in this paper, we particularly use the modeling idea of variational complementary data fusion, and propose a novel yet highly effective pan-sharpening method with variational Hessian transferring in the generalized IHS (GIHS) transform domain, which aims to estimate a more accurate intensity image. More specifically, the novelty of proposed method consists in building a variational Hessian transferring model in the GIHS transform domain to transfer the Hessian-based spatial geometric information of panchromatic (Pan) image to the new intensity image and meanwhile take the spectral information preserving into consideration. Finally, the experimental results demonstrate the effectiveness of the proposed method which can perform higher spectral and spatial qualities, and higher efficiency than some state-of-the-art methods.","2169-3536","","10.1109/ACCESS.2018.2866489","National Natural Science Foundation of China(grant numbers:61802202,61571230); Natural Science Foundation of Jiangsu Province(grant numbers:BK20170905); NUPTSF(grant numbers:NY217137,NY218025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444368","Pan-sharpening;variational Hessian transferring;GIHS;intensity image","TV;Multiresolution analysis;Wavelet transforms;Geometry;Distortion;Histograms","image colour analysis;image fusion;spectral analysis;transforms","variational complementary data fusion;generalized IHS;GIHS;variational Hessian transferring model;Hessian-based spatial geometric information;panchromatic image;spatial qualities;novel generalized intensity-hue-saturation based Pan-sharpening method","","5","","35","OAPA","22 Aug 2018","","","IEEE","IEEE Journals"
"Superpixel-Oriented Unsupervised Classification for Polarimetric SAR Images Based on Consensus Similarity Network Fusion","H. Zou; M. Li; N. Shao; X. Qin","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; School of Information and Navigation, Air Force Engineering University, Xi’an, China","IEEE Access","24 Jun 2019","2019","7","","78347","78366","Unsupervised polarimetric synthetic aperture radar (PolSAR) image classification is an important task in PolSAR automatic image analysis and interpretation. Generally, a group of features is insufficient to effectively classify PolSAR images, especially in multiple terrain scenarios. Therefore, multiple features need to be extracted for PolSAR image classification. However, how to combine and integrate these features effectively to fully utilize each feature's information and discriminability need to be determined. Such integrated work has traditionally received little attention. In this paper, a novel unsupervised classification framework for PolSAR images is proposed. First, a PolSAR image is oversegmented via a fast superpixel segmentation method. Second, five feature vectors are extracted from PolSAR images via superpixels, resulting in five corresponding similarity matrices that are constructed by using Gaussian kernels. Third, consensus similarity network fusion (CSNF), originally proposed and widely used for biomedical sciences, is employed to combine and integrate the five similarity matrices to obtain a fused similarity matrix. Fourth, spectral clustering method, based on the fused similarity matrix, is used to cluster the PolSAR image. Finally, a novel classification postprocessing procedure is presented and exploited to smooth the initial clusters and correct some misclassified pixels. The extensive experimental results conducted on one simulated and two real-world PolSAR images demonstrate the feasibility and superiority of the proposed method compared with five other state-of-the-art classification approaches.","2169-3536","","10.1109/ACCESS.2019.2922473","National Natural Science Foundation of China(grant numbers:61331015,41601436); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736007","Polarimetric synthetic aperture radar (PolSAR) images;unsupervised classification;superpixels segmentation;consensus similarity network fusion (CSNF);spectral clustering","Feature extraction;Image segmentation;Image classification;Kernel;Image color analysis;Speckle;Computational efficiency","feature extraction;Gaussian processes;image classification;image fusion;image segmentation;matrix algebra;pattern clustering;radar computing;radar imaging;radar polarimetry;synthetic aperture radar;unsupervised learning","superpixel-oriented unsupervised classification;polarimetric SAR images;consensus similarity network fusion;unsupervised polarimetric synthetic aperture radar image classification;PolSAR image classification;novel unsupervised classification framework;fused similarity matrix;real-world PolSAR images;PolSAR automatic image interpretation;fast superpixel segmentation method;feature vector extraction;biomedical sciences;spectral clustering method;classification postprocessing procedure","","4","","46","OAPA","12 Jun 2019","","","IEEE","IEEE Journals"
"SURE-Fuse WFF: A Multi-Resolution Windowed Fourier Analysis for Interferometric Phase Denoising","J. P. Krishnan; M. A. T. Figueiredo; J. M. Bioucas-Dias","Instituto de Telecomunicações, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Instituto de Telecomunicações, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; Instituto de Telecomunicações, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal","IEEE Access","5 Sep 2019","2019","7","","120708","120723","Interferometric phase (InPhase) images, acquired by phase imaging systems, often suffer from two major degradations: 1) phase wrapping, caused by the sinusoidal  $2\pi $ -periodic sensing mechanism, and 2) noise, introduced by the acquisition process or the system. This work focuses on InPhase denoising, which is a fundamental restoration step to many posterior applications of InPhase, namely to phase unwrapping. The presence of sharp fringes, which arises from phase wrapping, makes InPhase denoising a hard inverse problem. Motivated by the local sparsity often exhibited by InPhase images in Fourier domain, we propose a multi-resolution windowed Fourier filtering (WFF) analysis that fuses WFF estimates with different resolutions, thus overcoming the WFF fixed resolution limitation. The proposed fusion relies on an unbiased estimate of the mean square error derived using the Stein’s lemma adapted to complex-valued signals. This estimate, known as SURE, is minimized using an optimization framework to obtain the fusion weights. Strong experimental evidence, using synthetic and real (InSAR & MRI) data, that the developed algorithm, termed as SURE-fuse WFF, outperforms the best hand-tuned fixed resolution WFF counterpart, as well as other state-of-the-art InPhase denoising algorithms, is provided.","2169-3536","","10.1109/ACCESS.2019.2936991","H2020 Marie Skłodowska-Curie Actions(grant numbers:642685 MacSeNet); FCT/MEC through national funds; FEDER PT2020 Partnership Agreement(grant numbers:UID/EEA/50008/2019); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8809725","Phase estimation;interferometric phase estimation;interferometry;phase unwrapping;Stein’s unbiased risk estimate (SURE);windowed Fourier transform (WFT);multi-resolution WFT","Noise reduction;Estimation;Wrapping;Magnetic resonance imaging;Optical interferometry;Frequency-domain analysis","Fourier analysis;image denoising;image filtering;image fusion;image resolution;image restoration;inverse problems;light interferometry;mean square error methods;optimisation","SURE-fuse WFF;sinusoidal 2π-periodic sensing mechanism;posterior applications;phase wrapping;hard inverse problem;image restoration step;InPhase image denoising;interferometric phase image denoising;multiresolution windowed Fourier filtering analysis;mean square error estimation;Stein's lemma;complex-valued signal;optimization framework;hand-tuned fixed resolution WFF counterpart","","3","","45","CCBY","22 Aug 2019","","","IEEE","IEEE Journals"
"Image Restoration for the MRA-Based Pansharpening Method","J. Jiao; L. Wu","Science and Technology on Complex Electronic System Simulation Laboratory, Space Engineering University, Beijing, China; Science and Technology on Complex Electronic System Simulation Laboratory, Space Engineering University, Beijing, China","IEEE Access","23 Jan 2020","2020","8","","13694","13709","By merging high-resolution panchromatic (PAN) images with low-resolution multispectral (MS) images, high-resolution MS images with complementary information can be obtained, i.e., pansharpening. Multiresolution analysis (MRA) methods have attracted widespread attention in the pansharpening field. The spatial detail information injected into MS images is extracted from PAN images by MRA tools. Since such methods often suffer from spatial distortion and ringing artifacts, a restoration algorithm based on blind deblurring and iterative back-projection (IBP) is proposed in this paper. First, a blind deblurring method based on the Tikhonov regular constraint model is used to estimate the blurring filter. Second, spatial details extracted from PAN images are modulated into MS images using a high-pass modulation (HPM) framework, and then fusion images are spatially enhanced based on the modulation results and blurring filter. Finally, the IBP technique is used to project the reconstruction error back to iteratively update and optimize the desired high-resolution images. Experiments are performed on data sets acquired by different satellites at full and reduced resolution, and eight state-of-the-art MRA-based pansharpening methods are used for validation. Compared to the enhanced back-projection (EBP) algorithm, the proposed restoration method is better in improving spectral and spatial quality of MRA-based pansharpening. The results indicate the effectiveness and superiority of the proposed method.","2169-3536","","10.1109/ACCESS.2020.2965921","National Natural Science Foundation of China(grant numbers:61801513); Defense Equipment Pre-Research Foundation(grant numbers:61420100103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957038","Blind deblurring;image restoration;iterative back-projection;multiresolution analysis;pansharpening","Image restoration;Distortion;Optical sensors;Multiresolution analysis;Modulation;Optical distortion","image enhancement;image fusion;image reconstruction;image resolution;image restoration;iterative methods","blurring filter;PAN images;MS images;Tikhonov regular constraint model;blind deblurring method;restoration algorithm;ringing artifacts;spatial distortion;spatial detail information;multiresolution analysis methods;complementary information;low-resolution multispectral images;high-resolution panchromatic images;MRA-based pansharpening method;image restoration;spatial quality;spectral quality;enhanced back-projection algorithm;high-resolution images;fusion images;high-pass modulation framework","","3","","47","CCBY","13 Jan 2020","","","IEEE","IEEE Journals"
"Accurate Extraction Method for Structural Features of Building Facades Through Texture Fusion","Y. Wang; J. Xi; Y. Ma","Department of Surveying and Mapping Engineering, Jiangxi University of Science and Technology, Ganzhou, China; Department of Surveying and Mapping Engineering, Jiangxi University of Science and Technology, Ganzhou, China; Geography Department, Shihezi University, Shihezi, China","IEEE Access","4 Nov 2020","2020","8","","195799","195809","Facade structural features can represent the overall framework of buildings. However, structural features extracted by the current methods contain a quantity of trivial unstructured information. In this study, we proposed an accurate extraction method for structural features of building facades through texture fusion. By performing texture fusion on building facade images, the interference of textural elements on structural feature extraction could be eliminated. After texture fusion, the line segment detector (LSD) algorithm is used to extract the structural features from the building facade images, and random sample consensus (RANSAC) is used to improve the continuity of structural features. The accuracy and effectiveness of the proposed method was demonstrated by comparing results with the state-of-the-art methods, such as LSD, MLSD, CannyLines, and MCMLSD. Value setting of three important parameters is discussed in detail. The imagery facade features extracted through the proposed method provide valuable support for many fields, such as image feature registration and 3D reconstruction of building surfaces.","2169-3536","","10.1109/ACCESS.2020.3033559","National Natural Science Foundation of China(grant numbers:41601429); Natural Science Foundation of Jiangxi Provincial Department of Science and Technology(grant numbers:20171BAB203028); Program of QingJiang Excellent Young Talents, Jiangxi University of Science and Technology(grant numbers:JXUSTQJBJ2018002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9239332","Structural features of building facades;relative total variation;gradient magnitude;texture fusion;feature extraction","Feature extraction;Buildings;Image segmentation;Interference;TV;Data mining;Image texture","building;buildings (structures);feature extraction;image fusion;image reconstruction;image registration;image segmentation;image texture;random processes;structural engineering computing","building facades;texture fusion;facade structural features;building facade images;structural feature extraction;image feature registration;imagery facade feature extraction;line segment detector;random sample consensus;RANSAC;3D reconstruction","","2","","42","CCBY","26 Oct 2020","","","IEEE","IEEE Journals"
"Hyperspectral Image Classification Based on Stacked Contractive Autoencoder Combined With Adaptive Spectral-Spatial Information","P. Guo; Z. Liu; H. Lu; Z. Wang","School of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin, China; School of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin, China; School of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin, China; School of Computer Science and Information Security, Guilin University of Electronic Technology, Guilin, China","IEEE Access","13 Jul 2021","2021","9","","96404","96415","Hyperspectral image (HSI) contain abundant spectral and spatial information, enabling the accurate classification of ground objects. However, many existing machine learning methods have poor performance, and some existing CNN-based methods require high computational power, which considerably limits their real-world applications. To address these issues, in this paper, we propose an alternative HSI classification method based on the stacked contractive autoencoder (SCAE) and adaptive spectral-spatial information to improve the accuracy of HSI classification. Specifically, the non-subsampled shearlet transform (NSST) with the guided filtering (NG) enhances spatial structure information. Subsequently, we present an adaptive spatial information extraction method to extract the spatial information of pixels. Furthermore, we propose an HSI classification network, called SCAE-LR, for feature extraction and classification. The SCAE is implemented to extract the adaptive spectral-spatial feature, and a logistic regression (LR) layer is employed for classification. Extensive experiments on the Indian Pines data set and the Pavia University data set demonstrate the superior performance of our method.","2169-3536","","10.1109/ACCESS.2021.3095265","National Natural Science Foundation of China(grant numbers:61866009); Science and Technology Major Project of Guangxi(grant numbers:AA17202024); Guet Graduate Education(grant numbers:2017YJCX101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9475983","Hyperspectral image;adaptive spectral-spatial information;logistic regression;image classification","Feature extraction;Information retrieval;Data mining;Transforms;Image reconstruction;Adaptive systems;Logistics","feature extraction;geophysical image processing;hyperspectral imaging;image classification;image fusion;learning (artificial intelligence);regression analysis","hyperspectral image classification;stacked contractive autoencoder combined;adaptive spectral-spatial information;abundant spectral information;existing machine learning methods;existing CNN-based methods;alternative HSI classification method;spatial structure information;adaptive spatial information extraction method;HSI classification network;spectral-spatial feature","","1","","38","CCBY","7 Jul 2021","","","IEEE","IEEE Journals"
"3D Urban Buildings Extraction Based on Airborne LiDAR and Photogrammetric Point Cloud Fusion According to U-Net Deep Learning Model Segmentation","P. Zhang; H. He; Y. Wang; Y. Liu; H. Lin; L. Guo; W. Yang","Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; College of Forestry, Beijing Forestry University, Beijing, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China; Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning, Guangzhou, China","IEEE Access","1 Mar 2022","2022","10","","20889","20897","The LiDAR and photogrammetric point clouds fusion procedure for building extraction according to U-Net deep learning model segmentation is provided and tested. Firstly, an initial geo-localization process is performed for photogrammetric point clouds generated using structure-from-motion and dense-matching methods. Then, point cloud segmentation is carried out based on U-Net deep learning model. The precision of the U-Net model for buildings extraction reachs 87%, with F-score of 0.89 and IoU of 0.80. It is shown that the U-Net method is effective for high-resolution image extraction. The detailed information can accurately be identified and extracted, such as vegetation located between buildings and roads. After segmentation, each chunk of the LiDAR and photogrammetric point clouds are finely registered and merged based on the iterative closest point algorithm. Finally, the fused point clouds are obtained. It shows that the structure and shape of the buildings could be delineated from the fused point clouds when both enough ground points and a higher point density are available. Furthermore, color information improves both visualization effect and properties identification. The experiments are conducted to extract individual buildings from three types of point clouds in three plots. A DoN (Difference of Normals) approach is used to isolate 3D buildings from other objects in densely built-up areas. It shows that most building extraction results have a Precision > 0.9 and favorable Recall and F-score values. Although the LiDAR extraction results have some advantages over the photogrammetric and fused ones in terms of Precision, the Recall and F-score results appear best for the fused point clouds. It shows that the fused data contains a high point density and RGB color information and could improve the building extraction.","2169-3536","","10.1109/ACCESS.2022.3152744","Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2020B0101130009); Guangdong Enterprise Key Laboratory for Urban Sensing, Monitoring and Early Warning(grant numbers:2020B121202019); Smart Guangzhou Spatiotemporal Information Cloud Platform Construction(grant numbers:GZIT2016-A5-147); Gao Fen Project of China(grant numbers:30-Y20A34-9010-15/17); Construction of Public Service Platform: Building Information Modeling (BIM) and City Information Modeling (CIM)-Based Integrated Perspective(grant numbers:TC19083WA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716925","Building extraction;point clouds;U-Net;deep learning;segmentation;difference of normals","Buildings;Point cloud compression;Three-dimensional displays;Laser radar;Image segmentation;Deep learning;Data mining","buildings (structures);deep learning (artificial intelligence);geophysical image processing;image colour analysis;image fusion;image motion analysis;image registration;image resolution;image segmentation;iterative methods;optical radar;photogrammetry;solid modelling","3D urban buildings extraction;photogrammetric point cloud fusion;U-Net deep learning model segmentation;point cloud segmentation;U-Net model;U-Net method;high-resolution image extraction;iterative closest point algorithm;fused point clouds;LiDAR extraction;building extraction;airborne LiDAR;initial geo-localization process;structure-from-motion method;dense-matching method;Difference of Normals approach;DoN approach;RGB color information","","1","","39","CCBY","18 Feb 2022","","","IEEE","IEEE Journals"
"Ship Detection in SAR Images Based on Adjacent Context Guide Fusion Module and Dense Weighted Skip Connection","W. Shi; Z. Hu; H. Liu; S. Cen; J. Huang; X. Chen","School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China","IEEE Access","30 Dec 2022","2022","10","","134263","134276","Fusing features from different layers is essential to improve the ship target detection ability in the synthetic aperture radar (SAR) images. Mainstream methods usually perform simple addition or concatenation operations on adjacent feature layers without properly merging their semantic and spatial information, whereas the traditional skip connections are unable to explore sufficient information by the same scale. To address these issues, a ship detection network based on adjacent context guide fusion module and dense weighted skip connection (AFDN) in SAR images is proposed: Adjacent context guide fusion module is specially designed to capture the long-range dependencies of high-level features as weights to multiply with low-level features to fuse adjacent features more efficiently. Furthermore, the dual-path enhanced pyramid is constructed to refine and fuse multi-scale features. Finally, a dense weighted skip connection is proposed by weighted fusion of features of all sizes before the decoder to enrich the feature space. Our anchor-free AFDN outputs the spatial density map and clusters to obtain the rotatable bounding box. The experimental results indicate that the method proposed in this paper surpasses previous ship detection methods and achieves high accuracy on SSDD and AIRSARShip-1.0 datasets.","2169-3536","","10.1109/ACCESS.2022.3230140","National Natural Science Foundation of China(grant numbers:62061002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991152","Adjacent context guide fusion module;dense weighted skip connection;dual-path enhanced pyramid;spatial density map;ship detection;synthetic aperture radar (SAR)","Synthetic aperture radar;Object detection;Target tracking;Maritime vehicles;Spatial density","feature extraction;image fusion;object detection;radar imaging;ships;synthetic aperture radar","adjacent context guide fusion module;adjacent feature layers;adjacent features;dense weighted skip connection;feature space;high-level features;low-level features;multiscale features;paper surpasses previous ship detection methods;SAR images;ship detection network;ship target detection ability;synthetic aperture radar images;traditional skip connections;weighted fusion","","","","43","CCBYNCND","16 Dec 2022","","","IEEE","IEEE Journals"
"Fast and Accurate Super-Resolution of FY-2 Infrared Cloud Images via Multi-Scale Fusion Network","Y. Guo; P. Xiao; M. Xue","Jiangsu Key Laboratory of Meteorological Observation and Information Processing, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Meteorological Observation and Information Processing, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Access","25 Oct 2019","2019","7","","152149","152157","This paper proposes an effective method to improve the spatial resolution of FengYun-2 (FY-2) infrared cloud images via deep convolutional neural networks. The proposed model consists of four parts: shallow feature representation block, stacked multi-scale fusion blocks, global feature fusion block, and feature reconstruction block. The multi-scale fusion block combines dilated convolution, local feature fusion and local residual learning to extract multi-scale local features from the original low-resolution image directly. Then these local features are all merged by the global feature fusion block to reconstruct the residual representations in high-resolution space. For training and testing, we have specially built a dataset of infrared cloud images. We evaluated the proposed method both on simulated and real data. Experimental results demonstrate that the proposed approach achieves improved reconstruction accuracy than the state-of-the-art methods. Besides, the concise structure of the proposed model enables it to be applicable in practice.","2169-3536","","10.1109/ACCESS.2019.2948037","National Natural Science Foundation of China(grant numbers:61673222,61701245,61701247); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8873577","Infrared cloud image;super-resolution;deep convolutional neural network;multi-scale fusion network","Feature extraction;Clouds;Image reconstruction;Convolution;Spatial resolution;Task analysis","convolutional codes;feature extraction;image fusion;image reconstruction;image representation;image resolution;learning (artificial intelligence)","multiscale local features;low-resolution image;global feature fusion block;high-resolution space;multiscale fusion network;spatial resolution;FengYun-2;deep convolutional neural networks;shallow feature representation block;stacked multiscale fusion blocks;feature reconstruction block;multiscale fusion block;local feature fusion;local residual learning;FY-2 infrared cloud images","","","","36","CCBY","17 Oct 2019","","","IEEE","IEEE Journals"
"A Novel Shadow and Layover Segmentation Network for Multi-Angle SAR Images Fusion","X. Li; G. Zhang; C. Yin; Y. Wu; X. Shen","Science and Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Unit 95865 of PLA, Beijing, China; Department of Aerospace Science and Technology, Space Engineering University, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China","IEEE Access","15 Nov 2022","2022","10","","117770","117781","Shadow and layover are geometric distortion phenomenons in side-view imaging synthetic aperture radar (SAR) systems, especially in mountainous areas and densely populated urban areas. The shadow can block the target of the observation area, making it impossible to obtain the scattering characteristics of the target. The layover causes phase distortion and alters target characteristics. Shadow and layover severely hinder the interpretation of SAR images. To confront the above problems, a multi-angle fusion algorithm based on unsupervised progressive segmentation network is proposed. Firstly, inspired by mega-constellations of low earth orbit, a spaceborne SAR collaborative observation model is proposed to generate multi-angle images of fluctuant terrain. Secondly, according to the difference of echos in the shadow and layover regions, an unsupervised progressive segmentation network is designed to sequentially segment the shadow and layover regions. Finally, to improve the contrast and brightness of the fused SAR image, a single-scale weighted fusion algorithm is designed. Experiments were conducted using the simulated multi-angle SAR images. Compared with single-angle images, the accuracy of target detection and figure-of-merit of the fused SAR image are significantly higher than those of other methods.","2169-3536","","10.1109/ACCESS.2022.3217510","Chinese Academy of Sciences(grant numbers:E1YD5906,E2YD590901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931024","Shadow and layover;synthetic aperture radar;unsupervised progressive segmentation network;multi-angle fusion","Radar polarimetry;Image segmentation;Synthetic aperture radar;Feature extraction;Convolutional neural networks;Fuses;Satellites;Unsupervised learning","geophysical signal processing;image fusion;radar imaging;synthetic aperture radar","alters target characteristics;fused SAR image;geometric distortion phenomenons;layover regions;layover segmentation network;mountainous areas;multiangle fusion algorithm;multiangle images;novel shadow;observation area;phase distortion;scattering characteristics;side-view imaging synthetic aperture radar systems;simulated multiangle SAR images;single-angle images;single-scale weighted fusion algorithm;spaceborne SAR collaborative observation model;unsupervised progressive segmentation network;urban areas","","","","38","CCBY","26 Oct 2022","","","IEEE","IEEE Journals"
"Structure Extraction With Total Variation for Hyperspectral Image Classification","Q. Li; H. Wang; G. Chen; K. Saruta; Y. Terata","Graduate School of Systems Science and Technology, Akita Prefectural University, Akita, Japan; Huawei Research Institute, Xi’an, China; Department of Electronics and Information Systems, Akita Prefectural University, Akita, Japan; Department of Electronics and Information Systems, Akita Prefectural University, Akita, Japan; Department of Electronics and Information Systems, Akita Prefectural University, Akita, Japan","IEEE Access","22 Jul 2019","2019","7","","91019","91033","This paper proposes a novel structure extraction approach that is able to achieve high classification accuracy and low computing burden to hyperspectral image (HSI) classification based on total variation (SETV). Specifically, a two-scale decomposition-based relative total variation (TSD-RTV) method is presented for the first time to process the information of different scales, such that the structure can be well extracted. Moreover, a new weighted-average fusion method is introduced, which can reduce the dimensionality and also remove the noise due to hyperspectral sensors. The support vector machine (SVM) is applied to HSI classification as a classifier. The experiments are conducted on three real hyperspectral datasets: Indian Pines, Salinas, and Kennedy Space Center. The experimental results show the outstanding performance of the proposed SETV model in terms of classification accuracy and computational efficiency.","2169-3536","","10.1109/ACCESS.2019.2922675","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736233","Structure extraction;hyperspectral image classification;total variation;fusion","Feature extraction;Hyperspectral imaging;Silicon;TV;Data mining;Image segmentation","feature extraction;hyperspectral imaging;image classification;image fusion;support vector machines;variational techniques","hyperspectral image classification;two-scale decomposition-based relative total variation method;weighted-average fusion method;hyperspectral sensors;structure extraction approach;total variation;SETV;TSD-RTV method;support vector machine;SVM;Indian Pines;Kennedy Space Center;Salinas","","","","45","OAPA","13 Jun 2019","","","IEEE","IEEE Journals"
"WHDA-FCM: Wolf Hunting-Based Dragonfly With Fuzzy C-Mean Clustering For Change Detection In SAR Images","J. T. Kumar; Y. M. Reddy; B. P. Rao","Jawaharlal Nehru Technological University, Kakinada, 533003 Andhra Pradesh, India; kumarthrisul9@gmail.com; Vasireddy Venkatadri Institute of Technology, Guntur, 522508 Andhra Pradesh, India; Jawaharlal Nehru Technological University, Kakinada, 533003 Andhra Pradesh, India","The Computer Journal","1 Jun 2020","2020","63","1","308","321","For the past few years, the automated addressing of changes in remote sensing images plays a significant role. However, the change detection (CD) model often suffers from the issue of speckle noise. More investigations have been proceeded to overcome this obstacle. This paper also considers the same issue and proposes a new CD model in synthetic aperture radar (SAR) images. Here, two SAR images that are captivated at different times will be considered as the input of the detection process. At first, discrete wavelet transform is incurred for image fusion, where the coefficients are optimally selected through a hybrid model that hybridizes the gray wolf optimization and dragonfly (DA) optimization. At last, the fused images after inverse transform are clustered via the fuzzy c-mean (FCM) clustering approach, and a similarity measure is performed between the segmented image and the ground truth image. The proposed model, wolf hunting-based DA with FCM, compares its performance over other conventional methods in terms of measures like accuracy, specificity, sensitivity, precision, negative predictive value, F1 score and Matthews correlation coefficient. Similarly, the negative measures are false positive rate, false negative rate and false discovery rate, and the betterment is proven.","1460-2067","","10.1093/comjnl/bxz130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9104624","SAR image;discrete wavelet transform;fuzzy c-mean;filter coefficient;wolf hunting-based dragonfly with FCM","","","","","","","","","1 Jun 2020","","","OUP","OUP Journals"
"Dual-Branch Multi-Level Feature Aggregation Network for Pansharpening","G. Cheng; Z. Shao; J. Wang; X. Huang; C. Dang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Department of Geosciences, University of Arkansas, Fayetteville, AR, USA; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE/CAA Journal of Automatica Sinica","10 Oct 2022","2022","9","11","2023","2026","Dear Editor, In pansharpening task, the most existing deep-learning-based pan-sharpening methods fail to fully utilize the different level features, inevitably leading to spectral or spatial distortions. To address this challenge, in this letter, we propose a dual-branch multi-level feature aggregation network for pansharpening (DMFANet). The experimental results on the WorldView-II (WV-II) and QuickBird (QB) dataset confirmed the notable superiority of our method over the current state-of-the-art methods from quantitative and qualitative point of view. The source code is available at https://github.com/Gui-Cheng/DMFANet.","2329-9274","","10.1109/JAS.2022.105956","National Natural Science Foundation of China(grant numbers:42090012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915490","","","deep learning (artificial intelligence);geophysical image processing;image fusion;image resolution;remote sensing","deep-learning-based pansharpening methods;DMFANet;dual-branch multilevel feature aggregation network;QB dataset;QuickBird;spatial distortions;spectral distortions;WorldView-II;WV-II","","1","","16","","10 Oct 2022","","","IEEE","IEEE Journals"
