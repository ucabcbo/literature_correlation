"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Research on Generative Adversarial Networks and Their Applications in Image Generation","J. Zhou","Beijing Aidi School, Beijing, China","2022 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)","18 Oct 2022","2022","","","1144","1147","This paper first introduces the basic principles, model structures, and advantages and disadvantages of generative adversarial networks. Then we give a detailed introduction to three application areas of generative adversarial networks in image generation: medical imaging, 3D reconstruction, and image fusion. Finally, the development trend of generative adversarial networks and their applications in the field of image generation prospects.","","978-1-6654-8090-1","10.1109/AEECA55500.2022.9918833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918833","Generative Adversarial Networks;Image Generation;Application Research","Training;Heating systems;Three-dimensional displays;Image synthesis;Focusing;Generative adversarial networks;Market research","image fusion;image reconstruction;stereo image processing","generative adversarial networks;image generation;medical imaging;3D reconstruction;image fusion","","","","5","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"Infrared and visible image fusion algorithms based on generative adversarial networks","W. -h. Zhuang; X. -g. Tang; B. Zhang; G. -m. Yuan","School of Aerospace Information, Space Engineering University, Beijing, China; School of Aerospace Information, Space Engineering University, Beijing, China; School of Aerospace Information, Space Engineering University, Beijing, China; School of Aerospace Information, Space Engineering University, Beijing, China","2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","16 Nov 2022","2022","","","549","554","Visible images have good contour and texture information, while infrared images have the advantage of working in all weather. Therefore, for the detection and analysis of targets in low illumination at night, the information from visible and infrared images can be fused to improve the detection accuracy and anti-interference capability of detection systems for nighttime targets. In this paper, we propose a generative adversarial network-based fusion algorithm for IR and visible images, which can effectively extract the feature information of IR and visible images by adversarial training of two discriminators and generators, improve the feature extraction ability and the quality of fused images by introducing attention mechanism and structural similarity loss function, and enhance the stability of network training by TTUR. The experimental results show that the algorithm in this paper outperforms other typical algorithms in both subjective and objective evaluations.","","978-1-6654-8474-9","10.1109/AEMCSE55572.2022.00113","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9948320","image fusion;generative adversarial networks;attention mechanism;infrared image;visible image","Training;Software algorithms;Lighting;Feature extraction;Visual effects;Generators;Stability analysis","feature extraction;image fusion;image texture;infrared imaging;neural nets","anti-interference capability;detection accuracy;detection systems;feature information;generative adversarial network-based fusion algorithm;generative adversarial networks;infrared image fusion algorithms;infrared images;texture information;visible image fusion algorithms","","","","19","IEEE","16 Nov 2022","","","IEEE","IEEE Conferences"
"AttentionFGAN: Infrared and Visible Image Fusion Using Attention-Based Generative Adversarial Networks","J. Li; H. Huo; C. Li; R. Wang; Q. Feng","Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Information Technology and Cyber Security, People's Public Security University of China, Beijing, China; Remote Sensing Center of Public Security, People's Public Security University of China, Beijing, China","IEEE Transactions on Multimedia","26 Apr 2021","2021","23","","1383","1396","Infrared and visible image fusion aims to describe the same scene from different aspects by combining complementary information of multi-modality images. The existing Generative adversarial networks (GAN) based infrared and visible image fusion methods cannot perceive the most discriminative regions, and hence fail to highlight the typical parts existing in infrared and visible images. To this end, we integrate multi-scale attention mechanism into both generator and discriminator of GAN to fuse infrared and visible images (AttentionFGAN). The multi-scale attention mechanism aims to not only capture comprehensive spatial information to help generator focus on the foreground target information of infrared image and background detail information of visible image, but also constrain the discriminators focus more on the attention regions rather than the whole input image. The generator of AttentionFGAN consists of two multi-scale attention networks and an image fusion network. Two multi-scale attention networks capture the attention maps of infrared and visible images respectively, so that the fusion network can reconstruct the fused image by paying more attention to the typical regions of source images. Besides, two discriminators are adopted to force the fused result keep more intensity and texture information from infrared and visible image respectively. Moreover, to keep more information of attention region from source images, an attention loss function is designed. Finally, the ablation experiments illustrate the effectiveness of the key parts of our method, and extensive qualitative and quantitative experiments on three public datasets demonstrate the advantages and effectiveness of AttentionFGAN compared with the other state-of-the-art methods.","1941-0077","","10.1109/TMM.2020.2997127","Key Program of High-Resolution Earth Observation System(grant numbers:GFZX0404130307); National Natural Science Foundation of China(grant numbers:41901350); Fundamental Research Funds of People's Public Security University of China(grant numbers:2019JKF330); Fundamental Research Funds for the Central Universities(grant numbers:JZ2019HGBZ0151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103116","Attention mechanism;generative adversarial networks;infrared and visible image fusion","Image fusion;Generative adversarial networks;Feature extraction;Transforms;Generators;Gallium nitride;Fuses","image fusion;image texture;infrared imaging;neural nets;object detection","AttentionFGAN;multiscale attention networks;attention region;infrared image fusion;multiscale attention mechanism;visible image fusion;generative adversarial networks;multimodality images","","61","","47","IEEE","28 May 2020","","","IEEE","IEEE Journals"
"New Stereo High Dynamic Range Imaging Method Using Generative Adversarial Networks","Y. Chen; M. Yu; K. Chen; G. Jiang; Y. Song; Z. Peng; F. Chen","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","3502","3506","Stereo high dynamic range (HDR) image/video can be generated by using a pair of stereo cameras with different exposure parameters. This paper proposes a new stereo HDR imaging method using generative adversarial networks (GAN) with a low dynamic range (LDR) stereo imaging system. It is assumed here that the left-view (LV) image is under-exposed and the right-view (RV) image is overexposed. First, a view exposure transfer GAN (VET-GAN) is constructed to transfer exposure information of the RV image to the LV image to generate the multi-exposure LV images, and then an HDR fusion GAN is constructed to fuse the generated multi-exposure LV images into an LV HDR image. Similarly, an RV HDR image can be generated using the same way to form a stereo HDR image pair. The experimental results show that the proposed method can obtain stereo HDR images with high visual quality and effectively avoid the ghost artifacts caused by parallax.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803656","Stereo vision;high dynamic range;multi-view multi-exposure images;generative adversarial networks","Generative adversarial networks;Dynamic range;Gallium nitride;Image fusion;Generators;Cameras","cameras;stereo image processing","RV HDR imaging;stereo high dynamic range imaging method;left-view imaging;right-view imaging;generated multiexposure LV imaging;LV HDR imaging;HDR fusion GAN;VET-GAN;view exposure transfer GAN;low dynamic range stereo imaging system;stereo cameras;generative adversarial networks;stereo HDR images","","7","","18","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Hyperspectral Image Classification Based on Generative Adversarial Networks with Feature Fusing and Dynamic Neighborhood Voting Mechanism","Y. Zhan; J. Qin; T. Huang; K. Wu; D. Hu; Z. Zhao; Y. Wang; Y. Cao; R. Jiao; Y. Medjadba; G. Wang; X. Yu","College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; Beijing Institute of Geology, Beijing, China; Beijing Institute of Geology, Beijing, China; Beijing Institute of Geology, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China; Libary, Beijing Normal University, Beijing, China; College of Information Science and Technology, Beijing Normal University, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","811","814","Classifying Hyperspectral images with few training samples is a challenging problem. The generative adversarial networks (GAN) are promising techniques to address the problems. GAN constructs an adversarial game between a discriminator and a generator. The generator generates samples that are not distinguishable by the discriminator, and the discriminator determines whether or not a sample is composed of real data. In this paper, by introducing multilayer features fusion in GAN and a dynamic neighborhood voting mechanism, a novel algorithm for HSIs classification based on 1-D GAN was proposed. Extracting and fusing multiple layers features in discriminator, and using a little labeled samples, we fine-tuned a new sample 1-D CNN spectral classifier for HSIs. In order to improve the accuracy of the classification, we proposed a dynamic neighborhood voting mechanism to classify the HSIs with spatial features. The obtained results show that the proposed models provide competitive results compared to the state-of-the-art methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899291","Hyperspectral images classification;semi-supervised learning (SSL);generative adversarial networks (GAN);deep learning;spectral-spatial classification","Generative adversarial networks;Feature extraction;Hyperspectral imaging;Semisupervised learning;Image classification;Gallium nitride","convolutional neural nets;feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence)","hyperspectral image classification;generative adversarial networks;dynamic neighborhood voting mechanism;multilayer features fusion;HSI classification;CNN spectral classifier;feature extraction;Indian Pines dataset","","4","","15","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Multi-Source Medical Image Fusion Based on Wasserstein Generative Adversarial Networks","Z. Yang; Y. Chen; Z. Le; F. Fan; E. Pan","School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Access","12 Dec 2019","2019","7","","175947","175958","In this paper, we propose the medical Wasserstein generative adversarial networks (MWGAN), an end-to-end model, for fusing magnetic resonance imaging (MRI) and positron emission tomography (PET) medical images. Our method establishes two adversarial games between a generator and two discriminators to generate a fused image with the details of soft tissue structures in organs from MRI images and the functional and metabolic information from PET images. Different information from source images can be effectively adjusted with a specifically designed loss function. In addition, we use WGAN instead of the traditional generative adversarial networks to make the training process more stable and allow our architecture to deal with source images of different resolutions. Qualitative and quantitative comparisons on publicly available datasets demonstrate the superiority of MWGAN over the state-of-the-art networks. Furthermore, our MWGAN is applied to the fusion of MRI and computed tomography images of different resolutions, achieving a satisfactory performance.","2169-3536","","10.1109/ACCESS.2019.2955382","National Natural Science Foundation of China(grant numbers:61605146); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911320","Medical image fusion;Wasserstein generative adversarial networks;end-to-end;different resolutions","Magnetic resonance imaging;Image fusion;Generative adversarial networks;Biomedical imaging;Deep learning;Training;Image resolution","biomedical MRI;image fusion;image resolution;medical image processing;neural nets;positron emission tomography","multisource medical image fusion;medical Wasserstein generative adversarial networks;MWGAN;end-to-end model;adversarial games;fused image;MRI images;functional information;metabolic information;PET images;source images;specifically designed loss function;computed tomography images;magnetic resonance imaging;positron emission tomography medical images;soft tissue structures;biological organs","","13","","36","CCBY","25 Nov 2019","","","IEEE","IEEE Journals"
"STPGANsFusion: Structure and Texture Preserving Generative Adversarial Networks for Multi-modal Medical Image Fusion","D. Shah; H. Wani; M. Das; D. Gupta; P. Radeva; A. Bakde","Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Electronics and Communication Engineering, Visvesvaraya National Institute of Technology, Nagpur, India; Department of Mathematics and Computer Science, Universitat de Barcelona & Computer Vision Center, Barcelona, Spain; Department of Radio-Diagnosis, All India Institute of Medical Sciences, Nagpur, India","2022 National Conference on Communications (NCC)","4 Jul 2022","2022","","","172","177","Medical images from various modalities carry diverse information. The features from these source images are combined into a single image, constituting more information content, beneficial for subsequent medical applications. Recently, deep learning (DL) based networks have demonstrated the ability to produce promising fusion results by integrating the feature extraction and preservation task with less manual interventions. However, using a single network for extracting features from multi-modal source images characterizing distinct information results in the loss of crucial diagnostic information. Addressing this problem, we present structure and texture preserving generative adversarial networks based medical image fusion method (STPGANsFusion). Initially, the textural and structural components of the source images are separated using structure gradient and texture decorrelating regularizer (SGTDR) based image decomposition for more complementary information preservation and higher robustness for the model. Next, the fusion of the structure and the texture components is carried out using two generative adversarial networks (GANs) consisting of a generator and two discriminators to get fused structure and texture components. The loss function for each GAN is framed as per the characteristic of the component being fused to minimize the loss of complementary information. The fused image is reconstructed and undergoes adaptive mask-based structure enhancement to further boost its contrast and visualization. Substantial experimentation is carried out on a wide variety of neurological images. Visual and qualitative results exhibit notable improvement in the fusion performance of the proposed method in comparison to the state-of-the-art fusion methods.","","978-1-6654-5136-9","10.1109/NCC55593.2022.9806733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9806733","Computed tomography;single-photon emission computed tomography;positron emission tomography;image fusion;generative adversarial networks;structure-texture decomposition","Visualization;Adaptation models;Statistical analysis;Generative adversarial networks;Feature extraction;Image decomposition;White matter","feature extraction;gradient methods;image fusion;image texture;medical image processing","STPGANsFusion;generative adversarial networks;multimodal medical image fusion;medical images;diverse information;single image;information content;subsequent medical applications;deep learning based networks;promising fusion results;feature extraction;preservation task;multimodal source images;distinct information results;crucial diagnostic information;medical image fusion method;textural components;structural components;structure gradient;complementary information preservation;texture components;fused structure;fused image;adaptive mask-based structure enhancement;neurological images;fusion performance;state-of-the-art fusion methods","","","","17","IEEE","4 Jul 2022","","","IEEE","IEEE Conferences"
"A Cross-Spectral Image Fusion Based Visual Odometry","Y. Zhou; K. Li","School of Automotive Engineering, Wuhan University of Technology, Wuhan, China; School of Automotive Engineering, Wuhan University of Technology, Wuhan, China","2020 35th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","5 Feb 2021","2020","","","428","432","This paper proposes a cross-spectral image fusion based visual odometry with GAN (Generative Adversarial Networks) and Residual Dense Blocks. It can well preserve the intensity information in infrared images and the gradient information in visible images. Based on fused images, the pose can be obtained. Experimental verification was performed on All-Day Visual Place Recognition Benchmark Dataset. And results in experiment show that compared to merely infrared and visible images, the positioning accuracy of the fusion image has been greatly improved.","","978-1-7281-7684-0","10.1109/YAC51587.2020.9337596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337596","GAN;Residual Dense Blocks;Image Fusion;Visual Odometry","Visualization;Neural networks;Generative adversarial networks;Feature extraction;Image fusion;Optical flow;Visual odometry","distance measurement;image fusion;image recognition;infrared imaging;mobile robots;object recognition;robot vision;sensor fusion","visible images;fusion image;cross-spectral image fusion;visual odometry;Generative Adversarial Networks;Residual Dense Blocks;infrared images;All-Day Visual Place Recognition Benchmark Dataset","","","","23","IEEE","5 Feb 2021","","","IEEE","IEEE Conferences"
"MEF-GAN: Multi-Exposure Image Fusion via Generative Adversarial Networks","H. Xu; J. Ma; X. -P. Zhang","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Department of Electrical, Computer, and Biomedical Engineering, Ryerson University, Toronto, Canada","IEEE Transactions on Image Processing","8 Jul 2020","2020","29","","7203","7216","In this paper, we present an end-to-end architecture for multi-exposure image fusion based on generative adversarial networks, termed as MEF-GAN. In our architecture, a generator network and a discriminator network are trained simultaneously to form an adversarial relationship. The generator is trained to generate a real-like fused image based on the given source images which is expected to fool the discriminator. Correspondingly, the discriminator is trained to distinguish the generated fused images from the ground truth. The adversarial relationship makes the fused image not limited to the restriction of the content loss. Therefore, the fused images are closer to the ground truth in terms of probability distribution, which can compensate for the insufficiency of single content loss. Moreover, aiming at the problem that the luminance of multi-exposure images varies greatly with spatial location, the self-attention mechanism is employed in our architecture to allow for attention-driven and long-range dependency. Thus, local distortion, confusing results, or inappropriate representation can be corrected in the fused image. Qualitative and quantitative experiments are performed on publicly available datasets, where the results demonstrate that MEF-GAN outperforms the state-of-the-art, in terms of both visual effect and objective evaluation metrics. Our code is publicly available at https://github.com/jiayi-ma/MEF-GAN.","1941-0042","","10.1109/TIP.2020.2999855","National Natural Science Foundation of China(grant numbers:61773295); Natural Science Foundation of Hubei Province(grant numbers:2019CFA037); Natural Sciences and Engineering Research Council of Canada (NSERC)(grant numbers:RGPIN-2020-04661); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9112609","Image fusion;multi-exposure;generative adversarial network;self-attention","Gallium nitride;Image fusion;Generative adversarial networks;Generators;Dynamic range;Feature extraction;Training","image fusion;neural nets;probability;statistical distributions","probability distribution;multiexposure images;source images;discriminator network;end-to-end architecture;generative adversarial networks;multiexposure image fusion;MEF-GAN","","73","","53","IEEE","9 Jun 2020","","","IEEE","IEEE Journals"
"Cross-Modality Distillation: A Case for Conditional Generative Adversarial Networks","S. Roheda; B. S. Riggan; H. Krim; L. Dai","Department of Electrical and Computer Enzineering, North Carolina State University, Raleigh, NC, USA; U.S. Army Research Laboratory, Adelphi, MD, USA; Department of Electrical and Computer Enzineering, North Carolina State University, Raleigh, NC, USA; U.S. Army Research Office, Durham, NC, USA","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","2926","2930","In this paper, we propose to use a Conditional Generative Adversarial Network (CGAN) for distilling (i.e. transferring) knowledge from sensor data and enhancing low-resolution target detection. In unconstrained surveillance settings, sensor measurements are often noisy, degraded, corrupted, and even missing/absent, thereby presenting a significant problem for multi-modal fusion. We therefore specifically tackle the problem of a missing modality in our attempt to propose an algorithm based on CGANs to generate representative information from the missing modalities when given some other available modalities. Despite modality gaps, we show that one can distill knowledge from one set of modalities to another. Moreover, we demonstrate that it achieves better performance than traditional approaches and recent teacher-student models.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8462082","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462082","Missing Modalities;Generative Adversarial Networks;Target Detection;Multi-Modal Fusion","Generators;Frequency modulation;Training;Acoustics;Acoustic sensors;Object detection;Generative adversarial networks","feature extraction;image fusion;image resolution;object detection;sensor fusion;surveillance","unconstrained surveillance settings;sensor measurements;significant problem;multimodal fusion;CGAN;cross-modality distillation;sensor data;conditional generative adversarial network;low-resolution target detection enhancement","","10","","21","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"Laplacian Pyramid Generative Adversarial Network for Infrared and Visible Image Fusion","H. Yin; J. Xiao","College of Automation and College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Automation and College of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Signal Processing Letters","28 Sep 2022","2022","29","","1988","1992","Generative adversarial network (GAN) has recently demonstrated a powerful tool for infrared and visible image fusion. However, existing methods extract the features incompletely, miss some textures, and lack the stability of training. To cope with these issues, this article proposes a novel image fusion Laplacian pyramid GAN (IF-LapGAN). Firstly, a generator is constructed which consists of shallow features extraction module, Laplacian pyramid module, and reconstruction module. Specifically, the Laplacian pyramid module is a pyramid-style encoder-decoder architecture, which progressively extracts the multi-scale features. Moreover, the attention module is equipped in the decoder to effectively decode the salient features. Then, two discriminators are adopted to discriminate the fused image and two different modalities respectively. To improve the stability of adversarial learning, we propose to develop another side supervised loss based on the side pre-trained fusion network. Extensive experiments show that IF-LapGAN achieves 3.27%, 27.28%, 6.32%, 1.39%, 3.14%, 1.15% and 1.07% improvement gains in terms of $Q_{NMI}$, $Q_{M}$, $Q_{Yang}$, $Q^{AB/F}$, MI, VIF, and FMI, respectively, compared with the second best values.","1558-2361","","10.1109/LSP.2022.3207621","National Natural Science Foundation of China(grant numbers:61971237); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894670","Image fusion;generative adversarial network;Laplacian pyramid;attention module","Feature extraction;Laplace equations;Image fusion;Generative adversarial networks;Generators;Training;Decoding","feature extraction;image coding;image fusion;learning (artificial intelligence)","adversarial learning;side pre-trained fusion network;Laplacian pyramid generative adversarial network;visible image fusion;image fusion Laplacian pyramid GAN;shallow feature extraction module;Laplacian pyramid module;reconstruction module;pyramid-style encoder-decoder architecture;multiscale features;attention module;salient features","","","","34","IEEE","19 Sep 2022","","","IEEE","IEEE Journals"
"Fusion of Infrared and Visible Images Based on Improved Generative Adversarial Networks","S. Wang; X. Li; W. Huo; J. You","University Of Science & Technology Beijing, Beijing, China; University Of Science & Technology Beijing, Beijing, China; University Of Science & Technology Beijing, Beijing, China; University Of Science & Technology Beijing, Beijing, China","2022 3rd International Conference on Information Science, Parallel and Distributed Systems (ISPDS)","7 Sep 2022","2022","","","247","251","In order to improve the fusion quality of infrared and visible light images, enhance the visual effect of fused images, and solve the problems that traditional fusion methods need to manually set fusion rules and the background details of fused images are poorly preserved, this paper proposes an improved generative adversarial network that combines multi-scale information. The generator used in this method is a typical encoder and decoder structure, and the discriminator uses a dual discriminator to establish the confrontation relationship between the infrared source image, the visible light source image and the fusion image respectively. Before the source image is input to the encoder, multi-scale information is introduced through the Inception network, which effectively extracts the multi-scale features of the image, which ensures the subsequent improvement of the quality of the fusion image. In addition, the loss function is improved to retain more background details and highlight infrared feature information. The control experiment results show that the method in this paper obtains better fusion effect in subjective and objective evaluation.","","978-1-6654-8747-4","10.1109/ISPDS56360.2022.9874034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874034","image fusion;generative adversarial network;multi-scale;infrared image;visible light image","Information science;Generative adversarial networks;Feature extraction;Visual effects;Generators;Decoding;Data mining","discriminators;feature extraction;image fusion;infrared imaging","infrared images;visible images;improved generative adversarial network;fusion quality;visible light images;visual effect;traditional fusion methods;fusion rules;multiscale information;infrared source image;visible light source image;fusion image;inception network;multiscale features;highlight infrared feature information;fusion effect;dual discriminator","","","","12","IEEE","7 Sep 2022","","","IEEE","IEEE Conferences"
"Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","L. Maggiolo; D. Solarna; G. Moser; S. B. Serpico","University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2089","2092","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type l2 similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an l2 similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323235","European Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323235","Multisensor image registration;conditional generative adversarial network;$\ell^{2}$ similarity;COBYLA","Radar polarimetry;Optical imaging;Measurement;Optical sensors;Feature extraction;Training;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;image registration;radar imaging;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","automatic area-based registration;generative adversarial networks;correlation-type metric;automatic registration;multisensor remote sensing images;textural properties;optical-SAR images;deep learning;area-based registration concepts;conditional generative adversarial network;area-based cross-correlation-type l;image translation capabilities;Sentinel-2 data","","4","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Single Sensor Image Fusion Using A Deep Convolutional Generative Adversarial Network","F. Palsson; J. R. Sveinsson; M. O. Ulfarsson","Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering Hjardarhagi 2-6, University of Iceland, Reykjavik, Iceland","2018 9th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","27 Jun 2019","2018","","","1","5","Recently deployed multispectral sensors can acquire multispectral images where different bands have different spatial resolution depending on wavelength. An example is the Sentinel-2 constellation which can acquire multispectral bands of 10 m, 20 m, and 60 m resolution, covering the visible, near-infrared (NIR) and short-wave infrared (SWIR) parts of the electromagnetic spectrum. In this paper, a method to perform image fusion of the fine and coarse spatial resolution bands to increase the resolution of the coarser bands is proposed. The method is based on a so-called Generative Adversarial Network (GAN) and uses a deep convolutional design for both the generator and the discriminator. In experiments, it is demonstrated that the proposed method gives good results when compared to state-of-the-art single sensor image fusion methods using both simulated and real Sentinel-2 datasets.","2158-6276","978-1-7281-1581-8","10.1109/WHISPERS.2018.8747268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747268","Image fusion;generative adversarial network;convolutional network;Sentinel-2","Image resolution;Image fusion;Generators;Gallium nitride;Training;Generative adversarial networks;Signal resolution","geophysical image processing;geophysical techniques;image fusion;image resolution;image sensors;remote sensing","deep convolutional Generative Adversarial Network;multispectral images;Sentinel-2 constellation;multispectral bands;electromagnetic spectrum;fine resolution bands;coarse spatial resolution bands;deep convolutional design;Sentinel-2 datasets;multispectral sensors;visible band;short-wave infrared band;single sensor image fusion methods;near-infrared band;wavelength 10.0 m;wavelength 60.0 m;wavelength 20.0 m","","7","","16","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Multi-modal Medical Image Fusion Based on GAN and the Shift-Invariant Shearlet Transform","L. Wang; C. Chang; B. Hao; C. Liu","School of Computer Science and Technology, Shandong University of Technology, Zibo, China; School of Computer Science and Technology, Shandong University of Technology, Zibo, China; School of Computer Science and Technology, Shandong University of Technology, Zibo, China; Anhui Key Laboratory of Plant Resources and Plant Biology, Huaibei Normal University, Huaibei, China","2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","13 Jan 2021","2020","","","2538","2543","The typical medical image fusion methods are of great practicability in computer-aided medical diagnosis, however, they are facing with some confusing disadvantages, for example, the loss of the important features, the inefficiency of the used fusion rules. In order to obtain the better fusion results to facilitate the doctors to read the comprehensive medical information, a novel medical image fusion method based on the generative adversarial network (GAN) and the shift-invariant shearlet transform (SIST) is developed. The source images are firstly decomposed into the high-pass and low-pass sub-bands by the SIST. Then, the combination of the high-pass sub-bands in different levels is implemented by the trained GAN model, which mainly consists of two parts: the construction of the generator and the discriminator; for the low-frequency coefficients, the local energy weighted summation and bilateral filter are used to calculate the low-pass details by the preserving of the local energy. Finally, the fusion results can be shown by inversion of the SIST. Under the subjective comparison and four typical objective measurements, i.e. the standard deviation, entropy, QAB/F, and the mutual information, the experimental results indicate that the artifacts and distortions can be effectively suppressed and the detail information can be well preserved by the proposed method, as well as the better quantitative performance.","","978-1-7281-6215-7","10.1109/BIBM49941.2020.9313288","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313288","Medical image fusion;shift-invariant shearlet transform;Generative adversarial network;Bilateral filter","Generators;Gallium nitride;Laplace equations;Medical diagnostic imaging;Feature extraction;Image fusion;Generative adversarial networks","entropy;filtering theory;image fusion;medical image processing;transforms","multimodal medical image fusion;shift-invariant shearlet transform;computer-aided medical diagnosis;fusion rules;comprehensive medical information;generative adversarial network;SIST;source images;low-pass sub-bands;high-pass sub-bands;trained GAN model;low-frequency coefficients;low-pass details;typical objective measurements;local energy weighted summation;bilateral filter;local energy;standard deviation;entropy;mutual information;quantitative performance","","2","","15","IEEE","13 Jan 2021","","","IEEE","IEEE Conferences"
"Infrared and Visible Image Fusion via Multi-discriminators Wasserstein Generative Adversarial Network","J. Li; H. Huo; K. Liu; C. Li; S. Li; X. Yang","School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; Remote sensing center of public security, People’s Public Security University of China, BeiJing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China; School of Information Technology and Cyber Security, People’s Public Security University of China, BeiJing, China","2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)","17 Feb 2020","2019","","","2014","2019","Generative adversarial network (GAN) has been widely applied to infrared and visible image fusion. However, the existing GAN-based image fusion methods only establish one discriminator in the network to make the fused image capture gradient information from the visible image, which may result in the loss of some infrared intensity information and texture information on the fused images. To solve this problem and improve the performance of GAN, we extend GAN to multiple discriminators and propose an end-to-end multi-discriminators Wasserstein generative adversarial network (MD-WGAN). In this framework, the fused image can preserve major infrared intensity and detail information from the first discriminator, and keep more texture information that existing in visible image from the second discriminator. We also design a texture loss function via local binary patterns to preserve more texture from visible image. The extensive qualitative and quantitative experiments show the advantages of our method compared with other state-of-the-art fusion methods.","","978-1-7281-4550-1","10.1109/ICMLA.2019.00322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999112","Wasserstein generative adversarial network, image fusion, infrared image, visible image","Generators;Generative adversarial networks;Image fusion;Gallium nitride;Machine learning;Security;Information technology","gradient methods;image fusion;image texture;infrared imaging;neural nets","visible image;fused image capture gradient information;infrared intensity information;texture information;end-to-end multidiscriminators Wasserstein generative adversarial network;detail information;GAN-based image fusion methods","","1","","29","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"Multi-Focus Image Fusion Algorithm Based on Non-Uniform Rectangular Partition and Generative Adversarial Network","X. Hong; U. Kintak","Faculty of Information Technology, Macau University of Science and Technology, Macau, China; Faculty of Information Technology, Macau University of Science and Technology, Macau, China","2019 International Conference on Wavelet Analysis and Pattern Recognition (ICWAPR)","2 Jan 2020","2019","","","1","6","Based on Non-uniform Rectangular Partition (NURP) and Generative Adversarial Network (GAN), this paper proposes an effective multi-focus image fusion method to generate a full-focus image by combining multi-focus images. Firstly, NURP is applied to left-focus and right-focus images, the size of partitioning grids obtained can be used to judge the fusion pixel to form a rough Fusion Guiding Map (FGM) which will be further optimized by morphological operation and manual adjustment to form an optimized FGM. Then the rough FGM and optimized FGM become the training dataset for the pix2pix GAN. After finishing the training, the trained pix2pix model can be used to optimize any rough FGM from NURP. Finally, the fused pixels are determined according to the FGM to construct the final fused image. The experimental results show that the algorithm improves the visual clarity of the fused image by enhancing the spatial detail of the image and obtains better objective evaluation indicators.","2158-5709","978-1-7281-2996-9","10.1109/ICWAPR48189.2019.8946467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946467","Multi-Focus Image fusion;Non-uniform rectangular partition;Generative adversarial network","Training;Generative adversarial networks;Approximation algorithms;Wavelet analysis;Partitioning algorithms;Gallium nitride;Image fusion","image fusion;mathematical morphology;neural nets","multifocus image fusion algorithm;NURP;full-focus image;multifocus images;right-focus images;partitioning grids;fusion pixel;morphological operation;optimized FGM;rough FGM;pix2pix GAN;trained pix2pix model;fused image;left-focus images;generative adversarial network;nonuniform rectangular partition;fusion guiding map","","2","","17","IEEE","2 Jan 2020","","","IEEE","IEEE Conferences"
"Large-Factor Super-Resolution of Remote Sensing Images With Spectra-Guided Generative Adversarial Networks","Y. Meng; W. Li; S. Lei; Z. Zou; Z. Shi","Shanghai Artificial Intelligence Laboratory, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; AVIC Chengdu Aircraft Industrial (Group) Company Ltd., Chengdu, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2022","2022","60","","1","11","Large-factor image super-resolution (SR) is a challenging task due to the high uncertainty and incompleteness of the missing details to be recovered. In remote sensing images, the subpixel spectral mixing and semantic ambiguity of ground objects make this task even more challenging. In this article, we propose a novel method for large-factor SR of remote sensing images named spectra-guided generative adversarial networks (SpecGANs). In response to the above problems, we explore whether introducing additional hyperspectral images (HSIs) to GAN as conditional input can be the key to solving the problems. Different from previous approaches that mainly focus on improving the feature representation of a single source input, we propose a dual-branch network architecture to effectively fuse low-resolution (LR) red, green, blue (RGB) images and corresponding HSIs, which fully exploit the rich hyperspectral information as conditional semantic guidance. Due to the spectral specificity of ground objects, the semantic accuracy of the generated images is guaranteed. To further improve the visual fidelity of the generated output, we also introduce the Latent Code Bank with rich visual priors under a generative adversarial training framework so that high-resolution, detailed, and realistic images can be progressively generated. Extensive experiments show the superiority of our method over the state-of-art image SR methods in terms of both quantitative evaluation metrics and visual quality. Ablation experiments also suggest the necessity of adding spectral information and the effectiveness of our designed fusion module. To our best knowledge, we are the first to achieve up to 32x SR of remote sensing images with high visual fidelity under the premise of accurate ground object semantics. Our code can be publicly available at https://github.com/YapengMeng/SpecGAN.","1558-0644","","10.1109/TGRS.2022.3222360","National Key Research and Development Program of China (Titled “Brain-inspired General Vision Models and Applications”); National Natural Science Foundation of China(grant numbers:62125102); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950553","Deep convolutional neural networks (CNNs);generative adversarial networks (GANs);hyperspectral image (HSI);remote sensing image;super-resolution (SR)","Superresolution;Hyperspectral imaging;Semantics;Task analysis;Visualization;Generative adversarial networks;Image reconstruction","geophysical image processing;image classification;image fusion;image reconstruction;image resolution;learning (artificial intelligence);remote sensing","accurate ground object semantics;additional hyperspectral images;factor super-resolution;generative adversarial training framework;ground objects;large-factor image super-resolution;large-factor SR;realistic images;remote sensing images;spectra-guided generative adversarial networks;state-of-art image SR methods","","","","63","IEEE","14 Nov 2022","","","IEEE","IEEE Journals"
"Semantic-Aware Infrared and Visible Image Fusion","W. Zhou; W. Wu; H. Zhou","School of Computer Science and Engineering, Wuhan Institute of Technology, Wuhan, China; School of Computer Science and Engineering, Wuhan Institute of Technology, Wuhan, China; School of Computer Science and Engineering, Wuhan Institute of Technology, Wuhan, China","2021 4th International Conference on Robotics, Control and Automation Engineering (RCAE)","14 Dec 2021","2021","","","82","85","Aiming at the problems of poor visual effect and lack of background details in infrared and visible image fusion, we proposed an image fusion algorithm based on semantic segmentation. This method obtains the position and shape of each target in the source image through semantic segmentation, and we will set the weight value for each target, so that the information of the image can be preserved to a large extent. In addition, we also design a generative adversarial network, which uses different loss functions to adjust the generator and discriminator to ensure that the fused image is clearer and has richer texture features. Experimental results show that our method is superior to the new method in both visual effect and qualitative index.","","978-1-6654-2730-2","10.1109/RCAE53607.2021.9638835","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9638835","image fusion;semantic segmentation;generative adversarial network","Deep learning;Image segmentation;Shape;Semantics;Medical treatment;Visual effects;Generative adversarial networks","image enhancement;image fusion;image resolution;image segmentation;image texture;infrared imaging","source image;semantic segmentation;generative adversarial network;fused image;semantic-aware infrared image fusion;visible image fusion;poor visual effect;background details;image fusion algorithm;texture features","","","","14","IEEE","14 Dec 2021","","","IEEE","IEEE Conferences"
"An Unsupervised GAN-based Quality-enhanced Medical Image Fusion Network","L. Yanli; L. Zimu; F. Junce; Y. Gu","Maynooth International Engineering College, Fuzhou University, Fuzhou City, China; Maynooth International Engineering College, Fuzhou University, Fuzhou City, China; Maynooth International Engineering College, Fuzhou University, Fuzhou City, China; Maynooth International Engineering College, Fuzhou University, Fuzhou City, China","2022 IEEE Conference on Telecommunications, Optics and Computer Science (TOCS)","18 Jan 2023","2022","","","429","432","Medical image fusion technology can improve the precision of clinical diagnosis by fusing medical information from different modalities. However, the quality of fusion is restricted due to the particular imaging mechanism. This paper proposes a quality-enhanced medical image fusion algorithm based on a generative adversarial network for the lossless fusion of MRI and PET images. It consists of a lightweight image enhancement depth network to make the quality of the fused image suit human vision perceptual system better and a generative adversarial network to enhance texture details and edge information further. Our model is unsupervised and does not require paired fused images for training. The test results show that our algorithm performs better in both subjective visual effects and objective evaluation metrics.","","978-1-6654-7053-7","10.1109/TOCS56154.2022.10016141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016141","Medical image fusion;Multi-modality;Quality-enhanced GAN;Unsupervised learning","Training;Measurement;Magnetic resonance imaging;Generative adversarial networks;Visual effects;Telecommunications;Medical diagnostic imaging","biomedical MRI;image enhancement;image fusion;image texture;medical image processing;positron emission tomography;sensor fusion","fused image suit human vision perceptual system;generative adversarial network;lightweight image enhancement depth network;lossless fusion;medical image fusion technology;medical information;particular imaging mechanism;quality-enhanced medical image fusion algorithm;unsupervised GAN-based quality-enhanced medical image fusion network","","","","4","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"LatRAIVF: An Infrared and Visible Image Fusion Method Based on Latent Regression and Adversarial Training","X. Luo; A. Wang; Z. Zhang; X. Xiang; X. -J. Wu","School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China; School of Electronic and Information Engineering, Suzhou University of Science and Technology, Suzhou, China; Key Laboratory of Information Perception and Systems for Public Security of MIIT, Nanjing University of Science and Technology, Nanjing, China; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China","IEEE Transactions on Instrumentation and Measurement","31 Aug 2021","2021","70","","1","16","In this article, we propose a novel method for infrared and visible image fusion based on latent regression and adversarial training, which is named as LatRAIVF. Compared to existing deep learning (DL)-based image fusion method that only focuses on the spatial information, we consider to utilize the information provided by high-level feature maps from latent space, which can guide the network to learn about semantically important feature information. The proposed method is based on the framework of conditional generative adversarial network (GAN), and two encoders are adopted to learn the respective semantic latent representations for the infrared and visible images, which are then combined by max-selection strategy and input into the decoder, with skip connections between the corresponding layers of the encoder and the decoder, to achieve the fused image. Apart from the adversarial process that enables the fused image to obtain more realistic details, we design two branches to constrain the generation of the image: a content loss to make the fused image close to the label image, and a latent regression loss to ensure the fused image with salient features from the infrared and visible images. Due to the lack of physical ground-truth fused images in public infrared and visible image datasets and the difficulties in defining desired fused image, we make use of existing RGB-D dataset to synthesize an infrared and visible image dataset with ground truths based on the widely used optical model for better network training. Comparison experiments show that the fused results of the proposed method can transfer meaningful features from the source image and provide good fusion quality.","1557-9662","","10.1109/TIM.2021.3105250","National Natural Science Foundation of China(grant numbers:61772237); Six Talent Peaks Project in Jiangsu Province(grant numbers:XYDXX-030); 111 Project of Ministry of Education of China(grant numbers:B12018); Key Laboratory Foundation of Information Perception and Systems for Public Security of Ministry of Industry and Information Technology (MIIT), (Nanjing University of Science and Technology)(grant numbers:202008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520770","Deep learning (DL);generative adversarial networks (GANs);image fusion;infrared and visible image;latent space regression","Image fusion;Training;Generative adversarial networks;Task analysis;Semantics;Generators;Optical imaging","deep learning (artificial intelligence);image colour analysis;image fusion;image representation;regression analysis","source image;LatRAIVF;adversarial training;infrared image fusion;visible image fusion;high-level feature maps;latent space;semantically important feature information;conditional generative adversarial network;semantic latent representations;label image;latent regression loss;public infrared image datasets;visible image dataset;deep learning;GAN;encoders;max-selection strategy;decoder;skip connections;content loss;RGB-D dataset;optical model;network training;fusion quality","","1","","59","IEEE","23 Aug 2021","","","IEEE","IEEE Journals"
"Remote Sensing Data Fusion With Generative Adversarial Networks: State-of-the-art methods and future research directions","P. Liu; J. Li; L. Wang; G. He","Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Magazine","13 Jul 2022","2022","10","2","295","328","In the past decades, remote sensing (RS) data fusion has always been an active research community. A large number of algorithms and models have been developed. Generative adversarial networks (GANs), as an important branch of deep learning, show promising performances in a variety of RS image fusions. This review provides an introduction to GANs for RS data fusion. We briefly review the frequently used architecture and characteristics of GANs in data fusion and comprehensively discuss how to use GANs to realize fusion for homogeneous RS, heterogeneous RS, and RS and ground observation (GO) data. We also analyze some typical applications with GAN-based RS image fusion. This review provides insight into how to make GANs adapt to different types of fusion tasks and summarizes the advantages and disadvantages of GAN-based RS data fusion. Finally, we discuss promising future research directions and make a prediction on their trends.","2168-6831","","10.1109/MGRS.2022.3165967","National Natural Science Foundation of China(grant numbers:61731022,41971397); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779854","","Data integration;Generators;Generative adversarial networks;Computer architecture;Degradation;Data models;Image fusion","data fusion;deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;remote sensing;reviews","GO data;deep learning;state-of-the-art methods;active research community;remote sensing;generative adversarial networks;GAN-based RS data fusion;GAN-based RS image fusion;ground observation data;heterogeneous RS;homogeneous RS","","4","","238","IEEE","23 May 2022","","","IEEE","IEEE Magazines"
"UFN-GAN: An unsupervised generative adversarial network for remote sensing image fusion","H. Dai; X. Liu; Y. Qiao; K. Zheng; X. Xiao; Z. Cai","School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Automation China, University of Geosciences, Whuhan, China; School of Computer Science, China University of Geosciences, Whuhan, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1803","1808","Different sensors acquire different images in the same area, such as multi-spectral (MS) images and panchromatic (PAN) images. Normally, the MS images possess high spectral resolution but low spatial resolution, while PAN images are opposite in the distribution of spectral and spatial information. Image fusion is a common method to obtain the information of PAN and MS images simultaneously. To generate clearer fusion image with abundant information, we design an unsupervised fusion net based on generative adversarial network (GAN), named UFNGAN for remote sensing image fusion. In our proposed UFNGAN, an adversarial net is designed between our generator and two discriminators to adequately retain the spectral and spatial information of original images without supervision. MS images and PAN images are fused by our generator, which consists of an encoder and a decoder. Our encoder is used to extract deeper feature maps of the original images, and the decoder is applied to rebuild images. Furthermore, the Spatial-Information-Enhancement (SIE) model is utilized to obtain spatial information of MS images for enhancing PAN image, and the Edge-Detection-Registration (EDR) method is applied to register the original images to avoid fused images distortion. At last, experiments are performed on QuickBird and GaoFen-2 datasets.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727490","Unsupervised learning;Image fusion;Generative adversarial network;Deep learning;Remote sensing images","Image edge detection;Generative adversarial networks;Feature extraction;Distortion;Generators;Decoding;Sensors","geophysical image processing;image fusion;image resolution;neural nets;remote sensing;unsupervised learning","remote sensing image fusion;multispectral images;panchromatic images;MS images;high spectral resolution;PAN images;spatial information;fusion image;unsupervised fusion net;spectral information;original images;fused images distortion;unsupervised generative adversarial network;spatial-information-enhancement model;UFNGAN;QuickBird;GaoFen-2 datasets;encoder","","","","10","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Retinal Vessel Segmentation Combined With Generative Adversarial Networks and Dense U-Net","X. Guo; C. Chen; Y. Lu; K. Meng; H. Chen; K. Zhou; Z. Wang; R. Xiao","School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Department of Ultrasound, Chinese PLA General Hospital, Beijing, China; Department of Gastroenterology and Hepatology, Chinese PLA General Hospital, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Institute of Artificial Intelligence, University of Science and Technology Beijing, Beijing, China","IEEE Access","2 Nov 2020","2020","8","","194551","194560","Retinal blood vessels are the basis for clinical diagnosis of some diseases. Achieving automatic retinal vessel segmentation from fundus images is an important and challenging work. In this paper, a neural network architecture based on the Dense U-net using Inception module is proposed for retinal vessel segmentation. First, the skip connections in traditional U-net are replaced with Dense Block to achieve full fusion of features from shallow layers to deep layers. Then, the Inception module is applied to supersede the traditional convolution operation. Thus, vessel features corresponding to the convolution kernels of different sizes can be extracted. Finally, Generative Adversarial Networks (GAN) are adopted in the training phase. The Dense U-net using Inception module is treated as the generator of GAN, and a multilayer neural network is created as the discriminator of GAN. The generator and discriminator are trained alternately. The loss function is a combination of segmentation loss and GAN loss. So that segmentation results can be fitted to the ground truth from both pixel value and pixel distribution. The algorithm proposed in this paper is verified on the public Digital Retinal Images for Vessel Extraction (DRIVE) dataset, where the Dice rate reaches 82.15%, and the AU-ROC and AU-PR reach 0.9772 and 0.9058, respectively. Experiments show that the proposed algorithm is effective in realizing automatic retinal vessel segmentation.","2169-3536","","10.1109/ACCESS.2020.3033273","National Key Research and Development Program of China(grant numbers:2017YFB1002804,2017YFB1401203); National Natural Science Foundation of China(grant numbers:61701022); Beijing Science and Technology Program(grant numbers:7182158); Fundamental Research Funds for the Central Universities(grant numbers:FRF-TP-19-015A2); Beijing Top Discipline for Artificial Intelligent Science and Engineering, University of Science and Technology Beijing; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238003","Retinal vessel segmentation;U-net;dense block;inception module;generative adversarial networks","Training;Image segmentation;Neural networks;Blood vessels;Generative adversarial networks;Retinal vessels;Generators","blood vessels;convolutional neural nets;diseases;eye;feature extraction;image fusion;image segmentation;medical image processing;neural net architecture","generative adversarial networks;dense U-net;retinal blood vessels;automatic retinal vessel segmentation;neural network architecture;Inception module;dense block;vessel features;multilayer neural network;generator;segmentation loss;GAN loss;clinical diagnosis;feature fusion;convolution kernels;loss function;pixel value;pixel distribution;digital retinal images for vessel extraction dataset;dice rate;diseases","","13","","32","CCBY","23 Oct 2020","","","IEEE","IEEE Journals"
"Overcoming Missing and Incomplete Modalities with Generative Adversarial Networks for Building Footprint Segmentation","B. Bischke; P. Helber; F. Koenig; D. Borth; A. Dengel","TU Kaiserslautern, Germany; TU Kaiserslautern, Germany; TU Kaiserslautern, Germany; German Research Center for Artificial Intelligence (DFKI), Germany; TU Kaiserslautern, Germany","2018 International Conference on Content-Based Multimedia Indexing (CBMI)","1 Nov 2018","2018","","","1","6","The integration of information acquired with different modalities, spatial resolution and spectral bands has shown to improve predictive accuracies. Data fusion is therefore one of the key challenges in remote sensing. Most prior work focusing on multi-modal fusion, assumes that modalities are always available during inference. This assumption limits the applications of multi-modal models since in practice the data collection process is likely to generate data with missing, incomplete or corrupted modalities. In this paper, we show that Generative Adversarial Networks can be effectively used to overcome the problems that arise when modalities are missing or incomplete. Focusing on semantic segmentation of building footprints with missing modalities, our approach achieves an improvement of about 2% on the Intersection over Union (IoU) against the same network that relies only on the available modality.","","978-1-5386-7021-7","10.1109/CBMI.2018.8516271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516271","Generative Adversarial Networks;Semantic Segmentation;Missing Modalities","Generators;Image segmentation;Gallium nitride;Generative adversarial networks;Training;Buildings;Feature extraction","image fusion;image segmentation;remote sensing","missing modalities;incomplete modalities;spatial resolution;spectral bands;data fusion;remote sensing;multimodal fusion;data collection process;corrupted modalities;semantic segmentation;generative adversarial networks;footprint segmentation;intersection over union;IoU","","7","","30","IEEE","1 Nov 2018","","","IEEE","IEEE Conferences"
"Multi-Source Fusion Image Semantic Segmentation Model of Generative Adversarial Networks Based on FCN","L. Zhao; Y. Wang; Z. Duan; D. Chen; S. Liu","College of Information and Control Engineering, Xi’an University of Architecture and Technology, Xi’an, China; College of Information and Control Engineering, Xi’an University of Architecture and Technology, Xi’an, China; College of Information and Control Engineering, Xi’an University of Architecture and Technology, Xi’an, China; School of Building Services Science and Engineering, Xi’an University of Architecture and Technology, Xi’an, China; College of Information and Control Engineering, Xi’an University of Architecture and Technology, Xi’an, China","IEEE Access","26 Jul 2021","2021","9","","101985","101993","At present, most of the methods used in the research of image semantic segmentation ignore the low-level feature information of image, such as space, edge, etc., which leads to the problems that the segmentation of edge and small part is not precise enough and the accuracy of segmentation result is not high. To solve this problem, this paper proposes a multi-source fusion image semantic segmentation model of generative adversarial networks based on FCN: SCAGAN. In VGG19 network, add super-pixel and edge detection algorithm, and introduce the efficient spatial pyramid module to reduce the number of parameters while adding the spatial and edge information of image; Adjust the skipping structure to better integrate the low-level features and high-level features; build a generation model DeepLab-SCFCN combining with the atrous spatial pyramid pooling to better capture the feature information of different scales of the target for segmentation; The FCN with five modules is designed as the discrimination model for GAN. It is verified on the data set PASCAL VOC 2012 that the model achieves IoU of 70.1% with a small number of network layers, and the segmentation effect of edge and small part is better at the same time. This technology can be used in image semantic segmentation.","2169-3536","","10.1109/ACCESS.2021.3097054","National Natural Science Foundation of China(grant numbers:12002251,51209167); Nature Science Foundation of Shaanxi Province(grant numbers:2019JM-474); Science and Technology Project of Xi’an(grant numbers:2020KJRC0055); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483944","Image semantic segmentation;fully convolutional networks;generative adversarial networks;efficient spatial pyramid;atrous spatial pyramid pooling","Image segmentation;Convolution;Feature extraction;Semantics;Image edge detection;Generative adversarial networks","edge detection;feature extraction;image classification;image fusion;image representation;image segmentation;object detection","spatial edge;low-level features;high-level features;generation model DeepLab-SCFCN;FCN;segmentation effect;multisource fusion image semantic segmentation model;generative adversarial networks;low-level feature information","","","","26","CCBY","14 Jul 2021","","","IEEE","IEEE Journals"
"Image Fusion via Domain and Feature Transfer","Y. Yang; H. Zhou; W. Zhang; C. Yang; Z. Yu","Computer Science and Engineering School Hubei Provincial Key Laboratory of Intelligent Robot Wuhan Institute of Technology, Wuhan, China; Computer Science and Engineering School Hubei Provincial Key Laboratory of Intelligent Robot Wuhan Institute of Technology, Wuhan, China; Computer Science and Engineering School Hubei Provincial Key Laboratory of Intelligent Robot Wuhan Institute of Technology, Wuhan, China; College of Computer and Information Sciences College of Mechanical and Electrical Engineering Fujian Agriculture and Forestry University, FuZhou, China; College of Computer and Information Sciences College of Mechanical and Electrical Engineering Fujian Agriculture and Forestry University, FuZhou, China","2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","26 Mar 2020","2019","","","1168","1172","The infrared and visible images fusion technique is designed to simultaneously preserve the thermal radiation information of the infrared image and the detailed texture information of the visible image. Therefore, the fused image with rich detail information and a clear target area in order to find it more efficiently. Inspired by the powerful GANs technology in recent years, we proposed a novel method for fusing infrared and visible images that based on domain and feature transfer. First, it is designated as an optimization problem with a latent encoding which can be mapped into a pixel intensity consistent image on the latent image. We employ generative adversarial network, which can capture content characteristics of one image dataset and figure out how these characteristics could be translated into the domain of another image dataset, to transfer the appearance of an image from visible image to infrared one. And then we use the feature consistent constrains to enhance the features of the fused image. Moreover, by adding gradient constraints to preserve the details of visible image, therefore we combine the fused image with a similar gradient to the visible image. Qualitative and quantitative comparisons on public datasets demonstrate our proposed method is superior to the state-of-the-art technology. The fused image we have generated is more like a high-resolution enhanced infrared image, which is more efficient for discovering the target and tracking.","","978-1-7281-4328-6","10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9047282","infrared and visible images fusion, pixel intensity consistent, feature/gradient consistent constrains","Image fusion;Generative adversarial networks;Feature extraction;Gallium nitride;Imaging;Convolutional neural networks;Manifolds","image enhancement;image fusion;image resolution;image texture;infrared imaging;neural nets;optimisation","image dataset;image fusion;feature transfer;infrared images;visible images;pixel intensity consistent image;infrared image resolution;capture content characteristics;optimization;latent encoding;GAN;generative adversarial network;texture information;thermal radiation information","","","","19","IEEE","26 Mar 2020","","","IEEE","IEEE Conferences"
"An Infrared and Visible Image Fusion Method Based on Adaptive Weight Learning","D. Yi; Z. Jia; H. Wei; P. Shi; C. Feng","Science and Technology on Optical Radiation Laboratory, Beijing, China; Science and Technology on Electromagnetic Scattering Laboratory, Beijing, China; Science and Technology on Optical Radiation Laboratory, Beijing, China; Science and Technology on Optical Radiation Laboratory, Beijing, China; Science and Technology on Optical Radiation Laboratory, Beijing, China","2021 International Conference on Intelligent Computing, Automation and Systems (ICICAS)","7 Mar 2022","2021","","","55","59","Aiming at the low utilization efficiency of depth features and insufficient restoration of fusion image texture information in existing fusion algorithm based on adversarial neural network, this paper proposed an infrared and visible image fusion technology based on adaptive weight Learning. Under the framework of the adversarial neural network, a deep feature adaptive extraction module based on pixel-level attention mechanism and fusion weight adaptive generation was designed to efficiently extract image features, and then effectively combine multiple depth features adaptive extraction modules through hierarchical cascade to further improve the ability of deep-level feature extraction and build a generator network. Then, a discriminator group based on a two-way neural network was constructed. Finally, the fusion image was completed through the game confrontation between the generator and the discriminator. The experimental results show that the fusion image generated by the proposed method can well restore the infrared characteristic information of the infrared band data and the detailed scene information in the visible light data, and has a strong ability to enhance the fusion of infrared and visible images.","","978-1-6654-2810-1","10.1109/ICICAS53977.2021.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9723715","adaptive weight learning;image fusion;generative adversarial networks;deep learning","Integrated optics;Image texture;Adaptive systems;Neural networks;Optical fiber networks;Feature extraction;Optical imaging","feature extraction;image fusion;image texture;infrared imaging;learning (artificial intelligence);neural nets","fusion image texture information;adversarial neural network;infrared image fusion;visible image fusion;adaptive weight Learning;deep feature adaptive extraction module;pixel-level attention mechanism;image features;adaptive extraction modules;deep-level feature extraction;generator network;two-way neural network;infrared characteristic information;infrared band data;visible light data;depth features","","","","15","IEEE","7 Mar 2022","","","IEEE","IEEE Conferences"
"Fusion of Infrared-Visible Images in UE-IoT for Fault Point Detection Based on GAN","B. Liao; Y. Du; X. Yin","School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China; School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China; School of Electrical and Electronic Engineering, North China Electric Power University, Beijing, China","IEEE Access","8 May 2020","2020","8","","79754","79763","In the development of modern intelligent Ubiquitous Electric Internet of Things (UE-IoT), infrared thermal imaging always plays an important role in automated early-warning detection of developing failures of critical assets such as transformers, disconnects and capacity banks in electrical power substation non-intrusively. However, the low resolution and contrast of infrared images hinder the subsequent analysis and recognition of fault points. In contrast, visible images present abundant texture details of the equipment without thermal information. In order to assist the detection of fault points, this paper proposes a Generative adversarial networks (GAN) based infrared and visible image fusion method to produce a composite image with enhanced edges and better quality. The edge loss function is added to represent the perceptual edges. In the discriminator, the proposed method improves the texture similarity between fusion image and visible image by minimizing the Wasserstein distance in VGG (Visual Geometry Group network) feature space. The experimental results show that the fault regions become more salient and the details are enhanced. In this way, it can facilitate the detection of fault points both reliably and accurately.","2169-3536","","10.1109/ACCESS.2020.2990539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9078757","Image fusion;VGG;generative adversarial networks;ubiquitous electric Internet of Things","Generative adversarial networks;Image fusion;Feature extraction;Fault detection;Transforms;Internet of Things;Image edge detection","alarm systems;edge detection;failure analysis;fault diagnosis;feature extraction;image fusion;image resolution;image segmentation;image texture;infrared imaging;Internet of Things;minimisation;neural nets;power engineering computing;substation protection","texture details;thermal information;generative adversarial networks;GAN;visible image fusion method;composite image;enhanced edges;edge loss function;perceptual edges;fault regions;infrared-visible images;UE-IoT;fault point detection;infrared thermal imaging;automated early-warning detection;electrical power substation;critical asset failure;intelligent ubiquitous electric Internet of Things;Wasserstein distance minimization;VGG feature space;visual geometry group network feature space","","8","","45","CCBY","27 Apr 2020","","","IEEE","IEEE Journals"
"Fusion of Heterogeneous Adversarial Networks for Single Image Dehazing","J. Park; D. K. Han; H. Ko","School of Electrical Engineering, Korea University, Seoul, South Korea; Information Science Division, Army Research Laboratory (ARL), Adelphi, USA; School of Electrical Engineering, Korea University, Seoul, South Korea","IEEE Transactions on Image Processing","2 Mar 2020","2020","29","","4721","4732","In this paper, we propose a novel image dehazing method. Typical deep learning models for dehazing are trained on paired synthetic indoor dataset. Therefore, these models may be effective for indoor image dehazing but less so for outdoor images. We propose a heterogeneous Generative Adversarial Networks (GAN) based method composed of a cycle-consistent Generative Adversarial Networks (CycleGAN) for producing haze-clear images and a conditional Generative Adversarial Networks (cGAN) for preserving textural details. We introduce a novel loss function in the training of the fused network to minimize GAN generated artifacts, to recover fine details, and to preserve color components. These networks are fused via a convolutional neural network (CNN) to generate dehazed image. Extensive experiments demonstrate that the proposed method significantly outperforms the state-of-the-art methods on both synthetic and real-world hazy images.","1941-0042","","10.1109/TIP.2020.2975986","Brain Korea 21Plus Project in2019; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018375","Image dehazing;generative adversarial networks;fusion method","Atmospheric modeling;Image color analysis;Training;Scattering;Estimation;Gallium nitride;Generative adversarial networks","convolutional neural nets;image colour analysis;image fusion;image restoration;learning (artificial intelligence)","loss function;cGAN;CycleGAN;deep learning models;real-world hazy images;dehazed image;convolutional neural network;fused network;conditional generative adversarial networks;haze-clear images;cycle-consistent generative adversarial networks;GAN;heterogeneous generative adversarial networks based method;outdoor images;indoor image dehazing;paired synthetic indoor dataset;image dehazing method;single image dehazing","","46","","55","IEEE","28 Feb 2020","","","IEEE","IEEE Journals"
"Semisupervised Remote Sensing Image Fusion Using Multiscale Conditional Generative Adversarial Network With Siamese Structure","X. Jin; S. Huang; Q. Jiang; S. -J. Lee; L. Wu; S. Yao","Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Institute of Technology Management, National Chiao Tung University, Hsinchu, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China; Engineering Research Center of Cyberspace, Yunnan University, Kunming, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Jul 2021","2021","14","","7066","7084","Remote sensing image fusion (RSIF) can generate an integrated image with high spatial and spectral resolution. The fused remote sensing image is conducive to applications including disaster monitoring, ecological environment investigation, and dynamic monitoring. However, most existing deep learning based RSIF methods require ground truths (or reference images) to train a model, and the acquisition of ground truths is a difficult problem. To address this, we propose a semisupervised RSIF method based on the multiscale conditional generative adversarial networks by combining the multiskip connection and pseudo-Siamese structure. This new method can simultaneously extract the features of panchromatic and multispectral images to fuse them without a ground truth; the adopted multiskip connection contributes to presenting image details. In addition, we propose a composite loss function, which combines the least squares loss, L1 loss, and peak signal-to-noise ratio loss to train the model; the composite loss function can help to retain the spatial details and spectral information of the source images. Moreover, we verify the proposed method by extensive experiments, and the results show that the new method can achieve outstanding performance without relying on the ground truth.","2151-1535","","10.1109/JSTARS.2021.3090958","National Natural Science Foundation of China(grant numbers:61863036,62002313); China Postdoctoral Science Foundation(grant numbers:2020T130564,2019M653507); Key Areas Research Program of Yunnan Province in China(grant numbers:202001BB050076); Key Laboratory in Software Engineering of Yunnan Province in China(grant numbers:2020SE408); Yunnan Provincial Postdoctoral Science Foundation; Yunnan University's Research Innovation Fund for Graduate Students(grant numbers:2020230,2020231); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9461404","Conditional generative adversarial network (cGAN);deep learning (DL);image fusion;loss function;remote sensing image fusion (RSIF)","Image fusion;Spatial resolution;Remote sensing;Feature extraction;Generative adversarial networks;Sensors;Fuses","geophysical image processing;geophysical techniques;image classification;image fusion;learning (artificial intelligence);neural nets;remote sensing","semisupervised remote sensing image fusion;multiscale conditional generative adversarial network;Siamese structure;spatial resolution;spectral resolution;ecological environment investigation;dynamic monitoring;deep learning;RSIF methods;ground truth;semisupervised RSIF method;pseudoSiamese structure;panchromatic images;multispectral images;image details;composite loss function;signal-to-noise ratio loss;multiskip connection","","8","","50","CCBY","21 Jun 2021","","","IEEE","IEEE Journals"
"A Method for Face Fusion Based on Variational Auto-Encoder","X. Li; J. -M. Wen; A. -L. Chen; B. Chen","Department of Information and Software Engineering, University of Electronic Science and Technology of China, ChengDu, China; Department of Information and Software Engineering, University of Electronic Science and Technology of China, ChengDu, China; Department of Information and Software Engineering, University of Electronic Science and Technology of China, ChengDu, China; Department of Information and Software Engineering, University of Electronic Science and Technology of China, ChengDu, China","2018 15th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","3 Feb 2019","2018","","","77","80","Face fusion refers to fuse two different facial images into a new face image that retains the facial features of the original image. Our network model is combine Variational Auto-Encoder(V Ae)and Generative Adversarial Networks(GAN), which achieved the end-to-end fusion task. Not only does it guarantee the quality of the fusion image(the image generated by GAN is sharp and photorealistic), but also doesn't lose specific details of the face(This is guaranteed by VAE). In the end, the experiment achieved a promising result on the CelebA dataset.","2576-8964","978-1-7281-1536-8","10.1109/ICCWAMTIP.2018.8632589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632589","Face fusion;VAE;GAN","Face;Gallium nitride;Image reconstruction;Generative adversarial networks;Task analysis;Generators;Image fusion","face recognition;feature extraction;image fusion","variational auto-encoder;generative adversarial networks;GAN;fusion image;end-to-end fusion task;network model;original image;facial features;face image;face fusion","","1","","16","IEEE","3 Feb 2019","","","IEEE","IEEE Conferences"
"HPGAN: Hyperspectral Pansharpening Using 3-D Generative Adversarial Networks","W. Xie; Y. Cui; Y. Li; J. Lei; Q. Du; J. Li","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Department of Electronic and Computer Engineering, Mississippi State University, Starkville, MS, USA; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2020","2021","59","1","463","477","Hyperspectral (HS) pansharpening, as a special case of the superresolution (SR) problem, is to obtain a high-resolution (HR) image from the fusion of an HR panchromatic (PAN) image and a low-resolution (LR) HS image. Though HS pansharpening based on deep learning has gained rapid development in recent years, it is still a challenging task because of the following requirements: 1) a unique model with the goal of fusing two images with different dimensions should enhance spatial resolution while preserving spectral information; 2) all the parameters should be adaptively trained without manual adjustment; and 3) a model with good generalization should overcome the sensitivity to different sensor data in reasonable computational complexity. To meet such requirements, we propose a unique HS pansharpening framework based on a 3-D generative adversarial network (HPGAN) in this article. The HPGAN induces the 3-D spectral-spatial generator network to reconstruct the HR HS image from the newly constructed 3-D PAN cube and the LR HS image. It searches for an optimal HR HS image by successive adversarial learning to fool the introduced PAN discriminator network. The loss function is specifically designed to comprehensively consider global constraint, spectral constraint, and spatial constraint. Besides, the proposed 3-D training in the high-frequency domain reduces the sensitivity to different sensor data and extends the generalization of HPGAN. Experimental results on data sets captured by different sensors illustrate that the proposed method can successfully enhance spatial resolution and preserve spectral information.","1558-0644","","10.1109/TGRS.2020.2994238","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Young Talent fund of University Association for Science and Technology in Shaanxi of China(grant numbers:20190103); Special Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2019T120878); Higher Education Discipline Innovation Project(grant numbers:B08038); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ153,2016JQ6023,2016JQ6018); General Financial Grant from the China Postdoctoral Science Foundation(grant numbers:2017M620440); Yangtse Rive Scholar Bonus Schemes(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097446","Generative adversarial networks;global constraint;hyperspectral pansharpening;spectral–spatial constraints;3-D high-frequency block","Generators;Generative adversarial networks;Spatial resolution;Gallium nitride;Bayes methods;Data models","computational complexity;deep learning (artificial intelligence);image fusion;image resolution;spectral analysis","spectral constraint;spatial constraint;sensor data;HPGAN;spatial resolution;spectral information;hyperspectral pansharpening;3D generative adversarial network;superresolution problem;high-resolution image;HR panchromatic image;low-resolution HS image;deep learning;unique HS pansharpening framework;LR HS image;optimal HR HS image;3D PAN cube;3D spectral-spatial generator network;3D training;PAN discriminator network;successive adversarial learning;image fusion;computational complexity","","32","","36","IEEE","20 May 2020","","","IEEE","IEEE Journals"
"MAFusion: Multiscale Attention Network for Infrared and Visible Image Fusion","X. Li; H. Chen; Y. Li; Y. Peng","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China","IEEE Transactions on Instrumentation and Measurement","21 Jun 2022","2022","71","","1","16","The infrared and visible image fusion aims to generate one image with rich information by integrating thermal regions from the infrared image and texture details from the visible image, which is beneficial to facilitate the capacity of video surveillance and object detection in complex environments. Although there is great progress in image fusion algorithms, artifacts and inconsistencies are still challenging tasks. To alleviate these problems, a multiscale attention network for infrared and visible image fusion (MAFusion) is proposed. The network consists of an encoder, a fusion strategy, and a decoder. Specifically, the encoder is adopted to extract multiscale features by feeding the source images. An attention-based model is then designed as the fusion strategy to integrate different features in the infrared and visible images. The attention-based model can highlight the thermal targets in the infrared image and maintain details in the visible image, so as to avoid the generation of artifacts. The decoder is based on a multiscale skip connection to incorporate low-level details with high-level semantics at different scales. The vital features of infrared and visible images can be fully preserved by the multiscale skip connection network to restrict the introduction of inconsistencies. Furthermore, we develop a feature-preserving loss function to train the proposed network. Experimental results demonstrate that the proposed network delivers advantages and effectiveness compared with the state-of-the-art fusion methods in qualitative and quantitative assessments. Besides, we apply the fused image generated by MAFusion to crowd counting (CC), which can effectively improve the CC performance in low-illumination conditions.","1557-9662","","10.1109/TIM.2022.3181898","National Science Foundation of China(grant numbers:62172029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9794945","Attention-based model;image fusion;infrared image;multiscale network;skip connection;visible image","Image fusion;Feature extraction;Task analysis;Transforms;Decoding;Semantics;Generative adversarial networks","feature extraction;image fusion;image texture;infrared imaging;object detection;sensor fusion;video surveillance","image fusion algorithms;multiscale attention network;visible image;fusion strategy;source images;attention-based model;infrared image;multiscale skip connection network;fused image","","1","","58","IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"GANMcC: A Generative Adversarial Network With Multiclassification Constraints for Infrared and Visible Image Fusion","J. Ma; H. Zhang; Z. Shao; P. Liang; H. Xu","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Instrumentation and Measurement","25 Dec 2020","2021","70","","1","14","Visible images contain rich texture information, whereas infrared images have significant contrast. It is advantageous to combine these two kinds of information into a single image so that it not only has good contrast but also contains rich texture details. In general, previous fusion methods cannot achieve this goal well, where the fused results are inclined to either a visible or an infrared image. To address this challenge, a new fusion framework called generative adversarial network with multiclassification constraints (GANMcC) is proposed, which transforms image fusion into a multidistribution simultaneous estimation problem to fuse infrared and visible images in a more reasonable way. We adopt a generative adversarial network with multiclassification to estimate the distributions of visible light and infrared domains at the same time, in which the game of multiclassification discrimination will make the fused result to have these two distributions in a more balanced manner, so as to have significant contrast and rich texture details. In addition, we design a specific content loss to constrain the generator, which introduces the idea of main and auxiliary into the extraction of gradient and intensity information, which will enable the generator to extract more sufficient information from source images in a complementary manner. Extensive experiments demonstrate the advantages of our GANMcC over the state-of-the-art methods in terms of both qualitative effect and quantitative metric. Moreover, our method can achieve good fused results even the visible image is overexposed. Our code is publicly available at https://github.com/jiayi-ma/GANMcC.","1557-9662","","10.1109/TIM.2020.3038013","Natural Science Fund of Hubei Province(grant numbers:2019CFA037); National Natural Science Foundation of China(grant numbers:61773295,41890820); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9274337","Deep learning;generative adversarial network (GAN);image fusion;infrared;multiclassification","Image fusion;Generative adversarial networks;Generators;Gallium nitride;Task analysis;Neural networks;Data mining","game theory;image classification;image fusion;image texture;infrared imaging;neural nets;pose estimation;transforms;unsupervised learning","GANMcC;generative adversarial network;multiclassification constraints;visible image fusion;infrared image fusion;visible light distribution;infrared domains;gradient extraction;image fusion transform","","37","","54","IEEE","1 Dec 2020","","","IEEE","IEEE Journals"
"Deep Learning-Based Multi-Focus Image Fusion: A Survey and a Comparative Study","X. Zhang","Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.","IEEE Transactions on Pattern Analysis and Machine Intelligence","4 Aug 2022","2022","44","9","4819","4838","Multi-focus image fusion (MFIF) is an important area in image processing. Since 2017, deep learning has been introduced to the field of MFIF and various methods have been proposed. However, there is a lack of survey papers that discuss deep learning-based MFIF methods in detail. In this study, we fill this gap by giving a detailed survey on deep learning-based MFIF algorithms, including methods, datasets and evaluation metrics. To the best of our knowledge, this is the first survey paper that focuses on deep learning-based approaches in the field of MFIF. Besides, extensive experiments have been conducted to compare the performance of deep learning-based MFIF algorithms with conventional MFIF approaches. By analyzing qualitative and quantitative results, we give some observations on the current status of MFIF and discuss some future prospects of this field.","1939-3539","","10.1109/TPAMI.2021.3078906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428544","Multi-focus image fusion;image fusion;deep learning;image processing","Image fusion;Deep learning;Frequency modulation;Transforms;Generative adversarial networks;Visualization;Task analysis","deep learning (artificial intelligence);image fusion","deep learning-based MFIF;deep learning-based approaches;deep learning-based multifocus image fusion;survey paper;qualitative results;quantitative results;image processing","","19","","131","IEEE","11 May 2021","","","IEEE","IEEE Journals"
"A Comparison between AttnGAN and DF GAN: Text to Image Synthesis","P. Sumi; S. Sindhuja; S. Sureshkumar","Department of Computer Science and Engineering, Bannari Amman Institute of Technology, Sathyamangalam; Department of Computer Science and Engineering, Bannari Amman Institute of Technology, Sathyamangalam; Department of Computer Science and Engineering, Bannari Amman Institute of Technology, Sathyamangalam","2021 3rd International Conference on Signal Processing and Communication (ICPSC)","15 Jun 2021","2021","","","615","619","Nowadays conversion from text to high resolution image is a challenging task due to its wide variety of application area. For text to image conversion almost all systems use Generative Adversarial Networks as the basic part of the system and GAN guarantees semantic consistency between the text input and the generated image output. In this paper we are comparing two algorithms that is used for generating image from text. The first algorithm is the AttnGAN and the second one is the DF-GAN. AttnGAN builds on top of StackGAN by using attention network which allows it to capture word level information along with the broader sentence level information. The second algorithm is the DF-GAN, which uses single generator and discriminator model to synthesize high resolution images and also uses Matching-Aware Gradient Penalty (MA-GP) to get real images with real description. The model contains a Deep text-image Fusion Block (DFBlock) to generate image features from text. Both algorithms work efficiently for image generation from text but DF-GAN generates the perfect output than AttnGAN. The AttnGAN always focus on the textual part to generate output image but DF-GAN also focuses on background of image.","","978-1-6654-2864-4","10.1109/ICSPC51351.2021.9451789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451789","AttnGAN;DF GAN;BLSTM;CNN","Image resolution;Image synthesis;Fuses;Semantics;Signal processing algorithms;Signal processing;Generative adversarial networks","deep learning (artificial intelligence);feature extraction;image fusion;image resolution;text analysis","DF-GAN;AttnGAN;text to image synthesis;image conversion;generative adversarial networks;GAN;text input;image output;sentence level information;Deep text-image Fusion Block;image features;image generation;matching-aware gradient penalty;MA-GP;StackGAN;attention network","","1","","10","IEEE","15 Jun 2021","","","IEEE","IEEE Conferences"
"STDFusionNet: An Infrared and Visible Image Fusion Network Based on Salient Target Detection","J. Ma; L. Tang; M. Xu; H. Zhang; G. Xiao","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; College of Computer and Control Engineering, Minjiang University, Fuzhou, China","IEEE Transactions on Instrumentation and Measurement","6 May 2021","2021","70","","1","13","In this article, we propose an infrared and visible image fusion network based on the salient target detection, termed STDFusionNet, which can preserve the thermal targets in infrared images and the texture structures in visible images. First, a salient target mask is dedicated to annotating regions of the infrared image that humans or machines pay more attention to, so as to provide spatial guidance for the integration of different information. Second, we combine this salient target mask to design a specific loss function to guide the extraction and reconstruction of features. Specifically, the feature extraction network can selectively extract salient target features from infrared images and background texture features from visible images, while the feature reconstruction network can effectively fuse these features and reconstruct the desired results. It is worth noting that the salient target mask is only required in the training phase, which enables the proposed STDFusionNet to be an end-to-end model. In other words, our STDFusionNet can fulfill salient target detection and key information fusion in an implicit manner. Extensive qualitative and quantitative experiments demonstrate the superiority of our fusion algorithm over the current state of the arts, where our algorithm is much faster and the fusion results look like high-quality visible images with clear highlighted infrared targets. Moreover, the experimental results on the public datasets reveal that our algorithm can improve the entropy (EN), mutual information (MI), visual information fidelity (VIF), and spatial frequency (SF) metrics with about 1.25%, 22.65%, 4.3%, and 0.89% gains, respectively. Our code is publicly available at https://github.com/jiayi-ma/STDFusionNet.","1557-9662","","10.1109/TIM.2021.3075747","National Natural Science Foundation of China(grant numbers:61773295,62072223); Key Research and Development Program of Hubei Province(grant numbers:2020BAB113); Natural Science Foundation of Hubei Province(grant numbers:2019CFA037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9416507","Deep learning;image fusion;infrared image;mask;salient target detection","Image fusion;Feature extraction;Object detection;Image reconstruction;Data mining;Transforms;Generative adversarial networks","entropy;feature extraction;image fusion;image texture;infrared imaging;object detection","SF metrics;spatial frequency;VIF;visual information fidelity;MI;mutual information;EN;entropy;end-to-end model;spatial guidance;clear highlighted infrared targets;high quality visible images;key information fusion;feature reconstruction network;background texture features;salient target features;feature extraction network;salient target mask;thermal targets;visible image fusion network;infrared image fusion network;salient target detection;STDFusionNet","","45","","34","IEEE","26 Apr 2021","","","IEEE","IEEE Journals"
"DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis","M. Tao; H. Tang; F. Wu; X. Jing; B. -K. Bao; C. Xu","Nanjing University of Posts and Telecommunications; CVL, ETH Zürich; Nanjing University of Posts and Telecommunications; Wuhan University; Nanjing University of Posts and Telecommunications; NLPR, Institute of Automation, CAS","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","16494","16504","Synthesizing high-quality realistic images from text descriptions is a challenging task. Existing text-to-image Generative Adversarial Networks generally employ a stacked architecture as the backbone yet still remain three flaws. First, the stacked architecture introduces the entanglements between generators of different image scales. Second, existing studies prefer to apply and fix extra networks in adversarial learning for text-image semantic consistency, which limits the supervision capability of these networks. Third, the cross-modal attention-based text-image fusion that widely adopted by previous works is limited on several special image scales because of the computational cost. To these ends, we propose a simpler but more effective Deep Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose: (i) a novel one-stage text-to-image backbone that directly synthesizes high-resolution images without entanglements between different generators, (ii) a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty and One-Way Output, which enhances the text-image semantic consistency without introducing extra networks, (iii) a novel deep text-image fusion block, which deepens the fusion process to make a full fusion between text and visual features. Compared with current state-of-the-art methods, our proposed DF-GAN is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance on widely used datasets. Code is available at https://github.com/tobran/DF-GAN.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01602","Natural Science Foundation of Jiangsu Province(grant numbers:bk20200037,BK20210595); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879122","Vision + language; Image and video synthesis and generation","Visualization;Computer vision;Codes;Semantics;Computer architecture;Generative adversarial networks;Generators","feature extraction;image fusion;image matching;image resolution;learning (artificial intelligence);sensor fusion;text analysis","DF-GAN;text-to-image synthesis;high-quality realistic images;text descriptions;Existing text-to-image Generative Adversarial Networks;stacked architecture;different image scales;extra networks;text-image semantic consistency;cross-modal attention-based text-image fusion;special image scales;simpler but more effective Deep Fusion Generative Adversarial Networks;one-stage text-to-image backbone;high-resolution images;different generators;novel deep text-image fusion block;text-matching images","","5","","60","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Improving Satellite Image Fusion via Generative Adversarial Training","X. Luo; X. Tong; Z. Hu","College of Life Sciences and Oceanography, Shenzhen University, Shenzhen, China; College of Surveying and Geo-informatics, Tongji University, Shanghai, China; MNR Key Laboratory for Geo-Environmental Monitoring of Great Bay Area & Guangdong Key Laboratory of Urban Informatics & Shenzhen Key Laboratory of Spatial Smart Sensing and Services, Shenzhen University, Shenzhen, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2021","2021","59","8","6969","6982","The optical images acquired from satellite platforms are commonly multiresolution images, and converting multiresolution satellite images into full higher-resolution (HR) images has been a critical technique for improving the image quality. In this study, we introduced the generative adversarial network (GAN) and proposed a new fusion GAN (FusGAN) approach for solving the remote sensing image fusion problem. Specifically, we developed a new adversarial training strategy: 1) downscaled multiresolution images are adopted for generative network (G-Net) training, and 2) the discriminative network (D-Net) is used to adversarially train the G-Net by discriminating whether the original multiresolution images have been fused well enough. To further improve the capability of the network, we structured our G-Net with residual dense blocks by combining state-of-the-art residual and dense connection ideas. Our proposed FusGAN approach is evaluated both visually and quantitatively on Sentinel-2 and Landsat Operational Land Imager (OLI) multiresolution images. As demonstrated by the results, the proposed FusGAN approach outperforms the selected benchmark methods and both perfectly preserves spectral information and reconstructs spatial information in image fusion. Considering the common resolution disparities among intra- and intersatellite images, the proposed FusGAN approach can contribute to the quality improvement of satellite images and thus improve remote sensing applications.","1558-0644","","10.1109/TGRS.2020.3025821","National Natural Science Foundation of China(grant numbers:41631178); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9212572","Deep learning;generative adversarial networks (GANs);Landsat 8;remote sensing image fusion;residual dense blocks;Sentinel-2","Image fusion;Satellites;Training;Spatial resolution;Remote sensing","geophysical image processing;image fusion;image resolution;optical images;remote sensing","satellite image fusion;generative adversarial training;optical images;satellite platforms;multiresolution satellite images;higher-resolution images;image quality;generative adversarial network;fusion GAN approach;remote sensing image fusion problem;adversarial training strategy;generative network;G-Net;discriminative network;original multiresolution images;residual dense blocks;FusGAN approach;Landsat Operational Land Imager multiresolution images;common resolution disparities;intersatellite images;quality improvement","","7","","74","IEEE","5 Oct 2020","","","IEEE","IEEE Journals"
"Bidirectional Mapping Generative Adversarial Networks for Brain MR to PET Synthesis","S. Hu; B. Lei; S. Wang; Y. Wang; Z. Feng; Y. Shen","Department of Computer Science, University of Chinese Academy of Sciences, Beijing, China; School of Biomedical Engineering, Shenzhen University, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Automation, Central South University, Changsha, China; College of Intelligent Systems Science and Engineering, Harbin Engineering University, Harbin, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","IEEE Transactions on Medical Imaging","30 Dec 2021","2022","41","1","145","157","Fusing multi-modality medical images, such as magnetic resonance (MR) imaging and positron emission tomography (PET), can provide various anatomical and functional information about the human body. However, PET data is not always available for several reasons, such as high cost, radiation hazard, and other limitations. This paper proposes a 3D end-to-end synthesis network called Bidirectional Mapping Generative Adversarial Networks (BMGAN). Image contexts and latent vectors are effectively used for brain MR-to-PET synthesis. Specifically, a bidirectional mapping mechanism is designed to embed the semantic information of PET images into the high-dimensional latent space. Moreover, the 3D Dense-UNet generator architecture and the hybrid loss functions are further constructed to improve the visual quality of cross-modality synthetic images. The most appealing part is that the proposed method can synthesize perceptually realistic PET images while preserving the diverse brain structures of different subjects. Experimental results demonstrate that the performance of the proposed method outperforms other competitive methods in terms of quantitative measures, qualitative displays, and evaluation metrics for classification.","1558-254X","","10.1109/TMI.2021.3107013","National Natural Science Foundations of China(grant numbers:62172403,61872351); International Science and Technology Cooperation Projects of Guangdong(grant numbers:2019A050510030); Distinguished Young Scholars Fund of Guangdong(grant numbers:2021B1515020019); Excellent Young Scholars of Shenzhen(grant numbers:RCYX20200714114641211); Shenzhen Key Basic Research Project(grant numbers:JCYJ20200109115641762); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521163","Medical image synthesis;generative adversarial network;bidirectional mapping mechanism","Generators;Generative adversarial networks;Three-dimensional displays;Positron emission tomography;Training;Image synthesis;Computed tomography","brain;image fusion;medical image processing;positron emission tomography","perceptually realistic PET images;diverse brain structures;Bidirectional Mapping Generative Adversarial Networks;fusing multimodality medical images;positron emission tomography;functional information;PET data;end-to-end synthesis network;image contexts;brain MR-to-PET synthesis;bidirectional mapping mechanism;high-dimensional latent space;Dense-UNet generator architecture;cross-modality synthetic images","Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Positron-Emission Tomography;Semantics","28","","58","IEEE","24 Aug 2021","","","IEEE","IEEE Journals"
"Structured Coupled Generative Adversarial Networks for Unsupervised Monocular Depth Estimation","M. M. Puscas; D. Xu; A. Pilzer; N. Sebe","Huawei Technologies Ireland, Dublin, Ireland; Department of Engineering Science, University of Oxford, Oxford, UK; DISI, University of Trento, Povo, TN, Italy; DISI, University of Trento, Povo, TN, Italy","2019 International Conference on 3D Vision (3DV)","31 Oct 2019","2019","","","18","26","Inspired by the success of adversarial learning, we propose a new end-to-end unsupervised deep learning framework for monocular depth estimation consisting of two Generative Adversarial Networks (GAN), deeply coupled with a structured Conditional Random Field (CRF) model. The two GANs aim at generating distinct and complementary disparity maps and at improving the generation quality via exploiting the adversarial learning strategy. The deep CRF coupling model is proposed to fuse the generative and discriminative outputs from the dual GAN nets. As such, the model implicitly constructs mutual constraints on the two network branches and between the generator and discriminator. This facilitates the optimization of the whole network for better disparity generation. Extensive experiments on the KITTI, Cityscapes, and Make3D datasets clearly demonstrate the effectiveness of the proposed approach and show superior performance compared to state of the art methods. The code and models are available at https://github.com/mihaipuscas/3dv-coupled-crf-disparity.","2475-7888","978-1-7281-3131-3","10.1109/3DV.2019.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8885951","Unsupervised;Depth Estimation;CRF;GAN;unsupervised monocular depth estimation","Estimation;Generative adversarial networks;Couplings;Generators;Gallium nitride;Task analysis;Testing","image fusion;optimisation;random processes;stereo image processing;unsupervised learning;video signal processing","disparity generation;unsupervised monocular depth estimation;end-to-end unsupervised deep learning framework;disparity maps;complementary disparity maps;generation quality;adversarial learning strategy;deep CRF coupling model;generative outputs;discriminative outputs;dual GAN nets;structured coupled generative adversarial networks;conditional random field model;KITTI;Cityscapes;Make3D datasets;optimization","","8","","33","IEEE","31 Oct 2019","","","IEEE","IEEE Conferences"
"Infrared and Visible Image Fusion via Texture Conditional Generative Adversarial Network","Y. Yang; J. Liu; S. Huang; W. Wan; W. Wen; J. Guan","School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Computer Science and Technology, Tiangong University, Tianjin, China; School of Software and Internet of Things Engineering, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China","IEEE Transactions on Circuits and Systems for Video Technology","3 Dec 2021","2021","31","12","4771","4783","This paper proposes an effective infrared and visible image fusion method based on a texture conditional generative adversarial network (TC-GAN). The constructed TC-GAN generates a combined texture map for capturing gradient changes in image fusion. The generator in the TC-GAN is designed as a codec structure for extracting more details, and a squeeze-and-excitation module is applied to this codec structure to increase the weight of significant texture information in the combined texture map. The generator loss function is designed by combing the gradient loss and adversarial loss to retain the texture information of the source images. The discriminator brings the texture of the generated image closer to the visible image. To obtain significant texture information from the source images, a multiple decision map-based fusion strategy is proposed using a combined texture map and an adaptive guided filter. Extensive experiments on the public TNO and RoadScene datasets demonstrate that the proposed method is superior to other state-of-the-art algorithms in terms of a subjective evaluation and quantitative indicators.","1558-2205","","10.1109/TCSVT.2021.3054584","National Natural Science Foundation of China(grant numbers:62072218,61862030,61662026); Natural Science Foundation of Jiangxi Province(grant numbers:20182BCB22006,20181BAB202010,20192ACB20002,20192ACBL21008); 15th Student Research Project of Jiangxi University of Finance and Economics(grant numbers:20200612113027283); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335976","Infrared and visible image fusion;combined texture map;TC-GAN;multiple decision maps","Image fusion;Generators;Information filters;Generative adversarial networks;Feature extraction;Infrared imaging;Training data","codecs;discriminators;feature extraction;image capture;image fusion;image texture;neural nets","image fusion;codec structure;texture information;generator loss function;adversarial loss;source images;visible image;texture conditional generative adversarial network;TC-GAN;infrared image;squeeze-and-excitation module;decision map-based fusion strategy;adaptive guided filter;public TNO;RoadScene datasets;discriminator","","22","","56","IEEE","26 Jan 2021","","","IEEE","IEEE Journals"
"IR-MSDNet: Infrared and Visible Image Fusion Based On Infrared Features and Multiscale Dense Network","A. Raza; J. Liu; Y. Liu; J. Liu; Z. Li; X. Chen; H. Huo; T. Fang","Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, and Key Laboratory of System Control and Information Processing, Ministry of Education, Shanghai Jiao Tong University, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Apr 2021","2021","14","","3426","3437","Infrared (IR) and visible images are heterogeneous data, and their fusion is one of the important research contents in the remote sensing field. In the last decade, deep networks have been widely used in image fusion due to their ability to preserve high-level semantic information. However, due to the lower resolution of IR images, deep learning-based methods may not be able to retain the salient features of IR images. In this article, a novel IR and visible image fusion based on IR Features & Multiscale Dense Network (IR-MSDNet) is proposed to preserve the content and key target features from both visible and IR images in the fused image. It comprises an encoder, a multiscale decoder, a traditional processing unit, and a fused unit, and can capture incredibly rich background details in visible images and prominent target details in IR features. When the dense and multiscale features are fused, the background details are obtained by utilizing attention strategy, and then combined with complimentary edge features. While IR features are extracted by traditional quadtree decomposition and Bezier interpolation, and further intensified by refinement. Finally, both the decoded multiscale features and IR features are used to reconstruct the final fused image. Experimental evaluation with other state-of-the-art fusion methods validates the superiority of our proposed IR-MSDNet in both subjective and objective evaluation metrics. Additional objective evaluation conducted on the object detection (OD) task further verifies that the proposed IR-MSDNet has greatly enhanced the details in the fused images, which bring the best OD results.","2151-1535","","10.1109/JSTARS.2021.3065121","National Key Research and Development Program of China(grant numbers:2018YFB0505000); National Science and Technology Major Project(grant numbers:21-Y20A06-9001-17/18); National Natural Science Foundation of China(grant numbers:61221003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376604","Feature attention;image fusion;multiscale feature fusion;object detection (OD);remote sensing","Feature extraction;Image fusion;Decoding;Remote sensing;Generative adversarial networks;Transforms;Object detection","feature extraction;image fusion;infrared imaging;learning (artificial intelligence);object detection;quadtrees","Bezier interpolation;quadtree decomposition;multiscale dense network;fusion methods;fused image;multiscale features;complimentary edge features;dense features;IR features;salient features;deep learning-based methods;IR images;visible images;infrared features;visible image fusion;IR-MSDNet","","10","","42","CCBY","11 Mar 2021","","","IEEE","IEEE Journals"
"End to End Infrared and Visible Image Fusion With Texture Details and Contrast Information","J. Ji; Y. Zhang; Z. Lin; Y. Li; C. Wang; Y. Hu; F. Huang; J. Yao","Department of UAV, Army Engineering University, Shijiazhuang, China; Department of UAV, Army Engineering University, Shijiazhuang, China; Department of UAV, Army Engineering University, Shijiazhuang, China; Department of UAV, Army Engineering University, Shijiazhuang, China; Department of UAV, Army Engineering University, Shijiazhuang, China; Department of UAV, Army Engineering University, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang, China; Equipment Simulation Training Center, Army Engineering University, Shijiazhuang, China","IEEE Access","9 Sep 2022","2022","10","","92410","92425","Infrared and visible image fusion combine data information from different sensors to achieve a richer description of the same scene. In order to highlight the salient features of the infrared image and the visible image in the fusion image and obtain a fusion image with good performance, an end-to-end infrared and visible image fusion algorithm is proposed in this paper. The contrast attention module and visible image cascade part are introduced in the generator, so that the fusion image can focus on the detail information in the visible image and the contrast information in the infrared image. And in order to retain more structural contour information in the original image, the contour loss is added to the content loss function. In addition, the contrast and detail information in infrared and visible images are balanced by two discriminators. And a goal-guided reward function is introduced into the discriminator, which further facilitates the generator to produce effective fused images. Finally, extensive fusion experiments on public datasets verify the advantages of the proposed algorithm compared with other classical algorithms, and ablation experiments demonstrate the effectiveness of the improved part of the algorithm.","2169-3536","","10.1109/ACCESS.2022.3202974","National Natural Science Foundation of China(grant numbers:62171467); Natural Science Foundation of Hebei Province(grant numbers:F2021506004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869825","End-to-end infrared and visible image fusion;contrast attention module;contour loss;target-guided reward function","Image fusion;Generative adversarial networks;Deep learning;Generators;Feature extraction;Training data;Data mining","image fusion;image texture;infrared imaging","contrast information;visible image fusion;infrared image;texture details","","","","37","CCBY","29 Aug 2022","","","IEEE","IEEE Journals"
"MGMDcGAN: Medical Image Fusion Using Multi-Generator Multi-Discriminator Conditional Generative Adversarial Network","J. Huang; Z. Le; Y. Ma; F. Fan; H. Zhang; L. Yang","Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China; Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China; Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China; Institute of Aerospace Science and Technology, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; School of Electronic and Information, Zhongyuan University of Technology, Zhengzhou, China","IEEE Access","26 Mar 2020","2020","8","","55145","55157","In this paper, we propose a novel end-to-end model for fusing medical images characterizing structural information, i.e., IS, and images characterizing functional information, i.e., IF, of different resolutions, by using a multi-generator multi-discriminator conditional generative adversarial network (MGMDcGAN). In the first cGAN, the generator aims to generate a real-like fused image based on a specifically designed content loss to fool two discriminators, while the discriminators aim to distinguish the structure differences between the fused image and source images. On this basis, we employ the second cGAN with a mask to enhance the information of dense structure in the final fused image, while preventing the functional information from being weakened. Consequently, the final fused image is forced to concurrently keep the structural information in IS and the functional information in IF. In addition, as a unified method, MGMDcGAN can be applied to different kinds of medical image fusion, i.e., MRI-PET, MRI-SPECT, and CT-SPECT, where MRI and CT are two kinds of IS of high resolution, PET and SPECT are typical kinds of IF of low resolution. Qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our MGMDcGAN over the state-of-the-art.","2169-3536","","10.1109/ACCESS.2020.2982016","National Natural Science Foundation of China(grant numbers:61903279); Science and Technology Innovation Team of Colleges and Universities in Henan Province(grant numbers:18IRTSTHN013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9042283","Medical image fusion;generative adversarial network;different resolutions;end-to-end;unified method","Image fusion;Image resolution;Medical diagnostic imaging;Deep learning;Generative adversarial networks;Magnetic resonance imaging","biomedical MRI;image fusion;image resolution;medical image processing;neural nets;positron emission tomography;single photon emission computed tomography","CT-SPECT;MRI-SPECT;MRI-PET;end-to-end model;medical image fusion;source images;MGMDcGAN;multigenerator multidiscriminator conditional generative adversarial network;functional information;structural information","","23","","48","CCBY","19 Mar 2020","","","IEEE","IEEE Journals"
"GAN-FM: Infrared and Visible Image Fusion Using GAN With Full-Scale Skip Connection and Dual Markovian Discriminators","H. Zhang; J. Yuan; X. Tian; J. Ma","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China","IEEE Transactions on Computational Imaging","27 Oct 2021","2021","7","","1134","1147","A good result of infrared and visible image fusion should not only maintain significant contrast for distinguishing targets from the backgrounds, but also contain rich scene textures to cater for human visual perception. However, previous fusion methods usually do not fully utilize the information, and hence their fused results sacrifice either the salience of thermal targets or the sharpness of textures. To address this challenge, we propose a novel Generative Adversarial Network with Full-scale skip connection and dual Markovian discriminators (GAN-FM) to fully preserve effective information in infrared and visible images. First, a full-scale skip connected generator is designed to extract and fuse deep features of different scales, which can promote the direct transmission of shallow high-contrast features to the deep level, preserving the thermal radiation targets from the semantic level. As a result, the fused image can maintain significant contrast. Second, we propose two Markovian discriminators to establish adversarial games with the generator, so as to estimate probability distributions of infrared and visible modalities at the same time. Unlike conventional global discriminator, the Markovian discriminators try to distinguish each patch of input images, thus the attention of network is restricted to local regions and the fused results are forced to contain more details. In addition, we propose an effective joint gradient loss to ensure the harmonious coexistence of contrast and texture, which prevents the background texture pollution caused by the edge diffusion of the high-contrast target regions. Extensive qualitative and quantitative experiments demonstrate that our GAN-FM has advantages over the state-of-the-art methods in preserving significant contrast and rich textures. Moreover, we apply the fused image generated by our method to object detection and image segmentation, which can effectively improve the performance.","2333-9403","","10.1109/TCI.2021.3119954","National Natural Science Foundation of China(grant numbers:61773295); Key Research and Development Program of Hubei Province(grant numbers:2020BAB113); Natural Science Foundation of Hubei Province(grant numbers:2019CFA037); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573457","Image fusion;full-scale skip connection;Markovian discriminator;infrared;generative adversarial network","Image fusion;Generative adversarial networks;Feature extraction;Infrared imaging;Object detection;Deep learning;Games","feature extraction;image fusion;image segmentation;image texture;infrared imaging;Markov processes;neural nets;object detection;probability;visual perception","probability;object detection;adversarial games;thermal radiation targets;shallow high-contrast features;deep features;full-scale skip connected generator;visible images;generative adversarial network;thermal targets;human visual perception;scene textures;dual Markovian discriminators;visible image fusion;image segmentation;GAN-FM;high-contrast target regions;background texture pollution;visible modalities","","23","","56","IEEE","14 Oct 2021","","","IEEE","IEEE Journals"
"Res2Fusion: Infrared and Visible Image Fusion Based on Dense Res2net and Double Nonlocal Attention Models","Z. Wang; Y. Wu; J. Wang; J. Xu; W. Shao","School of Applied Science, Taiyuan University of Science and Technology, Taiyuan, China; School of Applied Science, Taiyuan University of Science and Technology, Taiyuan, China; School of Applied Science, Taiyuan University of Science and Technology, Taiyuan, China; College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou, China; School of Applied Science, Taiyuan University of Science and Technology, Taiyuan, China","IEEE Transactions on Instrumentation and Measurement","1 Mar 2022","2022","71","","1","12","Infrared and visible image fusion intends to generate a synthetic image with superior scene representation and better visual perception. The existing deep learning-based fusion methods merely make use of the convolution operation to extract features with a local receptive field, without fully considering their multiscale and long-range dependency characteristics, which may fail to preserve some essential global context from source images. To this end, we develop a novel and efficient fusion network based on dense Res2net and double nonlocal attention models, termed Res2Fusion. We introduce Res2net and dense connections into the encoder network with multiple available receptive fields, which is used to extract the multiscale features, and can retain as much meaningful information as possible for fusion tasks. In addition, we develop double nonlocal attention models as a fusion layer to model long-range dependencies on the local features. Specifically, these attention models can refine feature maps obtained by the encoder network to more focus on prominent infrared targets and distinct visible details. Finally, the comprehensive attention maps are used to generate a fused result through the simple decoder network. Extensive experiments demonstrate that the proposed method can simultaneously retain highlighted infrared targets and rich visible details and transcends other state-of-the-art fusion methods in terms of subjective and objective evaluation. The corresponding code is publicly available at https://github.com/Zhishe-Wang/Res2Fusion.","1557-9662","","10.1109/TIM.2021.3139654","Fundamental Research Program of Shanxi Province(grant numbers:201901D111260); Open Foundation of Shanxi Key Laboratory of Signal Capturing and Processing(grant numbers:ISPT2020-4); Taiyuan University of Science and Technology Research Initial Funding(grant numbers:20162004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9670874","Deep learning;image fusion;infrared image;nonlocal attention;visible image","Feature extraction;Convolution;Image fusion;Task analysis;Generative adversarial networks;Image reconstruction;Transforms","computer vision;deep learning (artificial intelligence);feature extraction;image fusion;image representation;object detection","visible image fusion;dense Res2net;double nonlocal attention models;synthetic image;deep learning-based fusion methods;local receptive field;multiscale range dependency characteristics;long-range dependency characteristics;source images;fusion network;dense connections;encoder network;multiple available receptive fields;fusion layer;local features;prominent infrared targets;comprehensive attention maps;Res2Fusion","","6","","43","IEEE","5 Jan 2022","","","IEEE","IEEE Journals"
"Fusion-UDCGAN: Multifocus Image Fusion via a U-Type Densely Connected Generation Adversarial Network","Y. Gao; S. Ma; J. Liu; X. Xiu","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; State Key Laboratory of ASIC and System, School of Microelectronics, Fudan University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China","IEEE Transactions on Instrumentation and Measurement","12 Apr 2022","2022","71","","1","13","Multifocus image fusion has attracted considerable attention because it can overcome the physical limitations of optical imaging equipment and fuse multiple images with different depths of the field into one full-clear image. However, most existing deep learning-based fusion methods concentrate on the segmentation of focus–defocus regions, resulting in the loss of the details near the boundaries. To address the issue, this article proposes a novel generation adversarial network with dense connections (Fusion-UDCGAN) to fuse multifocus images. More specifically, the encoder and the decoder are first composed of dense modules with dense long connections to ensure the generated image’s quality. The content and clarity loss based on the  $L1$  norm and the novel sum-modified-Laplacian (NSML) is further embedded to provide the fused images retaining more texture features. Considering that the previous dataset-making approaches may lose the relation between the overall structure and the information near the boundaries, a new dataset, which is uniformly distributed and can simulate natural focusing boundary conditions, is constructed for model training. Subjective and objective experimental results indicate that the proposed method significantly improves the sharpness, contrast, and detail richness compared to several state-of-the-art methods.","1557-9662","","10.1109/TIM.2022.3159978","Natural Science Foundation of Shanghai(grant numbers:19ZR1420800); State Key Laboratory of ASIC and System(grant numbers:2021KF009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9737010","Dense connections;generation adversarial network (GAN);multifocus image fusion;novel sum-modified-Laplacian (NSML)","Image fusion;Generative adversarial networks;Image segmentation;Generators;Training;Feature extraction;Decoding","deep learning (artificial intelligence);image fusion;image segmentation;image texture","Fusion-UDCGAN;multifocus image fusion;U-type densely connected generation adversarial network;optical imaging equipment;full-clear image;deep learning-based fusion methods;dense modules;dense long connections;focus-defocus region segmentation;novel sum-modified-Laplacian;NSML;texture features","","2","","45","IEEE","16 Mar 2022","","","IEEE","IEEE Journals"
"Preference Learning to Multifocus Image Fusion via Generative Adversarial Network","M. He; S. Yu; R. Nie; C. Wang","School of Information Science and Engineering, Yunnan University, Kunming, China; School of Information Science and Engineering, Yunnan University, Kunming, China; School of Automation, Southeast University, Nanjing, China; School of Information Science and Engineering, Yunnan University, Kunming, China","IEEE Transactions on Cognitive and Developmental Systems","8 Dec 2022","2022","14","4","1604","1614","Multifocus image fusion (MFIF) is to produce an all-in-focus fused image by integrating a pair of multifocus images with the same scene. In this article, an effective focus detection method is proposed for MFIF, by a generative adversarial network (GAN) with preference learning (PL). Benefitting from more obvious focus characteristics from the luminance channel in HSV, we take this luminance as the input of our GAN to carry out the easier focus detection, instead of the grayscale images in existing methods. On the other hand, to train our GAN more effectively, we utilize the  ${\ell }_{{2},{1}}$  norm to construct a focus fidelity loss with structural group sparseness, to regularize the generator loss, pledging a more accurate focus confidence map. More importantly, a novel learning strategy, termed PL, is further developed to enhance model training. Functionally, it assigns a larger learning weight to a sample more difficult to be learned. Extensive experiments demonstrate that our proposed method is superior to other state-of-the-art methods.","2379-8939","","10.1109/TCDS.2021.3126330","National Natural Science Foundation of China(grant numbers:61966037,61463052); China Postdoctoral Science Foundation(grant numbers:2017M621586); Science and Technology Plan of Yunnan Province of China(grant numbers:2014AB016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9608976","Generative adversarial network (GAN);HSV space;multifocus image fusion (MFIF);preference learning (PL);structural loss","Generators;Generative adversarial networks;Task analysis;Learning systems;Gray-scale;Transforms;Feature extraction;Image fusion","image colour analysis;image fusion;learning (artificial intelligence);neural nets","accurate focus confidence map;all-in-focus fused image;effective focus detection method;focus fidelity loss;GAN;generative adversarial network;generator loss;grayscale images;learning weight;luminance channel;MFIF;multifocus image fusion;novel learning strategy;preference learning","","","","50","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"A Deep Learning Framework for Fusion of Sar and Optical Satellite Imagery","N. Gupta; H. S. Srivastava; T. Sivasankar; P. Patel","NIIT University, Neemrana, India; Indian Institute of Remote Sensing, ISRO, Dehradun, India; NIIT University, Neemrana, India; Space Applications Centre, ISRO, Ahmedabad, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","488","491","In remote sensing, image fusion is the process of converting information from various source images to a single image such that the features of the source are preserved and relevant information is being highlighted. Through this research work, we propose an unsupervised deep learning Generative Adversarial Network (GAN) for the fusion process of SAR and optical Images. For SAR image, we chose VV, VH, VV-VH bands and for optical image we did Principal Component Analysis (PCA) on its image bands to extract the top three principal components and compose an image out of it. Images were then converted into HSV space. The GAN is primarily trained to capture the maximum gradient features from both the images and secondarily to capture other noticeable features. Experimental results on both training and test samples indicate that the proposed method is able to preserve gradient features and other details of the images with respect to input images.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9792062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792062","SAR;optical;image fusion;deep learning;GAN","Deep learning;Training;Neural networks;Optical computing;Optical imaging;Generative adversarial networks;Optical sensors","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;principal component analysis;remote sensing;synthetic aperture radar;unsupervised learning","remote sensing;image fusion;GAN;optical Images;SAR image;VV-VH bands;optical image;principal component analysis;image bands;maximum gradient features;optical satellite imagery;unsupervised deep learning generative adversarial network;PCA","","","","18","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Multi-Feature Fusion based Image Steganography using GAN","Z. Wang; Z. Zhang; J. Jiang","School of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, China; School of Computer Science and Technology, Shanghai University of Electric Power, Shanghai, China; School of Software Engineering, Tongji University, Shanghai, China","2021 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)","14 Feb 2022","2021","","","280","281","In order to solve the problem of information loss, some image steganography methods utilize generative adversarial networks (GANs), while the existing methods can not capture both texture information and semantic features. In this paper, a more accurate image steganography method is proposed, where a multi-level feature fusion procedure based on GAN is designed. Firstly, convolution and pooling operations are added to the network for feature extraction. Then, short links are used to fuse multi-level feature information. Finally, the stego image is generated by confrontation learning between discriminator and generator. Experimental results show that the proposed method has higher steganalysis security under the detection of high-dimensional feature steganalysis and neural network steganalysis. Comprehensive experiments show that the performance of the proposed method is better than ASDL-GAN and UT-GAN.","","978-1-6654-2603-9","10.1109/ISSREW53611.2021.00079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9700244","image steganography;generative adversarial network;multi-feature fusion","Training;Steganography;Fuses;Semantics;Neural networks;Generative adversarial networks;Feature extraction","feature extraction;image coding;image fusion;image texture;steganography","multifeature fusion;information loss;image steganography methods;generative adversarial networks;texture information;semantic features;accurate image steganography method;multilevel feature fusion procedure;convolution;pooling operations;feature extraction;short links;multilevel feature information;stego image;generator;higher steganalysis security;high-dimensional feature steganalysis;neural network steganalysis;ASDL-GAN;UT-GAN","","","","10","IEEE","14 Feb 2022","","","IEEE","IEEE Conferences"
"CycleGAN-STF: Spatiotemporal Fusion via CycleGAN-Based Image Generation","J. Chen; L. Wang; R. Feng; P. Liu; W. Han; X. Chen","Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Aerospace Information Research Institute (AIR), Chinese Academy of Sciences (CAS), Beijing, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","23 Jun 2021","2021","59","7","5851","5865","Due to the trade-off of temporal resolution and spatial resolution, spatiotemporal image-fusion uses existing high-spatial-low-temporal (HSLT) and high-temporal-low-spatial (HTLS) images as prior knowledge to reconstruct high-temporal-high-spatial (HTHS) images. However, some existing spatiotemporal image-fusion algorithms ignore the issue that the spatial information of HTLS images is insufficient to support the acquisition of spatial information, which leads to the unsatisfactory accuracy of the fusion result. To introduce more spatial information, the algorithm in this article uses Cycle-generative adversarial networks (GANs) to simulate the change process of two HSLT images at k-1 and k+1, and to generate some simulated images between k-1 and k+1. Then, the generated images are selected under the help of HTLS images, and the selected ones are then enhanced with wavelet transform. Finally, the image with spatial information is introduced into the Flexible Spatiotemporal DAta Fusion (FSDAF) framework to improve the performance of spatiotemporal image-fusion. Extensive experiments on two real data sets demonstrate that our proposed method outperforms current state-of-the-art spatiotemporal image-fusion methods.","1558-0644","","10.1109/TGRS.2020.3023432","National Natural Science Foundation of China(grant numbers:U1711266,41925007,41701429,41371344); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206067","Generative adversarial network (GAN);image-generation;remote sensing;spatiotemporal image-fusion;wavelet transform","Spatiotemporal phenomena;Spatial resolution;Remote sensing;Wavelet transforms;Generative adversarial networks","image enhancement;image fusion;image resolution;neural nets;wavelet transforms","cycle generative adversarial networks;HSLT images;HTLS images;temporal resolution;spatial resolution;spatiotemporal image fusion;high temporal high spatial images;CycleGAN based image generation;flexible spatiotemporal data fusion framework;high temporal low spatial images;image enhancement;wavelet transform","","19","","52","IEEE","25 Sep 2020","","","IEEE","IEEE Journals"
"Development of Multimodal Fusion Technique for Medical Images","B. Singhal; A. Aggarwal","UPES, Dehradun, India; UPES, Dehradun, India","2022 Second International Conference on Advanced Technologies in Intelligent Control, Environment, Computing & Communication Engineering (ICATIECE)","24 Feb 2023","2022","","","1","6","Imaging technology plays a vital role in medical imaging, diagnosis and treatment using different modalities in imaging devices. Unfortunately, a single imaging device cannot create information rich images synchronously at the same time due to the difference between the fundamental imaging principles of these modalities. As a result, medical professionals have to spend a lot of energy and time to analyze the complete medical data information gathered from multiple devices. Hence a multimodal image fusion method is required in order to enhance the medical images which further can help medical professionals provide an accurate diagnosis in a timely manner. For which, we have gone thoroughly through the existing state-of-art techniques related to Medical Image Fusion and designed a multi-modal Image Fusion Technique based on GAN in order to enhance the quality of the medical image for better diagnosis of diseases and perform comparative evaluation and validation of proposed techniques using the standard datasets.","","978-1-6654-9396-3","10.1109/ICATIECE56365.2022.10047033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10047033","Multimodal medical image fusion;GAN;CT;MRI","Visualization;Generative adversarial networks;Generators;Medical diagnostic imaging;Image fusion;Standards;Diseases","","","","","","38","IEEE","24 Feb 2023","","","IEEE","IEEE Conferences"
"Multigrained Attention Network for Infrared and Visible Image Fusion","J. Li; H. Huo; C. Li; R. Wang; C. Sui; Z. Liu","Department of Information Technology and Cyber Security, People’s Public Security University of China, Beijing, China; Department of Information Technology and Cyber Security, People’s Public Security University of China, Beijing, China; Department of Biomedical Engineering, Hefei University of Technology, Hefei, China; Department of Information Technology and Cyber Security, People’s Public Security University of China, Beijing, China; School of Opto-electronic Information Science and Technology, Yantai University, Yantai, China; Graduate School, People’s Public Security University of China, Beijing, China","IEEE Transactions on Instrumentation and Measurement","24 Nov 2020","2021","70","","1","12","Methods based on generative adversarial network (GAN) have been widely used in infrared and visible images fusion. However, these methods cannot perceive the discriminative parts of an image. Therefore, we introduce a multigrained attention module into encoder-decoder network to fuse infrared and visible images (MgAN-Fuse). The infrared and visible images are encoded by two independent encoder networks due to their diverse modalities. Then, the results of the two encoders are concatenated to calculate the fused result by the decoder. To exploit the features of multiscale layers fully and force the model focus on the discriminative regions, we integrate attention modules into multiscale layers of the encoder to obtain multigrained attention maps, and then, the multigrained attention maps are concatenated with the corresponding multiscale features of the decoder network. Thus, the proposed method can preserve the foreground target information of the infrared image and capture the context information of the visible image. Furthermore, we design an additional feature loss in the training process to preserve the important features of the visible image, and a dual adversarial architecture is employed to help the model capture enough infrared intensity information and visible details simultaneously. The ablation studies illustrate the validity of the multigrained attention network and feature loss function. Extensive experiments on two infrared and visible image data sets demonstrate that the proposed MgAN-Fuse has a better performance than state-of-the-art methods.","1557-9662","","10.1109/TIM.2020.3029360","Applications Program of Infrared Sensor in Public Security(grant numbers:501-01-2017-1051); Key Program of Highresolution Earth observation System(grant numbers:GFZX0404130307); National Key Research and Development Program of China(grant numbers:2018YFC0825806); National Natural Science Foundation of China(grant numbers:41901350,61601397); Provincial Natural Science Foundation of Anhui(grant numbers:2008085QF285); Fundamental Research Funds for the Central Universities(grant numbers:JZ2019HGBZ0151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9216075","Feature loss;generative adversarial network (GAN);image fusion;multigrained attention mechanism","Generators;Generative adversarial networks;Image fusion;Gallium nitride;Machine learning;Decoding;Fuses","decoding;feature extraction;image fusion;infrared imaging;neural nets;object detection","MgAN-Fuse;infrared images;independent encoder networks;multiscale layers;attention modules;multigrained attention maps;multiscale features;multigrained attention network;visible image data sets;generative adversarial network;visible images fusion;multigrained attention module;encoder-decoder network;GAN","","10","","46","IEEE","7 Oct 2020","","","IEEE","IEEE Journals"
"Learning Spectral Cues for Multispectral and Panchromatic Image Fusion","Y. Xing; S. Yang; Y. Zhang; Y. Zhang","Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, and the National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Image Processing","8 Nov 2022","2022","31","","6964","6975","Recently, deep learning based multispectral (MS) and panchromatic (PAN) image fusion methods have been proposed, which extracted features automatically and hierarchically by a series of non-linear transformations to model the complicated imaging discrepancy. But they always pay more attention to the extraction and compensation of spatial details and use the mean squared error or mean absolute error as a loss function, regardless of the preservation of spectral information contained in multispectral images. For the sake of the improvements in both spatial and spectral resolution, this paper presents a novel fusion model that takes the spectral preservation into consideration, and learns the spectral cues from the process of generating a spectrally refined multispectral image, which is constrained by a spectral loss between the generated image and the reference image. Then these spectral cues are used to modulate the PAN features to obtain final fusion result. Experimental results on reduced-resolution and full-resolution datasets demonstrate that the proposed method can obtain a better fusion result in terms of visual inspection and evaluation indices when compared with current state-of-the-art methods.","1941-0042","","10.1109/TIP.2022.3215906","National Natural Science Foundation of China (NFSC)(grant numbers:62201467,U19B2037); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110544); Natural Science Basic Research Program of Shaanxi(grant numbers:2022JQ-686); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935814","Image fusion;pansharpening;spectral cues;spectral loss;generative adversarial networks","Feature extraction;Image fusion;Pansharpening;Task analysis;Spatial resolution;Remote sensing;Modulation","feature extraction;geophysical image processing;image fusion;image resolution;learning (artificial intelligence);sensor fusion;wavelet transforms","spectral cues;spectrally refined multispectral image;spectral loss;reference image;PAN features;final fusion result;reduced-resolution;full-resolution datasets;panchromatic image;deep learning;nonlinear transformations;complicated imaging discrepancy;compensation;spatial details;mean squared error;absolute error;spectral information;multispectral images;spatial resolution;spectral resolution;fusion model;spectral preservation","","","","58","IEEE","2 Nov 2022","","","IEEE","IEEE Journals"
"Attention GANs: Unsupervised Deep Feature Learning for Aerial Scene Classification","Y. Yu; X. Li; F. Liu","Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Air Defense and Anti-Missile College, Air Force Engineering University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","27 Dec 2019","2020","58","1","519","531","With the development of deep learning, supervised feature learning methods have achieved prominent performance in the field of aerial scene classification. However, supervised feature learning methods require a large amount of labeled training data. To address this limitation, in this article, a novel unsupervised deep feature learning method, namely, Attention generative adversarial networks (Attention GANs), is proposed for aerial scene classification. First, Attention GANs integrates the attention mechanism into GANs to enhance the representation power of the discriminator. Then, to obtain contextual information, a context-aggregation-based feature fusion architecture is designed in the discriminator. Furthermore, the generator and discriminator losses are improved on basis of the Relativistic GAN. At the same time, a content loss is formed by using the feature representations from the context-aggregation-based feature fusion architecture. In the experiments, our Attention GANs is evaluated via comprehensive experiments with four publicly available remote sensing scene data sets, i.e., the UC-Merced data set with 21 scene classes, the RSSCN7 data set with 7 scene classes, the AID data set with 30 scene classes, and the NWPU-RESISC45 data set with 45 scene classes. Experimental results demonstrate that our Attention GANs can obtain the best performance compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2019.2937830","National Natural Science Foundation of China(grant numbers:71771216,71701209); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8842616","Aerial scene classification;attention mechanism;context aggregation;generative adversarial networks (GANs);unsupervised deep feature learning","Learning systems;Gallium nitride;Feature extraction;Generators;Task analysis;Remote sensing;Generative adversarial networks","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);remote sensing;unsupervised learning","AID data set;RSSCN7 data set;NWPU-RESISC45 data set;UC-Merced data set;attention GAN;scene classes;attention generative adversarial networks;deep feature learning method;learning methods;aerial scene classification;unsupervised deep feature learning;publicly available remote sensing scene data sets;feature representations;relativistic GAN;context-aggregation-based;attention mechanism","","62","","77","IEEE","17 Sep 2019","","","IEEE","IEEE Journals"
"Multimodal MR Image Synthesis Using Gradient Prior and Adversarial Learning","X. Liu; A. Yu; X. Wei; Z. Pan; J. Tang","Hubei Province Key Laboratory of Intelligent Information Processing, Real-Time Industrial System, Wuhan, China; Hubei Province Key Laboratory of Intelligent Information Processing, Real-Time Industrial System, Wuhan, China; Hubei Province Key Laboratory of Intelligent Information Processing, Real-Time Industrial System, Wuhan, China; Information Technology Center, Wenzhou Medical University, Wenzhou, China; College of Computing, Michigan Technological University, Houghton, USA","IEEE Journal of Selected Topics in Signal Processing","24 Sep 2020","2020","14","6","1176","1188","In magnetic resonance imaging (MRI), several images can be obtained using different imaging settings (e.g. T1, T2, DWI, and Flair). These images have similar anatomical structures but are with different contrasts, which provide a wealth of information for diagnosis. However, the images under specific imaging settings may not be available due to the limitation of scanning time or corruption caused by noises. It is attractive to derive missing images with some settings from the available MR images. In this paper, we propose a novel end-to-end multisetting MR image synthesis method. The proposed method is based on generative adversarial networks (GANs) - a deep learning model. In the proposed method, different MR images obtained by different settings are used as the inputs of a GANs and each image is encoded by an encoder. Each encoder includes a refinement structure which is used to extract a multiscale feature map from an input image. The multiscale feature maps from different input images are then fused to generate several desired target images under specific settings. Because the resultant images obtained with GANs have blurred edges, we fuse gradient prior information in the model to protect high frequency information such as important tissue textures of medical images. In the proposed model, the multiscale information is also adopted in the adversarial learning (not just in the generator or discriminator) so that we can produce high quality synthesized images. We evaluated the proposed method on two public datasets: BRATS and ISLES. Experimental results demonstrate that the proposed approach is superior to current state-of-the-art methods.","1941-0484","","10.1109/JSTSP.2020.3013418","National Natural Science Foundation of China(grant numbers:61403287,61472293,61572381); Wenzhou Municipal Science and Technology Bureau(grant numbers:2018ZG016); Natural Science Foundation of Zhejiang Province(grant numbers:LY16F030010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153921","Generative adversarial networks;gradient prior;image synthesis;magnetic resonance imaging","Medical diagnostic imaging;Magnetic resonance imaging;Generative adversarial networks;Image segmentation","biomedical MRI;brain;feature extraction;image fusion;image reconstruction;image segmentation;image texture;learning (artificial intelligence);medical image processing","adversarial learning;magnetic resonance imaging;anatomical structures;missing images;MR image synthesis method;generative adversarial networks;deep learning model;refinement structure;multiscale feature map;gradient prior information;high-frequency information;medical images;multiscale information","","33","","48","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"In2I: Unsupervised Multi-Image-to-Image Translation Using Generative Adversarial Networks","P. Perera; M. Abavisani; V. M. Patel","Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ, USA; Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ, USA; Department of Electrical and Computer Engineering, Rutgers University, Piscataway, NJ, USA","2018 24th International Conference on Pattern Recognition (ICPR)","29 Nov 2018","2018","","","140","146","In unsupervised image-to-image translation, the goal is to learn the mapping between an input image and an output image using a set of unpaired training images. In this paper, we propose an extension of the unsupervised image-to-image translation problem to multiple input setting. Given a set of paired images from multiple modalities, a transformation is learned to translate the input into a specified domain. For this purpose, we introduce a Generative Adversarial Network (GAN) based framework along with a multi-modal generator structure and a new loss term, latent consistency loss. Through various experiments we show that leveraging multiple inputs generally improves the visual quality of the translated images. Moreover, we show that the proposed method outperforms current state-of-the-art unsupervised image-to-image translation methods.","1051-4651","978-1-5386-3788-3","10.1109/ICPR.2018.8545464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8545464","","Generators;Feature extraction;Gallium nitride;Generative adversarial networks;Task analysis;Semantics;Image fusion","image processing;unsupervised learning","unsupervised multiimage-to-image translation;input image;output image;unpaired training images;multiple input setting;paired images;multimodal generator structure;translated images;multiple inputs;state-of-the-art unsupervised image-to-image translation;generative adversarial network based framework;In2I;GAN","","14","","30","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"Transformer based Conditional GAN for Multimodal Image Fusion","J. Zhang; L. Jiao; W. Ma; F. Liu; X. Liu; L. Li; P. Chen; S. Yang","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, School of Artificial Intelligence, Xidian University, Xi'an, Shaanxi Province, China","IEEE Transactions on Multimedia","","2023","PP","99","1","14","Multimodal Image fusion is becoming urgent in multi-sensor information utilization. However, existing end-to-end image fusion frameworks ignore a priori knowledge integration and long-distance dependencies across domains, which brings challenges to the network convergence and global image perception in complex scenes. In this paper, a conditional generative adversarial network with transformer (TCGAN) is proposed for multimodal image fusion. The generator is to generate a fused image with the source images content. The discriminators are adopted to distinguish the differences between the fused image and the source images. Adversarial training makes the final fused image to maintain the structural and textural details in the cross-modal images simultaneously. In particular, a wavelet fusion module makes the inputs contain image content from different domains as much as possible. The extracted convolutional features interact in the multiscale cross-modal transformer fusion module to fully complement the associated information. It makes the generator to focus on both local and global context. TCGAN fully considers the training efficiency of the adversarial process and the integrated retention of redundant information. Various experimental results of TCGAN have highlighted targets, rich details, and fast convergence properties on public datasets.","1941-0077","","10.1109/TMM.2023.3243659","Key Scientific Technological Innovation Research Project by Ministry of Education, the State Key Program and the Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61836009); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103,91838303); National Natural Science Foundation of China(grant numbers:U1701267,62076192,62006177,61902298,61573267,61906150,62276199); Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT15R53); ST Innovation Project from the Chinese Ministry of Education, the Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-06); National Science Basic Research Plan in Shaanxi Province of China(grant numbers:2022JQ-607); China Postdoctoral fund(grant numbers:2022T150506); Scientific Research Project of Education Department In Shaanxi Province of China(grant numbers:20JY023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041783","Multimodal image fusion;generative adversarial network;transformer","Image fusion;Generators;Training;Feature extraction;Transformers;Generative adversarial networks;Thermal sensors","","","","","","","IEEE","9 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Multi-Model Medical Image Segmentation Using Multi-Stage Generative Adversarial Networks","A. Khaled; J. -J. Han; T. A. Ghaleb","School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada","IEEE Access","21 Mar 2022","2022","10","","28590","28599","Image segmentation is a challenging problem in medical applications. Medical imaging has become an integral part of machine learning research, as it enables inspecting interior human body with no surgical intervention. Much research has been conducted to study brain segmentation. However, prior studies usually employ one-stage models to segment brain tissues, which could lead to a significant information loss. In this paper, we propose a multi-stage Generative Adversarial Network ( $GAN$ ) model to resolve existing issues of one-stage models. To do this, we apply a coarse-to-fine method to improve brain segmentation using a multi-stage  $GAN$ . In the first stage, our model generates a coarse outline for both the background and brain tissues. Then, in the second stage, the model generates a refine outline for the white matter ( $WM$ ), gray matter ( $GM$ ), and cerebrospinal fluid ( $CSF$ ). We perform a fusion of the coarse and refine outlines to achieve high results. Despite using very limited data, we obtain an improved Dice Coefficient (DC) accuracy of up to 5% compared to one-stage models. We conclude that our model is more efficient and accurate in practice for brain segmentation of both infants and adults. In addition, we observe that our multi-stage model is 2.69–13.93 minutes faster than prior models. Moreover, our multi-stage model achieves higher performance with only a few-shot learning, in which only limited labeled data is available. Therefore, for medical images, our solution is applicable to a wide range of image segmentation applications for which convolution neural networks and one-stage methods have failed. This helps to advance the process of analyzing brain images, thus providing many advantages to the healthcare system, especially in critical health situations where urgent intervention is needed.","2169-3536","","10.1109/ACCESS.2022.3158342","National Science Foundation of China (NSFC)(grant numbers:61872411,61472150); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732489","Brain segmentation;coarse-to-fine;generative adversarial network;semi-supervised learning;multi-stage method","Brain modeling;Image segmentation;Generative adversarial networks;Generators;Training;Semantics;Convolution","biological tissues;biomedical MRI;brain;convolutional neural nets;health care;image fusion;image segmentation;learning (artificial intelligence);medical image processing","one-stage models;brain tissues;brain segmentation;multistage GAN;gray matter;coarse outlines;refine outlines;brain images;multimodel medical image segmentation;medical imaging;machine learning research;multistage generative adversarial network model;white matter;cerebrospinal fluid;Dice Coefficient;few-shot learning;convolution neural networks;healthcare system;coarse-to-fine method;time 2.69 min to 13.93 min","","3","","48","CCBY","10 Mar 2022","","","IEEE","IEEE Journals"
"Unsupervised Cycle-Consistent Generative Adversarial Networks for Pan Sharpening","H. Zhou; Q. Liu; D. Weng; Y. Wang","State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China; School of Biomedical Engineering, Capital Medical University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, Hangzhou Innovation Institute, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","26 Apr 2022","2022","60","","1","14","Deep learning-based pan sharpening has received significant research interest in recent years. Most of the existing methods fall into the supervised learning framework in which they downsample the multispectral (MS) and panchromatic (PAN) images and regard the original MS images as ground truths to form training samples based on Wald’s protocol. Although impressive performance could be achieved, they have difficulties when generalizing to the original full-scale images due to the scale gap, which makes them lack of practicability. In this article, we propose an unsupervised generative adversarial framework that learns from the full-scale images without the ground truths to alleviate this problem. We first extract the modality-specific features from the PAN and MS images with a two-stream generator, perform fusion in the feature domain, and then reconstruct the pan-sharpened images. Furthermore, we introduce a novel hybrid loss based on the cycle-consistency and adversarial scheme to improve the performance. Comparison experiments with the state-of-the-art methods are conducted on GaoFen-2 (GF-2) and WorldView-3 satellites. Results demonstrate that the proposed method can greatly improve the pan-sharpening performance on the full-scale images, which clearly shows its practical value. Codes are available at https://github.com/zhysora/UCGAN.","1558-0644","","10.1109/TGRS.2022.3166528","NSFC(grant numbers:62176017,41871283,U1804157); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755137","Cycle consistency;generative adversarial network (GAN);image fusion;pan sharpening;unsupervised learning","Feature extraction;Deep learning;Training;Task analysis;Generators;Generative adversarial networks;Spatial resolution","","","","3","","53","IEEE","11 Apr 2022","","","IEEE","IEEE Journals"
"Semantic-Supervised Infrared and Visible Image Fusion Via a Dual-Discriminator Generative Adversarial Network","H. Zhou; W. Wu; Y. Zhang; J. Ma; H. Ling","Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Hubei Key Laboratory of Intelligent Robot, Wuhan Institute of Technology, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; Department of Computer Science, Stony Brook University, Stony Brook, USA","IEEE Transactions on Multimedia","7 Feb 2023","2023","25","","635","648","Image fusion synthesizes a new image from multiple images of the same scene. The synthesized image should be suitable for human visual perception and follow-up high-level image-processing tasks. However, existing methods focus on fusing low-level features, ignoring high-level semantic perception information. We propose a new end-to-end model to obtain a more semantically consistent image in infrared and visible image fusion, termed semantic-supervised dual-discriminator generative adversarial network (SDDGAN). In particular, we design an information quantity discrimination (IQD) block to guide fusion progress. For each source image, the block determines the weight for preserving each semantic object’s feature. By this way, the generator learns to fuse various semantic objects via different weights to preserve their characteristics. Moreover, the dual discriminator is employed to identify the distribution of infrared and visible information in the fused image. Each discriminator acts on a certain modality (infrared/visible) of different semantic objects in the fused image to preserve and enhance their modality features. Thus, our fused image is more informative. Both the thermal radiation in the infrared image and the visible image texture details can be well preserved. Qualitative and quantitative experiments demonstrate the superiority of our SDDGAN over state-of-the-art methods in terms of visual effects, efficiency, and quantitative metrics.","1941-0077","","10.1109/TMM.2021.3129609","National Natural Science Foundation of China(grant numbers:62171327,62171328,62072350,61773295); Hubei Technology Innovation Project(grant numbers:2019AAA045); Key Scientific, and Technological Research Project of Hubei Provincial Education Department(grant numbers:D20201507); Application Basic Technology, and Science Research Foundation; Hubei Nuclear Power Operation Engineering Technology Research Center(grant numbers:B210610); Nuclear Energy Development Project; Artificial Intelligence in Nuclear Reactors; State Administration of Science, Technology, and Industry for National Defence, PRC(grant numbers:ZX200302); Graduate Innovative Fund of Wuhan Institute of Technology(grant numbers:CX2020225); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623476","Image fusion;infrared image;visible image;semantic supervised;dual-discriminator","Image fusion;Semantics;Feature extraction;Generators;Transforms;Generative adversarial networks;Games","","","","6","","55","IEEE","22 Nov 2021","","","IEEE","IEEE Journals"
"MrFDDGAN: Multireceptive Field Feature Transfer and Dual Discriminator-Driven Generative Adversarial Network for Infrared and Color Visible Image Fusion","J. Li; B. Li; Y. Jiang; L. Tian; W. Cai","Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; Shanghai Key Laboratory of Navigation and Location-Based Services, Shanghai Jiao Tong University, Shanghai, China; School of Artificial Intelligence and Computer Science, Jiangnan University, Wuxi, China","IEEE Transactions on Instrumentation and Measurement","13 Feb 2023","2023","72","","1","28","Most of the previous infrared and visible image fusion methods based on deep learning were on the basis of gray-scale images and used the single convolution kernel receptive field to extract deep features, which would inevitably cause information loss in the process of feature transfer. Accordingly, this article proposes a new generative adversarial network fusion architecture based on multireceptive field feature transfer and dual discriminator, which is named MrFDDGAN. It is applied to infrared and color visible image fusion. First, three different receptive fields are used to extract multiscale and multilevel deep features of multimodal images on three channels, so as to obtain significant features of source images from multiview and multiperspective in a more comprehensive way. Second, a feature interaction module is introduced in the encoder to realize the information interaction and information prefusion among the three feature channels. Third, the multilevel features fused in the encoder are cascaded with the deep features in the decoder to enhance feature transfer and feature reuse. Finally, this experiment adopts a dual discriminator adversarial network structure to keep the balance of infrared image intensity and visible image texture preserved by fusion results. Qualitative and quantitative experimental analyses are carried out on two public datasets of gray-scale infrared and visible images, and one dataset of infrared and color visible images. The experimental results prove that the proposed MrFDDGAN algorithm has a better subjective visual effect and objective performance than the existing state-of-the-art fusion methods.","1557-9662","","10.1109/TIM.2023.3241999","National Natural Science Foundation of China(grant numbers:11673009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035924","Dual discriminator generative adversarial network (GAN);feature interaction;feature reuse;infrared and color visible image fusion;MrFDDGAN;multireceptive field feature transfer","Feature extraction;Image fusion;Generative adversarial networks;Kernel;Data mining;Task analysis;Generators","","","","","","49","IEEE","3 Feb 2023","","","IEEE","IEEE Journals"
"FuseGAN: Learning to Fuse Multi-Focus Image via Conditional Generative Adversarial Network","X. Guo; R. Nie; J. Cao; D. Zhou; L. Mei; K. He","School of Information Science and Engineering, Yunnan University, Kunming, China; School of Information Science and Engineering, Yunnan University, Kunming, China; School of Mathematics, Southeast University, Nanjing, China; School of Information Science and Engineering, Yunnan University, Kunming, China; School of Information Science and Engineering, Yunnan University, Kunming, China; School of Information Science and Engineering, Yunnan University, Kunming, China","IEEE Transactions on Multimedia","19 Jul 2019","2019","21","8","1982","1996","We study the problem of multi-focus image fusion, where the key challenge is detecting the focused regions accurately among multiple partially focused source images. Inspired by the conditional generative adversarial network (cGAN) to image-to-image task, we propose a novel FuseGAN to fulfill the images-to-image for multi-focus image fusion. To satisfy the requirement of dual input-to-one output, the encoder of the generator in FuseGAN is designed as a Siamese network. The least square GAN objective is employed to enhance the training stability of FuseGAN, resulting in an accurate confidence map for focus region detection. Also, we exploit the convolutional conditional random fields technique on the confidence map to reach a refined final decision map for better focus region detection. Moreover, due to the lack of a large-scale standard dataset, we synthesize a large enough multi-focus image dataset based on a public natural image dataset PASCAL VOC 2012, where we utilize a normalized disk point spread function to simulate the defocus and separate the background and foreground in the synthesis for each image. We conduct extensive experiments on two public datasets to verify the effectiveness of the proposed method. Results demonstrate that the proposed method presents accurate decision maps for focus regions in multi-focus images, such that the fused images are superior to 11 recent state-of-the-art algorithms, not only in visual perception, but also in quantitative analysis in terms of five metrics.","1941-0077","","10.1109/TMM.2019.2895292","National Natural Science Foundation of China(grant numbers:61463052,61365001); China Postdoctoral Science Foundation(grant numbers:171740); Yunnan University(grant numbers:YDY17111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8625482","Conditional generative adversarial network;convolutional conditional random fields;images-to-image;multi-focus image fusion;synthesize dataset","Task analysis;Image fusion;Generative adversarial networks;Transforms;Generators;Gallium nitride;Training","image fusion;learning (artificial intelligence);object detection","conditional generative adversarial network;image-to-image task;images-to-image;multifocus image fusion;FuseGAN;accurate confidence map;focus region detection;convolutional conditional random fields technique;multifocus image dataset;public natural image dataset PASCAL VOC 2012;focused regions;multiple partially focused source images;decision maps;normalized disk point spread function;quantitative analysis","","91","","56","IEEE","24 Jan 2019","","","IEEE","IEEE Journals"
"DCDR-GAN: A Densely Connected Disentangled Representation Generative Adversarial Network for Infrared and Visible Image Fusion","Y. Gao; S. Ma; J. Liu","School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; School of Mechatronic Engineering and Automation, Shanghai University, Shanghai, China; State Key Laboratory of ASIC and System, School of Microelectronics, Fudan University, Shanghai, China","IEEE Transactions on Circuits and Systems for Video Technology","3 Feb 2023","2023","33","2","549","561","This paper proposes a new infrared and visible image fusion method based on the densely connected disentangled representation generative adversarial network (DCDR-GAN), which strips the content and the modal features of infrared and visible images through disentangled representation (DR) and fuses them separately. To deal with the mutually exclusive features in infrared and visible images, inject the modal features into the reconstruction of content features through adaptive instance normalization (AdaIN), reducing the interference. To reduce feature loss and ensure the expression of all-level features in the fused image, DCDR-GAN designs the densely connected content encoders and fusion decoder and constructs the multi-scale fusion structures between the enc-dec through long connections. Meanwhile, the content and the modal reconstruction losses are proposed to preserve the information of the source images. Finally, through the two-phase trained model, generate the fused image. The subjective and objective evaluation results of the TNO and INO datasets show that the proposed method has better visual effects and higher index values than other state-of-the-art methods.","1558-2205","","10.1109/TCSVT.2022.3206807","National Natural Science Foundation of China(grant numbers:61671285,62204044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893115","Cycle reconstruction;dense connection;disentangled representation;generative adversarial network;infrared;visible image fusion","Image fusion;Generative adversarial networks;Decoding;Feature extraction;Generators;Image reconstruction;Encoding","","","","1","","47","IEEE","15 Sep 2022","","","IEEE","IEEE Journals"
"Coupled GAN With Relativistic Discriminators for Infrared and Visible Images Fusion","Q. Li; L. Lu; Z. Li; W. Wu; Z. Liu; G. Jeon; X. Yang","College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; College of Electronics and Information Engineering, Sichuan University, Chengdu, China; Faculty of Applied Science, University of British Columbia, Kelowna, Canada; Department of Embedded Systems Engineering, Incheon National University, Incheon, South Korea; College of Electronics and Information Engineering, Sichuan University, Chengdu, China","IEEE Sensors Journal","18 Feb 2021","2021","21","6","7458","7467","Infrared and visible images are a pair of multi-source multi-sensors images. However, the infrared images lack structural details and visible images are impressionable to the imaging environment. To fully utilize the meaningful information of the infrared and visible images, a practical fusion method, termed as RCGAN, is proposed in this paper. In RCGAN, we introduce a pioneering use of the coupled generative adversarial network to the field of image fusion. Moreover, the simple yet efficient relativistic discriminator is applied to our network. By doing so, the network converges faster. More importantly, different from the previous works in which the label for generator is either infrared image or visible image, we innovatively put forward a strategy to use a pre-fused image as the label. This is a technical innovation, which makes the process of generating fused images no longer out of thin air, but from “existence” to “excellent.” The extensive experiments demonstrate the proposed RCGAN can produce a faithful fused image, which can efficiently persevere the rich texture from visible images and thermal radiation information from infrared images. Compared with traditional methods, it successfully avoids the complex manual designed fusion rules, and also shows a clear advantages over other deep learning-based fusion methods.","1558-1748","","10.1109/JSEN.2019.2921803","National Science Foundation of P.R. China(grant numbers:61711540303,61701327,61601266); Science Foundation of Sichuan Science and Technology Department(grant numbers:2018GZ0178); China Post-Doctoral Science Foundation Funded Project(grant numbers:2018M640916); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733843","Image fusion;infrared image;visible image;coupled generative adversarial network;relativistic discriminator;deep learning","Generators;Generative adversarial networks;Gallium nitride;Sensors;Image fusion;Transforms;Infrared image sensors","image fusion;infrared imaging;learning (artificial intelligence);sensor fusion","visible image;multisource multisensors images;infrared image;imaging environment;infrared images;visible images;image fusion;pre-fused image;faithful fused image","","33","","32","IEEE","10 Jun 2019","","","IEEE","IEEE Journals"
"A Multi-Model Fusion Framework for NIR-to-RGB Translation","L. Yan; X. Wang; M. Zhao; S. Liu; J. Chen","CIAIC, School of Marine Science and Technology, Northwestern Polytechnical University, China; CIAIC, School of Marine Science and Technology, Northwestern Polytechnical University, China; CIAIC, School of Marine Science and Technology, Northwestern Polytechnical University, China; CIAIC, School of Marine Science and Technology, Northwestern Polytechnical University, China; Blueye Intelligence, Zhenjiang, China","2020 IEEE International Conference on Visual Communications and Image Processing (VCIP)","29 Dec 2020","2020","","","459","462","Near-infrared (NIR) images provide spectral information beyond the visible light spectrum and thus are useful in many applications. However, single-channel NIR images contain less information per pixel than RGB images and lack visibility for human perception. Transforming NIR images to RGB images is necessary for performing further analysis and computer vision tasks. In this work, we propose a novel NIR-to-RGB translation method. It contains two sub-networks and a fusion operator. Specifically, a U-net based neural network is used to learn the texture information while a CycleGAN based neural network is adopted to excavate the color information. Finally, a guided filter based fusion strategy is applied to fuse the outputs of these two neural networks. Experiment results show that our proposed method achieves superior NIR-to-RGB translation performance.","2642-9357","978-1-7281-8068-7","10.1109/VCIP49819.2020.9301787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301787","NIR-to-RGB translation;U-net;CycleGAN;guided filter;image fusion","Image color analysis;Image edge detection;Generative adversarial networks;Gallium nitride;Task analysis;Convolution;Conferences","biomedical optical imaging;computer vision;image colour analysis;image fusion;image texture;neural nets;sensor fusion","novel NIR-to-RGB translation method;fusion operator;U-net based neural network;texture information;color information;guided filter based fusion strategy;NIR-to-RGB translation performance;multimodel fusion framework;spectral information;visible light spectrum;single-channel NIR images;RGB images;lack visibility;computer vision tasks","","6","","14","IEEE","29 Dec 2020","","","IEEE","IEEE Conferences"
"FGF-GAN: A Lightweight Generative Adversarial Network for Pansharpening via Fast Guided Filter","Z. Zhao; J. Zhan; S. Xu; K. Sun; L. Huang; J. Liu; C. Zhang","School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Pansharpening is a widely used image enhancement technique for remote sensing. Its principle is to fuse the input high-resolution single-channel panchromatic (PAN) image and low-resolution multi-spectral image and to obtain a high-resolution multi-spectral (HRMS) image. The existing deep learning pansharpening method has two shortcomings. First, features of two input images need to be concatenated along the channel dimension to reconstruct the HRMS image, which makes the importance of PAN images not prominent, and also leads to high computational cost. Second, the implicit information of features is difficult to extract through the manually designed loss function. To this end, we propose a generative adversarial network via the fast guided filter (FGF) for pansharpening. In generator, traditional channel concatenation is replaced by FGF to better retain the spatial information while reducing the number of parameters. Meanwhile, the fusion objects can be highlighted by the spatial attention module. In addition, the latent information of features can be preserved effectively through adversarial training. Numerous experiments illustrate that our network generates high-quality HRMS images that can surpass existing methods, and with fewer parameters.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428272","National Key Research and Development Program of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428272","Pansharpening;Fast guided filter;Generative adversarial network;Image fusion","Training;Earth;Fuses;Pansharpening;Feature extraction;Information filters;Generative adversarial networks","deep learning (artificial intelligence);geophysical image processing;image enhancement;image fusion;image resolution;remote sensing","lightweight generative adversarial network;fast guided filter;image enhancement technique;remote sensing;input high-resolution single-channel panchromatic image;low-resolution multispectral image;high-resolution multispectral image;channel dimension;PAN images;high computational cost;channel concatenation;adversarial training;high-quality HRMS images;FGF-GAN;deep learning pansharpening method","","3","","20","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"Fusion Detection of Closed Water in Medium-Low Resolution Remote Sensing Imagery","Y. Ning; Y. You; J. Cao; F. Liu; Q. Yan; Y. Zhang","School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4027","4030","Aiming at the closed water detection in remote sensing imagery at medium-low resolution, this paper proposes a novel method for closed water detection based on fusion detection which conducts detection via informative fused images blended by Synthetic Aperture Radar (SAR) and optical images. Firstly, it utilizes SAR and optical image pairs containing the same closed water object to generate aligned image pairs according to latitude and longitude information. Next, generative adversarial network (GAN) is adopted to fuse two categories of images. At last, a target detection network driven by optical image samples is used to detect the closed water on the fused image. The experiment result on Sentinel-1&2 shows that the proposed method can effectively make up for the shortage of SAR image in closed water detection and improve the detection performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553554","Beijing Natural Science Foundation, China(grant numbers:4214058); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553554","Closed water detection;Image fusion;GAN","Image resolution;Object detection;Optical fiber networks;Optical imaging;Generative adversarial networks;Adaptive optics;Radar polarimetry","geophysical image processing;geophysical techniques;image fusion;image resolution;object detection;optical images;radar imaging;remote sensing;synthetic aperture radar","target detection network;optical image samples;fused image;SAR image;closed water detection;detection performance;fusion detection;medium-low resolution remote sensing imagery;informative fused images;optical images;optical image pairs;closed water object;aligned image pairs","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"DB-GAN: A Low Contrast Image Enhancer Based on NIR-RGB Fusion","L. Tang; L. Yan; J. Chen","Centre of Intelligent Acoustics and Immersive Communications, School of Marine Science and Technology, Northwestern Polytechnical University, China; Centre of Intelligent Acoustics and Immersive Communications, School of Marine Science and Technology, Northwestern Polytechnical University, China; Centre of Intelligent Acoustics and Immersive Communications, School of Marine Science and Technology, Northwestern Polytechnical University, China","2022 IEEE 32nd International Workshop on Machine Learning for Signal Processing (MLSP)","17 Nov 2022","2022","","","1","6","RGB images captured under haze or over-/under-exposure conditions frequently have low contrast and lack of detail. Due to the limited information in the original image, the majority of enhancement techniques that rely solely on visible information fail to restore the original image satisfactorily. This emphasizes the need for information beyond the visible spectrum. In this paper, we formulate the low contrast image enhancement problem based on near-infrared (NIR)-RGB fusion. A Dual-Branch Generative Adversarial Network (DB-GAN) is designed based on the specific characteristics of NIR-RGB fusion problem. To be specific, with the guidance of the two discriminators that respectively extract information from RGB and NIR images, a U-net based generator generates informative, high-quality fused images. In addition, we create an NIR-RGB dataset with over 1300 aligned image pairs for training the network. Quantitative and qualitative experimental results show the superior performance of our proposed framework.","2161-0371","978-1-6654-8547-0","10.1109/MLSP55214.2022.9943490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9943490","Image fusion;low contrast enhancement;RGB;near-infrared(NIR);Adversarial Generative Network","Training;Machine learning;Signal processing;Generative adversarial networks;Generators;Image restoration;Data mining","image capture;image colour analysis;image enhancement;image fusion;neural nets","aligned image pairs;DB-GAN;dual-branch generative adversarial network;high-quality fused images;low contrast image enhancement problem;near-infrared-RGB fusion;NIR-RGB dataset;NIR-RGB fusion problem;RGB images capture;U-net based generator;visible information;visible spectrum","","","","28","IEEE","17 Nov 2022","","","IEEE","IEEE Conferences"
"Enhancing Underwater Imagery Using Generative Adversarial Networks","C. Fabbri; M. J. Islam; J. Sattar","Air Force Research Laboratory, Rome, NY; Air Force Research Laboratory, Rome, NY; Air Force Research Laboratory, Rome, NY","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7159","7165","Autonomous underwater vehicles (AUVs) rely on a variety of sensors - acoustic, inertial and visual - for intelligent decision making. Due to its non-intrusive, passive nature and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion affect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face difficult challenges and consequently exhibit poor performance on vision-driven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that effect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460552","","Nonlinear distortion;Gallium nitride;Generators;Image color analysis;Visualization;Sensors","autonomous underwater vehicles;decision making;image colour analysis;image denoising;image fusion;image restoration;neural nets;robot vision","Generative Adversarial Networks;autonomous underwater vehicles;AUVs;intelligent decision making;color distortion;noisy images;distorted images;underwater image restoration;underwater imagery;visual data quality;visual underwater scene quality","","164","","35","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"DCFGAN: Dynamic Convolutional Fusion Generative Adversarial Network for Text-to-Image Synthesis","M. Tao; S. Wu; X. Zhang; C. Wang","School of Automation, Nanjing University of Posts and Telecommunications, NanJing, China; School of Automation, Nanjing University of Posts and Telecommunications, NanJing, China; School of Automation, Nanjing University of Posts and Telecommunications, NanJing, China; School of Automation, Nanjing University of Posts and Telecommunications, NanJing, China","2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)","14 Dec 2020","2020","1","","1250","1254","Text-to-image synthesis is the task of synthesizing realistic and text-matching images according to given text descriptions. Most text-to-image generative networks consist of two modules: a pre-trained text-image encoder and a text-to-image generative adversarial network. In this paper, we propose a stronger text encoder which employs a text Transformer to extract semantically meaningful parts from text descriptions. With the stronger text encoder, the generator can obtain more meaningful text information to synthesize realistic and text-matching images. In addition, we propose a Dynamic Convolutional text-image Fusion Generative Adversarial Network (DCFGAN) which employs the Dynamic Convolutional Fusion Block to fuse text and image features efficiently. The Dynamic Convolutional Fusion block adjusts the parameters in the convolution layer according to different text descriptions to synthesize text-matching images. It improves the efficiency of fusing text features and image features in generator network. We evaluate the proposed DCF-GAN on two benchmark datasets, the CUB and the Oxford-102. The extensive experiments demonstrate that our proposed stronger text encoder and Dynamic Convolutional Fusion Layer can greatly promote the performance of text-to-image synthesis.","","978-1-7281-5224-0","10.1109/ICIBA50161.2020.9277299","Nanjing University of Posts and Telecommunications General School(grant numbers:NY220057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277299","Text-to-Image synthesis;generative adversarial network;cross-modal;image-text matching;deep representation learning","Generators;Convolution;Generative adversarial networks;Computational modeling;Semantics;Feature extraction;Head","convolutional neural nets;feature extraction;image fusion;image matching;text analysis","text features;image features;generator network;stronger text encoder;text-to-image synthesis;text-matching images;given text descriptions;text-to-image generative networks;pre-trained text-image encoder;text-to-image generative adversarial network;text Transformer;meaningful text information;Dynamic Convolutional text-image Fusion Generative Adversarial Network;Dynamic Convolutional Fusion Block;Dynamic Convolutional Fusion block;different text descriptions;dynamic convolutional fusion generative adversarial network;dynamic convolutional fusion layer","","12","","24","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"A Domain-Guided Noise-Optimization-Based Inversion Method for Facial Image Manipulation","N. Yang; Z. Zheng; M. Zhou; X. Guo; L. Qi; T. Wang","Shenyang Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; Shenyang Institute of Automation, University of Chinese Academy of Sciences, Beijing, China; Institute of Systems Engineering and Collaborative Laboratory for Intelligent Science and Systems, Macau University of Science and Technology, Macau, China; Computer and Communication Engineering College, Liaoning Shihua University, Fushun, China; College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao, China; Shenyang Institute of Automation, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","9 Jul 2021","2021","30","","6198","6211","A style-based architecture (StyleGAN2) yields outstanding results in data-driven unconditional generative image modeling. This work proposes a Domain-guided Noise-optimization-based Inversion (DNI) method to perform facial image manipulation. It works based on an inverse code that includes: 1) a novel domain-guided encoder called Image2latent to project the image to StyleGAN2 latent space, which can reconstruct an input image with high-quality and maintain its semantic meaning well; 2) a noise optimization mechanism in which a set of noise vectors are used to capture the high-frequency details such as image edges, further improving image reconstruction quality; and 3) a mask for seamless image fusion and local style migration. We further propose a novel semantic alignment evaluation pipeline. It evaluates the semantic alignment with an inverse code by using different attribute boundaries. Extensive qualitative and quantitative comparisons show that DNI can capture rich semantic information and achieve a satisfactory image reconstruction. It can realize a variety of facial image manipulation tasks and outperform state of the art.","1941-0042","","10.1109/TIP.2021.3089905","National Natural Science Foundation of China(grant numbers:61773367,61903358,61903229,61821005); National Key Research and Development Program of China(grant numbers:2020YFB1313400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462525","Deep learning;generative adversarial networks;domain-guided encoder;noise optimization","Semantics;Optimization;Image reconstruction;Generative adversarial networks;Generators;Aerospace electronics;Training","face recognition;feature extraction;image fusion;image reconstruction;image resolution","Domain-guided Noise-optimization-based Inversion method;style-based architecture yields outstanding results;data-driven unconditional generative image modeling;inverse code;novel domain-guided encoder;Image2latent;StyleGAN2 latent space;input image;noise optimization mechanism;noise vectors;high-frequency details;image edges;image reconstruction quality;seamless image fusion;local style migration;semantic alignment evaluation pipeline;satisfactory image reconstruction;facial image manipulation tasks","","5","","57","IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"Text to Video GANs:TFGAN, IRC-GAN, BoGAN","R. Mehmood; R. Bashir; K. J. Giri","Department of Computer Science, Islamic University of Science &Technology, J&K, India; Department of Computer Science, Islamic University of Science &Technology, J&K, India; Department of Computer Science, Islamic University of Science &Technology, J&K, India","2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)","7 Jun 2022","2022","1","","1234","1239","Generative adversarial networks (GANs) have demonstrated high accuracy on image generation tasks. A large number of studies have applied image generating models to video generation as well. However, because of the complexities of video generation, it's not that trivial to use GANs in the video domain. In the video generation, the resulting content has to be spatially and temporally coherent. Moreover, generating videos from text is even more challenging since besides maintaining the temporal and spatial coherence, semantic consistency also needs to be maintained. In this paper, we have compared three recently proposed text-to-video GAN architectures. Text-Filter Conditioning Generative Adversarial Network (TFGAN) is the first architecture, which employs a superior feature fusion method in which firstly the discriminative convolutional filters are produced from text features and then convolved with image features in the discriminator. The Introspective Recurrent Convolutional GAN (IRC-GAN) is the second architecture, which leverages mutual-information introspection to maintain semantic consistency between the generated videos and the input text. The third model is Bottom-up GAN (BoGAN) which introduces three levels of losses viz, region level loss, frame-level loss, and video level loss.","2575-7288","978-1-6654-0816-5","10.1109/ICACCS54159.2022.9785103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785103","GAN;TFGAN;IRC-GAN;BoGAN;computation power;resolution","Three-dimensional displays;Image synthesis;Semantics;Spatial coherence;Deep architecture;Generative adversarial networks;Information filters","convolution;feature extraction;image classification;image fusion;image texture;text analysis;video signal processing","generated videos;Generative adversarial networks;image generation tasks;Introspective Recurrent Convolutional GAN;IRC-GAN;spatial coherence;temporal coherence;text features;Text-Filter Conditioning Generative Adversarial Network;text-to-video GAN architectures;video domain;Video GANs:TFGAN;video generation;video level loss","","","","19","IEEE","7 Jun 2022","","","IEEE","IEEE Conferences"
"A Residual Dense Generative Adversarial Network For Pansharpening With Geometrical Constraints","A. GASTINEAU; J. -F. AUJOL; Y. BERTHOUMIEU; C. GERMAIN","Bordeaux INP, CNRS, IMB, UMR 5251, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMB, UMR 5251, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMS, UMR 5218, Univ. Bordeaux, Talence, France; Bordeaux INP, CNRS, IMS, UMR 5218, Univ. Bordeaux, Talence, France","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","493","497","The pansharpening problem consists in fusing a high resolution panchromatic image with a low resolution multispectral image in order to obtain a high resolution multispectral image. In this paper, we adapt a Residual Dense architecture for the generator in a Generative Adversarial Network framework. Indeed, this type of architecture avoids the vanishing gradient problem faced when training a network by re-injecting previous information thanks to dense and residual connections. Moreover, an important point for the pansharpening problem is to preserve the geometry of the image. Hence, we propose to add a regularization term in the loss function of the generator: it preserves the geometry of the target image so that a better solution is obtained. In addition, we propose geometrical measures that illustrate the advantages of this new method.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9191230","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191230","Pansharpening;Generative Adversarial Network;residual dense network;regularization;remote sensing","Generators;Gallium nitride;Spatial resolution;Generative adversarial networks;Geometry;Satellites","geometry;hyperspectral imaging;image colour analysis;image fusion;image resolution;neural nets","geometrical measures;Residual Dense Generative Adversarial Network;geometrical constraints;pansharpening problem;low resolution multispectral image;high resolution multispectral image;residual dense architecture;vanishing gradient problem;residual connections;high resolution panchromatic image fusion;regularization term;loss function;target image geometry","","3","","22","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Joint Inpainting of RGB and Depth Images by Generative Adversarial Network with a Late Fusion Approach","R. Fujii; R. Hachiuma; H. Saito",Keio University; Keio University; Keio University,"2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)","9 Jan 2020","2019","","","203","204","Image inpainting aims to restore texture of missing regions in scene from an RGB image. In this paper, we aim to restore not only the texture but also the geometry of the missing regions in scene from a pair of RGB and depth images. Inspired by the recent development of generative adversarial network, we employ an encoder-decoderbased generative adversarial network with the input of RGB and depth image. The experimental results show that our method restores the missing region of both RGB and depth image.","","978-1-7281-4765-9","10.1109/ISMAR-Adjunct.2019.00-46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951904","Artificial-intelligence;Computer-vision;Computer-vision-tasks;Scene-understanding;Computer-graphics;Graphics-systems-and-interfaces;Mixed-/-augmented-reality","Image restoration;Geometry;Generative adversarial networks;Task analysis;Cameras;Three-dimensional displays;Convolution","decoding;image coding;image colour analysis;image fusion;image restoration;image texture","depth image inpainting;RGB image fusion approach;image restoration;image texture;encoder-decoder based generative adversarial network","","1","","9","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"UPHDR-GAN: Generative Adversarial Network for High Dynamic Range Imaging With Unpaired Data","R. Li; C. Wang; J. Wang; G. Liu; H. -Y. Zhang; B. Zeng; S. Liu","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Megvii Technology, Chengdu, China; Tencent AI Laboratory, Shenzhen, China; Department of Cardiology, West China Hospital, Sichuan University, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Circuits and Systems for Video Technology","28 Oct 2022","2022","32","11","7532","7546","The paper proposes a method to effectively fuse multi-exposure inputs and generate high-quality high dynamic range (HDR) images with unpaired datasets. Deep learning-based HDR image generation methods rely heavily on paired datasets. The ground truth images play a leading role in generating reasonable HDR images. Datasets without ground truth are hard to be applied to train deep neural networks. Recently, Generative Adversarial Networks (GAN) have demonstrated their potentials of translating images from source domain  $X$  to target domain  $Y$  in the absence of paired examples. In this paper, we propose a GAN-based network for solving such problems while generating enjoyable HDR results, named UPHDR-GAN. The proposed method relaxes the constraint of the paired dataset and learns the mapping from the LDR domain to the HDR domain. Although the pair data are missing, UPHDR-GAN can properly handle the ghosting artifacts caused by moving objects or misalignments with the help of the modified GAN loss, the improved discriminator network and the useful initialization phase. The proposed method preserves the details of important regions and improves the total image perceptual quality. Qualitative and quantitative comparisons against the representative methods demonstrate the superiority of the proposed UPHDR-GAN.","1558-2205","","10.1109/TCSVT.2022.3190057","National Natural Science Foundation of China (NSFC)(grant numbers:62071097,61872067,62031009,61720106004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9826814","Multi-exposure HDR imaging;generative adversarial network;unpaired data","Training;Generative adversarial networks;Dynamic range;Image reconstruction;Optimization;Fuses;Dynamics","data compression;deep learning (artificial intelligence);image colour analysis;image enhancement;image fusion;image motion analysis;image representation","deep learning-based HDR image generation methods;deep neural networks;discriminator network;GAN-based network;generative adversarial network;generative adversarial networks;ground truth images;high dynamic range imaging;image perceptual quality;LDR domain;multiexposure inputs;paired dataset;unpaired datasets;UPHDR-GAN","","1","","59","IEEE","12 Jul 2022","","","IEEE","IEEE Journals"
"Physics-Based GAN With Iterative Refinement Unit for Hyperspectral and Multispectral Image Fusion","J. Xiao; J. Li; Q. Yuan; M. Jiang; L. Zhang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Jul 2021","2021","14","","6827","6841","Hyperspectral image (HSI) fusion can effectively improve the spatial resolution of HSIs by integrating high-resolution multispectral images (MSIs). Considering the spatial and spectral degradation relationship between a fused image and input images, a physics-based GAN is proposed to fuse HSI and MSI. A physical model estimating degradation of image is introduced in the generator and in the discriminators. For the generator, a set of recursive modules including a physical degradation model and a multiscale residual channel attention fusion module integrate the spectral-spatial difference information between input images and estimated degradation images to restore the details of the fused image. Subsequently, the residual spatial attention fusion module is used to combine the results of all recursions to obtain the final reconstructed result. As for the discriminators, three networks with the final fused image, estimated LR HSI and estimated MSI as inputs share the same architecture. Finally, the loss function that contains adversarial losses and L1 losses of the fused image and estimated degradation images is used to optimize network parameters. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2021.3075727","National Natural Science Foundation of China(grant numbers:62071341,41922008,61971319); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435191","Attention module;GAN;hyperspectral image (HSI);physics model","Degradation;Mathematical model;Generators;Feature extraction;Physics;Training;Generative adversarial networks","geophysical image processing;hyperspectral imaging;image fusion;image restoration;iterative methods;neural nets","spatial degradation relationship;spectral degradation relationship;physics-based GAN;physical degradation model;multiscale residual channel attention fusion module;spectral-spatial difference information;residual spatial attention fusion module;iterative refinement unit;hyperspectral image fusion;high-resolution multispectral imaging;LR HSI estimation;degradation image estimation","","6","","64","CCBY","19 May 2021","","","IEEE","IEEE Journals"
"ResFPA-GAN: Text-to-Image Synthesis with Generative Adversarial Network Based on Residual Block Feature Pyramid Attention","J. Sun; Y. Zhou; B. Zhang","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Communication, University of China, China","2019 IEEE International Conference on Advanced Robotics and its Social Impacts (ARSO)","6 Jan 2020","2019","","","317","322","Text-to-image synthesis based on generative adversarial networks (GAN) is a challenging task. The developed methods have show prominent progress on visual quality of the synthesized images, but it still face challenge in the image synthesis of details. In this paper, we introduce an image synthesis algorithm based on semantic description and propose a residual block feature pyramid attention generative adversarial network, called ResFPA-GAN. This network introduces multiscale feature fusion by embedding feature pyramid structure to achieve the fine-grained image synthesis. The quality of the image synthesis can be improved via the iterative training of GAN, while the reference of attention can enhance the network's learning of the details of image texture. Through extensive experimental comparison on the CUB dataset, our method can achieve significant improvement on the variety and authenticity for the synthesised images.","2162-7576","978-1-7281-3176-4","10.1109/ARSO46408.2019.8948717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948717","","Training;Visualization;Image synthesis;Semantics;Generative adversarial networks;Task analysis;Robots","feature extraction;image fusion;image texture;iterative methods;learning (artificial intelligence);text analysis","multiscale feature fusion;feature pyramid structure;fine-grained image synthesis;image texture;text-to-image synthesis;synthesized images;image synthesis algorithm;residual block feature pyramid attention generative adversarial network;ResFPA-GAN;CUB dataset","","3","","24","IEEE","6 Jan 2020","","","IEEE","IEEE Conferences"
"Identification of Abnormal Conditions for Fused Magnesium Melting Process Based on Deep Learning and Multisource Information Fusion","P. Zhou; B. Gao; S. Wang; T. Chai","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China","IEEE Transactions on Industrial Electronics","6 Dec 2021","2022","69","3","3017","3026","Fused magnesium furnace (FMF) is the vital equipment for magnesia refractory production. The melting process of FMF is subject to melting temperature, magnesite quality and composition, and much more dynamic factors, which are prone to abnormal conditions such as overheating, abnormal exhausting, semi-molten, and affect the stability and safety of the production seriously. Due to the difficulties of effectively monitoring and accurately identifying the abnormal conditions with the existing methods, this article proposes a novel method for identifying the abnormal conditions of FMF based on deep learning and multi-information fusion. First, the Wasserstein distance based generative adversarial networks with gradient penalty is developed to solve the problem of obtaining overheating condition image samples with obvious visual characteristics. Then, the image-based deep convolutional neural network model is established to recognize the abnormal condition images and extract image features whose training set is the combination of the generated images and the original images. Finally, based on multisource information fusion, the traditional process current data are fused with the extracted image features, and a support vector machine model is established to complete the classification and identification of the FMF working conditions. Various experiments on actual industrial data show that the proposed method is effective and advanced.","1557-9948","","10.1109/TIE.2021.3070512","National Natural Science Foundation of China(grant numbers:61890934,61890930,61991400); Liaoning Revitalization Talents Program(grant numbers:XLYC1907132); Research Funds for the Central Universities(grant numbers:N180802003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9398584","Abnormal condition;deep learning;fused magnesium furnace (FMF);generative adversarial networks (GAN);multisource information fusion;working-condition detection","Feature extraction;Generative adversarial networks;Furnaces;Deep learning;Production;Smelting;Image recognition","convolutional neural nets;deep learning (artificial intelligence);feature extraction;furnaces;image classification;image fusion;melting;production engineering computing;refractories;support vector machines","fused magnesium melting process;deep learning;multisource information fusion;fused magnesium furnace;magnesia refractory production;melting temperature;multiinformation fusion;overheating condition image samples;image-based deep convolutional neural network model;abnormal condition images;FMF working conditions;image feature extraction;support vector machine","","4","","25","IEEE","7 Apr 2021","","","IEEE","IEEE Journals"
"Infrared Image Generation By Pix2pix Based on Multi-receptive Field Feature Fusion","Y. Ma; Y. Hua; Z. Zuo","School of Artificial Intelligence and Automation, Huazhong University of Science & Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science & Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science & Technology, Wuhan, China","2021 International Conference on Control, Automation and Information Sciences (ICCAIS)","9 Dec 2021","2021","","","1029","1036","Infrared imaging has the advantages of strong anti-interference capability, long-range imaging, and night imaging, and has important applications in both civilian and military fields. In the development of infrared-related equipment, a large number of infrared images under a variety of conditions are required as verification test data. The field test of infrared images requires huge manpower and material resources, and it is difficult to obtain full-time infrared images. To address the problem of insufficient infrared image samples, the paper introduces generative adversarial networks into the infrared image generation task and investigates the infrared image generation method based on visible images by applying Pix2pix networks to paired visible infrared image datasets. To address the problem of missing detailed information of infrared images generated by the Pix2pix network, the paper proposes a Pix2pix network based on multi-receptive field feature fusion and constructs a multi-receptive field feature extractor based on Unet++ structure; the multi-receptive field feature fusion mechanism of nested pixel level by level is proposed. Experiments show that the Pix2pix network based on multi-receptive field feature fusion achieves finer infrared texture generation.","2475-7896","978-1-6654-4029-5","10.1109/ICCAIS52680.2021.9624500","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9624500","Generative Adversarial Networks;infrared image generation;multi-receptive-field feature fusion","Automation;Image synthesis;Infrared imaging;Feature extraction;Generative adversarial networks;Data mining;Task analysis","feature extraction;image fusion;image sensors;image texture;infrared imaging;object detection","multireceptive field feature fusion;Pix2pix network;infrared imaging;long-range imaging;night imaging;infrared-related equipment;full-time infrared images;insufficient infrared image samples;infrared image generation task;infrared image generation method;visible images;paired visible infrared image datasets","","","","12","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"Generative Adversarial Network for SAR-to-Optical Image Translation with Feature Cross-Fusion Inference","J. Wei; H. Zou; L. Sun; X. Cao; M. Li; S. He; S. Liu","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6025","6028","The translation of synthetic aperture radar (SAR) to optical images provides a new solution for the interpretation of SAR images. Most of the existing translation networks are based on generative adversarial networks and use 9-residual blocks or U-Net structures in the feature inference phase. Both structures cause a large amount of information lost during the conversion of SAR image features to optical features, making the outline of the translated image blurred or semantic information lost. Aiming at this problem, this paper proposes a cross-fusion inference network structure, which preserves both high-resolution features and low-resolution features in the whole process of feature inference. Our proposed method broadens the network horizontally while deepening it vertically and improving the image translation performance. The experiments conducted on the public dataset sen1-2 show that the proposed method is superior to other networks.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884166","National Natural Science Foundation of China(grant numbers:62071474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884166","SAR-to-optical image translation;Generative adversarial network (GAN);Cross-fusion inference structure","Optical losses;Laser radar;Semantics;Optical fiber networks;Optical imaging;Generative adversarial networks;Radar polarimetry","feature extraction;image fusion;image resolution;image restoration;inference mechanisms;neural nets;optical images;radar imaging;radar resolution;synthetic aperture radar","generative adversarial network;SAR-to-optical image translation;synthetic aperture radar;optical images;U-Net structures;SAR image features;cross-fusion inference network structure;high-resolution features;low-resolution features;translated image blurred;semantic information lost","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Fusion of Brain PET and MRI Images Using Tissue-Aware Conditional Generative Adversarial Network With Joint Loss","J. Kang; W. Lu; W. Zhang","School of Electronics Engineering, Jiangsu Ocean University, Lianyungang, China; Department of Nuclear Medicine, The First People’s Hospital of Lianyungang, Lianyungang, China; School of Computer Engineering, Jiangsu Ocean University, Lianyungang, China","IEEE Access","13 Jan 2020","2020","8","","6368","6378","Positron emission tomography (PET) has rich pseudo color information that reflects the functional characteristics of tissue, but lacks structural information and its spatial resolution is low. Magnetic resonance imaging (MRI) has high spatial resolution as well as strong structural information of soft tissue, but lacks color information that shows the functional characteristics of tissue. For the purpose of integrating the color information of PET with the anatomical structures of MRI to help doctors diagnose diseases better, a method for fusing brain PET and MRI images using tissue-aware conditional generative adversarial network (TA-cGAN) is proposed. Specifically, the process of fusing brain PET and MRI images is treated as an adversarial machine between retaining the color information of PET and preserving the anatomical information of MRI. More specifically, the fusion of PET and MRI images can be regarded as a min-max optimization problem with respect to the generator and the discriminator, where the generator attempts to minimize the objective function via generating a fused image mainly contains the color information of PET, whereas the discriminator tries to maximize the objective function through urging the fused image to include more structural information of MRI. Both the generator and the discriminator in TA-cGAN are conditioned on the tissue label map generated from MRI image, and are trained alternatively with joint loss. Extensive experiments demonstrate that the proposed method enhances the anatomical details of the fused image while effectively preserving the color information from the PET. In addition, compared with other state-of-the-art methods, the proposed method achieves better fusion effects both in subjectively visual perception and in objectively quantitative assessment.","2169-3536","","10.1109/ACCESS.2019.2963741","National Natural Science Foundation of China(grant numbers:61601194); Natural Science Foundation of the Jiangsu Higher Education Institution of China(grant numbers:17KJB520003); Lianyungang Science and Technology Bureau(grant numbers:SH1508); Huaihai Institute of Technology(grant numbers:Z2015009); Jiangsu University of Science and Technology(grant numbers:HZ20190005); Lianyungang 521 Project Fund(grant numbers:LYG52105-2018033); Lianyungang Haiyan Plan Fund(grant numbers:2018-QD-011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8949485","Positron emission tomography;magnetic resonance imaging;image fusion;generative adversarial network;loss function","Magnetic resonance imaging;Generative adversarial networks;Positron emission tomography;Image color analysis;Generators;Image fusion;Gallium nitride","biological tissues;biomedical MRI;brain;diseases;image classification;image colour analysis;image fusion;image reconstruction;image representation;image segmentation;learning (artificial intelligence);medical image processing;minimax techniques;positron emission tomography","anatomical information;soft tissue;strong structural information;magnetic resonance imaging;rich pseudocolor information;tissue-aware conditional generative adversarial network;brain PET;MRI image;fused image;objective function;color information","","8","","39","CCBY","3 Jan 2020","","","IEEE","IEEE Journals"
"Deep-Learning-Based Spatio-Temporal-Spectral Integrated Fusion of Heterogeneous Remote Sensing Images","M. Jiang; H. Shen; J. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences and the Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","21 Jul 2022","2022","60","","1","15","It is a challenging task to integrate the spatial, temporal, and spectral information of multisource remote sensing images, especially in the case of heterogeneous images. To this end, for the first time, this article proposes a heterogeneous integrated framework based on a novel deep residual cycle generative adversarial network (GAN). The proposed network consists of a forward fusion part and a backward degeneration feedback part. The forward part generates the desired fusion result from the various observations; the backward degeneration feedback part considers the imaging degradation process and regenerates the observations inversely from the fusion result. The heterogeneous integrated fusion framework supported by the proposed network can simultaneously merge the complementary spatial, temporal, and spectral information of multisource heterogeneous observations to achieve heterogeneous spatiospectral fusion, spatiotemporal fusion, and heterogeneous spatiotemporal–spectral fusion. Furthermore, the proposed heterogeneous integrated fusion framework can be leveraged to relieve the two bottlenecks of land-cover change and thick cloud cover. Thus, the inapparent and unobserved variation trends of surface features, which are caused by the low-resolution imaging and cloud contamination, can be detected and reconstructed well. Images from many different remote sensing satellites, i.e., Moderate Resolution Imaging Spectroradiometer (MODIS), Landsat 8, Sentinel-1, and Sentinel-2, were utilized in the experiments conducted in this study, and both the qualitative and quantitative evaluations confirmed the effectiveness of the proposed image fusion method.","1558-0644","","10.1109/TGRS.2022.3188998","National Natural Science Foundation of China(grant numbers:42130108,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829539","Deep residual cycle generative adversarial network (GAN);heterogeneous integrated framework;land-cover change;thick cloud cover","Generators;Remote sensing;Spatial resolution;Generative adversarial networks;Feature extraction;Image fusion;Optical sensors","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;sensor fusion;spatiotemporal phenomena","imaging degradation process;heterogeneous integrated fusion framework;complementary spatial information;temporal, information;spectral information;multisource heterogeneous observations;heterogeneous spatiospectral fusion;spatiotemporal fusion;heterogeneous spatiotemporal-spectral fusion;low-resolution imaging;cloud contamination;different remote sensing satellites;Moderate Resolution Imaging Spectroradiometer;image fusion method;spatio-temporal-spectral integrated fusion;heterogeneous remote sensing images;multisource remote sensing images;heterogeneous images;heterogeneous integrated framework;deep residual cycle generative adversarial network;forward fusion part;backward degeneration feedback part;forward part;desired fusion result","","","","54","IEEE","14 Jul 2022","","","IEEE","IEEE Journals"
"Infrared and Visible Image Fusion Based on Blur Suppression Generative Adversarial Network","S. Yi; X. Liu; L. Li; X. Cheng; C. Wang","College of Mechanical and Electrical Engineering, Chengdu University of Technology, Chengdu, China; College of Mechanical and Electrical Engineering, Chengdu University of Technology, Chengdu, China; College of Mechanical and Electrical Engineering, Chengdu University of Technology, Chengdu, China; College of Mechanical and Electrical Engineering, Chengdu University of Technology, Chengdu, China; College of Mechanical and Electrical Engineering, Chengdu University of Technology, Chengdu, China","Chinese Journal of Electronics","7 Feb 2023","2023","32","1","177","188","The key to multi-sensor image fusion is the fusion of infrared and visible images. Fusion of infrared and visible images with generative adversarial network (GAN) has great advantages in automatic feature extraction and subjective vision improvement. Due to different principle between infrared and visible imaging, the blur phenomenon of edge and texture is caused in the fusion result of GAN. For this purpose, this paper conducts a novel generative adversarial network with blur suppression. Specifically, the generator uses the residual-in-resid-ual dense block with switchable normalization layer as the elemental network block to retain the infrared intensity and the fused image textural details and avoid fusion artifacts. Furthermore, we design an anti-blur loss function based on Weber local descriptor. Finally, numerous experiments are performed qualitatively and quantitatively on public datasets. Results justify that the proposed method can be used to produce a fusion image with sharp edge and clear texture.","2075-5597","","10.23919/cje.2021.00.084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038792","Image fusion;Blur suppression generative adversarial network;Weber local descriptor (WLD);Infrared image;Visible image","Training;Image edge detection;Imaging;Switches;Generative adversarial networks;Feature extraction;Generators","","","","","","47","","7 Feb 2023","","","CIE","CIE Journals"
"Multi-Scale Weighted Fusion Attentive Generative Adversarial Network for Single Image De-Raining","X. Bi; J. Xing","College of Information Engineering, Minzu University of China, Beijing, China; College of Information and Communication Engineering, Harbin Engineering University, Harbin, China","IEEE Access","22 Apr 2020","2020","8","","69838","69848","With the rapid development of outdoor vision system, removing rain streaks from a single image has attracted considerable attention as rain streaks can affect the quality of the image taken in rainy days, and interrupt the key information, which will greatly reduce the use value of the image, thus affecting the performance of traffic, safety monitoring and other facilities. Although the deep learning methods have achieved satisfying performance in single image de-raining, there are still two problems: First, the rain streaks contained in one dataset we can use are limited, and in the case of real rainy days, the rain streak density is diverse, it is impossible to accurately classify them. Therefore, the existing rain removal models cannot remove rain streaks properly for images with different rain streak density which attend to over or under rain removal. Secondly, the results of single image after rain removal model often appear the phenomenon of variegated spots, image contrast saturation change and even unsmooth rain streak after rain removal. We use a three-way multi-scale weighted fusion module to enhance the feature extraction, and then generate an attention map through the improved spatial attentive module to accurately locate the location of the rain streaks. After the combination of the two, we will obtain the foreground information of the rain streaks. Through the characteristic of mutual game in the training mechanism of GAN, we can enhance the rain streak location recognition and effectively remove the rain at the same time. Through the training mechanism of the GAN network game, we can enhance the rain line location recognition and effectively remove the rain at the same time. Experiments show that our network achieves superior performance, it has high generalization for different rain streak density, and ensures that the contrast and saturation of the image are not changed.","2169-3536","","10.1109/ACCESS.2020.2983436","Research on many-objective optimization design method for ship hydrodynamic performance with green and safety(grant numbers:#51779050); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9047869","Improved spatial attentive mechanism;single image rain removal;condition generative adversarial networks;multi-scale weighted fusion","Rain;Generative adversarial networks;Training;Deep learning;Task analysis;Feature extraction;Gallium nitride","computer vision;feature extraction;image fusion;learning (artificial intelligence);neural nets;object detection;sensor fusion","GAN network game;deep learning methods;outdoor vision system;rain line location recognition;rain streak location recognition;unsmooth rain streak;rain removal model;rain streak density;existing rain removal models;rain streaks removal;single image deraining;multiscale weighted fusion attentive generative adversarial network","","3","","30","CCBY","26 Mar 2020","","","IEEE","IEEE Journals"
"Identity-Unrelated Information Decoupling Model for Vehicle Re-Identification","Z. Lu; R. Lin; X. Lou; L. Zheng; H. Hu","School of Electronics and Information Technology, Sun Yat-Sen University, Guangzhou, China; School of Electronics and Information Technology, Sun Yat-Sen University, Guangzhou, China; School of Electronics and Information Technology, Sun Yat-Sen University, Guangzhou, China; School of Electronics and Information Technology, Sun Yat-Sen University, Guangzhou, China; School of Electronics and Information Technology, Sun Yat-Sen University, Guangzhou, China","IEEE Transactions on Intelligent Transportation Systems","12 Oct 2022","2022","23","10","19001","19015","As an indispensable part of intelligent transportation system (ITS), vehicle re-identification (Re-ID) aims to retrieve all target vehicle images captured from non-overlapping cameras. However, this task remains very challenging due to the variation of camera perspective and the similar appearance among the vehicles with the same type and color, i.e., large intra-class variances and small inter-class variances. Previous methods have made a great progress on vehicle Re-ID by leveraging local details and aligning local features, this issue is still far from being solved. In this work, we propose to decouple identity-unrelated information from vehicle representation, tackling the problems of camera perspective variation and vehicle appearance similarity. The keypoint of this method is to learn a distinguishable feature embedding that is independent of identity-unrelated information. Specially, the novel Identity-Unrelated Information Decoupling (IUID) paradigm is designed to learn invariant features of the vehicle with the same ID in different scenes. In our approach, identity-unrelated information can be divided into two kinds of information, i.e., camera perspective information and background information. For the former, through a feature-level camera generative adversarial module, we can decouple camera perspective information from the feature embedding after extracting invariant features across different cameras perspective. For the latter, we propose a vehicle-mask transformer to enhance the attention of the model on local details while reducing the impact of background information. Extensive experiments on two public datasets demonstrate the superiority of IUID over the current state-of-the-arts methods.","1558-0016","","10.1109/TITS.2022.3157463","National Natural Science Foundation of China(grant numbers:62076262,61673402,61273270,60802069); National Key Research and Development Program of China(grant numbers:2018YFB1601101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9736328","Vehicle re-identification;transformers;generative adversarial networks;information decoupling","Cameras;Feature extraction;Transformers;Task analysis;Generative adversarial networks;Image color analysis;Data mining","feature extraction;image fusion;image recognition;intelligent transportation systems;neural nets;road vehicles","background information;camera perspective information;camera perspective variation;feature-level camera generative adversarial module;identity-unrelated information decoupling paradigm;intelligent transportation system;invariant feature extraction;local details;Re-ID;target vehicle images;vehicle appearance similarity;vehicle re-identification;vehicle representation;vehicle-mask transformer","","2","","65","IEEE","16 Mar 2022","","","IEEE","IEEE Journals"
"Comparative Analysis of AttnGAN, DF-GAN and SSA-GAN","R. Mehmood; R. Bashir; K. J. Giri","Department of Computer Science, Islamic University of Science & Technology, J&K, India; Department of Computer Science, Islamic University of Science & Technology, J&K, India; Department of Computer Science, Islamic University of Science & Technology, J&K, India","2021 3rd International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)","9 Mar 2022","2021","","","370","375","In computer vision, generating visuals based on the associated text is important, demanding, and interesting task. GANs (Generative Adversarial Networks) are the most powerful generative models for computer vision and natural language processing. GANs ensure that the synthesized image matches the input text semantically. The purpose of this paper is to compare three algorithms for producing images from text. These include Attentional Generative Adversarial Networks (AttnGAN), Deep-Fusion Generative Adversarial Networks (DF-GAN) and Semantic-Spatial Aware Generative Adversarial Networks (SSA-GAN). AttnGAN extends StackGAN++ by including attention in a multi-stage refining pipeline. DF-GAN, and SSA-GAN on the other hand are based on one stage architecture. DF-GAN incorporates Deep Fusion Block (DFBlock) that effectively fuses textual and visual information whereas SSA-GAN uses novel Semantic-Spatial Aware Convolutional Network (SSACN) block for efficient and deep text and image features fusion.","","978-1-6654-3811-7","10.1109/ICAC3N53548.2021.9725424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9725424","GAN;AttnGAN;DF-GAN;SSA-GAN;mode collapse;diversity;generative model","Measurement;Visualization;Computer vision;Computational modeling;Refining;Pipelines;Computer architecture","computer vision;convolution;feature extraction;image classification;image fusion;image matching;information retrieval;natural language processing;natural languages;object detection;text analysis","AttnGAN;DF-GAN;SSA-GAN;computer vision;associated text;GANs;powerful generative models;Attentional Generative Adversarial Networks;Deep-Fusion Generative Adversarial Networks;novel Semantic-Spatial Aware Convolutional Network block;efficient text;deep text","","","","26","IEEE","9 Mar 2022","","","IEEE","IEEE Conferences"
"IQMA Network: Image Quality Multi-scale Assessment Network","H. Guo; Y. Bin; Y. Hou; Q. Zhang; H. Luo","Meituan, Beijing, China; Meituan, Beijing, China; Meituan, Beijing, China; Meituan, Beijing, China; Meituan, Beijing, China","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","1 Sep 2021","2021","","","443","452","Image Quality Assessment (IQA), which aims to provide computational models for automatically predicting perceptual image quality, is an important computer vision task with many applications. In recent years, a variety of IQA methods have been proposed based on different metric de-signs, which measure the quality of images affected by various types of distortion. However, with the rapid development of Generative Adversarial Networks (GAN), a new challenge has been brought to the IQA community. Especially, the GAN-based Image Reconstruction (IR) methods overfit the traditional PSNR-based IQA methods by generating images with sharper edges and texture-like noises, leading the outputs to be similar to the reference image in appearance but with loss of details. In this paper, we propose a bilateral-branch multi-scale image quality estimation network, named IQMA network. The two branches are designed with Feature Pyramid Network (FPN)-like architecture, extracting multi-scale features for patches of the reference image and corresponding patches of the distorted image separately. Then features of the same scale from both branches are sent into several scale-specific feature fusion modules. Each module performs feature fusion and a novelly designed pooling operation for corresponding features. Then several score regression modules are used to learn a quality score for each scale. Finally, image scores for different scales are fused as the quality score of the image. IQMA network has achieved 1st place on the NTIRE 21 IQA public leaderboard and 2nd place on the NTIRE 21 IQA private leaderboard, and consistently outperforms existing state-of-the-art (SOTA) methods on LIVE and TID2013.","2160-7516","978-1-6654-4899-4","10.1109/CVPRW53098.2021.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9523177","","Image quality;Computer vision;Image edge detection;Computer architecture;Predictive models;Feature extraction;Generative adversarial networks","computer vision;feature extraction;image enhancement;image fusion;image reconstruction;learning (artificial intelligence);regression analysis","Image Quality multiscale Assessment Network;computational models;perceptual image quality;computer vision task;Generative Adversarial Networks;IQA community;GAN-based Image Reconstruction methods;sharper edges;texture-like noises;reference image;bilateral-branch multiscale image quality estimation network;Feature Pyramid Network-like architecture;multiscale features;distorted image;scale-specific feature fusion modules;quality score;image scores;NTIRE 21 IQA public leaderboard;NTIRE 21 IQA private leaderboard;IQMA network;PSNR-based IQA methods;FPN-like architecture;state-of-the-art methods;SOTA methods;score regression modules","","11","","31","IEEE","1 Sep 2021","","","IEEE","IEEE Conferences"
"Deep Fusion Local-Content and Global-Semantic for Image Inpainting","B. Jiang; W. Huang; Y. Huang; C. Yang; F. Xu","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","IEEE Access","4 Sep 2020","2020","8","","156828","156838","The upsampling layers are adopted in almost all the existing encoder-decoder based generative adversarial networks (GANs), which have shown promising results in the image inpainting field. However, existing upsampling layers (e.g. deconvolution and bilinear interpolation) suffer from two limitations: (1) they obtain few semantic information from the global structure. (2) upsampling layer could hardly capture the local content details. To eliminate the above issues, we propose a deep Fusion local-content and global-semantic (DFLG) model that is both effective and general. The DFLG model mainly consists of four components: the Local Content-Response (LCR) module, the pixel-shuffle operator, the Global Semantic-Aware (GSA) module and the reassembly module. Firstly, the LCR module divides the channel into several groups, then utilizes the squeeze-and-excitation mechanism within each group to capture the correlation between channels. Secondly, the pixel shuffle operator reshapes depth on the channel space into width and height on the spatial space, which transforms the correlation within groups on the channel space into correlation within patches on the spatial space. Next, the GSA module employs a patch-based spatial attention mechanism to calculate the correlation between different patches. Finally, the reassembly module refines the feature map. Furthermore, we propose a novel loss function called Attention Loss (ATLoss), which guides the network to concentrate on regions with obvious artifacts. The experiments on CelebA-HQ, Places2, and Paris StreetView datasets demonstrate the effectiveness of our proposed methods in image inpainting tasks and the capability of obtaining images with higher quality.","2169-3536","","10.1109/ACCESS.2020.3019826","National Natural Science Foundation of China(grant numbers:61702176); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9180279","Image inpainting;local content-response;global semantic-aware;attention loss","Correlation;Kernel;Semantics;Deconvolution;Adaptation models;Generative adversarial networks;Transforms","codecs;decoding;image coding;image fusion;image reconstruction;image resolution;image restoration;image sampling;learning (artificial intelligence);neural nets","Paris StreetView dataset;Places2;CelebA-HQ;ATLoss;attention loss;LCR module;GAN;pixel shuffle operator;encoder-decoder based generative adversarial networks;image inpainting tasks;patch-based spatial attention mechanism;GSA module;spatial space;channel space;reassembly module;Global Semantic-Aware module;Content-Response module;DFLG model;global-semantic model;deep Fusion local-content;global structure;semantic information;image inpainting field","","2","","40","CCBYNCND","31 Aug 2020","","","IEEE","IEEE Journals"
"Deep Coupled GAN-Based Score-Level Fusion for Multi-Finger Contact to Contactless Fingerprint Matching","M. M. Hasan; N. Nasrabadi; J. Dawson","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA","2022 International Conference of the Biometrics Special Interest Group (BIOSIG)","27 Sep 2022","2022","","","1","7","Interoperability between contact to contactless images in fingerprint matching is a key factor in the success of contactless fingerprinting devices, which have recently witnessed an increasing demand for biometric authentication. However, due to the presence of perspective distortion and the absence of elastic deformation in contactless fingerphotos, direct matching between contactless fingerprint probe images and legacy contact-based gallery images produces a low accuracy. In this paper, to improve interoperability, we propose a coupled deep learning framework that consists of two Conditional Generative Adversarial Networks. Generative modeling is employed to find a projection that maximizes the pairwise correlation between these two domains in a common latent embedding subspace. Extensive experiments on three challenging datasets demonstrate significant performance improvements over the state-of-the-art methods and two top-performing commercial off-the-shelf SDKs, i.e., Verifinger 12.0 and Innovatrics. We also achieve a high-performance gain by combining multiple fingers of the same subject using a score fusion model.","1617-5468","978-1-6654-7666-9","10.1109/BIOSIG55365.2022.9897056","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897056","contactless fingerphoto recognition;fingerprint interoperability;coupled GAN","Performance evaluation;Pairwise error probability;Fingers;Fingerprint recognition;Sensor fusion;Generative adversarial networks;Sensors","biometrics (access control);feature extraction;fingerprint identification;gallium compounds;image fusion;image matching;learning (artificial intelligence)","GAN-based score-level fusion;multifinger contact;interoperability;contactless images;fingerprint matching;contactless fingerprinting devices;biometric authentication;perspective distortion;elastic deformation;contactless fingerphotos;direct matching;contactless fingerprint probe images;legacy contact-based gallery images;coupled deep learning framework;Conditional Generative Adversarial Networks;Generative modeling;common latent embedding subspace;multiple fingers;score fusion model","","","","19","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"GAN Evaluation by Multi-Method Fusion","S. Chen; Q. Xu; S. Zhong; K. Chen","College of Mathematics and Computer Science, Fuzhou University, Fuzhou, Fujian, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, Fujian, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, Fujian, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, Fujian, China","2019 2nd International Conference on Artificial Intelligence and Big Data (ICAIBD)","16 Sep 2019","2019","","","36","44","Generative adversarial networks (GAN) have made great progress in areas, such as computer vision, but evaluation of GAN models is still a daunting task. Numerous researchers have proposed many methods for GAN model evaluation. However, most methods have strong pertinence, which leads to large deviations in the results of different evaluation methods. To date, no consensus has been reached on which method best captures the advantages and limitations of GAN models. In this study, a multi-method fusion approach is proposed to evaluate GAN models from many aspects, such as accuracy, diversity of the images generated by the model, and similarity with the training images. This method provides a new idea for GAN model assessment. Meanwhile, a strategy similar to ensemble learning is used to analyze the generalization of multi-method fusion for GAN assessment. This method and the human eye cognitive assessment model are used on celebA benchmarks and self-acquired image data sets to generate results. Through human eye cognition and comparison with the latest GAN model assessment method, showing that the multi-method fusion GAN evaluation method proposed in this paper has strong robustness and effectiveness.","","978-1-7281-0831-5","10.1109/ICAIBD.2019.8836993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8836993","GAN evaluation;images evaluation;multi-method;ensemble learning","Gallium nitride;Generative adversarial networks;Computational modeling;Data models;Entropy;Training;Generators","computer vision;image capture;image fusion;neural nets","generative adversarial networks;multimethod fusion approach;human eye cognitive assessment model;multimethod fusion GAN evaluation method;GAN model assessment method;computer vision;celebA benchmarks;self-acquired image data sets","","","","31","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Infrared and Visible Image Fusion via Interactive Compensatory Attention Adversarial Learning","Z. Wang; W. Shao; Y. Chen; J. Xu; X. Zhang","School of Applied Science, Taiyuan University of Science and Technology, Taiyuan, China; School of Applied Science, Taiyuan University of Science and Technology, Taiyuan, China; School of Applied Science, Taiyuan University of Science and Technology, Taiyuan, China; Key Laboratory of Intelligent Informatics for Safety & Emergency of Zhejiang Province, Wenzhou University, Wenzhou, China; Key Laboratory of Intelligent Informatics for Safety & Emergency of Zhejiang Province, Wenzhou University, Wenzhou, China","IEEE Transactions on Multimedia","","2022","PP","99","1","13","The existing generative adversarial fusion methods generally concatenate source images or deep features, and extract local features through convolutional operations without considering their global characteristics, which tends to produce a limited fusion performance. Toward this end, we propose a novel interactive compensatory attention fusion network, termed ICAFusion. In particular, in the generator, we construct a multi-level encoder-decoder network with a triple path, and design infrared and visible paths to provide additional intensity and gradient information for the concatenating path. Moreover, we develop the interactive and compensatory attention modules to communicate their pathwise information, and model their long-range dependencies through a cascading channel-spatial model. The generated attention maps can more focus on infrared target perception and visible detail characterization, and are used to reconstruct the fusion image. Therefore, the generator takes full advantage of local and global features to further increase the representation ability of feature extraction and feature reconstruction. Extensive experiments illustrate that our ICAFusion obtains superior fusion performance and better generalization ability, which precedes other advanced methods in the subjective visual description and objective metric evaluation. Our codes will be public at https://github.com/Zhishe-Wang/ICAFusion.","1941-0077","","10.1109/TMM.2022.3228685","Fundamental Research Program of Shanxi Province(grant numbers:201901D111260); Open Foundation of Shanxi Key Laboratory of Signal Capturing & Processing(grant numbers:ISPT2020-4); Outstanding Graduate Student Innovation Program of Taiyuan University of Science and Technology(grant numbers:SY2022079); National Natural Science Foundation of China(grant numbers:61922064,U2033210); Natural Science Foundation of Zhejiang Province(grant numbers:LY23F030001,LDT23F02024F02); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982426","image fusion;attention interaction;attention compensation;dual discriminators;adversarial learning","Feature extraction;Generators;Training;Task analysis;Image reconstruction;Generative adversarial networks;Decoding","","","","","","","IEEE","12 Dec 2022","","","IEEE","IEEE Early Access Articles"
"A Conditional Generative Adversarial Network to Fuse Sar And Multispectral Optical Data For Cloud Removal From Sentinel-2 Images","C. Grohnfeldt; M. Schmitt; X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1726","1729","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and haze-free MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-l SAR data confirm that our extended SAR-Opt-cGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519215","SAR;optical remote sensing;data fusion;deep learning;generative adversarial network (GAN);cloud-removal","Synthetic aperture radar;Optical sensors;Optical imaging;Clouds;Remote sensing;Adaptive optics;Generative adversarial networks","geophysical image processing;image fusion;radar imaging;synthetic aperture radar","cloud-free MS optical data;Sentinel-l SAR data;single-sensor MS data;reconstruct MS images;auxiliary SAR information;extended SAR-Opt-cGAN model;auxiliary SAR image;cloud-corrupted MS input;haze-free MS optical data;multispectral image data;synthetic aperture radar;conditional generative adversarial network architecture;Sentinel-2;cloud removal;multispectral optical data","","51","3","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"MLFcGAN: Multilevel Feature Fusion-Based Conditional GAN for Underwater Image Color Correction","X. Liu; Z. Gao; B. M. Chen","Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1488","1492","Color correction for underwater images has received increasing interest, due to its critical role in facilitating available mature vision algorithms for underwater scenarios. Inspired by the stunning success of deep convolutional neural network (DCNN) techniques in many vision tasks, especially the strength in extracting features in multiple scales, we propose a deep multiscale feature fusion net based on the conditional generative adversarial network (GAN) for underwater image color correction. In our network, multiscale features are extracted first, followed by augmenting local features in each scale with global features. This design was verified to facilitate more effective and faster network learning, resulting in better performance in both color correction and detail preservation. We conducted extensive experiments and compared the results with state-of-the-art approaches quantitatively and qualitatively, showing that our method achieves significant improvements.","1558-0571","","10.1109/LGRS.2019.2950056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894129","Conditional generative adversarial network (cGAN);feature extraction and fusion;image enhancement;underwater image color correction","Feature extraction;Image color analysis;Gallium nitride;Loss measurement;Generative adversarial networks;Image restoration;Generators","computer vision;convolutional neural nets;feature extraction;image classification;image colour analysis;image fusion;learning (artificial intelligence)","feature fusion-based conditional GAN;underwater image color correction;underwater scenarios;deep convolutional neural network;deep multiscale feature fusion net;conditional generative adversarial network;network learning;mature vision algorithms;DCNN","","38","","34","IEEE","7 Nov 2019","","","IEEE","IEEE Journals"
"Depth Estimation From Single Image And Semantic Prior","P. Hambarde; A. Dudhane; P. W. Patil; S. Murala; A. Dhall","Computer Vision and Pattern Recognition Lab, IIT Ropar; Computer Vision and Pattern Recognition Lab, IIT Ropar; Computer Vision and Pattern Recognition Lab, IIT Ropar; Computer Vision and Pattern Recognition Lab, IIT Ropar; Computer Vision and Pattern Recognition Lab, IIT Ropar","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","1441","1445","The multi-modality sensor fusion technique is an active research area in scene understating. In this work, we explore the RGB image and semantic-map fusion methods for depth estimation. The LiDARs, Kinect, and TOF depth sensors are unable to predict the depth-map at illuminate and monotonous pattern surface. In this paper, we propose a semantic-to-depth generative adversarial network (S2D-GAN) for depth estimation from RGB image and its semantic-map. In the first stage, the proposed S2D-GAN estimates the coarse level depthmap using a semantic-to-coarse-depth generative adversarial network (S2CD-GAN) while the second stage estimates the fine-level depth-map using a cascaded multi-scale spatial pooling network. The experimental analysis of the proposed S2D-GAN performed on NYU-Depth-V2 dataset shows that the proposed S2D-GAN gives outstanding result over existing single image depth estimation and RGB with sparse samples methods. The proposed S2D-GAN also gives efficient results on the real-world indoor and outdoor image depth estimation.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9190985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190985","Depth estimation;Single image;Semantic map;Coarse-level depth-map;Fine-level depth-map.","Estimation;Semantics;Generators;Robot sensing systems;Laser radar;Generative adversarial networks;Training","computer vision;image colour analysis;image fusion;image reconstruction;image segmentation;image sensors;neural nets;object detection;optical radar","single image depth estimation;outdoor image depth estimation;multimodality sensor fusion technique;RGB image;semantic-map fusion methods;illuminate pattern surface;monotonous pattern surface;semantic-to-depth generative adversarial network;S2D-GAN estimates;semantic-to-coarse-depth generative adversarial network;S2CD-GAN;fine-level depth-map;cascaded multiscale spatial pooling network;NYU-Depth-V2 dataset","","25","","20","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Shadow Compensation for Synthetic Aperture Radar Target Classification by Dual Parallel Generative Adversarial Network","H. Zhu; R. Leung; M. Hong","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; School of Engineering, The University of Tokyo, Tokyo, Japan; Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA","IEEE Sensors Letters","20 Jul 2020","2020","4","8","1","4","Due to the incident angle of synthetic aperture radar electromagnetic wave, the ground target is usually captured with some parts missing in the raw SAR image instead of an area of shadow. In recent years, most of the deep learning methods on SAR target classification problems are only applied to the raw SAR images, which require a large number of images to train the neural network for a good result. Aiming at this problem, we propose a novel method for reconstructing the complete target profile to study the SAR target classification problem, which merges two raw SAR images with the opposite azimuth together to one new image. Then, a dual parallel generative adversarial network is proposed to extend the fused SAR image dataset. Finally, we construct a convolutional neural network to train the extended fused image dataset, which outputs the label for each ground target. The experimental result shows that the whole network framework conducted on the MSTAR dataset achieves the average classification accuracy of 99.93% with far fewer data than the state-of-the-art methods, which is a breakthrough in the research process for the SAR target classification with limited labeled data.","2475-1472","","10.1109/LSENS.2020.3009179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9140351","Sensor signals processing;generative adversarial network (GAN);MSTAR;synthetic aperture radar (SAR);target classification;shadow","Synthetic aperture radar;Training;Gallium nitride;Azimuth;Generative adversarial networks;Generators;Neural networks","convolutional neural nets;image classification;image fusion;learning (artificial intelligence);radar imaging;synthetic aperture radar","SAR target classification problem;raw SAR image;target profile;dual parallel generative adversarial network;fused SAR image dataset;convolutional neural network;extended fused image dataset;ground target;shadow compensation;synthetic aperture radar target classification;deep learning methods;electromagnetic wave;MSTAR dataset","","16","","13","IEEE","14 Jul 2020","","","IEEE","IEEE Journals"
"PTGAN: A Proposal-Weighted Two-Stage GAN with Attention for Hyperspectral Target Detection","H. Qin; W. Xie; Y. Li; K. Jiang; J. Lei; Q. Du","State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; State Key Lab. of Integrated Services Networks, Xidian University, Xi'an, China; The Department of Electrical and Computer Engineering, Mississippi State University, USA","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4428","4431","In this paper, a proposal-weighted two-stage generative adversarial network (GAN) with attention mechanism is proposed for hyperspectral target detection (HTD). PTGAN leverages GAN to estimate spectral background distribution and realize mapping from the latent space to the spectral space. Meanwhile, PTGAN conducts the reversed mapping through latent-spectral-latent and spectral-latent-spectral learning. On this basis, PTGAN implements accurate reconstruction of background spectrum via latent space. Therefore, targets of interest can be detected through larger pixel-level reconstruction error. In particular, the variance attention module is designed to make full use of global information among spectral bands to selectively emphasize channel-wise spectral features. Furthermore, a proposal-weighted strategy in a two-stage manner reduces the false alarm of detection by refining the previous detection proposal. Finally, exponential nonlinear fusion combines the discriminative feature from two stages to suppress the background. Extensive experiments on two real hyperspectral images (HSIs) verify the effectiveness of PTGAN.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553721","National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); China Postdoctoral Science Foundation(grant numbers:2019T120878,2017M620440); Fundamental Research Funds for the Central Universities(grant numbers:JB180104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553721","Hyperspectral target detection;generative adversarial network;background reconstruction;two-stage","Refining;Geoscience and remote sensing;Object detection;Generative adversarial networks;Feature extraction;Proposals;Data mining","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image reconstruction;learning (artificial intelligence);neural nets;object detection","reversed mapping;spectral-latent-spectral learning;latent space;pixel-level reconstruction error;variance attention module;spectral bands;channel-wise spectral features;detection proposal;hyperspectral images;proposal-weighted two-stage GAN;hyperspectral target detection;attention mechanism;PTGAN;spectral background distribution;spectral space;proposal-weighted two-stage generative adversarial network;HTD;latent-spectral-latent learning;background spectrum reconstruction;exponential nonlinear fusion;discriminative feature;HSIs","","2","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Image restoration method based on GAN and multi-scale feature fusion","X. Jin; Y. Hu; C. -Y. Zhang","Dalian Maritime University, Dalian; Dalian Maritime University, Dalian; Dalian Maritime University, Dalian","2020 Chinese Control And Decision Conference (CCDC)","11 Aug 2020","2020","","","2305","2310","In the research of existing image restore algorithms, defective images and target images exist in pairs, which leads to poor applicability of the algorithm. Aiming at the shortcomings of the existing algorithms, such as low accuracy and poor visual consistency, an image restoration method based on GAN and multi-scale feature fusion was proposed. The generator of this algorithm uses a standard encoder-decoder structure. The encoder based on the VGG-16 full convolutional neural network is used to extract the features of the defect image. Low-dimensional features and high Dimensional features are fused to enrich the input of deep convolutions. Aiming at the problems of large error oscillation amplitude, gradient disappearance or gradient explosion in GAN training, the idea of WGAN is used, and the EM distance is used to simulate the sample data distribution. The similarity between the output restored image and the target image is enhanced by introducing L1 loss. This paper performs experiments on the same face data set. The experimental results show that the algorithm improves the accuracy of the restored image and can generate more realistic restored images.","1948-9447","978-1-7281-5855-6","10.1109/CCDC49329.2020.9164498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164498","GAN;Fully convolutional neural network;VGG-16;Feature fusion;WGAN","Image restoration;Generators;Gallium nitride;Training;Generative adversarial networks;Feature extraction;Decoding","convolutional neural nets;feature extraction;image fusion;image restoration","low-dimensional features;high dimensional features;GAN training;image restoration method;multiscale feature fusion;defective images;standard encoder-decoder structure;VGG-16 full convolutional neural network","","1","","18","IEEE","11 Aug 2020","","","IEEE","IEEE Conferences"
"Multi-Scale Feature Guided Low-Light Image Enhancement","L. Guo; R. Wan; G. -M. Su; A. C. Kot; B. Wen","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Dolby Laboratories, Sunnyvale, CA, USA; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","554","558","Low-light image enhancement aims at enlarging the intensity of image pixels to better match human perception and to improve the performance of subsequent vision tasks. While it is relatively easy to enlighten a globally low-light image, the lighting condition of realistic scenes is usually non-uniform and complex, e.g., some images may contain both bright and extremely dark regions, with or without rich features and information. Existing methods often generate abnormal light-enhancement results with over-exposure artifacts without proper guidance. To tackle this challenge, we propose a multi-scale feature guided attention mechanism in the deep generator, which can effectively perform a spatially-varying light enhancement. The attention map is fused by both the gray map and extracted feature map of the input image, to focus more on those dark and informative regions. Our baseline is an unsupervised generative adversarial network, which can be trained without any low/normal light image pair. Experimental results demonstrate the superiority in visual quality and performance of subsequent object detection over state-of-the-art alternatives.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506785","Low-Light;Generative Adversarial Network (GAN);Attention;Unsupervised Learning","Visualization;Inverse problems;Conferences;Lighting;Object detection;Feature extraction;Generative adversarial networks","feature extraction;image colour analysis;image enhancement;image fusion;image matching;neural nets","multiscale feature guided low-light image enhancement;multiscale feature guided attention mechanism;feature map extraction;human perception matching;spatially-varying light enhancement;gray map fusion;unsupervised generative adversarial network","","1","","30","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Multi-poses Face Frontalization based on Pose Weighted GAN","J. Ma; F. Zhou","Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia, School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China","2019 IEEE 3rd Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)","6 Jun 2019","2019","","","1271","1276","In many scenes, the frontal face image is the only criterion for judging the identity of a person. However, it is difficult to collect a standard frontal image in an uncontrolled environment. To get a clear frontal image from a large variety of profile images, there are many studies on face frontalization. Some researches need three-dimension face data or prior pose information while others do not take into account the effect of pose information. And there are restrictions on the number of poses of input face images. Because of the ill-consideration of pose information, the authenticity of generated frontal face images is not high when we input multi-poses profile images. To resolve this problem, this paper proposes a Pose-weighted Generative Adversarial Network (PWGAN), which adds a pre-trained pose certification module to learn face pose information. For the single input image, PWGAN combines fusion features with pose features. And for multiple input images, PWGAN uses pose information to dynamic distribute weights when fusing feature maps. PWGAN makes full use of pose information to make the generation network learn more about facial features and get better-generating effect. Through contrastive experiments, this paper proves that PWGAN has a better effect on multi-poses face frontalization than the above methods.","","978-1-5386-6243-4","10.1109/ITNEC.2019.8729088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8729088","face frontalization;feature map;feature fusion;autoencoder;generative adversarial network","Face;Feature extraction;Face recognition;Generators;Certification;Generative adversarial networks;Deep learning","face recognition;feature extraction;image fusion;learning (artificial intelligence);pose estimation","pose features;multiple input images;PWGAN;pose information;multiposes face frontalization;frontal face image;standard frontal image;three-dimension face data;input face images;Pose-weighted Generative Adversarial Network;single input image;pose weighted GAN;pose-weighted generative adversarial network;feature map fusion;fusion features","","1","","17","IEEE","6 Jun 2019","","","IEEE","IEEE Conferences"
"HFGAN: A Heterogeneous Fusion Generative Adversarial Network for Sar-to-Optical Image Translation","N. Yu; A. Ma; Y. Zhong; X. Gong","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Beijing Institute of Remote Sensing Information, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2864","2867","Due to the influence of the imaging mechanism of SAR images, it is difficult to interpret ground information through SAR images without expert knowledge. On the contrary, optical images have rich spatial and color information, so it is necessary to conduct research on the translation of SAR to optical remote sensing images. In this end, we propose a heterogeneous fusion generative adversarial network (HFGAN) for SAR-to-optical image translation. There are two main improvements: (1) Complementary generation of global structure and texture information. A heterogeneous fusion generator and a multi-scale discriminator are proposed to ensure that the global and detailed features of the generated image are more accurate and rich. (2) Color fidelity. Chromatic aberration loss are introduced to reduce the color difference between the generated image and the real optical image. Through qualitative and quantitative experiments, it is proved that the proposed method not only obtains better visual effects, but also has certain progress in the evaluation metrics, which proves that the proposed method is superior to the previous advanced methods in SAR-to-optical image translation.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883519","Remote sensing;Generative Adversarial Network;Image tranlation","Optical losses;Image color analysis;Optical imaging;Generative adversarial networks;Visual effects;Generators;Radar polarimetry","aberrations;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image texture;optical images;radar imaging;remote sensing;synthetic aperture radar","optical remote sensing images;heterogeneous fusion generative adversarial network;sar-to-optical image translation;heterogeneous fusion generator;imaging mechanism;SAR images;rich spatial;color information","","","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"3D Point Cloud Generation Using Adversarial Training for Large-Scale Outdoor Scene","T. Shinohara; H. Xiu; M. Matsuoka","Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan; Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan; Department of Architecture and Building Engineering, Tokyo Institute of Technology, Yokohama, Japan","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2935","2938","Three-dimensional (3D) point clouds are becoming an important part of the geospatial domain. During research on 3D point clouds, deep-learning models have been widely used for the classification and segmentation of 3D point clouds observed by airborne LiDAR. However, most previous studies used discriminative models, whereas few studies used generative models. Specifically, one unsolved problem is the synthesis of large-scale 3D point clouds, such as those observed in outdoor scenes, because of the 3D point clouds' complex geometric structure. In this paper, we propose a generative model for generating large-scale 3D point clouds observed from airborne LiDAR. Generally, because the training process of the famous generative model called generative adversarial network (GAN) is unstable, we combine a variational autoen-coder and GAN to generate a suitable 3D point cloud. We experimentally demonstrate that our framework can generate high-density 3D point clouds by using data from the 2018 IEEE GRSS Data Fusion Contest.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554523","KAKENHI(grant numbers:19H02408); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554523","Generative Adversarial Network;Variational Autoencoder;Deep Learning;Point Clouds;Airborne LiDAR","Training;Solid modeling;Three-dimensional displays;Laser radar;Atmospheric modeling;Geoscience and remote sensing;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;learning (artificial intelligence);optical radar;remote sensing by laser beam;sensor fusion;terrain mapping","airborne LiDAR;large-scale 3D point clouds;famous generative model;generative adversarial network;suitable 3D point cloud;high-density 3D point clouds;3D point cloud generation;large-scale outdoor scene;three-dimensional point clouds;deep-learning models","","","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"An Improved Method for Pan-Sharpening Based on Pan-GAN","Y. Li; J. Li; X. Du; Y. Huang; J. Lei","School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China","2022 7th International Conference on Image, Vision and Computing (ICIVC)","19 Sep 2022","2022","","","282","286","Pan-sharpening refers to the fusion of pan images and low-resolution multispectral remote sensing images to obtain high-resolution multispectral images. Generative adversarial network (GAN)-based pan-sharpening methods have recently became popular due to the lack of ground-truth data during training. However, GAN-based methods suffer from training instability and convergence difficulties. To deal the issues, we propose a novel GAN-based pan-sharpening method using additional constraint. First, we adopt the geometric consistency constraint to enforce the network to preserve the spatial structure of image. Second, we introduce an attention mechanism in the generator to extract useful information through the features of pan image and multispectral images and pay more attention to meaningful regions. Experimental results show the effectiveness of our method in terms of the quantitative and visual results.","","978-1-6654-6734-6","10.1109/ICIVC55077.2022.9887169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887169","pan-sharpening;geometric consistency;attention mechanism;Generative adversarial network","Training;Visualization;Feature extraction;Generative adversarial networks;Generators;Data mining;Remote sensing","geophysical image processing;image enhancement;image fusion;image resolution;remote sensing","GAN-based methods;training instability;convergence difficulties;novel GAN-based pan-sharpening method;pan image;pan-GAN;low-resolution multispectral remote sensing images;high-resolution multispectral images;generative adversarial network-based pan-sharpening methods;ground-truth data","","","","19","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Binocular Infrared Depth Estimation Based On Generative Adversarial Network","J. Wang; K. Geng; G. Yin; X. Cheng; Y. Sun; P. Ding","School of Mechanical Engeering, Southeast University, Nanjing, China; School of Mechanical Engeering, Southeast University, Nanjing, China; School of Mechanical Engeering, Southeast University, Nanjing, China; School of Mechanical Engeering, Southeast University, Nanjing, China; School of Mechanical Engeering, Southeast University, Nanjing, China; School of Mechanical Engeering, Southeast University, Nanjing, China","2022 6th CAA International Conference on Vehicular Control and Intelligence (CVCI)","8 Dec 2022","2022","","","1","6","Dedicated to the fulfillment of the all-weather scene perception of intelligent vehicle, our work focus on depth estimation based on binocular infrared image. First of all, considering the rareness of binocular infrared datasets, we apply a top-down attention and gradient alignment based on generative adversarial network (GAN) to convert binocular visible datasets into pseudo-infrared datasets, which was used to fine-tune the depth estimation network. In this way, the basal function of binocular infrared depth estimation could be achieved. Then, the enhancement and super-resolution (SR) preprocessing method based on high-order degradation modeling process was introduced to enhance the image edge information and enrich the image details. The experiment results showed that the image details are richer after SR, which improves the accuracy and breadth of binocular depth perception by 5%.","","978-1-6654-5374-5","10.1109/CVCI56766.2022.9964565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964565","Infrared image;binocular depth estimation;generative adversarial network;super-resolution","Degradation;Intelligent vehicles;Image edge detection;Superresolution;Estimation;Generative adversarial networks","edge detection;image fusion;infrared imaging;neural nets;spatial variables measurement;stereo image processing;visual perception","all-weather scene perception;binocular depth perception;binocular infrared datasets;binocular infrared depth estimation;binocular infrared image;binocular visible datasets;depth estimation network;enhancement method;GAN;generative adversarial network;gradient alignment;high-order degradation modeling process;image details;image edge information enhancement;intelligent vehicle;pseudoinfrared datasets;super-resolution preprocessing method;top-down attention","","","","11","IEEE","8 Dec 2022","","","IEEE","IEEE Conferences"
"Cycle GAN Based Heterogeneous Spatial-Spectral Fusion for Soil Moisture Downscaling","M. Jiang; H. Shen; J. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","4819","4822","Soil moisture (SM) downscaling aims to solve the coarse resolution problem of passive microwave SM products. On the basis of SMAP SM products and related MODIS products, this study develops a deep residual cycle generative adversarial network (GAN) based heterogeneous spatial-spectral fusion method to downscale SMAP SM from 36km to 9km. On the one hand, the proposed method creatively regards the MODIS products that can reflect the SM state as the spectral features of SM in a broad sense and performs the heterogeneous spatial-spectral fusion between the low-resolution (LR) SM product and high-resolution (HR) MODIS products. On the other hand, considering the spatial correlation of SM, the proposed method utilizes a deep residual cycle generative adversarial network (GAN) to extract and fuse features of heterogeneous images through convolutions. Both qualitative and quantitative evaluation of experimental results shows that the proposed method can generate high accuracy SM products.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884702","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884702","Soil moisture;downscale;heterogeneous spatial-spectral fusion;cycle GAN","Fuses;Soil moisture;Geoscience and remote sensing;Generative adversarial networks;Feature extraction;Microwave theory and techniques;Spatial resolution","hydrological techniques;image fusion;image resolution;microwave measurement;moisture;remote sensing;soil","cycle GAN;heterogeneous spatial-spectral fusion;soil moisture downscaling;coarse resolution problem;passive microwave SM products;SMAP SM products;related MODIS products;deep residual cycle generative adversarial network;spatial-spectral fusion method;downscale SMAP SM;SM state;spectral features;low-resolution SM product;heterogeneous images;high accuracy SM products;size 9.0 km to 36.0 km","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Sparse to Dense: LiDAR Point Cloud Upsampling by Multi-modal GAN","C. Liu; A. Jiang; H. K. Kwan","College of Internet of Things Engineering Hohai University, Changzhou, Jiangsu, China; College of Internet of Things Engineering Hohai University, Changzhou, Jiangsu, China; Department of Electrical and Computer Engineering, Uinversity of Windsor, Windsor, Ontario, Canada","TENCON 2022 - 2022 IEEE Region 10 Conference (TENCON)","20 Dec 2022","2022","","","1","5","LiDARs and cameras are critical sensors that provide complementary information for 3D detection in autonomous driving. However, current state-of-art methods on 3D object detection benchmarks achieve unsatisfied performance. In this paper, we propose a point cloud upsampling method based on generative adversarial network (GAN). The generator of our network includes deep feature fusion module based on multi-modality, which combines rich contextual information of images and geometric information of point clouds. To distinguish background and foreground areas, a class-aware vector is incorporated after fusion. The generated high resolution point clouds assist the discriminator to learn global properties of point clouds. Objective and subjective experimental results demonstrate that the visual quality of the upsampled point clouds generated by our method is better than those of the state-of-the-art methods.","2159-3450","978-1-6654-5095-9","10.1109/TENCON55691.2022.9978157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978157","feature fusion;GAN;point cloud;upsampling","Point cloud compression;Visualization;Three-dimensional displays;Laser radar;Image resolution;Object detection;Generative adversarial networks","image colour analysis;image fusion;image resolution;image sampling;neural nets;object detection;optical radar","3D object detection benchmarks;autonomous driving;cameras;complementary information;critical sensors;deep feature fusion module;generative adversarial network;geometric information;high resolution point clouds;LiDAR point cloud upsampling;multimodal GAN;multimodality;upsampled point clouds","","","","25","IEEE","20 Dec 2022","","","IEEE","IEEE Conferences"
"A Temporal-Spectral Generative Adversarial Fusion Network for Improving Satellite Hyperspectral Temporal Resolution","K. Ren; W. Sun; J. Zhou; X. Meng; G. Yang; J. Peng","Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; Department of Geography and Spatial Information Techniques, Ningbo University, Ningbo, China; Faculty of Mathematics and Statistics, Hubei University, Wuhan, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","899","902","The improvement of temporal resolution of hyperspectral (HS) data is a fundamental and challenging problem. In this paper, we propose a Temporal-Spectral fusion method based on Generative Adversarial Network (TSF-GAN). First, the generator is used to train the nonlinear relationship between multispectral (MS) and HS data pairs at time T1 and T3, and we map the relationship to the MS data at T2 to obtain the HS data. Second, the discriminator is used to identify whether the differential image of HS data at different times is consistent with that of MS data, and whether the HS data at time T2 after spectral down-sampling is consistent with that of MS data at time T2. Preliminary experimental results demonstrate that the proposed TSF-GAN achieves comparative fidelity and has strong practicability.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884778","National Natural Science Foundation of China(grant numbers:42122009,42171351,41971296,61871177,42171326,41801256,41801252); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884778","Temporal-Spectral fusion;TSF-GAN;hyperspectral;multispectral","Satellites;Image resolution;Geoscience and remote sensing;Feature extraction;Generative adversarial networks;Generators;Hyperspectral imaging","hyperspectral imaging;image fusion;image resolution;neural nets","TSF-GAN;data pairs;MS data;HS data;spectral down-sampling;hyperspectral data;temporal-spectral generative adversarial fusion network;satellite hyperspectral temporal resolution;differential image","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Semantic GAN: Application for Cross-Domain Image Style Transfer","P. Li; M. Yang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","2019 IEEE International Conference on Multimedia and Expo (ICME)","5 Aug 2019","2019","","","910","915","Image style transfer has attracted much attention from many fields and received promising performance. However, style transfer in the cross-domain field, e.g., the transfer between near-infrared and visible light images, is rarely studied. In the cross-domain image style transfer, one key issue is mismatching problem existing in the generated semantic regions. In this paper, we propose a novel model of Semantic GAN, which integrates the semantic guidance and the recent CycleGAN. In particular, we present a semantic style loss with Gram matrix to well preserve the semantic information in the generated images. The proposed Semantic GAN can control the transfer in the right way with semantic masks and solve the mismatching problem. We apply our approach to two outdoor scene datasets to evaluate the performance of all competing methods. The experimental results show that our approach outperforms previous methods in addressing the mismatching problem and providing a good quality result.","1945-788X","978-1-5386-9552-4","10.1109/ICME.2019.00161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784957","SemanticGAN;Style Transfer;Cross Domain;mismatching problem","Semantics;Gallium nitride;Image segmentation;Generative adversarial networks;Generators;Training;Image synthesis","feature extraction;image classification;image fusion;image motion analysis;image representation;image segmentation;image texture;neural nets;object detection;video signal processing","Semantic GAN;cross-domain image style transfer;cross-domain field;near-infrared images;visible light images;semantic guidance;semantic style loss;semantic information;semantic masks;CycleGAN","","","","16","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"Max-Fusion of Random Ensemble Subspace Discriminant with Aggregation of MFCCs and High Scalogram Coefficients for Acoustics Classification","C. Siong Chin; J. Xiao","Faculty of Science, Agriculture, and Engineering, Newcastle University Singapore, Singapore; Faculty of Science, Agriculture, and Engineering, Newcastle University Singapore, Singapore","2021 IEEE/ACIS 19th International Conference on Computer and Information Science (ICIS)","23 Aug 2021","2021","","","54","59","In this paper, a random sub-space discriminant classifier for classifying acoustic devices that combines the features obtained from Mel-frequency cepstral coefficients (MFCCs), and scalogram coefficients is proposed. The aggregated features for the random ensemble sub-space discriminant classifier model are used. The maximum weight fusion mechanisms are used to fuse the ensemble classifier’s results from the two sets of coefficients. With a higher concentration of scalogram coefficients, the accuracy improves by around 8.15% compared to single feature extraction via MFCCs. The random subspace discriminant classifier achieves the classification accuracy of approximately 74.9% or 20.8% better than the baseline result of 54.1% obtained in the DCASE2020-Task1A Challenge.","","978-1-6654-1893-5","10.1109/ICIS51600.2021.9516854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516854","random ensemble subspace discriminant;acoustic classification;Mel-frequency cepstral coefficients;scalogram coefficients;classification accuracy","Performance evaluation;Information science;Fuses;Generative adversarial networks;Feature extraction;Naive Bayes methods;Decision trees","cepstral analysis;feature extraction;image fusion;pattern classification;signal classification;speech recognition","random subspace discriminant classifier;classification accuracy;max-fusion;random ensemble subspace discriminant;aggregation;MFCCs;high scalogram coefficients;acoustics classification;random sub-space discriminant classifier;acoustic devices;Mel-frequency cepstral coefficients;aggregated features;random ensemble sub-space discriminant classifier model;maximum weight fusion mechanisms;ensemble classifier;single feature extraction","","","","21","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Metal Surface Defect Detection Based on Weighted Fusion","C. Wang; J. Xu; X. Liang; D. Yin","School of Computer Science and Technology Changchun University of Science and Technology, Changchun, China; School of Computer Science and Technology Changchun University of Science and Technology, Changchun, China; School of Computer Science and Technology Changchun University of Science and Technology, Changchun, China; School of Computer Science and Technology Changchun University of Science and Technology, Changchun, China","2020 International Conference on Virtual Reality and Visualization (ICVRV)","15 Jul 2021","2020","","","179","184","The texture of the metal surface is irregular, and the distinction between defect feature and texture feature is not obvious, which brings great difficulties to automatic defect detection. Traditional methods of defect detection have low accuracy, difficult design, and poor robustness, and the speed and accuracy of existing deep learning methods cannot reach the high standards in the factory. In this paper, a YOLOv4 defect detection algorithm based on weighted fusion is proposed. It uses a GAN network to generate a mask map in real time, and performs weighted fusion of the feature maps in YOLOv4. Experimental results show that the proposed method improves the recognition accuracy by 1% without seriously affecting the speed of the YOLOv4.","2375-141X","978-1-6654-0497-6","10.1109/ICVRV51359.2020.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478644","defect detection;YOLO;GAN;mask map","Visualization;Shape;Metals;Virtual reality;Feature extraction;Generative adversarial networks;Surface texture","automatic optical inspection;feature extraction;image fusion;image texture;learning (artificial intelligence);neural nets;production engineering computing","metal surface defect detection;defect feature;automatic defect detection;deep learning methods;YOLOv4 defect detection algorithm;weighted fusion;feature maps","","","","22","IEEE","15 Jul 2021","","","IEEE","IEEE Conferences"
"Remote Sensing Image Super-Resolution Via Attentional Feature Aggregation Generative Adversarial Network","F. Cai; K. -Y. Wu; F. Wang","Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key Laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2598","2601","The extraction of high-frequency details is generally neglected in single image super-resolution (SISR) for remote sensing images. In this paper, we propose an attentional feature aggregation generative adversarial network (AFA-GAN) with the capability of strong feature extraction and attentional feature fusion to generate high-resolution remote sensing images. We adopt the residual feature aggregation framework for the feature extraction to make full use of the hierarchical features on the residual branches. To better fuse global and local features with inconsistent scales, an attentional feature fusion mechanism is utilized in residual feature aggregation modules. The comprehensive experiments with state-of-the-art SISR methods on the UC Merced dataset demonstrate the effectiveness and superiority of our AFA-GAN.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884863","National Natural Science Foundation of China(grant numbers:61901122); Natural Science Foundation of Shanghai(grant numbers:20ZR1406300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884863","Remote sensing images;single image super-resolution (SISR);attentional feature aggregation (AFA);generative adversarial network (GAN)","Visualization;Fuses;Superresolution;Feature extraction;Generative adversarial networks;Remote sensing;Image reconstruction","feature extraction;geophysical image processing;image fusion;image reconstruction;image resolution;remote sensing","high-resolution remote sensing images;residual feature aggregation framework;hierarchical features;global features;local features;attentional feature fusion mechanism;residual feature aggregation modules;AFA-GAN;remote sensing image super-resolution;high-frequency details;single image super-resolution;attentional feature aggregation generative adversarial network;strong feature extraction","","","","18","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Music to Dance: Motion Generation Based on Multi-Feature Fusion Strategy","Y. Gao; W. Yu; X. Zhang; X. Deng; Z. Zhang","Southwest University of science and technology, Mianyang, China; Southwest University of science and technology, Mianyang, China; Southwest University of science and technology, Mianyang, China; Southwest University of science and technology, Mianyang, China; Hosei University, Tokyo, Japan","2022 IEEE International Symposium on Circuits and Systems (ISCAS)","11 Nov 2022","2022","","","2735","2739","Studies on generating dance sequences from music can greatly promote dance teaching and animation production. However, the current results of such tasks are not very satisfactory. Due to the lack of consideration of human body structure, unreal phenomena such as slippery feet and twisted joints will appear in the resulting movements. Since only a single correlation feature between music and dance is learned, there is a problem of poor matching between generated actions and music. To solve these problems, we propose a learning framework based on GAN and a multi-feature fusion strategy to realize the mapping from music to dance. We use two discriminators to constrain style and authenticity respectively to make our actions generated by the network more coherent and natural, which is essential in producing authentic dance sequences. Moreover, we extracted the three characteristics of music style, beat, and structure. Then we used the feature fusion method to obtain a comprehensive feature representation, which guarantees the consistency of the generated dance sequence with the input music to the greatest extent. The experimental quantitative and qualitative results show that our method can generate accurate, consistent, beat-matching dance movements from music. In addition, we also collected and produced a data set containing music features and corresponding pose sequences, which is convenient for pose generation based on music features.","2158-1525","978-1-6654-8485-5","10.1109/ISCAS48785.2022.9937431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9937431","Music to dance;Multi-feature fusion;GAN","Training;Humanities;Correlation;Production;Generative adversarial networks;Feature extraction;Data models","computer animation;feature extraction;humanities;image fusion;image motion analysis;learning (artificial intelligence);music","multifeature fusion strategy;dance teaching;animation production;human body structure;generated actions;authentic dance sequences;music style;feature fusion method;comprehensive feature representation;generated dance sequence;input music;dance movements;music features;motion generation","","","","24","IEEE","11 Nov 2022","","","IEEE","IEEE Conferences"
"Object Quality Guided Feature Fusion for Person Re-identification","L. Zhang; N. Jiang; Q. Diao; D. Huang; Z. Zhou; W. Wu","State Key Lab of Virtual Reality Technology and Systems, Beihang University, China; Information Engineering College, Capital Normal University, China; State Key Lab of Virtual Reality Technology and Systems, Beihang University, China; State Key Lab of Virtual Reality Technology and Systems, Beihang University, China; State Key Lab of Virtual Reality Technology and Systems, Beihang University, China; State Key Lab of Virtual Reality Technology and Systems, Beihang University, China","2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)","21 Dec 2021","2021","","","1083","1087","Person re-identification (Re-ID) is an essential task in computer vision, which aims to match a person of interest across multiple non-overlapping camera views. It is a fundamental challenging task because of the conflicts between large variations of samples and the limited scale of training sets. Data augmentation method based on generative adversarial network (GAN) is an efficient way to relieve this dilemma. However, existing methods do not consider how to keep identity information and filter the noise of the generated auxiliary samples during Re-ID training. In this paper, we propose object quality guided feature fusion network for person re-identification, which consists of a self-supervised object quality estimation module and a feature fusion module. Specifically, the former evaluates the quality of the auxiliary data to filter the noise and the disturbing features, while the later accomplishes the feature fusion based on object quality estimation in the collection-to-collection recognition manner to make full use of auxiliary data. Extensive performance analysis and experiments are conducted on two benchmark datasets (Market-1501 and DukeMTMC-reID) to show that our proposed approach outperforms or shows comparable results to the existing best performed methods.","2375-0197","978-1-6654-0898-1","10.1109/ICTAI52525.2021.00171","National Natural Science Foundation of China; Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643363","person re-identification;feature fusion;quality estimation;data augmentation","Training;Computer vision;Image recognition;Conferences;Estimation;Generative adversarial networks;Information filters","cameras;computer vision;feature extraction;image fusion;learning (artificial intelligence);sensor fusion","object quality guided feature fusion;person re-identification;computer vision;nonoverlapping camera views;data augmentation method;generative adversarial network;identity information;noise filtering;Re-ID training;quality guided feature fusion network;self-supervised object quality estimation module;feature fusion module;auxiliary data;Market-1501 dataset;DukeMTMC-reID dataset;collection-to-collection recognition","","","","17","IEEE","21 Dec 2021","","","IEEE","IEEE Conferences"
"Nonlocal Regularized CNN for Inpainting Using Edge Prior","C. Luo; Y. You; Q. Jin","Inner Mongolia University, IMU, Hohhot, China; Inner Mongolia University, IMU, Hohhot, China; Inner Mongolia University, IMU, Hohhot, China","2021 5th International Conference on Communication and Information Systems (ICCIS)","21 Dec 2021","2021","","","144","148","Image inpainting, filling missing regions of an image, is a crucial and hard task. Numerous existing works concerning generative adversarial network (GAN) have made outstanding performance, especially those methods that use image edge as prior information. However, better application of edge prior and long-distance correlation of deteriorated images are still open problems. In our paper, a nonlocal regularized convolutional neural network with edge prior is proposed. We fuse low-level features and high-level features of the edge image, which aims at providing more details such as lines and textures for image inpainting. Furthermore, we introduce a non-local block to enhance the correlation among the missing regions and existing regions to improve visual consistency. Experimental results are reported qualitatively and quantitatively to show that our proposed framework outperforms previous well-known ones.","","978-1-6654-2216-1","10.1109/ICCIS53528.2021.9645765","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645765","image inpainting;generative adversarial network;deep learning;feature fusion;nonlocal CNN","Deep learning;Visualization;Correlation;Fuses;Image edge detection;Generative adversarial networks;Filling","edge detection;feature extraction;image denoising;image fusion;image reconstruction;image representation;image restoration;image segmentation;image texture;learning (artificial intelligence);neural nets;video signal processing","nonlocal regularized CNN;edge prior;image inpainting;missing regions;crucial task;numerous existing works;generative adversarial network;image edge;long-distance correlation;deteriorated images;nonlocal regularized convolutional neural network;low-level features;high-level features;edge image;existing regions","","","","33","IEEE","21 Dec 2021","","","IEEE","IEEE Conferences"
"ZeRGAN: Zero-Reference GAN for Fusion of Multispectral and Panchromatic Images","W. Diao; F. Zhang; J. Sun; Y. Xing; K. Zhang; L. Bruzzone","School of Information Science and Engineering, Shandong Normal University, Jinan 250358, China.; School of Information Science and Engineering, Shandong Normal University, Jinan 250358, China.; School of Information Science and Engineering, Shandong Normal University, Jinan 250358, China (e-mail: jiandesun@hotmail.com); National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology and the School of Computer Science, Northwestern Polytechnical University, Xi'an 710072, China.; School of Information Science and Engineering, Shandong Normal University, Jinan 250358, China, and also with the Department of Information Engineering and Computer Science, University of Trento, 38123 Trento, Italy (e-mail: zhangkainuc@163.com); Department of Information Engineering and Computer Science, University of Trento, 38123 Trento, Italy.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","15","In this article, we present a new pansharpening method, a zero-reference generative adversarial network (ZeRGAN), which fuses low spatial resolution multispectral (LR MS) and high spatial resolution panchromatic (PAN) images. In the proposed method, zero-reference indicates that it does not require paired reduced-scale images or unpaired full-scale images for training. To obtain accurate fusion results, we establish an adversarial game between a set of multiscale generators and their corresponding discriminators. Through multiscale generators, the fused high spatial resolution MS (HR MS) images are progressively produced from LR MS and PAN images, while the discriminators aim to distinguish the differences of spatial information between the HR MS images and the PAN images. In other words, the HR MS images are generated from LR MS and PAN images after the optimization of ZeRGAN. Furthermore, we construct a nonreference loss function, including an adversarial loss, spatial and spectral reconstruction losses, a spatial enhancement loss, and an average constancy loss. Through the minimization of the total loss, the spatial details in the HR MS images can be enhanced efficiently. Extensive experiments are implemented on datasets acquired by different satellites. The results demonstrate that the effectiveness of the proposed method compared with the state-of-the-art methods. The source code is publicly available at https://github.com/RSMagneto/ZeRGAN.","2162-2388","","10.1109/TNNLS.2021.3137373","National Natural Science Foundation of China(grant numbers:61901246); China Postdoctoral Science Foundation(grant numbers:2019TQ0190,2019M662432); Open Fund of National Engineering Laboratory for Integrated Aerospace-Ground-Ocean Big Data Application Technology China(grant numbers:20200208); China Scholarship Council(grant numbers:202008370035); Natural Science Foundation for Distinguished Young Scholars of Shandong Province(grant numbers:JQ201718); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669094","Generative adversarial network (GAN);image fusion;multispectral image;panchromatic (PAN) image;zero-reference training","Training;Generators;Generative adversarial networks;Pansharpening;Spatial resolution;Training data;Satellites","","","","2","","","IEEE","4 Jan 2022","","","IEEE","IEEE Early Access Articles"
"MARTA GANs: Unsupervised Representation Learning for Remote Sensing Image Classification","D. Lin; K. Fu; Y. Wang; G. Xu; X. Sun","Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Key Laboratory of Technology in Geospatial Information Processing and Application System, Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Geoscience and Remote Sensing Letters","25 Oct 2017","2017","14","11","2092","2096","With the development of deep learning, supervised learning has frequently been adopted to classify remotely sensed images using convolutional networks. However, due to the limited amount of labeled data available, supervised learning is often difficult to carry out. Therefore, we proposed an unsupervised model called multiple-layer feature-matching generative adversarial networks (MARTA GANs) to learn a representation using only unlabeled data. MARTA GANs consists of both a generative model G and a discriminative model D. We treat D as a feature extractor. To fit the complex properties of remote sensing data, we use a fusion layer to merge the mid-level and global features. G can produce numerous images that are similar to the training data; therefore, D can learn better representations of remotely sensed images using the training data provided by G. The classification results on two widely used remote sensing image databases show that the proposed method significantly improves the classification performance compared with other state-of-the-art methods.","1558-0571","","10.1109/LGRS.2017.2752750","National Natural Science Foundation of China(grant numbers:41501485,61331017); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8059820","Generative adversarial networks (GANs);scene classification;unsupervised representation learning","Generators;Gallium nitride;Training;Remote sensing;Feature extraction;Training data;Computational modeling","feature extraction;geophysical image processing;image classification;image fusion;image matching;image representation;remote sensing;unsupervised learning","global features;training data;unsupervised representation;remote sensing image classification;deep learning;supervised learning;convolutional networks;unsupervised model;multiple-layer feature-matching generative adversarial networks;unlabeled data;generative model G;discriminative model D;feature extractor;remote sensing data;remote sensing image databases;MARTA GAN;fusion layer","","131","","15","IEEE","5 Oct 2017","","","IEEE","IEEE Journals"
"Revisiting Modality-Specific Feature Compensation for Visible-Infrared Person Re-Identification","J. Liu; J. Wang; N. Huang; Q. Zhang; J. Han","Center for Complex Systems, School of Mechano-Electronic Engineering, Xidian University, Shaanxi, Xi’an, China; Center for Complex Systems, School of Mechano-Electronic Engineering, Xidian University, Shaanxi, Xi’an, China; Center for Complex Systems, School of Mechano-Electronic Engineering, Xidian University, Shaanxi, Xi’an, China; Center for Complex Systems, School of Mechano-Electronic Engineering, Xidian University, Shaanxi, Xi’an, China; Department of Computer Science, Aberystwyth University, Aberystwyth, U.K","IEEE Transactions on Circuits and Systems for Video Technology","4 Oct 2022","2022","32","10","7226","7240","Although modality-specific feature compensation becomes a prevailing paradigm for Visible-Infrared Person Re-Identification (VI-ReID) to learn features, it, performance-wise, is not promising, especially when compared to modality-shared feature learning. In this paper, by revisiting the modality-specific feature compensation based models, we reveal that the reasons for being under-performed are: (1) generated images of one modality from another modality may be poor in quality; (2) such existing models usually achieve the modality-specific feature compensation just via simple pixel-level fusion strategies; (3) generated images cannot fully replace corresponding missing ones, which brings in extra modality discrepancy. To address these issues, we propose a new Two-Stage Modality Enhancement Network (TSME) for VI-ReID. Concretely, it first considers the modality discrepancy for cross-modality style translation and optimizes the structures of image generators by involving a new Deeper Skip-connection Generative Adversarial Networks (DSGAN) to generate high-quality images. Then, it presents an attention mechanism based feature-level fusion module, i.e., Pair-wise Image Fusion (PwIF) module, and an auxiliary learning module, i.e., Invoking All-Images (IAI) module, to better exploit the generated and original images for reducing modality discrepancy from the perspectives of feature fusion and feature constraints, respectively. Comprehensive experiments are carried out to demonstrate the success of TSME in tackling the modality discrepancy issue exposed in VI-ReID.","1558-2205","","10.1109/TCSVT.2022.3168999","National Natural Science Foundation of China(grant numbers:61773301); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760399","Visible-infrared person re-identification;modality-specific feature compensation;high-quality generated images;feature fusion;constrains","Feature extraction;Generators;Image color analysis;Generative adversarial networks;Gray-scale;Representation learning;Semantics","feature extraction;image enhancement;image fusion;image recognition;inference mechanisms;learning (artificial intelligence)","pair-wise image fusion module;modality discrepancy issue;feature constraints;feature fusion;attention mechanism based feature-level fusion module;cross-modality style translation;Two-Stage Modality Enhancement Network;extra modality discrepancy;modality-specific feature compensation based models;modality-shared feature learning;VI-ReID;visible-infrared person re-identification","","2","","72","IEEE","20 Apr 2022","","","IEEE","IEEE Journals"
"Two Exposure Fusion Using Prior-Aware Generative Adversarial Network","J. -L. Yin; B. -H. Chen; Y. -T. Peng","College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; Department of Computer Science and Engineering, Yuan Ze University, Taoyuan, Taiwan; Department of Computer Science, National Chengchi University, Taipei, Taiwan","IEEE Transactions on Multimedia","9 Jun 2022","2022","24","","2841","2851","Producing a high dynamic range (HDR) image from two low dynamic range (LDR) images with extreme exposures is challenging due to the lack of well-exposed contents. Existing works either use pixel fusion based on weighted quantization or conduct feature fusion using deep learning techniques. In contrast to these methods, our core idea is to progressively incorporate the pixel domain knowledge of LDR images into the feature fusion process. Specifically, we propose a novel Prior-Aware Generative Adversarial Network (PA-GAN), along with a new dual-level loss for two exposure fusion. The proposed PA-GAN is composed of a content-prior-guided encoder and a detail-prior-guided decoder, respectively in charge of content fusion and detail calibration. We further train the network using a dual-level loss that combines the semantic-level loss and pixel-level loss. Extensive qualitative and quantitative evaluations on diverse image datasets demonstrate that our proposed PA-GAN has superior performance than state-of-the-art methods.","1941-0077","","10.1109/TMM.2021.3089324","Fujian Provincial Youth Education, and Scientific Research Project(grant numbers:JAT200055); Ministry of Science and Technology, Taiwan(grant numbers:MOST 108-2221-E-155-034-MY3,MOST 107-2221-E-155-052-MY2,MOST 109-2634-F-019-001,MOST 109-2634-F-004-001,MOST 109-2622-E-004-002,MOST 109-2221-E-004-014,MOST 110-2221-E-004-010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9456042","High dynamic range image;exposure fusion;deep learning","Semantics;Decoding;Generative adversarial networks;Dynamic range;Quantization (signal);Calibration;Image fusion","deep learning (artificial intelligence);feature extraction;image fusion;image segmentation","exposure fusion;high dynamic range image;low dynamic range images;extreme exposures;well-exposed contents;pixel fusion;weighted quantization;deep learning techniques;pixel domain knowledge;LDR images;feature fusion process;PA-GAN;dual-level loss;content-prior-guided encoder;content fusion;semantic-level loss;pixel-level loss;diverse image datasets;prior-aware generative adversarial network","","4","","44","IEEE","15 Jun 2021","","","IEEE","IEEE Journals"
"Fusion of Hyperspectral and Panchromatic Images Using Generative Adversarial Network and Image Segmentation","W. Dong; Y. Yang; J. Qu; W. Xie; Y. Li","State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Network, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","29 Dec 2021","2022","60","","1","13","Hyperspectral (HS) image fusion aims at integrating a panchromatic (PAN) image and an HS image, featuring the fused image with the spatial quality of the former and the spectral diversity of the latter. The classic fusion algorithm generally includes three consecutive procedures that are upsampling, detail extraction, and detail injection. In this article, we propose an HS and PAN image fusion method based on generative adversarial network and local estimation of injection gain. Instead of upsampling the HS image by classical interpolation techniques, a generative adversarial super-resolution network (GASN) is designed to obtain the interpolated HS image in the fusion framework. GASN establishes a spectral-information-based discriminator to conduct adversarial learning with the generator, so as to preserve the spectral information of the low-resolution HS image. An image segmentation-based injection gain estimation (ISGE) algorithm is subsequently proposed for HS and PAN images fusion. The injection gain is estimated over image segments obtained by a binary partition tree approach to improve the fusion performance. The proposed GASN and ISGE are implemented into two credible global estimation pansharpening methods, and experimental results prove the performance improvement of the proposed method. The proposed method is also compared with existing state-of-the-art methods, and experiments on several public databases demonstrate that the proposed method is competitive or superior to the state-of-the-art fusion methods.","1558-0644","","10.1109/TGRS.2021.3078711","National Defense Pre-Research Foundation; Higher Education Discipline Innovation Project(grant numbers:B08038); National Natural Science Foundation of China(grant numbers:61801359,61571345,91538101,61501346,61502367,61701360); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2021JQ-194,2021JQ-197); Fundamental Research Funds for the Central Universities(grant numbers:XJS210108,XJS210104); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515110856); Yangtze River Scholar Bonus Schemes of China(grant numbers:CJT160102); Ten Thousand Talent Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9437429","Details injection;hyperspectral (HS) fusion;image segment;injection gains","Estimation;Spatial resolution;Pansharpening;Image segmentation;Generators;Generative adversarial networks;Feature extraction","geophysical image processing;image fusion;image representation;image resolution;image sampling;image segmentation;interpolation;learning (artificial intelligence);neural nets;spectral analysis","spectral-information-based discriminator;adversarial learning;low-resolution HS image;image segmentation-based injection gain estimation algorithm;GASN;panchromatic image;hyperspectral image fusion;interpolation techniques;generative adversarial super-resolution network;interpolated HS image;HS image upsampling;PAN images fusion;global estimation pansharpening methods","","6","","43","IEEE","20 May 2021","","","IEEE","IEEE Journals"
"DFGAN: Image Deblurring through Fusing Light-Weight Attention and Gradient-Based Filters","A. S. Saqlain; F. Fang; L. -Y. Wang; T. Ahmad; Z. U. Abidin","School of Control and Computer Engineering, North China Electric Power University, Beijing, China; School of Control and Computer Engineering, North China Electric Power University, Beijing, China; School of Computer Engineering, Portland State University, Portland, USA; School of Information and Control Engineering, Xian University of Architecture and Technology, Xian, China; School of Information Science and Technology, Southwest Jiaotong University, Chengdu, China","2022 IEEE 2nd International Conference on Information Communication and Software Engineering (ICICSE)","18 Jul 2022","2022","","","110","114","Recovering a latent sharp image from a spatially variant blurred image is a challenging task in the field of computer vision especially in blind image deblurring, where the source of the blur kernel is unknown and may vary. To remove the intricate motion blur in the images, recently deep learning-based methods perform latent clean image recovery without the need of knowing the blur kernel explicitly. Unlike conventional blind deblurring methods that assume the blur to be spatially invariant across the image. However, simply stacking convolution layers in deep multi-scale networks does not guarantee the complete removal of motion blur in the images and may lead to a poor performance for blind image deblurring task. Thus, we propose a GAN-based approach for single image blind motion deblurring task in an end-to-end manner, for simplicity its called DeblurFusedGAN (DFGAN). The proposed method improves the performance for image deblurring task by fusing the light-weight attention (LSA) mechanism and gradient-based filters in the generator network. Furthermore, we show the sophisticated performance of our proposed approach both qualitatively and quantitatively in comparison with the other state-of-the-art methods.","","978-1-6654-8220-2","10.1109/ICICSE55337.2022.9829002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9829002","Blind image deblurring;generative adversarial networks (GANs);image restoration;deep learning","Learning systems;Convolution;Conferences;Stacking;Information filters;Generators;Image restoration","computer vision;deep learning (artificial intelligence);image filtering;image fusion;image motion analysis;image restoration","spatially variant blurred image;computer vision;blur kernel;deep learning;latent clean image recovery;deep multiscale networks;GAN-based approach;single image blind motion deblurring task;DFGAN;light-weight attention mechanism;gradient-based filters;latent sharp image;light-weight attention fusion;image deblurring;intricate motion blur removal;DeblurFusedGAN","","1","","23","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"Phase Retrieval Based on Enhanced Generator Conditional Generative Adversarial Network","S. Pu; L. Li; Y. Xiang; X. Qiu","School of Science, Xi'an Shiyou University, Xian, China; School of Science, Xi'an Shiyou University, Xian, China; School of Science, Xi'an Shiyou University, Xian, China; School of Science, Xi'an Shiyou University, Xian, China","2022 4th International Conference on Intelligent Control, Measurement and Signal Processing (ICMSP)","25 Aug 2022","2022","","","825","829","Phase retrieval refers to the recovery of the original image using only the Fourier amplitude of the image. Due to the small amount of information contained in the Fourier amplitude, the common network structure cannot achieve accurate reconstruction of the image when the oversampling rate of the image is low. It is the key issue of phase retrieval to improve the structure of the neural network. We propose an application of end-to-end adversarial network to solve phase retrieval problems by adding a U-Net model to the conditional generative adversarial network(U-NetCGAN). This desired approach realizes multi-scale recognition and fusion of image features and improves the quality of image reconstruction. The experimental results show that the model is significantly better than the traditional phase retrieval algorithm. Compared to other algorithms, the evaluation indicators of PSNR and SSIM values in our approach have increased about 6 dB and 0.1, respectively.","","978-1-6654-8658-3","10.1109/ICMSP55950.2022.9858954","Xi'an Shiyou University(grant numbers:YCS21112082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858954","phase retrieval;Conditional generative Adversarial Networks;U-Net","Visualization;Image recognition;Phase measurement;Inverse problems;Superresolution;Neural networks;Signal processing algorithms","feature extraction;Fourier transform optics;image fusion;image reconstruction;image retrieval;image sampling;iterative methods;neural nets;optical information processing","enhanced generator conditional generative adversarial network;Fourier amplitude;common network structure;neural network;end-to-end adversarial network;image reconstruction;traditional phase retrieval algorithm;original image recovery;image oversampling rate;U-NetCGAN;image feature multiscale recognition;image feature fusion;PSNR;SSIM values","","","","15","IEEE","25 Aug 2022","","","IEEE","IEEE Conferences"
"A Two-Layers Super-Resolution Based Generation Adversarial Spatiotemporal Fusion Model","S. Fang; Q. Guo; Y. Cao; J. Zhang","Anhui Province Key Laboratory of Industry Safety and Emergency Technology, Hefei, China; Key Laboratory of Knowledge Engineering with Big Data, Hefei University of Technology, Ministry of Education, Hefei, China; University of Science and Technology of China, Hefei, Anhui, China; Key Laboratory of Knowledge Engineering with Big Data, Hefei University of Technology, Ministry of Education, Hefei, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","891","894","Remote sensing image spatiotemporal fusion (STF) algorism plays an important role by supplementing the lack of original high-resolution remote sensing satellite images in the study scenarios of dense time-series data. In recent years, the deep-learning-based STF algorithm has become a research hotspot with comparatively higher accuracy and robustness. However, due to the lack of sufficient high-quality images for training and the huge resolution gap between low-resolution images and high-resolution images, it is difficult to recover detailed information, especially for areas of land-cover change. In this paper, we propose a two-layers super-resolution based generation adversarial spatiotemporal fusion model(TLSRSTF) using smaller inputs to reduce pressure on data requirements and a mutual affine convolution to reduce model parameters. Specifically, we only use a pair of high-resolution and low-resolution images and a high-resolution image at any time. A spatial degradation consistency is constructed to adaptively determine the ratio of two layers of the super-resolution STF model. The quantitative and qualitative experimental results on public spatiotemporal fusion datasets demonstrate our superiority over the state-of-the-art methods.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883547","National Natural Science Foundation of China(grant numbers:61872327,61175033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883547","Spatiotemporal Fusion;Generative Adversarial Networks(GAN);Mutual Affine Convolution","Training;Adaptation models;Satellites;Convolution;Superresolution;Data models;Robustness","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);remote sensing;spatiotemporal phenomena","sensing image spatiotemporal fusion algorism;high-resolution remote sensing satellite images;dense time-series data;STF algorithm;comparatively higher accuracy;robustness;high-quality images;huge resolution gap;low-resolution images;high-resolution image;super-resolution STF model;public spatiotemporal fusion datasets","","","","9","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Text-Guided Image Manipulation Based on Sentence-Aware and Word-Aware Network","Z. Zhang; C. Fu; M. M. Ho; J. Zhou; N. Jiang; W. Yu","Hosei University, Tokyo, Japan; Hosei University, Tokyo, Japan; Hosei University, Tokyo, Japan; Hosei University, Tokyo, Japan; Southwest University of Science and Technology, Mianyang, China; Southwest University of Science and Technology, Mianyang, China","2022 IEEE International Conference on Multimedia and Expo (ICME)","26 Aug 2022","2022","","","1","6","Text-guided image manipulation aims to use the given text description to modify the semantic content of the corresponding part in the input image. Although researchers have been obtained satisfactory performance in this field, they only 1) utilize the global sentence information at the initial modification stage and 2) exploit the fixed word information for regional adjustment in the subsequent modification process, hindering the improvement of image manipulation quality. Motivated by the mentioned issues, this paper proposes a novel approach to improve the performance of text-guided image manipulation by using sentence-aware and word-aware network. Concretely, we utilize global sentence information throughout the image manipulation process to improve the semantic consistency with the input text. On the other hand, we employ the dynamic selection method to dynamically adjust the word information corresponding to the regional image content to further improve the manipulation quality. As a result, our work surpasses the existing state-of-the-art methods on CUB and Oxford-102 flower datasets, demonstrating our effectiveness and superiority. In terms of Inception Score, our proposed method performs the most excellent performance. In terms of NIMA, the score of our method is closest to the score of the original dataset images, proving that our manipulated results are the most authentic.","1945-788X","978-1-6654-8563-0","10.1109/ICME52920.2022.9859585","Hosei University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859585","Text-guided Image Manipulation;Sentence-ware and Word-aware Network;Generative Adversarial Networks;Computer Vision","Semantics;Manipulator dynamics","image classification;image fusion;image segmentation;natural language processing;text analysis;visual databases","text-guided image manipulation;sentence-aware;word-aware network;given text description;input image;global sentence information;fixed word information;image manipulation quality;image manipulation process;input text;regional image content;original dataset images","","","","31","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"On the possibility of conditional adversarial networks for multi-sensor image matching","N. Merkle; P. Fischer; S. Auer; R. Müller","German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2633","2636","A major research area in remote sensing is the problem of multi-sensor data fusion. Especially the combination of images acquired by different sensor types, e.g. active and passive, is a difficult task. Over the last years deep learning methods have proven their high potential for remote sensing applications. In this paper we will show how a deep learning method can be valuable for the problem of optical and SAR image matching. We investigate the possible of conditional generative adversarial networks (cGANs) for the generation of artificial templates. Contrary to common template generation approaches for image matching, the generation of templates using cGANs does not require the extraction of features. Our results show the possibility of realistic SAR-like template generation from optical images through cGANs and the potential of these templates for enhancing the matching of optical and SAR images by means of reliability and accuracy.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127535","conditional GANs;deep learning;image matching;multi-sensor;artificial template generation","Optical imaging;Adaptive optics;Optical sensors;Feature extraction;Synthetic aperture radar;Image matching;Gallium nitride","feature extraction;image fusion;image matching;learning (artificial intelligence);radar imaging;remote sensing;sensor fusion;synthetic aperture radar","artificial templates;optical images;multisensor image matching;research area;multisensor data fusion;remote sensing applications;deep learning method;optical SAR image matching;conditional generative adversarial networks;template generation;cGAN","","21","","10","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Localization in Aerial Imagery with Grid Maps using LocGAN","H. Hu; J. Zhu; S. Wirges; M. Lauer","Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany; FZI Research Center for Information Technology, Karlsruhe, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Karlsruhe, Germany","2019 IEEE Intelligent Transportation Systems Conference (ITSC)","28 Nov 2019","2019","","","2860","2865","In this work, we present LocGAN, our localization approach based on a geo-referenced aerial imagery and LiDAR grid maps. Currently, most self-localization approaches relate the current sensor observations to a map generated from previously acquired data. Unfortunately, this data is not always available and the generated maps are usually sensor setup specific. Global Navigation Satellite Systems (GNSS) can overcome this problem. However, they are not always reliable especially in urban areas due to multi-path and shadowing effects. Since aerial imagery is usually available, we can use it as prior information. To match aerial images with grid maps, we use conditional Generative Adversarial Networks (cGANs) which transform aerial images to the grid map domain. The transformation between the predicted and measured grid map is estimated using a localization network (LocNet). Given the geo-referenced aerial image transformation the vehicle pose can be estimated. Evaluations performed on the data recorded in region Karlsruhe, Germany show that our LocGAN approach provides reliable global localization results.","","978-1-5386-7024-8","10.1109/ITSC.2019.8917236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917236","","Generators;Global navigation satellite system;Principal component analysis;Training;Laser radar;Reliability;Transforms","distance measurement;geophysical image processing;image fusion;mobile robots;optical radar;remote sensing;remotely operated vehicles","self-localization approaches;current sensor observations;generated maps;Global Navigation Satellite Systems;shadowing effects;grid map domain;conditional generative adversarial networks;predicted measured grid map;localization network;geo-referenced aerial image transformation;LocGAN approach;reliable global localization results;localization approach;geo-referenced aerial imagery;LiDAR grid maps","","1","","22","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Coal Gangue Data Set Expansion Method Based on DCGAN","J. Xie; X. Cai; H. Yang; Q. Meng; B. Li; Y. Yan; L. Zhao","School of Communication, Xi'an University of Science and Technology, Xi'an, China; Shenma Industrial Co., Ltd., Cord Fabric Company, Pingdingshan, China; The Sixth Mine of Pingdingshan Tianan Coal Industry Co., Ltd., Pingdingshan, China; The Sixth Mine of Pingdingshan Tianan Coal Industry Co., Ltd., Pingdingshan, China; The Sixth Mine of Pingdingshan Tianan Coal Industry Co., Ltd., Pingdingshan, China; The Sixth Mine of Pingdingshan Tianan Coal Industry Co., Ltd., Pingdingshan, China; The Sixth Mine of Pingdingshan Tianan Coal Industry Co., Ltd., Pingdingshan, China","2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)","21 Feb 2022","2022","","","303","308","Coal gangue separation is one of the important links in the process of coal mining. High gangue content will lead to low combustion efficiency of power plants and serious environmental pollution. Through field investigation and investigation, it is found that at present, most mines still use manual gangue separation. The dust in the coal preparation bunker is large and the light is dark. Long-term and high load work will not only harm the health of workers, It will also affect the sorting efficiency. Based on this, this paper proposes a deep learning target detection algorithm to replace manual gangue selection, but the target detection algorithm depends on a large number of coal and gangue images for training to achieve better robustness. In order to overcome the problem of small number of data sets and poor quality in the automatic sorting of coal gangue based on deep learning methods, the paper proposes an image expansion method based on Deep Convolutional Generative Adversarial Networks(DCGAN),which adds a feature fusion part to the generator network It is used to improve the diversity of the output image, and at the same time, the loss function is improved to Wasserstein distance to overcome the problem of the disappearance of the generator gradient. The experimental results show that the images generated by the improved DCGAN network have good performance in diversity and quality. The images generated by the improved DCGAN network increase the accuracy of coal gangue recognition by 5.87%.","","978-1-6654-0886-8","10.1109/ICCECE54139.2022.9712795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712795","DCGAN;Image Expansion;Coal Gangue Image;Object detection","Deep learning;Training;Image recognition;Target recognition;Coal;Object detection;Manuals","coal;combustion;convolutional neural nets;deep learning (artificial intelligence);dust;image fusion;mining;mining industry;object detection;occupational health;personnel;power plants;separation;sorting","coal gangue separation;coal mining process;low combustion efficiency;power plants;serious environmental pollution;manual gangue separation;coal preparation bunker;deep learning target detection algorithm;gangue images;automatic sorting;image expansion method;improved DCGAN network;coal gangue data set expansion method;dust;workers health;deep convolutional generative adversarial networks;Wasserstein distance;feature fusion part","","","","7","IEEE","21 Feb 2022","","","IEEE","IEEE Conferences"
"Deep Learning Based Fusion of RGB and Infrared Images for the Detection of Abnormal Condition of Fused Magnesium Furnace","S. Lu; H. Gao","State Key Laboratory of Synthetical Automation for Process Industries, Northeastern University, Shenyang, China; School of Software Engineering, Huazhong University of Science and Technology, Wuhan, China","2019 IEEE 15th International Conference on Control and Automation (ICCA)","14 Nov 2019","2019","","","987","993","In fused magnesium furnace (FMF) process, semi-molten is one of the most harmful abnormal working conditions, under which the furnace wall is thinned by the overheated fused magnesium because of the uneven impurities in raw material. If the condition is not detected on its early stage, the furnace can be burnt through. At present, semi-molten is detected by experienced operators by directly “observing the fire” at the production site of FMF. There is a high risk and the labor intensity is high. Such practice of detection relied on human may cause safety issues and can lead to missed or false detection. This work introduces a detection technology for the semi-molten working condition of FMF based on the fusion of RGB images and infrared thermal images. The classifier is established using deep Convolutional Neural Network (CNN) model trained using historical data. Also, to tackle the problem of insufficient training data, the Deep Convolutional Generative Adversarial Networks (DCGAN) is employed to generate extra samples. Finally, industrial experiments carried out in a magnesium oxide plant to show the effectiveness of the technology.","1948-3457","978-1-7281-1164-3","10.1109/ICCA.2019.8899693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899693","","Furnaces;Employee welfare;Smelting;Training;Magnesium;Fires","convolutional neural nets;feature extraction;furnaces;image colour analysis;image fusion;industrial plants;infrared imaging;learning (artificial intelligence);occupational safety;production engineering computing;raw materials;risk analysis","deep learning;fused magnesium furnace process;furnace wall;overheated fused magnesium;raw material;RGB image fusion;deep convolutional generative adversarial networks;semimolten condition detection;deep convolutional neural network model;FMF production;safety issues;infrared thermal image fusion;classifier;CNN model training","","3","","20","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Pseudo RGB-D Face Recognition","B. Jin; L. Cruz; N. Gonçalves","Department of Electrical and Computer Engineering, Institute of Systems and Robotics, University of Coimbra, Coimbra, Portugal; Align Technology Inc., San Jose, CA, USA; Portuguese Mint and Official Printing Office, Lisbon, Portugal","IEEE Sensors Journal","11 Nov 2022","2022","22","22","21780","21794","In the last decade, advances and popularity of low-cost RGB-D sensors have enabled us to acquire depth information of objects. Consequently, researchers began to solve face recognition problems by capturing RGB-D face images using these sensors. Until now, it is not easy to acquire the depth of human faces because of limitations imposed by privacy policies, and RGB face images are still more common. Therefore, obtaining the depth map directly from the corresponding RGB image could be helpful to improve the performance of subsequent face processing tasks, such as face recognition. Intelligent creatures can use a large amount of experience to obtain 3D spatial information only from 2D plane scenes. It is machine learning methodology, which is to solve such problems, that can teach computers to generate correct answers by training. To replace the depth sensors by generated pseudo-depth maps, in this article, we propose a pseudo RGB-D face recognition framework and provide data-driven ways to generate the depth maps from 2D face images. Specially we design and implement a generative adversarial network model named “D+GAN” to perform the multiconditional image-to-image translation with face attributes. By this means, we validate the pseudo RGB-D face recognition with experiments on various datasets. With the cooperation of image fusion technologies, especially non-subsampled shearlet transform (NSST), the accuracy of face recognition has been significantly improved.","1558-1748","","10.1109/JSEN.2022.3197235","Fundação para a Ciência e a Tecnologia (FCT)(grant numbers:UIDB/00048/2020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857779","Depth plus generative adversarial network (D+GAN);face recognition;monocular face depth estimation;pseudo-depth;RGB-D","Face recognition;Sensors;Three-dimensional displays;Generative adversarial networks;Feature extraction;Faces;Estimation","face recognition;image colour analysis;image fusion;image sensors;learning (artificial intelligence);neural nets;transforms","2D face images;2D plane scenes;3D spatial information;depth map;depth sensors;face attributes;face processing tasks;face recognition problems;generated pseudodepth maps;generative adversarial network;human faces;image fusion technologies;image-to-image translation;low-cost RGB-D;low-cost RGB-D sensors;machine learning methodology;nonsubsampled shearlet transform;privacy policies;pseudoRGB-D face recognition;RGB-D face images","","22","","50","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"Super-Resolution Reconstruction Method of Remote Sensing Image Based on Multi-Feature Fusion","Z. -X. Huang; C. -W. Jing","Zhejiang Yijia Geographic Information Technology Company, Ltd., Hangzhou, China; Hangzhou Normal University, Hangzhou, China","IEEE Access","30 Jan 2020","2020","8","","18764","18771","The acquisition of remote sensing images is affected by imaging equipment and environmental conditions. Usually on lower performance devices, the resolution of the acquired images is also low. Among many methods, the super-resolution reconstruction method based on generative adversarial networks has obvious advantages over previous network models in reconstructing image texture details. However, it is found in experiments that not all of these reconstructed textures exist in the image itself. Aiming at the problem of whether the texture details of the reconstructed image are accurate and clear, we propose a super-resolution reconstruction method combining wavelet transform and generative adversarial network. Using wavelet multi-resolution analysis, training wavelet decomposition coefficients in the generative adversarial network can effectively improve the local detail information of the reconstructed image. Experimental results show that our method can effectively reconstruct more natural image textures and make the images more visually clear. In the remote sensing image test set, the four indicators of the algorithm, peak signal to noise ratio (PSNR), structural similarity (SSIM), Feature Similarity (FSIM) and Universal Image Quality (UIQ) are slightly better than the algorithms mentioned in the article.","2169-3536","","10.1109/ACCESS.2020.2967804","Key Special Projects through the Provincial Scientific Research Institutes Program of Zhejiang Province, China(grant numbers:2014F50022); Construction of Agricultural Science Park Program of Zhejiang Province, China(grant numbers:2019E70002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8963714","Remote sensing image;super-resolution;self-correlation;image texture","Image reconstruction;Wavelet transforms;Reconstruction algorithms;Remote sensing;Generative adversarial networks","image fusion;image reconstruction;image resolution;image texture;neural nets;remote sensing;wavelet transforms","wavelet multiresolution analysis;generative adversarial network;reconstructed image;natural image textures;remote sensing image test;super-resolution reconstruction method;remote sensing images;imaging equipment;image texture details;reconstructed textures;universal image quality;multi-feature fusion;environmental conditions;wavelet transform;training wavelet decomposition coefficients;peak signal to noise ratio;structural similarity;feature similarity","","11","","27","CCBY","20 Jan 2020","","","IEEE","IEEE Journals"
"PSGAN: A Generative Adversarial Network for Remote Sensing Image Pan-Sharpening","Q. Liu; H. Zhou; Q. Xu; X. Liu; Y. Wang","Hangzhou Innovation Institute, Beihang University, Hangzhou, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; School of the Computer Science and Engineering, Beihang University, Beijing, China; Hangzhou Innovation Institute, Beihang University, Hangzhou, China","IEEE Transactions on Geoscience and Remote Sensing","23 Nov 2021","2021","59","12","10227","10242","This article addresses the problem of remote sensing image pan-sharpening from the perspective of generative adversarial learning. We propose a novel deep neural network-based method named pansharpening GAN (PSGAN). To the best of our knowledge, this is one of the first attempts at producing high-quality pan-sharpened images with generative adversarial networks (GANs). The PSGAN consists of two components: a generative network (i.e., generator) and a discriminative network (i.e., discriminator). The generator is designed to accept panchromatic (PAN) and multispectral (MS) images as inputs and maps them to the desired high-resolution (HR) MS images, and the discriminator implements the adversarial training strategy for generating higher fidelity pan-sharpened images. In this article, we evaluate several architectures and designs, namely, two-stream input, stacking input, batch normalization layer, and attention mechanism to find the optimal solution for pan-sharpening. Extensive experiments on QuickBird, GaoFen-2, and WorldView-2 satellite images demonstrate that the proposed PSGANs not only are effective in generating high-quality HR MS images and superior to state-of-the-art methods but also generalize well to full-scale images.","1558-0644","","10.1109/TGRS.2020.3042974","NSFC(grant numbers:41871283,61601011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306912","Convolutional neural network (CNN);deep learning;generative adversarial network (GAN);pan-sharpening;residual learning","Generative adversarial networks;Generators;Neural networks;Computer architecture;Training;Spatial resolution;Data models","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","PSGAN;generative adversarial network;remote sensing image pan-sharpening;generative adversarial learning;deep neural network-based method named pansharpening GAN;high-quality pan-sharpened images;generative network;discriminative network;MS images;adversarial training strategy;higher fidelity pan-sharpened images;WorldView-2 satellite images;full-scale images","","49","","77","IEEE","24 Dec 2020","","","IEEE","IEEE Journals"
"Rethinking CNN-Based Pansharpening: Guided Colorization of Panchromatic Images via GANs","F. Ozcelik; U. Alganci; E. Sertel; G. Unal","Artificial Intelligence and Data Science Research and Application Center, Istanbul Technical University (ITU), Istanbul, Turkey; Research Center for Satellite Communications and Remote Sensing (CSCRS), Istanbul Technical University (ITU), Istanbul, Turkey; Research Center for Satellite Communications and Remote Sensing (CSCRS), Istanbul Technical University (ITU), Istanbul, Turkey; Artificial Intelligence and Data Science Research and Application Center, Istanbul Technical University (ITU), Istanbul, Turkey","IEEE Transactions on Geoscience and Remote Sensing","24 Mar 2021","2021","59","4","3486","3501","Convolutional neural network (CNN)-based approaches have shown promising results in the pansharpening of the satellite images in recent years. However, they still exhibit limitations in producing high-quality pansharpening outputs. To that end, we propose a new self-supervised learning framework, where we treat pansharpening as a colorization problem, which brings an entirely novel perspective and solution to the problem compared with the existing methods that base their solution solely on producing a super-resolution version of the multispectral image. Whereas the CNN-based methods provide a reduced-resolution panchromatic image as the input to their model along with the reduced-resolution multispectral images and, hence, learn to increase their resolution together, we instead provide the grayscale transformed multispectral image as the input and train our model to learn the colorization of the grayscale input. We further address the fixed downscale ratio assumption during training, which does not generalize well to the full-resolution scenario. We introduce a noise injection into the training by randomly varying the downsampling ratios. Those two critical changes, along with the addition of adversarial training in the proposed PanColorization generative adversarial network (PanColorGAN) framework, help overcome the spatial-detail loss and blur problems that are observed in CNN-based pansharpening. The proposed approach outperforms the previous CNN-based and traditional methods, as demonstrated in our experiments.","1558-0644","","10.1109/TGRS.2020.3010441","Research Fund of the Istanbul Technical University Project(grant numbers:MGA-2017-40811); Turkcell-ITU Researcher Funding Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153037","AI;colorization;convolutional neural networks (CNNs);deep learning;generative adversarial networks (GANs);image fusion;PanColorization generative adversarial network (PanColorGAN);pansharpening;self-supervised learning;super-resolution (SR)","Task analysis;Spatial resolution;Training;Standards;Sensors;Multiresolution analysis","convolutional neural nets;geophysical image processing;image colour analysis;image resolution;supervised learning","adversarial training;grayscale transformed multispectral image;blur problems;PanColorGAN;panchromatic image resolution;PanColorization generative adversarial network;CNN based pansharpening;panchromatic image colorization;self supervised learning;satellite images;convolutional neural network;multispectral image resolution","","37","","44","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"Text to Image Generation with Semantic-Spatial Aware GAN","W. Liao; K. Hu; M. Y. Yang; B. Rosenhahn","TNT, Leibniz University, Hannover, Germany; TNT, Leibniz University, Hannover, Germany; SUG, University of Twente, The Netherlands; TNT, Leibniz University, Hannover, Germany","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","18166","18175","Text-to-image synthesis (T2I) aims to generate photorealistic images which are semantically consistent with the text descriptions. Existing methods are usually built upon conditional generative adversarial networks (GANs) and initialize an image from noise with sentence embedding, and then refine the features with fine-grained word embedding iteratively. A close inspection of their generated images reveals a major limitation: even though the generated image holistically matches the description, individual image regions or parts of somethings are often not recognizable or consistent with words in the sentence, e.g. “a white crown”. To address this problem, we propose a novel framework Semantic-Spatial Aware GAN for synthesizing images from input text. Concretely, we introduce a simple and effective Semantic-Spatial Aware block, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a semantic mask in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code available at https://github.com/wtliao/text2image.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01765","Federal Ministry of Education and Research(grant numbers:01DD20003); Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879138","Image and video synthesis and generation; Vision + language","Visualization;Computer vision;Image recognition;Image synthesis;Fuses;Computational modeling;Semantics","feature extraction;image classification;image colour analysis;image fusion;image processing;image representation;image segmentation;learning (artificial intelligence);text analysis","input text description;image generation;text-to-image synthesis;photorealistic images;text descriptions;conditional generative adversarial networks;GANs;sentence embedding;fine-grained word;individual image regions;novel framework Semantic-Spatial Aware GAN;Semantic-Spatial Aware block;semantic-adaptive transformation;text features;image features;semantic mask;current text-image fusion process","","","","39","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Cross-Domain Person Re-Identification Based on Feature Fusion","X. Luo; Z. Ouyang; N. Du; J. Song; Q. Wei","College of Computer Science and Technology, Guizhou University, Guiyang, China; Guizhou Provincial Key Laboratory of Public Big Data, Guiyang, China; College of Computer Science and Technology, Guizhou University, Guiyang, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Guizhou Provincial Key Laboratory of Public Big Data, Guiyang, China","IEEE Access","15 Jul 2021","2021","9","","98327","98336","Person re-identification (ReID) is one of the commonly used criminal investigation methods in reconnaissance. Although the current ReID has achieved robust results on single domains, the focus of researches has shifted to cross-domain in recent years, which is caused by domain bias between different datasets. Generative Adversarial Networks (GAN) is used to realize the image style transfer of different datasets to alleviate the effect of cross-domain. However, the existing GAN-based models ignore complete expressions and occlusion of pedestrian characteristics, resulting in low accuracy in feature extraction. To address these issues, we introduce a cross domain model based on feature fusion (FFGAN) to fuse global, local and semantic features to extract more delicate pedestrian features. Before extracting pedestrian features, we preprocess feature maps with a feature erasure block to solve an occlusion issue. Finally, FFGAN enables a more complete visual description of pedestrian characteristics, thereby improving the accuracy of FFGAN in identifying pedestrians. Experimental results show that the effect of FFGAN is significantly improved compared with some advanced cross-domain ReID algorithms.","2169-3536","","10.1109/ACCESS.2021.3091647","Major Scientific and Technological Special Project of Guizhou Province China(grant numbers:20183002); Cultivation Project of Guizhou University [Qian-kehe Platform Talents (2017)](grant numbers:5788); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462145","Person re-identification;global feature;local features;semantic features;feature erasure;cross domain","Feature extraction;Semantics;Cameras;Image segmentation;Generative adversarial networks;Adaptation models;Training","feature extraction;image fusion;image matching;object detection;pedestrians","FFGAN;advanced cross-domain ReID algorithms;Cross-Domain Person Re-Identification;feature fusion;commonly used criminal investigation methods;current ReID;robust results;single domains;domain bias;existing GAN-based models;pedestrian characteristics;feature extraction;cross domain model;global features;local features;semantic features;delicate pedestrian features;feature erasure block","","","","49","CCBYNCND","22 Jun 2021","","","IEEE","IEEE Journals"
"Controlled Multi-modal Image Generation for Plant Growth Modeling","M. Miranda; L. Drees; R. Roscher","German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","5118","5124","Predicting plant development is an important task in precision farming and an essential metric for decision-making by researchers and farmers. In this work, we propose a novel generative modeling technique for plant growth prediction based on conditional generative adversarial networks. We formulate plant growth as an image-to-image translation task and predict the appearance of a plant growth stage as a function of its previous stage. We take into account that plant growth is inherently multi-modal, depending on numerous and highly variable environmental factors, and thus a single input belongs to a distribution of potential outputs. We encode the ambiguity in an interpretable and low-dimensional latent vector space representing the various factors of variation that are influencing plant growth. We use a novel encoder-based data fusion technique and combine information contained in remote sensing imagery of different cropping systems with data containing the factors of variation to adequately model plant growth. This offers several advantages over existing methods: (1) we show that we can model a distribution of potential appearances and simultaneously outperform existing methods in providing more realistic predictions, (2) the complexity of plant growth is more adequately captured, as various factors influencing plant growth can be included, (3) predictions are controllable by being conditioned by an interpretable latent vector representing the factors of variation along with an input image of a previous growth stage.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956115","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956115","","Measurement;Image synthesis;Ecosystems;Predictive models;Generative adversarial networks;Pattern recognition;Task analysis","agriculture;crops;encoding;environmental factors;geophysical image processing;image fusion;regression analysis;remote sensing;sensor fusion;vectors","controlled multimodal image generation;image-to-image translation task;plant growth modeling;plant growth prediction;plant growth stage;predicting plant development;previous growth stage","","","","36","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"3D Auto-Context-Based Locality Adaptive Multi-Modality GANs for PET Synthesis","Y. Wang; L. Zhou; B. Yu; L. Wang; C. Zu; D. S. Lalush; W. Lin; X. Wu; J. Zhou; D. Shen","School of Computer Science, Sichuan University, Chengdu, China; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; North Carolina State University, Raleigh, NC, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; IDEA Lab, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Medical Imaging","31 May 2019","2019","38","6","1328","1339","Positron emission tomography (PET) has been substantially used recently. To minimize the potential health risk caused by the tracer radiation inherent to PET scans, it is of great interest to synthesize the high-quality PET image from the low-dose one to reduce the radiation exposure. In this paper, we propose a 3D auto-context-based locality adaptive multi-modality generative adversarial networks model (LA-GANs) to synthesize the high-quality FDG PET image from the low-dose one with the accompanying MRI images that provide anatomical information. Our work has four contributions. First, different from the traditional methods that treat each image modality as an input channel and apply the same kernel to convolve the whole image, we argue that the contributions of different modalities could vary at different image locations, and therefore a unified kernel for a whole image is not optimal. To address this issue, we propose a locality adaptive strategy for multimodality fusion. Second, we utilize 1 × 1 × 1 kernel to learn this locality adaptive fusion so that the number of additional parameters incurred by our method is kept minimum. Third, the proposed locality adaptive fusion mechanism is learned jointly with the PET image synthesis in a 3D conditional GANs model, which generates high-quality PET images by employing large-sized image patches and hierarchical features. Fourth, we apply the auto-context strategy to our scheme and propose an auto-context LA-GANs model to further refine the quality of synthesized images. Experimental results show that our method outperforms the traditional multi-modality fusion methods used in deep networks, as well as the state-of-the-art PET estimation approaches.","1558-254X","","10.1109/TMI.2018.2884053","National Natural Science Foundation of China(grant numbers:61701324); Australian Research Council(grant numbers:DE160100241); NIH(grant numbers:EB006733); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552676","Image synthesis;positron emission topography (PET);generative adversarial networks (GANs);locality adaptive fusion;multi-modality","Positron emission tomography;Generators;Three-dimensional displays;Kernel;Gallium nitride;Image generation;Magnetic resonance imaging","image fusion;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography","LA-GANs model;synthesized images;traditional multimodality fusion methods;state-of-the-art PET estimation approaches;auto-context-based;PET synthesis;positron emission tomography;PET scans;high-quality PET image;high-quality FDG PET image;accompanying MRI images;image modality;locality adaptive strategy;locality adaptive fusion mechanism;PET image synthesis;3D conditional GANs model;large-sized image patches;auto-context strategy;image locations","Brain;Databases, Factual;Deep Learning;Humans;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Phantoms, Imaging;Positron-Emission Tomography;Radiation Dosage","85","","48","IEEE","29 Nov 2018","","","IEEE","IEEE Journals"
"Residual Encoder–Decoder Conditional Generative Adversarial Network for Pansharpening","Z. Shao; Z. Lu; M. Ran; L. Fang; J. Zhou; Y. Zhang","College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Computer Science, Sichuan University, Chengdu, China; College of Computer Science, Sichuan University, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","27 Aug 2020","2020","17","9","1573","1577","Due to the limitation of the satellite sensor, it is difficult to acquire a high-resolution (HR) multispectral (HRMS) image directly. The aim of pansharpening (PNN) is to fuse the spatial in panchromatic (PAN) with the spectral information in multispectral (MS). Recently, deep learning has drawn much attention, and in the field of remote sensing, several pioneering attempts have been made related to PNN. However, the big size of remote sensing data will produce more training samples, which require a deeper neural network. Most current networks are relatively shallow and raise the possibility of detail loss. In this letter, we propose a residual encoder-decoder conditional generative adversarial network (RED-cGAN) for PNN to produce more details with sharpened images. The proposed method combines the idea of an autoencoder with generative adversarial network (GAN), which can effectively preserve the spatial and spectral information of the PAN and MS images simultaneously. First, the residual encoder-decoder module is adopted to extract the multiscale features from the last step to yield pansharpened images and relieve the training difficulty caused by deepening the network layers. Second, to further enhance the performance of the generator to preserve more spatial information, a conditional discriminator network with the input of PAN and MS images is proposed to encourage that the estimated MS images share the same distribution as that of the referenced HRMS images. The experiments conducted on the Worldview2 (WV2) and Worldview3 (WV3) images demonstrate that our proposed method provides better results than several state-of-the-art PNN methods.","1558-0571","","10.1109/LGRS.2019.2949745","National Natural Science Foundation of China(grant numbers:61671312,61922029); Sichuan Science and Technology Program(grant numbers:2018HH0070); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894479","Deep learning;generative adversarial network (GAN);multispectral (MS) image;panchromatic (PAN);pansharpening (PNN)","Feature extraction;Generative adversarial networks;Gallium nitride;Training;Generators;Task analysis;Decoding","feature extraction;geophysical image processing;hyperspectral imaging;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing","residual encoder-decoder conditional generative adversarial network;high-resolution multispectral image;PNN;PAN;remote sensing data;deeper neural network;spectral information;residual encoder-decoder module;conditional discriminator network;referenced HRMS images;Worldview3 images;MS image estimation;Worldview2 images;RED-cGAN;satellite sensor;pansharpening;spatial fusion;feature extraction","","36","","22","IEEE","8 Nov 2019","","","IEEE","IEEE Journals"
"Capsule Feature Pyramid Network for Building Footprint Extraction From High-Resolution Aerial Imagery","Y. Yu; Y. Ren; H. Guan; D. Li; C. Yu; S. Jin; L. Wang","Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; School of Energy and Power Engineering, Nanjing Institute of Technology, Nanjing, China; School of Remote Sensing and Geomatics Engineering, Nanjing University of Information Science and Technology, Nanjing, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China; Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, Huaian, China","IEEE Geoscience and Remote Sensing Letters","22 Apr 2021","2021","18","5","895","899","Building footprint extraction plays an important role in a wide range of applications. However, due to size and shape diversities, occlusions, and complex scenarios, it is still challenging to accurately extract building footprints from aerial images. This letter proposes a capsule feature pyramid network (CapFPN) for building footprint extraction from aerial images. Taking advantage of the properties of capsules and fusing different levels of capsule features, the CapFPN can extract high-resolution, intrinsic, and semantically strong features, which perform effectively in improving the pixel-wise building footprint extraction accuracy. With the use of signed distance maps as ground truths, the CapFPN can extract solid building regions free of tiny holes. Quantitative evaluations on an aerial image data set show that a precision, recall, intersection-over-union (IoU), and F-score of 0.928, 0.914, 0.853, and 0.921, respectively, are obtained. Comparative studies with six existing methods confirm the superior performance of the CapFPN in accurately extracting building footprints.","1558-0571","","10.1109/LGRS.2020.2986380","Natural Science Foundation of Jiangsu Province(grant numbers:BK20160427,BK20191214); National Natural Science Foundation of China(grant numbers:61603146,51975239,41971414,41671454); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075171","Aerial imagery;building extraction;building footprint;capsule feature pyramid network (CapFPN);capsule network","Feature extraction;Buildings;Remote sensing;Training;Shape;Generative adversarial networks;Gallium nitride","building information modelling;cartography;feature extraction;geophysical image processing;image classification;image fusion;image resolution","IoU;intersection-over-union;quantitative evaluations;ground truths;signed distance maps;semantically strong feature extraction;intrinsic feature extraction;high-resolution feature extraction;occlusions;shape diversities;aerial image data set;solid building region extraction;pixel-wise building footprint extraction accuracy;CapFPN;high-resolution aerial imagery;capsule feature pyramid network","","18","","28","IEEE","21 Apr 2020","","","IEEE","IEEE Journals"
"Perceptual-Aware Sketch Simplification Based on Integrated VGG Layers","X. Xu; M. Xie; P. Miao; W. Qu; W. Xiao; H. Zhang; X. Liu; T. -T. Wong","State Key Laboratory of Subtropical Building Science, Guangdong Provincial Key Lab of Computational Intelligence and Cyberspace Information, China; Chinese University of Hong Kong, China; School of Computer Science and Engineering, South China University of Technology, Guangdong, China; School of Computer Science and Engineering, South China University of Technology, Guangdong, China; School of Computer Science and Engineering, South China University of Technology, Guangdong, China; School of Computer Science and Engineering, South China University of Technology, Guangdong, China; School of Computing and Information Sciences, Caritas Institute of Higher Education, China; Chinese University of Hong Kong, China","IEEE Transactions on Visualization and Computer Graphics","24 Nov 2020","2021","27","1","178","189","Deep learning has been recently demonstrated as an effective tool for raster-based sketch simplification. Nevertheless, it remains challenging to simplify extremely rough sketches. We found that a simplification network trained with a simple loss, such as pixel loss or discriminator loss, may fail to retain the semantically meaningful details when simplifying a very sketchy and complicated drawing. In this paper, we show that, with a well-designed multi-layer perceptual loss, we are able to obtain aesthetic and neat simplification results preserving semantically important global structures as well as fine details without blurriness and excessive emphasis on local structures. To do so, we design a multi-layer discriminator by fusing all VGG feature layers to differentiate sketches and clean lines. The weights used in layer fusing are automatically learned via an intelligent adjustment mechanism. Furthermore, to evaluate our method, we compare our method to state-of-the-art methods through multiple experiments, including visual comparison and intensive user study.","1941-0506","","10.1109/TVCG.2019.2930512","National Natural Science Foundation of China(grant numbers:61772206,U1611461,61472145); Guangdong R&D key project of China(grant numbers:2018B010107003); Guangdong High-level personnel program(grant numbers:2016TQ03X319); Guangdong NSF(grant numbers:2017A030311027); Guangzhou key project in industrial technology(grant numbers:201802010027); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8771128","Convolutional neural network;perceptual awareness;sketch simplification","Feature extraction;Semantics;Task analysis;Generative adversarial networks;Visualization;Lighting;Image segmentation","art;feature extraction;image fusion;learning (artificial intelligence);multilayer perceptrons;visual perception","visual comparison;VGG feature layer fusion;multilayer discriminator;local structures;semantically important global structures;multilayer perceptual loss;discriminator loss;pixel loss;simplification network;extremely rough sketches;raster-based sketch simplification;deep learning;integrated VGG layers;perceptual-aware sketch simplification","","17","","31","IEEE","24 Jul 2019","","","IEEE","IEEE Journals"
"Cross-Level Parallel Network for Crowd Counting","J. Li; Y. Xue; W. Wang; G. Ouyang","School of Information Engineering, Nanchang University, Nanchang, China; School of Information Engineering, Nanchang University, Nanchang, China; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Cognitive Neuroscience and Learning and IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China","IEEE Transactions on Industrial Informatics","8 Jan 2020","2020","16","1","566","576","Automated people counting in crowd scenes is challenging due to large variations in scale, density, and background clutter. To tackle them, we propose a novel cross-level parallel network (CLPNet) by extracting multiple low-level features from VGG16 and fusing them with specific scale aggregation modules in the high-level stage. To deal with scale variation, we design five different aggregation modules for multiscale fusion. Furthermore, the ground truth is processed skillfully to eliminate the mismatches caused by the scale variation between heads and density maps. To cope with background clutter, cross-level feature fusion is implemented. Higher-level semantic information could effectively separate head from background and regain the lost low-level detailed information. To address the variation of density, we design a parallel network, in which two separate channels focus on different density-level estimation, and attain more accurate counting results. Finally, we evaluate the proposed CLPNet on four representative crowd counting datasets, i.e., ShanghaiTech, UCF_CC_50, WorldExpo'10, and UCF_QNRF. The experimental results demonstrate that with the cross-level and multiscale structure CLPNet achieves superior performance compared with the state-of-the-art crowd counting methods.","1941-0050","","10.1109/TII.2019.2935244","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFC0820205); National Natural Science Foundation of China(grant numbers:61703198); Natural Science Foundation for Distinguished Young Scholars of Jiangxi Province(grant numbers:2018ACB21014); Open Fund of State Key Laboratory of Management and Control for Complex Systems(grant numbers:20180109); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798674","Convolutional neural network (CNN);cross-level and multiscale features;crowd counting;density map;scale aggregation network","Feature extraction;Convolution;Head;Semantics;Estimation;Kernel;Generative adversarial networks","feature extraction;image fusion;neural nets;object detection;parallel processing","crowd scenes;multiple low-level feature extraction;VGG16;multiscale fusion;cross-level feature fusion;multiscale structure CLPNet;cross-level parallel network;crowd counting;cross-level CLPNet;automated people counting;density-level estimation;ShanghaiTech dataset;UCF_CC_50 dataset;WorldExpo'10 dataset;UCF_QNRF dataset","","14","","40","IEEE","14 Aug 2019","","","IEEE","IEEE Journals"
"Conditional GAN and 2-D CNN for Bearing Fault Diagnosis With Small Samples","J. Yang; J. Liu; J. Xie; C. Wang; T. Ding","School of Traffic and Transportation Engineering, Central South University, Changsha, China; School of Printing Packaging and Digital Media, Xi’an University of Technology, Xi’an, China; School of Traffic and Transportation Engineering, Central South University, Changsha, China; School of Printing Packaging and Digital Media, Xi’an University of Technology, Xi’an, China; School of Printing Packaging and Digital Media, Xi’an University of Technology, Xi’an, China","IEEE Transactions on Instrumentation and Measurement","21 Oct 2021","2021","70","","1","12","The rolling bearing is the key component of rotating machinery, and it is also a failure–prone component. The intelligent fault diagnosis method has been widely used to accurately diagnose bearing faults. However, in engineering practice, it is difficult to obtain sufficient sample data to train the intelligent diagnosis model. Therefore, in this article, a fusion diagnosis model CGAN-2-D-CNN that combines a conditional generative adversarial network (CGAN) and a two-dimensional convolutional neural network (2-D-CNN) is proposed for bearing fault diagnosis with small samples. Considering the problem of insufficient sample data, CGAN is used to learn the data distribution of real samples to generate new samples with similar data distribution by the confrontation training of the generator and discriminator. Then, 2-D preprocessing is conducted to convert the generated 1-D data into 2-D gray images. Finally, these images are input into the 2-D-CNN to extract the features and classify the bearing fault types. Two experimental cases are implemented to validate the effectiveness and feasibility of the proposed CGAN-2-D-CNN. The experimental results illustrate that the diagnosis accuracy of the proposed method used on the small sample data is close to that of the 2-D-CNN directly used on the enough original sample data whose size is equal to the expanded sample size. In addition, compared with the 1-D-CNN, support vector machine (SVM), and long short-term memory (LSTM) models, the 2-D-CNN model after 2-D preprocessing has the higher fault classification accuracy.","1557-9662","","10.1109/TIM.2021.3119135","National Natural Science Foundation of China(grant numbers:51905422); Natural Science Basic Research Program of Shaanxi(grant numbers:2020JQ-630); China Postdoctoral Science Foundation(grant numbers:2020M673613XB); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9565927","Bearing fault diagnosis;conditional generative adversarial network (CGAN);convolutional neural network;deep learning;small samples","Data models;Fault diagnosis;Feature extraction;Generative adversarial networks;Generators;Convolutional neural networks;Vibrations","convolutional neural nets;deep learning (artificial intelligence);fault diagnosis;image fusion;mechanical engineering computing;recurrent neural nets;rolling bearings;support vector machines","higher fault classification accuracy;conditional GAN;fault diagnosis bearing;rolling bearing;failure-prone component;intelligent fault diagnosis method;fusion diagnosis model CGAN-2-D-CNN;conditional generative adversarial network;two-dimensional convolutional neural network;data distribution;2-D gray images;1-D-CNN;2-D preprocessing;support vector machine;long short-term memory models;fault types bearing","","13","","43","IEEE","8 Oct 2021","","","IEEE","IEEE Journals"
"Multipath Fusion Mask R-CNN With Double Attention and Its Application Into Gear Pitting Detection","D. Xi; Y. Qin; J. Luo; H. Pu; Z. Wang","State Key Laboratory of Mechanical Transmission, College of Mechanical Engineering, Chongqing University, Chongqing, China; State Key Laboratory of Mechanical Transmission, College of Mechanical Engineering, Chongqing University, Chongqing, China; State Key Laboratory of Mechanical Transmission, College of Mechanical Engineering, Chongqing University, Chongqing, China; State Key Laboratory of Mechanical Transmission, College of Mechanical Engineering, Chongqing University, Chongqing, China; State Key Laboratory of Mechanical Transmission, College of Mechanical Engineering, Chongqing University, Chongqing, China","IEEE Transactions on Instrumentation and Measurement","20 Jan 2021","2021","70","","1","11","Since the traditional gear pitting detection technique based on artificial observation has low accuracy and efficiency, an accurate vision detection method based on a new development of Mask R-CNN is explored. In order to solve the issue of small pitting samples, the generated adversarial network is firstly employed to enlarge the training samples with multilevel pitting. Then, we put forward a multipath fusion Mask R-CNN with double attention (DAMF Mask R-CNN) to implement the simultaneous segmentation of tooth surface and gear pitting. This new type of Mask R-CNN enhances the ability of target segmentation by embedding two attention modules, which can strengthen the feature expression. Considering the difference in the grayscale texture shapes of different-level pitting images, a multicascade multipath feature extraction network is constructed by designing a multicascade multipath pyramid module and adding a dual attention module, which significantly improves the generalization ability and segmentation performance of small pitting. Finally, we segment the gear pitting and tooth surface by the DAMF Mask R-CNN neural networks, and then compare it with Mask R-CNNs, Mask Scoring R-CNNs, U-Net, and Deeplabv3+. It can be known from the comparative results that the proposed DAMF Mask R-CNN has higher detection accuracy and convergence performance than the traditional segmentation networks.","1557-9662","","10.1109/TIM.2021.3049276","National Key Research and Development Program of China(grant numbers:2018YFB2001300); National Natural Science Foundation of China(grant numbers:51675065,62033001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314236","Attention mechanism;feature texture transfer module;gear pitting;Mask R-CNN;multicascade multipath","Gears;Feature extraction;Image segmentation;Generative adversarial networks;Gallium nitride;Loss measurement;Training","computer vision;convolutional neural nets;edge detection;feature extraction;gears;image colour analysis;image fusion;image segmentation;image sensors;image texture","multipath fusion Mask R-CNN;double attention;application into gear pitting detection;traditional gear pitting detection technique;accurate vision detection method;pitting samples;generated adversarial network;multilevel pitting;DAMF Mask R-CNN;tooth surface;attention modules;different-level pitting images;multicascade multipath feature extraction network;multicascade multipath pyramid module;dual attention module;Mask Scoring R-CNNs;higher detection accuracy;convergence performance;traditional segmentation networks","","12","","43","IEEE","5 Jan 2021","","","IEEE","IEEE Journals"
"Domain-Aware Unsupervised Hyperspectral Reconstruction for Aerial Image Dehazing","A. Mehta; H. Sinha; M. Mandal; P. Narang","Department of CSIS, BITS, Pilani, India; Department of CSIS, BITS, Pilani, India; Department of CSE, IIIT, Kota, India; Department of CSIS, BITS, Pilani, India","2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 Jun 2021","2021","","","413","422","Haze removal in aerial images is a challenging problem due to considerable variation in spatial details and varying contrast. Changes in particulate matter density often lead to degradation in visibility. Therefore, several approaches utilize multi-spectral data as auxiliary information for haze removal. In this paper, we propose SkyGAN for haze removal in aerial images. SkyGAN consists of 1) a domain-aware hazy-to-hyperspectral (H2H) module, and 2) a conditional GAN (cGAN) based multi-cue image-to-image translation module (I2I) for dehazing. The proposed H2H module reconstructs several visual bands from RGB images in an unsupervised manner, which overcomes the lack of hazy hyperspectral aerial image datasets. The module utilizes task supervision and domain adaptation in order to create a ""hyperspectral catalyst"" for image dehazing. The I2I module uses the hyperspectral catalyst along with a 12-channel multi-cue input and performs effective image de-hazing by utilizing the entire visual spectrum. In addition, this work introduces a new dataset, called Hazy Aerial-Image (HAI) dataset, that contains more than 65,000 pairs of hazy and ground truth aerial images with realistic, non- homogeneous haze of varying density. The performance of SkyGAN is evaluated on the recent SateHaze1k dataset as well as the HAI dataset. We also present a comprehensive evaluation of HAI dataset with a representative set of state-of-the-art techniques in terms of PSNR and SSIM.","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423159","","Degradation;Visualization;Image color analysis;Catalysts;Conferences;Generative adversarial networks;Task analysis","feature extraction;image classification;image colour analysis;image denoising;image enhancement;image fusion;image reconstruction;neural nets;unsupervised learning","multispectral data;haze removal;SkyGAN;domain-aware hazy-to-hyperspectral module;H2H;conditional GAN based multicue image-to-image translation module;RGB images;hazy hyperspectral aerial image datasets;task supervision;domain adaptation;hyperspectral catalyst;12-channel multicue input;hazy aerial-image dataset;hazy truth aerial images;ground truth aerial images;HAI dataset;domain-aware unsupervised hyperspectral reconstruction;aerial image dehazing;particulate matter density;image dehazing;I2I module","","11","","53","IEEE","14 Jun 2021","","","IEEE","IEEE Conferences"
"Generative Adversarial Network for Pansharpening With Spectral and Spatial Discriminators","A. Gastineau; J. -F. Aujol; Y. Berthoumieu; C. Germain","CNRS, UMR 5251, Bordeaux INP, Institut de Mathématique de Bordeaux (IMB), University of Bordeaux, Talence, France; CNRS, UMR 5251, Bordeaux INP, Institut de Mathématique de Bordeaux (IMB), University of Bordeaux, Talence, France; Laboratoire de Intégration du Matériau au Systéme (IMS), CNRS, UMR 5218, Bordeaux INP, University of Bordeaux, Talence, France; Laboratoire de Intégration du Matériau au Systéme (IMS), CNRS, UMR 5218, Bordeaux INP, University of Bordeaux, Talence, France","IEEE Transactions on Geoscience and Remote Sensing","8 Dec 2021","2022","60","","1","11","The pansharpening problem amounts to fusing a high-resolution panchromatic image with a low-resolution multispectral image so as to obtain a high-resolution multispectral image. Therefore, the preservation of the spatial resolution of the panchromatic image and the spectral resolution of the multispectral image is of key importance for the pansharpening problem. To cope with it, we propose a new method based on a bidiscriminator in a generative adversarial network (GAN) framework. The first discriminator is optimized to preserve textures of images by taking as input the luminance and the near-infrared band of images, and the second discriminator preserves the color by comparing the chroma components Cb and Cr. Thus, this method allows to train two discriminators, each one with a different and complementary task. Moreover, to enhance these aspects, the proposed method based on bidiscriminator, and called MDSSC-GAN SAM, considers a spatial and a spectral constraint in the loss function of the generator. We show the advantages of this new method on experiments carried out on Pléiades and World View 3 satellite images.","1558-0644","","10.1109/TGRS.2021.3060958","French State through the French National Research Agency (ANR) in the Framework of the Investments for the Future Programme IdEx Bordeaux—SysNum(grant numbers:ANR-10-IDEX-03-02); ANR(grant numbers:ANR-18-CE92-0050 SUPREMATIM); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9371303","Bidiscriminator;deep learning;generative adversarial network (GAN);pansharpening;remote sensing","Spatial resolution;Pansharpening;Gallium nitride;Generative adversarial networks;Vegetation mapping;Satellites;Generators","image colour analysis;image fusion;image resolution;image texture;neural nets;spectral analysis","pansharpening;high-resolution panchromatic image;low-resolution multispectral image;high-resolution multispectral image;spatial resolution;spectral resolution;generative adversarial network;spectral discriminator;spatial discriminator;loss function;image texture","","11","","29","IEEE","5 Mar 2021","","","IEEE","IEEE Journals"
"Adversarial Multi-Path Residual Network for Image Super-Resolution","Q. Wang; Q. Gao; L. Wu; G. Sun; L. Jiao","Key Laboratory of Ministry of Education of Intellisense and Image Understanding, School of Telecommunication Engineering, Xidian University, Xi’an, China; Xidian-Ningbo Information Technology Institute, Ningbo, China; Huawei Consumer Business Group, Xi’an, China; Institutes for Robotics and Intelligent Manufacturing, Chinese Academy of Sciences, Shenyang, China; Key Laboratory of Ministry of Education of Intellisense and Image Understanding, School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","26 Jul 2021","2021","30","","6648","6658","Recently, deep convolutional neural networks have demonstrated remarkable progresses on single image super-resolution (SR) problem. However, most of them use more deeper and wider networks to improve SR performance, which is not practical in real-world applications due to large complexity, high computation cost, and low efficiency. In addition, they cannot provide high perception quality and guarantee objective quality simultaneously. To address these limitations, we in this paper propose a novel Adversarial Multi-path Residual Network (AMPRN), which can largely suppress the number of network parameters and achieve a higher SR performance compared with the state-of-the-art methods. More specifically, we propose a multi-path residual block (MPRB) for multi-path residual network (MPRN) with fewer network parameters, which can extract abundant local features by fully using features from different paths generated by channel slices. These hierarchical features from all the MPRBs are then jointly aggregated by global gradual feature fusion. Following MPRN, we construct an adversarial gradient network with a gradient loss to make the gradient distribution of the generated SR images and ground truth image closer. In this way, the generated SR images of our model can provide high perception quality and objective quality. Finally, several experimental results demonstrate that our AMPRN achieves better performance in comparison with fewer parameters than the state-of-the-art methods.","1941-0042","","10.1109/TIP.2021.3096089","Initiative Postdoctoral Supporting Program(grant numbers:BX20190262); China Postdoctoral Science Foundation(grant numbers:2019M663642); National Natural Science Foundation of Shaanxi Province(grant numbers:2020JZ-19,2020JQ-327); Natural Science Foundation of Ningbo(grant numbers:2018A610049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490520","Single image super-resolution (SISR);residual learning;deep convolutional neural network","Feature extraction;Residual neural networks;Superresolution;Generative adversarial networks;Image reconstruction;Generators;Training","convolutional neural nets;feature extraction;image fusion;image resolution","generated SR images;ground truth image;high perception quality;deep convolutional neural networks;single image super-resolution problem;deeper networks;wider networks;high computation cost;multipath residual block;local features;hierarchical features;global gradual feature fusion;adversarial gradient network;adversarial multipath residual network","","10","","40","IEEE","19 Jul 2021","","","IEEE","IEEE Journals"
"Multi-Modal MRI Image Synthesis via GAN With Multi-Scale Gate Mergence","B. Zhan; D. Li; X. Wu; J. Zhou; Y. Wang","School of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, Sichuan, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, Sichuan, China; School of Computer Science, Sichuan University, Chengdu, Sichuan, China","IEEE Journal of Biomedical and Health Informatics","17 Jan 2022","2022","26","1","17","26","Multi-modal magnetic resonance imaging (MRI) plays a critical role in clinical diagnosis and treatment nowadays. Each modality of MRI presents its own specific anatomical features which serve as complementary information to other modalities and can provide rich diagnostic information. However, due to the limitations of time consuming and expensive cost, some image sequences of patients may be lost or corrupted, posing an obstacle for accurate diagnosis. Although current multi-modal image synthesis approaches are able to alleviate the issues to some extent, they are still far short of fusing modalities effectively. In light of this, we propose a multi-scale gate mergence based generative adversarial network model, namely MGM-GAN, to synthesize one modality of MRI from others. Notably, we have multiple down-sampling branches corresponding to input modalities to specifically extract their unique features. In contrast to the generic multi-modal fusion approach of averaging or maximizing operations, we introduce a gate mergence (GM) mechanism to automatically learn the weights of different modalities across locations, enhancing the task-related information while suppressing the irrelative information. As such, the feature maps of all the input modalities at each down-sampling level, i.e., multi-scale levels, are integrated via GM module. In addition, both the adversarial loss and the pixel-wise loss, as well as gradient difference loss (GDL) are applied to train the network to produce the desired modality accurately. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art multi-modal image synthesis methods.","2168-2208","","10.1109/JBHI.2021.3088866","National Natural Science Foundation of China; National Stem Cell Foundation(grant numbers:62071314); Sichuan Science and Technology Program(grant numbers:2020YFG0079,2021YFG0326); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9454402","Multi-modal synthesis;magnetic resonance imaging (MRI);generative adversarial network (GAN);gate mergence;multi-scale fusion","Magnetic resonance imaging;Logic gates;Generative adversarial networks;Feature extraction;Generators;Image synthesis;Convolution","biomedical MRI;feature extraction;image fusion;image registration;image segmentation;image sequences;learning (artificial intelligence);medical image processing","multiscale gate mergence;multiscale levels;multimodal MRI image synthesis;multimodal magnetic resonance imaging;clinical diagnosis;multimodal image synthesis methods;multimodal image synthesis approaches","Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging","9","","37","IEEE","14 Jun 2021","","","IEEE","IEEE Journals"
"Feature Enhancement Based on CycleGAN for Nighttime Vehicle Detection","X. Shao; C. Wei; Y. Shen; Z. Wang","School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, China","IEEE Access","5 Jan 2021","2021","9","","849","859","Existing night vehicle detection methods mainly detect vehicles by detecting headlights or taillights. However, these features are adversely affected by the complex road lighting environment. In this paper, a cascade detection network framework FteGanOd is proposed with a feature translate-enhancement (FTE) module and the object detection (OD) module. First, the FTE module is built based on CycleGAN and multi-scale feature fusion is proposed to enhance the detection of vehicle features at night. The features of night and day are combined by fusing different convolutional layers to produce enhanced feature (EF) maps. Second, the OD module, based on the existing object detection network, is improved by cascading with the FTE module to detect vehicles on the EF maps. The proposed FteGanOd method recognizes vehicles at night with greater accuracy by improving the contrast between vehicles and the background and by suppressing interference from ambient light. The proposed FteGanOd is validated on the Berkeley Deep Drive (BDD) dataset and our private dataset. The experimental results show that our proposed method can effectively enhance vehicle features and improve the accuracy of vehicle detection at night.","2169-3536","","10.1109/ACCESS.2020.3046498","National Natural Science Foundation of China(grant numbers:61702032,61573057,61771042); Fund(grant numbers:61404130316,61400010302); Fundamental Research Funds for the Central Universities(grant numbers:2017JBZ002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303452","Convolutional neural network (CNN);nighttime vehicle detection;feature enhancement;generative adversarial network (GAN);detection network","Feature extraction;Vehicle detection;Object detection;Generative adversarial networks;Proposals;Training;Lighting","convolutional neural nets;feature extraction;image enhancement;image fusion;object detection;road vehicles;traffic engineering computing","vehicle feature enhancement;generative adversarial network;convolutional neural network;Berkeley Deep Drive dataset;convolutional layers;feature fusion;cascade detection network framework;feature translate enhancement module;road lighting;nighttime vehicle detection;FteGanOd;object detection;enhanced feature maps;multiscale feature fusion;CycleGAN;FTE module","","8","","40","CCBY","22 Dec 2020","","","IEEE","IEEE Journals"
"TMS-GAN: A Twofold Multi-Scale Generative Adversarial Network for Single Image Dehazing","P. Wang; H. Zhu; H. Huang; H. Zhang; N. Wang","School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; College of Information and Electrical Engineering, Shanghai Normal University, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Circuits and Systems for Video Technology","4 May 2022","2022","32","5","2760","2772","In recent years, learning-based single image dehazing networks have been comprehensively developed. However, performance improvement is limited due to domain shift between trained synthetic hazy images and untrained real-world hazy images. To alleviate this issue, this paper proposes a real-world dehazing targeted training scheme which nearly realizes paired real-world data training. As a result, a Twofold Multi-scale Generative Adversarial Network (TMS-GAN) consisting of a Haze-generation GAN (HgGAN) and a Haze-removal GAN (HrGAN) is designed. HgGAN attributes real haze properties to synthetic images and HrGAN removes haze from both synthetic and generated fake realistic data under supervision. Thus, the proposed method can better adapt to real-world image dehazing using this cooperative training scheme. Meanwhile, several structural advances of TMS-GAN also improve dehazing performance. Specifically, a haze residual map based on atmospheric scattering model is deduced in HgGAN for fake realistic data generation. The dual-branch generator in HrGAN draws attention to detail restoration by one branch along with another color-branch. A plug-and-play Multi-attention Progressive Fusion Module (MAPFM) is proposed and inserted in both HgGAN and HrGAN. MAPFM incorporates multi-attention mechanism to guide multi-scale feature fusion in a progressive manner, in which Adjacency-attention Block (AAB) can capture contributing features of each level and Self-attention Block (SAB) can establish non-local dependency of feature fusion. Experiments on mainstream benchmarks show that the proposed framework is superior especially on real-world hazy images among single image dehazing methods.","1558-2205","","10.1109/TCSVT.2021.3097713","National Nature Science Foundation of China(grant numbers:61872143); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9489298","Single image dehazing;twofold;multi-scale;fake realistic haze generation;multi-attention progressive fusion","Atmospheric modeling;Training;Scattering;Image restoration;Generative adversarial networks;Decoding;Image color analysis","image colour analysis;image denoising;image enhancement;image fusion;image restoration;learning (artificial intelligence);neural nets","multiscale feature fusion;TMS-GAN;learning-based single image dehazing networks;trained synthetic hazy images;HgGAN;Haze-removal GAN;HrGAN;haze residual map;fake realistic data generation;dual-branch generator;haze-generation GAN;untrained real-world hazy images;atmospheric scattering model;adjacency-attention block;plug-and-play multiattention progressive fusion module;twofold multiscale generative adversarial network","","5","","39","IEEE","16 Jul 2021","","","IEEE","IEEE Journals"
"Semantic Segmentation Using a GAN and a Weakly Supervised Method Based on Deep Transfer Learning","S. Wen; W. Tian; H. Zhang; S. Fan; N. Zhou; X. Li","Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Key Laboratory of Industrial Computer Control Engineering of Hebei Province, Yanshan University, Qinhuangdao, China; Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Key Laboratory of Industrial Computer Control Engineering of Hebei Province, Yanshan University, Qinhuangdao, China; Key Laboratory of Industrial Computer Control Engineering of Hebei Province, Yanshan University, Qinhuangdao, China; Key Laboratory of Industrial Computer Control Engineering of Hebei Province, Yanshan University, Qinhuangdao, China","IEEE Access","2 Oct 2020","2020","8","","176480","176494","Semantic image segmentation is of crucial importance to many applications, such as autonomous driving, robot vision, and scene understanding. However, the border of a segmented image tends to be rough, and the labeling process is tedious and labor-intensive. Therefore, this study is the first proposing to use a deep generative adversarial network (GAN) with double-layered upsampling based on max-pooling indexed deconvolution. Our proposed upsampling method replaces the bilinear interpolation upsampling method; i.e., we fuse the deep deconvolution method by saving the indices of relative locations of the max weights computed during pooling. Combined with the deep GAN, our upsampling method can improve the extraction of low-resolution features, and compensate for the loss of the image size. To further reduce the whole network's dependence on labeled datasets, a weakly supervised feedback method is proposed. The unlabeled data can improve the generalization ability of the model. Considering the generalization to unseen image domains, we introduce transfer learning based on a deep GAN and a weakly supervised method. The segmentation model using the trained data in the source domain can obtain good segmentation in the target domain using transfer learning. Extensive experiments in various domains demonstrate the advantages of the proposed method compared to the generalization ability of semantic segmentation. This method also significantly decreases the dependence on labeled data and ensures the network accuracy.","2169-3536","","10.1109/ACCESS.2020.3026684","National Natural Science Foundation of China(grant numbers:61673125,61773333); China Scholarship Council (CSC)(grant numbers:201908130016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206031","Semantic segmentation;GAN;deep transfer learning","Semantics;Image segmentation;Gallium nitride;Generative adversarial networks;Object segmentation;Deconvolution;Feature extraction","feature extraction;image colour analysis;image fusion;image representation;image resolution;image segmentation;learning (artificial intelligence);neural nets;robot vision;sampling methods","weakly supervised method;deep transfer learning;semantic image segmentation;autonomous driving;robot vision;scene understanding;segmented image;labeling process;deep generative adversarial network;good segmentation;segmentation model;unseen image domains;generalization ability;weakly supervised feedback method;labeled datasets;image size;deep GAN;max weights;deep deconvolution method;bilinear interpolation upsampling method;max-pooling indexed deconvolution","","4","","46","CCBY","25 Sep 2020","","","IEEE","IEEE Journals"
"Efficient Traffic Accident Warning Based on Unsupervised Prediction Framework","Y. -F. Zhou; K. Xie; X. -Y. Zhang; C. Wen; J. -B. He","Western Institute of Yangtze University, Karamay, China; Western Institute of Yangtze University, Karamay, China; Western Institute of Yangtze University, Karamay, China; School of Computer Science, Yangtze University, Jingzhou, China; School of Computer Science and Engineering, Central South University, Changsha, China","IEEE Access","13 May 2021","2021","9","","69100","69113","Recognizing potentially hazardous objects is crucial in the field of transportation, especially in assisted and unmanned driving. However, most existing studies do not focus on defensive driving as they only identify accidents ahead of the vehicle in which they are not involved. In this paper, a driving assistance system is proposed to predict the risk score of potential targets ahead of the vehicle and provide an early warning, which relies on a deep architecture called Fusion-Residual Predictive Network (FRPN) that fused multi-scale residual features and improved adversarial learning. This architecture provides an environment for the generator to perform joint learning from ground-truth images and discriminators with gradient penalty constraints. The deeper convolutional neural network can greatly improve the quality of the image by fusing residual features. Several deep convolutional neural network models were used to evaluate the method on various datasets; among them, the prediction model based on the VGG network, with peak signal-to-noise ratio of 32.67 and structural similarity index of 0.921, respectively, yielded the best results. Subsequently, we utilize the tracking model to design a risk score evaluation method based on the location of the target and it have an improvement in ability to give early warning with 1.95s earlier in the best case. These results prove that our method can effectively reduce the risk of traffic accidents.","2169-3536","","10.1109/ACCESS.2021.3077120","Natural Science Foundation of Xinjiang Uygur Autonomous Region(grant numbers:2020D01A131); Fund of Hubei Ministry of Education(grant numbers:B2019039); Graduate Teaching and Research Fund of Yangtze University(grant numbers:YJY201909); Teaching and Research Fund of Yangtze University(grant numbers:JY2019011); Undergraduate Training Programs for Innovation and Entrepreneurship of Yangtze University(grant numbers:2019100,Yz2020057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420733","Generative adversarial network;video prediction;recurrent neural network;convolution neural network;object tracking;traffic warning;unsupervised learning;risk score assessment","Accidents;Predictive models;Target tracking;Prediction algorithms;Generative adversarial networks;Dynamics;Automobiles","convolutional neural nets;deep learning (artificial intelligence);gradient methods;image fusion;object detection;road safety;traffic engineering computing","efficient traffic accident warning;unsupervised prediction framework;potentially hazardous objects;assisted driving;unmanned driving;defensive driving;driving assistance system;early warning;deep architecture;Fusion-Residual Predictive Network;multiscale residual features;adversarial learning;joint learning;ground-truth images;gradient penalty constraints;deep convolutional neural network models;prediction model;VGG network;peak signal-to-noise ratio;structural similarity index;tracking model;risk score evaluation method;traffic accidents;time 1.95 s","","4","","42","CCBY","3 May 2021","","","IEEE","IEEE Journals"
"Focussing Learned Image Compression to Semantic Classes for V2X Applications","J. Löhdefink; A. Bär; N. M. Schmidt; F. Hüger; P. Schlicht; T. Fingscheidt","Institute for Communications Technology, Technische Universität Braunschweig, Braunschweig, Germany; Institute for Communications Technology, Technische Universität Braunschweig, Braunschweig, Germany; Volkswagen Group Innovation, Wolfsburg, Germany; Volkswagen Group Innovation, Wolfsburg, Germany; Volkswagen Group Innovation, Wolfsburg, Germany; Institute for Communications Technology, Technische Universität Braunschweig, Braunschweig, Germany","2020 IEEE Intelligent Vehicles Symposium (IV)","8 Jan 2021","2020","","","1641","1648","Cooperative perception with many sensors involved greatly improves the performance of perceptual systems in autonomous vehicles. However, the increasing amount of sensor data leads to a bottleneck due to limited capacity of vehicle-to-X (V2X) communication channels. We leverage lossy learned image compression by means of an autoencoder with adversarial loss function to reduce the overall bitrate. Our key contribution is to focus image compression on regions of interest (ROIs) governed by a binary mask. A transmitter-sided semantic segmentation network extracts semantically important classes being the basis for the generation of a ROI. A second key contribution is that the mask is not transmitted as side information, only the quantized bottleneck data is transmitted. To train the network, we use a loss function operating only on the pixels in the ROI. We report peak-signal-to-noise ratio (PSNR) both in the entire image and only in the ROI, evaluating various fusion architectures and fusion operations involving input image and mask. Showing the high generalizability of our approach, we achieve consistent improvements in the ROI in all experiments on the Cityscapes dataset.","2642-7214","978-1-7281-6673-5","10.1109/IV47402.2020.9304779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9304779","","Image coding;Semantics;Image segmentation;Image reconstruction;Sensors;Receivers;Generative adversarial networks","data compression;image coding;image fusion;image segmentation;learning (artificial intelligence);neural nets;traffic engineering computing;vehicular ad hoc networks","ROI;binary mask;transmitter-sided semantic segmentation network;quantized bottleneck data;peak-signal-to-noise ratio;input image;semantic classes;V2X Applications;perceptual systems;autonomous vehicles;sensor data;vehicle-to-X communication channels;adversarial loss function;autoencoder;regions of interest;fusion operations;lossy learned image compression","","4","","57","IEEE","8 Jan 2021","","","IEEE","IEEE Conferences"
"RGB-Depth Fusion GAN for Indoor Depth Completion","H. Wang; M. Wang; Z. Che; Z. Xu; X. Qiao; M. Qi; F. Feng; J. Tang","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; AI Innovation Center, Midea Group; AI Innovation Center, Midea Group; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications; School of Computer Science, Beijing University of Posts and Telecommunications; AI Innovation Center, Midea Group; AI Innovation Center, Midea Group","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","6199","6208","The raw depth image captured by the indoor depth sen-sor usually has an extensive range of missing depth values due to inherent limitations such as the inability to perceive transparent objects and limited distance range. The incomplete depth map burdens many downstream vision tasks, and a rising number of depth completion methods have been proposed to alleviate this issue. While most existing meth-ods can generate accurate dense depth maps from sparse and uniformly sampled depth maps, they are not suitable for complementing the large contiguous regions of missing depth values, which is common and critical. In this paper, we design a novel two-branch end-to-end fusion network, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map. The first branch employs an encoder-decoder structure to regress the local dense depth values from the raw depth map, with the help of local guidance information extracted from the RGB image. In the other branch, we propose an RGB-depth fusion GAN to transfer the RGB image to the fine-grained textured depth map. We adopt adaptive fusion modules named W-AdaIN to propagate the features across the two branches, and we append a confidence fusion head to fuse the two out-puts of the branches for the final depth map. Extensive ex-periments on NYU-Depth V2 and SUN RGB-D demonstrate that our proposed method clearly improves the depth completion performance, especially in a more realistic setting of indoor environments with the help of the pseudo depth map.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00611","Fundamental Research Funds for the Central Universities(grant numbers:2019XD-A03-3); 111 Project(grant numbers:21PJI420300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878677","3D from multi-view and sensors","Training;Three-dimensional displays;Head;Fuses;Generative adversarial networks;Sampling methods;Sensors","decoding;image coding;image colour analysis;image fusion;image texture","depth completion methods;two-branch end-to-end fusion network;RGB imaging;RGB-depth fusion GAN;NYU-Depth V2;pseudodepth mapping;fine-grained textured depth mapping;uniformly sparse sampled depth mapping;indoor depth sensor;indoor depth completion performance;SUN RGB-D;W-AdaIN;adaptive fusion modules","","3","","42","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"MLFF-GAN: A Multilevel Feature Fusion With GAN for Spatiotemporal Remote Sensing Images","B. Song; P. Liu; J. Li; L. Wang; L. Zhang; G. He; L. Chen; J. Liu","Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Guangdong Key Laboratory for Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Key Laboratory for Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Eight Department, Aerospace Information Research Institute and the School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","30 May 2022","2022","60","","1","16","Due to the limitation of technology and budget, it is often difficult for sensors of a single remote sensing satellite to have both high-temporal and high-spatial (HTHS) resolution at the same time. In this article, we proposed a new multilevel feature fusion with generative adversarial network (MLFF-GAN) for generating fusion HTHS images. The MLFF-GAN mainly uses U-net-like architecture, and its generator is composed of three stages: feature extraction, feature fusion, and image reconstruction. In the feature extraction and reconstruction stage, the generator employs the encoding and decoding structure to extract three groups of multilevel features (MLFs), which can cope with the huge difference of resolution between high-resolution images and low-resolution images. In the feature fusion stage, adaptive instance normalization (AdaIN) block is designed to learn the global distribution relationship between multitemporal images, and an attention module (AM) is used to learn the local information weights for the change of small areas. The proposed MLFF-GAN was tested on two Landsat and Moderate-Resolution Imaging Spectroradiometer (MODIS) datasets. Some state-of-the-art algorithms are comprehensively compared with MLFF-GAN. We also carried on the ablation experiment to test the effectiveness of different submodules in the MLFF-GAN. The experiment results and ablation analysis show the better performances of the proposed method when compared with other methods. The code is available at https://github.com/songbingze/MLFF-GAN.","1558-0644","","10.1109/TGRS.2022.3169916","National Natural Science Foundation of China(grant numbers:61731022,41971397); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9781347","Adaptive instance normalization (AdaIN);generative adversarial network (GAN);spatial attention mechanism;U-net","Spatial resolution;Feature extraction;Generators;Generative adversarial networks;Spatiotemporal phenomena;Image resolution;Image reconstruction","feature extraction;geophysical image processing;image coding;image fusion;image reconstruction;image resolution;neural nets;remote sensing","multitemporal images;spatiotemporal remote sensing images;single remote sensing satellite;feature extraction;image reconstruction;high-resolution images;multilevel feature fusion with generative adversarial network;HTHS images;MLFF-GAN;adaptive instance normalization block;attention module;moderate-resolution imaging spectroradiometer;high-temporal and high-spatial resolution","","3","","58","IEEE","24 May 2022","","","IEEE","IEEE Journals"
"High-Resolution Crowd Density Maps Generation With Multi-Scale Fusion Conditional GAN","S. Huang; H. Zhou; Y. Liu; R. Chen","Key Laboratory of Hunan Province for New Retail Virtual Reality Technology, Hunan University of Technology and Business, Changsha, China; School of Computer and Information Engineering, Hunan University of Technology and Business, Changsha, China; Mobile E-Business Collaborative Innovation Center of Hunan Province, Hunan University of Technology and Business, Changsha, China; Mobile E-Business Collaborative Innovation Center of Hunan Province, Hunan University of Technology and Business, Changsha, China","IEEE Access","16 Jun 2020","2020","8","","108072","108087","The major challenges for density maps estimation and accurate counting stem from the large-scale variations, serious occlusions, and perspective distortions. Existing methods generally suffer from the blurred density maps, which are caused by average convolution kernel, and the ineffective estimation across different crowd scenes. In this paper, we propose a multi-scale fusion conditional generative adversarial network (MFC-GAN) that can generate high-resolution and high-quality density maps. The fusion module of MFC-GAN is embedded in a multi-scale generator and discriminator architecture with a novel adversarial loss, which is designed to guide high-resolution density maps generation. In order to address the problem of scale variation, we further propose a bidirectional fusion module. It combines deep global semantic features and shallow local information by leveraging feature maps presented in different layers of the generator. Furthermore, in order to increase the effectiveness of the multi-scale fusion, we design a cross-attention fusion module, which weights the multi-scale fused feature and learns context-aware feature maps for generating high quality density maps. The experiments on four challenging datasets show the effectiveness, feasibility and robustness of the proposed MFC-GAN.","2169-3536","","10.1109/ACCESS.2020.3000741","Scientific Research Fund of Hunan Provincial Education Department(grant numbers:19A270,15b127); Social Science Foundation of Hunan Province(grant numbers:18YBA258); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9110913","Crowd counting;bidirectional fusion;cross-attention fusion;conditional GAN","Feature extraction;Estimation;Generative adversarial networks;Generators;Gallium nitride;Task analysis;Training","feature extraction;image fusion;image representation;image resolution;learning (artificial intelligence);neural nets;object detection","MFC-GAN;multiscale generator;high-resolution density maps generation;scale variation;bidirectional fusion module;cross-attention fusion module;multiscale fused feature;context-aware feature maps;high quality density maps;high-resolution crowd density maps generation;multiscale fusion conditional GAN;density maps estimation;large-scale variations;blurred density maps;multiscale fusion conditional generative adversarial network","","3","","54","CCBY","8 Jun 2020","","","IEEE","IEEE Journals"
"A New Data Augmentation Method for Time Series Wearable Sensor Data Using a Learning Mode Switching-Based DCGAN","H. Jeon; D. Lee","Mechanical Engineering Department, Soongsil University, Seoul, South Korea; Mechanical Engineering Department, Soongsil University, Seoul, South Korea","IEEE Robotics and Automation Letters","5 Oct 2021","2021","6","4","8671","8677","This letter describes a new image augmentation method based on a DCGAN considering mode switching in terms of transient learning accuracy threshold on the problem of human gesture recognition through imaging of wearable sensor time-series data and a deep CNN structure. Because the discriminator in GANs learns faster than the generator, it is known that mode collapse occurs, in which only image modes biased to a specific image type are augmented among various image forms. In this study, to solve the mode collapse caused by the learning difficulty mismatch between networks, we add a learning mode switching layer between the generator and discriminator and receive feedback from both networks’ transient learning accuracies to switch the learning mode if predefined thresholds are exceeded. We confirm that the proposed approach balanced the learning rate between the generator and discriminator networks, resolved the mode collapse problem, and increased the test accuracy of a deep CNN trained with an augmented image set by approximately 20.35% compared to a conventional DCGAN. In addition, it showed better accuracy on a performance comparison with other improved DCGAN methods.","2377-3766","","10.1109/LRA.2021.3103648","National Research Foundation of Korea(grant numbers:2019R1F1A104583413); Korea Institute for Advancement of Technology(grant numbers:N000P0017033,N000P0017123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511290","Time series data;DCGAN;Deep CNN;data augmentation","Generative adversarial networks;Time series analysis;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);gesture recognition;image fusion;time series","learning difficulty mismatch;learning mode switching layer;image augmentation;DCGAN;data augmentation;human gesture recognition;wearable sensor time-series data;deep CNN structure;generator network;discriminator network;learning mode switching","","3","","34","IEEE","10 Aug 2021","","","IEEE","IEEE Journals"
"Feature Fusion and Center Aggregation for Visible-Infrared Person Re-Identification","X. Wang; C. Chen; Y. Zhu; S. Chen","School of Physics and Electronic Engineering, Fuyang Normal University, Fuyang, China; School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; School of Physics and Electronic Engineering, Fuyang Normal University, Fuyang, China; School of Physics and Electronic Engineering, Fuyang Normal University, Fuyang, China","IEEE Access","24 Mar 2022","2022","10","","30949","30958","The visible-infrared pedestrian re- identification (VI Re-ID) task aims to match cross-modality pedestrian images with the same labels. Most current methods focus on mitigating the modality discrepancy by adopting a two-stream network and identity supervision. Based on current methods, we propose a novel feature fusion and center aggregation learning network ( $F^{2}$ CALNet) for cross-modality pedestrian re- identification.  $F^{2}$ CALNet focuses on learning modality-irrelevant features by simultaneously reducing inter-modality discrepancies and increasing the inter-identity variations in a single framework. Specifically, we first adopt a two-stream backbone network to extract modality-independent and modality-shared information. Then, we embed modality mitigation modules in a two-stream network to learn feature maps that are stripped of the modality information. Finally, we devise a feature fusion and center aggregation learning module, which first merges two different granularity features to learn distinguishing features, then, we organize two kinds of center-based loss functions to reduce the intra-identity inter- and intra-modality differences and increase inter-identity variations by simultaneously pulling the features of the same identity close to their centers and pushing far away the centers of different identities. Extensive experiments on two public cross-modality datasets (SYSU-MM01 and RegDB) show that  $F^{2}$ CALNet is superior to the state-of-the-art approaches. Furthermore, on the SYSU-MM01 datasets, our model outperforms the baseline by 5.52% and 4.25% for the accuracy of rank1 and mAP, respectively.","2169-3536","","10.1109/ACCESS.2022.3159805","Natural Science Foundation of Anhui Province(grant numbers:KJ2019ZD39); Anhui Science and Technology Department(grant numbers:202004a05020030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9736957","Visible-infrared;Re-ID;modality discrepancy;cross-modality","Feature extraction;Measurement;Surveillance;Physics;Infrared imaging;Generative adversarial networks","feature extraction;image fusion;image matching;image recognition;learning (artificial intelligence)","two-stream network;identity supervision;feature fusion;center aggregation learning network;2 CALNet;cross-modality pedestrian re- identification;modality-irrelevant features;inter-modality discrepancies;inter-identity variations;two-stream backbone network;modality-independent;modality-shared information;modality mitigation modules;feature maps;modality information;center aggregation learning module;granularity features;distinguishing features;center-based loss;intra-identity inter;intra-modality differences;identity close;public cross-modality datasets;visible-infrared person re-identification;visible-infrared pedestrian re- identification task aims;cross-modality pedestrian images;modality discrepancy","","2","","64","CCBYNCND","16 Mar 2022","","","IEEE","IEEE Journals"
"Pan-Sharpening Based on Joint Visual Saliency Analysis and Parallel Bidirectional Network","W. Zhu; L. Zhang","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","12 Oct 2022","2022","19","","1","5","In remote sensing (RS) images, the demands for spectral and spatial quality of different regions are different, which means that the unified fusion strategy on the whole image is not suitable for pan-sharpening task. Saliency, derived from visual attention mechanism, provides an effective way to satisfy these demands. Inspired by this, we propose a novel pan-sharpening method based on joint visual saliency analysis and parallel bidirectional network (JSPBN). First, considering the complex scenes and uneven distribution of targets in RS images, we develop a Bayesian optimization-based joint visual saliency analysis (B-JVSA) method that integrates prior saliency based on global color contrast with likelihood saliency based on joint co-occurrence histogram, which can highlight common salient regions while suppressing individual ones and irrelevant background by exploring the correlation among multiple RS images. Second, we construct a parallel bidirectional feature pyramid (PBFP) network to obtain coarse fusion features, fully considering individual characteristics of panchromatic (PAN) images and multispectral (MS) images. Finally, we design a saliency-aware layer (SAL) according to B-JVSA to further refine the fusion effect in salient regions and nonsalient regions. With the help of SAL, diverse strategies for certain regions are learned through two independent residual dense networks (RDNs) and thereby generating accurate fusion results. Experimental results show that our proposal performs better than the competing methods in both spatial quality enhancement and spectral fidelity preservation.","1558-0571","","10.1109/LGRS.2022.3209787","National Natural Science Foundation of China(grant numbers:62271060,61571050,41771407); Beijing Natural Science Foundation(grant numbers:4222046); BNU Interdisciplinary Research Foundation for the First-Year Doctoral Candidates(grant numbers:BNUXKJC2120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903428","Joint visual saliency analysis;pan-sharpening;parallel bidirectional network;remote sensing (RS)","Visualization;Image color analysis;Feature extraction;Bayes methods;Histograms;Convolution;Generative adversarial networks","Bayes methods;deep learning (artificial intelligence);feature extraction;geophysical image processing;image colour analysis;image fusion;image resolution;optimisation;remote sensing","visual attention mechanism;pan-sharpening method;parallel bidirectional network;Bayesian optimization-based joint visual saliency analysis method;prior saliency;likelihood saliency;joint co-occurrence histogram;multiple RS images;parallel bidirectional feature pyramid network;coarse fusion features;panchromatic images;multispectral images;saliency-aware layer;independent residual dense networks;remote sensing images;spectral quality;spatial quality;unified fusion strategy;B-JVSA;RDNs;SAL;PAN;MS;global color contrast","","2","","16","IEEE","26 Sep 2022","","","IEEE","IEEE Journals"
"Cut and Compare: End-to-end Offline Signature Verification Network","X. Lu; L. Huang; F. Yin","Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Institute of Automation Chinese Academy of Sciences, Beijing, China","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","3589","3596","Offline signature verification, to determine whether a handwritten signature image is genuine or forged for a claimed identity, is needed in many applications. How to extract salient features and how to calculate similarity scores are the major issues. In this paper, we propose a novel end-to-end cut-and-compare network for offline signature verification. Based on the Spatial Transformer Network (STN), discriminative regions are segmented from a pair of input signature images and are compared attentively with help of Attentive Recurrent Comparator (ARC). An adaptive distance fusion module is proposed to fuse the distances of these regions. To address the intra personal variability problem, we design a smoothed double-margin loss to train the network. The proposed network achieves state-of-the-art performance on CEDAR, GPDS Synthetic, BHSig-H and BHSig-B datasets of different languages. Furthermore, our network shows strong generalization ability on cross-language test.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412377","National Key Research and Development Program(grant numbers:2018YFB1005000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412377","","Training;Handwriting recognition;Image segmentation;Fuses;Focusing;Feature extraction;Generative adversarial networks","feature extraction;handwriting recognition;image fusion;image segmentation;learning (artificial intelligence);neural nets","end-to-end offline signature verification network;handwritten signature image;Spatial Transformer Network;input signature images;Attentive Recurrent Comparator;adaptive distance fusion module;salient feature extraction;end-to-end cut-and-compare network;STN;discriminative region segmentation;intra personal variability problem;smoothed double-margin loss;BHSig-B datasets;CEDAR datasets;GPDS Synthetic dataset;BHSig-H dataset","","2","","33","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"CS-GANomaly: A Supervised Anomaly Detection Approach with Ancillary Classifier GANs for Chromosome Images","Z. Li; G. Zhao; A. Yin; T. Wang; H. Chen; L. Guo; H. Yang; J. Yang; C. Lin","School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; Center for Medical Genetics, Guangdong Women's and Children's Hospital, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; Center for Medical Genetics, Guangdong Women's and Children's Hospital, Guangzhou, China; Center for Medical Genetics, Guangdong Women's and Children's Hospital, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China","2020 IEEE 3rd International Conference of Safe Production and Informatization (IICSPI)","1 Feb 2021","2020","","","492","499","Although anomaly detection is an urgent and significant task in various applications, there are few good enough methods to meet the requirements. The challenge of anomaly detection lies in how to distinguish the diverse or even unknown anomalies. Therefore, previous studies have focused on imagelevel anomaly detection tasks, and mostly based on unsupervised methods. These methods only consider the characteristics of normal samples and are confined to the “normal” category, ignoring information between samples. In particular, the limitations of unsupervised methods are more obvious for pixellevel abnormalities such as the chromosomal structural anomalies investigated in this paper, because the similarity between normal and abnormal samples is relatively high. In this work, we propose a supervised anomaly detection approach Classification - Enhanced GANomaly (abbreviated as CS-GANomaly). By fusing the category information with the sample, the spatial distribution of the sample has regional characteristics and then combining the anomaly detection strategies to carry out multidimension anomaly detection. The CS-GANomaly adopts the GAN framework for adversarial training and adapts the GAN output to anomaly detection. The experimental results prove the superiority of our approach, whether it is a pixel-level anomaly in abnormal chromosome dataset or image-level anomaly in the benchmark dataset.","","978-1-7281-7738-0","10.1109/IICSPI51290.2020.9332331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9332331","Anomaly detection;Generative adversarial network;Chromosome classification","Adaptation models;Graphical models;Generative adversarial networks;Task analysis;Gallium nitride;Anomaly detection;Biological cells","cellular biophysics;data mining;image classification;image fusion;object recognition;pattern classification;unsupervised learning","CS-GANomaly;ancillary classifier GANs;urgent task;diverse anomalies;even unknown anomalies;imagelevel anomaly detection tasks;unsupervised methods;normal samples;chromosomal structural anomalies;abnormal samples;supervised anomaly detection approach Classification;anomaly detection strategies;multidimension anomaly detection;pixel-level anomaly;abnormal chromosome dataset","","2","","39","IEEE","1 Feb 2021","","","IEEE","IEEE Conferences"
"Multiscale Structure and Texture Feature Fusion for Image Inpainting","L. Li; M. Chen; H. Shi; Z. Duan; X. Xiong","College of Automation and Information Engineering, Sichuan University of Science and Engineering, Yibin, China; College of Automation and Information Engineering, Sichuan University of Science and Engineering, Yibin, China; College of Automation and Information Engineering, Sichuan University of Science and Engineering, Yibin, China; College of Automation and Information Engineering, Sichuan University of Science and Engineering, Yibin, China; College of Automation and Information Engineering, Sichuan University of Science and Engineering, Yibin, China","IEEE Access","11 Aug 2022","2022","10","","82668","82679","In order to achieve interaction between structure and texture information in generative adversarial image inpainting networks and improve the semantic veracity of the restored images, unlike the original two-stage inpainting ideas where texture and structure are restored separately, this paper constructs a multi-scale fusion approach to image generation, which embeds images into two collaborative subtasks, that is, structure generation and texture synthesis under structural constraints. We also introduce a self-attention mechanism into the partial convolution of the encoder to enhance the long range contextual information acquisition of the model in image inpainting, and design a multi-scale fusion network to fuse the generated structure and texture feature, so that the structure and texture information can be reused for reconstruction, perception and style loss compensation, thus enabling the fused images to achieve global consistency. In the training phase, feature matching loss are introduced to enhance the image in terms of structural generation plausibility. Finally, through comparison experiments with other inpainting networks on the CelebA, Paris StreetView and Places2 datasets, it is demonstrated that our method constructed in this paper has better objective evaluation metrics, more effective inpainting of structural and texture information of corrupted images and better image inpainting performance.","2169-3536","","10.1109/ACCESS.2022.3196021","Sichuan Provincial Science and Technology Department Project(grant numbers:2022ZHCG0035); Open Fund of Sichuan Provincial University Key Laboratory of Enterprise Informationization and IoT Measurement and Control Technology(grant numbers:2021WYY01); Artificial Intelligence Key Laboratory Project of Sichuan Province(grant numbers:2021RYY04); Sichuan University of Science and Engineering Postgraduate Innovation Fund Project(grant numbers:y2021078); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9848468","Image inpainting;generative model;deep learning;generative adversarial network","Image edge detection;Convolution;Generators;Deep learning;Kernel;Generative adversarial networks;Image reconstruction","deep learning (artificial intelligence);image fusion;image restoration;image texture","multiscale fusion network;corrupted images;texture feature fusion;generative adversarial image inpainting networks;image restoration;structure generation;texture synthesis;contextual information acquisition","","2","","50","CCBY","3 Aug 2022","","","IEEE","IEEE Journals"
"Wasserstein Generative Adversarial Network for Depth Completion With Anisotropic Diffusion Depth Enhancement","T. M. Nguyen; M. Yoo","Department of Information Communication Convergence Technology, Soongsil University, Seoul, South Korea; School of Electronic Engineering, Soongsil University, Seoul, South Korea","IEEE Access","20 Jan 2022","2022","10","","6867","6877","The objective of depth completion is to generate a dense depth map by upsampling a sparse one. However, irregular sparse patterns or the lack of groundtruth data caused by unstructured data make depth completion extremely challenging. Sensor fusion using both RGB and LIDAR sensors can help produce a more reliable context with higher accuracy. Compared with previous approaches, this method takes semantic segmentation images as additional input and develops an unsupervised loss function. Thus, when combined with supervised depth loss, the depth completion problem is considered as semi-supervised learning. We used an adapted Wasserstein Generative Adversarial Network architecture instead of applying the traditional autoencoder approach and post-processing process to preserve valid depth measurements received from the input and further enhance the depth value precision of the results. Our proposed method was evaluated on the KITTI depth completion benchmark, and its performance in depth completion was proven to be competitive.","2169-3536","","10.1109/ACCESS.2022.3142916","Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology(grant numbers:NRF-2021R1A2B5B01002559); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680719","Depth completion;LIDAR sparse depth;anisotropic diffusion;generative adversarial network;Wasserstein GAN","Generators;Laser radar;Generative adversarial networks;Computer architecture;Feature extraction;Training;Semantics","image colour analysis;image fusion;image segmentation;neural net architecture;optical radar;semi-supervised learning (artificial intelligence)","dense depth map;supervised depth loss;depth completion problem;depth value precision;KITTI depth completion benchmark;anisotropic diffusion depth enhancement;Wasserstein generative adversarial network architecture;sensor fusion;LIDAR sensors;RGB;unsupervised loss function;autoencoder;semantic image segmentation;semi-supervised learning","","2","","42","CCBY","13 Jan 2022","","","IEEE","IEEE Journals"
"PSTAF-GAN: Progressive Spatio-Temporal Attention Fusion Method Based on Generative Adversarial Network","Q. Liu; X. Meng; F. Shao; S. Li","Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; Faculty of Electrical Engineering and Computer Science, Ningbo University, Ningbo, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","14 Apr 2022","2022","60","","1","13","Spatio-temporal fusion aims to integrate multisource remote sensing images with complementary high spatial and temporal resolutions, so as to obtain time-series high spatial resolution fused images. Currently, deep learning (DL)-based spatio-temporal fusion methods have received broad attention. However, on one hand, most of the existing DL-based methods train the model in a band-by-band manner, ignoring the correlations among bands. On the other hand, the general coarse spatio-temporal changes in low spatial resolution images (e.g., MODIS) calculated at the pixel domain cannot completely cover the fine spatio-temporal changes in high spatial resolution images (e.g., Landsat), due to complex surface features and the general large spatial resolution ratio between fine and coarse images. Besides, the existing DL-based spatio-temporal fusion methods are insufficient in exploring multiscale information by only stacking convolutional kernels with different sizes. To alleviate the above challenges, we propose a progressive spatio-temporal attention fusion model in a multiband training manner based on generative adversarial network (PSTAF-GAN). Specifically, we design a flexible multiscale feature extraction architecture to extract multiscale feature hierarchies. Then, spatio-temporal changes are calculated on the feature domain in different feature hierarchies. Besides, a spatio-temporal attention fusion architecture is proposed to fuse the spatio-temporal changes and ground details in a coarse-to-fine manner, which can explore multiscale information more sufficient and gradually recover the target image. The results of quantitative and qualitative experiments on two publicly available benchmark datasets show that the proposed PSTAF-GAN can achieve the best performance compared with the state-of-the-art methods.","1558-0644","","10.1109/TGRS.2022.3161563","National Natural Science Foundation of China(grant numbers:42171326,41801252,62071261); Natural Science Foundation of Zhejiang Province(grant numbers:LY22F010014); Fellowship of China Postdoctoral Science Foundation(grant numbers:2020M672490); K. C. Wong Magna Fund in Ningbo University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740577","Generative adversarial network (GAN);multilevel feature;remote sensing;spatio-temporal attention fusion;weight sharing","Feature extraction;Spatial resolution;Image resolution;Generative adversarial networks;Convolution;Predictive models;Convolutional neural networks","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;image resolution;object detection;remote sensing;spatiotemporal phenomena;time series","multiscale information;progressive spatio-temporal attention fusion model;multiband training manner;generative adversarial network;PSTAF-GAN;flexible multiscale feature extraction architecture;multiscale feature hierarchies;spatio-temporal attention fusion architecture;coarse-to-fine manner;target image;multisource remote sensing images;temporal resolutions;time-series high spatial resolution;deep learning-based spatio-temporal fusion methods;DL-based methods;band-by-band manner;low spatial resolution images;high spatial resolution images;complex surface features","","1","","53","IEEE","23 Mar 2022","","","IEEE","IEEE Journals"
"Remote Sensing Image Spatiotemporal Fusion via a Generative Adversarial Network With One Prior Image Pair","Y. Song; H. Zhang; H. Huang; L. Zhang","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","17 May 2022","2022","60","","1","17","Spatiotemporal fusion (STF) is an effective solution to promote the application of remote sensing images, given that the tradeoff between the temporal resolution and the spatial resolution is ubiquitous in the production of remote sensing images. However, cloud coverage makes it difficult to obtain dense cloud-free Landsat–Moderate Resolution Imaging Spectroradiometer (MODIS) image pairs on the timeline, which limits the application of existing STF methods. Considering the lack of prior image pairs and the huge spatial resolution gap between Landsat and MODIS images, this article presents a novel remote sensing image STF method based on a generative adversarial network to handle one Landsat–MODIS prior image pair case (OPGAN), which contains a generator and a discriminator simultaneously trained in a min–max game. OPGAN is built based on the STF observation model that learns the base information from the prior Landsat image and then captures temporal change (TC) information from a difference image constructed from MODIS images collected at times 1 and 2 and sensor difference information from the difference image between Landsat and MODIS images at time 1. They are combined together to reconstruct the Landsat image at time 2 at both high spatial and high temporal resolution. Moreover, a change loss is proposed to further improve the accuracy of TC prediction. Extensive experiments on the STF dataset illustrate that the proposed OPGAN method can obtain more accurate prediction of spatial information and TCs in the case of insufficient prior information.","1558-0644","","10.1109/TGRS.2022.3171331","National Natural Science Foundation of China(grant numbers:61871298,42071322); Natural Science Foundation of Hubei Province(grant numbers:2020CFA053); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765452","Change loss;generative adversarial network (GAN);one prior image pair;spatiotemporal fusion (STF)","Remote sensing;Earth;Artificial satellites;Spatial resolution;Image resolution;Generative adversarial networks;MODIS","geophysical image processing;image fusion;image resolution;neural nets;radiometry;remote sensing","STF methods;huge spatial resolution gap;MODIS images;novel remote sensing image STF method;generative adversarial network;Landsat-MODIS prior image pair case;STF observation model;prior Landsat image;temporal change information;difference image;high temporal resolution;insufficient prior information;dense cloud-free Landsat-Moderate Resolution Imaging Spectroradiometer;remote sensing image spatiotemporal fusion;OPGAN;high spatial resolution","","1","","54","IEEE","29 Apr 2022","","","IEEE","IEEE Journals"
"TAS2-Net: Triple-Attention Semantic Segmentation Network for Small Surface Defect Detection","T. Liu; Z. He","Guangdong Key Laboratory of IoT Information Technology, Guangzhou, China; Key Laboratory for IoT Intelligent Information Processing and System Integration of Ministry of Education, Guangzhou, China","IEEE Transactions on Instrumentation and Measurement","1 Mar 2022","2022","71","","1","12","Surface defect detection plays a vital role in manufacturing. However, it remains challenging: 1) small samples due to fewer defective products relative to numerous normal products and 2) small defects due to the small size of the defect relative to the large surface. In order to address these problems, this article proposes a triple attention semantic segmentation network for pixel-wise segmentation of surface defects, named as TAS2-Net. In the TAS2-Net framework, for solving the issue of small samples, a generative adversarial network (GAN)-based data augmentation method is proposed to generate defect training samples from a limited number of true defects. Furthermore, for avoiding the loss of small defects, a multilevel features extraction (MFE) module is proposed to extract multi-level features of small defects. Then, a triple context attention module is designed to capture more defect context information for better identifying these small defects. Finally, the focus context module (FCM) is utilized to fuse defect information and achieve pixel-wise segmentation on small defects. Experiments on three actual defect datasets show that the proposed TAS2-Net outperforms the state-of-the-art methods in terms of precision, recall,  ${F}$ -measure, and Intersection over Union (IoU) (DAGM: 86.9%, NEU-DET: 86.3%, Magnetic-Tile: 85.1%).","1557-9662","","10.1109/TIM.2022.3142023","National Natural Science Foundation of China(grant numbers:61773127,U1911401,61727810); Ten Thousand Talent Program approved in 2018; Guangdong Province Foundation(grant numbers:2019B1515120036,501200069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9676674","Deep learning;semantic segmentation;small defects;small samples;surface defect detection","Feature extraction;Semantics;Image segmentation;Training;Deep learning;Data mining;Generative adversarial networks","feature extraction;image fusion;image segmentation;learning (artificial intelligence);neural nets;product quality;production engineering computing","triple-attention semantic segmentation network;small surface defect detection;pixel-wise segmentation;TAS2-Net;generative adversarial network;multilevel feature extraction module;triple context attention module;data augmentation;defect training sample generation;defect information fusion;MFE module","","1","","46","IEEE","11 Jan 2022","","","IEEE","IEEE Journals"
"Cross-Spectral Iris Matching Using Conditional Coupled GAN","M. Mostofa; F. Taherkhani; J. Dawson; N. M. Nasrabadi",West Virginia University; West Virginia University; West Virginia University; West Virginia University,"2020 IEEE International Joint Conference on Biometrics (IJCB)","6 Jan 2021","2020","","","1","9","Cross-spectral iris recognition is emerging as a promising biometric approach to authenticating the identity of individuals. However, matching iris images acquired at different spectral bands shows significant performance degradation when compared to single-band near-infrared (NIR) matching due to the spectral gap between iris images obtained in the NIR and visual-light (VIS) spectra. Although researchers have recently focused on deep-learning-based approaches to recover invariant representative features for more accurate recognition performance, the existing methods cannot achieve the expected accuracy required for commercial applications. Hence, in this paper, we propose a conditional coupled generative adversarial network (CpGAN) architecture for cross-spectral iris recognition by projecting the VIS and NIR iris images into a low-dimensional embedding domain to explore the hidden relationship between them. The conditional CpGAN framework consists of a pair of GAN-based networks, one responsible for retrieving images in the visible domain and other responsible for retrieving images in the NIR domain. Both networks try to map the data into a common embedding subspace to ensure maximum pair-wise similarity between the feature vectors from the two iris modalities of the same subject. To prove the usefulness of our proposed approach, extensive experimental results obtained on the PolyU dataset are compared to existing state-of-the-art cross-spectral recognition methods.","2474-9699","978-1-7281-9186-7","10.1109/IJCB48548.2020.9304929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9304929","","Iris recognition;Iris;Generative adversarial networks;Generators;Gallium nitride;Pattern matching;Training","feature extraction;image fusion;image matching;infrared imaging;iris recognition;learning (artificial intelligence);neural nets","cross-spectral iris matching;conditional coupled GAN;cross-spectral iris recognition;iris images;spectral bands;spectral gap;deep-learning-based approaches;accurate recognition performance;conditional coupled generative adversarial network architecture;conditional CpGAN framework;GAN-based networks;NIR domain;iris modalities","","1","","24","IEEE","6 Jan 2021","","","IEEE","IEEE Conferences"
"Facial Landmarks and Generative Priors Guided Blind Face Restoration","H. Wang; Z. Teng; C. Wu; S. Coleman","Northeastern University, Shenyang, China; Northeastern University, Shenyang, China; Northeastern University, Shenyang, China; Ulster University, Londonderry, U.K.","2022 IEEE 20th International Conference on Industrial Informatics (INDIN)","15 Dec 2022","2022","","","101","106","Blind face restoration (BFR) from severely degraded face images is important in face image processing, and has attracted increasing attention due to its wide applications. How-ever, due to the complex unknown degradations in real-world scenarios, existing priors-based methods tend to restore faces with unstable quality. In this paper, we propose a Facial Landmarks and Generative Priors Guided Blind Face Restoration Network (FGPNet) to seamlessly integrate the advantages of generative priors and face-specific geometry priors. Specifically, we pretrain a high-quality (HQ) face synthesis generative adversarial network (GAN) and a landmarks prediction network, and then embed them into a U-shaped deep neural network (DNN) as decoder priors to guide face restoration, during which the generative priors can provide adequate details and the landmarks priors provide geometry and semantic information. Furthermore, we design facial priors fusion (FPF) blocks to incorporate the prior features from pretrained face synthesis GAN and landmarks prediction network in an adaptive and progressive manner, making our FGPNet exhibits good generalization in real-world application. Experiments demonstrate the superiority of our FGPNet in comparison to state-of-the-arts, and also show its potential in handling real-world low-quality images from several practical applications.","","978-1-7281-7568-3","10.1109/INDIN51773.2022.9976126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9976126","Blind face restoration;deep neural networks;generative adversarial network;facial priors transformation","Geometry;Degradation;Adaptive systems;Semantics;Neural networks;Generative adversarial networks;Image restoration","convolutional neural nets;deep learning (artificial intelligence);face recognition;geometry;image enhancement;image fusion;image restoration","decoder priors;DNN;face image processing;face-specific geometry priors;facial landmarks and generative priors guided blind face restoration network;facial priors fusion blocks;FGPNet;FPF blocks;GAN;generative adversarial network;landmark prediction network;U-shaped deep neural network","","","","30","IEEE","15 Dec 2022","","","IEEE","IEEE Conferences"
"Insulator Defect Detection Based on Feature Fusion and Attention Mechanism","Y. Zhang; B. Wei; L. Zhao; J. Liu; Z. Hao; L. Li; X. Li","School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; Aero Photogrammetry and Remote Sensing Bureau of China Coal, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China; School of Electronic Information, Northwestern Polytechnical University, Xi’an, China","2022 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","23 Dec 2022","2022","","","1","6","The performance of insulator defect detection model is not satisfactory due to the small object size, imbalanced and insufficient data. In this paper, based on YOLOv5 model, we propose an insulator defect detection method incorporating feature fusion and attention mechanism. Firstly, multi-scale feature fusion is introduced to strengthen the ability to extract minute features from images. Secondly, an attention mechanism based on SE-C module is proposed to improve the detection of defective objects. In addition, K-means++ is used to customize anchor boxes to meet the actual requirements and avoid mismatches. The experimental results show that the proposed model achieves 92.4% precision on the public insulator dataset, which demonstrates the applicability of the auto-detection system for insulator defects significantly.","","978-1-6654-6972-2","10.1109/ICSPCC55723.2022.9984418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984418","insulator;defect detection;attention mechanism;object detection","Power transmission lines;Object detection;Signal processing;Insulators;Feature extraction;Generative adversarial networks;Data models","feature extraction;flaw detection;image fusion;image recognition;insulators;neural nets;power engineering computing","anchor boxes;attention mechanism;feature fusion;insulator defect detection model;K-means++;public insulator dataset;small object size;YOLOv5 model","","","","19","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Lightweight Face Detection Algorithm under Occlusion Based on Improved CenterNet","B. Liu; Y. Zhou; J. Li","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","18 Nov 2022","2022","","","803","808","Face detection tasks under the current epidemic prevention situation often acquire images with partial occlusion. General face detectors ignore the challenge brought by occlusion, making it difficult to meet daily needs. In order to address this problem, this paper proposes a real-time occluded face detection network based on the improved CenterNet with information dropping strategy. First, depth separable convolution and attention mechanism are introduced into the backbone to reduce parameters and extract occlusion-robust features. Second, a feature fusion neck is designed to improve the performance of multi-scale face detection. In addition, the data augmentation method with information removal strategy enriches the diversity of occlusion samples. Experiments indicate that our model improves the fps as well as maintains the accuracy.","2577-1655","978-1-6654-5258-8","10.1109/SMC53654.2022.9945612","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945612","Occlusion Face Detection;Multi-scale Feature Fusion;Lightweight Network;CenterNet","Epidemics;Runtime;Convolution;Feature extraction;Generative adversarial networks;Real-time systems;Neck","convolution;face recognition;feature extraction;image fusion;image sensors","attention mechanism;data augmentation method;depth separable convolution;epidemic prevention situation;feature fusion neck;improved CenterNet;information dropping strategy;information removal strategy;lightweight face detection algorithm;multiscale face detection;occlusion-robust features;partial occlusion samples","","","","26","IEEE","18 Nov 2022","","","IEEE","IEEE Conferences"
"Multi-Source Domain Adaptation and Fusion for Speaker Verification","D. Zhu; N. Chen","East China University of Science and Technology, Shanghai, China; East China University of Science and Technology, Shanghai, China","IEEE/ACM Transactions on Audio, Speech, and Language Processing","24 Jun 2022","2022","30","","2103","2116","Since most of the conventional domain adaptation models for Speaker Verification (SV) task apply adaptation from single source domain to the target domain, they may not be robust enough against various mismatches between the samples in the source domain and those in the target domain. In this paper, multiple source domain adaptation and fusion model was studied to solve this problem. First, the x-vector-based SV scheme is pretrained on each source domain. Second, UnShared Network (USN) combining weight regularization loss-based adaptation model is adopted to adapt each pretrained model to the target domain and maintain the speaker-related feature as much as possible. Third, the Registered-Utterance-To-Test-Utterance (RUTTU) similarity matrix is constructed based on the x-vector feature extracted by the adapted model from each source domain. Fourth, the Similarity Network Fusion (SNF) technique is introduced and modified to obtain Modified Similarity Network Fusion (MSNF) scheme, which is used to fuse the RUTTU similarity matrices obtained from multiple adapted models. Finally, speaker matching is achieved by averaging the obtained fused similarities between the test utterance and each of the registered utterances of a specific speaker. Extensive experimental results demonstrate that i) the proposed fusion model outperforms the Multi-source Distilling Domain Adaptation (MDDA) model and Logistic Regression Fusion (LRF) model in SV task; ii) the USN combining weight regularization loss-based adaptation strategy, the MSNF fusion scheme, and the modified speaker matching mechanism all contribute to the performance enhancement; iii) the fusion effectiveness is little influenced by the hyper-parameters setting of MSNF.","2329-9304","","10.1109/TASLP.2022.3182271","National Natural Science Foundation of China(grant numbers:61771196); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9794604","Speaker Verification (SV);multi-source domain fusion;X-vector;Similarity Network Fusion (SNF);Wasserstein Generative Adversarial Network (WGAN)","Adaptation models;Feature extraction;Task analysis;Speech processing;Fuses;Training;Generative adversarial networks","feature extraction;image fusion;learning (artificial intelligence);matrix algebra;regression analysis;speaker recognition","target domain;speaker-related feature;Registered-Utterance-To-Test-Utterance similarity matrix;adapted model;Similarity Network Fusion technique;Modified Similarity Network Fusion scheme;multiple adapted models;Domain Adaptation model;Logistic Regression Fusion model;USN combining weight regularization loss-based adaptation strategy;MSNF fusion scheme;modified speaker;Multisource Domain Adaptation;conventional domain adaptation models;Speaker Verification task apply adaptation;single source domain;multiple source domain adaptation;x-vector-based SV scheme;UnShared Network combining weight regularization loss-based adaptation model;pretrained model","","","","35","IEEE","13 Jun 2022","","","IEEE","IEEE Journals"
"SAdvGAN : Multiple Information Fusion For Adversary Generation","Q. Sun; Z. Yang; P. Li; M. Li; B. Zhao","Network and Data Center, Northwest University, Xi’an, China; Network and Data Center, Northwest University, Xi’an, China; Network and Data Center, Northwest University, Xi’an, China; Network and Data Center, Northwest University, Xi’an, China; Network and Data Center, Northwest University, Xi’an, China","2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)","21 Dec 2021","2021","","","1287","1294","There are many methods for generating adversarial samples. However, most methods do not achieve considerable performances both in the attack accuracy and image realism. In this paper, we generate adversarial samples based on the improved generative adversarial network SAdvGAN. For generator, multidimensional feature fusion is adopted to generate high-quality noises. For discriminator, self-attention is introduced to encourage model to focus on features which benefit the discrimination process. After training with Relativistic average GANs(RaGANs), it reduces the probability of real samples being identified as true. Compared with the current mainstream attack strategies, adversarial samples generated by SAdvGAN perform well in terms of perceived similarity. Concretely, the success rates of SAdvGAN in the semi-white box setting on MNIST and CIFAR10 is higher than that of AdvGAN by 9.68% and 6.1% respectively.","2375-0197","978-1-6654-0898-1","10.1109/ICTAI52525.2021.00203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643179","adversarial samples;generative adversarial network;multi-dimensional feature fusion;self-attention mechanism;attack success rate","Training;Conferences;Generative adversarial networks;Generators;Artificial intelligence","feature extraction;image fusion;learning (artificial intelligence);neural nets;probability","adversarial samples;attack accuracy;image realism;improved generative adversarial network SAdvGAN;multidimensional feature fusion;high-quality noises;multiple information fusion;adversary generation;mainstream attack strategies","","","","33","IEEE","21 Dec 2021","","","IEEE","IEEE Conferences"
"Cross-Domain Association Mining Based Generative Adversarial Network for Pansharpening","L. He; W. Zhang; J. Shi; F. Li","Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China; Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China; School of Automation Science and Engineering, Xi'an Jiaotong University, Xi'an, China; Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology, School of Information and Communications Engineering, Xi'an Jiaotong University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Sep 2022","2022","15","","7770","7783","Multispectral (MS) pansharpening can improve the spatial resolution of MS images, which plays an increasingly important role in agriculture and environmental monitoring. Existing neural network-based methods tend to focus on global features of images, without considering the inherent relationships between similar substances in MS images. However, there is a high probability that different substances at the junction mix with each other, which leads to spectral distortion in the final pansharpened image. In this article, we propose a cross-domain association mining-based generative adversarial network for pansharpening, which consists of a spectral fidelity generator and dual discriminators. In our spectral fidelity generator, the cross-region similarity attention module is designed to establish dependencies between similar substances at different positions in the image, thereby leveraging the similar spectral features to generate pansharpened images with better spectral preservation. To mine the potential relationship between the MS image domain and the panchromatic image domain, we pretrain a spatial information extraction network. The network is then transferred to the dual-discriminator architecture to obtain the spatial information of the pansharpened images more accurately and prevent the loss of spatial details. The experimental results show that our method outperforms several state-of-the-art pansharpening methods in both quantitative and qualitative evaluations.","2151-1535","","10.1109/JSTARS.2022.3204824","National Natural Science Foundation of China(grant numbers:U1903213); Shaanxi Key Laboratory of Deep Space Exploration Intelligent Information Technology(grant numbers:2021SYS-04); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0966); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880485","Deep learning;dual discriminators;image association;multispectral (MS) pansharpening","Pansharpening;Generators;Feature extraction;Spatial resolution;Junctions;Generative adversarial networks;Superresolution","data mining;geophysical image processing;geophysical signal processing;image fusion;image resolution;neural nets;remote sensing","spectral fidelity generator;cross-region similarity attention module;similar substances;similar spectral features;pansharpened images;spectral preservation;MS image domain;panchromatic image domain;spatial information extraction network;state-of-the-art pansharpening methods;multispectral pansharpening;spatial resolution;MS images;neural network-based methods;inherent relationships;different substances;spectral distortion;final pansharpened image;cross-domain association mining-based generative adversarial network","","","","57","CCBY","7 Sep 2022","","","IEEE","IEEE Journals"
"A Comprehensive Real-World Photometric Stereo Dataset for Unsupervised Anomaly Detection","J. Jung; S. Han; J. Park; D. Cho","Department of Electronics Engineering, Chungnam National University, Yuseong-gu, Daejeon, Republic of Korea; Department of Computer Science and Engineering, Chungnam National University, Yuseong-gu, Daejeon, Republic of Korea; School of Computer Science and Engineering, Pusan National University (PNU), Busan, Republic of Korea; Department of Electronics Engineering, Chungnam National University, Yuseong-gu, Daejeon, Republic of Korea","IEEE Access","18 Oct 2022","2022","10","","108914","108923","Unsupervised anomaly detection is a technology that trains a model to detect anomalies using only normal data. One of the main challenges of anomaly detection is detecting defects that are not visible or obscured by lighting conditions. However, most existing methods use a single image as an input to detect anomalies in the target object, thus it is difficult to properly cope with such a challenge. In this paper, we explore an anomaly detection model using multiple images taken under different lighting conditions. For this purpose, we propose a new anomaly detection dataset based on the photometric stereo (PS) setup, and demonstrate its characteristics and advantages. In addition, we apply existing anomaly detection methods to our new dataset to verify their performance and present benchmark results. Furthermore, we present a technique for fusing multi-image and 3D surface information. Anomaly detection based on our new setup using multiple images achieves dramatic improvement of  $+10.15\%$ ,  $+11.68\%$  and  $+4.38\%$  over baseline setup using a single image for Skip-GANomaly, MKDAD and DifferNet models, respectively. Our PS-based anomaly detection dataset and analysis reports will be publicly available for future researches.","2169-3536","","10.1109/ACCESS.2022.3214003","Institute of Information and Communications Technology Planning and Evaluation (IITP); Korea Government (MSIT)(grant numbers:RS-2022-00155857); Artificial Intelligence Convergence Innovation Human Resources Development (Chungnam National University); National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:2021R1A4A1032580,2022R1C1C1009334); BK21 FOUR Program by Chungnam National University Research Grant 2022; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9916256","Anomaly detection;photometric stereo;unsupervised learning","Anomaly detection;Lighting;Generative adversarial networks;Feature extraction;Training;Visualization;Benchmark testing;Unsupervised learning","feature extraction;image fusion;image reconstruction;object detection;security of data;stereo image processing;unsupervised learning","comprehensive real-world photometric stereo dataset;PS-based anomaly detection dataset;multiple images achieves dramatic improvement;anomaly detection methods;photometric stereo setup;anomaly detection model;single image;unsupervised anomaly detection","","","","29","CCBYNCND","12 Oct 2022","","","IEEE","IEEE Journals"
"Wi-Piga: A Personnel-independentMethod For Actions Recognition Based On Wi-Fi","Z. Hao; J. Niu; X. -c. Dang; Z. Qiao","Gansu Province Internet of Things Engineering Research Center, College of Computer Science and Engineering. Northwest Normal University, Lanzhou, China; Gansu Province Internet of Things Engineering Research Center, College of Computer Science and Engineering. Northwest Normal University, Lanzhou, China; Gansu Province Internet of Things Engineering Research Center, College of Computer Science and Engineering. Northwest Normal University, Lanzhou, China; Gansu Province Internet of Things Engineering Research Center, College of Computer Science and Engineering. Northwest Normal University, Lanzhou, China","2021 7th International Conference on Big Data Computing and Communications (BigCom)","1 Oct 2021","2021","","","52","59","In the process of action recognition, due to the difference of channel state information (CSD collected by different experimenters, the identification method can not be Personnel independent. To address this issue, activity recognition method for combining convolutional neural network(CNN) and generative adversarial network(GAN) is put forward. The CSI data of yoga movements are collected, and the CSI data are preprocessed by second-order Butterworth and principal component analysis (PCA) at first. The high-level motion features which are personnel-independent are extracted by the fusion method, and then these motions are classified and recognized At the same time, whether the action is standardis judged. The experimental results show that this method can identify yoga movements successfully and it has nothing to do with people, which is of great significance for family sports guidance and sports experience.","","978-1-6654-4252-7","10.1109/BigCom53800.2021.00021","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9546884","Channel State Information (CSI);Action Recognition;Principal Component Analysis (PCA);Convolutional Neural Networks (CNN);Generative Adversarial Network (GAN)","Neural networks;Generative adversarial networks;Feature extraction;Convolutional neural networks;Personnel;Wireless fidelity;Standards","convolutional neural nets;feature extraction;image classification;image fusion;image motion analysis;principal component analysis;sport;wireless LAN","Wi-Piga;Wi-Fi;action recognition;channel state information;identification method;activity recognition;generative adversarial network;CSI data;yoga movements;fusion method;personnel-independent method;convolutional neural network;CNN;GAN;principal component analysis;PCA;high-level motion features;family sports guidance;sports experience;second-order Butterworth","","","","16","IEEE","1 Oct 2021","","","IEEE","IEEE Conferences"
"Image Comes Dancing With Collaborative Parsing-Flow Video Synthesis","B. Wu; Z. Xie; X. Liang; Y. Xiao; H. Dong; L. Lin","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Dark Matter AI Research, Guangzhou, China","IEEE Transactions on Image Processing","12 Nov 2021","2021","30","","9259","9269","Transferring human motion from a source to a target person poses great potential in computer vision and graphics applications. A crucial step is to manipulate sequential future motion while retaining the appearance characteristic. Previous work has either relied on crafted 3D human models or trained a separate model specifically for each target person, which is not scalable in practice. This work studies a more general setting, in which we aim to learn a single model to parsimoniously transfer motion from a source video to any target person given only one image of the person, named as Collaborative Parsing-Flow Network (CPF-Net). The paucity of information regarding the target person makes the task particularly challenging to faithfully preserve the appearance in varying designated poses. To address this issue, CPF-Net integrates the structured human parsing and appearance flow to guide the realistic foreground synthesis which is merged into the background by a spatio-temporal fusion module. In particular, CPF-Net decouples the problem into stages of human parsing sequence generation, foreground sequence generation and final video generation. The human parsing generation stage captures both the pose and the body structure of the target. The appearance flow is beneficial to keep details in synthesized frames. The integration of human parsing and appearance flow effectively guides the generation of video frames with realistic appearance. Finally, the dedicated designed fusion network ensure the temporal coherence. We further collect a large set of human dancing videos to push forward this research field. Both quantitative and qualitative results show our method substantially improves over previous approaches and is able to generate appealing and photo-realistic target videos given any input person image. All source code and dataset will be released at https://github.com/xiezhy6/CPF-Net.","1941-0042","","10.1109/TIP.2021.3123549","National Key Research and Development Program of China(grant numbers:2020AAA0109700); National Natural Science Foundation of China (NSFC)(grant numbers:U19A2073,61976233); Guangdong Province Basic and Applied Basic Research (Regional Joint Fund-Key)(grant numbers:2019B1515120039); Guangdong Outstanding Youth Fund(grant numbers:2021B1515020061); Shenzhen Fundamental Research Program(grant numbers:RCYX20200714114642083,JCYJ20190807154211365); China Society of Image and Graphics (CSIG) Youth Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606534","Motion transfer;video synthesis;generative model","Solid modeling;Three-dimensional displays;Task analysis;Collaboration;Generative adversarial networks;Training data;Image resolution","computer vision;image fusion;image motion analysis;image sequences;learning (artificial intelligence);solid modelling;video signal processing","input person image;human motion;target person;sequential future motion;appearance characteristic;crafted 3D human models;source video;structured human parsing;appearance flow;realistic foreground synthesis;CPF-Net decouples;human parsing sequence generation;foreground sequence generation;final video generation;human parsing generation stage;video frames;realistic appearance;human dancing videos;photo-realistic target videos;collaborative parsing-flow network;collaborative parsing-flow video synthesis","Algorithms;Humans;Motion","","","52","IEEE","8 Nov 2021","","","IEEE","IEEE Journals"
"MSFSA-GAN: Multi-Scale Fusion Self Attention Generative Adversarial Network for Single Image Deraining","W. Xue; C. Huan-Xin; S. Sheng-Yi; J. Ze-Qin; C. Kai; C. Li","School of Electronic Engineering and Automation, Qingdao, China; School of Electronic Engineering and Automation, Qingdao, China; School of Electronic Engineering and Automation, Qingdao, China; School of Electronic Engineering and Automation, Qingdao, China; School of Electronic Engineering and Automation, Qingdao, China; Xinjiang Technical Institute of Physics & Chemistry, Urumqi, China","IEEE Access","4 Apr 2022","2022","10","","34442","34448","Bad weather such as rainy days will seriously affect the image quality and the accuracy of visual processing algorithm. In order to improve the image deraining quality, a multi-scale fusion self attention generation adversarial network (MSFSA-GAN) is proposed. This network uses different scales to extract input characteristics of rain lines. First, Gaussian pyramid rain maps with different scales are generated by Gaussian algorithm. Then, in order to extract the features of rain lines with different scales, the coarse fusion module and fine fusion module are designed respectively. Next, the extracted features are fused at different scales. In this process, the self attention mechanism is introduced to make the network focus on the extracted features of different scales. And before the fusion, the rain pattern reconstruction operation is also carried out, so that the network can reproduce the input image more perfectly. Finally, it is input into the discriminator network with dense blocks to obtain the image that removes the rain lines. We used R100H and R100L datasets to train and test our network. The results show that our method as high as 27.79 in PSNR and UQI is 0.94, which is superior to the existing methods in performance. Meanwhile, we also compared the cost of time, the result of our network is only 0.02s.","2169-3536","","10.1109/ACCESS.2022.3162224","Major Special Projects of the State Oceanic Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9741786","Rain removal;MSFSA-GAN;self attention;dense block","Rain;Feature extraction;Generators;Generative adversarial networks;Deep learning;Data mining;Task analysis","adaptive filters;feature extraction;Gaussian processes;image enhancement;image fusion;image reconstruction;image restoration;image sequences;rain;wavelet transforms","MSFSA-GAN;attention generative adversarial network;single image deraining;image quality;image deraining quality;rain lines;coarse fusion module;fine fusion module;network focus;input image;discriminator network;multiscale fusion self attention generative adversarial network;visual processing algorithm;Gaussian pyramid rain map;feature extraction;self attention mechanism;rain pattern reconstruction operation;R100H dataset;R100L dataset;PSNR;UQI","","","","32","CCBY","25 Mar 2022","","","IEEE","IEEE Journals"
"RefineDNet: A Weakly Supervised Refinement Framework for Single Image Dehazing","S. Zhao; L. Zhang; Y. Shen; Y. Zhou","School of Software Engineering, Tongji University, Shanghai, China; School of Software Engineering, Tongji University, Shanghai, China; School of Software Engineering, Tongji University, Shanghai, China; Department of Computer and Information Science, University of Macau, Zhuhai, Macau","IEEE Transactions on Image Processing","9 Mar 2021","2021","30","","3391","3404","Haze-free images are the prerequisites of many vision systems and algorithms, and thus single image dehazing is of paramount importance in computer vision. In this field, prior-based methods have achieved initial success. However, they often introduce annoying artifacts to outputs because their priors can hardly fit all situations. By contrast, learning-based methods can generate more natural results. Nonetheless, due to the lack of paired foggy and clear outdoor images of the same scenes as training samples, their haze removal abilities are limited. In this work, we attempt to merge the merits of prior-based and learning-based approaches by dividing the dehazing task into two sub-tasks, i.e., visibility restoration and realness improvement. Specifically, we propose a two-stage weakly supervised dehazing framework, RefineDNet. In the first stage, RefineDNet adopts the dark channel prior to restore visibility. Then, in the second stage, it refines preliminary dehazing results of the first stage to improve realness via adversarial learning with unpaired foggy and clear images. To get more qualified results, we also propose an effective perceptual fusion strategy to blend different dehazing outputs. Extensive experiments corroborate that RefineDNet with the perceptual fusion has an outstanding haze removal capability and can also produce visually pleasing results. Even implemented with basic backbone networks, RefineDNet can outperform supervised dehazing approaches as well as other state-of-the-art methods on indoor and outdoor datasets. To make our results reproducible, relevant code and data are available at https://github.com/xiaofeng94/RefineDNet-for-dehazing.","1941-0042","","10.1109/TIP.2021.3060873","National Natural Science Foundation of China(grant numbers:61973235,61936014,61972285); Natural Science Foundation of Shanghai(grant numbers:19ZR1461300); Shanghai Science and Technology Innovation Plan(grant numbers:20510760400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366772","Single image dehazing;weak supervision;image fusion;unpaired dehazing dataset","Training;Image restoration;Learning systems;Gallium nitride;Atmospheric modeling;Image color analysis;Generative adversarial networks","computer vision;image colour analysis;image denoising;image enhancement;image restoration","haze-free images;vision systems;single image dehazing;computer vision;prior-based methods;learning-based methods;natural results;paired foggy images;clear outdoor images;dehazing task;visibility restoration;two-stage weakly supervised dehazing framework;preliminary dehazing results;unpaired foggy images;dehazing outputs;haze removal capability;weakly supervised refinement framework","","62","","50","IEEE","2 Mar 2021","","","IEEE","IEEE Journals"
"Synthesize High-Quality Multi-Contrast Magnetic Resonance Imaging From Multi-Echo Acquisition Using Multi-Task Deep Generative Model","G. Wang; E. Gong; S. Banerjee; D. Martin; E. Tong; J. Choi; H. Chen; M. Wintermark; J. M. Pauly; G. Zaharchuk","Department of Biomedical Engineering, Tsinghua University, Beijing, China; Department of Electrical Engineering, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Biomedical Engineering, Tsinghua University, Beijing, China; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Biomedical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Electrical Engineering, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA","IEEE Transactions on Medical Imaging","30 Sep 2020","2020","39","10","3089","3099","Multi-echo saturation recovery sequence can provide redundant information to synthesize multi-contrast magnetic resonance imaging. Traditional synthesis methods, such as GE's MAGiC platform, employ a model-fitting approach to generate parameter-weighted contrasts. However, models' over-simplification, as well as imperfections in the acquisition, can lead to undesirable reconstruction artifacts, especially in T2-FLAIR contrast. To improve the image quality, in this study, a multi-task deep learning model is developed to synthesize multi-contrast neuroimaging jointly using both signal relaxation relationships and spatial information. Compared with previous deep learning-based synthesis, the correlation between different destination contrast is utilized to enhance reconstruction quality. To improve model generalizability and evaluate clinical significance, the proposed model was trained and tested on a large multi-center dataset, including healthy subjects and patients with pathology. Results from both quantitative comparison and clinical reader study demonstrate that the multi-task formulation leads to more efficient and accurate contrast synthesis than previous methods.","1558-254X","","10.1109/TMI.2020.2987026","GE Healthcare; Stanford University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9063444","Magnetic resonance imaging (MRI);image synthesis;generative adversarial network (GAN);deep learning (DL);image fusion","Magnetic resonance imaging;Generators;Machine learning;Generative adversarial networks;Training;Image reconstruction;Pathology","biomedical MRI;data acquisition;image reconstruction;image sequences;learning (artificial intelligence);medical image processing","multiecho acquisition;multitask deep generative model;multiecho saturation recovery sequence;parameter-weighted contrasts;T2-FLAIR contrast;image quality;multitask deep learning;multicontrast neuroimaging;reconstruction quality;high-quality multicontrast magnetic resonance imaging;pathology","Artifacts;Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Neuroimaging","17","","61","IEEE","10 Apr 2020","","","IEEE","IEEE Journals"
"GAN-Based Focusing-Enhancement Method for Monochromatic Synthetic Aperture Imaging","G. Ye; Z. Zhang; L. Ding; Y. Li; Y. Zhu","Terahertz Technology Innovation Research Institute, University of Shanghai for Science and Technology, Shanghai, China; Terahertz Technology Innovation Research Institute, University of Shanghai for Science and Technology, Shanghai, China; Terahertz Science Cooperative Innovation Center, Shanghai, China; Shanghai Institute of Intelligent Science and Technology, Tongji University, Shanghai, China; Terahertz Science Cooperative Innovation Center, Shanghai, China","IEEE Sensors Journal","3 Sep 2020","2020","20","19","11484","11489","Two-dimensional (2-D) synthetic aperture imaging with a single frequency suffers from limited depth-of-focus (DOF), and leads to the difficulty of focusing volume targets. In this paper,as opposed to using a wide band for 3-D imaging, this out-of-focus problem is examined as a multi-focal imaging issue. To solve the limited DOF problem, we propose a generative adversarial network (GAN) based focusing-enhancement method (GAN-FEM) to fit an unknown out-of-focus kernel for MMW monochromatic synthetic aperture imaging. To determine which type of MMW-images dataset of input can be better suitable for GAN, the grayscale and pseudo-color images dataset are tested respectively to train the neural network. Proof-of-principle experiments are performed at 94 GHz and the results prove that our proposed GAN-FEM can greatly improve the focusing performance for volume targets. The effectiveness of our proposed method confirms the focusing-enhancement capacity of 2-D monochromatic imaging system for 3-D targets, and provides a possible solution to reduce the system complexity for practical 3-D imaging missions.","1558-1748","","10.1109/JSEN.2020.2996656","National Key Research and Development ProgramofChina(grant numbers:2018YFF01013003); National Natural Science Foundation of China(grant numbers:61731020); Higher Education Discipline Innovation Project(grant numbers:D18014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098931","MMW near field imaging;monochromatic full-focus;SAR;image fusion;GAN-FEM","Imaging;Generators;Sensors;Apertures;Generative adversarial networks;Gallium nitride;Standards","image colour analysis;image enhancement;learning (artificial intelligence);millimetre wave imaging;neural nets;stereo image processing","single frequency suffers;out-of-focus problem;multifocal imaging issue;DOF problem;GAN-FEM;MMW monochromatic synthetic aperture imaging;neural network;focusing-enhancement capacity;3D imaging missions;2D monochromatic imaging system;pseudocolor image dataset;depth-of-focus problem;generative adversarial network-based focusing-enhancement method;frequency 94.0 GHz","","5","","21","IEEE","22 May 2020","","","IEEE","IEEE Journals"
"IR2VI: Enhanced Night Environmental Perception by Unsupervised Thermal Image Translation","S. Liu; V. John; E. Blasch; Z. Liu; Y. Huang","University of British Columbia, BC, Canada; University of British Columbia, BC, Canada; Toyota Institute of Technology, Nagoya, Japan; Air Force Research Laboratory, VA, United States; Chongqing University of Posts and Telecommunications, Chongqing, China","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","1234","12347","Context enhancement is critical for night vision (NV) applications, especially for the dark night situation without any artificial lights. In this paper, we present the infrared-to-visual (IR2VI) algorithm, a novel unsupervised thermal-to-visible image translation framework based on generative adversarial networks (GANs). IR2VI is able to learn the intrinsic characteristics from VI images and integrate them into IR images. Since the existing unsupervised GAN-based image translation approaches face several challenges, such as incorrect mapping and lack of fine details, we propose a structure connection module and a region-of-interest (ROI) focal loss method to address the current limitations. Experimental results show the superiority of the IR2VI algorithm over baseline methods.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575315","","Training;Gallium nitride;Image fusion;Generators;Night vision;Task analysis;Semantics","image processing;infrared imaging;neural nets;night vision;unsupervised learning","unsupervised thermal image translation;context enhancement;night vision applications;dark night situation;infrared-to-visual algorithm;IR images;IR2VI algorithm;enhanced night environmental perception;unsupervised GAN-based image translation;unsupervised thermal-to-visible image translation framework;generative adversarial networks;region-of-interest focal loss method;structure connection module","","12","","30","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"HDR-GAN: HDR Image Reconstruction From Multi-Exposed LDR Images With Large Motions","Y. Niu; J. Wu; W. Liu; W. Guo; R. W. H. Lau","Ministry of Education, Key Laboratory of Spatial Data Mining and Information Sharing, Fujian, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; Department of Computer Science, City University of Hong Kong, Hong Kong","IEEE Transactions on Image Processing","26 Mar 2021","2021","30","","3885","3896","Synthesizing high dynamic range (HDR) images from multiple low-dynamic range (LDR) exposures in dynamic scenes is challenging. There are two major problems caused by the large motions of foreground objects. One is the severe misalignment among the LDR images. The other is the missing content due to the over-/under-saturated regions caused by the moving objects, which may not be easily compensated for by the multiple LDR exposures. Thus, it requires the HDR generation model to be able to properly fuse the LDR images and restore the missing details without introducing artifacts. To address these two problems, we propose in this paper a novel GAN-based model, HDR-GAN, for synthesizing HDR images from multi-exposed LDR images. To our best knowledge, this work is the first GAN-based approach for fusing multi-exposed LDR images for HDR reconstruction. By incorporating adversarial learning, our method is able to produce faithful information in the regions with missing content. In addition, we also propose a novel generator network, with a reference-based residual merging block for aligning large object motions in the feature domain, and a deep HDR supervision scheme for eliminating artifacts of the reconstructed HDR images. Experimental results demonstrate that our model achieves state-of-the-art reconstruction performance over the prior HDR methods on diverse scenes.","1941-0042","","10.1109/TIP.2021.3064433","National Natural Science Foundation of China(grant numbers:61672158,61972097,61702104,62072110); Natural Science Foundation of Fujian Province(grant numbers:2018J07005,2019J02006); GRF from the Research Grants Council of Hong Kong(grant numbers:RGC 11205620); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387148","High dynamic range imaging;generative adversarial networks;multi-exposed imaging","Fuses;Merging;Imaging;Dynamic range;Generators;Image restoration;Image reconstruction","image enhancement;image fusion;image motion analysis;image reconstruction","multiexposed LDR images fusing;deep HDR supervision scheme;object motions;HDR reconstruction;GAN-based approach;synthesizing HDR images;novel GAN-based model;HDR generation model;multiple LDR exposures;missing content;dynamic scenes;multiple low-dynamic range exposures;high dynamic range images;HDR image reconstruction;HDR-GAN;prior HDR methods;reconstructed HDR images","","38","","41","IEEE","25 Mar 2021","","","IEEE","IEEE Journals"
"Auxiliary Loss Multimodal GRU Model in Audio-Visual Speech Recognition","Y. Yuan; C. Tian; X. Lu","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Access","21 Feb 2018","2018","6","","5573","5583","Audio-visual speech recognition (AVSR) utilizes both audio and video modalities for the robust automatic speech recognition. Most deep neural network (DNN) has achieved promising performances in AVSR owing to its generalized and nonlinear mapping ability. However, these DNN models have two main disadvantages: 1) the first disadvantage is that most models alleviate the AVSR problems neglecting the fact that the frames are correlated; and 2) the second disadvantage is the feature learned by the mentioned models is not credible. This is because the joint representation learned by the fusion fails to consider the specific information of categories, and the discriminative information is sparse, while the noise, reverberation, irrelevant image objection, and background are redundancy. Aiming at relieving these disadvantages, we propose the auxiliary loss multimodal GRU (alm-GRU) model including three parts: feature extraction, data augmentation, and fusion & recognition. The feature extraction and data augmentation are a complete effective solution for the processing raw complete video and training, and precondition for later core part: fusion & recognition using alm-GRU equipped with a novel loss which is an end-to-end network combining both fusion and recognition, furthermore considering the modal and temporal information. The experiments show the superiority of our model and necessity of the data augmentation and generative component in the benchmark data sets.","2169-3536","","10.1109/ACCESS.2018.2796118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8279447","Aduio-visual systems;recurrent neural networks;generative adversarial networks","Feature extraction;Hidden Markov models;Automatic speech recognition;Data models;Robustness;Training","audio-visual systems;feature extraction;image fusion;learning (artificial intelligence);neural nets;speech recognition;video signal processing","data augmentation;feature extraction;auxiliary loss multimodal GRU model;audio-visual speech recognition;video modalities;robust automatic speech recognition;deep neural network;generalized mapping ability;nonlinear mapping ability;DNN models;AVSR problems;video processing;alm-GRU model;feature learning;joint representation;fusion & recognition","","22","","48","CCBY","2 Feb 2018","","","IEEE","IEEE Journals"
"ATFaceGAN: Single Face Image Restoration and Recognition from Atmospheric Turbulence","C. P. Lau; H. Souri; R. Chellappa","Center for Automation Research, UMIACS University of Maryland, College Park; Center for Automation Research, UMIACS University of Maryland, College Park; Center for Automation Research, UMIACS University of Maryland, College Park","2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)","18 Jan 2021","2020","","","32","39","Image degradation due to atmospheric turbulence is common while capturing images at long ranges. To mitigate the degradation due to turbulence which includes deformation and blur, we propose a generative single frame restoration algorithm which disentangles the blur and deformation due to turbulence and reconstructs a restored image. The disentanglement is achieved by decomposing the distortion due to turbulence into blur and deformation components using deblur generator and deformation correction generator respectively. Two paths of restoration are implemented to regularize the disentanglement and generate two restored images from one degraded image. A fusion function combines the features of the restored images to reconstruct a sharp image with rich details. Adversarial and perceptual losses are added to reconstruct a sharp image and suppress the artifacts respectively. Extensive experiments demonstrate the effectiveness of the proposed restoration algorithm, which achieves satisfactory performance in face restoration and face recognition.","","978-1-7281-3079-8","10.1109/FG47880.2020.00012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320151","Image Restoration;Face Recognition;Face Verification;Turbulence Mitigation;Generative Adversarial Networks","Image restoration;Face recognition;Strain;Image reconstruction;Degradation;Faces;Training","atmospheric turbulence;face recognition;image fusion;image restoration;neural nets","single face image restoration;atmospheric turbulence;image degradation;generative single frame restoration algorithm;disentanglement;deformation components;deblur generator;deformation correction generator;restored image;sharp image;face restoration;face recognition","","11","","48","IEEE","18 Jan 2021","","","IEEE","IEEE Conferences"
"Recognizing Facial Sketches by Generating Photorealistic Faces Guided by Descriptive Attributes","X. Li; X. Yang; H. Su; Q. Zhou; S. Zheng","Institute of Image Processing and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Processing and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Institute of Image Processing and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Institute of Image Processing and Network Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","23 Dec 2018","2018","6","","77568","77580","Recognizing or retrieve a face based on its sketch-photo similarity has important applications in law enforcement and public security. While many existing methods focus on recognizing facial sketch from image-based queries, this setting has the major limitations in practice, since the abstract sketch only captures sparse global structure of the human face. To address this issue, in this paper, we propose to bridge the gap between the sketch-photo pair by “translating”the abstract visual sketch into a photorealistic face with the help of descriptive attributes. Specifically, we propose an improved multi-modal conditional generative adversarial network (MMC-GAN) to jointly utilize the complementary information of visual sketches and semantic facial attributes to reduce the uncertainties of the facial image generation. A fusion network is introduced to better leverage the information from different modalities (visual sketch and semantic attributes). In order to improve the details of the generated facial images, we adopt a two-path generator structure in which the global feature and the local feature of human faces are learned in parallel. An identitypreserving constraint is further introduced to enhance the identity consistency between the sketches and facial images. Extensive experiments demonstrate that we can effectively manipulate the face image generation by varying the input facial attributes. Besides, the generated photorealistic face image is validated to improve the sketch-photo face recognition and retrieval.","2169-3536","","10.1109/ACCESS.2018.2883463","National Natural Science Foundation of China(grant numbers:61671289,61571261,61771303); Science and Technology Commission of Shanghai Municipality(grant numbers:18DZ2270700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8546745","Generative adversarial networks;human face recognition;multi-modal image generation","Face recognition;Image generation;Image recognition;Facial features;Semantics;Visualization;Gallium nitride","face recognition;image fusion;image retrieval;neural nets","sketch-photo face retrieval;identity preserving constraint;fusion network;MMC-GAN;law enforcement;public security;face retrieval;photorealistic face image generation;facial sketch recognition;image-based queries;sketch-photo similarity;sketch-photo face recognition;two-path generator structure;semantic facial attributes;multimodal conditional generative adversarial network;descriptive attributes;abstract visual sketch;sketch-photo pair;human face;sparse global structure","","2","","53","OAPA","27 Nov 2018","","","IEEE","IEEE Journals"
"Chinese Character Style Transfer Based on the Fusion of Multi-scale Content and Style Features","X. Zhou; Z. Zhang; X. Chen; M. Qin","Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, P. R. China; School of Arts and Communication, China University of Geosciences, Wuhan, P. R. China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, P. R. China; Hubei Key Laboratory of Advanced Control and Intelligent Automation for Complex Systems, Wuhan, P. R. China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","8247","8252","Style transfer has received much attention, and Chinese character style transfer is an important application. In the previous Chinese character style transfer methods, most of the methods separately establish a mapping from source font to target Chinese character for each specific target font. It is impossible to extend a single model to other styles. The model training is based on the amount of Chinese character data from a single font, and the demand of the character is greater. In fact, different font style transfer tasks have certain commonalities. Therefore, we use multiple fonts in the data set and combine these fonts into multiple sets of Chinese character style transfer tasks. These tasks will be trained at the same time, instead of that only one font style transfer task training is achieved at a time like the previous work. In order to achieve the purpose of extracting and fusing the content features and style feature, we propose a style transfer network composed of content encoder, style encoder, feature fusion module, and decoder, based on the fusion of multi-scale content and style features. Through the experimental results, it can be found that, using our method, training the style transfer of multiple fonts at the same time can help any one of them to generate better results with only a few samples.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9550311","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550311","Chinese characters style transfer;Generative adversarial networks;Content and style features","Training;Interference;Feature extraction;Data models;Decoding;Task analysis","character recognition;image fusion","multiple fonts;Chinese character style transfer tasks;style transfer network;multiscale content;source font;Chinese character data","","1","","22","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Multi-Agent Trajectory Prediction With Spatio-Temporal Sequence Fusion","Y. Wang; S. Chen","College of Electronics and Information Engineering, Tongji University, Shanghai, China; Bilibili, Shanghai, China","IEEE Transactions on Multimedia","13 Jan 2023","2023","25","","13","23","Accurate trajectory prediction of surrounding agents is an important issue for building up an intelligent transportation system. Frequent interactions among agents have a major impact on their movement patterns. Current research mainly relies on agents’ spatial structure associated with the last frame of the observation to model social interactions, while paying less attention to structure information from previous moments. In addition, existing methods merely consider temporal features of a single trajectory sequence, while neglecting temporal dependencies across multiple trajectories. In this work, we endeavor to capture comprehensively social interactions among agents with the proposed Spatio-Temporal Sequence Fusion Network (STSF-Net). Specifically, we construct a spatio-temporal sequence that encodes contextual information taking explicitly spatial distributions of agents during movement into account while capturing socially temporal dependencies across multiple trajectory sequences. Besides, a social recurrent mechanism is introduced to explicitly capture temporal correlations between interactions by concerning spatial structure at each time-step. Finally, our model is evaluated on datasets covering pedestrian, vehicle, and heterogeneous multi-agent trajectories. Experimental evidence manifests that our method achieves excellent performance.","1941-0077","","10.1109/TMM.2021.3120535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9580659","Multi-modal trajectory prediction;saptio-temporal sequence fusion;generative adversarial networks;sequence-to-sequence","Analytical models;Correlation;Graphical models;Buildings;Trajectory;Intelligent transportation systems;Distribution functions","computer vision;image fusion;image sequences;intelligent transportation systems;multi-agent systems;spatiotemporal phenomena","computer vision;explicitly spatial distributions;heterogeneous multiagent trajectories;intelligent transportation system;multiagent trajectory prediction;multiple trajectory sequences;single trajectory sequence;social interaction modelling;social recurrent mechanism;socially temporal dependencies;spatial structure;spatio-temporal sequence fusion network;structure information;temporal correlations;temporal features","","","","45","IEEE","19 Oct 2021","","","IEEE","IEEE Journals"
"Multi-Knowledge Fusion Network for Generalized Zero-Shot Learning","H. Xiang; C. Xie; T. Zeng; Y. Yang","National Pilot School of Software, Yunnan University, Kunming, China; National Pilot School of Software, Yunnan University, Kunming, China; National Pilot School of Software, Yunnan University, Kunming, China; National Pilot School of Software, Yunnan University, Kunming, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Suffering from the semantic insufficiency and domain-shift problems, most of existing state-of-the-art methods fail to achieve satisfactory results for Zero-Shot Learning (ZSL). In order to alleviate these problems, we propose a novel generative ZSL method to learn more generalized features from multi-knowledge in semantic-to-visual embedding. In our approach, the proposed Multi-Knowledge Fusion Net-work (MKFNet) alleviates the semantic insufficiency problem by fusing the domain information of different knowledge, which enables more relevant semantic features to be trained for semantic-to-visual feature embedding. The pro-posed knowledge regularization LKR greatly improves the intersection between the synthesized visual features generated by MKFNet and the unseen visual features, which can alleviate the domain-shift problem. Empirically, we show that our approach consistently outperforms these state-of-the-art methods on a large number of available benchmarks on the generalized ZSL (GZSL).","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428278","Yunnan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428278","Image Recognition;Zero-Shot Learning;Generative Adversarial Networks;Knowledge Engineering","Visualization;Image recognition;Conferences;Semantics;Benchmark testing","feature extraction;image fusion;learning (artificial intelligence);object recognition;semantic networks","generalized zero-shot learning;domain-shift problem;generative ZSL method;semantic-to-visual embedding;MKFNet;semantic insufficiency problem;domain information;semantic-to-visual feature embedding;knowledge regularization LKR;unseen visual features;multiknowledge fusion network","","","","25","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"Spatial Fusion GAN for Image Synthesis","F. Zhan; H. Zhu; S. Lu","Nanyang Technological University, Singapore; Institute for Infocomm, A*STAR, Singapore; Nanyang Technological University, Singapore","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","3648","3657","Recent advances in generative adversarial networks (GANs) have shown great potentials in realistic image synthesis whereas most existing works address synthesis realism in either appearance space or geometry space but few in both. This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a geometry synthesizer and an appearance synthesizer to achieve synthesis realism in both geometry and appearance spaces. The geometry synthesizer learns contextual geometries of background images and transforms and places foreground objects into the background images unanimously. The appearance synthesizer adjust the color, brightness and styles of the foreground objects and embeds them into background images harmoniously, where a guided filter is incorporated for detail preserving. The two synthesizers are inter-connected as mutual references which can be trained end-to-end with little supervision. The SF-GAN has been evaluated in two tasks: (1) realistic scene text image synthesis for training better recognition models; (2) glass and hat wearing for realistic matching glasses and hats with real portraits. Qualitative and quantitative comparisons with the state-of-the-art demonstrate the superiority of the proposed SF-GAN.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953967","Image and Video Synthesis;Deep Learning;Document Analysis","Geometry;Training;Image recognition;Image synthesis;Text recognition;Synthesizers;Computational modeling","brightness;computational geometry;image colour analysis;image filtering;image fusion;image matching;image representation;learning (artificial intelligence);realistic images;text detection","foreground objects;background images;appearance synthesizer;SF-GAN;hats;generative adversarial networks;realistic scene text image synthesis;geometry synthesizer;appearance spaces;contextual geometries;spatial fusion GAN;realistic matching;glasses;color adjustment;brightness adjustment;foreground object style adjustment;guided filter;detail preserving","","55","2","55","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Stagewise Unsupervised Domain Adaptation With Adversarial Self-Training for Road Segmentation of Remote-Sensing Images","L. Zhang; M. Lan; J. Zhang; D. Tao","School of Computer Science, Institute of Artificial Intelligence, Wuhan University, Wuhan, China; School of Computer Science, Institute of Artificial Intelligence, Wuhan University, Wuhan, China; School of Computer Science, Faculty of Engineering, The University of Sydney, Sydney, NSW, Australia; JD Explore Academy, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","28 Jan 2022","2022","60","","1","13","Road segmentation from remote-sensing images is a challenging task with wide ranges of application potentials. Deep neural networks have advanced this field by leveraging the power of large-scale labeled data, which, however, are extremely expensive and time-consuming to acquire. One solution is to use cheap available data to train a model and deploy it to directly process the data from a specific application domain. Nevertheless, the well-known domain shift (DS) issue prevents the trained model from generalizing well on the target domain. In this article, we propose a novel stagewise domain adaptation model called RoadDA to address the DS issue in this field. In the first stage, RoadDA adapts the target domain features to align with the source ones via generative adversarial networks (GANs)-based interdomain adaptation. Specifically, a feature pyramid fusion module is devised to avoid information loss of long and thin roads and learn discriminative and robust features. Besides, to address the intradomain discrepancy in the target domain, in the second stage, we propose an adversarial self-training method. We generate the pseudo labels of the target domain using the trained generator and divide it to labeled easy split and unlabeled hard split based on the road confidence scores. The features of hard split are adapted to align with the easy ones using adversarial learning and the intradomain adaptation process is repeated to progressively improve the segmentation performance. Experiment results on two benchmarks demonstrate that RoadDA can efficiently reduce the domain gap and outperforms state-of-the-art methods. The code is available at https://github.com/LANMNG/RoadDA.","1558-0644","","10.1109/TGRS.2021.3104032","National Natural Science Foundation of China(grant numbers:62076188); Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies)(grant numbers:2019AEA170); Fundamental Research Funds for the Central Universities(grant numbers:2042021kf0196); supercomputing system in the Supercomputing Center of Wuhan University; Australian Research Council (ARC)(grant numbers:FL-170100117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516689","Remote sensing (RS);road segmentation;self-training;unsupervised domain adaptation (UDA)","Roads;Image segmentation;Adaptation models;Task analysis;Data models;Feature extraction;Predictive models","feature extraction;image fusion;image segmentation;learning (artificial intelligence);neural nets;remote sensing;road traffic;traffic engineering computing","deep neural networks;RoadDA;DS;generative adversarial networks-based interdomain adaptation;feature pyramid fusion module;road confidence scores;adversarial learning;intradomain adaptation process;road segmentation;remote-sensing images;stagewise unsupervised domain adaptation;adversarial self-training","","32","","49","IEEE","18 Aug 2021","","","IEEE","IEEE Journals"
"Generating a Fusion Image: One's Identity and Another's Shape","D. Joo; D. Kim; J. Kim","KAIST, School of Electrical Engineering, South Korea; KAIST, School of Electrical Engineering, South Korea; KAIST, School of Electrical Engineering, South Korea","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","1635","1643","Generating a novel image by manipulating two input images is an interesting research problem in the study of generative adversarial networks (GANs). We propose a new GAN-based network that generates a fusion image with the identity of input image x and the shape of input image y. Our network can simultaneously train on more than two image datasets in an unsupervised manner. We define an identity loss LI to catch the identity of image x and a shape loss LS to get the shape of y. In addition, we propose a novel training method called Min-Patch training to focus the generator on crucial parts of an image, rather than its entirety. We show qualitative results on the VGG Youtube Pose dataset, Eye dataset (MPIIGaze and UnityEyes), and the Photo-Sketch-Cartoon dataset.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00176","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578274","","Shape;Generators;Training;Gallium nitride;Task analysis;YouTube;Face","computer vision;feature extraction;image classification;image fusion;neural nets;pose estimation;unsupervised learning","generative adversarial networks;GAN-based network;image datasets;identity loss;min-patch training;shape loss;VGG Youtube Pose dataset;Eye dataset;MPIIGaze dataset;UnityEyes dataset;Photo-Sketch-Cartoon dataset;fusion image generation","","23","","26","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Object Recognition at Night Scene Based on DCGAN and Faster R-CNN","K. Wang; M. Z. Liu","College of Electronic Information and Automation, Civil Aviation University of China, Tianjin, China; College of Electronic Information and Automation, Civil Aviation University of China, Tianjin, China","IEEE Access","29 Oct 2020","2020","8","","193168","193182","In the past few years, with the rapid development of computer execution capabilities, target recognition strategies based on convolutional neural networks have become mainstream algorithms in the field of object detection. However, due to the blurred background and dim light, the object detection task in the night environment still faces greater visual challenges. This article is strongly inspired by DCGAN (Deep Convolution Generative Adversarial Networks). We use night images as input, generate virtual target scenes similar to the daytime environment through game training of generators and discriminators; and to obtain high-precision detection results, we combine the currently very advanced Faster R-CNN (Region-based Convolution Neural Networks) target detection system, through deep convolution feature fusion and multi-scale ROI (Region Of Interest) pooling. A series of experimental results show that our method achieves an mAP of 82.6% in the detection of its own night scene dataset, which is significantly higher than the original Faster R-CNN alone of 80.4%. Therefore, our method can meet the actual needs of target detection tasks in night scene. We sincerely hope that our approach will contribute to future research.","2169-3536","","10.1109/ACCESS.2020.3032981","National Natural Science Foundation of China(grant numbers:U1733119); Central University Basic Scientific Research Business Fee Project of Civil Aviation University of China(grant numbers:3122018C001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9235491","Convolutional neural network;DCGAN;faster R-CNN;night scene;object detection","Object detection;Convolution;Feature extraction;Task analysis;Training;Generators;Object recognition","convolutional neural nets;feature extraction;image fusion;night vision;object detection;object recognition","object recognition;DCGAN;computer execution capabilities;target recognition;object detection;night environment;deep convolution generative adversarial networks;night images;virtual target scenes;daytime environment;generators;discriminators;region-based convolution neural networks;deep convolution feature fusion;night scene;faster R-CNN;target detection;game training;high-precision detection;multiscale ROI;region of interest","","15","","33","CCBY","22 Oct 2020","","","IEEE","IEEE Journals"
"Pixel-Wise Wasserstein Autoencoder for Highly Generative Dehazing","G. Kim; S. W. Park; J. Kwon","School of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea; School of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea; School of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea","IEEE Transactions on Image Processing","9 Jun 2021","2021","30","","5452","5462","We propose a highly generative dehazing method based on pixel-wise Wasserstein autoencoders. In contrast to existing dehazing methods based on generative adversarial networks, our method can produce a variety of dehazed images with different styles. It significantly improves the dehazing accuracy via pixel-wise matching from hazy to dehazed images through 2-dimensional latent tensors of the Wasserstein autoencoder. In addition, we present an advanced feature fusion technique to deliver rich information to the latent space. For style transfer, we introduce a mapping function that transforms existing latent spaces to new ones. Thus, our method can produce highly generative haze-free images with various tones, illuminations, and moods, which induces several interesting applications, including low-light enhancement, daytime dehazing, nighttime dehazing, and underwater image enhancement. Experimental results demonstrate that our method quantitatively outperforms existing state-of-the-art methods for synthetic and real-world datasets, and simultaneously generates highly generative haze-free images, which are qualitatively diverse.","1941-0042","","10.1109/TIP.2021.3084743","National Research Foundation of Korea(grant numbers:NRF-2020R1C1C1004907); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447190","Dehazing;wasserstein autoencoder;image enhancement","Tensors;Image enhancement;Lighting;Network architecture;Estimation;Channel estimation;Transforms","feature extraction;geophysical image processing;image colour analysis;image denoising;image enhancement;image fusion;image matching;image representation;image resolution;image restoration","pixel-wise wasserstein autoencoder;highly generative dehazing method;pixel-wise Wasserstein autoencoders;existing dehazing methods;generative adversarial networks;dehazed images;dehazing accuracy;pixel-wise matching;2-dimensional latent tensors;advanced feature fusion technique;latent space;highly generative haze-free images;daytime dehazing;nighttime dehazing;underwater image enhancement","","12","","54","IEEE","4 Jun 2021","","","IEEE","IEEE Journals"
"SoilingNet: Soiling Detection on Automotive Surround-View Cameras","M. Uřičář; P. Křížek; G. Sistu; S. Yogamani","Valeo R&D Prague, Czech Republic; Valeo R&D Prague, Czech Republic; Valeo Visions Systems, Ireland; Valeo Visions Systems, Ireland","2019 IEEE Intelligent Transportation Systems Conference (ITSC)","28 Nov 2019","2019","","","67","72","Cameras are an essential part of sensor suite in autonomous driving. Surround-view cameras are directly exposed to external environment and are vulnerable to get soiled. Cameras have a much higher degradation in performance due to soiling compared to other sensors. Thus it is critical to accurately detect soiling on the cameras, particularly for higher levels of autonomous driving. We created a new dataset having multiple types of soiling namely opaque and transparent. It will be released publicly as part of our WoodScape dataset [15] to encourage further research. We demonstrate high accuracy using a Convolutional Neural Network (CNN) based architecture. We also show that it can be combined with the existing object detection task in a multi-task learning framework. Finally, we make use of Generative Adversarial Networks (GANs) to generate more images for data augmentation and show that it works successfully similar to the style transfer.","","978-1-5386-7024-8","10.1109/ITSC.2019.8917178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917178","","Cameras;Task analysis;Lenses;Gallium nitride;Autonomous vehicles;Degradation;Rain","cameras;convolutional neural nets;image fusion;learning (artificial intelligence);object detection;traffic engineering computing","data augmentation;generative adversarial networks;multitask learning framework;sensor suite;automotive surround-view cameras;soiling detection;object detection task;convolutional neural network based architecture;WoodScape dataset;autonomous driving","","7","","17","IEEE","28 Nov 2019","","","IEEE","IEEE Conferences"
"Deep Learning for SAR-Optical Image Matching","L. H. Hughes; N. Merkle; T. Bürgmann; S. Auer; M. Schmitt","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; Airbus Defence and Space GmbH, Immenstaad, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4877","4880","The automatic matching of corresponding regions in remote sensing imagery acquired by synthetic aperture radar (SAR) and optical sensors is a crucial pre-requesite for many data fusion endeavours such as target recognition, image registration, or 3D-reconstruction by stereogrammetry. Driven by the success of deep learning in conventional optical image matching, we have carried out extensive research with regard to deep matching for SAR-optical multi-sensor image pairs in the recent past. In this paper, we summarize the achieved findings, including different concepts based on (pseudo-)siamese convolutional neural network architectures, hard negative mining, alternative formulations of the underlying loss function, and creation of artificial images by generative adversarial networks. Based on data from state-of-the-art remote sensing missions such as TerraSAR-X, Prism, Worldview-2, and Sentinel-1/2, we show what is already possible today, while highlighting challenges to be tackled by future research endeavors.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898635","Deep Learning;Image Matching;Optical Images;SAR Images;Data Fusion","Optical imaging;Optical sensors;Image matching;Optical fiber networks;Training data;Training;Deep learning","convolutional neural nets;image fusion;image matching;learning (artificial intelligence);optical images;optical sensors;radar imaging;remote sensing;synthetic aperture radar","remote sensing imagery;optical sensors;data fusion;deep learning;SAR-optical multisensor image pairs;artificial images;SAR-optical image matching;synthetic aperture radar;Siamese convolutional neural network architectures","","23","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Underwater Image Enhancement Method Based on Dark Channel Prior and Guided Filtering","H. Song; W. Xia; J. Kang; S. Zhang; C. Ye; W. Kang; T. Teik Toe","School of Information Science and Engineering, Harbin Institute of Technology, Weihai, Weihai, China; School of Information Science and Engineering, Harbin Institute of Technology, Weihai, Weihai, China; School of Information Science and Engineering, Harbin Institute of Technology, Weihai, Weihai, China; School of Information Science and Engineering, Harbin Institute of Technology, Weihai, Weihai, China; College of New Energy, Harbin Institute of Technology, Weihai, Weihai, China; Jinxi Defense Equipment Research Institute, Jinxi Industries Group Co.,Ltd., Taiyuan, China; NTU Business AI Lab Nanyang Technological University, Singapore","2022 International Conference on Automation, Robotics and Computer Engineering (ICARCE)","22 Feb 2023","2022","","","1","7","This paper presents a comprehensive enhancement method based upon Underwater Dark Channel Prior (UDCP) and Guided Filtering for standard RGB underwater images without depth information. Firstly, color compensation and Gray World Algorithm are used to correct the color of images obtained underwater. After that, the restored image is dehazed by using the optimized dehazing algorithm created on UDCP. The dehazing algorithm proposed in this study is obtained by reconstructing the ambient light transmittance expression in UDCP. It effectively avoids the “excessive dehazing” caused by traditional dehazing algorithms, and it can also optimize the depth of field of dehazed images. At the same time, due to the complexity of underwater dark channel image dehazing, the dehazed image will still produce fuzzy white areas in zones with large pixel color difference changes (such as the boundary of objects). Therefore, our method adds an image fusion approach built upon guided filtering to optimize the dehazed image to eliminate the white areas to enhance the image clarity further. At last, this paper compares the image enhancement effect of our method with that of other four methods such as Unified Generative Adversarial Networks (UGAN) by using five objective image evaluation indexes such as Underwater Color Image Quality Evaluation (UCIQE).","","978-1-6654-7548-8","10.1109/ICARCE55724.2022.10046569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10046569","dark channel prior;underwater image enhancement;color compensation;guided filtering","Performance evaluation;Filtering;Image color analysis;Image edge detection;Image restoration;Indexes;Image enhancement","","","","","","11","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
