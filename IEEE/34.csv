"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Synthetic aperture radar image change detection based on an image fusion strategy","Z. Zhao; Z. Zhu; G. Chen; J. Zhao","North Automatic Control Technology Insititute, Taiyuan, China; North Automatic Control Technology Insititute, Taiyuan, China; Unit 32381, Beijing, China; College of Geoexploration Science and Technology, Jilin University, Changchun, China","2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)","12 Jan 2023","2022","","","151","155","Change detection is a hot topic and of great importance in remote sensing. The logarithm operation can be an effective means of eliminating the influence of multiplicative noise in the synthetic aperture radar (SAR) image. However, due to the nature of the logarithmic function, regions of change with high gray values are attenuated. In this study, we propose SAR to detect image changes based on an image fusion strategy by combining two different methods. First, we reduce speckle by non-subsampled shearlet transform in the log-domain, since this logarithmic function has the advantage of transforming multiplicative speckle noise into additive noise. As we know, the difference method always detects the areas with the largest changes. The difference method based on saliency extraction (SE) is applied to the exponent transformational SAR image to complement the disadvantage of WLR. WLR is used to obtain a change map of WLR to reduce the influence of SE when WLR does not detect the changed areas. Finally, two change maps can be added to obtain the final result. Experimental results for real SAR image pairs show the effectiveness of the proposed method in terms of detection rate, false alarm rate, and overall accuracy.","","978-1-6654-6468-0","10.1109/ICICML57342.2022.10009685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009685","Change detection;image fusion strategy;non-subsampled shearlet transform;saliency extraction;synthetic aperture radar","Additive noise;Radar detection;Transforms;Speckle;Radar imaging;Radar polarimetry;Synthetic aperture radar","geophysical image processing;image denoising;image fusion;object detection;radar imaging;speckle;synthetic aperture radar;transforms","additive noise;change map;changed areas;detection rate;difference method;exponent transformational SAR image;high gray values;image changes;image fusion strategy;largest changes;logarithm operation;logarithmic function;multiplicative noise;multiplicative speckle noise;remote sensing;SAR image pairs;synthetic aperture radar image change detection;WLR","","","","11","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Multi-angle SAR image fusion algorithm based on visibility classification of non-layover region targets","S. Zhu; D. Ran","School of Space Information, Space Engineering University, Beijing, China; School of Space Information, Space Engineering University, Beijing, China","2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)","1 Mar 2018","2017","","","642","647","In order to reduce the layover region in traditional single-angle synthetic aperture radar (SA-SAR) image of mountainous area, a multi-angle synthetic aperture radar (MA-SAR) image fusion algorithm based on visibility classification of non-layover region targets is proposed. By defining a practical index named multi-angle visibility of non-layover region targets which used for automatic pixel classification, the proposed algorithm calculates the visibility index of every pixel of SA-SAR images obtained from different observation angles and fuses those pixels with the same visibility index to form the fused image. The fused image can effectively eliminate all adverse effects on target detection and classification which caused by the phenomenon of layover, and realize a precision MA-SAR imaging of mountainous area. The simulation results have verified the effectiveness of the proposed algorithm.","","978-1-5386-3016-7","10.1109/SPAC.2017.8304355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8304355","Multi-angle synthetic aperture radar (MA-SAR);Image fusion;Multi-angle visibility index;Automatic pixel classification","Filtering;Indexes;Synthetic aperture radar;Image fusion;Image segmentation;Speckle;Imaging","geophysical image processing;image classification;image fusion;object detection;radar imaging;radar resolution;remote sensing by radar;synthetic aperture radar;visibility","multiangle SAR image fusion algorithm;visibility classification;layover region;single-angle synthetic aperture radar image;mountainous area;multiangle synthetic aperture radar image fusion algorithm;multiangle visibility;automatic pixel classification;visibility index;SA-SAR images;target detection;precision MA-SAR imaging;observation angles;image fusion;nonlayover region targets","","4","","9","IEEE","1 Mar 2018","","","IEEE","IEEE Conferences"
"High Resolution SAR and Panchromatic Image Fusion based on Bidimensional Empirical Mode Decomposition","T. Xu; T. Wu; H. Xiang","Key Laboratory of Aperture Array and Space Application, No. 38 Research Institute China Electronics Technology Group Corporation, Hefei, China; Key Laboratory of Aperture Array and Space Application, No. 38 Research Institute China Electronics Technology Group Corporation, Hefei, China; Key Laboratory of Aperture Array and Space Application, No. 38 Research Institute China Electronics Technology Group Corporation, Hefei, China","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","30 Mar 2020","2019","","","1","5","Bidimensional empirical mode decomposition (BEMD) method is considered advantageous for analyzing non-stationary and non-linear signals. Recently, it has been introduced in non-stationary high resolution Synthetic Aperture Radar (SAR) image processing. This letter proposes a new method for fusing high resolution Synthetic Aperture Radar (SAR) and Panchromatic images, based on BEMD method. Under this method, first, multi-resolution decomposition images are obtained, from the original images based on BEMD algorithm, the features of the original images are separated into multiple scales of spatial frequencies, called intrinsic mode functions (IMF), and then an area-based image fusion scheme is applied to fuse the images at each decomposition level. Experimental results from GaoFen-1 (GF-1) Panchromatic and COSMO-SkyMed SAR images show that the proposed method not only preserves the high spatial resolution of Panchromatic image, but also combines the target and surface information in SAR image, which are difficult to identify in the Panchromatic image. The new algorithm outperforms the wavelet transform and non-subsampled contourlet transform (NSCT), in terms of both visual effect and quantitative analysis.","2474-2333","978-1-7281-2912-9","10.1109/APSAR46974.2019.9048373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048373","Image fusion;Empirical mode decomposition (EMD);Area-based fusion scheme;Laplace filter","","Hilbert transforms;image fusion;image resolution;radar imaging;radar resolution;synthetic aperture radar","NSCT;wavelet transform;GF-1 panchromatic image fusion;IMF;nonstationary signal analysis;nonlinear signal analysis;nonstationary high resolution synthetic aperture radar image processing;multiresolution decomposition imaging;GaoFen-1 panchromatic image fusion;high resolution SAR image fusion;nonsubsampled contourlet transform;high spatial image resolution;COSMO-SkyMed SAR images;area-based image fusion scheme;intrinsic mode functions;BEMD algorithm;bidimensional empirical mode decomposition method","","","","22","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"Shadow Compensation for Synthetic Aperture Radar Target Classification by Dual Parallel Generative Adversarial Network","H. Zhu; R. Leung; M. Hong","School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen, China; School of Engineering, The University of Tokyo, Tokyo, Japan; Electrical and Computer Engineering, University of Minnesota, Minneapolis, USA","IEEE Sensors Letters","20 Jul 2020","2020","4","8","1","4","Due to the incident angle of synthetic aperture radar electromagnetic wave, the ground target is usually captured with some parts missing in the raw SAR image instead of an area of shadow. In recent years, most of the deep learning methods on SAR target classification problems are only applied to the raw SAR images, which require a large number of images to train the neural network for a good result. Aiming at this problem, we propose a novel method for reconstructing the complete target profile to study the SAR target classification problem, which merges two raw SAR images with the opposite azimuth together to one new image. Then, a dual parallel generative adversarial network is proposed to extend the fused SAR image dataset. Finally, we construct a convolutional neural network to train the extended fused image dataset, which outputs the label for each ground target. The experimental result shows that the whole network framework conducted on the MSTAR dataset achieves the average classification accuracy of 99.93% with far fewer data than the state-of-the-art methods, which is a breakthrough in the research process for the SAR target classification with limited labeled data.","2475-1472","","10.1109/LSENS.2020.3009179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9140351","Sensor signals processing;generative adversarial network (GAN);MSTAR;synthetic aperture radar (SAR);target classification;shadow","Synthetic aperture radar;Training;Gallium nitride;Azimuth;Generative adversarial networks;Generators;Neural networks","convolutional neural nets;image classification;image fusion;learning (artificial intelligence);radar imaging;synthetic aperture radar","SAR target classification problem;raw SAR image;target profile;dual parallel generative adversarial network;fused SAR image dataset;convolutional neural network;extended fused image dataset;ground target;shadow compensation;synthetic aperture radar target classification;deep learning methods;electromagnetic wave;MSTAR dataset","","16","","13","IEEE","14 Jul 2020","","","IEEE","IEEE Journals"
"Airborne SAR and optical image fusion based on IHS transform and joint non-negative sparse representation","C. Liu; Y. Qi; W. Ding","State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and System, Beihang University, Beijing, China; Research Institute of Unmanned Aerial Vehicle, Beihang University, Beijing, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7196","7199","In this paper, a novel airborne synthetic aperture radar (SAR) and optical image fusion method based on Intensity-Hue-Saturation (IHS) and joint non-negative sparse representation (JNNSR) is proposed. Firstly, the color optical image is transformed into IHS space. Then, the intensity component of the optical image and the SAR image are decomposed by JNNSR into common component and innovation components. Based on the non-negative property of the sparse coefficients, the innovation component of the SAR image is fused with the intensity component of the optical image. Finally, the fused result is obtained by performing inverse IHS transform. The experimental result shows that our method is superior to the traditional methods in terms of several universal quality evaluation indexes, as well as in the visual quality.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730877","Airborne SAR image;Airborne optical image;Image fusion;IHS;Joint sparse representation","Dictionaries;Optical imaging;Optical sensors;Image fusion;Transforms;Synthetic aperture radar;Adaptive optics","airborne radar;image fusion;inverse transforms;synthetic aperture radar","airborne SAR image;inverse IHS transform;joint non-negative sparse representation;novel airborne synthetic aperture radar;optical image fusion method;intensity-hue-saturation transform;JNNSR;intensity component","","3","","11","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"SAR image segmentation for land cover change detection","A. Das; A. Sahi; U. Nandini","Department of Computer Science, Sathyabama university, Chennai, India; Department of Computer Science, Sathyabama university, Chennai, India; Faculty of computing, Sathyabama University, Chennai, India","2016 Online International Conference on Green Engineering and Technologies (IC-GET)","4 May 2017","2016","","","1","6","Synthetic aperture radar is used for land cover change detection which can either be mounted on a drone or an aircraft, spacecraft. It can be used for land cover change detection by comparing two images which are taken at different intervals of time. For this we are using differencing methods. The operators used in differencing methods are log ratio and mean ratio. For obtaining a better difference in image. The image fusion technique is applied using complementary information obtained from log ratio and mean ratio differenced image then the image is segmented using k means clustering algorithms in which k centroids are placed as one for each cluster such that they are at maximum distance away from each other. The image obtained is compared with the ground truth, which has been already implemented. Now if the image is normal there is no change, and if the image is abnormal then the change has occurred during that time interval.","","978-1-5090-4556-3","10.1109/GET.2016.7916810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916810","Change detection;Image fusion;Non sub sampled contoured transform (NSCT);Synthetic Aperture Radar (SAR)","Synthetic aperture radar;Image fusion;Transforms;Image segmentation;Clustering algorithms;Image edge detection;Radar imaging","geophysical image processing;image fusion;image segmentation;land cover;remote sensing by radar;synthetic aperture radar","image fusion technique;synthetic aperture radar image segmentation;land cover change detection","","6","","24","IEEE","4 May 2017","","","IEEE","IEEE Conferences"
"Fusion of multispectral and SAR images using sparse representation","H. Zhang; H. Shen; L. Zhang","School of Resource and Environmental Sciences, Wuhan University, P. R. China; School of Resource and Environmental Sciences, Wuhan University, P. R. China; Mapping and Remote Sensing, Wuhan University, P. R. China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7200","7203","Complementary information from multi-sensor can be integrated to effectively solve many problems in remote sensing application. Synthetic Aperture Radar (SAR) imaging can be a feasible alternative to traditional optical remote sensing techniques because it is independent of solar illumination and weather conditions. This paper proposes a novel fusion framework combining IHS transform with sparse representation theory to fuse multispectral and SAR images. In addition, the simultaneous orthogonal matching pursuit (SOMP) technique is introduced to guarantee the efficiency. Experiments on various datasets have verified the effectiveness of proposed method.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730878","Image fusion;Synthetic Aperture Radar;sparse representation;simultaneous orthogonal matching pursuit","Synthetic aperture radar;Remote sensing;Image fusion;Optical imaging;Optical sensors;Transforms;Adaptive optics","geophysical image processing;image fusion;remote sensing by radar;synthetic aperture radar","multispectral image fusion;SAR image fusion;multisensor complementary information;remote sensing application;synthetic aperture radar;traditional optical remote sensing techniques;weather conditions;solar illumination;novel fusion framework;IHS transform;simultaneous orthogonal matching pursuit;SOMP technique","","4","","11","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"SAR Image Fusion Based on Mathematical Frame","Y. Liu; G. Liao; S. Li; G. Nie; C. Zeng; P. Du","National Laboratory of Radar Signal Processing, XiDian University, Xi'an, China; National Laboratory of Radar Signal Processing, XiDian University, Xi'an, China; San Francisco State University, San Francisco, The United States of America; International Cooperation Base of Integrated Electronic Information System of Ministry of Science and Technology, XiDian University, Xi'an, China; Radar Cognitive Detection, Imaging and Target Recognition Overseas Expertises introduction Center for Discipline innovation, XiDian University, Xi'an, China; National Laboratory of Radar Signal Processing, XiDian University, Xi'an, China","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","30 Mar 2020","2019","","","1","6","In this paper, we proposed a frame-based approach to synthetic aperture radar (SAR) images fusion. Specifically, the method consists two parts: image fusion preprocessing and fusion reconstruction. Firstly, we fuse a set of low-resolution (LR) SAR images to form an aligned coarse SAR image which still has image blur caused by the point spread function (PSF) by fusion preprocessing. Then Frame fundamental iterative regularization is used to reconstruct high-resolution (HR) SAR image. Both simulated and real SAR images are used to verify the validity of the method. The results show that details and definition of the LR image processed with the method are effectively improved.","2474-2333","978-1-7281-2912-9","10.1109/APSAR46974.2019.9048460","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048460","frame;SAR;regularization;image fusion","","image fusion;image reconstruction;image resolution;iterative methods;optical transfer function;radar imaging;synthetic aperture radar","real SAR images;LR image;SAR image fusion;frame-based approach;synthetic aperture radar images fusion;image fusion preprocessing;fusion reconstruction;low-resolution SAR images;aligned coarse SAR image;image blur;iterative regularization;simulated SAR images;point spread function;high-resolution SAR image reconstruction","","","","6","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"A Deep Learning Approach for SAR Image Fusion","M. Rout; S. Nahak; S. Priyadarshinee; P. Mohapatra; K. D. Sa; D. Dash","Electronics and Telecommunication Engineering, Indira Gandhi Institute of Technology, Odisha, India; Electronics and Telecommunication Engineering, Indira Gandhi Institute of Technology, Odisha, India; Electronics and Telecommunication Engineering, Indira Gandhi Institute of Technology, Odisha, India; Electronics and Telecommunication Engineering, Indira Gandhi Institute of Technology, Odisha, India; Electronics and Telecommunication Engineering, Indira Gandhi Institute of Technology, Odisha, India; Electronics and Telecommunication Engineering, Indira Gandhi Institute of Technology, Odisha, India","2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)","13 Feb 2020","2019","1","","335","339","Image Fusion is an application of digital image processing. Image Fusion is a phenomenon of amalgamating the substantial features from similar pair of images into a single image, where the fused image will be of superior quality than either of the source images. This research work proposes a Pixel level Deep learning method using a 3-Channel convolutional neural network to fuse two multi focus Synthetic Aperture Radar (SAR) images obtain a high definition or high quality fused image. As the environment is in static condition and the radar is in floating, the radar imagery sensor captures source image at different time stamps. Size reduction of the image is performed before the fusion procedure in order to highly reduce the computational time and make the method more immune to noise. In the proposed method, source images are decomposed into pixels using the deep learning framework. After feature extraction, appropriate weights are assigned to all pixels. Then averaging and max pooling of pixel values of both the source images are done to get the resultant features of the fused image. Then smoothening filter is used to minimize noise in the fused image. Experimental results show that the proposed fusion technique demonstrates better PSNR value and less computational time.","","978-1-7281-0283-2","10.1109/ICICICT46008.2019.8993376","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8993376","Convolutional Neural Networks (CNNs);Deep Learning;Feature extraction;Image fusion;Signal to Noise Ratio (SNR);Synthetic Aperture Radar (SAR)","Deep learning;Convolution;Image fusion;Discrete wavelet transforms;Radar polarimetry;Feature extraction;Discrete cosine transforms","convolutional neural nets;feature extraction;image capture;image filtering;image fusion;image sensors;learning (artificial intelligence);minimisation;radar imaging;smoothing methods;synthetic aperture radar","SAR image fusion;digital image processing;high quality fused image;multifocus synthetic aperture radar images;source image capture;pixel level deep learning method;3-Channel convolutional neural network;radar imagery sensor;image size reduction;computational time reduction;noise immunity;feature extraction;smoothening filter;noise minimization","","1","","15","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Oil spill segmentation in fused Synthetic Aperture Radar images","F. S. Longman; L. Mihaylova; D. Coca","Department of Automatic Control and System Engineering, University of Sheffield, UK; Department of Automatic Control and System Engineering, University of Sheffield, UK; Department of Automatic Control and System Engineering, University of Sheffield, UK","2016 4th International Conference on Control Engineering & Information Technology (CEIT)","18 May 2017","2016","","","1","6","Synthetic Aperture Radar (SAR) satellite systems are very efficient in oil spill monitoring due to their capability to operate under all weather conditions. Systems such as the Envisat and RADARSAT have been used independently in many studies to detect oil spill. This paper presents an automatic feature based image registration and fusion algorithm for oil spill monitoring using SAR images. A range of metrics are used to evaluate the performance of the algorithm and to demonstrate the benefits of fusing SAR images of different modalities. The proposed framework has shown 45% improvement of the oil spill location when compared with the individual images before the fusion.","","978-1-5090-1055-4","10.1109/CEIT.2016.7929055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7929055","Oil Spill;Synthetic Aperture Radar (SAR);Registration;Image Fusion;Segmentation","Discrete wavelet transforms;Image resolution;Matched filters","image fusion;image segmentation;remote sensing;synthetic aperture radar","oil spill segmentation;fused synthetic aperture radar images;SAR satellite systems;oil spill monitoring;Envisat;RADARSAT;image registration;fusion algorithm;SAR images;oil spill location","","3","","25","IEEE","18 May 2017","","","IEEE","IEEE Conferences"
"Multi-angle SAR images for earthquake damage assessment","L. Wei-Hua; S. Ming-xin; Y. Huai-Ning; D. Xiao-Xia","National Earthquake Response Support Service, China Earthquake Administration, Beijing, China; National Earthquake Response Support Service, China Earthquake Administration, Beijing, China; National Earthquake Response Support Service, China Earthquake Administration, Beijing, China; National Earthquake Response Support Service, China Earthquake Administration, Beijing, China","2015 IEEE 5th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","29 Oct 2015","2015","","","714","716","Space-borne synthetic aperture radar (SAR) is very useful due to the whole-day-work capability, which has been widely used in earthquake damage assessment and rescue. This paper presents a novel method for earthquake damage assessment by using multi-angle SAR images. The scatter of building, especially for the damaged buildings, will change with the elevation angle and squint angle, which means a serious lack of information. In order to get more information, the multi-angle SAR images should be obtained. Moreover, a fast and accurate image information algorithm should be used. Furthermore, image geometry correction is realized in slant range plane. After correction operation, fusion operation is implemented. Finally, the simulation results justify the proposed method.","","978-1-4673-7297-8","10.1109/APSAR.2015.7306305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306305","SAR;multi-angles;geometry correction;image fusion","Synthetic aperture radar;Earthquakes;Buildings;Geometry;Azimuth;Image fusion;Image resolution","earthquakes;geophysical techniques;radar imaging;spaceborne radar;synthetic aperture radar","earthquake damage assessment;space-borne synthetic aperture radar;rescue;elevation angle;squint angle;multi-angle SAR images;image information algorithm;image geometry correction","","","","10","IEEE","29 Oct 2015","","","IEEE","IEEE Conferences"
"Image Fusion of RISAT-1 SAR Backscattered Image with AWiFS Optical Data","S. S. Mohd Naseem Akhter; P. P. Rege","Electronics and Telecommunication, College of Engineering, Pune (Affiliated to Savitribai Phule Pune University), Pune, India; Electronics and Telecommunication, College of Engineering, Pune (Affiliated to Savitribai Phule Pune University), Pune, India","2018 IEEE Punecon","27 Jun 2019","2018","","","1","5","Synthetic Aperture Radar (SAR) imaging is independent of solar illumination and cloud cover. The interpretability of optical images is improved by fusing with SAR images. The pixel intensity values of SAR image are often converted to physical quantity called backscattering coefficient measured in dB. It is an important property of SAR which is used to differentiate among the land surfaces. Hence, converting SAR image to backscattered image using backscattering coefficient provides more information compared to SAR image. SAR images are captured in four polarization modes. HH and VV polarization modes provide information of smooth surface and water bodies while HV and VH polarization modes provide information of vegetation and crop. In this paper, fusion of backscattered image in HH and HV polarization mode with optical image is carried out. Product from RISAT-1 and AWiFS are used for fusion.","","978-1-5386-7278-5","10.1109/PUNECON.2018.8745384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8745384","Image Fusion;Synthetic Aperture Radar;RISAT-1;AWiFS;Backscattered image","","backscatter;geophysical image processing;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar;vegetation;vegetation mapping","image fusion;RISAT-1 SAR;Synthetic Aperture Radar imaging;optical image;SAR image;solar illumination;cloud cover;backscattering coefficient;HH polarization mode;VV polarization mode;HV polarization mode;VH polarization mode;vegetation information;crop information;backscattered image fusion;pixel intensity values","","1","","16","IEEE","27 Jun 2019","","","IEEE","IEEE Conferences"
"Random Walks for Synthetic Aperture Radar Image Fusion in Framelet Domain","X. Yang; J. Wang; R. Zhu","Key Laboratory of Mathematics, Informatics, and Behavioral Semantics, Ministry of Education, Beihang University, Beijing, China; Key Laboratory of Mathematics, Informatics, and Behavioral Semantics, Ministry of Education, Beihang University, Beijing, China; Key Laboratory of Mathematics, Informatics, and Behavioral Semantics, Ministry of Education, Beihang University, Beijing, China","IEEE Transactions on Image Processing","22 Nov 2017","2018","27","2","851","865","A new framelet-based random walks (RWs) method is presented for synthetic aperture radar (SAR) image fusion, including SAR-visible images, SAR-infrared images, and Multi-band SAR images. In this method, we build a novel RWs model based on the statistical characteristics of framelet coefficients to fuse the high-frequency and low-frequency coefficients. This model converts the fusion problem to estimate the probability of each framelet coefficient being assigned each input image. Experimental results show that the proposed approach improves the contrast while preserves the edges simultaneously, and outperforms many traditional and state-of-the-art fusion techniques in both qualitative and quantitative analysis.","1941-0042","","10.1109/TIP.2017.2747093","Natural Science Foundation of Beijing Municipality(grant numbers:4152029); National Natural Science Foundation of China(grant numbers:61671002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8022919","Image fusion;framelet transform;random walks;SAR image;infrared image;visible image","Image fusion;Synthetic aperture radar;Image edge detection;Probability;Wavelet transforms;Fuses","geophysical image processing;image fusion;image resolution;infrared imaging;radar imaging;synthetic aperture radar","synthetic aperture radar image fusion;random walks method;SAR-visible images;RWs model;framelet coefficient;fusion problem;multiband SAR images","","15","","54","IEEE","30 Aug 2017","","","IEEE","IEEE Journals"
"Fusion of WorldView-2 Stereo and Multitemporal TerraSAR-X Images for Building Height Extraction in Urban Areas","Y. Xu; P. Ma; E. Ng; H. Lin","Institute of Future Cities, The Chinese University of Hong Kong, Hong Kong, China; Institute of Space and Earth Information Science and Shenzhen Research Institute, The Chinese University of Hong Kong, Hong Kong, China; School of Architecture, the Institute of Environment, Energy and Sustainability (IEES), and the Institute of Future Cities (IOFC), The Chinese University of Hong Kong, Hong Kong, China; Institute of Space and Earth Information Science and Shenzhen Research Institute, The Chinese University of Hong Kong, Hong Kong, China","IEEE Geoscience and Remote Sensing Letters","15 Jun 2015","2015","12","8","1795","1799","We investigated the joint use of the high-resolution WorldView-2 optical satellite images and the multitemporal TerraSAR-X synthetic aperture radar (SAR) satellite images to extract building height information in high-density urban areas. The main idea of the proposed fusion approach is to take full advantage of both data sets in building height retrieval. The proposed approach includes two main stages. First, initial building height estimates are extracted from WorldView-2 stereo images and multitemporal SAR images. These initial results are then combined using a novel object-based fusion approach, in which the heights of points for the same building footprint are retrieved and integrated. Experiments on the Mong Kok area of Hong Kong showed that the proposed approach using both data sets outperforms the use of either stereo images or SAR images alone. According to the results of the proposed approach, the average absolute height retrieval error is 6.53 m, which is much lower than using stereo and SAR images (9.08 and 12.24 m, respectively). The proposed fusion approach is suitable for building height retrieval in urban areas where single satellite data have limitations.","1558-0571","","10.1109/LGRS.2015.2427738","Hong Kong Research Grants Council(grant numbers:CUHK 14408214); Innovation and Technology Support Program of HKSAR(grant numbers:ITS/075/13); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7109861","Building height;object-based fusion;synthetic aperture radar (SAR);WorldView-2;Building height;object-based fusion;synthetic aperture radar (SAR);WorldView-2","Buildings;Synthetic aperture radar;Remote sensing;Urban areas;Laser radar;Optical imaging;Optical sensors","geophysical image processing;image fusion;image resolution;image retrieval;radar imaging;spaceborne radar;stereo image processing;synthetic aperture radar","Hong Kong;Mong Kok area;object-based fusion approach;image retrieval;high-density urban area;synthetic aperture radar;high-resolution WorldView-2 optical satellite image fusion;building height information extraction;multitemporal TerraSAR-X satellite image fusion;distance 6.53 m;distance 9.08 m;distance 12.24 m","","14","1","21","IEEE","18 May 2015","","","IEEE","IEEE Journals"
"A SAR/Infrared Image Fusion Method Based on NSCT and PCNN","W. Luo; H. Zhang; J. Ding","AVIC Leihua Electronic Technology Research Institude, Wuxi, China; AVIC Leihua Electronic Technology Research Institude, Wuxi, China; AVIC Leihua Electronic Technology Research Institude, Wuxi, China","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","30 Mar 2020","2019","","","1","4","In order to improve the quality of the fusion image between SAR image and infrared image, a novel SAR/infrared image fusion method based on non-subsampled contourlet transform (NSCT) and pulse coupled neural network (PCNN) is proposed. Firstly, this method decomposes the SAR image and infrared image respectively via NSCT. Then PCNN is used as the fusion rule of our method. Finally, the fusion image is obtained by taking the inverse NSCT transform. The results indicate that, compared with other fusion methods, higher resolution fusion image with rich information and strong contrast can be get by the method proposed in this paper.","2474-2333","978-1-7281-2912-9","10.1109/APSAR46974.2019.9048470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048470","SAR;infrared;image fusion;non-subsampled contourlet transform;pulse coupled neural network","","image fusion;image resolution;infrared imaging;neural nets;radar imaging;synthetic aperture radar;transforms","nonsubsampled contourlet transform;PCNN;SAR image;fusion rule;inverse NSCT;fusion methods;higher resolution fusion image;pulse coupled neural network","","1","","11","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"Multi-static MIMO-SAR three dimensional deformation measurement system","T. Zeng; C. Mao; C. Hu; X. Yang; W. Tian","School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China; School of Information and Electronics, Beijing Key Laboratory of Embedded Real-time Information Processing Technology, Beijing, China","2015 IEEE 5th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","29 Oct 2015","2015","","","297","301","Herein is proposed a novel high-steep rock slope landslide monitoring system based on the configuration of multi-static multiple-input multiple-output synthetic aperture radar (MIMO-SAR). In this system, three spastically distributed radars are employed. Each is a Ku-band MIMO-SAR, transmitting mutually orthogonal waveforms and sensing the slope movement from different view angles. The three radars can then achieve the deform measurements in the three line of sight (LOS) directions. After an effective fusion of these measurements, the deform vector, with direction and amount, can be resolved. This vector is particularly important for risk warning as well as for understanding the motion mechanism of landslide. Moreover, the data acquisition period can be largely reduced, since the use of MIMO radar allows a simultaneous sampling of the scene echo. The time decorrelation is therefore very small, and hence high measurement accuracy can be expected. It can be proved that, using this system, the three-dimensional accuracy can reach 0.1 ~ 1 mm. Simulations and experimental datasets are used to validate this proposal.","","978-1-4673-7297-8","10.1109/APSAR.2015.7306212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306212","Deformation measurement;ground based SAR;MIMO;three-dimensional measurement","Accuracy;Arrays;Synthetic aperture radar;Monitoring;Geologic measurements;Terrain factors;Imaging","decorrelation;deformation;environmental monitoring (geophysics);geomorphology;geophysical image processing;image fusion;mechanical variables measurement;MIMO radar;radar imaging;remote sensing by radar;synthetic aperture radar","multistatic MIMO-SAR 3D deformation measurement system;high-steep rock slope landslide monitoring system;multistatic multiple-input multiple-output synthetic aperture radar;Ku-band MIMO-SAR;slope movement;mutually orthogonal waveform transmission;line-of-sight directions;risk warning;landslide motion mechanism;data acquisition period reduction;simultaneous scene echo sampling;time decorrelation","","12","1","14","IEEE","29 Oct 2015","","","IEEE","IEEE Conferences"
"A Better View Over Titan Drainage Networks Through RGB Fusion of Cassini SAR Images","E. I. Alves; A. I. A. S. S. Andrade; D. A. Vaz","Center for Earth and Space Research, University of Coimbra, Coimbra, Portugal; Center for Earth and Space Research, University of Coimbra, Coimbra, Portugal; Center for Earth and Space Research, University of Coimbra, Coimbra, Portugal","IEEE Geoscience and Remote Sensing Letters","23 Feb 2018","2018","15","3","414","418","We present a simple method to enhance the view of Titan drainage networks, thus allowing extracting relevant hydrological parameters. The method uses RGB fusion of three Cassini synthetic aperture radar images acquired at different times, and is tested on one drainage network. Comparison with previous studies of the same network shows an increase in all the measured parameters. The present results help constrain previous estimates of erosion times, terrain, and tectonic models for the area and indicate that, whenever possible, geomorphological inference from drainage network geometry should be drawn on multiply sampled scenes.","1558-0571","","10.1109/LGRS.2018.2791018","Portuguese National Funds through the Foundation for Science and Technology(grant numbers:UID/Multi/00611/2013); European Regional Development Fund through COMPETE 2020, Operational Program for Competitiveness and Internationalization(grant numbers:POCI-01-0145-FEDER-006922); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8275480","Drainage networks;image enhancement;image fusion;synthetic aperture radar (SAR);Titan","Synthetic aperture radar;Saturn;Satellites;Radar imaging;Image resolution;Image color analysis","image colour analysis;image enhancement;image fusion;image sampling;inference mechanisms;radar imaging;synthetic aperture radar","RGB fusion;relevant hydrological parameter extraction;Cassini SAR imaging;Titan drainage network geometry;Cassini synthetic aperture radar imaging;geomorphological inference","","4","","23","IEEE","30 Jan 2018","","","IEEE","IEEE Journals"
"A HSV-Based Fusion of InIRA SAR and GoogleEarth Optical Images","D. Li; Y. Zhang; X. Dong; X. Shi; W. Zhai","Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China; Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China; Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China; Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China; Chinese Acadcmy of Sceicnes, Key Laboratory of Microwave Remote Sensing National Space Science Center, Beijing, China","2018 Asia-Pacific Microwave Conference (APMC)","17 Jan 2019","2018","","","848","850","Interferometric Imaging Radar Altimeter (InIRA) on board Chinese TG-2 space laboratory is dedicated to enable a wide-swath measure of the sea surface height by interferometric processing synthetic aperture radar (SAR) images. In this paper, a simple fusion algorithm is developed for InIRA SAR image and GoogleEarth optical image based on hue-saturation-value (HSV) color model. While the H and S components of optical image are reserved for the fusion, the V component of the fusion image is a weighted linear combination of SAR image and the V component of optical image, with the image entropy as weight. Experimental results exhibit not only the nice performance of the fusion scheme but also the potential of InIRA in inland water observation.","","978-4-9023-3945-1","10.23919/APMC.2018.8617352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8617352","HSV;image entropy;image fusion;synthetic aperture radar;optical sensor","Synthetic aperture radar;Optical imaging;Optical interferometry;Optical scattering;Optical sensors;Rivers","entropy;geophysical image processing;image colour analysis;image fusion;oceanographic techniques;radar altimetry;radar imaging;radar interferometry;synthetic aperture radar","Chinese TG-2 space laboratory;fusion algorithm;HSV-based fusion;fusion scheme;image entropy;fusion image;hue-saturation-value color model;GoogleEarth optical image;InIRA SAR image;interferometric processing synthetic aperture radar images;sea surface height;wide-swath measure;Interferometric Imaging Radar Altimeter","","1","","6","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Review of Research on Registration of SAR and Optical Remote Sensing Image Based on Feature","L. Kai; Z. Xueqing","National Defense University of Technology, Information and Communication Engineering College, Wu Han, China; National Defense University of Technology, Information and Communication Engineering College, Wu Han, China","2018 IEEE 3rd International Conference on Signal and Image Processing (ICSIP)","3 Jan 2019","2018","","","111","115","Synthetic Aperture Radar(SAR) and optical remote sensing image registration is the prerequisite for image fusion and it is of important theoretical significance and practical value. The image registration methods are mainly divided into the methods based on feature, the methods based on Gray-scale and others. This article systematically sorts out feature-based optical and SAR remote sensing image registration techniques, summarizes all types of image registration, points out their advantages and disadvantages and predicts the prospects of their future.","","978-1-5386-6396-7","10.1109/SIPROCESS.2018.8600443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600443","synthetic aperture radar(SAR);remote sensing;image registration;feature-based","Synthetic aperture radar;Optical sensors;Remote sensing;Optical imaging;Image registration;Adaptive optics;Feature extraction","geophysical image processing;geophysical techniques;image fusion;image registration;radar imaging;remote sensing by radar;synthetic aperture radar","SAR remote sensing image registration techniques;optical remote sensing image registration;image fusion;image registration methods;Synthetic Aperture Radar;Gray-scale methods","","3","","46","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"A Novel Loss Function for Optical and SAR Image Matching: Balanced Positive and Negative Samples","Y. He; X. Wang; Y. Zhang; Y. Liu; Z. Jiang; G. Li; Y. He","Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Troops, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Shenzhen International Graduate School, Tsinghua University, Shenzhen, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","16 Dec 2022","2022","19","","1","5","Image matching is a primary technology for optical and synthetic aperture radar (SAR) image fusion but often shows limited performance due to the highly nonlinear differences between optical and SAR modalities. Recently, deep neural networks (DNNs) have been investigated to effectively extract nonlinear features for image matching tasks, where DNNs are trained based on the elaborated design of loss functions and a low loss value is often expected to obtain better image matching performance. In this letter, we first theoretically demonstrate that when the value of a state-of-the-art loss function decreases, the corresponding matching performance may not consistently improve due to the imbalanced effect of positive and negative samples. To tackle this issue, we proposed an improved loss function to train DNNs for image matching of SAR and optical images. We theoretically prove that the improved loss function ensures the improvement of the matching performance when the loss value decreases based on Taylor’s series expansion analysis. Experimental results on an open dataset with extensive optical and SAR image pairs show that: 1) the proposed loss function is better than the original one in terms of image matching performance and 2) the combination of our loss function and existing multiscale convolutional gradient feature (MCGF)-based network provides better matching performance than the other state-of-the-art approaches.","1558-0571","","10.1109/LGRS.2022.3225965","National Key Research and Development Program of China(grant numbers:2021YFA0715202); National Natural Science Foundation of China(grant numbers:61790551,61925106,62022092,62101303); Autonomous Research Project of Department of Electronic Engineering at Tsinghua University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968003","Loss function;optical and synthetic aperture radar (SAR) image matching;sample balance;Taylor’s series expansion","Optical imaging;Optical sensors;Optical losses;Image matching;Radar polarimetry;Synthetic aperture radar;Adaptive optics","convolution;deep learning (artificial intelligence);feature extraction;gradient methods;image fusion;image matching;optical information processing;radar computing;radar imaging;synthetic aperture radar","corresponding matching performance;deep neural network;DNN;highly nonlinear differences;image matching;image matching performance;improved loss function;low loss value;MCGF;multiscale convolutional gradient feature;negative samples;nonlinear feature extraction;novel loss function;optical image matching;SAR image matching;SAR image pairs;SAR modalities;state-of-the-art loss function;synthetic aperture radar image fusion;Taylor series expansion analysis","","1","","16","IEEE","1 Dec 2022","","","IEEE","IEEE Journals"
"A fast implementation method for the FFBP algorithm","Z. Yang; Q. Dong; G. Sun; M. Xing","National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China","2015 IEEE Radar Conference (RadarCon)","25 Jun 2015","2015","","","0411","0414","A fast implementation method for the fast factorized back-projection (FIM-FFBP) algorithm is proposed to focus high-resolution spotlight synthetic aperture radar (SAR) data. Different from the original FFBP utilizing two-dimensional image-domain interpolation for sub-aperture fusion, FIM-FFBP finishes the image fusion using chirp-z transform and circular shifting. FIM-FFBP yields enhanced efficiency over the nearest interpolation with 4 times upsampling based FFBP, and keeps the high precision simultaneously. Real-data experiment verifies the efficiency superiorities of the FIM-FFBP.","2375-5318","978-1-4799-8232-5","10.1109/RADAR.2015.7131034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131034","Spotlight;synthetic aperture radar (SAR);fast factorized back-projection (FFBP)","Interpolation;Algorithm design and analysis;Synthetic aperture radar;Azimuth;Apertures;Image resolution;Signal processing algorithms","image fusion;image resolution;interpolation;radar imaging;radar resolution;synthetic aperture radar;Z transforms","fast implementation method;fast factorized back-projection algorithm;FIM-FFBP algorithm;high-resolution spotlight SAR data;synthetic aperture radar;two-dimensional image domain interpolation;subaperture fusion;image fusion;chirp-z transform;circular shifting","","","","5","IEEE","25 Jun 2015","","","IEEE","IEEE Conferences"
"FDFNet: A Fusion Network for Generating High-Resolution Fully PolSAR Images","L. Lin; H. Shen; J. Li; Q. Yuan","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology and the School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology and the School of Geodesy and Geomatics, Wuhan University, Wuhan, China","IEEE Geoscience and Remote Sensing Letters","7 Jan 2022","2022","19","","1","5","Deep learning shows potential superiority in the image fusion field. To solve the problem of the spatial resolution degradation of polarimetric synthetic aperture radar (PolSAR) images caused by system limitation, we propose a fully PolSAR images and DualSAR images fusion network (FDFNet). We use low resolution (LR)-PolSAR super-resolution (LPSR) and modified cross attention mechanism (MCroAM) to perform data fusion on LR-PolSAR and high resolution (HR)-dual-polarization synthetic aperture radar (DualSAR) and design a polarimetric decomposition attention module to introduce the polarimetric parameters of LR-PolSAR images to maintain polarimetric information. Besides, we use the differential information between LR-PolSAR and HR-DualSAR to guide spatial resolution reconstruction. The loss function based on the  $L_{1} $  norm is used to constrain the network training process. The experimental results show the superiority of the proposed method over the existing methods in visual and quantitative evaluation. In addition, polarimetric decomposition experiments verify the effectiveness of the proposed method to maintain polarimetric information.","1558-0571","","10.1109/LGRS.2021.3127958","National Natural Science Foundation of China(grant numbers:61671334,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9614198","Differential information;dual-polarization synthetic aperture radar (DualSAR);fully-polarimetric synthetic aperture radar (PolSAR);fusion;polarimetric decomposition","Feature extraction;Spatial resolution;Scattering;Image reconstruction;Superresolution;Image fusion;Handheld computers","geophysical image processing;image fusion;image reconstruction;image resolution;learning (artificial intelligence);radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","generating high-resolution fully PolSAR;image fusion field;spatial resolution degradation;polarimetric synthetic aperture radar images;fully PolSAR images;LR-PolSAR images;polarimetric information;DualSAR images fusion network;low resolution-PolSAR super-resolution","","","","8","IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"A New Coregistration Algorithm for Recent Applications on Urban SAR Images","A. Plyer; E. Colin-Koeniguer; F. Weissgerber","Département Traitement de l'Information et Modélisation, ONERA, Palaiseau Cedex, France; Département Traitement de l'Information et Modélisation, ONERA, Palaiseau Cedex, France; Département Traitement du Signal et des Images, Télécom ParisTech, France, France","IEEE Geoscience and Remote Sensing Letters","4 Nov 2015","2015","12","11","2198","2202","In this letter, a fast and robust optical-flow estimation algorithm is investigated for synthetic aperture radar (SAR) images' coregistration. The principle of the initial algorithm is described, as well as its adaptation to the case of radar images. A performance evaluation method is proposed to fix the choice of the parameters of the algorithm. Promising results in change detection or interferometry between SAR images of different resolutions are presented. They offer the opportunity to use this kind of algorithm in the case of high-resolution images containing many structural elements as in urban areas.","1558-0571","","10.1109/LGRS.2015.2455071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202849","Image fusion;image registration;radar interferometry;synthetic aperture radar (SAR);Image fusion;image registration;radar interferometry;synthetic aperture radar (SAR)","Synthetic aperture radar;Image resolution;Optical imaging;Optical sensors;Optical interferometry;Robustness","image registration;remote sensing by radar;synthetic aperture radar","urban area;high-resolution images;interferometry;change detection;synthetic aperture radar image coregistration;optical-flow estimation algorithm;urban SAR images;coregistration algorithm","","33","","9","IEEE","14 Aug 2015","","","IEEE","IEEE Journals"
"Sar and Optical Image Fusion for Coastal Surveillance","L. Zheng; J. Pei; Y. Zhang; Y. Huang; J. Wu; J. Yang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, P.R. China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2802","2805","Coastal surveillance has long been paid a lot of attention for the threat of flooding due to some natural phenomena, such as global warming. Prompt and accurate reaction to the visualization of the flooded areas is the key. An image fusion rule is thus proposed in this paper to achieve image enhancement of the flooded areas. The rule, targeted at high-frequency parts of the synthetic aperture radar (SAR) and optical images, is able to exploit and combine the merits of both SAR and optical images to obtain the exact flooded areas with the clear boundaries. Experimental results validate the performance of the proposed fusion rule and show that not only the clarity of fusion images is improved, but also the texture and brightness contrast are greatly enhanced.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900420","image fusion;monitor flooded area;non-subsampled contourlet transform","Optical imaging;Radar polarimetry;Optical sensors;Synthetic aperture radar;Sea measurements;Standards;Adaptive optics","floods;image enhancement;image fusion;optical images;radar imaging;synthetic aperture radar","coastal surveillance;natural phenomena;global warming;accurate reaction;image fusion rule;image enhancement;high-frequency parts;SAR;optical images;exact flooded areas;fusion images","","","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Fusion of POLSAR and Multispectral Satellite Images: A New Insight for Image Fusion","J. Wang; J. Chen; Q. Wang","College of Computer and Information Engineering, Hohai University, Nanjing, China; College of Computer and Information Engineering, Hohai University, Nanjing, China; College of Computer and Information Engineering, Hohai University, Nanjing, China","2020 IEEE International Conference on Computational Electromagnetics (ICCEM)","12 Oct 2020","2020","","","83","84","Polarized synthetic aperture radar (POLSAR) and multispectral images have great complementarity in information volume. That is to say, POLSAR have high resolution but poor color information. Multispectral images have rich spectral channel information, but the resolution is low. Therefore, this work has explored the fusion problem of the two data source. A framework was proposed, which merged polarized channel fusion data and multispectral images based on the Sentinel-2 and GF-3 data. The experimental results showed that the fusion results greatly integrated the characteristics of each channel of POLSAR and optical image. Therefore, our work has great application potential in improving the accuracy of feature recognition.","","978-1-7281-3448-2","10.1109/ICCEM47450.2020.9219457","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9219457","fusion;polarized synthetic aperture radar (POL-SAR);GF-3;Sentinel-2;feature recognition","Integrated optics;Visualization;Optical polarization;Satellites;Soft sensors;Optical imaging;Adaptive optics","geophysical image processing;image colour analysis;image fusion;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","optical image;fusion results;channel fusion data;fusion problem;rich spectral channel information;poor color information;information volume;multispectral images;polarized synthetic aperture radar;image fusion;multispectral satellite images;POLSAR","","1","","7","IEEE","12 Oct 2020","","","IEEE","IEEE Conferences"
"Edge detection in polarimetric SAR images based on the nonsubsampled contourlet transform","Ruijin Jin; Junjun Yin; Wei Zhou; J. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2015 IEEE Radar Conference (RadarCon)","25 Jun 2015","2015","","","0319","0323","Due to the presence of speckle noise, the detection of edge features from synthetic aperture radar (SAR) images is complicated. In this paper, a new method is proposed for edge detection of polarimetric SAR (Pol-SAR) images, based on the nonsubsampled contourlet transform (NSCT). Firstly, we develop the nonsubsampled contrast pyramid (CP) for polarimetric images by introducing an adaptive optimal polarimetric contrast enhancement (OPCE) model, then the obtained multiscale images are filtered by the nonsubsampled directional filter banks. The point-wise modulus maximum is evaluated across of multidirection subbands in each scale. Finally, the multiscale edge images are fused to reduce the influence of noise. The performance of the proposed method is demonstrated with Pol-SAR data acquired by Convair-580, showing great potential of this method.","2375-5318","978-1-4799-8232-5","10.1109/RADAR.2015.7131017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131017","polarimetric synthetic aperture radar (Pol-SAR);nonsubsampled contourlet transform (NSCT);edge detection;optimal polarimetric contrast enhancement (OPCE);multiscale","Image edge detection;Synthetic aperture radar;Detectors;Wavelet transforms;Roads;Filter banks","channel bank filters;edge detection;feature extraction;image denoising;image enhancement;image filtering;image fusion;radar imaging;radar polarimetry;synthetic aperture radar;transforms","edge feature detection;polarimetric SAR images;polarimetric synthetic aperture radar images;nonsubsampled contourlet transform;speckle noise;NSCT;adaptive optimal polarimetric contrast enhancement model;OPCE model;point-wise modulus maximum;multidirection subbands;pol-SAR data;Convair-580;nonsubsampled contrast pyramid;multiscale image filtering;multiscale edge image fusion;nonsubsampled directional filter banks","","","","15","IEEE","25 Jun 2015","","","IEEE","IEEE Conferences"
"Intelligent Resource Management and Optimization of Clustered UAV Airborne SAR System","Y. -Q. Yang; Z. Xia; Z. Zhao; T. Zhang; K. Li; X. Yin; H. Shi; T. Peng","Beijing Institute of Satellite Information Engineering, Beijing, China; Beijing Institute of Satellite Information Engineering, Beijing, China; Beijing Institute of Satellite Information Engineering, Beijing, China; Beijing Institute of Satellite Information Engineering, Beijing, China; Beijing Institute of Satellite Information Engineering, Beijing, China; Beijing Institute of Satellite Information Engineering, Beijing, China; Beijing Institute of Satellite Information Engineering, Beijing, China; Beijing Institute of Satellite Information Engineering, Beijing, China","2021 IEEE 4th International Conference on Electronics Technology (ICET)","16 Jun 2021","2021","","","987","991","This paper proposes a resource management method of clustered UAV airborne SAR system based on the advanced genetic algorithm (GA). The clustered UAVs is taken as a Multiple Input Multiple Output (MIMO) system for SAR imaging. The imaging result is improved by fusing the images of different UAV nodes from diverse observation angles. By considering spatial configuration of UAV nodes, the transmitting power and frequency band allocation, the system resource management is optimally scheduled for providing a satisfied image fusion results using the niching GA method. From numerical results, the proposed method can offer more features for image fusion when detecting the cloaking target.","","978-1-7281-7673-4","10.1109/ICET51757.2021.9451105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451105","Synthetic Aperture Radar (SAR);unmanned aerial vehicle (UAV) swarm;advanced genetic algorithm;optimization of system resource","Imaging;Object detection;Radar polarimetry;Resource management;Synthetic aperture radar;MIMO communication;Image fusion","airborne radar;autonomous aerial vehicles;frequency allocation;genetic algorithms;image fusion;MIMO radar;object detection;pattern clustering;radar detection;radar imaging;synthetic aperture radar","clustered UAV airborne SAR system;resource management method;multiple input multiple output system;SAR imaging;system resource management;image fusion;clustered UAV nodes;MIMO system;niching GA method;cloaking target detection;genetic algorithm;frequency band allocation;power allocation","","","","8","IEEE","16 Jun 2021","","","IEEE","IEEE Conferences"
"Comparative analysis of different fusion rules for SAR and multi-spectral image fusion based on NSCT and IHS transform","X. J. Chong; C. Xuejiao","College of Earth Science and Engineering, Hohai University, Nanjing, China; College of Earth Science and Engineering, Hohai University, Nanjing, China","2015 International Conference on Computer and Computational Sciences (ICCCS)","21 Dec 2015","2015","","","271","274","In order to improve the fusion quality of SAR and multi-spectral image, this paper proposes an image fusion method based on nonsubsampled contourlet transform (NSCT) and IHS transform. Since the fusion rule plays a very important role during the fusion process, four fusion rules are analyzed and compared. Three fusion rules are commonly used in previous works and a new fusion rule is proposed in this paper. To evaluate the performance of different fusion rules, fusion experiments are carried on COSMO-SkyMed SAR and Landsat OLI image. The experimental results indicate that the proposed rule is more effective than the other three regular fusion rules.","","978-1-4799-1819-5","10.1109/ICCACS.2015.7361364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361364","Iimage fusion;NSCT;IHS transform;fusion rule","Transforms;Synthetic aperture radar;Image fusion;Remote sensing;Satellites;Filter banks;Image resolution","image fusion;radar imaging;synthetic aperture radar;transforms","multispectral image fusion;nonsubsampled contourlet transform;NSCT;intensity hue saturation;IHS transform;fusion rule;COSMO-SkyMed SAR;Landsat OLI image","","1","","11","IEEE","21 Dec 2015","","","IEEE","IEEE Conferences"
"A Multichannel SAR-GMTI Method Based on Multi-Polarization SAR Image Fusion","L. Song; A. Liu; Z. Huang","Nanjing Research Institute of Electronics Technology, Nanjing, China; Nanjing Research Institute of Electronics Technology, Nanjing, China; Nanjing Research Institute of Electronics Technology, Nanjing, China","2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","5 Apr 2021","2021","5","","2678","2684","Displaced phase center array (DPCA) is a classical multichannel synthetic aperture radar(SAR)-based ground moving target indication (SAR-GMTI) method. It has been widely used in the real SAR system because of its simple operation and low computational complexity. However, in the complex clutter environment, there are often many false alarms in the DPCA results, such as the urban area. In this paper, a new GMTI method based on the multi-polarization SAR image information is proposed. Since the amplitude of the urban area in the cross-polarization SAR image is weaker than that in the copolarization image, the urban area in the copolarization image can be replaced with the cross-polarization image, then the DPCA processing is performed for the fusional image. Compared with traditional DPCA technique, this method can reduce the false alarms in the complex environment, such as the urban area. Finally, the effectiveness of the proposed method is verified by the real multichannel quadrature-polarization SAR data.","2689-6621","978-1-7281-8028-1","10.1109/IAEAC50856.2021.9390769","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390769","synthetic aperture radar(SAR);displaced phase center array(DPCA);ground moving target indication(GMTI);polarization","Phased arrays;Urban areas;Buildings;Apertures;Radar polarimetry;Clutter;Synthetic aperture radar","airborne radar;image fusion;radar antennas;radar imaging;remote sensing by radar;synthetic aperture radar","complex clutter environment;false alarms;DPCA results;urban area;multipolarization SAR image information;cross-polarization SAR image;copolarization image;cross-polarization image;DPCA processing;fusional image;traditional DPCA technique;complex environment;multichannel quadrature-polarization SAR data;multichannel SAR-GMTI method;target indication method;SAR system;low computational complexity","","1","","6","IEEE","5 Apr 2021","","","IEEE","IEEE Conferences"
"A Fast BP Algorithm for Bistatic Spotlight SAR Based on Spectrum Compression","J. Ding; Y. Wu; H. Zhang","AVIC Leihua Electronic Technology Research Institude, Wuxi, China; AVIC Leihua Electronic Technology Research Institude, Wuxi, China; AVIC Leihua Electronic Technology Research Institude, Wuxi, China","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","30 Mar 2020","2019","","","1","5","For bistatic spotlight synthetic aperture radar (SAR) with parallel trajectory, this paper investigates a fast back-projection (BP) imaging algorithm based on spectrum compression. The long synthetic aperture is split into several short subapertures (SAs), and each of them constructs a SA image with coarse azimuth resolution in a unified rectangular coordinate system by BP integral. Then, by applying the spectrum compression technique introduced in the paper, the azimuth spectrum of the SA image can be compressed greatly and, as such, the images can be coherently accumulated after azimuth up-sampling and spectrum recovery. Since there is no need for data interpolation in the image fusion, the proposed algorithm can achieve high precision as the original BP algorithm does but with higher computational efficiency. Simulation experiments are presented to validate the effectiveness of the method.","2474-2333","978-1-7281-2912-9","10.1109/APSAR46974.2019.9048432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048432","bistatic spotlight SAR;fast BP algorithm;rectangular coordinate system;spectrum compression","","image fusion;radar imaging;synthetic aperture radar","spectrum recovery;image fusion;original BP algorithm;fast BP algorithm;bistatic spotlight SAR;bistatic spotlight synthetic aperture radar;parallel trajectory;synthetic aperture;short subapertures;SA image;coarse azimuth resolution;unified rectangular coordinate system;BP integral;spectrum compression technique;azimuth spectrum;azimuth up-sampling;fast back-projection imaging algorithm","","1","","8","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"Remote Sensing Image Fusion Using Hierarchical Multimodal Probabilistic Latent Semantic Analysis","R. Fernandez-Beltran; J. M. Haut; M. E. Paoletti; J. Plaza; A. Plaza; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Department of Technology of Computers and Communications, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4982","4993","The generative semantic nature of probabilistic topic models has recently shown encouraging results within the remote sensing image fusion field when conducting land cover categorization. However, standard topic models have not yet been adapted to the inherent complexity of remotely sensed data, which eventually may limit their resulting performance. In this scenario, this paper presents a new topic-based image fusion framework, specially designed to fuse synthetic aperture radar (SAR) and multispectral imaging (MSI) data for unsupervised land cover categorization tasks. Specifically, we initially propose a hierarchical multi-modal probabilistic latent semantic analysis (HMpLSA) model that takes advantage of two different vocabulary modalities, as well as two different levels of topics, in order to effectively uncover intersensor semantic patterns. Then, we define an SAR and MSI data fusion framework based on HMpLSA in order to perform unsupervised land cover categorization. Our experiments, conducted using three different SAR and MSI data sets, reveal that the proposed approach is able to provide competitive advantages with respect to standard clustering methods and topic models, as well as several multimodal topic model variants available in the literature.","2151-1535","","10.1109/JSTARS.2018.2881342","Generalitat Valenciana(grant numbers:APOSTD/2017/007); panish Ministry(grant numbers:FPU14/02012-FPU15/02090,ESP2016-79503-C2-2-P,TIN2015-63646-C5-5-R); Junta de Extremadura(grant numbers:GR15005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550740","Image fusion;multispectral imaging (MSI);probabilistic latent semantic analysis (pLSA);synthetic aperture radar (SAR)","Synthetic aperture radar;Data models;Remote sensing;Semantics;Image fusion;Probabilistic logic;Adaptation models","feature extraction;geophysical image processing;image classification;image fusion;pattern clustering;probability;remote sensing;sensor fusion;synthetic aperture radar;terrain mapping","hierarchical multimodal probabilistic latent semantic analysis;generative semantic nature;probabilistic topic models;remote sensing image fusion field;standard topic models;remotely sensed data;resulting performance;topic-based image fusion framework;unsupervised land cover categorization tasks;multimodal probabilistic latent semantic analysis model;HMpLSA;intersensor semantic patterns;multimodal topic model;vocabulary modalities;land cover categorization","","45","","47","IEEE","28 Nov 2018","","","IEEE","IEEE Journals"
"Coastal wetland classification based on high resolution SAR and optical image fusion","J. Yang; G. Ren; Y. Ma; Y. Fan","School of Geosciences, China University of Petroleum Beijing, Changping-qu, Beijing, CN; The First Institute of Oceanography, SOA, Qingdao, China; The First Institute of Oceanography, SOA, Qingdao, China; School of Geosciences, China University of Petroleum Beijing, Changping-qu, Beijing, CN","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","886","889","In this paper, the data source are GF-1 WFV image and Radarsat-2 SAR image covering the Yellow River Estuary wetland eastern area. The paper first uses Gram-Schmidt algorithm for fusing GF-1 image and different polarimetric mode SAR images, and then uses the method of SVM for supervised classification. Finally, the accuracy of the classification results and the capacity of information extraction are compared. The experiment results show:(1) the classification accuracy of fusing the VV polarimetric mode of SAR image and GF-1 image is better than other fusion image, reaching 83.78%, closing to the classification accuracy of GF-1 image. The classification accuracy of tidal flat reed in VV polarimetric fusion image is better than that of GF-1.(2) Tidal flat, river and aquaculture pond have the highest classification accuracy in all the fusion images.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729224","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729224","GF-1;SAR;image fusion;SVM;coastal wetland","Synthetic aperture radar;Wetlands;Support vector machines;Remote sensing;Sea measurements;Rivers;Satellites","geophysical image processing;image classification;image fusion;image resolution;radar polarimetry;remote sensing by radar;synthetic aperture radar;terrain mapping;wetlands","coastal wetland classification;data source;GF-1 WFV image;Radarsat-2 SAR image;Yellow River Estuary wetland eastern area;Gram-Schmidt algorithm;GF-1 image fusion;polarimetric mode SAR images;supervised classification accuracy;VV polarimetric mode;tidal flat reed;VV polarimetric fusion image;aquaculture pond;high resolution SAR-optical image fusion;information extraction","","12","","13","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Multiview Three-Dimensional Interferometric Inverse Synthetic Aperture Radar","F. Salvetti; M. Martorella; E. Giusti; D. Staglianò","CNIT, Pisa, Italy; CNIT, Pisa, Italy; CNIT, Pisa, Italy; CNIT, Pisa, Italy","IEEE Transactions on Aerospace and Electronic Systems","11 Apr 2019","2019","55","2","718","733","Three-dimensional (3D) inverse synthetic aperture radar (ISAR) imaging has been proven feasible by combining traditional ISAR imaging and interferometry. Such technique, namely Inteferometric ISAR (InISAR), allows for the main target scattering centers to be mapped into a 3-D spatial domain, therefore forming 3-D images under the form of 3-D point clouds. 3-D InISAR overcomes some main limitations of traditional 2-D ISAR imaging, such as the problem of cross-range scaling and unknown image projection plane. Despite the great advantage of 3-D imaging over traditional 2-D imaging, some issues remain, such as scatterer scintillation, shadowing effects, poor SNR, etc., which limit the effectiveness of 3-D imaging. A solution to these issues can be found in the use of multiple 3-D views, which can be obtained exploiting either multitemporal or multiperspective configurations or a combination of both. This paper proposes this concept and develops the image fusion algorithms that are necessary to produce multiview 3-D ISAR images. The effectiveness of the proposed technique is tested by using real data collected with a multistatic InISAR system.","1557-9603","","10.1109/TAES.2018.2864469","North Atlantic Treaty Organization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451947","Inverse synthetic aperture radar (ISAR);multi-view radar Imaging;three-dimensional (3D) incoherent image fusion;3-D Radar Imaging;radar interferometry","Three-dimensional displays;Receivers;Image reconstruction;Estimation;Data mining;Image fusion;Imaging","image fusion;radar imaging;radar interferometry;stereo image processing;synthetic aperture radar","unknown image projection plane;image fusion algorithms;target scattering centers;3D point clouds;2D ISAR imaging;three-dimensional interferometric inverse synthetic aperture radar imaging;3D spatial domain mapping;cross-range scaling problem;scatterer scintillation;SNR;multiperspective configuration;multitemporal configuration;multiview 3D ISAR imaging;multistatic InISAR system","","26","","40","IEEE","30 Aug 2018","","","IEEE","IEEE Journals"
"Multimodal Probabilistic Latent Semantic Analysis for Sentinel-1 and Sentinel-2 Image Fusion","R. Fernandez-Beltran; J. M. Haut; M. E. Paoletti; J. Plaza; A. Plaza; F. Pla","Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Hyperspectral Computing Laboratory, Escuela Politécnica, University of Extremadura, Cáceres, Spain; Institute of New Imaging Technologies, University Jaume I, Castellón de la Plana, Spain","IEEE Geoscience and Remote Sensing Letters","26 Aug 2018","2018","15","9","1347","1351","Probabilistic topic models have recently shown a great potential in the remote sensing image fusion field, which is particularly helpful in land-cover categorization tasks. This letter first studies the application of probabilistic latent semantic analysis (pLSA) and latent Dirichlet allocation to remote sensing synthetic aperture radar (SAR) and multispectral imaging (MSI) unsupervised land-cover categorization. Then, a novel pLSA-based image fusion approach is presented, which pursues to uncover multimodal feature patterns from SAR and MSI data in order to effectively fuse and categorize Sentinel-1 and Sentinel-2 remotely sensed data. Experiments conducted over two different data sets reveal the advantages of the proposed approach for unsupervised land-cover categorization tasks.","1558-0571","","10.1109/LGRS.2018.2843886","Generalitat Valenciana(grant numbers:APOSTD/2017/007); Ministerio de Educación, Cultura y Deporte(grant numbers:FPU14/02012-FPU15/02090,ESP2016-79503-C2-2-P); Consejería de Educación y Empleo, Junta de Extremadura(grant numbers:GR15005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392415","Image fusion;land-cover categorization;probabilistic latent semantic analysis (pLSA);Sentinel-1;Sentinel-2","Synthetic aperture radar;Remote sensing;Data models;Image fusion;Semantics;Feature extraction;Probabilistic logic","feature extraction;geophysical image processing;geophysical techniques;image fusion;land cover;remote sensing;synthetic aperture radar","MSI data;SAR data;multispectral imaging unsupervised land-cover categorization;remote sensing synthetic aperture radar;latent Dirichlet allocation;remote sensing image fusion field;probabilistic topic models;Sentinel-2 image fusion;multimodal probabilistic latent semantic analysis;unsupervised land-cover categorization tasks;Sentinel-2 remotely sensed data;Sentinel-1;multimodal feature patterns;novel pLSA-based image fusion approach","","28","","18","IEEE","21 Jun 2018","","","IEEE","IEEE Journals"
"Improved Flood Mapping Based on the Fusion of Multiple Satellite Data Sources and In-Situ Data","Y. -J. Kwak; R. Pelich; J. Park; W. Takeuchi","International Centre for Water Hazard and Risk Management (ICHARM-UNESCO), Public Works Research Institute, Tsukuba, Japan; Department of Environmental Research and Innovation (ERIN), Luxembourg Institute of Science and Technology (LIST), Belvaux, Luxembourg; Dept. of Environmental Information, Tokyo University of Information Sciences, Chiba, Japan; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3521","3523","For high accuracy flood mapping, an algorithm that integrates multiple satellite data sources is essential to maximize the sensor ability and compensate the limitations of optical and SAR data. The main objective of this study is to propose an algorithm of dynamic flood detection using optical and Synthetic Aperture Radar (SAR) images that compares and combines two different statistical thresholding approaches. To improve the flood detection accuracy, image fusion technique was investigated to maximize the utilization of calibrated and optimized flood maps as the integrated flood detection approach. To showcase the advantages of the proposed methodology, we employ MODIS, Landsat-8 and Sentinel-IA images acquired over a challenging area along the Brahmaputra River where flood events often occur.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517336","Dynamic flood mapping;multiple satellite data;MODIS;Sentinel-I;image fusion","Floods;MODIS;Synthetic aperture radar;Rivers;Optical imaging;Optical sensors;Earth","floods;geophysical image processing;hydrological techniques;image fusion;radar imaging;remote sensing by radar;rivers;synthetic aperture radar","multiple satellite data sources;high accuracy flood mapping;optical SAR data;dynamic flood detection;flood detection accuracy;image fusion technique;calibrated flood maps;optimized flood maps;integrated flood detection approach;flood events;statistical thresholding approaches;synthetic aperture radar;SAR images;MODIS images;Landsat-8 images;Sentinel-IA images","","3","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Improved Multiscale Edge Detection Method for Polarimetric SAR Images","R. Jin; J. Yin; W. Zhou; J. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2016","13","8","1104","1108","This letter presents a multiscale edge detection method for multilook polarimetric synthetic aperture radar (PolSAR) images based on the nonsubsampled contourlet transform (NSCT). The NSCT can provide flexible multiscale and directional decomposition. In the multiscale decomposition, the coefficients of the nonsubsampled pyramid in the NSCT are calculated via maximizing the polarimetric contrast between the adjacent subband levels, instead of using the difference of the adjacent subbands as used in the additive noise model. By this way, we make the NSCT applicable to PolSAR data and multiband data. Then, the edges are detected in the NSCT domain based on a fusion of the directional subband coefficients at different scales. Experimental results with both simulated and real PolSAR data show that the present approach is robust to noise and the extracted edges are complete and continuous.","1558-0571","","10.1109/LGRS.2016.2569534","National Natural Science Foundation of China(grant numbers:61490693,41171317); key project of the NSFC(grant numbers:61132008); Aeronautical Science Foundation of China; Fundamental Research Funds for the Central Universities(grant numbers:FRF-TP-15-090A1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484697","Edge detection;multiscale;nonsubsampled contourlet transform (CT) (NSCT);polarimetric synthetic aperture radar (PolSAR)","Image edge detection;Synthetic aperture radar;Computed tomography;Speckle;Image fusion;Wavelet transforms","feature extraction;geophysical image processing;radar imaging;remote sensing by radar;synthetic aperture radar","real PolSAR data;simulated PolSAR data;directional subband coefficient fusion;multiband data;PolSAR data;additive noise model;directional decomposition;flexible multiscale decomposition;nonsubsampled contourlet transform;synthetic aperture radar;polarimetric SAR images;multiscale edge detection method","","12","","16","IEEE","7 Jun 2016","","","IEEE","IEEE Journals"
"Multiangle BSAR Imaging Based on BeiDou-2 Navigation Satellite System: Experiments and Preliminary Results","T. Zeng; D. Ao; C. Hu; T. Zhang; F. Liu; W. Tian; K. Lin","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","10 Jun 2015","2015","53","10","5760","5773","This paper analyzes the multiangle imaging results for bistatic synthetic aperture radar (BSAR) based on global navigation satellite systems (GNSS-BSAR). Due to the shortcoming of GNSS-BSAR images, a multiangle observation and data processing strategy based on BeiDou-2 navigation satellites was put forward to improve the quality of images and the value of system application. Twenty-six BSAR experiments were conducted and analyzed in different configurations. Furthermore, a region-based fusion algorithm using region-of-interest (ROI) segmentation was proposed to generate a high-quality fusion image. Based on the fusion image, typical targets such as water area, vegetation area, and artificial targets were compared and interpreted among single/multiple-angle images. The results reveal that the multiangle imaging method was a good technique to enhance image information, which might extend the applications of GNSS-BSAR.","1558-0644","","10.1109/TGRS.2015.2430312","National Natural Science Foundation of China(grant numbers:61120106004,61225005,61427802,61471038); Chang Jiang Scholars Program(grant numbers:T2012122); Beijing Higher Education Young Elite Teacher Project(grant numbers:YETP1168); 111 Project of China(grant numbers:B14010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112619","Bistatic synthetic aperture radar (BSAR);global navigation satellite system (GNSS);image fusion;image interpretation;multiangle;Bistatic synthetic aperture radar (BSAR);global navigation satellite system (GNSS);image fusion;image interpretation;multiangle","Satellites;Imaging;Global Positioning System;Image resolution;Radar imaging;Synthetic aperture radar","geophysical image processing;Global Positioning System;image fusion;image segmentation;radar imaging;remote sensing by radar;synthetic aperture radar","image information enhancement;multiangle imaging method;image fusion;ROI segmentation;region of interest segmentation;region-based fusion algorithm;image quality improvement;data processing strategy;multiangle observation;GNSS-BSAR images;global navigation satellite system;bistatic synthetic aperture radar;BeiDou-2 navigation satellite system;multiangle BSAR imaging","","39","","37","IEEE","26 May 2015","","","IEEE","IEEE Journals"
"A Bayesian Method for ViSAR Image Fusion Using Effective Reflection Coefficient","D. Song; R. Tharmarasa; K. Han; W. Wang; M. McDonald; T. Kirubarajan","College of Information and Communication, National University of Defense Technology, Xi’an, China; Department of Electrical and Computer Engineering, McMaster University, Hamilton, ON, Canada; College of Information and Communication, National University of Defense Technology, Xi’an, China; School of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou, China; Defence Research and Development Canada, Ottawa, ON, Canada; Department of Electrical and Computer Engineering, McMaster University, Hamilton, ON, Canada","IEEE Sensors Journal","13 May 2022","2022","22","10","9743","9753","Since the isotropic scattering assumption does not hold in a wide-angle synthetic aperture radar, the video synthetic aperture radar (ViSAR), a new sensing mode by sequentially forming SAR images on a series of contiguous or overlapping sub-apertures, has the promising capability to capture aspect-dependent scattering behavior of objects. In this paper, a new Bayesian method for fusing ViSAR images is proposed. First, the notion of effective reflection coefficient is defined to characterize aspect-dependent scattering behavior of objects. Based on the ViSAR images sequence, the spatial effective reflection coefficient in the area of interest (AOI) when observing at different aspect angles are estimated. A Bayesian hypothesis testing is then derived to fuse the obtained effective reflection coefficient estimates for detecting scatterers in the AOI. The performance of the proposed method is evaluated and compared with that of the conventional GLRT-based ViSAR image fusion method in a simulated scenario. Numerical results demonstrate the superiority of the proposed method in capturing the aspect-dependent scattering characteristics as well as the spatial structure of objects.","1558-1748","","10.1109/JSEN.2022.3166825","National Science Foundation for Young Scientists of China(grant numbers:62101558); Research Foundation of the National University of Defense University(grant numbers:ZK21-38); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755882","Video synthetic aperture radar (ViSAR);aspect-dependent scattering;effective reflection coefficient","Imaging;Scattering;Apertures;Synthetic aperture radar;Radar polarimetry;Manganese;Image resolution","Bayes methods;image fusion;image sequences;radar imaging;synthetic aperture radar","Bayesian method;isotropic scattering assumption;wide-angle synthetic aperture radar;video synthetic aperture radar;SAR images;contiguous sub-apertures;overlapping sub-apertures;ViSAR images sequence;spatial effective reflection coefficient;different aspect angles;Bayesian hypothesis testing;conventional GLRT-based ViSAR image fusion method;aspect-dependent scattering characteristics","","","","37","IEEE","12 Apr 2022","","","IEEE","IEEE Journals"
"An Intensity-Space Domain CFAR Method for Ship Detection in HR SAR Images","C. Wang; F. Bi; W. Zhang; L. Chen","Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, North China University of Technology, Beijing, China; School of Information and Electronics, North China University of Technology, Beijing, China; Beijing Key Laboratory of Embedded Real-Time Information Processing Technology, Beijing Institute of Technology, Beijing, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","4","529","533","Synthetic aperture radar (SAR) is an indispensable and extensively used sensor in ship detection. As high-resolution SAR introduces more spatial details into images, this letter proposes an intensity-space (IS) domain constant false alarm rate (CFAR) ship detector to make good use of this information. The method fuses intensity of each pixel and correlations between pixels into one characteristic, i.e., IS index. All the detection procedures center on the calculation and analysis of IS index. First, a new transform maps an image into a new IS domain. Structures like ships and wakes are enhanced in IS domain. Second, a CFAR detector picks up high IS index pixels. Third, a chain of target features is checked to screen out false candidate target pixels. Also, enhanced wakes are taken to improve detection results. Experiments on real SAR images validate that the proposed transform does enhance these structures and the whole algorithm is of good performance, especially in the case of low-contrast targets.","1558-0571","","10.1109/LGRS.2017.2654450","National Natural Science Foundation of China(grant numbers:61601006,91438203); Chang Jiang Scholars Program(grant numbers:T2012122); Hundred Leading Talent Project of Beijing Science and Technology(grant numbers:Z141101001514005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7851007","Constant false alarm rate (CFAR);high-resolution (HR) synthetic aperture radar (SAR);intensity-space (IS) domain;ship detection;target enhancement","Marine vehicles;Synthetic aperture radar;Detectors;Transforms;Indexes;Image resolution;Clutter","geophysical image processing;image fusion;image resolution;object detection;radar imaging;remote sensing by radar;synthetic aperture radar;transforms","intensity-space domain CFAR method;ship detection;HR SAR Images;synthetic aperture radar;intensity-space domain constant false alarm rate ship detector;intensity fusion;IS index pixels","","86","","13","IEEE","13 Feb 2017","","","IEEE","IEEE Journals"
"Topology Design for Geosynchronous Spaceborne–Airborne Multistatic SAR","H. An; J. Wu; Z. Sun; J. Yang; Y. Huang; H. Yang","School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","2 Nov 2018","2018","15","11","1715","1719","Geosynchronous (GEO) spaceborne-airborne multistatic synthetic aperture radar (GEO MulSAR) is more flexible and accessible in remote sensing applications because of the high-altitude illuminator and the separation of the receivers and transmitter. In addition, the information obtained by the multiple airborne receivers can be fused to enhance the spatial resolution. However, the fused spatial resolution severely depends on the applied multistatic topology. To achieve the optimal fused spatial resolution by properly adjusting the imaging topology, a topology design method is proposed in this letter. First, the spatial resolution model of GEO MulSAR is given, and the dependence of the spatial resolution on the multistatic topology is analyzed in detail. Then, a topology design method is proposed to obtain the best multistatic topology that simultaneously optimizes the resolution cell area and resolution disequilibrium factor. Finally, the simulation results validate the effectiveness of the proposed method, and some insights into designing the multistatic topology are given.","1558-0571","","10.1109/LGRS.2018.2856502","National Natural Science Foundation of China(grant numbers:61771113); Fundamental Research Funds for Central Universities(grant numbers:2672018ZYGX2018J017); SAST Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434232","Geosynchronous (GEO) spaceborne–airborne synthetic aperture radar (SAR);multistatic SAR;resolution enhancement;topology design","Spatial resolution;Receivers;Synthetic aperture radar;Topology;Transmitters;Imaging","airborne radar;image fusion;image resolution;radar imaging;radar receivers;radar resolution;radar transmitters;remote sensing by radar;spaceborne radar;synthetic aperture radar","geosynchronous spaceborne-airborne multistatic synthetic aperture radar;GEO MulSAR;remote sensing applications;high-altitude illuminator;transmitter;multiple airborne receivers;optimal fused spatial resolution;imaging topology;topology design method;spatial resolution model;resolution cell area;resolution disequilibrium factor;fused spatial resolution;multistatic topology;resolution cell area optimization","","13","","11","IEEE","13 Aug 2018","","","IEEE","IEEE Journals"
"Unsupervised SAR Image Segmentation Using Ambiguity Label Information Fusion in Triplet Markov Fields Model","F. Wang; Y. Wu; P. Zhang; Q. Zhang; M. Li","Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; National Key Lab. of Radar Signal Processing, Xidian University, Xi’an, China; China Academy of Space Technology, Beijing Institute of Space System Engineering, Beijing, China; National Key Lab. of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","25 Aug 2017","2017","14","9","1479","1483","The recently proposed triplet Markov fields (TMF) model enhances the nonstationary image prior modeling ability by introducing an auxiliary field. Motivated by the TMF model, we propose a generalized TMF model based on ambiguity label information fusion (ALF-TMF) for synthetic aperture radar (SAR) image segmentation. The redefined auxiliary field in ALF-TMF indicates the dominant direction of local image contents and gives explicit nonstationary divisions of SAR images. To reduce the influence of unreliable observations caused by speckle noise, the original label field is adaptively generalized by introducing ambiguity class based on image observation and local nonstationary contextual information. Given the extended label field, prior and likelihood terms are constructed and merged to provide the posterior segmentation decision via the Bayesian fusion rule. Real SAR images are utilized in the experimental analysis, and the effectiveness of the proposed method is validated accordingly.","1558-0571","","10.1109/LGRS.2017.2715223","Natural Science Foundation of China(grant numbers:61272281,61271297,61301284); Fundamental Research Funds for the Central Universities(grant numbers:JB142001-1); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2015JM6288); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004442","Ambiguity label field extension;Bayesian fusion;nonstationary division;synthetic aperture radar (SAR) image segmentation;triplet Markov fields (TMF)","Synthetic aperture radar;Image segmentation;Uncertainty;Bayes methods;Markov processes;Speckle;Indexes","Bayes methods;image fusion;image segmentation;Markov processes;maximum likelihood estimation;radar imaging;synthetic aperture radar","unsupervised SAR image segmentation;ambiguity label information fusion;triplet Markov field model;generalized TMF model;nonstationary image enhancement;auxiliary field;synthetic aperture radar image segmentation;speckle noise;posterior segmentation decision;Bayesian fusion rule","","10","","10","IEEE","8 Aug 2017","","","IEEE","IEEE Journals"
"Bistatic aspect diversity for improved SAR target recognition","E. E. Laubie; B. D. Rigling; R. P. Penno","Department of Electrical and Computer Engineering, University of Dayton, Dayton, OH; Department of Electrical Engineering, Wright State University, Dayton, OH; Department of Electrical and Computer Engineering, University of Dayton, Dayton, OH","2015 IEEE Radar Conference (RadarCon)","25 Jun 2015","2015","","","0736","0741","This paper analyzes the potential for improvement in the performance of automatic target recognition (ATR) for synthetic-aperture radar (SAR) with bistatic aspect diversity. Initial assessments using decision-level fusion of monostatic observations with bistatic observations provide promising results. Data was generated using three civilian vehicle facet files and an electromagnetic scattering simulator. Classification was performed using normalized cross-correlation template matching and majority voting. Results showed an increase in the probability of correct classification with decision-level fusion of bistatic observations over classification using single observations.","2375-5318","978-1-4799-8232-5","10.1109/RADAR.2015.7131093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131093","automatic target recognition;aspect diversity;bistatic radar;synthetic aperture radar","Synthetic aperture radar;Correlation;Target recognition;Transmitters;Scattering;Vehicles;Receivers","image classification;image fusion;image matching;object recognition;radar imaging;synthetic aperture radar","bistatic aspect diversity;improved SAR automatic target recognition;synthetic aperture radar ATR;monostatic observations decision- level fusion;civilian vehicle facet files;electromagnetic scattering simulator;normalized cross-correlation template matching;correct classification probability;SAR images","","6","","12","IEEE","25 Jun 2015","","","IEEE","IEEE Conferences"
"Classification of fusing SAR and multispectral image via deep bimodal autoencoders","J. Geng; H. Wang; J. Fan; X. Ma","School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China; National Marine Environmental Monitoring Center, Dalian, China; School of Information and Communication Engineering, Dalian University of Technology, Dalian, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","823","826","Classification of multisensor data provides potential advantages over a single sensor in accuracy. In this paper, deep bimodal autoencoders are proposed for classification of fusing synthetic aperture radar (SAR) and multispectral images. The proposed deep network based on autoencoders is trained to discover both independencies of each modality and correlations across the modalities. Specifically, the sparse encoding layers in the front are applied to learn features of each modality, then shared representation layers in the middle are developed to learn fused features of two modalities, finally softmax classifier in the top is adopted for classification. Experimental results demonstrate that the proposed network is able to yield superior classification performance compared with some related networks.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127079","Data fusion;image classification;deep learning;synthetic aperture radar (SAR) image;multispectral image","Synthetic aperture radar;Feature extraction;Remote sensing;Data integration;Encoding;Kernel;Training","feature extraction;image classification;image fusion;neural nets;pattern classification;radar computing;radar imaging;sensor fusion;synthetic aperture radar","multispectral image classification;fusing SAR image classification;synthetic aperture radar;softmax classifier;multisensor data;fused features;sparse encoding layers;modality;deep network;multispectral images;deep bimodal autoencoders;single sensor","","4","","11","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Fusion of intensity/coherent information using region covariance features for unsupervised classification of SAR imagery","X. Yang; S. Tu; Y. Bai; W. Yang","School of Electronic Information, Wuhan University, Wuhan, China; Shanghai Institute of Satellite Engineering, Shanghai, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","941","944","Unsupervised classification of synthetic aperture radar (SAR) imagery is an essential step in SAR image interpretation. There is a growing demand for an efficient way to fuse multi-information of SAR imagery. This paper presents an intensity/coherent information fusion algorithm by using region covariance features for unsupervised classification. More precisely, we firstly extract the intensity properties and coherent characteristics from each pixel of SAR imagery, then use the region covariance descriptor to fuse the intensity and coherent features, and finally exploit the K-means algorithm to obtain the final unsupervised classification map. Experimental results on SAR imagery demonstrate the effectiveness of the proposed fusion scheme.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729238","unsupervised classification;synthetic aperture radar (SAR);feature fusion;region covariance","Synthetic aperture radar;Coherence;Feature extraction;Covariance matrices;Rivers;Buildings;Classification algorithms","geophysical image processing;image classification;image fusion;image resolution;remote sensing by radar;synthetic aperture radar","region covariance features;synthetic aperture radar imagery unsupervised classification;SAR image interpretation;SAR imagery multiinformation;intensity information fusion algorithm;intensity properties;coherent characteristics;region covariance descriptor;K-means algorithm;final unsupervised classification map;fusion scheme;coherent information fusion algorithm","","3","","15","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Multiresolution and Multimodality Sar Data Fusion Based on Markov and Conditional Random Fields for Unsupervised Change Detection","D. Solarna; G. Moser; S. B. Serpico","University of Genoa, Dept. of Electrical, Electronic, Telecommunication Eng. and Naval Architecture, Genoa, Italy; University of Genoa, Dept. of Electrical, Electronic, Telecommunication Eng. and Naval Architecture, Genoa, Italy; University of Genoa, Dept. of Electrical, Electronic, Telecommunication Eng. and Naval Architecture, Genoa, Italy","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","29","32","Current satellite missions (e.g., COSMO-SkyMed, Sentinel-1) collect single- or multipolarimetric synthetic aperture radar (SAR) images with multiple spatial resolutions and possibly short revisit times. The availability of heterogeneous data requires effective methods able to exploit all the available information. In the context of environmental monitoring and natural disaster recovery, this paper proposes an unsupervised change detection method able to properly fuse and exploit multiresolution and multimodality SAR data. The data fusion process is based on the estimation of the virtual images that would have been collected in case all the sensors worked at the same spatial resolution and on the definition of a probabilistic model based on generalized Gaussian distributions and Gram-Charlier approximations. The detection of changes is addressed in a probabilistic graphical framework through a novel conditional random field, by defining an energy function that is minimized through graph-cuts or belief propagation methods.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898122","synthetic aperture radar (SAR);multiresolution fusion;multimodality fusion;Markov random fields (MRF);conditional random fields (CRF)","Synthetic aperture radar;Spatial resolution;Lattices;Belief propagation;Maximum likelihood estimation;Markov processes","environmental monitoring (geophysics);Gaussian distribution;geophysical image processing;image fusion;image resolution;radar detection;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","multiple spatial resolutions;heterogeneous data;environmental monitoring;natural disaster recovery;unsupervised change detection method;multiresolution;data fusion process;virtual images;spatial resolution;conditional random field;belief propagation methods;conditional random fields;current satellite missions;COSMO-SkyMed;Sentinel-1;multipolarimetric synthetic aperture radar images;SAR","","3","","14","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Registration of SAR and Optical Images by Weighted Sift Based on Phase Congruency","S. Jiang; U. Jzang; B. Wang; X. Zhu; M. Xiang; X. FU; X. Sun","Institute of Electronics Chinese Academy of Sciences, Beijing, CN; Institute of Electronics Chinese Academy of Sciences, Beijing, CN; State Key Lab. of Microwave Imaging Technol., Inst. of Electron., Beijing, China; Institute of Electronics Chinese Academy of Sciences, Beijing, CN; Institute of Electronics Chinese Academy of Sciences, Beijing, CN; Institute of Electronics Chinese Academy of Sciences, Beijing, CN; University of Chinese Academy of Sciences, Beijing, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","8885","8888","In this paper, to address problems in the registration of synthetic aperture radar (SAR) and optical images due to large gray differences, the scale-invariant feature transform (SIFT) approach based on phase congruency (PC-SIFT) is proposed. This approach is used to address the gradient inversion in multi-source images, and it is based on optimizing the dominant direction interval of the descriptors. We construct a new descriptor by combining phase consistency and the gradient amplitude, which is referred to as PCG-SIFT descriptor. The proposed algorithm is suitable for multi-sensor images with large gray differences and significant edge features The results of experiments show that compared to the traditional gradient-based SIFT descriptor, the PC-SIFT descriptor and PCG-SIFT descriptor improve the robustness and matching probability of the registration algorithm for multi-source images.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519181","image registration;optical image;scale invariant feature transform;synthetic aperture radar","Optical imaging;Synthetic aperture radar;Adaptive optics;Feature extraction;Optical sensors;Optical filters;Image edge detection","gradient methods;image colour analysis;image fusion;image registration;image sensors;optical information processing;probability;radar imaging;synthetic aperture radar;transforms","optical image registration;multisensor imaging;weighted SIFT approach;synthetic aperture radar registration;scale-invariant feature transform approach;SAR registration algorithm;multisource image gradient inversion;gradient-based SIFT descriptor;probability;PC-SIFT descriptor;PCG-SIFT descriptor;phase consistency;phase congruency","","3","","7","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A novel fast back projection algorithm based on subaperture frequency spectrum fusion in Cartesian coordinates","L. Min; L. Zhou","Department of Applied Science and Frontier Technology, Qian Xuesen Laboratory of Space Technology, Beijing, china; Beijing Institute of remote sensing, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","999","1002","It is known to all that time-domain back projection (BP) algorithm has several advantages for synthetic aperture radar (SAR) imaging[1], such as easily motion compensation for any flight track, unlimited size of the observe scene, arbitrary wide bandwidth and large integration angle. Because of the heavy computational burden of BP algorithm, many fast back projection (FBP) algorithms have proposed in recent years[2-6]. We introduce a novel fast back projection algorithm which can speed up image formation and avoid coordinates transform at the same time. This method mainly utilizes the character of subapertures' oversampling and reconstructs the bandwidth by fusing all the subbands of subapertures. It also analyses the effects and influence factors for the number of subapertures. The simulation result shows that the algorithm can efficiently reduces the computational complexity while keep the image resolution.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729253","Synthetic aperture radar;fast back projection;polar coordinates;Cartesian coordinates","Apertures;Synthetic aperture radar;Bandwidth;Approximation algorithms;Azimuth;Image resolution;Algorithm design and analysis","computational complexity;image fusion;image reconstruction;image resolution;motion compensation;radar imaging;synthetic aperture radar","fast back projection algorithm;subaperture frequency spectrum fusion;Cartesian coordinates;time-domain back projection algorithm;synthetic aperture radar imaging;SARimaging;motion compensation;flight track;observe scene;arbitrary wide bandwidth;integration angle;FBP algorithm;image formation;subaperture oversampling;bandwidth reconstruction;subaperture fusion;computational complexity;image resolution","","2","","7","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Multifeature Transformation and Fusion-Based Ship Detection With Small Targets and Complex Backgrounds","M. Zha; W. Qian; W. Yang; Y. Xu","School of Software, Jiangxi Agricultural University, Nanchang, China; School of Software, Jiangxi Agricultural University, Nanchang, China; School of Software, Jiangxi Agricultural University, Nanchang, China; School of Software, Jiangxi Agricultural University, Nanchang, China","IEEE Geoscience and Remote Sensing Letters","29 Jul 2022","2022","19","","1","5","With the development of deep learning, synthetic aperture radar (SAR) image ship detection based on the convolutional neural network has made significant progress. However, there are two problems: 1) the false alarm detection rate is high due to complex background and coherent speckle noise interference and 2) for smaller ship targets, missed detection is prone to occur. In this letter, a novel ship detection model based on multifeature transformation and fusion (MFTF-Net) is proposed to address the issues. First, to avoid the randomness of initial point selection and the influence of outlier points, the anchor frame clustering approach based on the  $K$ -medians++ algorithm is presented to cluster the object candidate frames. Second, the low-level feature information is passed to the high level by constructing a local enhancement network; then, an improved transformer structure is introduced to replace the last convolutional block of the backbone network to obtain rich contextual information. Finally, a four-scale residual feature fusion network is designed, which fully fuses the object’s detailed and semantic information. In addition, improved convolutional block attention module (CBAM) and squeeze and excitation (SE) attention mechanisms are applied in the lower two layers and upper two layers of the network output to reduce the interference of confusing information, respectively. The experimental results demonstrate that the proposed method is superior to the state-of-the-art 13 baseline models on SAR ship detection dataset (SSDD), high-resolution SAR images dataset (HRSID), and SAR-ship-dataset public datasets in terms of the mean average precision (mAP), recall, accuracy, and  $F1$  metrics.","1558-0571","","10.1109/LGRS.2022.3192559","National Key Research and Development Program of China(grant numbers:2020YFD1100605); National Natural Science Foundation of China(grant numbers:61966016,62166020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9833507","Feature fusion;multifeature;ship detection;synthetic aperture radar (SAR) image;transformer","Feature extraction;Marine vehicles;Clustering algorithms;Transformers;Training;Interference;Synthetic aperture radar","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image enhancement;image fusion;image resolution;object detection;radar imaging;ships;synthetic aperture radar","network output;convolutional block attention module;four-scale residual feature fusion network;rich contextual information;backbone network;transformer structure;local enhancement network;low-level feature information;object candidate frames;anchor frame;outlier points;initial point selection;multifeature transformation;ship detection model;missed detection;smaller ship targets;coherent speckle noise interference;false alarm detection rate;convolutional neural network;synthetic aperture radar image ship detection;deep learning;complex background;SAR-ship-dataset public datasets","","1","","29","IEEE","20 Jul 2022","","","IEEE","IEEE Journals"
"Detection of flooded areas from multitemporal SAR images","N. Kalpana; A. Sivasankar","FACULTY/ECE, Anna University Regional Campus, Madurai, India; FACULTY/ECE, Anna University Regional Campus, Madurai, India","2016 Second International Conference on Science Technology Engineering and Management (ICONSTEM)","8 Sep 2016","2016","","","1","5","Multi temporal synthetic aperture radar images are available, precise calibration and perfect spatial register are required to get a useful image for displaying changes that contain occurred. SAR calibration is a extremely complex and sensitive problem; a few errors may persist after calibration that interferes with subsequent steps in the data fusion and visualization process. Because of the non-Gaussian model of radar backscattering, traditional image pre processing procedures cannot be used here. To solve this problem “cross-calibration/normalization,” method can be used. In image enhancement and the numerical comparison of many image takes together with data fusion and visualization processes. The proposed processing which contain filtering, histogram truncation, and equalization steps and region growing and merging algorithm applied in an adaptive way to the images. RGB composition is used to combining an pre & post flood image or identify an flooded areas.","","978-1-5090-1706-5","10.1109/ICONSTEM.2016.7560951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7560951","Data fusion;flood detection;image enhancement;multi temporal synthetic aperture radar (SAR) imagery;RGB composition","Synthetic aperture radar;Histograms;Entropy;Speckle;Calibration;Radar imaging","calibration;data visualisation;floods;geophysical image processing;image colour analysis;image enhancement;image filtering;image fusion;remote sensing by radar;synthetic aperture radar","flooded area detection;multitemporal synthetic aperture radar images;perfect spatial register;SAR calibration;data fusion;visualization process;nonGaussian model;radar backscattering;image preprocessing procedures;cross-calibration method;image enhancement;numerical comparison;filtering step;histogram truncation step;equalization step;RGB composition;postflood image;preflood image;cross-normalization method","","1","","15","IEEE","8 Sep 2016","","","IEEE","IEEE Conferences"
"Visual Saliency Aided SAR and Optical Image Matching","E. Citak; G. Bilgin","Department of Computer Engineering, Yildiz Technical University, Istanbul, Turkey; Department of Computer Engineering, Yildiz Technical University, Istanbul, Turkey","2019 Innovations in Intelligent Systems and Applications Conference (ASYU)","2 Jan 2020","2019","","","1","5","This paper proposes a novel deep neural network training procedure for matching optical image and synthetic aperture radar image. This deep neural network architecture uses convolutional Siamese neural network and visual saliency map to enhance convolutional Siamese neural network's power of features. Computational visual saliency maps can be thought as an indicator for neural network architecture to emphasize some important neural network features. Firstly, well-known visual saliency map extraction algorithms have analyzed then discussed to determine fusion strategy with main neural network, convolutional Siamese neural network. Another core idea is about Siamese network. Two different Siamese networks have studied, Pseudo-Siamese CNN and Identical-Siamese CNN, and experiments are resulted in detail. Experiments show that computational visual saliency map can help to Siamese networks to select more informative features in matching process. Incorporation with visual saliency map increases matching accuracy whether Siamese network shares its weights or not.","","978-1-7281-2868-9","10.1109/ASYU48272.2019.8946408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946408","Optical image;synthetic aperture radar (SAR);Siamese network;visual saliency map;image matching","Visualization;Optical imaging;Feature extraction;Optical sensors;Optical fiber networks;Adaptive optics;Synthetic aperture radar","convolutional neural nets;feature extraction;image fusion;image matching;image segmentation;neural net architecture;optical images;radar imaging;synthetic aperture radar","visual saliency aided SAR;deep neural network training procedure;synthetic aperture radar image;deep neural network architecture;convolutional Siamese neural network;neural network features;Siamese network;identical-Siamese CNN;pseudoSiamese CNN;visual saliency map extraction algorithms;optical image matching;fusion strategy","","1","","24","IEEE","2 Jan 2020","","","IEEE","IEEE Conferences"
"Multi-View Fusion Based on Expectation Maximization for SAR Target Recognition","Y. Zhang; X. Guo; H. Ren; Q. Wan; X. Shen","University of Electronic and Science Technology of China, Chengdu, China; University of Electronic and Science Technology of China, Chengdu, China; University of Electronic and Science Technology of China, Chengdu, China; University of Electronic and Science Technology of China, Chengdu, China; University of Electronic and Science Technology of China, Chengdu, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","778","781","Images from different aspect views for one target, known as multiple views, are widely applied to improve synthetic aperture radar (SAR) target recognition. However, most of existing multi-view methods have strict constraint on the angle interval among multiple views. In this paper, a new multiview fusion method free from interval limitation using expectation maximization (EM) is explored for SAR image classification. Firstly, we apply convolutional neural network (CNN) to extract features effectively owning to its powerful ability of feature learning and then obtain the classification probability. Secondly, Multi-view Label Set (MLS) is automatically constructed from multiple views according to the probability and finally we use EM algorithm to classify SAR images intelligently. It is worth noting that the proposed method can be used flexibly according to the number of perspectives obtained and without angle interval constraint among multiple views. Experiments demonstrate that the proposed method has better recognition performance than some state-of-the-art methods on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323545","National Natural Science Foundation of China(grant numbers:61371184,61671137,61771114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323545","Expectation maximization;synthetic aperture radar;Multi-view Label Set","Target recognition;Reliability;Feature extraction;Synthetic aperture radar;Testing;Radar polarimetry;Prediction algorithms","feature extraction;image classification;image fusion;image recognition;learning (artificial intelligence);neural nets;object recognition;radar imaging;radar target recognition;synthetic aperture radar","expectation maximization;SAR image classification;Multiview Label Set;SAR images;angle interval constraint;SAR Target Recognition;different aspect views;synthetic aperture radar target recognition;existing multiview methods;multiview fusion method free","","","","8","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Target Detection Method in SAR Images Based on Superpixel Segmentation","M. Liu; S. Chen; F. Lu; M. Xing; J. Wei","School of Computer Science, Shaanxi Normal University, Xi'an, China; Xi'an Modern Control Technology, Research Institute, Xi'an, China; Xi'an Modern Control Technology, Research Institute, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; Army Aviation Research Institute, Beijing, China","2020 IEEE 3rd International Conference on Electronic Information and Communication Technology (ICEICT)","1 Feb 2021","2020","","","528","530","A synthetic aperture radar (SAR) target detection method based on the fusion of multiscale superpixel segmentations is proposed in this paper. SAR images are segmented between land and sea firstly by using superpixel technology in different scales. Secondly, image segmentation results together with the constant false alarm rate (CFAR) detection result are coalesced. Finally, target detection is realized by fusing different scale results. The effectiveness of the proposed algorithm is tested on Sentinel-1A data.","","978-1-7281-9045-7","10.1109/ICEICT51264.2020.9334216","National Natural Science Foundation of China(grant numbers:61701289); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9334216","synthetic aperture radar (SAR) images;target detection;superpixel segmentation;constant false alarm rate (CFAR)","Image segmentation;Conferences;Object detection;Detectors;Radar polarimetry;Information and communication technology;Synthetic aperture radar","image fusion;image segmentation;radar detection;radar imaging;synthetic aperture radar","SAR images;synthetic aperture radar target detection method;multiscale superpixel segmentations;superpixel technology;image segmentation results;constant false alarm rate detection result;Sentinel-1A data","","","","10","IEEE","1 Feb 2021","","","IEEE","IEEE Conferences"
"Low-Resolution Fully Polarimetric SAR and High-Resolution Single-Polarization SAR Image Fusion Network","L. Lin; J. Li; H. Shen; L. Zhao; Q. Yuan; X. Li","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Geoscience and Remote Sensing","8 Feb 2022","2022","60","","1","17","The data fusion technology aims to aggregate the characteristics of different data and to obtain products with multiple data advantages. To solve the problem of reduced resolution of polarimetric synthetic aperture radar (PolSAR) images due to system limitations, we propose a fully PolSAR images and single-polarization synthetic aperture radar (SinSAR) images fusion network to generate high-resolution PolSAR (HR-PolSAR) images. To take advantage of the polarimetric information of the low-resolution PolSAR (LR-PolSAR) images and the spatial information of the high-resolution single-polarization SAR (HR-SinSAR) images, we propose a fusion framework for joint LR-PolSAR images and HR-SinSAR images and design a cross-attention mechanism to extract features from the joint input data. Besides, based on the physical imaging mechanism, we designed the PolSAR polarimetric loss functions for constrained network training. The experimental results confirm the superiority of the fusion network over traditional algorithms. The average peak signal-to-noise ratio (PSNR) is increased by more than 3.6 dB, and the average mean absolute error (MAE) is reduced to less than 0.07. Experiments on polarimetric decomposition and polarimetric signature show that it maintains polarimetric information well.","1558-0644","","10.1109/TGRS.2021.3121166","National Natural Science Foundation of China(grant numbers:61671334,62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583928","Cross-attention mechanism (CroAM);fully polarimetric synthetic aperture radar (PolSAR);fusion;polarimetric loss;RCNN;single-polarization SAR (SinSAR)","Feature extraction;Superresolution;Data mining;Synthetic aperture radar;Spatial resolution;Radar polarimetry;Training","correlation methods;image fusion;radar imaging;radar polarimetry;remote sensing by radar;sensor fusion;synthetic aperture radar","low-resolution fully polarimetric sar;high-resolution single-polarization sar image fusion network;data fusion technology;multiple data advantages;reduced resolution;polarimetric synthetic aperture radar images;fully PolSAR images;single-polarization synthetic aperture radar images fusion network;high-resolution PolSAR;polarimetric information;low-resolution PolSAR;high-resolution single-polarization SAR images;fusion framework;joint LR-PolSAR images;HR-SinSAR images;joint input data;physical imaging mechanism;PolSAR polarimetric loss functions;constrained network training;polarimetric decomposition;polarimetric signature show","","2","","45","IEEE","21 Oct 2021","","","IEEE","IEEE Journals"
"Analysis of images SAR to flood prevention implementing fusion methods","J. E. Vera; S. F. Mora; J. A. Torres; J. Avendano",Universidad ECCI; Universidad ECCI; Universidad ECCI; Universidad ECCI,"2016 XXI Symposium on Signal Processing, Images and Artificial Vision (STSIVA)","17 Nov 2016","2016","","","1","5","To improve the characteristics of images taken by the IDEAM, the results obtained applying two algorithms to Synthetic Aperture Radar images or SAR images in regions of Colombia affected by natural disasters are discussed and compared. Two techniques of digital image processing were used, pyramidal fusion Morphological and the Discrete Wavelet Transform. An analysis of the responses obtained by each method was performed for determining which method is suitable according to pixellevel image fusion, for testing purposes and compare the two techniques, data about of entropy and correlation was calculated in MATLAB®. The results obtained show that the morphological fusion method presents a high performance in the SAR image processing, significantly improving the grouping of points on the test image.","2329-6259","978-1-5090-3797-1","10.1109/STSIVA.2016.7743334","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7743334","","Discrete wavelet transforms;Entropy;Band-pass filters;Synthetic aperture radar;Low-pass filters;Image fusion","discrete wavelet transforms;image fusion;radar imaging;synthetic aperture radar","flood prevention;fusion methods;synthetic aperture radar images;digital image processing;pyramidal fusion morphological;discrete wavelet transform;SAR image processing","","2","","14","IEEE","17 Nov 2016","","","IEEE","IEEE Conferences"
"The Effect of SAR Speckle Removal in SAR-Optical Image Fusion","S. Gençay; C. Özcan","Bilgisayar Teknolojileri, Manisa Celal Bayar Üniversitesi, Manisa, Türkiye; Yazılım Mühendisliği, Karabük Üniversitesi, Karabük, Türkiye","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","Due to the imaging mechanism of Synthetic Aperture Radar (SAR) and the noise in the images, visual identification of objects in the scene is not as easy as in optical images. SAR images have limited color information and cannot reflect the spectral information of objects. Optical images, on the other hand, have rich spectral information. SAR-Optical image fusion is an important area of study so that SAR data can be easily evaluated by anyone, but it is difficult to find a matching SAR and optical image of the same scene. In order to overcome this difficulty, Sentinel-1 and Sentinel-2 datasets have been published and image fusion studies have been carried out with various methods. However, it has been observed that the effect of SAR noise removal before merging on image fusion methods has not been investigated. In the studies conducted to investigate this effect, five different fusion algorithms used in the literature were tested with twenty different image groups using different noise reduction ratios. The success of the fusion results obtained was compared with five different metrics that are widely used in the literature. The images and metric results obtained as a result of the tests showed that the removal of speckle noise in the SAR data has a positive effect on the fusion results.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864861","optical image;SAR image;image fusion;remote sensing","Measurement;Visualization;Speckle;Optical imaging;Adaptive optics;Optical sensors;Optical reflection","image fusion;optical images;radar imaging;speckle;synthetic aperture radar","SAR speckle removal;SAR-Optical image fusion;imaging mechanism;SAR images;Optical images;rich spectral information;SAR data;matching SAR;image fusion studies;SAR noise removal;image fusion methods;different fusion algorithms;different image groups;fusion results","","","","0","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"The TanDEM-X DEM Mosaicking: Fusion of Multiple Acquisitions Using InSAR Quality Parameters","A. Gruber; B. Wessel; M. Martone; A. Roth","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Weßling, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Weßling, Germany; Microwaves and Radar Institute, German Aerospace Center (DLR), Weßling, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Weßling, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2016","9","3","1047","1057","Since 2010, TanDEM-X and its twin satellite TerraSAR-X fly in a close orbit formation and form a single-pass synthetic aperture radar (SAR) interferometer. The formation was established to acquire a global high-precision digital elevation model (DEM) using SAR interferometry (InSAR). In order to achieve the required height accuracy of the TanDEM-X DEM, at least two global coverages have to be acquired. However, in difficult and mountainous terrain, up to five coverages are present. Here, acquisitions from ascending and descending orbits are needed to fill gaps and to overcome geometric limitations. Therefore, a strategy to properly combine the available height estimates is mandatory. The objective of this paper is the presentation of the operational TanDEM-X DEM mosaicking approach. In general, multiple InSAR DEM heights are combined by means of a weighted average with the height error as weight. Apart from this widely used mosaicking approach, one big challenge remains with the handling of larger height discrepancies between the input data, which are mainly caused by phase unwrapping errors, but also by temporal changes between acquisitions. In the case of inconsistencies, the TanDEM-X mosaicking approach performs a grouping into height levels. A priority concept is set up to evaluate the different groups of heights considering the number of DEMs and several InSAR quality parameters: the height error, the phase unwrapping method, and the height of ambiguity. This allows the identification of the most reliable height level for mosaicking. This fusion concept is verified on different test areas affected by phase unwrapping errors in flat and mountainous terrain as well as by height discrepancies in forests. The results show that the quality of the final TanDEM-X DEM mosaic benefits a lot from this mosaicking approach.","2151-1535","","10.1109/JSTARS.2015.2421879","German Federal Ministry for Economic Affairs and Energy(grant numbers:50 EE 1035); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7109106","Digital elevation models (DEMs);image fusion;interferometric synthetic aperture radar (InSAR);mosaicking;TanDEM-X;Digital elevation models (DEMs);image fusion;interferometric synthetic aperture radar (InSAR);mosaicking;TanDEM-X","Reliability;Orbits;Synthetic aperture radar;Calibration;Accuracy;Coherence;Earth","digital elevation models;geophysical image processing;image fusion;image segmentation;radar interferometry;remote sensing by radar;synthetic aperture radar;terrain mapping","InSAR quality parameters;satellite TerraSAR-X;close orbit formation;single-pass synthetic aperture radar interferometer;global high-precision digital elevation model;TanDEM-X DEM height accuracy;ascending orbit;descending orbit;geometric limitations;TanDEM-X DEM mosaicking approach;multiple InSAR DEM height levels;weighted average;height error;input data;phase unwrapping errors;phase unwrapping method;mountainous terrain;flat terrain","","51","","28","OAPA","15 May 2015","","","IEEE","IEEE Journals"
"An Improved Image Fusion Method of Infrared Image and SAR Image Based on Contourlet and Sparse Representation","X. Ji","Jincheng college, Nanjing University of Aeronautics and Astronautics, Nanning, China","2015 7th International Conference on Intelligent Human-Machine Systems and Cybernetics","23 Nov 2015","2015","1","","282","285","In this paper, we propose an improved image fusion method of infrared image and SAR image based on the Contour let transform and sparse representation. For the method, it decompose the source image into the low frequency sub band coefficients and the high frequency sub band coefficients with the Contour let transform. The low frequency coefficients with lower sparseness are dealed with sparse representation, construct over complete dictionary, solve sparse coefficient over the trained dictionary, and choose the low frequency coefficients with the larger energy fusion rule. The high frequency sub band coefficients are fused by gradient maxim in. Different frequency coefficients are used to reconstruct the fused image by the inverse Contour let transform. Experimental results show that the proposed method is a feasible and effective image fusion method in term of visual quality and objective evaluation.","","978-1-4799-8646-0","10.1109/IHMSC.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334704","SAR image;infrared image;image fusion;contourlet transform;sparse representation","Image fusion;Sparse matrices;Wavelet transforms;Fuses;Image resolution;Image edge detection","gradient methods;image fusion;image representation;inverse transforms;minimax techniques;radar imaging;synthetic aperture radar","improved image fusion method;infrared image;SAR image;inverse contourlet transform;sparse representation;source image decomposition;low frequency sub band coefficients;high frequency sub band coefficients;energy fusion rule;sparse coefficient;gradient maximin;visual quality;objective evaluation","","1","","9","IEEE","23 Nov 2015","","","IEEE","IEEE Conferences"
"Cartesian based FFBP algorithm for circular SAR using NUFFT interpolation","Z. Guo; H. Zhang; S. Ye","AVIC LeiHua Electronic Technology Research Institute, Wuxi, China; AVIC LeiHua Electronic Technology Research Institute, Wuxi, China; AVIC LeiHua Electronic Technology Research Institute, Wuxi, China","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","30 Mar 2020","2019","","","1","5","Circular SAR is able to achieve omni-directional observation and high-resolution imaging of targets. However, the traditional frequency-domain based imaging algorithm is not suitable for complicated curve trajectory. Moreover the time domain based back-projection (BP) algorithm is applicable but time consuming. Fast factorized back-projection (FFBP) algorithm based on aperture decomposition and image fusion can balance computational efficiency and accuracy. In this paper, we proposed a modified FFBP algorithm for circular SAR imaging. The principal improvement is the usage of Cartesian coordinate imaging and nonuniform fast Fourier transform (NUFFT) interpolation. First, sub-aperture BP imaging is implemented on local Cartesian coordinate system. Then azimuth bandwidth is compressed with a spatial variant phase function to reduce the sampling rate. Next the NUFFT interpolation method is applied during sub-images fusion to further improve the efficiency of the algorithm. Finally, through simulation and real data experiments, the correctness and accuracy of the algorithm is verified.","2474-2333","978-1-7281-2912-9","10.1109/APSAR46974.2019.9048561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048561","Fast factorized back-projection (FFBP);Circular synthetic aperture radar (CSAR);Nonuniform fast Fourier transform (NUFFT);Cartesian coordinate system","","fast Fourier transforms;image fusion;image sampling;interpolation;radar imaging;synthetic aperture radar","modified Cartesian based FFBP algorithm;omnidirectional observation;curve trajectory;time domain analysis;aperture decomposition;circular SAR imaging;local Cartesian coordinate system;NUFFT interpolation method;subimage fusion;subaperture BP imaging;frequency-domain based imaging algorithm;fast factorized back-projection algorithm","","","","7","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"A Fusion Approach for Water Area Classification Using Visible, Near Infrared and Synthetic Aperture Radar for South Asian Conditions","S. K. Ahmad; F. Hossain; H. Eldardiry; T. M. Pavelsky","Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Department of Civil and Environmental Engineering, University of Washington, Seattle, USA; Geological Sciences Department, University of North Carolina, Chapel Hill, USA","IEEE Transactions on Geoscience and Remote Sensing","25 Mar 2020","2020","58","4","2471","2480","Consistent estimation of water surface area from remote sensing remains challenging in regions such as South Asia with vegetation, mountainous topography, and persistent monsoonal cloud cover. High-resolution optical imagery, which is often used for global inundation mapping, is highly impacted by clouds, while synthetic aperture radar (SAR) imagery is not impacted by clouds and is affected by both topographic layover and vegetation. Here, we compare and contrast inundation extent measurements from visible (Landsat-8 and Sentinel-2) and SAR (Sentinel-1) imagery. Each data type (wavelength) has complementary strengths and weaknesses which were gauged separately over selected water bodies in Bangladesh. High-resolution cloud-free PlanetScope imagery at 3-m resolution was used as a reference to check the accuracy of each technique and data type. Next, the optical and radar images were fused for a rule-based water area classification algorithm to derive the optimal decision for the water mask. Results indicate that the fusion approach can improve the overall accuracy by up to 3.8%, 18.2%, and 8.3% during the wet season over using the individual products of Landsat8, Sentinel-1, and Sentinel-2, respectively, at three sites, while providing increased observational frequency. The fusion-derived products resulted in overall accuracy ranging from 85.8% to 98.7% and Kappa coefficient varying from 0.61 to 0.83. The proposed SAR-visible fusion technique has potential for improving satellite-based surface water monitoring and storage changes, especially for smaller water bodies in humid tropical climate of South Asia.","1558-0644","","10.1109/TGRS.2019.2950705","National Aeronautics and Space Administration(grant numbers:80NSSC18M0099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8906149","Area classification;remote sensing;synthetic aperture radar (SAR);visible imagery;water bodies","Synthetic aperture radar;Remote sensing;Optical surface waves;Surface topography;Vegetation mapping;Satellites;Earth","geophysical image processing;hydrological techniques;image classification;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","satellite-based surface water monitoring;South Asia;water surface area;remote sensing;vegetation;mountainous topography;high-resolution optical imagery;global inundation mapping;synthetic aperture radar imagery;Sentinel-2;SAR imagery;Sentinel-1;high-resolution cloud-free PlanetScope imagery;optical radar images;rule-based water area classification algorithm;water mask;Landsat8;fusion-derived products;monsoonal cloud cover;SAR-visible fusion;water area classification;Bangladesh;PlanetScope imagery;humid tropical climate","","24","","39","IEEE","19 Nov 2019","","","IEEE","IEEE Journals"
"Fusion of Multifeature Low-Rank Representation for Synthetic Aperture Radar Target Configuration Recognition","X. Zhang; Y. Wang; D. Li; Z. Tan; S. Liu","College of Communication Engineering, Chongqing University, Chongqing, China; College of Communication Engineering, Chongqing University, Chongqing, China; Guangxi Key Laboratory of Wireless Wideband Communication and Signal Processing, Guilin, China; College of Communication Engineering, Chongqing University, Chongqing, China; College of Communication Engineering, Chongqing University, Chongqing, China","IEEE Geoscience and Remote Sensing Letters","26 Aug 2018","2018","15","9","1402","1406","In this letter, we propose a synthetic aperture radar (SAR) target configuration recognition algorithm based on the fusion of multifeature low-rank representations (LRRs). First, Gabor, principal component analysis, and wavelet features are extracted for the SAR training set and test set, respectively. Second, with the LRR model, each feature of the test samples is represented by those of the training set, leading to the corresponding coefficient matrix. Then, the preliminary prediction labels of all features of the test sample are obtained according to the LRR coefficients. Third, in order to further improve the confidence of recognition and reduce the instability of the algorithm, a two-stage decision fusion strategy is adopted to obtain the final prediction labels. The first stage utilizes a vote fusion for the recognition results of multiaspect neighborhood test samples for each feature pattern, which exploits the strong correlation of these neighborhood samples. Furthermore, the second stage fuses the three results obtained in the first stage through Bayesian inference. Bayesian inference is widely used in decision fusion, which can improve the confidence of results by about 3%. Experiments on the moving and stationary target acquisition and recognition data set demonstrate the effectiveness and superiority of the proposed algorithm.","1558-0571","","10.1109/LGRS.2018.2842068","National Natural Science Foundation of China(grant numbers:61301224,61501068); Basic and Advanced Research Project in Chongqing(grant numbers:cstc2017jcyjA1378,cstc2016jcyjA0134); Guangxi Key Laboratory of Wireless Wideband Communication and Signal Processing, 2017 the Fund Project of Director(grant numbers:GXKL06170202); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388228","Decision fusion;low-rank representation (LRR);synthetic aperture radar (SAR);target configuration recognition","Synthetic aperture radar;Feature extraction;Training;Target recognition;Principal component analysis;Signal processing algorithms;Bayes methods","Bayes methods;feature extraction;image fusion;image representation;pattern clustering;principal component analysis;radar imaging;radar target recognition;synthetic aperture radar","wavelet features;SAR training set;test set;LRR model;test sample;corresponding coefficient matrix;preliminary prediction labels;LRR coefficients;two-stage decision fusion strategy;final prediction labels;vote fusion;recognition results;multiaspect neighborhood test samples;feature pattern;neighborhood samples;stationary target acquisition;recognition data;synthetic aperture radar target configuration recognition algorithm;principal component analysis;multifeature low-rank representation fusion","","14","","18","IEEE","19 Jun 2018","","","IEEE","IEEE Journals"
"A Novel Salient Feature Fusion Method for Ship Detection in Synthetic Aperture Radar Images","G. Zhang; Z. Li; X. Li; C. Yin; Z. Shi","Department of Aerospace Science and Technology, Space Engineering University, Beijing, China; Department of Aerospace Science and Technology, Space Engineering University, Beijing, China; Department of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China; Department of Aerospace Science and Technology, Space Engineering University, Beijing, China; Department of Aerospace Science and Technology, Space Engineering University, Beijing, China","IEEE Access","8 Dec 2020","2020","8","","215904","215914","Ship detection of synthetic aperture radar (SAR) images is one of the research hotspots in the field of marine surveillance. Fusing salient features to detection network can effectively improve the precision of ship detection. However, how to effectively fuse the salient features of SAR images is still a difficult task. In this paper, to improve the ship detection precision, we design a novel one-stage ship detection network to fuse salient features and deep convolutional neural network (CNN) features. Firstly, a saliency map extraction algorithm is proposed. The algorithm is applied to generate saliency map by using multi-scale pyramid features and frequency domain features. Secondly, the backbone of the ship detection network contains a two-stream network. The upper-stream network uses the original SAR image as input to extract multi-scale deep CNN features. The lower-stream network uses the corresponding saliency map as input to acquire multi-scale salient features. Thirdly, for integrating the salient features to deep CNN features, a novel salient feature fusion method is designed. Finally, an improved bi-directional feature pyramid network is applied to the ship detection network for reducing the computational complexity and network parameters. The proposed methods are evaluated on the public ship detection dataset and the experimental results shows that it can make a significant improvement in the precision of SAR image ship detection.","2169-3536","","10.1109/ACCESS.2020.3041372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273001","Ship detection;synthetic aperture radar images;feature fusion;saliency map;deep convolutional neural network","Marine vehicles;Feature extraction;Radar polarimetry;Proposals;Synthetic aperture radar;Surveillance;Fuses","convolutional neural nets;feature extraction;frequency-domain analysis;image fusion;image resolution;marine radar;object detection;radar computing;radar detection;radar imaging;search radar;ships;synthetic aperture radar","network parameters;public ship detection dataset;SAR image ship detection;synthetic aperture radar images;one-stage ship detection network;deep convolutional neural network features;saliency map extraction algorithm;multiscale pyramid features;frequency domain features;two-stream network;upper-stream network;lower-stream network;multiscale salient features;novel salient feature fusion method;improved bi-directional feature pyramid network;computational complexity;marine surveillance;multiscale deep CNN feature extraction","","13","","53","CCBY","30 Nov 2020","","","IEEE","IEEE Journals"
"Synthetic Aperture Radar Image Change Detection via Siamese Adaptive Fusion Network","Y. Gao; F. Gao; J. Dong; Q. Du; H. -C. Li","Qingdao Key Laboratory of Mixed Reality, and Virtual Ocean, School of Information Science, and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality, and Virtual Ocean, School of Information Science, and Engineering, Ocean University of China, Qingdao, China; Qingdao Key Laboratory of Mixed Reality, and Virtual Ocean, School of Information Science, and Engineering, Ocean University of China, Qingdao, China; Department of Electrical, and Computer Engineering, Mississippi State University, Starkville, MS, USA; Sichuan Provincial Key Laboratory of Information Coding and Transmission, Southwest Jiaotong University, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 Nov 2021","2021","14","","10748","10760","Synthetic aperture radar (SAR) image change detection is a critical yet challenging task in the field of remote sensing image analysis. The task is nontrivial due to the following challenges: First, intrinsic speckle noise of SAR images inevitably degrades the neural network because of error gradient accumulation. Furthermore, the correlation among various levels or scales of feature maps is difficult to be achieved through summation or concatenation. Toward this end, we proposed a siamese adaptive fusion (AF) network for SAR image change detection. To be more specific, two-branch CNN is utilized to extract high-level semantic features of multitemporal SAR images. Besides, an AF module is designed to adaptively combine multiscale responses in convolutional layers. Therefore, the complementary information is exploited, and feature learning in change detection is further improved. Moreover, a correlation layer is designed to further explore the correlation between multitemporal images. Thereafter, robust feature representation is utilized for classification through a fully connected layer with softmax. Experimental results on four real SAR datasets demonstrate that the proposed method exhibits superior performance against several state-of-the-art methods. Our codes are available at https://github.com/summitgao/SAR_CD_SAFNet.","2151-1535","","10.1109/JSTARS.2021.3120381","National Key Research, and Development Program of China(grant numbers:2018AAA0100602); National Natural Science Foundation of China(grant numbers:U1706218,61871335); Key Technology Research and Development Program of Shandong(grant numbers:2019GHY112048); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9576639","Attention mechanism;change detection;deep learning;siamese adaptive fusion network (SAFNet);synthetic aperture radar (SAR)","Feature extraction;Radar polarimetry;Correlation;Synthetic aperture radar;Convolutional neural networks;Task analysis;Speckle","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;radar imaging;remote sensing;remote sensing by radar;speckle;synthetic aperture radar","synthetic aperture radar image change detection;siamese adaptive fusion network;critical yet challenging task;remote sensing image analysis;intrinsic speckle noise;neural network;error gradient accumulation;feature maps;SAR image change detection;high-level semantic features;multitemporal SAR images;feature learning;correlation layer;multitemporal images;robust feature representation;SAR datasets","","10","","42","CCBY","15 Oct 2021","","","IEEE","IEEE Journals"
"Forward-Looking Geometric Configuration Optimization Design for Spaceborne-Airborne Multistatic Synthetic Aperture Radar","D. Mao; Y. Zhang; J. Pei; W. Huo; Y. Zhang; Y. Huang; J. Yang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Yangtze Delta Region, Quzhou, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Aug 2021","2021","14","","8033","8047","Spaceborne-airborne multistatic synthetic aperture radar (SA-MuSAR) has the ability to provide high-resolution forward-looking imagery for receivers, but it relies on careful design of the geometric configuration (GC). In this article, a forward-looking GC optimization design method is proposed to obtain a high-quality fused image with limited observation time. First, the relationship between the spatial resolution and GC is illustrated by the wavenumber spectrum distribution of SA-MuSAR. Second, GC evaluators depending on the distribution of multiple wavenumber spectrum data are proposed. The GC design problem of coherent SA-MuSAR is transformed into a constrained multiobjective optimization problem. An intelligent evolutionary algorithm is adopted to optimize the wavenumber spectrum distribution. With the proposed method, high-quality forward-looking imagery can be obtained with a short observation time. Numerical simulations are carried out to verify the effectiveness of the proposed method.","2151-1535","","10.1109/JSTARS.2021.3103802","National Natural Science Foundation of China(grant numbers:61901092,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511248","Forward-looking geometric configuration (GC) design;spaceborne-airborne multistatic synthetic aperture radar (SA-MuSAR);wavenumber spectrum distribution","Receivers;Spatial resolution;Transmitters;Synthetic aperture radar;Spaceborne radar;Brain modeling;Optimization methods","coherence;evolutionary computation;image fusion;image resolution;optimisation;radar imaging;spaceborne radar;synthetic aperture radar","forward-looking geometric configuration optimization design;spaceborne-airborne multistatic synthetic aperture radar;SA-MuSAR;GC optimization design method;high-quality fused image;wavenumber spectrum distribution;GC design problem;constrained multiobjective optimization problem","","8","","53","CCBY","10 Aug 2021","","","IEEE","IEEE Journals"
"Selection of Multi-Level Deep Features via Spearman Rank Correlation for Synthetic Aperture Radar Target Recognition Using Decision Fusion","L. Zhu","College of Mechanical and Electrical Engineering, Jiaxing University, Jiaxing, China","IEEE Access","28 Jul 2020","2020","8","","133914","133927","Convolutional neural networks (CNN) now become one of the most popular methods in synthetic aperture radar (SAR) target recognition. To fully exploit the deep features learned by CNN, this paper considers all the feature maps from different convolution layers. At each layer, the Spearman rank correlation is employed to evaluate the similarities between the feature maps and original SAR image. A certain proportion of feature maps with high similarities are selected and jointly represented based on the joint sparse representation (JSR) model. For the reconstruction error vectors from different layers, they are combined based on linear weighting using a random weight matrix. The fused reconstruction errors are analyzed to form a decision value for target recognition. The feature selection chooses the robust features and JSR considers the inner correlations between the feature maps from the same layer. In addition, the linear weighting using the random weight matrix could statistically reveal the correlations between the test sample and a certain training class. Therefore, the overall effectiveness and robustness of the proposed method can be enhanced. By performing experiments on the moving and stationary target acquisition and recognition (MSTAR) dataset, the proposed method could achieve a very high average recognition rate of 99.32% for ten classes of ground targets under the standard operating condition (SOC). Furthermore, under the extended operating conditions (EOCs) like configuration differences, depression angle differences, noise corruption, and partial occlusion, the proposed could also achieve superior robustness over some state-of-the-art SAR target recognition methods.","2169-3536","","10.1109/ACCESS.2020.3010969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145552","Synthetic aperture radar (SAR);target recognition;deep features;Spearman rank correlation;joint sparse representation (JSR);random weight matrix","Synthetic aperture radar;Target recognition;Convolution;Correlation;Robustness;Training;Machine learning","convolutional neural nets;feature extraction;image fusion;image representation;learning (artificial intelligence);matrix algebra;radar computing;radar imaging;radar target recognition;synthetic aperture radar","feature selection;robust features;feature maps;linear weighting;random weight matrix;stationary target acquisition;high average recognition rate;multilevel deep features;Spearman rank correlation;synthetic aperture radar target recognition;convolutional neural networks;CNN;convolution layers;joint sparse representation model;SAR target recognition;decision fusion;extended operating conditions;configuration differences;depression angle differences;noise corruption;partial occlusion","","6","","68","CCBY","21 Jul 2020","","","IEEE","IEEE Journals"
"Comparative Analysis of Pixel Level Fusion Algorithms in High Resolution SAR and Optical Image Fusion","H. Liu; Y. Ye; J. Zhang; C. Yang; Y. Zhao","Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; Faculty of Geosciences and Environmental Engineering, Southwest Jiaotong University, Chengdu, China; The Second Topographic Surveying Brigade of Ministry of Natural Resources, Xian, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2829","2832","Fusion of Synthetic aperture radar (SAR) and optical images is a significant topic in the field of remote sensing. As a typical category of image fusion methods, pixel level image fusion algorithms have been widely used in SAR-optical image fusion to integrate their complementary information and facilitate the subsequent interpretation and application. The effectiveness of these methods has been demonstrated in different literatures based on the experiment carried on specific, individual datasets, which make a comprehensive comparison of these algorithms difficult to achieve. This paper builds a sub-meter SAR and optical image dataset covering different types of scenes, the performance of 11 pixel level image methods is then investigated based on qualitative and quantitative analysis. Result shows the gradient pyramid (GP) achieve a high quality fusion when dealing with Optical-SAR image fusion task of residents, the non subsampled contourlet transform (NSCT) performs best when fusing images containing farmland and mountains.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883331","SAR;optical image;image fusion","Optical distortion;Transforms;Optical imaging;Adaptive optics;Radar polarimetry;Optical sensors;Task analysis","image fusion;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar;transforms","pixel level image fusion algorithms;SAR-optical image fusion;subsequent interpretation;specific datasets;individual datasets;sub-meter SAR;optical image dataset;11 pixel level image methods;qualitative analysis;quantitative analysis;high quality fusion;fusing images;comparative analysis;pixel level fusion algorithms;high resolution SAR;optical images;significant topic;image fusion methods","","","","10","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"The recent advances of data imaging and fusion processing for airborne X-SAR with high resolution","Ting Shen; Jun Li; Zhirui Wang; Lei Huang; Liwei Li; Ping Zhang","Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Beijing Institute of Radio Measurement, Beijing, China; Beijing Institute of Radio Measurement, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","2016 Progress in Electromagnetic Research Symposium (PIERS)","10 Nov 2016","2016","","","2843","2848","X-SAR system is the airborne imaging radar with multi-mode synthetic aperture radar (SAR) at high-resolution, interferometer and full-polarization, which has been developed by the Institute of Remote Sensing and Digital Earth (RADI), Chinese Academy of Sciences (CAS), funded by the CAS Large Research Infrastructures. The first-stage form 2009 to 2015, X-SAR was successfully implemented to an operational SAR in X-band with high resolution (up to 0.5 m).The system performances and data imaging quality have verified by the flight tests. Many valuable results of the visual interpretation in typical images, particularly SAR image fusion processing have emphasized the X-SAR's target recognition capabilities. This paper presents the core characteristics of X-SAR images, having achieved by the spatial resolution optimized by low side-lobe, exact geographical precision and radiometric accuracy. The visual inspection of typical targets in example images is described such as the surface of desert hill, the vehicle discrimination and aircraft recognition. Meanwhile, the image fusion processing for target recognition has been implemented. The recent advances of SAR-optical image fusion used to target classification and SAR-SAR image fusion processing for change detection are also presented.","","978-1-5090-6093-1","10.1109/PIERS.2016.7735138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7735138","","Spatial resolution;Synthetic aperture radar;Radiometry;Imaging;Radar imaging;Calibration","airborne radar;image fusion;radar imaging;radar resolution;synthetic aperture radar","data imaging;airborne X-SAR system;multimode synthetic aperture radar;Institute of Remote Sensing and Digital Earth;Chinese Academy of Sciences;CAS;SAR image fusion processing;target recognition;change detection","","","","5","IEEE","10 Nov 2016","","","IEEE","IEEE Conferences"
"3D ISAR imaging: Multi-view image fusion problem","J. Cai; M. Martorella; J. Guo; Q. Liu; Z. Ding; E. Giusti","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Department of Information Engineering, University of Pisa, Pisa, Italy; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Interuniversity Consortium for Telecommunication Radar and Surveillance System National Laboratory, Pisa, Italy","IET International Radar Conference (IET IRC 2020)","22 Sep 2021","2020","2020","","1200","1204","Inverse synthetic aperture radar (ISAR) is capable of generating the three-dimensional (3D) reconstructions of the non-cooperative target via a dual interferometric system, which is called 3D InISAR imaging. The output of 3D InISAR is a 3D point-like image of the target, namely point cloud, each point contains the 3D coordinates (range, cross-range and height with respect to the image plane). Compared to the traditional two-dimensional ISAR imaging, 3D InISAR can avoid the unknow image projection plane and cross-range scaling issues. However, due to some limitations of 3D InISAR imaging, such as scatterer scintillation, self-occlusion and so on, some information of the target in 3D ISAR reconstruction from a single observation view may be missing. In order to obtain a more complete 3D ISAR reconstruction, an incoherent multi-view image fusion method based on the principal component analysis and iterative closest points algorithm is proposed. Results of the simulated data verify the effectiveness of the proposed method.","","","10.1049/icp.2021.0620","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9545865","","","image fusion;image reconstruction;iterative methods;principal component analysis;radar imaging;radar interferometry;synthetic aperture radar","3D InISAR imaging;incoherent multiview image fusion method;Multiview image fusion problem;inverse synthetic aperture radar;three-dimensional reconstructions;point cloud;image plane;two-dimensional ISAR imaging;unknow image projection plane;cross-range scaling issues","","","","","","22 Sep 2021","","","IET","IET Conferences"
"A high resolution ISAR imaging method base on image fusion with low SNR","M. Lv; F. Xu; J. Ma; L. Chen; H. Chen","Radar NCO School of Air Force Early Warning Academy, Wuhan, People’s Republic of China; Radar NCO School, Air Force Early Warning Academy, Wuhan, China; Radar NCO School, Air Force Early Warning Academy, Wuhan, China; Radar NCO School, Air Force Early Warning Academy, Wuhan, China; Radar NCO School, Air Force Early Warning Academy, Wuhan, China","IET International Radar Conference (IET IRC 2020)","22 Sep 2021","2020","2020","","1308","1312","In order to achieve high resolution ISAR imaging by launching random chirp frequency stepped signal with sparse range and cross range under low SNR, a high resolution ISAR imaging method based on image fusion is proposed in this paper. Firstly, the ISAR image with high range resolution under low SNR, in which the range imaging is realized in view of joint sparse model and the cross range imaging is realized based on traditional compressed sensing model, is obtained. Secondly, the ISAR image with high cross range resolution under low SNR, in which the cross range imaging is realized in view of joint sparse model and the range imaging is realized based on traditional compressed sensing model, is received. Finally, the two ISAR images with higher range resolution and cross range resolution respectively are directly fused, then the final high resolution ISAR imaging results are required. Due to the same data and observation target, the ISAR imaging results via different imaging methods can be directly fused without image rotation or correction and other processing. Therefore, the proposed method not only can achieve high resolution imaging under low SNR, but also has the advantages of simple realization. The effectiveness of this method is verified by the simulation experiments.","","","10.1049/icp.2021.0659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9545675","","","compressed sensing;image fusion;image resolution;radar imaging;radar resolution;synthetic aperture radar","image fusion;cross range imaging;compressed sensing model;high cross range resolution;high resolution ISAR imaging;imaging methods;chirp frequency stepped signal;observation target","","","","","","22 Sep 2021","","","IET","IET Conferences"
"Unambiguous Reconstruction for Multichannel Nonuniform Sampling SAR Signal Based on Image Fusion","L. Zhou; X. Zhang; Y. Wang; L. Li; L. Pu; J. Shi; S. Wei","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Access","27 Apr 2020","2020","8","","71558","71571","Multichannel signal processing in azimuth is a vital technique to enable a wide-swath Synthetic Aperture Radar (SAR) with high azimuth resolution. However, the multichannel high-resolution and wide-swath (HRWS) SAR system always suffers from the problem of the azimuth nonuniform sampling resulting in the image ambiguity, when it does not satisfy the uniform sampling condition. In this paper, to suppress the azimuth image ambiguity, we propose a novel unambiguous reconstruction method based on image fusion. During this reconstruction processing, the Back Projection (BP) algorithm is first utilized for SAR imaging to obtain the designed sub-images. Then, the reconstruction expression is derived as the summation of the sub-images weighted by the interpolation coefficient. This method integrates the reconstruction into the imaging process and the image fusion makes the procedure simple. In addition, the interpolation period, which affects the reconstruction image quality and efficiency, is further analyzed. Moreover, as the curved trajectory platform brings more challenges for the unambiguous reconstruction, the performance of the proposed method applied to the curved trajectory platform is studied. Finally, experimental results clearly verify the effectiveness of the proposed method for ambiguity suppression and demonstrate its applicability to the curved trajectory.","2169-3536","","10.1109/ACCESS.2020.2987196","National Basic Research Program of China (973 Program)(grant numbers:2017YFB0502700); High Resolution Earth Observation Youth Foundation(grant numbers:GFZX04061502); National Natural Science Foundation of China(grant numbers:61671113,61501098,61571099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9064772","High resolution and wide swath;nonuniform sampling;signal reconstruction;synthetic aperture radar (SAR)","Image reconstruction;Azimuth;Reconstruction algorithms;Interpolation;Antennas;Synthetic aperture radar;Nonuniform sampling","image fusion;image reconstruction;image resolution;image sampling;interpolation;radar imaging;synthetic aperture radar","curved trajectory platform;multichannel nonuniform sampling SAR signal;image fusion;multichannel signal processing;wide-swath Synthetic Aperture Radar;high azimuth resolution;multichannel high-resolution;wide-swath SAR system;azimuth nonuniform sampling;uniform sampling condition;azimuth image ambiguity;novel unambiguous reconstruction method;reconstruction processing;SAR imaging;reconstruction expression;imaging process;reconstruction image quality;interpolation coefficient;back projection algorithm","","2","","30","CCBY","13 Apr 2020","","","IEEE","IEEE Journals"
"A Dual-Domain Super-Resolution Image Fusion Method With SIRV and GALCA Model for PolSAR and Panchromatic Images","W. Liu; J. Yang; J. Zhao; F. Guo","School of Geography, Geomatics and Planning, Jiangsu Normal University, Xuzhou, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Jiangsu Key Laboratory of Resources and Environmental Information Engineering, School Environment and Spatial Information and China University of Mining and technology, Xuzhou, China; School of Geography, Geomatics and Planning, Jiangsu Normal University, Xuzhou, China","IEEE Transactions on Geoscience and Remote Sensing","25 Feb 2022","2022","60","","1","14","Hyperspectral/multispectral and panchromatic of optical remote sensing images are commonly used for multisensor image fusion, which has been applied in various applications of Earth observation. However, the utilization of optical remote sensing data suffers from the limitation of bad weather and cloud contamination. To address aforementioned issue and enhance spatial details of polarimetric synthetic aperture radar (PolSAR) image, a novel dual-domain super-resolution image fusion method is proposed by combining improved spherically invariant random vector (ISIRV) model with generalized adaptive linear combination approximation (GALCA) technology in this study. The proposed method decomposes the task of image fusion into polarimetric and texture domain fusion by integrating polarimetric components of PolSAR image and texture detail component of panchromatic image, which can significantly improve spatial resolutions of the PolSAR image while preserving polarimetric information. The data fusion experiment is implemented with three data sets including panchromatic images of Gaofen-1 (GF-1) and Gaofen-2 (GF-2) and the quad-pol SAR data of Gaofen-3 (GF-3) and Radarsat-2. Results show that the proposed dual-domain image fusion method provides a better performance compared with state-of-the-art multisensor fusion methods (BT, PCA, GS, indusion, and PRACS) regarding qualitative and quantitative evaluations. In addition, results of image fusion are applied to image classification over agricultural and urban areas of China, which shows that classification accuracy is significantly improved when compared with the result using only the original image.","1558-0644","","10.1109/TGRS.2021.3134099","National Natural Science Foundation of China(grant numbers:41771377,42071295,41901286,62101219); Natural Science Foundation of Jiangsu(grant numbers:BK20201026,BK20210921); Science Foundation of Jiangsu Normal University(grant numbers:19XSRX006); Science and Technology Innovation Key project of Shenzhen(grant numbers:JCYJ20200109150833977); Open Research Fund of Jiangsu Key Laboratory of Resources and Environmental Information Engineering, China University of Mining and Technology(grant numbers:JS202107,JS202108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643026","Generalized adaptive linear combination approximation (ALCA) method;improved spherically invariant random vector (SIRV) model;polarimetric domain;super-resolution image fusion;texture domain","Image fusion;Optical imaging;Optical sensors;Spatial resolution;Optical scattering;Adaptive optics;Remote sensing","agriculture;geophysical image processing;image classification;image fusion;image resolution;image sensors;radar imaging;radar polarimetry;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","state-of-the-art multisensor fusion methods;image classification;panchromatic image;optical remote sensing images;multisensor image fusion;optical remote sensing data suffers;polarimetric synthetic aperture radar image;novel dual-domain super-resolution image fusion method;spherically invariant random vector model;generalized adaptive linear combination approximation;polarimetric texture domain fusion;PolSAR image;spatial resolutions;data fusion experiment;Gaofen-1;Gaofen-2;Gaofen-3;dual-domain image fusion method","","1","","46","IEEE","8 Dec 2021","","","IEEE","IEEE Journals"
"Research on Image Fusion Methodof SAR and Visible Image Based on CNN","B. Deng; H. Lv","Aviation Industry Corporation of China, Xi'an Aeronautics Computing Technique Research Institute, Xi’an, China; Aviation Industry Corporation of China, Xi'an Aeronautics Computing Technique Research Institute, Xi’an, China","2022 IEEE 4th International Conference on Civil Aviation Safety and Information Technology (ICCASIT)","27 Dec 2022","2022","","","1400","1403","Synthetic aperture radar (SAR) imaging guidance has all-weather detection capability, and has been applied to aerial vehicles in the field of air combat. The research on sar/ visible image fusion technology has important theoretical and practical significance for improving the ability of target detection and precision guidance in complex combat environment. The imaging mechanism of visible light image and SAR image is different, and there are great differences in characteristics between images. Using image fusion technology to organically combine different images, complement their advantages and disadvantages, and can better interpret their scene information. Aiming at the difference feature extraction of SAR optical images, the modal classification task and ground object classification task are proposed, and the fine-tuning VGG convolution neural network model is designed, which uses less training time and achieves better classification results. For SAR optical image mapping, space-frequency consistent generation countermeasure network model framework (SFGAN) is proposed, which makes the texture details of the generated image more realistic and the contour features of roads and rivers more clear. Training from the original data, ablation experiments were designed to verify the effectiveness of the loss function. Experimental results show that the SFGAN method can learn the matching relationship from the small-scale SAR optical image pair data, so as to realize the mapping from SAR image to optical image.","","978-1-6654-6766-7","10.1109/ICCASIT55263.2022.9987074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9987074","SAR;CNN;SFGAN;Image Translation","Integrated optics;Optical fiber networks;Optical imaging;Geometrical optics;Feature extraction;Radar polarimetry;Adaptive optics","feature extraction;image classification;image fusion;image texture;neural nets;object detection;optical images;radar imaging;synthetic aperture radar","all-weather detection capability;complex combat environment;difference feature extraction;fine-tuning VGG convolution neural network model;great differences;ground object classification task;image fusion methodof SAR;imaging mechanism;important theoretical significance;modal classification task;precision guidance;SAR image;SAR optical image mapping;SAR optical images;small-scale SAR optical image pair data;space-frequency consistent generation countermeasure network model framework;synthetic aperture radar imaging guidance;target detection;visible light image","","","","6","IEEE","27 Dec 2022","","","IEEE","IEEE Conferences"
"Change Detection in Multitemporal SAR Images Based on Slow Feature Analysis Combined With Improving Image Fusion Strategy","W. Li; X. Xiao; P. Xiao; H. Wang; F. Xu","Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China; Key Laboratory of Information Science of Electromagnetic Waves, Fudan University, Shanghai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","2 May 2022","2022","15","","3008","3023","Change detection in multitemporal synthetic aperture radar (SAR) images has been an important research content in the field of remote sensing for a long time. In this article, based on the slow feature analysis (SFA) theory and the nonsubsampled contourlet transform (NSCT) algorithm, we propose a novel unsupervised change detection method called NSCT nonlocal means (NSCT-NLM). The powerful extraction to the changed information of SFA and the superior information fusion of NSCT are jointly adopted in this method. The main framework consists of the following parts. First, SFA and the log-ratio operator are used to generate difference images (DIs) independently. Then, the NSCT is used to fuse two DIs into a new higher quality DI. The newly fused DI combines the complementary information of the two kinds of original DI. Therefore, the contrast of the changed regions and unchanged regions is greatly enhanced, as well as the changed details are preserved more completely. Furthermore, an NLM filtering algorithm is employed to suppress the strong speckles in the fused DI. We use the fuzzy C-means algorithm to generate the final binary change map. The experiments are carried out on two public datasets and three real-world SAR datasets from different scenarios. The results demonstrate that the proposed method has higher detection accuracy by comparing with the reference methods.","2151-1535","","10.1109/JSTARS.2022.3166234","Natural Science Foundation of Shanghai(grant numbers:22ZR1406700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755046","Nonsubsampled contourlet transform (NSCT);slow feature analysis (SFA);synthetic aperture radar (SAR) images;unsupervised change detection","Radar polarimetry;Change detection algorithms;Feature extraction;Transforms;Synthetic aperture radar;Image fusion;Data mining","fuzzy set theory;image filtering;radar detection;radar imaging;remote sensing by radar;speckle;synthetic aperture radar;transforms","multitemporal SAR images;slow feature analysis combined;improving image fusion strategy;multitemporal synthetic aperture radar images;remote sensing;slow feature analysis theory;SFA;nonsubsampled contourlet transform algorithm;unsupervised change detection method;NSCT-NLM;superior information fusion;log-ratio operator;difference images;NLM filtering algorithm;fuzzy C-means algorithm;final binary change map;real-world SAR datasets","","","","60","CCBY","11 Apr 2022","","","IEEE","IEEE Journals"
"Infrared and ISAR Image Fusion Based on Quadtree Decomposition and Bézier Interpolation","C. Han; D. Yang; Y. Lu; K. Hou; W. An; H. Wang","Xi'an Research Institute of Navigation Technology, Xi'an, China; Xi'an Research Institute of Navigation Technology, Xi'an, China; Xi'an Research Institute of Navigation Technology, Xi'an, China; School of Astronautics, Northwestern Polytechnical University, Xi'an, China; School of Astronautics, Northwestern Polytechnical University, Xi'an, China; School of Astronautics, Northwestern Polytechnical University, Xi'an, China","2021 IEEE 6th International Conference on Signal and Image Processing (ICSIP)","28 Jan 2022","2021","","","483","487","Inverse Synthetic Aperture Radar (ISAR) image obtained by stationary radar is the result of longitudinal and lateral two-dimensional high-resolution imaging of moving target. Infrared image is a thermal image corresponding to the temperature distribution of the scene, which formed by collecting infrared radiation emitted from the target and the surrounding. In order to make full use of the complementary information of infrared image and ISAR image, an algorithm that combines infrared image with ISAR image is researched. Firstly, this paper applies quadtree decomposition and Bézier interpolation to reconstruct the background of the ISAR image. Secondly, the salient features of the ISAR image are extracted by subtracting the reconstructed background from the original ISAR image. Finally, this algorithm fuses the salient features with the infrared image using the fusion rule proposed in this paper, through which to get the final fused image. Experimental results show that the algorithm can better preserve both the clear target and detailed information of infrared image and ISAR image in subjective evaluation. Compared with other fusion algorithms, experiments also verify the superiority of the algorithm in objective evaluation.","","978-1-6654-3904-6","10.1109/ICSIP52628.2021.9688709","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688709","image fusion;quadtree decomposition;Bézier interpolation;infrared image;ISAR image","Interpolation;Temperature distribution;Fuses;Shape;Surveillance;Radar imaging;Maintenance engineering","image denoising;image fusion;image reconstruction;image resolution;infrared imaging;interpolation;quadtrees;radar imaging;synthetic aperture radar","Inverse Synthetic Aperture Radar image;two-dimensional high-resolution imaging;infrared image;thermal image;original ISAR image;final fused image","","","","21","IEEE","28 Jan 2022","","","IEEE","IEEE Conferences"
"Fusion of multi-frequency SAR data with THAICHOTE optical imagery for maize classification in Thailand","C. Sukawattanavijit; J. Chen","School of Electronics and Information Engineering, Beihang University, Beijing, CHINA; School of Electronics and Information Engineering, Beihang University, Beijing, CHINA","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","617","620","Remote sensing data have been commonly used for agricultural crop monitoring. This paper was assessed the quality of using SAR and optical data fusion for maize classification. Two different SAR data sets from different sensors including dual polarization (HH and VV) X-band COSMO-SkyMed (CSK) and quad polarization (HH, HV, VH and VV) C-band RADARSAT-2 images were fused with THAICHOTE (namely, THEOS, an Earth observation mission of Thailand) optical data. This paper describes a comparative study of multi-sensor image fusion techniques in order to maintain spectral quality of the fused images. Principal Component Analysis (PCA), Intensity-Hue-Saturation (IHS), Brovey Transform (BT) and High-pass filter (HPF) techniques are implemented for image fusion. For the supervised classification, maximum likelihood was applied to the fused images to identify maize crop. Finally, the accuracy assessment was done by comparing maize maps generated from fused images and THAICHOTE classification. The PCA fused RADARSAT-2 with THAICHOTE images consistently provide excellent classification accuracies, well over 85%. The results obtained not only improving of the classification accuracy, but also can be identified the growing cycle of maize crop.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325839","Fusion;THAICHOTE;Cosmo-SkyMed;RADARSAT-2;Maize classification","Principal component analysis;Remote sensing;Accuracy;Image fusion;Optical sensors;Synthetic aperture radar;Optical imaging","crops;geophysical image processing;high-pass filters;image classification;image fusion;maximum likelihood estimation;principal component analysis;radar polarimetry;remote sensing by radar;synthetic aperture radar;vegetation mapping","remote sensing data;agricultural crop monitoring;SAR data fusion;optical data fusion;maize classification;dual polarization X-band COSMO-SkyMed images;quadpolarization C-band RADARSAT-2 images;THEOS;Earth observation mission;Thailand;THAICHOTE optical data classification accuracy;multisensor image fusion techniques;spectral quality;principal component analysis;intensity-hue-saturation;Brovey Transform;high-pass filter techniques;supervised classification;maximum likelihood;maize crop;accuracy assessment;THAICHOTE optical imagery;maize crop growing cycle","","3","","16","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A method of target fusion detection based on the multi-fractal analysis","Weigang Zhu; Weiqing Tian; Chuangzhan Zeng","Academy of Equipment, Beijing, China; Academy of Equipment, Beijing, China; Academy of Equipment, Beijing, China","2016 15th International Conference on Optical Communications and Networks (ICOCN)","13 Mar 2017","2016","","","1","3","A target detection method is proposed which is based on the multi-fractal spectrum characteristics of the remote sensing image. The method uses multi-fractal spectrum to reflect the image global features and effectively suppress noise characteristics at the same time. The membership function is constructed by multi-fractal spectrum of visible and SAR images to indicate the uncertainty of their sensors information. The trade-off between the intersection operation and union operation of the feature level fusion detection is achieved by the membership function, in order to reduce the false alarm rate and the missed alarm rate. The experimental results show that the method can combine the respective advantages of SAR and visible images, and effectively improve the accuracy of detection.","","978-1-5090-3491-8","10.1109/ICOCN.2016.7875601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7875601","Multi-fractal spectrum;Remote sensing image;Fusion;Detection","Fractals;Remote sensing;Synthetic aperture radar;Image edge detection;Noise measurement;Sensors;Image fusion","feature extraction;fractals;image fusion;image sensors;object detection;radar imaging;synthetic aperture radar","target fusion detection method;multifractal spectrum characteristics;remote sensing imaging;image global feature;noise characteristics;SAR imaging;visible imaging;level fusion detection;false alarm rate reduction","","","","5","IEEE","13 Mar 2017","","","IEEE","IEEE Conferences"
"Information Content of Very-High-Resolution SAR Images: Semantics, Geospatial Context, and Ontologies","C. O. Dumitru; S. Cui; G. Schwarz; M. Datcu","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR),, Weßling, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR),, Weßling, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR),, Weßling, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR),, Weßling, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 May 2015","2015","8","4","1635","1650","Currently, the amount of collected Earth Observation (EO) data is increasing considerably with a rate of several Terabytes of data per day. As a consequence of this increasing data volume, new concepts for exploration and information retrieval are urgently needed. To this end, we propose to explore satellite image data via an image information mining (IIM) approach in which the main steps are feature extraction, classification, semantic annotation, and interactive query processing. This leads to a new process chain and a robust taxonomy for the retrieved categories capitalizing on human interaction and judgment. We concentrated on land cover categories that can be retrieved from high-resolution synthetic aperture radar (SAR) images of the spaceborne TerraSAR-X instrument, where we annotated different urban areas all over the world and defined a taxonomy element for each prevailing surface cover category. The annotation resulted from a test dataset comprising more than 100 scenes covering diverse areas of Africa, Asia, Europe, the Middle East, and North and South America. The scenes were grouped into several collections with similar source areas and each collection was processed separately in order to discern regional characteristics. In the first processing step, each scene was tiled into patches. Then the features were extracted from each patch by a Gabor filter bank and a support vector machine with relevance feedback classifying the feature sets into user-oriented land cover categories. Finally, the categories were semantically annotated using Google Earth for ground truthing. The annotation followed a multilevel approach that allowed the fusion of information being visible on different resolution levels. The novelty of this paper lies in the fact that a semantic annotation was performed with a large number of high-resolution radar images that allowed the definition of more than 850 surface cover categories. This opens the way toward an automated identification and classification of urban areas, infrastructure (e.g., airports), geographic objects (e.g., mountains), industrial installations, military compounds, vegetation, and agriculture. Applications that may result from this work can be a semantic catalog of urban images to be used in crisis situations or after a disaster. In addition, the proposed taxonomies can become a basis for building a semantic catalog of satellite images. Finally, we defined four powerful types of high-level queries. Querying on such high levels provides new opportunities for users to search an image database for specific parameters or semantic relationships.","2151-1535","","10.1109/JSTARS.2014.2363595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960829","Annotation;classification;feature extraction;high-resolution images;indexing;ontologies;querying;semantic catalogs;synthetic aperture radar (SAR);taxonomies;TerraSAR-X;Annotation;classification;feature extraction;high-resolution images;indexing;ontologies;querying;semantic catalogs;synthetic aperture radar (SAR);taxonomies;TerraSAR-X","Synthetic aperture radar;Remote sensing;Feature extraction;Semantics;Satellites;Taxonomy;Earth","data mining;feature extraction;Gabor filters;geophysical image processing;image classification;image fusion;image resolution;image retrieval;land cover;ontologies (artificial intelligence);radar imaging;remote sensing by radar;support vector machines;synthetic aperture radar;terrain mapping;visual databases","Gabor filter bank;support vector machine;user-oriented land cover categories;Google Earth;ground truthing;high-resolution radar image fusion;surface cover categories;urban area automated identification;urban area automated classification;infrastructure;geographic objects;industrial installations;military compounds;vegetation;agriculture;semantic catalog;urban images;crisis situations;disaster;high-level queries;image database;semantic relationships;information content;very-high-resolution SAR images;geospatial context;ontologies;regional characteristics;source areas;South America;North America;Middle East;Europe;Asia;Africa;surface cover category;taxonomy element;spaceborne TerraSAR-X instrument;high-resolution synthetic aperture radar images;land cover categories;human judgment;human interaction;interactive query processing;semantic annotation;feature classification;feature extraction;image information mining approach;satellite image data;information retrieval;data volume;collected Earth Observation data","","44","","37","IEEE","20 Nov 2014","","","IEEE","IEEE Journals"
"Quantum Processing in Fusion of SAR and Optical Images for Deep Learning: A Data-Centric Approach","S. R. Majji; A. Chalumuri; R. Kune; B. S. Manoj","Indian Institute of Space Science and Technology, Thiruvananthapuram, India; Indian Institute of Space Science and Technology, Thiruvananthapuram, India; Advanced Data Processing Research Institute, Hyderabad, India; Indian Institute of Space Science and Technology, Thiruvananthapuram, India","IEEE Access","19 Jul 2022","2022","10","","73743","73757","Deep learning techniques are very prominent in processing remotely sensed synthetic aperture radar (SAR) images for real-time, high-impact applications, such as image classification, object detection, and semantic segmentation. The accuracy of deep learning models, such as convolutional neural networks (CNNs), depends on the quality of the input data. Compared to the model-centric approach, where the model parameters are optimized during training, the data-centric approach can enhance the performance accuracy as data quality is improved before training the models. Improving the data quality of SAR images is challenging as SAR image properties are different from optical images. Image fusion techniques proved to enhance the quality of SAR images when combined with optical images. Many fusion techniques exist for combining SAR and optical images in the classical domain. This paper proposes a novel approach to using quantum computing for the image fusion of SAR and optical images. Eight different quantum techniques are used to process and fuse the images. We designed and created a dataset for land-use classification by collecting data using the Google Earth Engine. The quality metric measurements show that the quality of SAR images has improved by using the proposed quantum processing techniques. In addition, performance evaluation of the deep learning CNNs on the dataset was carried out for all quantum processing techniques. Our approach improved the classification accuracy from 82.64%, with only SAR images for training, to 95.36% using the proposed image fusion techniques.","2169-3536","","10.1109/ACCESS.2022.3189474","IIT Palakkad Technology IHub Foundation Doctoral Fellowship(grant numbers:IPTIF/HRD/DF/032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9819905","Quantum processing;image fusion;deep learning;image classification;synthetic aperture radar (SAR) imagery;optical imagery","Radar polarimetry;Synthetic aperture radar;Optical sensors;Optical imaging;Adaptive optics;Data models;Deep learning","image classification;image fusion;image representation;image segmentation;learning (artificial intelligence);neural nets;object detection;radar imaging;remote sensing;synthetic aperture radar","optical images;data-centric approach;deep learning techniques;synthetic aperture radar images;image classification;deep learning models;model-centric approach;data quality;SAR images;SAR image properties;image fusion techniques;different quantum techniques;quantum processing techniques;deep learning CNNs","","1","","55","CCBY","7 Jul 2022","","","IEEE","IEEE Journals"
"Multisensor Earth Observation Image Classification Based on a Multimodal Latent Dirichlet Allocation Model","R. Bahmanyar; D. Espinoza-Molina; M. Datcu","German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany; German Aerospace Center, Remote Sensing Technology Institute, Wessling, Germany","IEEE Geoscience and Remote Sensing Letters","23 Feb 2018","2018","15","3","459","463","Many previous researches have already shown the advantages of multisensor land-cover classification. Here, we propose an innovative land-cover classification approach based on learning a joint latent model of synthetic aperture radar (SAR) and multispectral satellite images using multimodal latent Dirichlet allocation (mmLDA), a probabilistic generative model. It has already been successfully applied to various other problems dealing with multimodal data. For our experiments, we chose overlapping SAR and multispectral images of two regions of interest. The images were tiled into patches and their local primitive features were extracted. Then each image patch is represented by SAR and multispectral bag-of-words (BoW) models. The BoW values are both fed to the mmLDA, resulting in a joint latent data model. A qualitative and quantitative validation of the topics based on ground-truth data demonstrate that the land-cover categories of the regions are correctly classified, outperforming the topics obtained using individual single modality data.","1558-0571","","10.1109/LGRS.2018.2794511","Deutscher Akademischer Austauschdienst; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8278834","Image fusion;land-cover classification;multimodal latent Dirichlet allocation (mmLDA);multispectral images;synthetic aperture radar (SAR) images","Synthetic aperture radar;Visualization;Feature extraction;Data models;Dictionaries;Optical sensors","feature extraction;geophysical image processing;image classification;learning (artificial intelligence);sensor fusion;synthetic aperture radar;terrain mapping","innovative land-cover classification approach;joint latent model;SAR;multispectral satellite images;mmLDA;probabilistic generative model;multimodal data;multispectral images;local primitive features;image patch;bag-of-words models;joint latent data model;ground-truth data;land-cover categories;multisensor earth observation image classification;multimodal latent dirichlet allocation model;multisensor land-cover classification","","20","","16","IEEE","1 Feb 2018","","","IEEE","IEEE Journals"
"Different Levels Multi-source Remote Sensing Image Fusion","K. Lin; W. Li; H. Liu; J. Wu","School of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Telecommunications & Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Geographic and Biologic Information, Nanjing University of Posts and Telecommunications, Nanjing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","In order to study the fusion methods of multi-source remote sensing image, this paper plans to fuse the optical images and synthetic aperture radar (SAR) images in pixel level, feature level and decision level, respectively. For the purpose of combining the merits of fusion at aforementioned levels, the fusion results are assigned different weights for multi-level fusion. Several qualitative and quantitative evaluations are also presented, and they show that the multi-level fusion does pretty-well performance in brightness, sharpness, contrast, amount of information, classification effect, etc. It concludes that the multi-level fusion can significantly extract much information while retaining more details of the original image.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173281","Multi-source remote sensing image;SAR;fusion;qualitative and quantitative evaluation","","geophysical image processing;image fusion;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","multilevel fusion;different levels multisource remote sensing image;fusion methods;optical images;synthetic aperture radar images;pixel level;feature level;decision level;aforementioned levels;fusion results","","","","14","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"A Fusion Method of SAR Image and Optical Image Based on NSCT and Gram-Schmidt Transform","B. Yan; Y. Kong","College of Electrical and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Electrical and Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2332","2335","The purpose of remote sensing image fusion is to synthesize the characteristics of multi-source remote sensing images and generate a composite image with new spatial, spectral and temporal features. The Gram-Schmidt (GS) transform can better improve the spatial features and maintain the spectral characteristics of the original images to a large extent. However, for the fusion of Synthetic aperture radar (SAR) image and optical image, there are still obvious spectral distortion and detail blurring. In this paper, the GS transform method is improved with non-subsampled contourlet transform (NSCT) which is used to obtain a high-resolution image that contains both spectral information and SAR image detail information. The results show that the spectral information of the fusion image is well preserved and the indicators are better than other methods.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323158","Aeronautical Science Foundation of China(grant numbers:20182052012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323158","Gram-Schmidt transform;NSCT;SAR;image fusion;optical image","Transforms;Radar polarimetry;Optical imaging;Optical distortion;Optical reflection;Optical sensors;Optical filters","geophysical image processing;geophysical techniques;image fusion;image resolution;radar imaging;remote sensing;synthetic aperture radar","nonsubsampled contourlet transform;fusion image;spectral information;high-resolution image;GS transform method;detail blurring;spectral distortion;synthetic aperture radar image;spectral characteristics;composite image;multisource remote sensing images;remote sensing image fusion;Gram-Schmidt transform;NSCT;optical image;SAR image;fusion method","","2","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Fast Cartesian Back-Projection Algorithm Based on Ground Surface Grid for GEO SAR Focusing","Q. Chen; W. Liu; G. -C. Sun; X. Chen; L. Han; M. Xing","National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; Academy of Advanced Interdisciplinary Research, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; Physics and Optoelectronic Engineering, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","15 Feb 2022","2022","60","","1","14","Geosynchronous-Earth-orbit (GEO) synthetic aperture radar (SAR) provides excellent continuous observing capability and large swath. However, the extremely long synthetic aperture time, the curved orbit, and the nonplanar ground surface cause serious spatial variance in the GEO SAR signal. In this article, a novel fast Cartesian back-projection (BP) algorithm based on subaperture imaging on ground and multistage fusion is proposed for accurately and efficiently imaging of GEO SAR. The imaging grids are arranged on the ground surface to avoid the azimuth defocusing caused by the flat ground approximation. Then, a new two-step spectrum compression method is derived to solve the spectrum aliasing of subaperture images. Also, a multistage image fusion method is adopted to combine all the subaperture images with high efficiency. The computational complexity and the approximation of the proposed algorithm are also discussed. Simulation results verify the effectiveness of the proposed algorithm.","1558-0644","","10.1109/TGRS.2021.3125797","National Natural Science Foundation of China(grant numbers:62101404,61825105,61931025,61404130124); Shanghai Aerospace Science and Technology Innovation Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610055","Back-projection algorithm;geosynchronous-Earth-orbit (GEO) synthetic aperture radar (SAR);ground surface curvature;spectrum compression","Imaging;Synthetic aperture radar;Signal processing algorithms;Surface topography;Orbits;Radar polarimetry;Trajectory","approximation theory;computational complexity;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","fast Cartesian back-projection algorithm;ground surface grid;Geosynchronous-Earth-orbit synthetic aperture radar;extremely long synthetic aperture time;curved orbit;nonplanar ground surface cause serious spatial variance;GEO SAR signal;subaperture imaging;multistage fusion;imaging grids;flat ground approximation;two-step spectrum compression method;subaperture images;multistage image fusion method;GEO SAR focusing","","1","","30","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Fusion of SAR and optical remote sensing data — Challenges and recent trends","M. Schmitt; F. Tupin; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; LTCI, Université Paris Saclay, Paris, France; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5458","5461","In this paper, we summarize challenges, proposed solutions and recent trends in the field of SAR-optical remote sensing data fusion. Although being a pre-processing step before the actual fusion-by-estimation, it is shown that matching and coregistration is one of the core challenges in that regard, which is mainly due to the strongly different geometric and radiometric properties of the two observation types. We then review some of the published fusion methods and discuss the future trends of this topic.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128239","synthetic aperture radar (SAR);optical imagery;remote sensing;data fusion","Optical sensors;Remote sensing;Synthetic aperture radar;Optical imaging;Data integration;Estimation","image fusion;image matching;image registration;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","geometric properties;published fusion methods;radiometric properties;fusion-by-estimation;SAR-optical remote sensing data fusion","","21","","31","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Efficient object classification and recognition in SAR imagery","I. M. Gorovyi; D. S. Sharapov","Department of Microwave Electronics, Institute of Radio Astronomy of NAS of Ukraine, Kharkov, Ukraine; Department of Microwave Electronics, Institute of Radio Astronomy of NAS of Ukraine, Kharkov, Ukraine","2017 18th International Radar Symposium (IRS)","14 Aug 2017","2017","","","1","7","SAR is a very popular instrument for imaging of the ground surface. Possibility of high-resolution image formation makes it superior tool for various information extraction tasks. In the paper, a problem of automatic target recognition is comprehensively analyzed. An idea of azimuth and range target profiles fusion is proposed. It is demonstrated, that usage of a proper image preprocessing with appropriate feature extraction steps allow to achieve a competitive recognition accuracy while keeping a low-dimensionality of feature vectors. Experimental results are discussed for a publicly available MSTAR dataset.","2155-5753","978-3-7369-9343-3","10.23919/IRS.2017.8008228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008228","synthetic aperture radar;automatic target recognition;SAR image;feature extraction;support vector machines;object classification","Synthetic aperture radar;Feature extraction;Target recognition;Support vector machines;Databases;Azimuth;Optical imaging","feature extraction;image classification;image fusion;radar imaging;radar resolution;radar target recognition;synthetic aperture radar","SAR imagery;high-resolution image formation;information extraction tasks;automatic target recognition;azimuth profiles fusion;range target profiles fusion;image preprocessing;feature extraction steps;competitive recognition accuracy;feature vectors;object classification;object recognition","","9","","16","","14 Aug 2017","","","IEEE","IEEE Conferences"
"A novel fusion method of SAR and optical sensors to reconstruct 3-D buildings","H. Zhang; H. -p. Xu; Z. -f. Wu","School of Electronics & Information Engineering, Beihang University, Beijing, China; School of Electronics & Information Engineering, Beihang University, Beijing, China; School of Electronics & Information Engineering, Beihang University, Beijing, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","609","612","This paper investigates the method for 3-D buildings reconstruction using the fusion of SAR and optical sensors. First, the joint positioning equation is described as the foundation of this method. Then, the principal steps of this method are recommended. Image registration is employed to obtain a mapping function between the two images as the first step. And the 3-D coordinates are calculated with the joint positioning equation. Afterwards, the 3-D buildings are reconstructed using the 3-D coordinates obtained above. Finally, an experiment is conducted to show the reconstruction result of this method.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325837","3-D building;reconstruction;fusion;synthetic aperture radar;optical image","Synthetic aperture radar;Optical sensors;Optical imaging;Buildings;Image reconstruction;Adaptive optics;Mathematical model","buildings (structures);geophysical image processing;image fusion;image reconstruction;image registration;optical sensors;remote sensing by radar;synthetic aperture radar","fusion method;SAR sensor;optical sensor;3D building reconstruction;joint positioning equation;image registration;3D coordinate","","2","","8","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A New Method for Land Cover Characterization and Classification of Polarimetric SAR Data Using Polarimetric Signatures","M. Jafari; Y. Maghsoudi; M. J. Valadan Zoej","Department of Geomatics Engineering, K. N. Toosi University of Technology, Tehran, Iran; Department of Geomatics Engineering, K. N. Toosi University of Technology, Tehran, Iran; Department of Geomatics Engineering, K. N. Toosi University of Technology, Tehran, Iran","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Aug 2015","2015","8","7","3595","3607","Conventional methods for analyzing polarimetric synthetic aperture RADAR (PolSAR) data such as scattering matrix show polarimetric information just in a restricted number of polarization bases, whereas backscattering of the targets has information on wide range of polarizations. In order to solve this problem, polarimetric signatures have been investigated to have a better illustration of the target responses. Polarimetric signatures depict more details of physical information from target backscattering in various polarization bases. This paper presents a new method for generating polarimetric signatures for different features in PolSAR data by changing the polarization basis in the covariance matrix. Furthermore, various land cover classes were evaluated using their polarimetric signatures and the pattern recognition matching methods. On the basis of this background, an object-oriented and knowledge-based classification algorithm is proposed. The main idea of this method is to apply polarimetric signatures of various PolSAR features in the land cover classification. A Radarsat-2 image, acquired in leaf-off season of the forest areas, was chosen for this study. The backscattering from different classes, including six land cover classes: 1) red oak (Or); 2) white pine (Pw); 3) black spruce (Sb); 4) urban (Ur); 5) water (Wa); and 6) ground vegetation (GV) was analyzed by the proposed method. The results reported that the polarimetric signatures of PolSAR features introduce new concepts for the various targets which are different from the polarimetric power signatures. Also, the proposed classification was compared with the object-based form of the supervised Wishart classification as the baseline method. The mean accuracy of the proposed method is 6% better than the supervised Wishart classification.","2151-1535","","10.1109/JSTARS.2014.2387374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018944","Knowledge based;land cover classification;object oriented;polarimetric signatures;polarimetric synthetic aperture radar (PolSAR);scattering contributions;Knowledge based;land cover classification;object oriented;polarimetric signatures;polarimetric synthetic aperture radar (PolSAR);scattering contributions","Feature extraction;Covariance matrices;Scattering;Synthetic aperture radar;Pattern recognition;Remote sensing;Backscatter","geophysical image processing;image classification;image fusion;image segmentation;remote sensing by radar;synthetic aperture radar","land cover characterization;polarimetric SAR data classification;polarimetric signatures;polarimetric synthetic aperture radar data;PolSAR data;scattering matrix;polarimetric information;physical information;covariance matrix;pattern recognition matching methods;object-oriented classification algorithm;knowledge-based classification algorithm;Radarsat-2 image;polarimetric power signatures;supervised Wishart classification","","52","","40","IEEE","23 Jan 2015","","","IEEE","IEEE Journals"
"Automatic Feature-Based Geometric Fusion of Multiview TomoSAR Point Clouds in Urban Area","Y. Wang; X. X. Zhu","Helmholtz Young Investigators Group “SiPEO”, Technische Universität München, Munich, Germany; Helmholtz Young Investigators Group “SiPEO”, Technische Universität München, Munich, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","3","953","965","Interferometric synthetic aperture radar (InSAR) techniques, such as persistent scatterer interferometry (PSI) or SAR tomography (TomoSAR), deliver three-dimensional (3-D) point clouds of the scatterers' positions together with their motion information relative to a reference point. Due to the SAR side-looking geometry, minimum of two point clouds from cross-heading orbits, i.e., ascending and descending, are required to achieve a complete monitoring over an urban area. However, these two point clouds are usually not coregistered due to their different reference points with unknown 3-D positions. In general, no exact identical points from the same physical object can be found in such two point clouds. This article describes a robust algorithm for fusing such two point clouds of urban areas. The contribution of this paper is finding the theoretically exact point correspondence, which is the end positions of façades, where the two point clouds close. We explicitly define this algorithm as “L-shape detection and matching,” in this paper, because the façades commonly appear as L-shapes in InSAR point cloud. This algorithm introduces a few important features for a reliable result, including point density estimation using adaptive directional window for better façade points detection and L-shape extraction using weighed Hough transform. The algorithm is fully automatic. Its accuracy is evaluated using simulated data. Furthermore, the proposed method is applied on two TomoSAR point clouds over Berlin with ascending and descending geometry. The result is compared with the first PSI point cloud fusion method (S. Gernhardt and R. Bamler, “Deformation monitoring of single buildings using meter-resolution SAR data in PSI,” ISPRS J. Photogramm. Remote Sens., vol. 73, pp. 68-79, 2012.) for urban area. Submeter consistency is achieved.","2151-1535","","10.1109/JSTARS.2014.2361430","International Graduate School of Science and Engineering and Technische Universit¨t München; Helmholtz Association under the framework of the Young Investigators Group; German Research Foundation; Gauss Centre for Supercomputing e V; GCS Supercomputer SuperMUC at Leibniz Supercomputing Centre; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6942160","Point cloud fusion;SAR tomography (TomoSAR);synthetic aperture radar (SAR);TerraSAR-X;Point cloud fusion;SAR tomography (TomoSAR);synthetic aperture radar (SAR);TerraSAR-X","Three-dimensional displays;Synthetic aperture radar;Orbits;Estimation;Urban areas;Image color analysis;Robustness","feature extraction;geophysical image processing;image fusion;radar interferometry;remote sensing by radar;synthetic aperture radar","automatic feature-based geometric fusion;multiview TomoSAR point clouds;urban area;interferometric synthetic aperture radar;InSAR techniques;persistent scatterer interferometry;SAR tomography;scatterer position 3-D point clouds;SAR side-looking geometry;reference points;L-shape matching;L-shape detection;L-shape extraction;PSI point cloud fusion method","","21","","55","OAPA","31 Oct 2014","","","IEEE","IEEE Journals"
"CRIM-FCHO: SAR Image Two-Stage Segmentation With Multifeature Ensemble","H. Yu; L. Jiao; F. Liu","School of Aerospace Science and Technology, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, Xidian University, Xi'an, China","IEEE Transactions on Geoscience and Remote Sensing","10 Mar 2016","2016","54","4","2400","2423","This paper investigates the synthetic aperture radar (SAR) image segmentation in terms of feature analysis and fusion and develops a new algorithm based on multifeature ensemble accordingly. This paper is characterized by two aspects. First, multiple heterogeneous features are extracted to accurately describe the objects in SAR images. These features are then integrated in the feature level and the similarity level, respectively, to avoid the mutual influences between different kinds of features and maximize the discriminability of the similarity measure between objects. Second, a two-stage algorithm consisting of a coarse merging stage and a fine classification stage is proposed. In the coarse merging stage, a context-based region iterative merging algorithm is designed to merge most of the unambiguous superpixels in image domain at a high speed. In the fine classification stage, a fuzzy clustering algorithm incorporating hybrid optimization is developed to balance the efficiency and the robustness of the algorithm by simultaneously searching heuristically in the complete high-dimension feature space and searching along the direction of the gradient steepest descent in each feature subspace. The effectiveness of the proposed method has been successfully validated on synthetic and real SAR images.","1558-0644","","10.1109/TGRS.2015.2501162","National Natural Science Foundation of China(grant numbers:61203202,61201298,61473228,61501352); China Postdoctoral Science Foundation(grant numbers:2014M562376); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:S2015YFJQ0573); Fundamental Research Funds for the Central Universities(grant numbers:JB141304,JB151308); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368911","Fuzzy clustering;image segmentation;multifeature ensemble;region merging;similarity measures;synthetic aperture radar (SAR) image;Fuzzy clustering;image segmentation;multifeature ensemble;region merging;similarity measures;synthetic aperture radar (SAR) image","Synthetic aperture radar;Image segmentation;Feature extraction;Brightness;Merging;Algorithm design and analysis;Clustering algorithms","feature extraction;gradient methods;image fusion;image segmentation;optimisation;radar imaging;synthetic aperture radar","gradient steepest descent;high-dimension feature space;hybrid optimization;fuzzy clustering algorithm;image domain;context-based region iterative merging algorithm;fine classification stage;coarse merging stage;similarity measure discriminability;heterogeneous feature extraction;feature fusion;synthetic aperture radar image segmentation;multifeature ensemble;SAR image two-stage segmentation;CRIM-FCHO","","17","","90","IEEE","30 Dec 2015","","","IEEE","IEEE Journals"
"An Improved Image Fusion Method of Infrared Image and SAR Image via Shearlet and Sparse Representation","X. Ji","Nanjing University of Aeronautics and Astronautics Nanhang Jincheng College, Nanjing, Jiangsu, CN","2016 8th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)","15 Dec 2016","2016","01","","74","77","In this paper, a novel image fusion method of infrared image and SAR image is proposed combining Shearlet transform and sparse representation to avoid the disadvantages of Wavelet transform. The registered images are decomposed by the shearlet transform to obtain the low frequency subband and a series of high frequency subbands. The low frequency subband with lower sparseness is disposed with sparse representation, construct over complete dictionary, solve sparse coefficient over the trained dictionary, and choose the low frequency coefficients with the larger energy fusion rule. And the rule of gradient absolute value maximization is applied to the high frequency subbands. Then the fusion image is obtained by the inverse Shearlet transform. Experimental results show that the proposed method can retain good visual quality and objective evaluation index, and performs some related fusion approaches.","","978-1-5090-0768-4","10.1109/IHMSC.2016.141","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783563","image fusion;Shearlet transform;sparse representation;gradient","Man-machine systems;Cybernetics","gradient methods;image fusion;image registration;image representation;infrared imaging;optimisation;radar imaging;sparse matrices;synthetic aperture radar;transforms","improved image fusion method;infrared image;SAR image;sparse representation;Shearlet transform;registered image decomposition;low-frequency subband;high-frequency subbands;complete dictionary;sparse coefficient;trained dictionary;low-frequency coefficients;energy fusion rule;gradient absolute value maximization rule;visual quality;objective evaluation index","","2","","","IEEE","15 Dec 2016","","","IEEE","IEEE Conferences"
"An Algorithm Based on PCGP Image Fusion for Multi-Source Remote Sensing Images","Z. Ji; L. Xu; H. Wang; Y. Zhang","Key Laboratory of Marine Environmental Monitoring and Information Processing, Ministry of Industry; School of electronic and information engineering, Harbin Institute of Technology, Harbin, China; School of electronic and information engineering, Harbin Institute of Technology, Harbin, China; Key Laboratory of Marine Environmental Monitoring and Information Processing, Ministry of Industry","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2860","2863","Heterogeneous images imaged by different types of sensors have different imaging mechanisms, reflecting the characteristics of different sides of the target scene; while multi-source images formed by different working platforms or at different times have different imaging perspectives, and provide different target scene information. The use of multi-source heterogeneous images for fusion to obtain target and scene information more accurately and comprehensively has potential important applications in many fields such as military, medicine, and meteorology, and has become an important branch of image processing research. To this end, a PCGP algorithm is proposed in this paper to realize the fusion of optical images from different sources and SAR images. It first applies PCA transformation to the multi-source data images to obtain the principal component variables, then performs histogram matching on the first principal components of the transformed data sources, and finally uses the gradient pyramid decomposition algorithm to fuse the matched images to obtain a fused image. Then, the proposed fusion algorithm is tested in the fusion task of remote sensing images from different sources of GF2, GF6 and GF3. The experimental results show that the proposed fusion algorithm has better results.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884334","National Natural Science Foundation of China(grant numbers:61201304,61201308); Ministry of Industry and Information Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884334","Multi-sensor heterogeneous remote sensing image;image fusion;PCGP algorithm","Biomedical optical imaging;Transforms;Optical imaging;Optical sensors;Optical reflection;Task analysis;Remote sensing","feature extraction;geophysical image processing;gradient methods;image fusion;image matching;image processing;optical images;principal component analysis;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","PCGP image fusion;multisource remote sensing;different imaging mechanisms;multisource images;different working platforms;different imaging perspectives;different target scene information;multisource heterogeneous images;potential important applications;image processing research;PCGP algorithm;optical images;SAR images;multisource data images;transformed data sources;gradient pyramid decomposition algorithm;matched images;fused image;fusion algorithm;fusion task;remote sensing images","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A Conditional Generative Adversarial Network to Fuse Sar And Multispectral Optical Data For Cloud Removal From Sentinel-2 Images","C. Grohnfeldt; M. Schmitt; X. Zhu","Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1726","1729","In this paper, we present the first conditional generative adversarial network (cGAN) architecture that is specifically designed to fuse synthetic aperture radar (SAR) and optical multi-spectral (MS) image data to generate cloud- and haze-free MS optical data from a cloud-corrupted MS input and an auxiliary SAR image. Experiments on Sentinel-2 MS and Sentinel-l SAR data confirm that our extended SAR-Opt-cGAN model utilizes the auxiliary SAR information to better reconstruct MS images than an equivalent model which uses the same architecture but only single-sensor MS data as input.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8519215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519215","SAR;optical remote sensing;data fusion;deep learning;generative adversarial network (GAN);cloud-removal","Synthetic aperture radar;Optical sensors;Optical imaging;Clouds;Remote sensing;Adaptive optics;Generative adversarial networks","geophysical image processing;image fusion;radar imaging;synthetic aperture radar","cloud-free MS optical data;Sentinel-l SAR data;single-sensor MS data;reconstruct MS images;auxiliary SAR information;extended SAR-Opt-cGAN model;auxiliary SAR image;cloud-corrupted MS input;haze-free MS optical data;multispectral image data;synthetic aperture radar;conditional generative adversarial network architecture;Sentinel-2;cloud removal;multispectral optical data","","51","3","12","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Radar processing architecture for simultaneous SAR, GMTI, ATR, and tracking","R. K. Hersey; E. Culpepper","Sensors and Electromagnetic Applications Laboratory, Georgia Tech Research Institute, Atlanta, GA; Sensors Directorate, US Air Force Research Laboratory, OH","2016 IEEE Radar Conference (RadarConf)","9 Jun 2016","2016","","","1","5","Combined synthetic aperture radar (SAR) and ground moving target indication (GMTI) radar modes simultaneously generate SAR and GMTI products from the same radar data. Furthermore, the SAR and GMTI data products can be further exploited for target signature extractions, automatic target recognition (ATR), and feature-aided tracking. This hybrid mode provides the benefit of fused imaging and moving target displays along with enhanced target recognition and ground target tracking. The Air Force Research Laboratory (AFRL) Gotcha radar has collected wide-bandwidth, multi-channel data that can be utilized for these hybrid mode applications. This paper presents a processing architecture for simultaneous SAR, GMTI, ATR, and tracking, and includes the results of applying this processing to the AFRL Gotcha data.","2375-5318","978-1-5090-0863-6","10.1109/RADAR.2016.7485076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7485076","radar;signal processing;SAR;GMTI;ATR;tracking","Synthetic aperture radar;Radar tracking;Target tracking;Radar imaging;Clutter;Feature extraction","image fusion;radar imaging;radar target recognition;radar tracking;synthetic aperture radar;target tracking","radar processing architecture;SAR;GMTI radar;ATR;synthetic aperture radar;ground moving target indication;automatic target recognition;feature-aided tracking;fused imaging;moving target display;ground target tracking;air force research laboratory;AFRL Gotcha radar","","8","","10","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"SAR and AIS Fusion for Maritime Surveillance","L. Achiri; R. Guida; P. Iervolino","Surrey Space Centre, University of Surrey, Guildford, UK; Surrey Space Centre, University of Surrey, Guildford, UK; Surrey Space Centre, University of Surrey, Guildford, UK","2018 IEEE 4th International Forum on Research and Technology for Society and Industry (RTSI)","29 Nov 2018","2018","","","1","4","This paper presents a novel approach to fuse Synthetic Aperture Radar (SAR) images and Automatic Identification System (AIS) data for maritime surveillance. The procedure consists of four steps. First, ship detection is performed in the SAR image using a Constant False Alarm Rate (CFAR) algorithm; then feature extraction (ship position, heading and size) is performed on ships detected in the SAR image, the third step consists in identifying the detected ships and extracting the same features from the AIS data. The final step is to feed the fusion block with both features vectors extracted separately from the SAR and AIS. Here the arithmetic mean function is established. The algorithm is tested using simulated SAR images and AIS data. Preliminary results of the fusion of SAR and AIS data are presented and discussed.","","978-1-5386-6282-3","10.1109/RTSI.2018.8548352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8548352","SAR;AIS;ship detection;ship monitoring;data fusion;maritime surveillance","Marine vehicles;Artificial intelligence;Synthetic aperture radar;Feature extraction;Detectors;Surveillance","feature extraction;image fusion;radar detection;radar imaging;search radar;ships;synthetic aperture radar","AIS fusion;simulated SAR images;features vectors;fusion block;AIS data;ship position;feature extraction;Constant False Alarm Rate algorithm;SAR image;ship detection;Automatic Identification System data;Synthetic Aperture Radar images;maritime surveillance","","5","","14","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"The Fusion of Morphological and Contextual Information for Building Detection from Very High-Resolution SAR Images","S. Adelipour; H. Ghassemian","Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran","Electrical Engineering (ICEE), Iranian Conference on","27 Sep 2018","2018","","","389","393","Nowadays, very high-resolution synthetic aperture radar (VHR SAR) images are available for interpretation of the built-up area. Buildings are one of the most important parts of the urban area, and in this paper a new method for building detection from a single SAR image is proposed. First, the contextual information of the buildings, such as double bounce, layover and shadow areas are extracted. Using these features, a set of primary detection is made. Second, morphological profiles (MP), with different structural elements (SE), are utilized to build a differential morphological profile (DMP) that provides the building structural information. This structural information is used to make a secondary detection set of building candidates. The final detection result is made by fusion of these two sets. Performance evaluation of the proposed method is reported by the implementation of the method on two different real TerraSAR-X images. The results show that the proposed method has a high detection rate (DR), while the false alarm rate (FAR) is low.","","978-1-5386-4916-9","10.1109/ICEE.2018.8472581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8472581","Building detection;Feature Extraction;Information fusion;Morphological profile;VHR SAR images","Buildings;Synthetic aperture radar;Detectors;Feature extraction;Data mining;Urban areas;Electrical engineering","geophysical image processing;image fusion;object detection;radar imaging;remote sensing by radar;synthetic aperture radar","contextual information;building detection;high-resolution SAR images;high-resolution synthetic aperture radar images;built-up area;urban area;single SAR image;primary detection;morphological profiles;building structural information;secondary detection set;building candidates;final detection result;TerraSAR-X images;high detection rate;layover area;morphological profile;structural elements;shadow area;VHR SAR images","","4","","29","IEEE","27 Sep 2018","","","IEEE","IEEE Conferences"
"New hierarchical joint classification method for SAR-optical multiresolution remote sensing data","I. Hedhli; G. Moser; S. B. Serpico; J. Zerubia","DITEN, University of Genoa, Italy; DITEN, University of Genoa, Italy; DITEN, University of Genoa, Italy; INRIA, AYIN team, France","2015 23rd European Signal Processing Conference (EUSIPCO)","28 Dec 2015","2015","","","759","763","In this paper, we develop a novel classification approach for multiresolution, multisensor (optical and synthetic aperture radar), and/or multiband images. Accurate and time-efficient classification methods are particularly important tools to support rapid and reliable assessment of the ground changes. Given the huge amount and variety of data available currently from last-generation satellite missions, the main difficulty is to develop a classifier that can take benefit of multiband, multiresolution, and multisensor input imagery. The proposed method addresses the problem of multisensor fusion of SAR with optical data for classification purposes, and allows input data collected at multiple resolutions and additional multiscale features derived through wavelets to be fused.","2076-1465","978-0-9928-6263-3","10.1109/EUSIPCO.2015.7362485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362485","Multisensor;multiresolution remote sensing images;supervised classification;hierarchical Markov random fields","Image resolution;Optical sensors;Optical imaging;Signal resolution;Optical signal processing;Synthetic aperture radar","feature extraction;geophysical image processing;image classification;image fusion;image resolution;optical information processing;radar imaging;remote sensing by radar;synthetic aperture radar;wavelet transforms","hierarchical joint classification method;SAR-optical multiresolution remote sensing data;multisensor images;multiband images;synthetic aperture radar;ground changes assessment;last-generation satellite missions;multisensor fusion;optical data;multiscale features","","4","","20","","28 Dec 2015","","","IEEE","IEEE Conferences"
"SAR Object Detection With a Saliency Method Based on PCA and Global Contrast","H. -x. Li; X. -l. Yu; Y. -h. Tang; X. -g. Wang","University of Electronic Science and Technology of China, Chengdu, Sichuan, China; University of Electronic Science and Technology of China, Chengdu, Sichuan, China; University of Electronic Science and Technology of China, Chengdu, Sichuan, China; University of Electronic Science and Technology of China, Chengdu, Sichuan, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1172","1175","Object detection is a significant step in synthetic aperture radar (SAR) image interpretation. This paper contributes to propose a saliency method based on principle component analysis (PCA) and global contrast for SAR object detection. Firstly, the original SAR image is smoothed by Gaussian filter and then segmented into superpixels by the linear iterative cluster (SLIC) algorithm. Secondly, two feature maps are created based on PCA and global contrast respectively, and are fused together to construct the saliency map. Lastly, we adopt an adaptive threshold to segment the saliency map and extract object areas. Experimental results on real SAR images indicate that the constructed saliency map is able to highlight objects and inhibit other areas effectively. Moreover, the proposed method performs well in both detection accuracy and time cost.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900464","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900464","SAR;saliency detection;PCA;global contrast;superpixel","Radar polarimetry;Feature extraction;Synthetic aperture radar;Principal component analysis;Image segmentation;Object detection;Computational modeling","feature extraction;Gaussian processes;image filtering;image fusion;image segmentation;iterative methods;object detection;principal component analysis;radar detection;radar imaging;smoothing methods;synthetic aperture radar","principle component analysis;PCA;SAR object detection;linear iterative cluster algorithm;SAR images;saliency map;detection accuracy;saliency method;synthetic aperture radar image interpretation;superpixels by the linear iterative cluster algorithm;SLIC algorithm","","3","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"SAR Image Registration Based on Optimized Ransac Algorithm with Mixed Feature Extraction","F. Liao; Y. Chen; Y. Chen; Y. Lu","School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Automation Engineering, University of Electronic Science and Technology of China, Chengdu, China; China Centre for Resources Satellite Data and Application, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1153","1156","In this paper, to address the problem in the registration of synthetic aperture radar (SAR), the speeded up robust features (SURF) approach based on multi-level FAST and improved random sampling consistency is proposed. This approach is used to address the high mismatch rate in SAR image registration, and it is based on the characteristics of SAR images. We construct a new corner detection by combining with the hierarchical theory of the system, which is referred to as multi-level FAST (MFAST). The contribution of MFAST algorithm mainly lies in the efficiency of SURF algorithm, which determines the main direction and feature descriptor. And improving the random sampling consistency (RANSAC) to remove mismatched feature points and ameliorate the registration effectiveness. The proposed algorithm is suitable for multi-sensor images with large gray differences and significant edge features. The experimental results show that the registration efficiency of the proposed algorithm is more than three times of the SURF algorithm, and the registration accuracy also is better than traditional SURF algorithm, which can reach 0.39 pixels.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323180","image registration;random sampling consistency;speeded up robust features","Radar polarimetry;Feature extraction;Satellites;Synthetic aperture radar;Approximation algorithms;Image matching;Euclidean distance","edge detection;feature extraction;image fusion;image matching;image registration;radar imaging;synthetic aperture radar","SAR image registration;optimized ransac algorithm;mixed feature extraction;synthetic aperture radar;robust features approach;multilevel FAST;random sampling consistency;high mismatch rate;SAR images;MFAST algorithm;main direction;feature descriptor;mismatched feature points;registration effectiveness;multisensor images;significant edge features;registration efficiency;registration accuracy;traditional SURF algorithm","","3","","5","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Analysis of binary land cover change detection methods using optical and radar data","M. S. Reis; S. J. Siqueira Sant'Anna","Image Processing Division (DPI), Brazilian National Institute for Space Research (INPE), São José dos Campos, SP, Brazil; Image Processing Division (DPI), Brazilian National Institute for Space Research (INPE), SP, Brazil","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","4236","4239","This work evaluates change classifications obtained using four binary change detection methods based on region, applied to optical, Synthetic Aperture Radar (SAR) and fused data. Although optical data has presented the best results, in the cases that such data is unavailable, it is possible to detect changes with high accuracy using SAR data. The use of fused images didn't improve change classification when compared to the use of single optical or SAR data.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326761","Change Detection;data fusion;SAR","Image segmentation;Synthetic aperture radar;Adaptive optics;Optical imaging;Optical sensors;Standards;Remote sensing","geophysical image processing;geophysical techniques;image classification;image fusion;land cover;remote sensing by radar;synthetic aperture radar","binary land cover change detection methods;optical data;radar data;change classifications;synthetic aperture radar;fused data;SAR data;fused images","","1","","8","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"SAR image change detection method based on shearlet transform","Y. Zhang; S. Wang; C. Wang; H. Zhang; F. Wu; M. Liu; Q. Fu; Y. Wang","School of Communication Engineering of Jilin University, Changchun, China; School of Communication Engineering of Jilin University, Changchun, China; University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; Chinese Academy of Sciences, Institute of Remote Sensing and Digital Earth, Beijing, China; China Center for Resources Satellite Date and Application, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2017 Progress in Electromagnetics Research Symposium - Fall (PIERS - FALL)","19 Feb 2018","2017","","","1223","1229","Multi-temporal synthetic aperture radar (SAR) images have been successfully used for the detection of different types of terrain changes. However, SAR image change detection based on wavelet transform is still restrained from the existence of speckle noise and the nature of wavelet transform. In this paper, an unsupervised SAR image change detection fusion framework based on shearlet transform is proposed. In the proposed method, The Gauss filtering is combined with log-ratio to impair speckle. Then the difference map (DM) of Gauss-log ratio and the difference map of ratio based on Gabor feature are fused with shearlet transform. Meanwhile, DM is decomposed to low frequency image and four high frequency images, different fusion rules are used in multi-scales images respectively, the work of noise reduction is operated with mean filtering. After an inverse shearlet transformation, the final change map can be obtained via a simple OSTU segmentation. The real SAR image pairs in Bern area are used to verify proposed change detection method. The experimental results demonstrate the robustness of the proposed method.","","978-1-5386-1211-8","10.1109/PIERS-FALL.2017.8293318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8293318","","Synthetic aperture radar;Wavelet transforms;Electromagnetics;Speckle;Image segmentation;Feature extraction","feature extraction;geophysical image processing;image fusion;image segmentation;remote sensing by radar;speckle;synthetic aperture radar;terrain mapping;unsupervised learning;wavelet transforms","fusion rules;Gauss-log ratio;Gabor feature;OSTU segmentation;inverse shearlet transformation;Gauss filtering;unsupervised SAR image change detection;speckle noise;wavelet transform;terrain changes;multitemporal synthetic aperture radar images;SAR image change detection method;SAR image pairs","","","","11","IEEE","19 Feb 2018","","","IEEE","IEEE Conferences"
"Research progresss on multidimensional space joint-observation SAR","W. Hong","Institute of Electronics Chinese Academy of Sciences, Beijing, CN","2015 40th International Conference on Infrared, Millimeter, and Terahertz waves (IRMMW-THz)","12 Nov 2015","2015","","","1","1","Summary form only given. With the application requirement and technology development, the necessity and tendency of Synthetic Aperture Radar (SAR) imaging within the framework of multidimensional space joint-observation, which are polarimetry, frequency, angle, time series and etc., evoke catholic interests in SAR imaging research nowadays. Recent research progress on the Multidimensional Space Joint-observation SAR (MSJosSAR) in the National Key Lab of Microwave Imaging Technology, Institute of Electronics, Chinese Academy of, Sciences(MITL-IECAS) is reported in this talk, where the a sphere cluster cordinate system is defined as the modeling basis on the demand of information fusion for SAR multidimensional space joint-observation. Further more, the advantage of MSJosSAR is revealed by using Kronecker product decomposition for better understanding of target scattering mechanisms, with the hypothesis and basic framework on which the MSJosSAR signal processing relies on. Tentative studies on multi-layer material with PolinSAR technique, anisotropic scattering mechanisms with multi-directional observation (cuverture or circular SAR technique), and instantaneous time-variant target with array SAR technique are demonstrated as the initial verification of the above defined hypothesis and framework. Finally, the value of joint observation space numbers for typical SAR configurations is enumerated, followed by the perspective discussions on the future work for MSJosSAR study.","2162-2035","978-1-4799-8272-1","10.1109/IRMMW-THz.2015.7327714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7327714","","Synthetic aperture radar;Aerospace electronics;Microwave imaging;Microwave theory and techniques;Radar imaging;Scattering","decomposition;image fusion;radar imaging;radar polarimetry;spaceborne radar;synthetic aperture radar","multidimensional space joint-observation SAR imaging;synthetic aperture radar imaging;polarimetry;time series;MSJosSAR imaging;National Key Lab of Microwave Imaging Technology;Institute of Electronics;Chinese Academy of Sciences;MITL-IECAS;sphere cluster cordinate system;Kronecker product decomposition;target scattering mechanism;signal processing;multilayer material;PolinSAR technique;anisotropic scattering mechanisms;multidirectional observation;circular SAR technique;cuverture SAR technique;instantaneous time-variant target","","","","","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"An interpolation-free FFBP algorithm for spotlight SAR processing","Q. Dong; P. Shao; Z. Yang; Y. Li; M. Xing","National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","4498","4501","In this paper, an interpolation-free fast factorized back-projection (IF-FFBP) algorithm is proposed for high-resolution spotlight synthetic aperture radar (SAR) processing. Different from the original FFBP utilizing two-dimensional image-domain interpolation for sub-aperture fusion, IF-FFBP finishes the image merging steps using chirp-z transform and circular shifting. Under the restriction of the applicable scope, IF-FFBP yields enhanced efficiency over the 4 times upsampling interpolation based FFBP, and keeps the high precision simultaneously. Finally, Real-data experiment verifies the efficiency superiorities of the FIM-FFBP.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326827","SAR;FFBP;interpolation-free","Interpolation;Azimuth;Algorithm design and analysis;Apertures;Synthetic aperture radar;Image resolution;Signal processing algorithms","image fusion;image resolution;interpolation;radar imaging;radar resolution;synthetic aperture radar;Z transforms","interpolation-free FFBP algorithm;high-resolution spotlight SAR processing;IF fast factorized back-projection algorithm;high-resolution spotlight synthetic aperture radar processing;two-dimensional image-domain interpolation;subaperture fusion;chirp-z transform;circular shifting;upsampling interpolation","","","","2","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"SAR tomographic imaging technique based on fusion of the Prony-inspired parametric and MVDR-inspired non-parametric DOA spatial spectral estimators","G. D. Martín del Campo; Y. V. Shkvarko; J. I. Yañez","Centre for Advanced Research and Education of the National Polytechnic Institute CINVESTAV-IPN, Zapopan, Jalisco, México; Centre for Advanced Research and Education of the National Polytechnic Institute CINVESTAV-IPN, Zapopan, Jalisco, México; Centre for Advanced Research and Education of the National Polytechnic Institute CINVESTAV-IPN, Zapopan, Jalisco, México","2015 16th International Radar Symposium (IRS)","27 Aug 2015","2015","","","529","534","In this paper, the synthetic aperture radar tomography (SARTom) vertical distribution estimation problem is treated within the direction of arrival (DOA) estimation framework. Super-resolution parametric DOA estimation methods improve the vertical resolution and mitigate the effect of sidelobes. Nevertheless, these techniques have the main drawback related to the assumption that the scene is composed by a finite number of point-type backscattering sources. On the other hand, the minimum variance distortionless response (MVDR) inspired non-parametric DOA estimation methods are better suited to cope with scenarios characterized by the presence of distributed scatterers. In this work, we propose to decompose the SARTom vertical distribution estimation problem into two paradigms: parametric DOA estimation for point-type scatterers and non-parametric recovery of the spatial spectrum pattern (SSP) of the spatially distributed scattering components. The principal innovative contribution of this study relates to the proposition for fusion of the Prony-inspired parametric and MVDRinspired non-parametric DOA estimation paradigms through the use of the spectral positional invariance property of the point-type targets, which holds with the extended Prony model.","2155-5753","978-3-9540-4853-3","10.1109/IRS.2015.7226244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226244","","Direction-of-arrival estimation;Estimation;Array signal processing;Robustness;Synthetic aperture radar;Signal resolution;Scattering","direction-of-arrival estimation;image fusion;radar imaging;synthetic aperture radar","SAR tomographic imaging technique;prony inspired parametric;MVDR inspired nonparametric DOA spatial spectral estimators;synthetic aperture radar tomography;SARTom;vertical distribution estimation problem;direction of arrival estimation framework;finite number;point type backscattering source;distributed scatterers;parametric DOA estimation;spatial spectrum pattern;SSP;principal innovative contribution","","","","7","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Scattering and Regional Features Fusion Using Collaborative Representation for PolSAR Image Classification","M. Imani","Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran","2022 9th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)","19 Apr 2022","2022","","","1","6","While the collaborative representation has been used for classification of multi-channel images in several works, it is suggested for scattering and spatial features fusion of polarimetric synthetic aperture radar (PolSAR) images in this work. With considering a neighboring region around each pixel of the PolSAR image, its approximation is computed by its adjacent samples in the local region by solving a convex optimization problem. The samples with more similar scattering characteristics will have more important role in the pixel representation. The obtained collaborative representation can be considered as a fused polarimetric-contextual feature space, which can be given to any arbitrary classifier. The experimental results on three real PolSAR images show the good performance of the fused feature space in providing a clear and accurate classification map.","2771-1374","978-1-6654-7872-4","10.1109/CFIS54774.2022.9756487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756487","PolSAR;feature fusion;classification;collaborative representation","Collaboration;Scattering;Speckle;Polarimetric synthetic aperture radar;Convex functions;Intelligent systems;Image classification","feature extraction;geophysical image processing;image classification;image fusion;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","regional features fusion;collaborative representation;PolSAR image classification;multichannel images;spatial features fusion;polarimetric synthetic aperture radar images;neighboring region;adjacent samples;local region;convex optimization problem;similar scattering characteristics;pixel representation;polarimetric-contextual feature space;fused feature space;clear classification map;accurate classification map","","","","22","IEEE","19 Apr 2022","","","IEEE","IEEE Conferences"
"Multistatic SAR Information Fusion Based on Image Registration and Fake Color Synthesis","W. Wang; J. Wu; X. Yang; Y. Miao; J. Yang; H. Yang","School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China; School of Electronic Engineering, University of Electronic Science and Technology of China, Chengdu, P.R. China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","7275","7278","Due to finite of information dimension of single bistatic synthetic aperture radar (SAR), to expand more information about ground objects we research multistatic SAR which can be decomposed into groups of bistatic SAR. It is known that scattering properties of the different viewing angles is different. In this paper, we focus on system of single transmitter and triple receivers and use polar format algorithm(PFA) to obtain images of ground objects for the triple receivers. A geometric distortion correction method is proposed due to elevation of ground objects. After the distortion correction, the three SAR images are registrated, then we put images into red, green, blue(RGB) channels respectively to realize fake color synthesis, and thus realize information fusion.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518875","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518875","Multistatic SAR;geometric distortion;fake color;information fusion","Synthetic aperture radar;Receivers;Image color analysis;Distortion;Transmitters;Imaging;Scattering","image colour analysis;image fusion;image registration;radar imaging;synthetic aperture radar","SAR images;fake color synthesis;multistatic SAR information fusion;image registration;single bistatic synthetic aperture radar;polar format algorithm;ground objects image","","","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Improving SAR and Optical Image Fusion for Lulc Classification with Domain Knowledge","K. R. Prabhakar; V. H. Nukala; J. Gubbi; A. Pal; B. P","TCS Research, INDIA; TCS Research, INDIA; TCS Research, INDIA; TCS Research, INDIA; TCS Research, INDIA","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","711","714","Fusing SAR and multi-spectral images to generate a precise land cover map in a weakly supervised setting is a challenging yet essential problem. The inaccurate, noisy, and inexact ground truth labels pose difficulty training any machine learning models. In this paper, we make a fundamental and pivotal contribution towards improving the ground truth label quality using domain knowledge. We present a simple yet effective mechanism to refine the low-resolution noisy ground truth labels. The proposed approach is trained and tested on a publicly available DFC2020 dataset. Through experiments, we show the effectiveness of our method by training a deep learning model on the refined labels that outperform even the models trained with clean ground truth.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884283","CNN;SAR;multispectral;image fusion","Training;Deep learning;Training data;Geoscience and remote sensing;Optical imaging;Noise measurement;Reliability","geophysical image processing;image classification;image fusion;land cover;learning (artificial intelligence);optical images;radar imaging;synthetic aperture radar;terrain mapping","SAR;multispectral images;precise land cover map;weakly supervised setting;challenging yet essential problem;inaccurate ground truth labels;noisy, ground truth labels;inexact ground truth labels;machine learning models;fundamental contribution;pivotal contribution;domain knowledge;simple yet effective mechanism;low-resolution noisy ground truth labels;publicly available DFC2020 dataset;deep learning model;refined labels;clean ground truth;optical image fusion;lulc classification","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A New Image Fusion Method for Ship Target Enhancement in Spaceborne and Airborne SAR Collaboration","X. Wang; D. Zhu; G. Li; X. -P. Zhang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electrical, Computer and Biomedical, Engineering Ryerson University, Toronto, Canada","2021 IEEE 24th International Conference on Information Fusion (FUSION)","2 Dec 2021","2021","","","1","5","In this paper, we investigate the fusion of spaceborne synthetic aperture radar (SAR) and airborne SAR images and its application to ship target enhancement. In this paper, we propose a new target proposal and clutter copula (TPCC)-based image fusion method for the collaboration of spaceborne and airborne SARs. TPCC enhances the common ship target areas in spaceborne and airborne SAR images via the intersection of target proposals and suppresses the clutter areas by establishing the joint distribution of clutter in the spaceborne and airborne SAR images based on the copula theory. Compared with other commonly used image fusion methods, the target dependence and clutter dependence in the spaceborne and airborne SAR images are newly exploited in TPCC. We demonstrate the superiority of TPCC in terms of target-to-clutter ratios (TCRs) by using composite images combining Gaofen-3 satellite and unmanned aerial vehicle (UAV) SAR images.","","978-1-7377497-1-4","10.23919/FUSION49465.2021.9626972","National Natural Science Foundation of China; National Postdoctoral Program for Innovative Talents; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9626972","Sensor fusion;target proposals;copula theory;target enhancement","Satellites;Spaceborne radar;Collaboration;Autonomous aerial vehicles;Radar polarimetry;Proposals;Clutter","autonomous aerial vehicles;image fusion;radar imaging;remotely operated vehicles;ships;spaceborne radar;synthetic aperture radar","target proposal;TPCC;airborne SARs;common ship target areas;airborne SAR images;target proposals;spaceborne SAR images;commonly used image fusion methods;target dependence;clutter dependence;target-to-clutter ratios;composite images;new image fusion method;ship target enhancement;airborne SAR collaboration","","","","30","","2 Dec 2021","","","IEEE","IEEE Conferences"
"GA-SVM Algorithm for Improving Land-Cover Classification Using SAR and Optical Remote Sensing Data","C. Sukawattanavijit; J. Chen; H. Zhang","School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","3","284","288","Multisource remote sensing data have been widely used to improve land-cover classifications. The combination of synthetic aperture radar (SAR) and optical imagery can detect different land-cover types, and the use of genetic algorithms (GAs) and support vector machines (SVMs) can lead to improved classifications. Moreover, SVM kernel parameters and feature selection affect the classification accuracy. Thus, a GA was implemented for feature selection and parameter optimization. In this letter, a GA-SVM algorithm was proposed as a method of classifying multifrequency RADARSAT-2 (RS2) SAR images and Thaichote (THEOS) multispectral images. The results of the GA-SVM algorithm were compared with those of the grid search algorithm, a traditional method of parameter searching. The results showed that the GA-SVM algorithm outperformed the grid search approach and provided higher classification accuracy using fewer input features. The images obtained by fusing RS2 data and THEOS data provided high classification accuracy at over 95%. The results showed improved classification accuracy and demonstrated the advantages of using the GA-SVM algorithm, which provided the best accuracy using fewer features.","1558-0571","","10.1109/LGRS.2016.2628406","Geo-Informatics and Space Technology Development Agency; National Natural Science Foundation of China(grant numbers:61132006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831422","Genetic algorithms (GAs);image fusion;land-cover classification;multisource data;optical imagery;support vector machine (SVM);synthetic aperture radar (SAR)","Support vector machines;Genetic algorithms;Kernel;Synthetic aperture radar;Biological cells;Remote sensing;Adaptive optics","estimation theory;feature selection;geophysical image processing;image classification;land cover;parameter estimation;support vector machines;terrain mapping","GA-SVM algorithm;land-cover classification;optical remote sensing data;synthetic aperture radar;optical imagery;land-cover types;genetic algorithm;support vector machine;SVM kernel parameter;GA feature selection;higher classification accuracy;THEOS data;RS2 data fusion;Thaichote multispectral image;feature selection;multifrequency RADARSAT-2 SAR image classification data;grid search algorithm;traditional parameter searching method","","110","","17","IEEE","24 Jan 2017","","","IEEE","IEEE Journals"
"Small Vessel Detection Based on Adaptive Dual-Polarimetric Sar Feature Fusion and Attention-Enhanced Feature Pyramid Network","F. Zhang; Y. Zhou; F. Zhang; Q. Yin; F. Ma","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P. R. China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2218","2221","Small vessels in synthetic aperture radar (SAR) images usually have weak scattering intensity and occupy only a few numbers of image pixels, resulting in a high miss detection rate during the detection process. Regarding the problem, two solutions were presented in this paper. Firstly, dual-polarimetric SAR data were used and dual-polarimetric features were adaptively fused. Comparing to single-polarization and conventional non-adaptive fusion method, it optimally enhanced the characteristics of small vessels. Secondly, the conventional feature pyramid network (FPN) was enhanced by reducing the downsampling factor, adding spatial attention, and channel attention. The added spatial attention enhanced the significant features of small vessels on the large-scale feature map; the added channel attention filtered out the spliced features maps that were benefiting small vessel detection and reduced feature redundancy. Experimental results on the small vessel data set of Sentinel-1 verified that it not only reduced the miss detection rate but also improved calculation efficiency.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555096","National Natural Science Foundation of China(grant numbers:61871413,61801015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555096","Synthetic aperture radar (SAR);convolutional neural network (CNN);small vessel detection;feature fusion;attention enhanced","Adaptive systems;Surveillance;Oceans;Redundancy;Neural networks;Scattering;Geoscience and remote sensing","feature extraction;image fusion;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","vessel detection;adaptive dual-polarimetric SAR feature fusion;attention-enhanced feature pyramid network;vessels;synthetic aperture radar images;weak scattering intensity;image pixels;high miss detection rate;detection process;dual-polarimetric SAR data;dual-polarimetric features;nonadaptive fusion method;conventional feature pyramid network;spatial attention;large-scale feature map;added channel attention;spliced features maps;reduced feature redundancy;vessel data","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multilevel Information Fusion-Based Change Detection for Multiangle PolSAR Images","B. Zou; H. Li; L. Zhang","Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Information Engineering, Harbin Institute of Technology, Harbin, China","IEEE Geoscience and Remote Sensing Letters","17 Dec 2021","2022","19","","1","5","Change detection is a key technology in the field of polarimetric synthetic aperture radar (PolSAR) image processing. The current research on the change detection mainly focuses on studying PolSAR images with the same angle or small angle difference, and the angle problem is not considered. However, when the angle difference occurs, especially a large angle difference, some pixels might be falsely detected because the angle difference can affect the polarimetric characteristics. In this letter, we propose a multilevel information fusion-based (MIFB) method, which is suitable for extracting change information from PolSAR images with angle difference. In particular, the proposed method first adopts data resolution correction, then applies an improved feature-based registration algorithm, and finally, incorporates weighted graph theory with the superpixel segmentation algorithm to extract and merge pixel-based and object-based change areas to eliminate false alarms. Experimental results for multitemporal and multiangle PolSAR images reveal that the MIFB method can effectively eliminate false detection caused by angle differences and improve the detection accuracy.","1558-0571","","10.1109/LGRS.2020.3041307","National Natural Science Foundation of China(grant numbers:61401124,61871158); Scientific Research Foundation for the Returned Overseas Scholars of Heilongjiang Province(grant numbers:LC2018029); Aeronautical Science Foundation of China(grant numbers:20182077008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288684","Change detection;image registration;multiangle;polarimetric synthetic aperture radar (PolSAR)","Scattering;Data mining;Feature extraction;TV;Change detection algorithms;Signal processing algorithms;Image segmentation","feature extraction;graph theory;image fusion;image registration;image segmentation;radar imaging;radar polarimetry;synthetic aperture radar","multilevel information fusion-based change detection;multiangle PolSAR images;polarimetric synthetic aperture radar image processing;multilevel information fusion-based method;change information;small angle difference;improved feature-based registration algorithm;object-based change areas;data resolution correction","","1","","13","IEEE","9 Dec 2020","","","IEEE","IEEE Journals"
"A method for extracting InSAR image features of negative and positive obstacles","Z. Jiang; Q. Song; J. Wang; Z. Zhou","College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","1170","1173","Radar sensors have received more and more interest for unmanned ground vehicle to sense positive and negative obstacles in unstructured environments or out fields, especially on negative obstacle. In this paper, we present an approach for extracting the features of obstacles from radar images. Based on interferometric synthetic aperture radar (InSAR) images focused by the back-projection (BP) algorithm, range compensation, speckle filtering and threshold segmentation are performed. And morphological operations are used to perform some simple connectivity filtering to smooth the image and remove spurious pixels. Finally, feature fusion is applied to the amplitude and the correlation coefficient images. Both the theoretical analysis and the experimental results indicate that the proposed method is an efficient method.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729296","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729296","Positive and negative obstacles;interferometric synthetic aperture radar (InSAR);range compensation;threshold segmentation;morphological operations","Radar imaging;Coherence;Filtering;Image segmentation;Speckle;Feature extraction","feature extraction;image filtering;image fusion;image segmentation;radar imaging;radar interferometry;radar receivers;remotely operated vehicles;synthetic aperture radar","InSAR image feature extraction;negative obstacle;positive obstacle;radar sensors;unmanned ground vehicle;interferometric synthetic aperture radar image;back-projection algorithm;range compensation;speckle filtering;threshold segmentation;morphological operation;spurious pixel removal;feature fusion;correlation coefficient image filtering","","1","","11","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"SAR Image Feature Selection and Change Detection Based on Sparse Coefficient Correlation","W. Song; H. Quan; Y. Chen; P. Zhang","Xi'an Key Laboratory of Heterogeneous Network Convergence Communication, Xi'an University of Science and Technology, Xian, China; Xi'an Key Laboratory of Heterogeneous Network Convergence Communication, Xi'an University of Science and Technology, Xian, China; Xi'an Key Laboratory of Heterogeneous Network Convergence Communication, Xi'an University of Science and Technology, Xian, China; National Laboratory of Radar Signal Processing, Xidian University, Xian, China","2022 17th International Conference on Control, Automation, Robotics and Vision (ICARCV)","10 Jan 2023","2022","","","326","329","High-dimensional features extraction and selection is of great significance for synthetic aperture radar (SAR) image change detection. In this paper, a feature selection based on sparse coefficient correlation, abbreviated as SR-PCC, is proposed to realize the local reconstruction of known samples, so as to improve the accuracy of change detection. Firstly, high-dimensional texture features are extracted from real SAR images and then fused by stacking. Secondly, for the known samples, the sparse representation is performed and then the sparse coefficients are obtained. Then, the Pearson correlation coefficient method is used to select sparse coefficients related to the image itself, thus realizing local optimal reconstruction. Finally, the selected features are inputted into the support vector machine (SVM) to realize change detection. Experiments on real SAR images demonstrate the effectiveness of the proposed SR-PCC in high-dimensional feature selection and illustrate that it can provide better change detection maps.","","978-1-6654-7687-4","10.1109/ICARCV57592.2022.10004246","Natural Science Foundation of China(grant numbers:61901358); Scientific Research Plan Projects of Shaanxi Education Department(grant numbers:20JK0757); China Postdoctoral Science Foundation(grant numbers:2020M673347); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004246","Synthetic aperture radar (SAR);change detection;feature selection;sparse representation;sparse coefficient correlation","Support vector machines;Correlation coefficient;Correlation;Stacking;Feature extraction;Radar polarimetry;Image reconstruction","feature extraction;feature selection;image fusion;image reconstruction;image texture;optimisation;radar detection;radar imaging;support vector machines;synthetic aperture radar","high-dimensional features extraction;high-dimensional texture features;image stacking;local optimal reconstruction;local reconstruction;Pearson correlation coefficient method;SAR image feature selection;sparse coefficient correlation;sparse representation;SR-PCC;support vector machine;SVM;synthetic aperture radar image change detection","","","","11","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"SAR target recognition method of MSTAR data set based on multi-feature fusion","J. Shi","Beijing Institute of Remote Sensing Equipment, The Second Research Institute of China Aerospace Science and Industry Corporation, Beijing, China","2022 International Conference on Big Data, Information and Computer Network (BDICN)","20 Apr 2022","2022","","","626","632","To solve the problem of low recognition rate of synthetic aperture radar (SAR) target based on feature recognition, a target recognition method of SAR image based on multi-feature fusion is proposed, which combines Hu moment, Harris corner point and Gabor feature. The three kinds of features describe the target's geometric shape feature, corner feature and image texture feature respectively, which can improve the accuracy of SAR target recognition from the aspect of feature extraction. Based on the MSTAR data set, the experiment is carried out under standard and extended operating conditions. The results show that the proposed method can effectively overcome the deficiency of insufficient single feature description information and improve the SAR target recognition rate to a certain extent.","","978-1-6654-8476-3","10.1109/BDICN55575.2022.00120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9758449","component;synthetic aperture radar;target recognition;multi-feature fusion;feature extraction","Training;Image texture;Image recognition;Target recognition;Shape;Feature extraction;Radar polarimetry","feature extraction;image fusion;image texture;radar imaging;radar target recognition;synthetic aperture radar","low recognition rate;synthetic aperture radar target;feature recognition;SAR image;multifeature fusion;Harris corner point;Gabor feature;corner feature;image texture feature;feature extraction;MSTAR data;insufficient single feature description information;SAR target recognition rate;Hu moment","","","","21","IEEE","20 Apr 2022","","","IEEE","IEEE Conferences"
"A Novel Multispectral, Panchromatic and SAR Data Fusion for Land Classification","P. Iervolino; R. Guida; D. Riccio; R. Rea","Airbus Defence and Space, Guildford; Surrey Space Centre, University of Surrey, Guildford, U.K.; Dipartimento di Ingegneria Elettrica e Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli “Federico II”, Napoli, Italy; Dipartimento di Ingegneria Elettrica e Tecnologie dell'Informazione (DIETI), Università degli Studi di Napoli “Federico II”, Napoli, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Nov 2019","2019","12","10","3966","3979","Multisensor data fusion is addressed in this article for land classification purposes in a semiarid environment. A novel algorithm based on multispectral, panchromatic and synthetic aperture radar (SAR) data is here presented. The proposed multisensory data fusion approach relies on the generalized intensity-hue-saturation (G-IHS) transform and the À trous wavelet transform (ATWT). The fusion product is obtained by modulating the high features details of the panchromatic ATWT with the SAR texture and by replacing the high-pass details of the G-IHS Intensity component with this panchromatic-SAR modulation. After the fusion product is derived, a classification is performed by using a standard maximum likelihood classifier. The proposed algorithm is tested over a meaningful case study acquired over the Maspalomas Special Natural Reserve (Spain) and processing data from WorldView-2 (for both multispectral and panchromatic channels) and TerraSAR-X (for the SAR channel) missions. Results show a fine preservation of the spectral information contained in each multispectral band. Sharpened details are observed over built-up areas and a smoothing texture is perceived over homogeneous areas (lakes, sea, bare soil, and roads) due to the SAR-panchromatic modulation. This leads to a better overall classification accuracy of the fused image compared to outcomes obtained with a single sensor, resulting 7% and 2% more accurate than multispectral and pan-sharpening classification, respectively.","2151-1535","","10.1109/JSTARS.2019.2945188","University of Surrey; Universitá degli Studi di Napoli Federico II; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869906","Image classification;image fusion;multispectral imaging;synthetic aperture radar (SAR)","Synthetic aperture radar;Data integration;Earth;Remote sensing;Feature extraction;Satellites;Transforms","geophysical image processing;image classification;image fusion;image resolution;image texture;maximum likelihood estimation;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping;wavelet transforms","SAR channel;multispectral band;SAR-panchromatic modulation;classification accuracy;pan-sharpening classification;novel multispectral;SAR data fusion;land classification purposes;semiarid environment;synthetic aperture radar data;multisensory data fusion approach;generalized intensity-hue-saturation;fusion product;high features details;panchromatic ATWT;SAR texture;high-pass details;G-IHS Intensity component;panchromatic-SAR modulation;standard maximum likelihood classifier;Maspalomas Special Natural Reserve;WorldView-2;TerraSAR-X","","13","","51","IEEE","16 Oct 2019","","","IEEE","IEEE Journals"
"An Unsupervised SAR and Optical Image Fusion Network Based on Structure-Texture Decomposition","Y. Ye; W. Liu; L. Zhou; T. Peng; Q. Xu","State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; State-Province Joint Engineering Laboratory of Spatial Information Technology for High-Speed Railway Safety, Southwest Jiaotong University, Chengdu, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China","IEEE Geoscience and Remote Sensing Letters","16 Nov 2022","2022","19","","1","5","Although the unique advantages of optical and synthetic aperture radar (SAR) images promote their fusion, the integration of complementary features from the two types of data and their effective fusion remains a vital problem. To address that, a novel framework is designed based on the observation that the structure of SAR images and the texture of optical images look complementary. The proposed framework, named SOSTF, is an unsupervised end-to-end fusion network that aims to integrate structural features from SAR images and detailed texture features from optical images into the fusion results. The proposed method adopts the nest connect-based architecture, including an encoder network, a fusion part, and a decoder network. To maintain the structure and texture information of input images, the encoder architecture is utilized to extract multiscale features from images. Then, we use the densely connected convolutional network (DenseNet) to perform feature fusion. Finally, we reconstruct the fusion image using a decoder network. In the training stage, we introduce a structure-texture decomposition model. In addition, a novel texture-preserving and structure-enhancing loss function are designed to train the DenseNet to enhance the structure and texture features of fusion results. Qualitative and quantitative comparisons of the fusion results with nine advanced methods demonstrate that the proposed method can fuse the complementary features of SAR and optical images more effectively.","1558-0571","","10.1109/LGRS.2022.3219341","National Natural Science Foundation of China(grant numbers:41971281,61972021,42271446); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0537); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9936631","Image fusion;SOSTF;synthetic aperture radar (SAR) and optical images;unsupervised","Optical sensors;Optical imaging;Radar polarimetry;Adaptive optics;Optical filters;Optical reflection;Optical losses","feature extraction;geophysical image processing;image fusion;image texture;optical images;radar imaging;sensor fusion;synthetic aperture radar","complementary features;decoder network;densely connected convolutional network;detailed texture features;effective fusion;encoder network;feature fusion;fusion image;fusion part;fusion results;input images;multiscale features;novel texture-preserving;optical image fusion network;optical images;SAR images;structural features;structure-texture decomposition model;texture information;unsupervised end-to-end fusion network;unsupervised SAR","","","","20","IEEE","3 Nov 2022","","","IEEE","IEEE Journals"
"Airborne Circular W-Band SAR for Multiple Aspect Urban Site Monitoring","S. Palm; R. Sommer; D. Janssen; A. Tessmann; U. Stilla","Fraunhofer Institute for High Frequency Physics and Radar Techniques, Wachtberg, Germany; Fraunhofer Institute for High Frequency Physics and Radar Techniques, Wachtberg, Germany; Fraunhofer Institute for High Frequency Physics and Radar Techniques, Wachtberg, Germany; Fraunhofer Institute for Applied Solid State Physics IAF, Freiburg, Germany; Department of Photogrammetry and Remote Sensing, Technische Universität München, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","27 Aug 2019","2019","57","9","6996","7016","This paper presents a strategy for urban site monitoring by very high-resolution circular synthetic aperture radar (CSAR) imaging of multiple aspects. We analytically derive the limits of coherent azimuth processing for nonplanar objects in CSAR if no digital surface model (DSM) is available. The result indicates the level of maximum achievable resolution of these objects in this geometry. The difficulty of constantly illuminating a specific scene in full aspect mode (360°) for such small wavelengths is solved by a hardware- and software-side integration of the radar in a mechanical tracking mode. This results in the first demonstration of full aspect airborne subaperture CSAR images collected with an active frequency-modulated continuous wave (FMCW) radar at W-band. We describe the geometry and the implementation of the real-time beam-steering mode and evaluate resulting effects in the CSAR processing chain. The physical properties in W-band allow the use of extremely short subapertures in length while generating high azimuthal bandwidths. We use this feature to generate full aspect image stacks for CSAR video monitoring in very high frame rates. This technique offers the capability of detecting and observing moving objects in single channel data by shadow tracking. Due to the relatively strong echo of roads, the shadows of moving cars are rich in contrast. The image stack is further evaluated to present wide angular anisotropic properties of targets and first results on multiple aspect image fusion. Both topics show huge potential for further investigations in terms of image analysis and scene classification.","1558-0644","","10.1109/TGRS.2019.2909949","German Ministry of Defence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701523","Airborne circular synthetic aperture radar (CSAR);full aspect image analysis;millimeter-wave radar;nonplanar targets;radar signal processing;shadow tracking;video SAR","Synthetic aperture radar;Monitoring;Radar imaging;Radar tracking;Trajectory;Geometry","airborne radar;CW radar;FM radar;geophysical image processing;image classification;image fusion;object detection;radar imaging;remote sensing by radar;synthetic aperture radar","CSAR processing chain;high azimuthal bandwidths;aspect image stacks;CSAR video monitoring;high frame rates;image stack;multiple aspect image fusion;image analysis;scene classification;airborne circular W-band SAR;multiple aspect urban site monitoring;high-resolution circular synthetic aperture radar imaging;coherent azimuth processing;nonplanar objects;digital surface model;maximum achievable resolution;aspect mode;mechanical tracking mode;aspect airborne subaperture CSAR images;active frequency-modulated;real-time beam-steering mode","","25","","34","IEEE","29 Apr 2019","","","IEEE","IEEE Journals"
"An urban expansion model for African cities using fused multi temporal optical and SAR data","M. Shimoni; J. Lopez; Y. Forget; E. Wolff; C. Michellier; T. Grippa; C. Linard; M. Gilbert","Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Universite Libre de Bruxelles, Bruxelles, Bruxelles, BE; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium; Dept. of Electrical Engineering (SIC-RMA), Signal and Image Centre, Brussels, Belgium","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","1159","1162","The forecast of human population distribution in Africa is limited by the lack of spatial urban expansion model and the quality of its data sources. One way to overcome this shortcoming is to integrate multi-source and multi-temporal data for improving the delineation and the characterization of human settlements. This paper presents a fully automatic fusion processing scheme of multi-temporal SAR and optical data for improving the segmentation of African urban areas.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325977","Urban expansion;density;fusion processing;support vector machine (SVM)","Synthetic aperture radar;Optical imaging;Sociology;Statistics;Remote sensing;Cities and towns;Adaptive optics","geophysical image processing;image fusion;image segmentation;land use;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar","multitemporal optical data fusion;multitemporal SAR data fusion;synthetic aperture radar;human population distribution;spatial urban expansion model;human settlement characterization;automatic fusion processing scheme;African urban area segmentation","","2","","16","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"A coarse-to-precise matching strategy in navigation","H. Liu; Binbin Geng; Z. Wang; Yan Lv","School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; NA","2016 IEEE Chinese Guidance, Navigation and Control Conference (CGNCC)","23 Jan 2017","2016","","","2170","2174","In order to ensure that SAR scene matching aided navigation system can acquire the position errors and yawing errors simultaneously, it is important to select the matching method carefully. In this paper, a coarse-to-precise matching strategy is proposed to be applied in navigation. The proposed method combines area-based and feature-based matching method. Specifically, in the coarse phase, a wavelet-based pyramid multi-resolution technique is adopted to reduce the number of calculated pixels, and then the robustness of the strategy is achieved by a multi-scale circular template fusion. In the precise stage, a nonlinear scale space method is recommended to accomplish the high matching performance. Two experimental results, matching real-time images with reference image and matching multiple real-time subimages with reference image, are listed to validate the proposed strategy.","","978-1-4673-8318-9","10.1109/CGNCC.2016.7829128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829128","","Real-time systems;Navigation;Feature extraction;Synthetic aperture radar;Robustness;Distortion;Transforms","image fusion;image matching;image resolution;radar imaging;radionavigation;synthetic aperture radar;wavelet transforms","coarse-to-precise matching strategy;synthetic aperture radar;SAR scene matching aided navigation system;position errors;yawing errors;area-based matching method;feature-based matching method;wavelet-based pyramid multiresolution technique;multiscale circular template fusion;nonlinear scale space method;reference image;real-time subimages","","1","","13","IEEE","23 Jan 2017","","","IEEE","IEEE Conferences"
"An Adaptive Multisensor Image Fusion Method Based on Monogenic Features","X. Ji; X. Cheng","Nanjing Vocational College of Information Technology, Nanjing, China; Department of Computer Science, Middlesex University, London, U.K","IEEE Sensors Journal","15 Jul 2021","2021","21","14","15598","15606","Each kind of sensor is designed to adapt to a specific environment and usage scope. In this paper, an adaptive multisensor image fusion method based on monogenic features is proposed. For the proposed method, the monogenic features are extracted from the source images including monogenic amplitude, monogenic phase and monogenic orientation features. The energy matching similarity measure is used to measure the monogenic amplitude feature, and the complex coefficient structure similarity measure is used to characterize the monogenic phase and the monogenic orientation feature. Analyses the monogenic features in different types of regions, different fusion strategies are adopted for different characteristic regions. By means of visible and infrared image fusion, SAR and infrared image fusion experiments, the proposed method can effectively preserve a large amount of information and significantly improve the visual quality of the fused image, which can be evaluated with objective indicators.","1558-1748","","10.1109/JSEN.2020.3009242","Jiangsu Provincial Colleges Natural Science Research Project(grant numbers:19KJB510043); University-Level Fund Project of Nanjing Vocational College of Information Technology(grant numbers:YB20180101); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139992","Image fusion;adaptive;monogenic features;similarity measure","Image fusion;Transforms;Feature extraction;Energy measurement;Phase measurement;Sensors;Synthetic aperture radar","feature extraction;image fusion;infrared imaging","adaptive multisensor image fusion method;monogenic phase;monogenic orientation feature;monogenic amplitude feature;infrared image fusion","","5","","23","IEEE","14 Jul 2020","","","IEEE","IEEE Journals"
"Sentinel-1 and Sentinel-2 Data Fusion for Urban Change Detection","A. Benedetti; M. Picchiani; F. Del Frate","University of Tor Vergata, Rome, Italy; University of Tor Vergata, Rome, Italy; University of Tor Vergata, Rome, Italy","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","1962","1965","In this paper a new approach based on the fusion of Sentinel-1 and Sentinel-2 products to map urban change detection and to observe suburb's development is presented. The algorithm developed can process data in a fast, automatic and accurate way. To reach this goal, the processing chain uses an iterative multitemporal approach based, for each iteration, on three procedures. The first and second ones are based on Pulse Coupled Neural Network (PCNN) applied to SAR and optical images, respectively, while the third processing is an optical multiband filter, implementing the spectral difference computation. The three outputs of each iteration are fused together by means of a weighted average formulation. The algorithm may deal with multitemporal acquisitions to improve the overall accuracy in the detection of urban changes by the integration of the outputs at different time intervals.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517586","Sentinel-1;Sentinel-2;image fusion;change detection;global monitoring urbanization","Optical filters;Optical imaging;Synthetic aperture radar;Change detection algorithms;Remote sensing;Data integration;Adaptive optics","geophysical image processing;geophysical techniques;image fusion;neural nets;remote sensing by radar;synthetic aperture radar;terrain mapping","Sentinel-1;Sentinel-2 data fusion;Sentinel-2 products;map urban change detection;suburb;processing chain;iterative multitemporal approach;Pulse Coupled Neural Network;optical images;optical multiband filter;urban changes","","13","","13","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"A Real-Time Imaging Processing Method Based on Modified RMA with Sub-Aperture Images Fusion for Spaceborne Spotlight SAR","F. Zhou; J. Yang; G. Sun; J. Zhang","School of Computer and Information, Hefei University of Technology, Hefei, China; School of Computer and Information, Hefei University of Technology, Hefei, China; National Key Lab of Radar Signal Processing, Xidian University, Xi'an, China; Key Laboratory of Aperture Array and Space Application, East China Research Institute of Electronic Engineering, Hefei, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1905","1908","The small satellite SAR has received increasing attention due to its flexibility and low cost. But limited by the data transmission technology, real-time transmission of a large amount of raw data generated by the spaceborne spotlight SAR can hardly be achieved. Meanwhile, the azimuth bandwidth of the spotlight mode is larger than the PRF, resulting in aliasing of the azimuth spectrum. Based on these, this paper proposes a real-time scheme for small satellite SAR with spotlight mode. The method can solve the problem of data transmission and eliminate spectrum overlap in Doppler domain by means of sub-aperture processing. The modified range migration algorithm (RMA) is used to perform range compression and range cell migration compensation (RCMC) on sub-aperture data. Then dechirp in the azimuth time domain is applied to obtain the low-resolution complex image focused in the range time-azimuth frequency domain. Finally, all theected onto a grid image with azimuth interval matching the azimuth full-resolution to complete image fusion.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324118","National Natural Science Foundation of China(grant numbers:61701156,2016M592045); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324118","spaceborne spotlight SAR;real-time transmission;sub-aperture;real-time scheme","Azimuth;Spaceborne radar;Real-time systems;Synthetic aperture radar;Doppler effect;Imaging;Bandwidth","image fusion;image processing;image resolution;radar imaging;spaceborne radar;synthetic aperture radar","satellite SAR;flexibility;data transmission technology;real-time transmission;raw data;spaceborne spotlight SAR;azimuth bandwidth;spotlight mode;azimuth spectrum;real-time scheme;sub-aperture processing;modified range migration algorithm;range compression;range cell migration compensation;sub-aperture data;azimuth time domain;low-resolution complex image;range time-azimuth frequency domain;grid image;azimuth interval;azimuth full-resolution;complete image fusion;real-time imaging processing method;modified RMA;sub-aperture images fusion","","4","","12","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A modified fast factorized back projection algorithm for the spotlight SAR imaging","S. Zuo; G. Sun; M. Xing; W. Chang","National Key Lab of Radar Signal Processing, Xidian University, Xi'an, P. R. China; National Key Lab of Radar Signal Processing, Xidian University, Xi'an, P. R. China; National Key Lab of Radar Signal Processing, Xidian University, Xi'an, P. R. China; China Electronics Technology Group, Fourteenth Research Institute, Nanjing, P. R. China","2015 IEEE 5th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","29 Oct 2015","2015","","","756","759","In the Fast Factorized Back Projection Algorithm [1], the 2D interpolation in the image domain is an essential procedure to obtain a fine resolution image which is achieved point by point. The computational burden is intensive [2]. This paper presents a modified FFBPA for the spotlight SAR imaging. In this algorithm, the coarse sub-images obtained by back projecting the sub-aperture signals to the imaging grids in local polar coordinates are corrected by the shift in the range dimension and the rotation in the angle dimension. And then the sub-images are fused to synthesize the high resolution image. The method is convenient and efficient in practical operation.","","978-1-4673-7297-8","10.1109/APSAR.2015.7306315","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306315","SAR;spotlight;Fast Factorized Back Projection Algorithm (FFBPA);Geometric correction;Individual interpolation","Projection algorithms;Interpolation;Image resolution;Radar imaging;Radar polarimetry;Imaging;Apertures","image fusion;image resolution;interpolation;radar imaging;radar resolution;synthetic aperture radar","high resolution image;local polar coordinates;coarse subimage fusion;subaperture signal back projection;modified FFBPA;fine resolution image;image domain;2D interpolation;spotlight SAR imaging;modified fast factorized back projection algorithm","","2","","2","IEEE","29 Oct 2015","","","IEEE","IEEE Conferences"
"Sar And Optical Data Fusion Based On Anisotropic Diffusion With Pca And Classification Using Patch-Based Svm With Lbp","A. Shakya; M. Biswas; M. Pal","Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Civil Engineering Department, National Institute of Technology, Kurukshetra, India","2020 IEEE India Geoscience and Remote Sensing Symposium (InGARSS)","23 Feb 2021","2020","","","25","28","SAR (VV and VH polarization) and optical data are widely used in image fusion to use the complimentary information of each other and to obtain the better-quality image (in terms of spatial and spectral features) for the improved classification results. The optical data acquisition depends on whether conditions while SAR data can acquire the data in presence of clouds. This paper uses anisotropic diffusion with PCA for the fusion of SAR (Sentinel 1 (S1)) and Optical (Sentinel 2 (S2)) data for patch-based SVM Classification with LBP (LBP-PSVM). Fusion results with VV polarization performed better than VH polarization using considered fusion method. Classification results suggests that the LBP-PSVM classifier is more effective in comparison to SVM and PSVM classifiers for considered data.","","978-1-7281-3114-6","10.1109/InGARSS48198.2020.9358949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358949","Fusion;SVM;Anisotropic Diffusion;Principal Component Analysis;Local Binary Pattern","Support vector machines;Optical polarization;Optical imaging;Geometrical optics;Optical sensors;Synthetic aperture radar;Principal component analysis","feature extraction;geophysical image processing;image classification;image fusion;principal component analysis;radar imaging;remote sensing by radar;support vector machines;synthetic aperture radar","optical data fusion;anisotropic diffusion;VH polarization;image fusion;complimentary information;better-quality image;spatial features;spectral features;improved classification results;optical data acquisition;SAR data;PCA;VV polarization;fusion method;LBP-PSVM classifier;patch-based SVM classification","","1","","15","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"Comparative Analysis Between Optical and Fused Image with SAR","K. Aslam; R. M. Zahid Khalil; S. u. Haq; S. Ahmed","Dept of RS & GISc, Institute of Space Technology, Karachi, Pakistan; Dept of RS & GISc, Institute of Space Technology, Karachi, Pakistan; Dept of RS & GISc, Institute of Space Technology, Karachi, Pakistan; Dept of Physics, University of Karachi","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","1528","1531","Image fusion is a technique that integrates complementary information from multiple remote sensing images such that the fused image is more suitable for processing task and information extraction. Passive sensors are capable of sensing the reflected electromagnetic energy in the visible and infrared region while active sensors provide additional information using microwave region. This broad spectrum provides more information of earth surface as compared to optical data alone. This study compares the land cover classification results of optical imagery (Landsat-8) and fused imagery (Landsat-8 and Sentinel-1 VV polarized data). The image fusion was then performed using wavelet transformation technique. The data were classified into four classes namely water bodies, built-up area, vegetation cover, and barren land. Google Earth and Landsat imagery were used as a reference image for accuracy assessment. The fused image showed higher accuracy than optical image i.e. Kappa coefficient increased from 0.78 to 0.9 and overall accuracy increased from 89.4% to 92.7%. This study indicates that multi-source information i.e., image fusion can significantly improve the interpretation and accuracy of classification","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324255","Land cover classification;Fusion;Synthetic Aperture Radar","Remote sensing;Earth;Artificial satellites;Optical imaging;Optical sensors;Adaptive optics;Biomedical optical imaging","geophysical image processing;image classification;image fusion;land cover;remote sensing by radar;synthetic aperture radar;vegetation mapping","land cover classification results;optical imagery;Landsat-8;Sentinel-1 VV polarized data;image fusion;reference image;multisource information;comparative analysis;complementary information;multiple remote sensing images;passive sensors;reflected electromagnetic energy;visible region;infrared region;optical data;SAR;wavelet transformation technique;water bodies;built-up area;barren land;vegetation cover;Google Earth","","","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Improving tropical deforestation detection by fusing multiple SAR change measures","X. Dong; S. Quegan; W. Liu; K. Cui; X. Lv","School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; CTCD, University of Sheffield, Sheffield S3 7RH, United Kingdom; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Beijing 100081, China","IET International Radar Conference 2015","21 Apr 2016","2015","","","1","5","The paper studies tropical deforestation detection in Riau province, Indonesia with L-band ALOS PALSAR and Cband ENVISAT ASAR data. Multiple change measures as SAR image intensity, texture, and temporal variations of ScanSAR time series are extracted and employed for deforestation detection. These measures are then combined for improving the detection with subsequent performance evaluation by comparing with the World Wildlife Fund's land cover maps as reference data. When applied on the FBD scene overlaid by both the PALSAR and ASAR data, the detection rate achieves up to 83.2%(at a false alarm rate of 20%) with a significant improvement of over 18% compared with the detection based on the single measure.","","978-1-78561-039-4","10.1049/cp.2015.1309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455531","Change detection;forest monitoring;synthetic aperture radar;tropical deforestation","","forestry;geophysical image processing;geophysical techniques;image fusion;image texture;radar imaging;remote sensing by radar;synthetic aperture radar;time series","tropical deforestation detection;synthetic aperture radar;fusing multiple SAR change measures;Riau province;Indonesia;L-band ALOS PALSAR;C-band ENVISAT ASAR data;SAR image intensity;ScanSAR time series;World Wildlife Fund;land cover maps;FBD scene;false alarm rate","","","","","","21 Apr 2016","","","IET","IET Conferences"
"Fusion of Optical and Millimeter Wave SAR Sensing for Object Recognition in Indoor Environment","A. Batra; T. Hark; J. Schorlemer; N. Pohl; I. Rolfes; M. Wiemeler; D. Göhringer; T. Kaiser; J. Barowski","Institute of Digital Signal Processing, University of Duisburg-Essen, Duisburg, Germany; Institute of Microwave Systems, Ruhr University Bochum, Bochum, Germany; Institute of Microwave Systems, Ruhr University Bochum, Bochum, Germany; Integrated Systems, Ruhr University Bochum, Bochum, Germany; Institute of Microwave Systems, Ruhr University Bochum, Bochum, Germany; Institute of Digital Signal Processing, University of Duisburg-Essen, Duisburg, Germany; Adaptive Dynamic Systems, Technische Universität Dresden, Dresden, Germany; Institute of Digital Signal Processing, University of Duisburg-Essen, Duisburg, Germany; Institute of Microwave Systems, Ruhr University Bochum, Bochum, Germany","2022 Fifth International Workshop on Mobile Terahertz Systems (IWMTS)","21 Jul 2022","2022","","","1","5","Synthetic aperture radar (SAR) sensing at the millimeter-wave (mmWave) spectrum has emerging applications in indoor environments such as high-resolution imaging and localization, material characterization, and object recognition. Although a high-resolution environment map is obtained with SAR at the mmWave spectrum, recognition of objects based on shape is trivial, especially in the case of 2D imaging, where limited shape information is available. Therefore, this paper proposes a method for indoor object recognition in mmWave SAR sensing based on fusion with optical and mmWave SAR image fusion. The recognition includes detection, localization, and classification. Besides, extraction of geometric properties of the objects is also addressed. In this paper, firstly, a 2D multi-object indoor environment mapping is implemented at the spectrum of 68-92 GHz with a frequency modulated continuous wave (FMCW) radar. It replicates a setup of drone-based sensing with side-looking SAR. Two measurement scenarios are considered which primarily differ concerning object positions. The SAR image is reconstructed using a time-domain backprojection algorithm including a calibration procedure for generating a correct imaging plane. Finally, based on the presented method of SAR image fusion with the optical image at visible spectrum obtained using a CMOS camera, object recognition is evaluated.","","978-1-6654-8275-2","10.1109/IWMTS54901.2022.9832438","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832438","mmWave SAR;radar imaging;object detection;object classification;indoor mapping","Shape;Radar imaging;Millimeter wave radar;Optical imaging;Radar polarimetry;Adaptive optics;Indoor environment","CW radar;FM radar;image fusion;image reconstruction;image resolution;millimetre wave radar;object detection;object recognition;radar detection;radar imaging;radar receivers;radar resolution;shape recognition;synthetic aperture radar","millimeter wave SAR sensing;synthetic aperture radar sensing;millimeter-wave spectrum;high-resolution environment map;mmWave spectrum;indoor object recognition;mmWave SAR sensing;optical SAR image fusion;mmWave SAR image fusion;2D multiobject indoor environment mapping;frequency modulated continuous wave radar;object positions;imaging plane;optical image;FMCW radar;drone-based sensing;side-looking SAR;time-domain backprojection algorithm;frequency 68.0 GHz to 92.0 GHz","","","","9","IEEE","21 Jul 2022","","","IEEE","IEEE Conferences"
"A refined automatic co-registration method for high-resolution optical and sar images by maximizing mutual information","F. Saidi; J. Chen; P. Wang","School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China; School of Electronics and Information Engineering, Beihang University, Beijing, China","2016 IEEE International Conference on Signal and Image Processing (ICSIP)","30 Mar 2017","2016","","","231","235","The use of multisensors images for different applications, like change detection and image fusion, require an image to image registration. Actually, the image registration becomes a crucial task with the continuous increase in image resolution, especially in urban area. In this paper, a refined automatic Mutual Information based registration approach of spaceborne Synthetic Aperture radar (SAR) and optical image is proposed. The data set include TerraSar-X (1m) and Worldview-2 (0.5m) images and cover urban area of San Francisco. Our approach is divided into two steps. First, we use Discrete Canny edge detector to extract contours from the optical image, and Gabor-Wavelet filter to extract edge from the SAR image. Second, a rigid transformation (rotation and translation) is applied to the contours image obtained by Canny detector. Then, for each value of transformation, the Mutual Information is computed between this transformed image and the feature image outcome of application of Gabor-wavelet to the SAR image. The best transformation parameters are obtained when the Mutual Information is maximal. Also, in this work, we compared the proposed approach with the basic Mutual Information intensity based registration method. The results obtained demonstrated that our approach improve the registration accuracy comparatively to the basic Mutual Information intensity based registration method.","","978-1-5090-2377-6","10.1109/SIPROCESS.2016.7888258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888258","Image registration;mutual information;Gabor-wavelet;SAR image;canny edge detector","Optical filters;Optical imaging;Adaptive optics;Image registration;Feature extraction;Optical sensors;Synthetic aperture radar","edge detection;Gabor filters;image registration;radar imaging;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar;wavelet transforms","refined automatic coregistration method;high-resolution optical images;SAR images;mutual information;multisensors images;image registration;image resolution;urban area;refined automatic Mutual Information;spaceborne Synthetic Aperture radar;TerraSar-X (1m) images;Worldview-2 (0.5m) images;San Francisco;Discrete Canny edge detector;Gabor-Wavelet filter;Gabor-wavelet","","3","","26","IEEE","30 Mar 2017","","","IEEE","IEEE Conferences"
"Hybrid Transformer Networks for Urban Land Use Classification from Optical and SAR Images","R. Liu; H. Zhang; J. Ling","The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China; The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China; The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","707","710","Mapping the land cover/use type of urban area surface plays a vital role in many remote sensing applications. The performance of classification is inevitably limited by the finite amount of information available from a single data source, the restricted atmosphere condition and the complex landscape of the urban areas. Even when multiple sources of data are used, the fusion strategy is relatively homogeneous. In this paper, we aim to explore the potential of transformer based fusion method in mapping the urban regions with optical and synthetic aperture radar images. Specifically, we propose a hybrid fusion transformer network that simultaneously implements multi-source data fusion at both the feature and the decision levels. The experiments are conducted on the high resolution multiple remote sensing images, and the results show that the hybrid fusion based on transformer can achieve 82.17% in overall accuracy (OA) and 76.91 % in kappa coefficient. Moreover, compared with convolution neural network based methods, the transformer based methods are on average 2% higher in OA and 3.6% higher in kappa coefficient.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883122","urban land cover/use classification;trans-former;image fusion","Soft sensors;Urban areas;Optical computing;Transformers;Optical imaging;Adaptive optics;Radar polarimetry","geophysical image processing;image classification;image fusion;image resolution;neural nets;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","hybrid transformer networks;urban land use classification;urban area surface;remote sensing applications;single data source;restricted atmosphere condition;complex landscape;urban areas;fusion strategy;transformer based fusion method;urban regions;optical aperture radar images;synthetic aperture radar images;hybrid fusion transformer network;multisource data fusion;high resolution multiple remote sensing images;convolution neural network;transformer based methods","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Land Cover Classification of Huixian Wetland Based on SAR and Optical Image Fusion","J. Xiao; Y. Xiao; X. Sun; J. Huang; H. Wang","Lijiang School, Guilin Normal University, Guilin, China; Guanxi Key Laboratory of precision Navigation Technology and Application, Guilin University of Electronic Technology, Guilin, China; Guanxi Key Laboratory of precision Navigation Technology and Application, Guilin University of Electronic Technology, Guilin, China; Guanxi Key Laboratory of precision Navigation Technology and Application, Guilin University of Electronic Technology, Guilin, China; Guanxi Key Laboratory of precision Navigation Technology and Application, Guilin University of Electronic Technology, Guilin, China","2020 IEEE 3rd International Conference on Information Communication and Signal Processing (ICICSP)","20 Oct 2020","2020","","","316","320","In this paper, GF-1 WVF image and Sentinel-1 SAR image covering Huixian wetland area are used as data sources. The Gram Schmidt (GS) algorithm is first used to fuse GF-1 images and SAR images with different polarization modes, and then the Random Forest (RF) algorithm is used for supervised classification. Finally, the accuracy of classification results and the ability to extract information are compared. The experimental results show that the fusion image has obvious texture features and prominent karst landform features, compared with the GF-1 WVF image. Compared with the Sentinel-1 SAR image, the fusion image has obvious spectral features. Spectral differences between typical features are large; The overall classification accuracy of GF-1 images, GF-1 and Sentinel-1 VV polarization fusion images, and GF-1 and Sentinel-1 VH polarization fusion images have reached over 80%. The classification accuracy of GF-1 and Sentinel-1 VV polarization fusion images reaches 85.15%, which is better than GF-1 and Sentinel-1 VH polarization fusion images. The classification accuracy of water bodies in the VV polarization fusion image is better than that of GF-1. Bare ground has the highest classification accuracy among all fused images.","","978-1-7281-8823-2","10.1109/ICICSP50920.2020.9232103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9232103","GF-1 image;sar image;image fusion;random forest;huixian wetland","Wetlands;Radar polarimetry;Classification algorithms;Feature extraction;Radio frequency;Optical polarization;Optical imaging","feature extraction;geophysical image processing;image classification;image fusion;image texture;land cover;optical images;radar imaging;remote sensing by radar;synthetic aperture radar","land cover classification;optical image;GF-1 WVF image;Sentinel-1 SAR image;GF-1 images;Sentinel-1 VV polarization fusion images;Sentinel-1 VH polarization fusion images;VV polarization fusion image","","1","","16","IEEE","20 Oct 2020","","","IEEE","IEEE Conferences"
"Fusion of RISAT-1 SAR Image and Resourcesat-2 Multispectral Images Using Wavelet Transform","S. C. Kulkarni; P. P. Rege","Dept. of Electronics and Telecommunication Engineering, College of Engineering, Pune, Pune, India; Dept. of Electronics and Telecommunication Engineering, College of Engineering, Pune, Pune, India","2019 6th International Conference on Signal Processing and Integrated Networks (SPIN)","13 May 2019","2019","","","45","52","This paper presents a pixel level wavelet-based approach to fuse synthetic aperture radar (SAR) imagery with multispectral (MS) imagery. Image fusion combines information from two or more images to generate a new image, which is rich in information. Due to complementary nature of SAR and multispectral imagery, fusion of these images is of significant interest in the field of remote sensing. The primary objective of this work is to enhance spatial information in multispectral images by injecting structural information derived from SAR image. Due to negative correlation between SAR and multispectral data, conventional component substitution methods face the problem of spectral distortion in the fused image. Wavelet based fusion approaches overcome this problem due to excellent localization in spatial and frequency domain. Here, different wavelet-based fusion rules are applied for fusion of SAR and multispectral images. Fusion rules applied to fuse approximate sub-bands and detail sub-bands of these images consider spectral dis-similarity between them. Results are evaluated visually, as well as using standard quality metrics and are compared with component substitution fusion techniques namely, principal component analysis and generalized IHS transform. Trade-off between spectral and spatial quality of fused image has been observed while fusing SAR and multispectral images.","","978-1-7281-1380-7","10.1109/SPIN.2019.8711589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8711589","Image Fusion;Remote Sensing;SAR Imagery;Multispectral Imagery;Wavelet Transform","Synthetic aperture radar;Wavelet transforms;Radar polarimetry;Spatial resolution;Remote sensing","geophysical image processing;image fusion;image resolution;principal component analysis;radar imaging;remote sensing by radar;sensor fusion;synthetic aperture radar;wavelet transforms","RISAT-1 SAR image;wavelet transform;pixel level wavelet-based approach;synthetic aperture radar imagery;multispectral imagery;image fusion;spatial information;multispectral images;multispectral data;fused image;based fusion approaches;different wavelet-based fusion rules;component substitution fusion techniques;resourcesat-2 multispectral images;remote sensing","","1","","22","IEEE","13 May 2019","","","IEEE","IEEE Conferences"
"Registration of High Resolution Sar and Optical Satellite Imagery Using Fully Convolutional Networks","S. Hoffmann; C. -A. Brust; M. Shadaydeh; J. Denzler","Computer Vision Group, Friedrich Schiller University Jena, Jena, Germany; Computer Vision Group, Friedrich Schiller University Jena, Jena, Germany; Computer Vision Group, Friedrich Schiller University Jena, Jena, Germany; Computer Vision Group, Friedrich Schiller University Jena, Jena, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","5152","5155","Multi-modal image registration is a crucial step when fusing images which show different physical/chemical properties of an object. Depending on the compared modalities and the used registration metric, this process exhibits varying reliability. We propose a deep metric based on a fully convo-lutional neural network (FCN). It is trained from scratch on SAR-optical image pairs to predict whether certain image areas are aligned or not. Tests on the affine registration of SAR and optical images showing suburban areas verify an enormous improvement of the registration accuracy in comparison to registration metrics that are based on mutual information (MI).","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898714","Remote Sensing;SAR-optical Image Registration;Fully Convolutional Network","Measurement;Optical imaging;Optical sensors;Training;Synthetic aperture radar;Image registration;Remote sensing","convolutional neural nets;geophysical image processing;image fusion;image registration;learning (artificial intelligence);optical images;synthetic aperture radar","optical satellite imagery;fully convolutional networks;multimodal image registration;image fusion;compared modalities;registration metric;convolutional neural network;SAR-optical image pairs;image areas;affine registration;optical images;registration accuracy","","8","","17","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Fusion of very high resolution SAR and optical images for the monitoring of urban areas","C. V. Lopez; H. Anglberger; U. Stilla","Microwaves and Radar Institute, German Aerospace Center (DLR), Germany; Microwaves and Radar Institute, German Aerospace Center (DLR), Germany; Photogrammetry and Remote Sensing, Technical University of Munich (TUM), Munich, Germany","2017 Joint Urban Remote Sensing Event (JURSE)","11 May 2017","2017","","","1","4","Remote sensing data from SAR and optical sensors provide complementary information, with each type of data being better suited for certain tasks. In this paper, the potential of using both types of data together for the monitoring of urban areas will be shown. This is illustrated using a dataset of very high resolution SAR and optical images of the city of Oslo (Norway), containing over three years of TerraSAR-X images acquired with different orbits and incidence angles, and aerial optical images that were photogrammetrically processed to obtain a DSM and a true ortho mosaic.","","978-1-5090-5808-2","10.1109/JURSE.2017.7924551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924551","","Optical sensors;Optical imaging;Synthetic aperture radar;Urban areas;Buildings;Monitoring;Time series analysis","digital elevation models;geophysical image processing;image fusion;image resolution;image segmentation;photogrammetry;remote sensing by radar;synthetic aperture radar;terrain mapping","very high resolution SAR-optical image fusion;urban area monitoring;remote sensing data;optical sensors;Oslo city;Norway;TerraSAR-X images;incidence angles;aerial optical images;DSM","","1","","12","IEEE","11 May 2017","","","IEEE","IEEE Conferences"
"An Efficient Backprojection Algorithm Based on Spectral Splicing","H. Sun; Z. Sun; Y. Miao; T. Chen; J. Wu; J. Yang","University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China; University of Electronic Science and Technology of China, Chengdu, China","2021 CIE International Conference on Radar (Radar)","8 Feb 2023","2021","","","808","811","This paper introduces a fast back projection synthetic aperture radar imaging algorithm based on wavenumber spectrum splicing. The traditional algorithm establishes the polar coordinate system with the center of the sub aperture as the origin. It needs to constantly re-establish a new unified polar coordinate system for projection and fusion, which requires a large amount of calculation. In order to deal with this problem, this paper first proposes a novel fusion method, which splices sub-aperture images in the wavenumber domain, and then completes the image fusion, which accelerates the imaging speed. Finally, simulations are presented to demonstrate the efficiency of the algorithm.","2640-7736","978-1-6654-9814-2","10.1109/Radar53847.2021.10027989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10027989","synthetic aperture radar;fast back projection;polar coordinates;wavenumber","Splicing;Imaging;Radar imaging;Apertures;Radar polarimetry;Synthetic aperture radar;Image fusion","","","","","","10","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"Noisy Remote Sensing Image Fusion Based on JSR","X. Ma; S. Hu; S. Liu; J. Wang; S. Xu","Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; Beijing Key Laboratory of Advanced Information Science and Network Technology, Beijing Jiaotong University, Beijing, China; Machine Vision Engineering Research Center of Hebei Province, Hebei University, Baoding, China; Machine Vision Engineering Research Center of Hebei Province, Hebei University, Baoding, China; Research Institute of TV and Electro-Acoustics, Beijing, China","IEEE Access","19 Feb 2020","2020","8","","31069","31082","Compressed sensing has shown great potential and power in image representation, especially in image reconstruction by sparse representation. Due to complementary information and unavoidable noise existing in synthetic aperture radar (SAR) and other source images, joint sparse representation (JSR) is developed to separate redundancy and complementary information with different properties in source images and obtain a fused image, where image de-noising is done simultaneously owing to that noise is not sparse and cannot be represented by sparse representation. As a result, one noisy remote sensing image fusion method based on JSR is presented in this paper. After obtaining redundant and complementary sub-images by JSR, an improved fusion rule based on pulse coupled neural network (PCNN) is employed to fuse complementary sparse coefficients together. At the same time, because the types of noise in SAR and other source images are different, they can be treated as the complementary information in source images and suppressed at this step. Finally, a fused image can be reconstructed by adding the redundant and fused complementary sub-images. Quantitative and qualitative experimental results show that the proposed method outperforms most of other fusion methods and it is more robust to noise, having better visual effects and values of objective evaluation metrics.","2169-3536","","10.1109/ACCESS.2020.2973435","National Natural Science Foundation of China(grant numbers:61572063,61401308); Natural Science Foundation of Hebei Province(grant numbers:F2016201142,F2019201151,F2018210148); Natural Science Foundation of Hebei Province(grant numbers:QN2016085,QN2017306); Opening Foundation of Machine vision Engineering Research Center of Hebei Province(grant numbers:2018HBMV01,2018HBMV02); Hebei University(grant numbers:2014-303,8012605); Hebei University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995486","Remote sensing image fusion;joint sparse representation;pulse coupled neural network;SAR image processing","Image fusion;Dictionaries;Noise measurement;Transforms;Remote sensing;Image reconstruction;Synthetic aperture radar","image denoising;image fusion;image reconstruction;image representation;neural nets;remote sensing","JSR;image representation;image reconstruction;complementary information;source images;joint sparse representation;fused image;image de-noising;noisy remote sensing image fusion method;complementary sub-images;complementary sparse coefficients;pulse coupled neural network;PCNN","","1","","52","CCBY","12 Feb 2020","","","IEEE","IEEE Journals"
"Object based fusion of polarimetric SAR and hyperspectral imaging for land use classification","J. Hu; P. Ghamisi; A. Schmitt; X. X. Zhu","Remote Sensing Technology Institute, DLR German Aerospace Center, Wessling, Germany; Remote Sensing Technology Institute, DLR German Aerospace Center, Wessling, Germany; Remote Sensing Data Center, DLR German Aerospace Center, Wessling, Germany; Signal Processing in Earth Observation (SIPEO), Technische Universität München (TUM), Germany","2016 8th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","19 Oct 2017","2016","","","1","5","In this paper, we propose an object-based fusion approach for the joint use of polarimetric synthetic aperture radar (PolSAR) and hyperspectral data. The proposed approach extracts information from both datasets based on an object-level, which is used here for land use classification. The achieved classification result infers that the proposed methodology improves the classification performance of both hyperspectral and PolSAR data and can properly gather complementary information of the two kinds of dataset. The fusion approach also considers that only limited training samples are available, which is often the case in remote sensing.","2158-6276","978-1-5090-0608-3","10.1109/WHISPERS.2016.8071752","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8071752","PolSAR;hyperspectral image;fusion;classification","Hyperspectral imaging;Support vector machines;Training;Buildings;Image segmentation","geophysical image processing;image classification;image fusion;radar imaging;radar polarimetry;remote sensing;remote sensing by radar;synthetic aperture radar","polarimetric synthetic aperture radar;limited training samples;remote sensing;object-based fusion approach;hyperspectral imaging;polarimetric SAR;object based fusion;complementary information;hyperspectral PolSAR data;classification performance;classification result infers;land use classification;object-level;hyperspectral data","","3","","8","IEEE","19 Oct 2017","","","IEEE","IEEE Conferences"
"Fusing Deep Learning and Sparse Coding for SAR ATR","O. Kechagias-Stamatis; N. Aouf","Cranfield University, Defence Academy of the U.K., Shrivenham, U.K.; Cranfield University, Defence Academy of the U.K., Shrivenham, U.K.","IEEE Transactions on Aerospace and Electronic Systems","11 Apr 2019","2019","55","2","785","797","We propose a multimodal and multidiscipline data fusion strategy appropriate for automatic target recognition (ATR) on synthetic aperture radar imagery. Our architecture fuses a proposed clustered version of the AlexNet convolutional neural network with sparse coding theory that is extended to facilitate an adaptive elastic net optimization concept. Evaluation on the MSTAR dataset yields the highest ATR performance reported yet, which is 99.33% and 99.86% for the three- and ten-class problems, respectively.","1557-9603","","10.1109/TAES.2018.2864809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8432096","Automatic target recognition (ATR);convolutional neural networks (CNNs);data fusion;sparse coding (SC);synthetic aperture radar (SAR)","Synthetic aperture radar;Optimization;Training;Dictionaries;Fuses;Two dimensional displays;Encoding","convolutional neural nets;image fusion;image recognition;learning (artificial intelligence);network coding;object recognition;optimisation;pattern clustering;radar computing;radar imaging;synthetic aperture radar","automatic target recognition;synthetic aperture radar imagery;AlexNet convolutional neural network;sparse coding theory;adaptive elastic net optimization concept;SAR ATR;multimodal data fusion strategy;multidiscipline data fusion strategy;deep learning fusion;MSTAR dataset evaluation","","56","","64","IEEE","10 Aug 2018","","","IEEE","IEEE Journals"
"Spatial Resolution Improvement in GNSS-Based SAR Using Multistatic Acquisitions and Feature Extraction","F. Santi; M. Bucciarelli; D. Pastina; M. Antoniou; M. Cherniakov","Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Information Engineering, Electronics and Telecommunications, Sapienza University of Rome, Rome, Italy; Department of Electronic, Electrical and Computer Engineering, University of Birmingham, Birmingham, U.K.; Department of Electronic, Electrical and Computer Engineering, University of Birmingham, Birmingham, U.K.","IEEE Transactions on Geoscience and Remote Sensing","11 Aug 2016","2016","54","10","6217","6231","This paper considers the exploitation of navigation satellite systems as opportunity transmitters for bistatic and multistatic synthetic aperture radar (SAR). The simultaneous availability of multiple satellites over a scene of interest at different viewing angles allows multistatic SAR acquisitions using a single receiver on or near the ground. The resulting spatial diversity could be used to drastically improve image resolution or to enhance image information space. To exploit the availability of multiple satellites, two data fusion approaches are here considered. In the former, point features of the single images obtained from different perspectives are extracted and then combined, whereas in the latter, a multistatic image is first obtained by combining the single channel data at the image level and then the point features are extracted. This is achieved by considering ad hoc CLEAN-like techniques. These techniques have been developed on both the analytical and simulation levels and experimentally verified with real GNSS-based SAR imagery. The techniques described here are not limited to GNSS-based SAR but may be applied to any multistatic SAR system.","1558-0644","","10.1109/TGRS.2016.2583784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515203","Bistatic synthetic aperture radar (BSAR);CLEAN;feature extraction;global navigation satellite system (GNSS)-based SAR;multistatic SAR (MSAR);passive SAR","Synthetic aperture radar;Satellites;Feature extraction;Receivers;Spatial resolution;Transmitters","feature extraction;image enhancement;image fusion;image resolution;remote sensing by radar;satellite navigation;synthetic aperture radar","multistatic SAR system;GNSS-based SAR imagery;ad hoc CLEAN-like technique;image level;single channel data;multistatic image;data fusion approach;image information space;image resolution;spatial diversity;single receiver;multistatic SAR acquisition;bistatic SAR;navigation satellite system;feature extraction;multistatic acquisition;synthetic aperture radar;spatial resolution improvement","","53","","35","IEEE","18 Jul 2016","","","IEEE","IEEE Journals"
"Saliency-Guided Single Shot Multibox Detector for Target Detection in SAR Images","L. Du; L. Li; D. Wei; J. Mao","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2020","2020","58","5","3366","3376","The single shot multibox detector (SSD), a proposal-free method based on convolutional neural network (CNN), has recently been proposed for target detection and has found applications in synthetic aperture radar (SAR) images. Moreover, the saliency information reflected in the saliency map can highlight the target of interest while suppressing clutter, which is beneficial for better scene understanding. Therefore, in this article, we propose a saliency-guided SSD (S-SSD) for target detection in SAR images, in which we effectively integrate the saliency into the SSD network not only to suggest where to focus on but also to improve the representation capability in complex scenes. The proposed S-SSD contains two separated convolutional backbone subnetwork architectures, one with the original SAR image as input to extract features, and the other with the corresponding saliency map obtained from the modified Itti's method as input to acquire refined saliency information under supervision. In addition, the dense connection structure, instead of the plain structure used in original SSD, is applied in the two convolutional backbone architectures to utilize multiscale information with fewer parameters. Then, for integrating saliency information to guide the network to emphasize informative regions, multilevel fusion modules are utilized to merge the two streams into a unified framework, thereby making the whole network end-to-end jointly trained. Finally, the convolutional predictors are used to predict targets. The experimental results on the miniSAR real data demonstrate that the proposed S-SSD can achieve better detection performance than state-of-the-art methods.","1558-0644","","10.1109/TGRS.2019.2953936","National Science Foundation of China(grant numbers:61771362,U1833203,61671354); Higher Education Discipline Innovation Project(grant numbers:B18039); Shaanxi Innovation Team Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930938","Convolutional neural network (CNN);deep learning;saliency detection;single shot multibox detector (SSD);synthetic aperture radar (SAR);target detection","Object detection;Feature extraction;Radar polarimetry;Clutter;Synthetic aperture radar;Detectors;Convolution","convolutional neural nets;feature extraction;image fusion;image representation;interference suppression;neural net architecture;object detection;radar clutter;radar computing;radar detection;radar imaging;synthetic aperture radar","multilevel fusion modules;plain structure;dense connection structure;feature extraction;representation capability;clutter suppression;convolutional backbone subnetwork architectures;detection performance;convolutional predictors;informative regions;multiscale information;original SSD;refined saliency information;modified Itti method;original SAR image;complex scenes;SSD network;S-SSD;saliency-guided SSD;scene understanding;synthetic aperture radar images;convolutional neural network;SAR images;target detection;saliency-guided single shot multibox detector","","48","","27","IEEE","11 Dec 2019","","","IEEE","IEEE Journals"
"Visual Saliency Modeling for River Detection in High-Resolution SAR Imagery","F. Gao; F. Ma; J. Wang; J. Sun; E. Yang; H. Zhou","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Space Mechatronic Systems Technology Laboratory, Manufacture and Engineering Management, Strathclyde Space Institute, University of Strathclyde, Glasgow, U.K.; School of Electronic, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.","IEEE Access","14 Feb 2018","2018","6","","1000","1014","Accurate detection of rivers plays a significant role in water conservancy construction and ecological protection, where airborne synthetic aperture radar (SAR) data have already become one of the main sources. However, extracting river information from radar data efficiently and accurately still remains an open problem. The existing methods for detecting rivers are typically based on rivers’ edges, which are easily mixed with those of artificial buildings or farmland. In addition, pixel-based image processing approaches cannot meet the requirement of real-time processing. Inspired by the feature integration and target recognition capabilities of biological vision systems, in this paper, we present a hierarchical method for automated detection of river networks in the high-resolution SAR data using biologically visual saliency modeling. For effective saliency detection, the original image is first over-segmented into a set of primitive superpixels. A visual feature set is designed to extract a regional feature histogram, which is then quantized based on the optimal parameters learned from the labeled SAR images. Afterward, three saliency measurements based on the specificity of the rivers in the SAR images are proposed to generate a single layer saliency map, i.e., local region contrast, boundary connectivity, and edge density. Finally, by exploiting belief propagation, we propose a multi-layer saliency fusion approach to derive a high-quality saliency map. Extensive experimental results on three airborne SAR image data sets with the ground truth demonstrate that the proposed saliency model consistently outperforms the existing saliency target detection models.","2169-3536","","10.1109/ACCESS.2017.2777444","National Natural Science Foundation of China(grant numbers:61771027,61071139,61471019,61501011,61171122); Aeronautical Science Foundation of China(grant numbers:20142051022); Pre-Research Project(grant numbers:9140A07040515HK01009); RSE-NNSFC Joint Project (2017–2019) with the China University of Petroleum (Huadong)(grant numbers:6161101383); U.K. EPSRC(grant numbers:EP/N508664/1,EP/R007187/1,EP/N011074/1); Royal Society-Newton Advanced Fellowship(grant numbers:NA160342); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119784","Synthetic aperture radar (SAR);remote sensing;rivers;object detection;biological system modeling","Feature extraction;Synthetic aperture radar;Rivers;Merging;Visualization;Filtering algorithms;Dogs","airborne radar;feature extraction;geophysical image processing;geophysics computing;hydrological techniques;image colour analysis;image fusion;image resolution;image segmentation;object detection;radar imaging;rivers;synthetic aperture radar","river networks;biologically visual saliency modeling;visual feature set;regional feature histogram;labeled SAR images;saliency measurements;single layer saliency map;local region contrast;edge density;multilayer saliency fusion approach;high-quality saliency map;airborne SAR image data sets;river detection;high-resolution SAR imagery;water conservancy construction;ecological protection;airborne synthetic aperture radar data;pixel-based image processing approaches;biological vision systems;hierarchical method;automated detection;saliency detection;saliency target detection models;belief propagation","","47","","54","OAPA","24 Nov 2017","","","IEEE","IEEE Journals"
"MIMA: MAPPER-Induced Manifold Alignment for Semi-Supervised Fusion of Optical Image and Polarimetric SAR Data","J. Hu; D. Hong; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","30 Oct 2019","2019","57","11","9025","9040","Multi-modal data fusion has recently been shown promise in classification tasks in remote sensing. Optical data and radar data, two important yet intrinsically different data sources, are attracting more and more attention for potential data fusion. It is already widely known that a machine learning-based methodology often yields excellent performance. However, the methodology relies on a large training set, which is very expensive to achieve in remote sensing. The semi-supervised manifold alignment (SSMA), a multi-modal data fusion algorithm, has been designed to amplify the impact of an existing training set by linking labeled data to unlabeled data via unsupervised techniques. In this paper, we explore the potential of SSMA in fusing optical data and polarimetric synthetic aperture radar (SAR) data, which are multi-sensory data sources. Furthermore, we propose a MAPPER-induced manifold alignment (MIMA) for the semi-supervised fusion of multi-sensory data sources. Our proposed method unites SSMA with MAPPER, which is developed from the emerging topological data analysis (TDA) field. To the best of our knowledge, this is the first time that SSMA has been applied on fusing optical data and SAR data, and also the first time that TDA has been applied in remote sensing. The conventional SSMA derives a topological structure using k-nearest neighbor (kNN), while MIMA employs MAPPER, which considers the field knowledge and derives a novel topological structure through the spectral clustering in a data-driven fashion. The experimental results on data fusion with respect to land cover land use classification and local climate zone classification suggest superior performance of MIMA.","1558-0644","","10.1109/TGRS.2019.2924113","Deutsche Forschungsgemeinschaft(grant numbers:ZH 498/7-2); H2020 European Research Council(grant numbers:ERC-2016-StG-714087 (So2Sat)); Helmholtz-Gemeinschaft(grant numbers:VH-NG-1018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802291","Hyperspectral image;MAPPER;multi-modal data fusion;multi-sensory data fusion;multispectral image;polarimetric synthetic aperture radar (PolSAR);semi-supervised manifold alignment (SSMA);topological data analysis (TDA)","Optical imaging;Optical sensors;Synthetic aperture radar;Remote sensing;Optical distortion;Manifolds;Optical scattering","data analysis;geophysical image processing;image classification;image fusion;nearest neighbour methods;pattern clustering;radar imaging;radar polarimetry;remote sensing;synthetic aperture radar","k-nearest neighbor;unsupervised techniques;data-driven fashion;emerging topological data analysis field;multisensory data sources;polarimetric synthetic aperture radar data;unlabeled data;multimodal data fusion algorithm;SSMA;semisupervised manifold alignment;machine learning-based methodology;potential data fusion;optical data;remote sensing;polarimetric SAR data;semisupervised fusion;MAPPER-induced manifold alignment;MIMA","","43","","70","CCBY","15 Aug 2019","","","IEEE","IEEE Journals"
"Convolutional Neural Networks for Multimodal Remote Sensing Data Classification","X. Wu; D. Hong; J. Chanussot","School of Information and Electronics and the Beijing Key Laboratory of Fractional Signals and Systems, Beijing Institute of Technology, Beijing, China; Key Laboratory of Digital Earth Science, Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","14 Feb 2022","2022","60","","1","10","In recent years, enormous research has been made to improve the classification performance of single-modal remote sensing (RS) data. However, with the ever-growing availability of RS data acquired from satellite or airborne platforms, simultaneous processing and analysis of multimodal RS data pose a new challenge to researchers in the RS community. To this end, we propose a deep-learning-based new framework for multimodal RS data classification, where convolutional neural networks (CNNs) are taken as a backbone with an advanced cross-channel reconstruction module, called CCR-Net. As the name suggests, CCR-Net learns more compact fusion representations of different RS data sources by the means of the reconstruction strategy across modalities that can mutually exchange information in a more effective way. Extensive experiments conducted on two multimodal RS datasets, including hyperspectral (HS) and light detection and ranging (LiDAR) data, i.e., the Houston2013 dataset, and HS and synthetic aperture radar (SAR) data, i.e., the Berlin dataset, demonstrate the effectiveness and superiority of the proposed CCR-Net in comparison with several state-of-the-art multimodal RS data classification methods. The codes will be openly and freely available at https://github.com/danfenghong/IEEE_TGRS_CCR-Net for the sake of reproducibility.","1558-0644","","10.1109/TGRS.2021.3124913","National Natural Science Foundation of China(grant numbers:62101045); China Postdoctoral Science Foundation(grant numbers:2021M690385); MIAI@Grenoble Alpes(grant numbers:ANR-19-P3IA-0003); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9598903","Classification;convolutional neural networks (CNNs);cross-channel;hyperspectral (HS);light detection and ranging (LiDAR);multimodal;reconstruction;remote sensing (RS);synthetic aperture radar (SAR)","Feature extraction;Laser radar;Synthetic aperture radar;Task analysis;Convolutional neural networks;Hyperspectral imaging;Network architecture","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;hyperspectral imaging;image classification;image fusion;image reconstruction;image representation;optical radar;radar imaging;remote sensing;synthetic aperture radar","convolutional neural networks;airborne platforms;advanced cross-channel reconstruction module;CCR-Net learns more compact fusion representations;remote sensing data sources;multimodal remote sensing datasets;light detection and ranging data;synthetic aperture radar data;multimodal remote sensing data classification methods;single-modal remote sensing data;CNN;hyperspectral data;Houston2013 dataset;fusion representations;Berlin dataset","","39","","48","IEEE","2 Nov 2021","","","IEEE","IEEE Journals"
"HOG-ShipCLSNet: A Novel Deep Learning Network With HOG Feature Fusion for SAR Ship Classification","T. Zhang; X. Zhang; X. Ke; C. Liu; X. Xu; X. Zhan; C. Wang; I. Ahmad; Y. Zhou; D. Pan; J. Li; H. Su; J. Shi; S. Wei","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; College of Information Science and Technology, Dalian Maritime University, Dalian, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan University, Wuhan, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; 3rd Graduate Student Team, Naval Aviation University, Yantai, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jan 2022","2022","60","","1","22","Ship classification in synthetic aperture radar (SAR) images is a fundamental and significant step in ocean surveillance. Recently, with the rise of deep learning (DL), modern abstract features from convolutional neural networks (CNNs) have hugely improved SAR ship classification accuracy. However, most existing CNN-based SAR ship classifiers overly rely on abstract features, but uncritically abandon traditional mature hand-crafted features, which may incur some challenges for further improving accuracy. Hence, this article proposes a novel DL network with histogram of oriented gradient (HOG) feature fusion (HOG-ShipCLSNet) for preferable SAR ship classification. In HOG-ShipCLSNet, four mechanisms are proposed to ensure superior classification accuracy, that is, 1) a multiscale classification mechanism (MS-CLS-Mechanism); 2) a global self-attention mechanism (GS-ATT-Mechanism); 3) a fully connected balance mechanism (FC-BAL-Mechanism); and 4) an HOG feature fusion mechanism (HOG-FF-Mechanism). We perform sufficient ablation studies to confirm the effectiveness of these four mechanisms. Finally, our experimental results on two open SAR ship datasets (OpenSARShip and FUSAR-Ship) jointly reveal that HOG-ShipCLSNet dramatically outperforms both modern CNN-based methods and traditional hand-crafted feature methods.","1558-0644","","10.1109/TGRS.2021.3082759","National Natural Science Foundation of China(grant numbers:61571099,61501098,61671113); National Key Research and Development Program of China(grant numbers:2017YFB0502700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9445223","Convolutional neural networks (CNNs);deep learning (DL);feature fusion;histogram of oriented gradient (HOG);ship classification;synthetic aperture radar (SAR)","Marine vehicles;Synthetic aperture radar;Feature extraction;Radar polarimetry;Artificial intelligence;Support vector machines;Oceans","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image classification;image fusion;object detection;radar computing;radar imaging;ships;synthetic aperture radar","novel DL network;histogram of oriented gradient feature fusion;HOG-FF-Mechanism;HOG feature fusion mechanism;FC-BAL-Mechanism;GS-ATT-Mechanism;MS-CLS-Mechanism;multiscale classification mechanism;preferable SAR ship classification;traditional mature hand-crafted features;abstract features;existing CNN-based SAR ship classifiers;SAR ship classification accuracy;convolutional neural networks;modern features;ocean surveillance;synthetic aperture radar images;novel deep learning network;HOG-ShipCLSNet","","35","","132","IEEE","2 Jun 2021","","","IEEE","IEEE Journals"
"Fusion Similarity-Based Reranking for SAR Image Retrieval","X. Tang; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, International Research Center for Intelligent Perception and Computation, Joint International Research Laboratory of Intelligent Perception and Computation, Xidian University, Xi’an, China","IEEE Geoscience and Remote Sensing Letters","19 May 2017","2017","14","2","242","246","A new reranking method, fusion similarity-based reranking, is proposed in this letter to improve the performance of synthetic aperture radar (SAR) image retrieval. First, the top ranked SAR images within the initial retrieval results are picked for reranking. Considering the negative influence of the speckle noise, three SAR-oriented visual features are selected to represent them. In addition, the different relevance scores corresponding to an SAR image are estimated in various modalities (i.e., different feature spaces). Second, a fusion similarity is defined under the relevance score space to measure the resemblance between two SAR images. This fusion similarity is calculated using the modal-image matrix, which is construed by the estimated scores to integrate the contributions of all modalities. Finally, an existing reranking function is adopted to rerank the SAR images with the help of the estimated scores and calculated fusion similarities. The positive experimental results demonstrate that our reranking method is effective and efficient.","1558-0571","","10.1109/LGRS.2016.2636819","National Basic Research Program (973 Program) of China(grant numbers:2013CB329402); National Natural Science Foundation of China(grant numbers:61573267,61473215,61571342,61572383,61501353,61502369,61271302,61272282,61202176); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B07048); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103); Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT 15R53); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801895","Fusion similarity;reranking;synthetic aperture radar (SAR) image retrieval","Synthetic aperture radar;Feature extraction;Image retrieval;Visualization;Speckle;Estimation;Semantics","image fusion;image retrieval;matrix algebra;radar imaging;synthetic aperture radar","modal-image matrix;relevance score space;SAR-oriented visual features;speckle noise;SAR image retrieval;synthetic aperture radar image retrieval;fusion similarity-based reranking;reranking method","","31","","18","IEEE","29 Dec 2016","","","IEEE","IEEE Journals"
"Object-Based Analysis and Fusion of Optical and SAR Satellite Data for Dwelling Detection in Refugee Camps","K. Spröhnle; E. -M. Fuchs; P. Aravena Pelizari","German Aerospace Center (DLR), German Remote Sensing Data Center (DFD), 82234 Oberpfaffenhofen, Germany; German Aerospace Center (DLR), German Remote Sensing Data Center (DFD), 82234 Oberpfaffenhofen, Germany; German Aerospace Center (DLR), German Remote Sensing Data Center (DFD), 82234 Oberpfaffenhofen, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Apr 2017","2017","10","5","1780","1791","This study investigates the potential of very high spatial resolution (VHSR) optical WorldView-2 (WV-2) and single-polarized TerraSAR-X (TSX) synthetic aperture radar (SAR) satellite data for an automated detection of different dwelling types in a refugee camp by means of object-based image analysis (OBIA). First, the optical data and SAR data are analyzed independently, and then a fusion of both data sets is performed applying two different approaches: 1) an overlay operation-based procedure integrating the independent results of the optical- and SAR-based dwelling detection, and 2) a feature-based analysis approach taking advantage of the conjoint analysis of both data sets. The results of the single-sensor and the data fusion approaches are evaluated in detail on the basis of object-based and area-based accuracy assessments. Advantages and limitations of the analysis approaches are discussed. The accuracy rates reveal that the use of optical satellite data shows promising results regardless of the dwelling material, while the SAR data are suitable for the detection of metal sheet dwellings only. In complex camp areas, with closely spaced containers, the results of the independent analyses can be improved significantly by the proposed fusion approaches. The combination of SAR and optical data allows for the separation of contiguous dwellings in cases this was not possible by the optical sensor information.","2151-1535","","10.1109/JSTARS.2017.2664982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872448","Data fusion;dwelling detection;feature extraction;object-based image analysis (OBIA);refugee camp mapping;rule-based classification;synthetic aperture radar (SAR);very high spatial resolution (VHSR) data","Synthetic aperture radar;Optical sensors;Optical imaging;Satellites;Feature extraction;Adaptive optics;Optical network units","hyperspectral imaging;image fusion;optical sensors;remote sensing by radar;synthetic aperture radar","object-based analysis;optical satellite data;SAR satellite data;data fusion;refugee camp;very-high-spatial resolution optical WorldView-2;single-polarized TerraSAR-X;synthetic aperture radar;automated dwelling type detection;object-based image analysis;optical-based dwelling detection;SAR-based dwelling detection;feature-based analysis approach;optical sensor information;metal sheet dwelling detection","","20","","40","IEEE","6 Mar 2017","","","IEEE","IEEE Journals"
"High-Performance SAR Automatic Target Recognition Under Limited Data Condition Based on a Deep Feature Fusion Network","Q. Yu; H. Hu; X. Geng; Y. Jiang; J. An","Electronic Information School, Wuhan University, Wuhan, China; Electronic Information School, Wuhan University, Wuhan, China; College of Ocean and Earth, Xiamen University, Xiamen, China; Electronic Information School, Wuhan University, Wuhan, China; Chinese Antarctic Center of Surveying and Mapping, Wuhan University, Wuhan, China","IEEE Access","19 Nov 2019","2019","7","","165646","165658","Deep learning algorithms have been widely applied to the recognition of remote-sensing images due to their excellent performance on various recognition problems with sufficient data. However, limited data on synthetic aperture radar (SAR) images degrade the performance of neural networks for SAR automatic target recognition (ATR). To address this problem, this paper presents a new deep feature fusion framework by combining the Gabor features and information of raw SAR images and fusing the feature vectors extracted from different layers in our proposed neural network. Gabor features improve the richness of SAR image features. The number of free parameters of neural networks is largely reduced by utilizing large-scale convolutional kernel factorization and global average pooling. Moreover, the fusion of feature vectors from different layers helps improve the recognition performance of neural networks. Experimental results on the MSTAR dataset demonstrate the effectiveness of our proposed method. The proposed neural network can achieve an average accuracy of 99% on the classification of ten-class targets and even achieve a high recognition accuracy on limited data and noisy data.","2169-3536","","10.1109/ACCESS.2019.2952928","National Defense Innovation Science Foundation; National Natural Science Foundation of China(grant numbers:61174196); Shanghai Academy of Spaceflight Technology(grant numbers:SAST2017-080); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8895967","Automatic target recognition;deep convolutional networks;synthetic aperture radar;deep feature fusion;Gabor features","Feature extraction;Radar polarimetry;Neural networks;Synthetic aperture radar;Target recognition;Silicon;Computational modeling","convolutional neural nets;feature extraction;image classification;image fusion;learning (artificial intelligence);radar computing;radar imaging;radar target recognition;remote sensing by radar;synthetic aperture radar","SAR automatic target recognition;deep feature fusion network;deep learning algorithms;remote-sensing images;synthetic aperture radar images;neural network;deep feature fusion framework;Gabor features;feature vectors;SAR image features;noisy data;ATR;convolutional kernel factorization;global average pooling;MSTAR dataset","","19","","52","CCBY","11 Nov 2019","","","IEEE","IEEE Journals"
"SAR Image Change Detection Using Saliency Extraction and Shearlet Transform","Y. Zhang; S. Wang; C. Wang; J. Li; H. Zhang","Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; School of Communication Engineering, Jilin University, Changchun, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China; School of Communication Engineering, Jilin University, Changchun, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4701","4710","Change detection has recently become a topic of great significance in the field of remote sensing. However, as one of the traditional effective detection methods, synthetic aperture radar (SAR) image change detection based on wavelet transform fusion remains limited because of the existence of speckle noise and because multidirectional information has been underutilized. Therefore, we propose an unsupervised change detection method using saliency extraction and the shearlet transform. Saliency extraction is first used to homogenize registered images to reduce speckle noise. Using a subtraction operation for preprocessed binary images, we then obtain a saliency-guided difference image (DI) that includes the main contour change information. Then, the Gauss-log-ratio DI includes the detailed change information at the edge of the image and acts as an auxiliary DI. Next, two DIs are fused with the shearlet transform. During this process, the DI is decomposed into one low-frequency and four high-frequency subimages. The low-frequency subimage contains image contour information and the high-frequency subimages contain the edge information of the images. Compared to wavelet fusion, in our method, no extra fusion noise occurs because the shearlet transform performs multiscale analysis. The final change map can be obtained through maximum entropy segmentation. Real SAR image pairs in areas of Bern, Switzerland, and Suzhou, China, are used to verify the proposed change detection method. The experimental results demonstrate the effectiveness of the proposed method when compared to the reference methods.","2151-1535","","10.1109/JSTARS.2018.2866540","National Natural Science Foundation of China(grant numbers:41331176,61631009); National Key Research and Development Plan of the 13th five-year(grant numbers:2017YFB0404800); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454801","Change detection;multidirectional information;saliency extraction;shearlet transform;synthetic aperture radar (SAR)","Synthetic aperture radar;Feature extraction;Wavelet transforms;Remote sensing;Image edge detection","entropy;image fusion;image segmentation;radar imaging;remote sensing;speckle;synthetic aperture radar;wavelet transforms","SAR image change detection;saliency extraction;traditional effective detection methods;synthetic aperture radar image change detection;speckle noise;multidirectional information;unsupervised change detection method;registered images;preprocessed binary images;saliency-guided difference image;main contour change information;Gauss-log-ratio DI;detailed change information;auxiliary DI;high-frequency subimages;low-frequency subimage;image contour information;edge information;wavelet fusion;extra fusion noise;final change map;SAR image pairs;reference methods","","18","","31","IEEE","5 Sep 2018","","","IEEE","IEEE Journals"
"Squeeze-and-Excitation Laplacian Pyramid Network With Dual-Polarization Feature Fusion for Ship Classification in SAR Images","T. Zhang; X. Zhang","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Geoscience and Remote Sensing Letters","7 Jan 2022","2022","19","","1","5","This letter proposes a squeeze-and-excitation Laplacian pyramid network with dual-polarization feature fusion (SE-LPN-DPFF) for ship classification in synthetic aperture radar (SAR) images. SE-LPN-DPFF offers three contributions: 1) dual-polarization (VV and VH) feature fusion (DPFF); 2) channel modeling by the squeeze-and-excitation (SE) to balance each polarization feature’s contribution; and 3) Laplacian pyramid network (LPN) to achieve multiresolution analysis (MRA). Extensive ablation studies can confirm the effectiveness of each contribution. Results on the three- and six-category OpenSARShip datasets reveal the state-of-the-art SAR ship classification performance.","1558-0571","","10.1109/LGRS.2021.3119875","National Natural Science Foundation of China(grant numbers:61571099); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568903","Dual-polarization feature fusion (DPFF);Laplacian pyramid network (LPN);ship classification;squeeze-and-excitation (SE);synthetic aperture radar (SAR)","Marine vehicles;Synthetic aperture radar;Training;Feature extraction;Laplace equations;Radar polarimetry;Tensors","electromagnetic wave polarisation;image classification;image fusion;image resolution;radar imaging;radar resolution;ships;synthetic aperture radar","dual-polarization feature fusion;synthetic aperture radar images;SAR ship classification performance;SE-LPN-DPFF;squeeze-and-excitation Laplacian pyramid network;SAR images;channel modeling;multiresolution analysis;MRA","","16","","43","IEEE","13 Oct 2021","","","IEEE","IEEE Journals"
"Building Damage Detection via Superpixel-Based Belief Fusion of Space-Borne SAR and Optical Images","X. Jiang; Y. He; G. Li; Y. Liu; X. -P. Zhang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Institute of Information Fusion, Naval Aeronautical University, Yantai, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, Ryerson University, Toronto, Canada","IEEE Sensors Journal","24 Jan 2020","2020","20","4","2008","2022","Space-borne synthetic aperture radar (SAR) and optical sensors are important tools for building damage detection. Fusion of SAR and optical images improves detection performance. However, when the resolutions of the two different kinds of images differ, the performance of the existing pixel-level fusion methods deteriorates significantly due to interpolation-induced distortion. To solve this problem, this paper presents a new superpixel-based belief fusion (SBBF) model for building damage detection. The superpixels on the SAR and optical images are identified by the segmentation on the pre-earthquake optical image to perform the fusion on the superpixel-level instead of the pixel-level in existing methods. Then in the fusion stage, different from the commonly used direct fusion methods that do not consider the reliability in the fusion process, a novel belief fusion method that employs a basic belief assignment (BBA) to incorporate different reliabilities of superpixels is proposed to improve the accuracy of building damage detection. For each superpixel, the BBA is assigned based on the influence of noise and resolutions. The United Nations Operational Satellite Applications Programme (UNOSAT) datasets corresponding to the 2010 Haiti earthquake and the 2011 Tōhoku earthquake, are used to evaluate the performance of the proposed method. The experimental results show that the proposed method achieves significantly better performance than existing separate SAR or optical images based methods, and the existing pixel-level fusion methods.","1558-1748","","10.1109/JSEN.2019.2948582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8878121","Building damage detection;synthetic aperture radar (SAR);optical remote sensing;superpixel;fusion","Optical distortion;Optical sensors;Optical imaging;Synthetic aperture radar;Image resolution;Buildings;Optical reflection","buildings (structures);earthquakes;geophysical image processing;geophysical techniques;image fusion;image resolution;image segmentation;optical images;radar imaging;spaceborne radar;synthetic aperture radar","AD 2011;Haiti earthquake;AD 2010;United Nations Operational Satellite Applications Programme datasets;basic belief assignment;direct fusion methods;space-borne synthetic aperture radar;novel belief fusion method;fusion process;fusion stage;superpixel-level;pre-earthquake optical image;superpixel-based belief fusion model;pixel-level fusion methods;detection performance;optical sensors;space-borne SAR;building damage detection","","16","","42","IEEE","21 Oct 2019","","","IEEE","IEEE Journals"
"Automatic Detection and Positioning of Ground Control Points Using TerraSAR-X Multiaspect Acquisitions","S. Montazeri; C. Gisinger; M. Eineder; X. x. Zhu","Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center, Wessling, Germany; Chair of Remote Sensing Technology, Technische Universität München, Munich, Germany; Signal Processing for Earth Observation, Technische Universität München, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","20 Apr 2018","2018","56","5","2613","2632","Geodetic stereo synthetic aperture radar (SAR) is capable of absolute 3-D localization of natural persistent scatterers, which allows for ground control point (GCP) generation using only SAR data. The prerequisite for the method to achieve high-precision results is the correct detection of common scatterers in SAR images acquired from different viewing geometries. In this contribution, we describe three strategies for automatic detection of identical targets in SAR images of urban areas taken from different orbit tracks. Moreover, a complete workflow for automatic generation of large number of GCPs using SAR data is presented and its applicability is shown by exploiting TerraSAR-X high-resolution spotlight images over the city of Oulu, Finland, and a test site in Berlin, Germany.","1558-0644","","10.1109/TGRS.2017.2769078","European Research Council through the European Union Horizon 2020 Research And Innovation Program(grant numbers:ERC-2016-StG-714087); Helmholtz Association through the framework of the Young Investigators Group “SiPEO”(grant numbers:VH-NG-1018); Munich Aerospace e.V. Fakultät für Luft- und Raumfahrt; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8254388","Geodetic stereo synthetic aperture radar (SAR);ground control point;positioning;SAR;TerraSAR-X (TS-X)","Synthetic aperture radar;Satellites;Geometry;Timing;Radar tracking;Orbits;Target tracking","geophysical image processing;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","SAR images;SAR data;automatic detection;geodetic stereo synthetic aperture radar;3-D localization;natural persistent scatterers;ground control point generation;high-precision results;correct detection;common scatterers;identical targets;orbit tracks;TerraSAR-X high-resolution spotlight images;GCP automatic generation;ground control point positioning;TerraSAR-X Multiaspect Acquisitions;Oulu city;Finland","","12","","40","OAPA","11 Jan 2018","","","IEEE","IEEE Journals"
"Evaluation of Optical and Radar Images Integration Methods for LULC Classification in Amazon Region","L. O. Pereira; C. C. Freitas; S. J. S. Sant´Anna; M. S. Reis","Department of Geography, College of Life and Environmental Sciences, University of Exeter, Exeter, U.K.; Division of Image Processing (DPI), Brazilian National Institute for Space Research (INPE), São José dos Campos, Brazi; Division of Image Processing (DPI), Brazilian National Institute for Space Research (INPE), São José dos Campos, Brazi; Division of Image Processing (DPI), Brazilian National Institute for Space Research (INPE), São José dos Campos, Brazi","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Sep 2018","2018","11","9","3062","3074","The main objective of this study is to evaluate different methods to integrate (fusion and combination) Synthetic Aperture Radar (SAR) Advanced Land Observing Satellite (ALOS) Phased Arrayed L-band SAR (PALSAR-1) (Fine Beam Dual mode-FDB) and LANDSAT images in order to identify those which lead to higher accuracy of land-use and land-cover (LULC) mapping in an agricultural frontier region in Amazon. One method used to integrate the multipolarized information in SAR images before the fusion process was also evaluated. In this method, the first principal component (PC1) of SAR data was used. Color compositions of fused data that presented better LULC classification were visually analyzed. Considering the proposed objective, the following fusion methods must be highlighted: Ehlers, Wavelet á trous, Intensity, Hue and Saturation (IHS), and selective principal component analysis (SPC). These latter three methods presented good results when processed using PC1 from ALOS/PALSAR-1 FBD backscatter filtered image or three SAR extracted and selected features. These results corroborate with the applicability of the proposed method for SAR data information integration. Distinct methods better discriminate different LULC classes. In general, densely forested classes were better characterized by the Ehlers_TM6 fusion method, in which at least the polarization HV was used. Intermediate and initial regeneration classes were better discriminated using SPC-fused data with PC1 of ALOS/PALSAR-1 FBD data. Bare soil and pasture classes were better discriminated in optical features and the PC1 of ALOS/PALSAR-1 FBD data fused by the IHS method. Soybean with approximately 40 days from seeding was better discriminated in image classification obtained from ALOS/PALSAR-1 FBD image.","2151-1535","","10.1109/JSTARS.2018.2853647","Conselho Nacional de Desenvolvimento Científico e Tecnológico(grant numbers:301118/2017-5); Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; Monitoramento Ambiental por Satélite no Bioma Amazônia(grant numbers:1022114003005-MSA-BNDES); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416958","Brazilian Amazon;data integration;land-use and land-cover (LULC);multipolarized-synthetic aperture radar (SAR)","Synthetic aperture radar;Optical imaging;Optical sensors;Optical polarization;Data integration;Feature extraction;Earth","geophysical image processing;image classification;image fusion;land cover;land use;principal component analysis;radar polarimetry;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar;terrain mapping","LULC classification;Amazon region;L-band SAR;land-cover mapping;agricultural frontier region;SAR images;fusion process;fusion methods;SAR data information integration;SPC-fused data;IHS method;image classification;land-use mapping;principal component analysis;radar image integration method;optical image integration method;Synthetic Aperture Radar Advanced Land Observing Satellite;fine beam dual mode;ALOS-PALSAR-1 FBD backscatter;ALOS-PALSAR-1 FBD image;Ehlers_TM6 fusion method;densely forested class;LANDSAT image","","10","","52","IEEE","20 Jul 2018","","","IEEE","IEEE Journals"
"Modified Tensor Distance-Based Multiview Spectral Embedding for PolSAR Land Cover Classification","B. Ren; B. Hou; J. Chanussot; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xian, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xian, China; LJK, CNRS, Grenoble INP, Inria, Université Grenoble Alpes, Grenoble, France; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xian, China","IEEE Geoscience and Remote Sensing Letters","24 Nov 2020","2020","17","12","2095","2099","This letter proposes a novel method for combining multiview features in polarimetric synthetic aperture radar (PolSAR) for land cover classification. It is well-known that feature extraction and classifier design are two significant steps in machine learning methods for PolSAR data interpretation. Each PolSAR pixel can be represented in different feature spaces, such as polarimetric data scattering, or the polarimetric target decomposition spaces. In this letter, a tensor-based multiview embedding algorithm is proposed to fuse those features from different spaces in order to obtain a distinctive set of features for the subsequent classification. Based on the pixel-based classification tasks, a modified tensor distance (MTD) is designed to accurately calculate the distance between tensors. It emphasizes the importance of the central pixel, and decreases the influence of the neighbors in the feature patch when calculating tensor distance. Furthermore, the complementary properties of different views are exploited by an MTD measured tensor multiview spectral embedding method, so as to obtain relevant low-dimensional features. Compared with state-of-the-art methods, the validation and effectiveness of the proposed method is demonstrated on two real PolSAR data sets.","1558-0571","","10.1109/LGRS.2019.2962185","National Natural Science Foundation of China(grant numbers:61671350,61836009); Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61621005); Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-05); China Postdoctoral Science Foundation Funded Project(grant numbers:2018M633468); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8957317","Land cover classification;modified tensor distance (MTD);multiview spectral embedding (MSE);polarimetric synthetic aperture radar (PolSAR)","Tensors;Synthetic aperture radar;Feature extraction;Task analysis;Backscatter;Speckle;Measurement","feature extraction;geophysical image processing;image classification;image fusion;image resolution;land cover;radar imaging;radar polarimetry;synthetic aperture radar;tensors","PolSAR data sets;low-dimensional features;spectral embedding method;feature patch;central pixel;pixel-based classification tasks;subsequent classification;tensor-based multiview;polarimetric target decomposition spaces;polarimetric data scattering;feature spaces;PolSAR pixel;PolSAR data interpretation;classifier design;feature extraction;polarimetric synthetic aperture radar;multiview features;PolSAR land cover classification;modified tensor distance-based multiview spectral embedding","","9","","16","IEEE","13 Jan 2020","","","IEEE","IEEE Journals"
"Ship Size Extraction for Sentinel-1 Images Based on Dual-Polarization Fusion and Nonlinear Regression: Push Error Under One Pixel","B. Li; B. Liu; W. Guo; Z. Zhang; W. Yu","Shanghai Key Laboratory of Intelligent Sensing and Recognition, Shanghai Jiao Tong University, Shanghai, China; College of Marine Sciences, Shanghai Ocean University, Shanghai, China; Shanghai Key Laboratory of Intelligent Sensing and Recognition, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Intelligent Sensing and Recognition, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Intelligent Sensing and Recognition, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Geoscience and Remote Sensing","20 Jul 2018","2018","56","8","4887","4905","In this paper, we present a method of ship size extraction for Sentinel-1 synthetic aperture radar (SAR) images, which is composed of the image processing stage and the regression stage. In order to achieve extraction with high accuracy, considering the data characteristics of Sentinel-1 images, we propose to use the dual-polarization fusion and the nonlinear regression with the gradient boosting. The experiments and analyses on a relatively large data set show that: 1) compared with the existing and related studies, the proposed method achieves an improved performance. The extraction errors are pushed under one pixel, and they are 4.66% (8.80 m) and 7.01% (2.17 m) for length and width, respectively; 2) the dual-polarization information fusion does improve the size extraction accuracy; and 3) the nonlinear regression does exploit the relationship between the influential factors and the size parameters and provide a better performance than the linear regression. The experimental results verify that the proposed design is suitable for ship size extraction in Sentinel-1 SAR images.","1558-0644","","10.1109/TGRS.2018.2841882","National Natural Science Foundation of China(grant numbers:61331015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392506","Dual-polarization fusion;nonlinear regression;Sentinel-1;ship size extraction;synthetic aperture radar (SAR) image","Marine vehicles;Synthetic aperture radar;Image resolution;Scattering;Data mining;Surveillance;Azimuth","gradient methods;image fusion;marine radar;radar imaging;regression analysis;ships;synthetic aperture radar","ship size extraction;Sentinel-1 SAR images;dual-polarization fusion;nonlinear regression;Sentinel-1 synthetic aperture radar images;image processing stage;regression stage;dual-polarization information fusion;size extraction accuracy;size parameters;linear regression","","7","","49","IEEE","21 Jun 2018","","","IEEE","IEEE Journals"
"AFSar: An Anchor-Free SAR Target Detection Algorithm Based on Multiscale Enhancement Representation Learning","H. Wan; J. Chen; Z. Huang; R. Xia; B. Wu; L. Sun; B. Yao; X. Liu; M. Xing","Key Laboratory of Intelligent Computing and Signal Processing, Ministry of Education, School of Electronics and Information Engineering, Anhui University, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing, Ministry of Education, School of Electronics and Information Engineering, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing, Ministry of Education, School of Electronics and Information Engineering, Anhui University, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; 38th Research Institute, China Electronics Technology Group Corporation, Hefei, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","3 Mar 2022","2022","60","","1","14","Unlike optical images, synthetic aperture radar (SAR) images have unique characteristics, such as few samples, strong scattering, sparseness, multiple scales, complex interference and background, and inconspicuous target edge contour information. Current SAR target detection algorithms have difficulty in balancing accuracy and speed, and the performance of these algorithms is relatively limited, thus making it difficult to deploy practical applications. To this end, this article proposes AFSar, an innovative anchor-free SAR target detection algorithm based on multiscale enhancement representation learning. First, we introduce the latest anchor-free architecture YOLOX as the basic framework. Second, to reduce the computational complexity of the model and to improve the ability of multiscale feature extraction, we redesigned the lightweight backbone, namely, MobileNetV2S. Furthermore, we propose an attention enhancement PAN module, called CSEMPAN, which highlights the unique strong scattering characteristics of SAR targets by integrating channel and spatial attention mechanisms. Finally, in view of the multiscale and strong sparse characteristics of SAR targets, we propose a new target detection head, namely, ESPHead. ESPHead extracts the features of targets with different scales by using dilated convolution with different dilated rates, so as to enhance the detection ability of the model for targets with different scales. The results of ablation experiments on the SSDD dataset show that the mAP of our algorithm reaches 0.977, while the Flops is only 9.86 G, achieving state of the art.","1558-0644","","10.1109/TGRS.2021.3137817","National Natural Science Foundation of China(grant numbers:62001003); Natural Science Foundation of Anhui Province(grant numbers:2008085QF284); China Postdoctoral Science Foundation(grant numbers:2020M671851); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661358","Anchor-free;deep learning;enhancement representation learning;multiscale learning;synthetic aperture radar (SAR) target detection","Feature extraction;Object detection;Radar polarimetry;Synthetic aperture radar;Detection algorithms;Optical imaging;Prediction algorithms","feature extraction;image fusion;object detection;radar imaging;synthetic aperture radar","innovative anchor-free SAR target detection algorithm;multiscale enhancement representation learning;latest anchor-free architecture;multiscale feature extraction;attention enhancement PAN module;unique strong scattering characteristics;SAR targets;multiscale characteristics;strong sparse characteristics;target detection head;AFSar;optical images;synthetic aperture radar images;complex interference;inconspicuous target edge contour information;current SAR target detection algorithms","","6","","47","IEEE","23 Dec 2021","","","IEEE","IEEE Journals"
"Fusion of Sparse Model Based on Randomly Erased Image for SAR Occluded Target Recognition","Z. He; H. Xiao; C. Gao; Z. Tian; S. -W. Chen","National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","26 Oct 2020","2020","58","11","7829","7844","The recognition of partially occluded targets is a difficult problem in the field of synthetic aperture radar (SAR) target recognition. To eliminate the effect of occlusion, the intuitive idea is to determine the exact location and the size of the occluded area. However, this is very difficult, even impossible in practice. In order to avoid this difficulty and to improve the recognition performance for the partially occluded target, a fusion strategy of the sparse representation (SR) model based on randomly erased images is proposed to recognize the partially occluded target. The proposed method randomly erases some areas many times in both the test samples and the training samples. The erased training samples in each erasure are used to sparsely represent the corresponding erased test sample. Finally, all the SR results are fused to recognize the test sample. The proposed method utilizes random erasure to eliminate the possible occluded region. In addition, this method uses the fusion strategy to overcome under-erasing of the occluded region and erroneous erasure of the unoccluded region. The key parameter of the proposed method is the erasure ratio only. Although the erasure is random, the recognition performance of the method is relatively stable. Therefore, the method can eliminate the influence of occlusion without determining the details of occlusion. The experimental results show that the proposed method is significantly better than the state-of-the-art methods in the case of occlusion. Additionally, the recognition performance of the proposed method is similar to some comparison methods in the case of no occlusion.","1558-0644","","10.1109/TGRS.2020.2984577","National Natural Science Foundation of China(grant numbers:61771480); National Pre-Research Foundation(grant numbers:61404150104,61404160109); Science and Technology Planning Project of Hunan Province(grant numbers:2019RS2025); Project of National University of Defense Technology(grant numbers:ZK18-02-14); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9067033","Automatic target recognition (ATR);fusion strategy;occluded target;random erasure;sparse representation (SR);synthetic aperture radar (SAR)","Target recognition;Mathematical model;Synthetic aperture radar;Image reconstruction;Training;Electromagnetic scattering","image fusion;image recognition;image representation;learning (artificial intelligence);radar imaging;radar target recognition;synthetic aperture radar","sparse model;randomly erased image;SAR;partially occluded target;training samples;occluded target recognition;synthetic aperture radar;sparse representation model;randomly erased images;erroneous erasure;erasure ratio","","6","","39","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"SAR Ship Detection Based on End-to-End Morphological Feature Pyramid Network","C. Zhao; X. Fu; J. Dong; R. Qin; J. Chang; P. Lang","School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China; School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China; Shijiazhuang Campus, Army Engineering University, Qadirabad, Bangladesh; School of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Nan'An, China; School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China; School of Integrated Circuits and Electronics, Beijing Institute of Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Jun 2022","2022","15","","4599","4611","Intelligent ship detection based on high-precision synthetic aperture radar (SAR) images plays a vital role in ocean monitoring and maritime management. Denoising is an effective preprocessing step for target detection. Morphological network-based denoising can effectively remove speckle noise, while the smoothing effect of which blurs the edges of the image and reduces the detection accuracy. The fusion of edge extraction and morphological network can improve detection accuracy by compensating for the lack of edge information caused by smoothing. This article proposes an end-to-end lightweight network called morphological feature-pyramid Yolo v4-tiny for SAR ship detection. First, a morphological network is introduced to preprocess the SAR images for speckle noise suppression and edge enhancement, providing spatial high-frequency information for target detection. Then, the original and preprocessed images are combined into the multichannel as an input for the convolution layer of the network. The feature pyramid fusion structure is used to extract the high-level semantic features and shallow detailed features from the image, improving the performance of multiscale target detection. Experiments on the public SAR ship detection dataset and AIR SARShip-1.0 show that the proposed method performs better than the other convolution neural network-based methods.","2151-1535","","10.1109/JSTARS.2022.3150910","111 Project of China(grant numbers:B14010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716855","Convolution neural network (CNN);feature pyramid fusion;morphological network;synthetic aperture radar (SAR) target detection","Feature extraction;Marine vehicles;Image edge detection;Object detection;Kernel;Radar polarimetry;Synthetic aperture radar","edge detection;feature extraction;geophysical image processing;image denoising;image fusion;neural nets;object detection;radar imaging;ships;speckle;synthetic aperture radar","effective preprocessing step;morphological network-based denoising;smoothing effect;detection accuracy;edge extraction;edge information;end-to-end lightweight network;morphological feature-pyramid Yolo v;SAR images;speckle noise suppression;edge enhancement;high-frequency information;original images;preprocessed images;feature pyramid fusion structure;high-level semantic features;shallow detailed features;multiscale target detection;public SAR ship detection dataset;convolution neural network-based methods;end-to-end morphological feature pyramid network;intelligent ship detection;high-precision synthetic aperture radar images","","5","","45","CCBY","18 Feb 2022","","","IEEE","IEEE Journals"
"Triplet Hybrid Discriminative Fields Model Based on Bayesian Fusion for Unsupervised Nonstationary SAR Image Multiclass Segmentation","P. Zhang; M. Li; W. Song; Y. Wu; L. An","National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing and the Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China; Remote Sensing Image Processing and Fusion Group, School of Electronics Engineering, Xidian University, Xi'an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4848","4861","The discriminative random fields (DRF) model is suitable for analyzing images with complex textural structures and has achieved promising results in image segmentation. However, the DRF model does not consider the nonstationarity of synthetic aperture radar (SAR) images and lacks the ability to model SAR scattering statistics in nonstationary SAR image segmentation. In this paper, we propose a triplet hybrid discriminative random fields (THDF) model based on Bayesian fusion. According to its semantic structure, the THDF model belongs to hybrid discriminative models, and it provides the following promising contributions to nonstationary SAR image segmentation while inheriting the advantages of the discriminative models: first, it takes the nonstationarity of SAR images into account from the perspective of their texton appearances, and thus regulates the local label interaction patterns and considers the distribution differences of the congeneric image features in different stationary parts; and second, for nonstationary SAR images, it performs a fusion-type treatment of the nonstationary textural features and the SAR scattering statistics based on Bayesian fusion and, thus, captures the nonstationary information from SAR data in a more complete manner. The effectiveness of the proposed model is demonstrated through applications to both synthetic images and real SAR image segmentations.","2151-1535","","10.1109/JSTARS.2018.2875935","National Natural Science Foundation of China(grant numbers:61871312,61772390); Aeronautical Science Foundation of China(grant numbers:2016081011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516305","Bayesian fusion;nonstationary property;synthetic aperture radar (SAR) image multiclass segmentation;triplet hybrid discriminative random fields (THDF)","Synthetic aperture radar;Image segmentation;Hidden Markov models;Data models;Bayes methods;Scattering;Analytical models","Bayes methods;image fusion;image segmentation;image texture;radar imaging;synthetic aperture radar","SAR scattering statistics;texton appearances;local label interaction patterns;synthetic images;nonstationary textural features;nonstationary SAR images;congeneric image features;hybrid discriminative models;THDF model;triplet hybrid discriminative random fields;nonstationary SAR image segmentation;synthetic aperture radar images;DRF model;complex textural structures;discriminative random fields model;unsupervised nonstationary SAR image multiclass segmentation;Bayesian fusion","","5","","36","IEEE","31 Oct 2018","","","IEEE","IEEE Journals"
"Multiview Attention CNN-LSTM Network for SAR Automatic Target Recognition","C. Wang; X. Liu; J. Pei; Y. Huang; Y. Zhang; J. Yang","Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Dec 2021","2021","14","","12504","12513","Synthetic aperture radar (SAR) is a microwave remote sensing system. It has a broad scope of applications in both military and civilian fields. Benefited from the latest advances in deep learning, SAR automatic target recognition technology has made an excellent breakthrough However, most existing methods ignore the large variation of scattering characteristics of SAR target images with different azimuths, which limits the performance and practical application. The SAR images under different azimuths contain distinct feature information, and the images under adjacent azimuths are correlated in terms of features. Therefore, extracting the feature information of images under adjacent azimuths and leveraging their correlation can improve the recognition performance. In this article, we proposed a multiview attention convolutional neural network with long short-term memory (LSTM) network to extract and fuse the features from images with adjacent azimuths. It adopts multiple convolutional modules to extract deep features from each single-view SAR image and spatial attention module to locate the information of the target and suppress the useless noise. Then, the LSTM module performs feature fusion based on the correlation of features obtained from adjacent azimuths. Finally, based on these multiview images, deep features are extracted and fused to obtain precise recognition results. Experiments are performed on the moving and stationary target acquisition and recognition dataset, and the results have verified the effectiveness of the proposed method.","2151-1535","","10.1109/JSTARS.2021.3130582","National Natural Science Foundation of China(grant numbers:61901091,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9627549","Convolutional neural network (CNN);deep learning;long short-term memory (LSTM);multiview;synthetic aperture radar automatic target recognition (SAR ATR)","Radar polarimetry;Feature extraction;Azimuth;Synthetic aperture radar;Target recognition;Image recognition;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image fusion;image representation;radar computing;radar imaging;radar target recognition;recurrent neural nets;synthetic aperture radar","recognition dataset;stationary target acquisition;multiview images;LSTM module performs;spatial attention module;single-view SAR image;deep features;multiple convolutional modules;short-term memory network;multiview attention convolutional neural network;recognition performance;adjacent azimuths;distinct feature information;SAR images;SAR target images;SAR automatic target recognition technology;deep learning;civilian fields;military fields;microwave remote sensing system;synthetic aperture radar;multiview attention CNN-LSTM network","","4","","20","CCBY","25 Nov 2021","","","IEEE","IEEE Journals"
"Small Vessel Detection Based on Adaptive Dual-Polarimetric Feature Fusion and Sea–Land Segmentation in SAR Images","Y. Zhou; F. Zhang; F. Ma; D. Xiang; F. Zhang","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; Interdisciplinary Research Center for Artificial Intelligence, Beijing University of Chemical Technology, Beijing, China; Interdisciplinary Research Center for Artificial Intelligence, Beijing University of Chemical Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","5 Apr 2022","2022","15","","2519","2534","Detection of small sea vessels in synthetic aperture radar (SAR) images has received much attention in recent years because the small vessels have weak scattering intensity and few image pixels. The existing detection network structures are not well adapted to small-scale targets, the polarimetric data are not properly utilized, and the sea–land segmentation process to remove land false alarms is time-consuming. Regarding these problems, first, a single low-level path aggregation network is designed specifically for small targets. The structure reduces false alarms at the feature level by finding suitable single-scale feature maps for detection and adding a semantic enhancement module. Second, adaptive dual-polarimetric feature fusion is proposed to filter the multichannel features acquired by dual-polarimetric decomposition to reduce feature redundancy. Third, a segmentation layer is added to the network to shield the land from false alarms. The detection and segmentation layers share the feature extraction and feature fusion modules and are jointly trained by a joint loss. Finally, polarimetric SAR detection and segmentation dataset containing small vessel detection and sea–land segmentation labels is created with reference to the LS-SSDDv1.0 dataset, and experimental results on this dataset verify the improvement of this proposed method over other typical methods.","2151-1535","","10.1109/JSTARS.2022.3158807","National Natural Science Foundation of China(grant numbers:62171016,61871413); Fundamental Research Funds for the Central Universities(grant numbers:buctrc202001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9735422","Feature fusion;polarimetric synthetic aperture radar (SAR);sea–land segmentation;small vessel detection","Feature extraction;Radar polarimetry;Image segmentation;Semantics;Clutter;Synthetic aperture radar;Optical imaging","feature extraction;image fusion;image segmentation;marine vehicles;object detection;radar detection;radar imaging;radar polarimetry;synthetic aperture radar","polarimetric data;sea-land segmentation process;land false alarms;feature level;single-scale feature maps;adaptive dual-polarimetric feature fusion;multichannel features;dual-polarimetric decomposition;feature redundancy;segmentation layer;feature extraction;feature fusion modules;polarimetric SAR detection;segmentation dataset;sea-land segmentation labels;sea vessels;synthetic aperture radar images;scattering intensity;image pixels;small-scale targets;small vessel detection;SAR images;detection network structures;semantic enhancement module","","3","","61","CCBY","15 Mar 2022","","","IEEE","IEEE Journals"
"Change Detection in SAR Images Based on Improved Non-Subsampled Shearlet Transform and Multi-Scale Feature Fusion CNN","F. Shen; Y. Wang; C. Liu","School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","8 Dec 2021","2021","14","","12174","12186","Traditional methods for change detection in synthetic aperture radar images have difficulty in obtaining results from the generated differential image (DI) owing to speckle noise. In recent years, many deep learning-based methods have emerged because of their outstanding anti-noise and self-learning ability. However, they are limited by the requirement of abundant high-precision labels. Therefore, in this article, we propose a novel unsupervised method based on improved non-subsampled shearlet transform (NSST) and multi-scale feature fusion convolutional neural network for change detection. First, this method improves the traditional NSST algorithm and proposes a novel pseudo-label generator to obtain more pseudo-labels with higher confidence. It is noteworthy that the more accurate the pseudo-labels are, the better the change detection results will be. Second, this method designs a multi-scale feature fusion block in the network to make the feature images contain more complete information and reduces the number of pooling layers to avoid losing feature image details. The main idea of this method is to eliminate the step of generating the DI and directly obtain results from the original images. The theoretical analysis and final results conducted on three real datasets prove its validity. Furthermore, to verify the generality and potential of the proposed method, we apply it to the cross-region change detection and compare it with the supervised method, which achieve satisfactory results.","2151-1535","","10.1109/JSTARS.2021.3126839","Ministry of Science and Technology of the People's Republic of China; National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFB0503001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610984","Change detection;convolutional neural network (CNN);non-subsampled shearlet transform (NSST);synthetic aperture radar (SAR)","Feature extraction;Synthetic aperture radar;Speckle;Radar polarimetry;Convolutional neural networks;Generators;Clustering algorithms","feature extraction;geophysical image processing;image denoising;image fusion;image segmentation;learning (artificial intelligence);neural nets;radar imaging;synthetic aperture radar;transforms","SAR images;multiscale feature fusion CNN;synthetic aperture radar images;generated differential image;DI;deep learning-based methods;outstanding anti-noise;high-precision labels;novel unsupervised method;improved nonsubsampled shearlet transform;multiscale feature fusion convolutional neural network;traditional NSST algorithm;novel pseudolabel generator;pseudolabels;change detection results;multiscale feature fusion block;feature images;feature image details;original images;theoretical analysis;cross-region change detection;supervised method","","3","","40","CCBY","10 Nov 2021","","","IEEE","IEEE Journals"
"Self-Supervised Learning for Invariant Representations From Multi-Spectral and SAR Images","P. Jain; B. Schoen-Phelan; R. Ross","School of Computer Science, Technological University Dublin, Dublin, Ireland; School of Computer Science, Technological University Dublin, Dublin, Ireland; School of Computer Science, Technological University Dublin, Dublin, Ireland","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","21 Sep 2022","2022","15","","7797","7808","Self-supervised learning (SSL) has become the new state of the art in several domain classification and segmentation tasks. One popular category of SSL are distillation networks, such as Bootstrap Your Own Latent (BYOL). This work proposes RS-BYOL, which builds on BYOL in the remote sensing (RS) domain where data are nontrivially different from natural RGB images. Since multispectral (MS) and synthetic aperture radar (SAR) sensors provide varied spectral and spatial resolution information, we utilize them as an implicit augmentation to learn invariant feature embeddings. In order to learn RS-based invariant features with SSL, we trained RS-BYOL in two ways, i.e., single channel feature learning and three channel feature learning. This work explores the usefulness of single channel feature learning from random 10 MS bands of 10–20 m resolution and VV-VH of SAR bands compared to the common notion of using three or more bands. In our linear probing evaluation, these single channel features reached a 0.92 F1 score on the EuroSAT classification task and 59.6 mIoU on the IEEE Data Fusion Contest segmentation task for certain single bands. We also compare our results with ImageNet weights and show that the RS-based SSL model outperforms the supervised ImageNet-based model. We further explore the usefulness of multimodal data compared to single modality data, and it is shown that utilizing MS and SAR data allows better invariant representations to be learnt than utilizing only MS data.","2151-1535","","10.1109/JSTARS.2022.3204888","Science Foundation Ireland(grant numbers:13/RC/2106_P2); ADAPT SFI Research Centre at Technological University Dublin ADAPT; SFI Research Centre for AI-Driven Digital Content Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880533","Optical-synthetic aperture radar (SAR) fusion;satellite images;self-supervised learning (SSL);unsupervised learning","Synthetic aperture radar;Task analysis;Optical sensors;Optical imaging;Representation learning;Remote sensing;Satellites","feature extraction;geophysical image processing;image classification;image fusion;image resolution;image segmentation;radar imaging;remote sensing by radar;supervised learning;synthetic aperture radar","segmentation tasks;RS-BYOL;remote sensing domain;natural RGB images;synthetic aperture radar;spectral resolution information;spatial resolution information;invariant feature embeddings;RS-based invariant features;single channel feature learning;random 10 MS bands;SAR bands;single channel features;single bands;RS-based SSL model;supervised ImageNet-based model;single modality data;invariant representations;MS data;multispectral;SAR images;self-supervised learning;domain classification;IEEE data fusion contest segmentation task;size 10.0 m to 20.0 m","","3","","70","CCBY","7 Sep 2022","","","IEEE","IEEE Journals"
"In Situ Volumetric SAR","C. F. Barnes; S. Prasad","School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA; School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA","IEEE Transactions on Geoscience and Remote Sensing","26 Sep 2018","2018","56","10","6082","6100","Volumetric synthetic aperture radar (VolSAR) analysis techniques and image formation algorithms suitable for short ranges and large scenes are presented. From a diffractive wave field inversion perspective, ultrawide beamwidth and near range SAR imaging scenarios can be viewed as a form of in situ SAR. Novel Huygens–Fresnel processing methods are introduced that empower in situ volumetric imaging with 2-D and 3-D aperture synthesis. These methods support coherent fusion across multiple separated frequency bands and also support spatial subaperture fusion of data from sparse sensor swarms. A novel signal analysis tool we call a chirp couplet is developed and shown to be useful in the expression and exploitation of temporal and spatial SAR chirp signals. Chirp couplets provide a physically motivated and unifying tool for both temporal and spatial elements in SAR signal analysis. In the temporal domain, chirp couplets provide a generalized formulation of the coupling of time and frequency that exists, for example, in linear frequency modulated waveforms. In the spatial domain, chirp couplets describe the coupling of sensor position and sensed wavenumber. Formulations of chirp couplets suitable for ray tomographic SAR algorithms (i.e., polar format algorithms) are contrasted with chirp couplets capable of supporting diffractive tomographic SAR algorithms (i.e., Stolt format algorithms). Scenarios in which diffraction limited resolution can be achieved with in situ VolSAR and finite length synthetic apertures are explored. VolSAR imaging methods are shown to escape the approximations that constrain the application and performance of range-Doppler and polar format methods.","1558-0644","","10.1109/TGRS.2018.2831644","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368088","3D radar imaging;Huygens–Fresnel theory;radar signal processing;radar theory;synthetic aperture radar (SAR);volumetric radar imaging","Apertures;Chirp;Synthetic aperture radar;Radar imaging;Tomography;Image resolution","image fusion;image sensors;radar imaging;signal synthesis;synthetic aperture radar","multiple separated frequency bands;signal analysis tool;chirp couplet;temporal SAR chirp signals;spatial SAR chirp signals;temporal elements;spatial elements;SAR signal analysis;ray tomographic SAR algorithms;polar format algorithms;diffractive tomographic SAR algorithms;Stolt format algorithms;VolSAR imaging methods;volumetric synthetic aperture radar analysis techniques;image formation algorithms;diffractive wave field inversion perspective;ultrawide beamwidth;Novel Huygens-Fresnel processing methods;in situ volumetric SAR;in situ VolSAR;in situ volumetric imaging;near range SAR imaging scenarios;3D aperture synthesis;2D aperture synthesis;coherent fusion;sparse sensor swarms;temporal SAR chirp signals;generalized formulation;linear frequency modulated waveforms;sensor position coupling;sensed wavenumber;finite length synthetic apertures;polar format methods;range-Doppler format methods","","3","","80","OAPA","28 May 2018","","","IEEE","IEEE Journals"
"Spatial–Temporal Gray-Level Co-Occurrence Aware CNN for SAR Image Change Detection","X. Zhang; X. Su; Q. Yuan; Q. Wang","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; Collaborative Innovation Center of Geospatial Technology, Wuhan University, Wuhan, China; Air Force Research Institute, Beijing, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Deep learning-based synthetic aperture radar (SAR) image change detection has recently achieved remarkable success due to its great potential for extracting abstract features. However, the existing methods still have room for improvement in dealing with the speckle of SAR images. In this letter, a deep spatial–temporal gray-level co-occurrence aware convolutional neural network (STGCNet) is proposed, which can effectively mine the spatial–temporal information of the bitemporal images and obtain the speckle-robust results by introducing the 3-D gray-level co-occurrence matrix (3-D-GLCM) as auxiliary feature. Specifically, representative features are extracted from original image pairs and their corresponding 3-D-GLCM through two-stream network, followed by an adaptive fusion module to balance the contribution of each branch. Then, the final binary change detection results are obtained by a fully connected layer. The training process relies on reliable labels generated by unsupervised models rather than manually annotated data, and therefore, the proposed STGCNet is practical in reality. Experiments on synthesized and real SAR data sets demonstrate the robustness and competitiveness of the proposed method compared with the state-of-the-art algorithms.","1558-0571","","10.1109/LGRS.2021.3110302","Chang’an University, Xi’an, China, through the National Key Research and Development Program of China(grant numbers:2020YFC1512000); Excellent Youth Foundation of Hubei Scientific Committee(grant numbers:2020CFA051); National Natural Science Foundation of China(grant numbers:61801332,41901302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537572","3-D gray-level co-occurrence matrix (3-D-GLCM), change detection;deep learning;synthetic aperture radar (SAR)","Radar polarimetry;Feature extraction;Training;Speckle;Reliability;Synthetic aperture radar;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image fusion;image representation;radar imaging;speckle;synthetic aperture radar","Co-Occurrence Aware CNN;SAR image change detection;deep learning-based synthetic aperture radar image change detection;abstract features;SAR images;deep spatial-temporal gray-level;aware convolutional neural network;spatial-temporal information;bitemporal images;speckle-robust results;co-occurrence matrix;auxiliary feature;representative features;original image pairs;final binary change detection results;synthesized SAR data sets;real SAR data sets","","2","","18","IEEE","14 Sep 2021","","","IEEE","IEEE Journals"
"High-Precision Pixelwise SAR–Optical Image Registration via Flow Fusion Estimation Based on an Attention Mechanism","Q. Yu; Y. Jiang; W. Zhao; T. Sun","School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 May 2022","2022","15","","3958","3971","Due to the severe speckle noise and complex local deformation in synthetic aperture radar (SAR) images, the problem of high-precision pixelwise registration (dense registration) between SAR and optical images remains far from resolved. In this article, an attention mechanism based optical flow fusion algorithm is proposed to achieve high-precision dense SAR–optical image registration. First, two descriptors, the scale-invariant feature transform (SIFT) and a descriptor based on phase congruency (PC), are used to describe SAR and optical images to eliminate their intensity differences. Then, a salient feature map is extracted as a query matrix to weight the optical flow energy function. When extracting the salient feature map, the Contour Robuste d’Ordre Non Entier detector and the ratio of exponentially weighted averages operator are used to eliminate additive and multiplicative noise in the optical and SAR images, respectively. Finally, the optical flow fields based on SIFT and the PC-based descriptor are fused to compensate for registration ambiguity. Experimental results show that our method is feasible, effective, and robust to noise, and it enables high-precision registration under local deformation.","2151-1535","","10.1109/JSTARS.2022.3172449","Open Project Program Foundation of the Key Laboratory of Opto-Electronics Information Processing; Chinese Academy of Sciences(grant numbers:OEIP-O-202009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9769870","Optical flow;pixelwise registration (dense registration);SAR and optical;synthetic aperture radar (SAR) image","Optical imaging;Optical sensors;Synthetic aperture radar;Adaptive optics;Radar polarimetry;Optical distortion;Image registration","feature extraction;image fusion;image matching;image registration;image resolution;image segmentation;image sequences;optical images;radar imaging;sensor fusion;speckle;synthetic aperture radar;transforms","high-precision pixelwise SAR-optical image registration;flow fusion estimation;attention mechanism;severe speckle noise;complex local deformation;synthetic aperture radar images;high-precision pixelwise registration;dense registration;optical images;optical flow fusion algorithm;high-precision dense SAR-optical image registration;salient feature map;optical flow energy function;optical flow fields;PC-based descriptor;registration ambiguity;high-precision registration","","1","","53","CCBY","5 May 2022","","","IEEE","IEEE Journals"
"SAR Image Change Detection Based on Semisupervised Learning and Two-Step Training","C. Wang; W. Su; H. Gu","School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China","IEEE Geoscience and Remote Sensing Letters","20 Dec 2021","2022","19","","1","5","Change detection, as an essential part of synthetic aperture radar (SAR) automatic target recognition (ATR) systems, remains a challenging problem. The lack of labeled data seriously impedes the development of deep learning-based methods in SAR applications. In this letter, we propose a patch-based semisupervised method to detect changed pixels from limited training data in the field of SAR. The complete approach includes the unsupervised pretraining stage and the iterative discrimination stage. The states of pixels are determined by comparing the corresponding image patches. First, to reduce the impact of insufficient data, we train a trapezium U-Net-like structure to extract representative and generalized features by unsupervised training on all pixels. Feature maps contain detailed and semantic information due to repetitive feature fusions. A designed feature activation module is utilized to recalibrate fused features. Finally, the discrimination is finished by a simple classification convolutional neural network (CNN) following a two-step training strategy. Ablation studies indicate the function provided by the proposed modifications. Experiments on real SAR images demonstrate that the proposed method can improve the detection accuracy by more than 1.2% compared with other state-of-the-art deep learning-based methods.","1558-0571","","10.1109/LGRS.2021.3050746","National Natural Science Foundation of China(grant numbers:61671246,61801221,62001229); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9332238","Change detection;channel-attention;feature fusion;pseudolabels;semisupervised;synthetic aperture radar (SAR)","Feature extraction;Radar polarimetry;Training;Shape;Synthetic aperture radar;Convolution;Training data","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image classification;image fusion;image recognition;object detection;radar computing;radar imaging;synthetic aperture radar;unsupervised learning","representative features;generalized features;unsupervised training;feature maps;detailed information;semantic information;feature fusions;designed feature activation module;fused features;simple classification convolutional neural network;two-step training strategy;SAR images;detection accuracy;state-of-the-art deep learning-based methods;SAR image change detection;synthetic aperture radar automatic target recognition systems;SAR applications;patch-based semisupervised method;changed pixels;training data;unsupervised pretraining stage;iterative discrimination stage;corresponding image patches;trapezium U-Net-like structure","","1","","16","IEEE","21 Jan 2021","","","IEEE","IEEE Journals"
"Designing ISAR Lab Experiments for EO-SAR ATR","M. A. Saville; J. D. Compaleo; H. L. Judd; J. Smith; P. Sotirelis","Department of Electrical Engineering, Wright State University, Dayton, OH, USA; Department of Electrical Engineering, Wright State University, Dayton, OH, USA; Department of Electrical Engineering, Wright State University, Dayton, OH, USA; Department of Electrical Engineering, Wright State University, Dayton, OH, USA; Sensors Directorate, Air Force Research Lab, Wright-Patterson AFB, OH, USA","IEEE Photonics Journal","12 Jun 2019","2019","11","3","1","10","Radar-based automatic target recognition (ATR) experiments rely on accurate and repeatable synthetic aperture radar (SAR) measurements performed in an anechoic chamber. Yet, the chamber poses challenges to joint electro-optic (EO) and SAR experiments. Here, we present an open-room approach that is suitable for simultaneous EO collection. The local measurement technique, data processing, and SAR ATR feature extraction are discussed with emphasis on EO fusion and the various tradeoffs at each stage. Examples of inverse SAR imagery at Ka-band demonstrate the relatively inexpensive, fast, and repeatable measurements. Finally, candidate EO fusion experiments are discussed.","1943-0655","","10.1109/JPHOT.2019.2911489","Sensors Directorate, U.S. Air Force Research Lab.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693631","EO/SAR fusion experiments;automatic target recognition (ATR);synthetic aperture radar (SAR);vector network analyzer (VNA);near-field;non-anechoic chamber","Synthetic aperture radar;Pollution measurement;Atmospheric modeling;Antennas;Sensors;Data acquisition","anechoic chambers (electromagnetic);electro-optical devices;feature extraction;image fusion;radar imaging;radar target recognition;synthetic aperture radar","radar-based automatic target recognition experiments;anechoic chamber;open-room approach;simultaneous EO collection;local measurement technique;SAR ATR feature extraction;inverse SAR imagery;candidate EO fusion experiments;ISAR lab experiments;EO-SAR ATR;joint electro-optic experiment;data processing;Ka-band;repeatable synthetic aperture radar measurements","","1","","18","OAPA","17 Apr 2019","","","IEEE","IEEE Journals"
"Ship Detection in SAR Images Based on Adjacent Context Guide Fusion Module and Dense Weighted Skip Connection","W. Shi; Z. Hu; H. Liu; S. Cen; J. Huang; X. Chen","School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China; School of Electrical Engineering, Guangxi University, Nanning, China","IEEE Access","30 Dec 2022","2022","10","","134263","134276","Fusing features from different layers is essential to improve the ship target detection ability in the synthetic aperture radar (SAR) images. Mainstream methods usually perform simple addition or concatenation operations on adjacent feature layers without properly merging their semantic and spatial information, whereas the traditional skip connections are unable to explore sufficient information by the same scale. To address these issues, a ship detection network based on adjacent context guide fusion module and dense weighted skip connection (AFDN) in SAR images is proposed: Adjacent context guide fusion module is specially designed to capture the long-range dependencies of high-level features as weights to multiply with low-level features to fuse adjacent features more efficiently. Furthermore, the dual-path enhanced pyramid is constructed to refine and fuse multi-scale features. Finally, a dense weighted skip connection is proposed by weighted fusion of features of all sizes before the decoder to enrich the feature space. Our anchor-free AFDN outputs the spatial density map and clusters to obtain the rotatable bounding box. The experimental results indicate that the method proposed in this paper surpasses previous ship detection methods and achieves high accuracy on SSDD and AIRSARShip-1.0 datasets.","2169-3536","","10.1109/ACCESS.2022.3230140","National Natural Science Foundation of China(grant numbers:62061002); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991152","Adjacent context guide fusion module;dense weighted skip connection;dual-path enhanced pyramid;spatial density map;ship detection;synthetic aperture radar (SAR)","Synthetic aperture radar;Object detection;Target tracking;Maritime vehicles;Spatial density","feature extraction;image fusion;object detection;radar imaging;ships;synthetic aperture radar","adjacent context guide fusion module;adjacent feature layers;adjacent features;dense weighted skip connection;feature space;high-level features;low-level features;multiscale features;paper surpasses previous ship detection methods;SAR images;ship detection network;ship target detection ability;synthetic aperture radar images;traditional skip connections;weighted fusion","","","","43","CCBYNCND","16 Dec 2022","","","IEEE","IEEE Journals"
"A Novel Shadow and Layover Segmentation Network for Multi-Angle SAR Images Fusion","X. Li; G. Zhang; C. Yin; Y. Wu; X. Shen","Science and Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Unit 95865 of PLA, Beijing, China; Department of Aerospace Science and Technology, Space Engineering University, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China","IEEE Access","15 Nov 2022","2022","10","","117770","117781","Shadow and layover are geometric distortion phenomenons in side-view imaging synthetic aperture radar (SAR) systems, especially in mountainous areas and densely populated urban areas. The shadow can block the target of the observation area, making it impossible to obtain the scattering characteristics of the target. The layover causes phase distortion and alters target characteristics. Shadow and layover severely hinder the interpretation of SAR images. To confront the above problems, a multi-angle fusion algorithm based on unsupervised progressive segmentation network is proposed. Firstly, inspired by mega-constellations of low earth orbit, a spaceborne SAR collaborative observation model is proposed to generate multi-angle images of fluctuant terrain. Secondly, according to the difference of echos in the shadow and layover regions, an unsupervised progressive segmentation network is designed to sequentially segment the shadow and layover regions. Finally, to improve the contrast and brightness of the fused SAR image, a single-scale weighted fusion algorithm is designed. Experiments were conducted using the simulated multi-angle SAR images. Compared with single-angle images, the accuracy of target detection and figure-of-merit of the fused SAR image are significantly higher than those of other methods.","2169-3536","","10.1109/ACCESS.2022.3217510","Chinese Academy of Sciences(grant numbers:E1YD5906,E2YD590901); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931024","Shadow and layover;synthetic aperture radar;unsupervised progressive segmentation network;multi-angle fusion","Radar polarimetry;Image segmentation;Synthetic aperture radar;Feature extraction;Convolutional neural networks;Fuses;Satellites;Unsupervised learning","geophysical signal processing;image fusion;radar imaging;synthetic aperture radar","alters target characteristics;fused SAR image;geometric distortion phenomenons;layover regions;layover segmentation network;mountainous areas;multiangle fusion algorithm;multiangle images;novel shadow;observation area;phase distortion;scattering characteristics;side-view imaging synthetic aperture radar systems;simulated multiangle SAR images;single-angle images;single-scale weighted fusion algorithm;spaceborne SAR collaborative observation model;unsupervised progressive segmentation network;urban areas","","","","38","CCBY","26 Oct 2022","","","IEEE","IEEE Journals"
"SAR Image Change Detection Based on Joint Dictionary Learning With Iterative Adaptive Threshold Optimization","Q. Yu; M. Zhang; L. Yu; R. Wang; J. Xiao","School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China; School of Electronic Information, Wuhan University, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Jul 2022","2022","15","","5234","5249","Synthetic aperture radar (SAR) image change detection is still a challenge due to inherent speckle noise and scarce datasets. This article proposes a joint-related dictionary learning algorithm based on the k-singular value decomposition (K-SVD) algorithm called JR-KSVD and an iterative adaptive threshold optimization (IATO) algorithm for unsupervised change detection. The JR-KSVD algorithm adds dictionary correlation learning to the K-SVD algorithm to generate a uniform initial dictionary for dual-temporal SAR images, thereby reducing the instability of sparse representations due to atomic correlations and enhancing the extraction of image edges and details. The IATO approach employs thresholds obtained by the “difference-log ratio” fusion image for indefinite residual energy minimization iterations to gradually shrink the threshold variation range and finally generate the change images, which have a high degree of adaptivity and strong real-time performance. Finally, experiments on six real datasets demonstrate that the proposed algorithm exhibits superior detection performance and robustness against some state-of-the-art algorithms.","2151-1535","","10.1109/JSTARS.2022.3187108","Open Project Program Foundation of the Key Laboratory of Opto-Electronics Information Processing; Chinese Academy of Sciences(grant numbers:OEIP-O-202009); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810349","Change detection;difference-log ratio image;iterative adaptive threshold;joint-related dictionary learning;synthetic aperture radar (SAR) image","Radar polarimetry;Dictionaries;Machine learning;Image reconstruction;Change detection algorithms;Synthetic aperture radar;Matching pursuit algorithms","feature extraction;image fusion;image representation;iterative methods;optimisation;radar imaging;singular value decomposition;speckle;synthetic aperture radar","related dictionary learning algorithm;dual temporal SAR images;difference log ratio fusion image;K-SVD algorithm;JR-KSVD algorithm;unsupervised change detection;iterative adaptive threshold optimization algorithm;k-singular value decomposition algorithm;scarce datasets;speckle noise;synthetic aperture radar image change detection;SAR image change detection;detection performance;real-time performance;threshold variation range;indefinite residual energy minimization iterations;IATO approach;image edge extraction;atomic correlations","","","","47","CCBY","29 Jun 2022","","","IEEE","IEEE Journals"
"MetaBoost: A Novel Heterogeneous DCNNs Ensemble Network With Two-Stage Filtration for SAR Ship Classification","H. Zheng; Z. Hu; J. Liu; Y. Huang; M. Zheng","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China","IEEE Geoscience and Remote Sensing Letters","16 Jun 2022","2022","19","","1","5","Current synthetic aperture radar (SAR) ship classification research mainly focuses on modifying deep convolutional neural networks (DCNNs) and injecting manual features on DCNNs. Yet, the weak robustness of individual models in high-risk scenarios makes it difficult to gain the trust of SAR experts. In this letter, an automated method of heterogeneous DCNNs model ensemble based on two-stage filtration (MetaBoost) is proposed, effectively achieving robustness and high accuracy recognition on SAR ship classification. The principle of MetaBoost is generating a pool of diverse heterogeneous classifiers, selecting a subset of the most diverse and accurate classifiers, and finally fusing meta-features from the optimal subset. MetaBoost is a self-configuring algorithm that automatically determines the optimal type and the number of base classifiers to be combined. Extensive experiments on the OpenSARShip and FUSAR-Ship datasets show that MetaBoost significantly outperforms individual classifiers, traditional ensemble models, and feature injection techniques.","1558-0571","","10.1109/LGRS.2022.3180793","Natural Science Foundation of China(grant numbers:62172442); Youth Science Foundation of Natural Science Foundation of Hunan Province(grant numbers:2020JJ5775); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791346","Deep learning;heterogeneous ensemble;synthetic aperture radar (SAR);SAR ship classification","Synthetic aperture radar;Marine vehicles;Training;Manuals;Diversity reception;Robustness;Fuses","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);neural nets;pattern classification;radar imaging;remote sensing by radar;ships;synthetic aperture radar","diverse heterogeneous classifiers;diverse classifiers;accurate classifiers;MetaBoost;base classifiers;FUSAR-Ship datasets;traditional ensemble models;novel heterogeneous DCNNs ensemble network;two-stage filtration;SAR Ship classification;current synthetic aperture radar ship classification research;deep convolutional neural networks;manual features;weak robustness;high-risk scenarios;SAR experts;heterogeneous DCNNs model ensemble;high accuracy recognition;SAR ship classification","","","","18","IEEE","8 Jun 2022","","","IEEE","IEEE Journals"
"Deep Shearlet Network for Change Detection in SAR Images","H. Dong; L. Jiao; W. Ma; F. Liu; X. Liu; L. Li; S. Yang","Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China; Key Laboratory of Intelligent Perception and Image Understanding of the Ministry of Education, the International Research Center for Intelligent Perception and Computation, the Joint International Research Laboratory of Intelligent Perception and Computation, and the School of Artificial Intelligence, Xidian University, Xi’an, Shaanxi, China","IEEE Transactions on Geoscience and Remote Sensing","28 Dec 2022","2022","60","","1","15","Convolutional neural networks (CNNs) can extract shift-invariant features and have been widely applied in the change detection task. However, common CNN lacks noise robustness and needs supervised data to alleviate these problems; in this article, we propose a novel deep shearlet convolutional neural network (ShearNet) for change detection in synthetic aperture radar (SAR) images. In the network, a shearlet denoising layer (SDL) is designed to enhance the representation ability of common CNN. In SDL, feature maps are decomposed into subband coefficients by shearlet transform (ST). Due to the optimal sparse representation property and high direction sensitivity of ST, the network can capture important geometric information. Then, hard-threshold shrinkage is applied to high-frequency subbands to drop small coefficients that are most likely to be noise so that reducing the effect of noise. Finally, ShearNet is trained by introducing a noise-robust loss with noisy labels. The noisy labels are obtained by deep clustering that shows more robustness than existing preclassification methods. This fine-tuning process novelly follows the paradigm of learning from noisy labels to aside the difficulty of precisely labeling samples. Our experimental results on multiple real SAR datasets show that ShearNet can boost accuracy and have better applicability for change detection in SAR images. The source code is available at https://github.com/yizhilanmaodhh/ShearNet.","1558-0644","","10.1109/TGRS.2022.3228776","Key Scientific Technological Innovation Research Project by Ministry of Education; National Natural Science Foundation of China Innovation Research Group Fund(grant numbers:61621005); State Key Program and the Foundation for Innovative Research Groups of the National Natural Science Foundation of China(grant numbers:61836009); Major Research Plan of the National Natural Science Foundation of China(grant numbers:91438201,91438103,91838303); National Natural Science Foundation of China(grant numbers:U1701267,62076192,62006177,61902298,61573267,61906150,62276199); Higher Education Discipline Innovation Project; Program for Cheung Kong Scholars and Innovative Research Team in University(grant numbers:IRT 15R53); ST Innovation Project from the Chinese Ministry of Education; Key Research and Development Program in Shaanxi Province of China(grant numbers:2019ZDLGY03-06); National Science Basic Research Plan in Shaanxi Province of China(grant numbers:2019JQ-659,2022JQ-607); China Postdoctoral Fund(grant numbers:2022T150506); Scientific Research Project of Education Department in Shaanxi Province of China(grant numbers:20JY023); Fundamental Research Funds for the Central Universities(grant numbers:XJS201901,XJS201903,JBF201905,JB211908); CAAI-Huawei MindSpore Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982693","Convolutional neural networks (CNNs);deep clustering;noise-robustness loss;shearlet transform (ST);synthetic aperture radar (SAR) image change detection","Feature extraction;Convolutional neural networks;Radar polarimetry;Task analysis;Synthetic aperture radar;Noise measurement;Speckle","feature extraction;image classification;image denoising;image enhancement;image fusion;image representation;learning (artificial intelligence);neural nets;radar imaging;synthetic aperture radar;transforms;wavelet transforms","change detection task;common CNN;convolutional neural networks;deep clustering;deep shearlet convolutional neural network;deep shearlet network;feature maps;high direction sensitivity;high-frequency subbands;noise robustness;noise-robust loss;noisy labels;optimal sparse representation property;representation ability;SAR images;SDL;ShearNet;shift-invariant features;ST;subband coefficients;supervised data;synthetic aperture radar images","","","","62","IEEE","12 Dec 2022","","","IEEE","IEEE Journals"
"Fractional Floodwater-Pixel Fusion for Emergency Response Using ALOS-2 and Sentinel-1 Data","Y. -J. Kwak; R. Pelich","International Centre for Water Hazard and Risk Management (ICHARM-UNESCO), Public Works Research Institute (PWRI), Ibaraki, Japan; Department of Environmental Research and Innovation (ERIN), Luxembourg Institute of Science and Technology (LIST), Belvaux, Luxembourg","2019 IEEE Aerospace Conference","20 Jun 2019","2019","","","1","8","Emergency hazard and risk mapping services are crucial in providing rapid-response information for disaster management and reducing damage of water-driven cascading disasters such as floods. For rapid-response flood mapping, this study introduces an improved flood detection algorithm to fuse two different C-band and L-band Synthetic Aperture Radar (SAR)images acquired over the same area of interest, e.g., mega-floodplain. The main objective of this study is to propose a new algorithm for dynamic flood detection using two different SAR images acquired at different times and under different conditions. As an image fusion technique, we propose an improved floodwater detection algorithm using the pixel-based water fractional fusion and wavelet-based image fusion. This approach allows investigating the optimization of fusion parameters from two different products in the case of a pre-monsoon flash flood. This preliminary study was conducted to identify and estimate large-scale flood inundation area over a challenging area, where flash flood events often occur along the Maghna River in northen Bangladesh. The resultant fused map of the maximum flood-detect extent suggested the possibility of a rapid and accurate floodwater detection to identify the flood location and extent area with the validation data from ground-truth survey in the representative experiment sites.","1095-323X","978-1-5386-6854-2","10.1109/AERO.2019.8741708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741708","Floodwater fraction;SAR;flash flood;emergency response;wavelet image fusion","Floods;Emergency services;Synthetic aperture radar;Rivers;Risk management;Spatial resolution","disasters;floods;hazards;hydrological techniques;image fusion;radar imaging;rivers;synthetic aperture radar","C-band;L-band Synthetic Aperture Radarimages;dynamic flood detection;image fusion technique;improved floodwater detection algorithm;pixel-based water fractional fusion;wavelet-based image fusion;fusion parameters;pre-monsoon flash flood;large-scale flood inundation area;flash flood events;maximum flood-detect extent;rapid floodwater detection;accurate floodwater detection;flood location;extent area;fractional floodwater-pixel fusion;emergency response;ALOS-2;Sentinel-1 data;risk mapping services;rapid-response information;disaster management;water-driven cascading disasters;rapid-response flood mapping;improved flood detection algorithm;SAR images;damage reduction;emergency hazard;Maghna River;northen Bangladesh","","1","","15","IEEE","20 Jun 2019","","","IEEE","IEEE Conferences"
"Space-borne tri-band SAR for ship detection in medium and high sea conditions","T. Zhang; Z. Xia; Z. Zhao; D. Liu; Y. Zhang; H. Shi; F. Yue; T. Peng","State Key Laboratory of Space-Ground Integrated Information Technology, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, Beijing, China","IET International Radar Conference (IET IRC 2020)","22 Sep 2021","2020","2020","","1255","1260","For ship target detection applications under complex sea conditions, the ship/sea contrast of traditional single-band spaceborne Synthetic Aperture Radar (SAR) is low, and the ship target detection performance decreases sharply. In this paper, a concept of spaceborne tri-band common caliber SAR system is proposed. The antenna technology of tri-band common caliber SAR is adopted to reduce the antenna area and weight, and the electromagnetic scattering information of L, S and X bands can be obtained simultaneously. This paper introduces the tri-band SAR echo information joint processing method. Firstly, the joint processing from the perspective of SAR signal is carried out, and quickly detect the ship target location, then small scene imaging of target area is obtained. After that, through the tri-band SAR image fusion processing, further enhance the Signal-Clutter-Noise Ratio (SCNR) and outline of the ship target information, and improve the ship target detection performance of complex sea condition.","","","10.1049/icp.2021.0666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9545697","","","electromagnetic wave scattering;image fusion;microwave antennas;microwave imaging;multifrequency antennas;object detection;radar antennas;radar detection;radar imaging;ships;spaceborne radar;synthetic aperture radar;UHF antennas","ship target detection performance improvement;signal-clutter-noise ratio;small scene imaging;medium sea conditions;spaceborne triband common caliber SAR system;triband SAR image fusion processing;ship target information;ship target location;SAR signal;triband SAR echo information joint processing method;electromagnetic scattering information;antenna area reduction;synthetic aperture radar;complex sea condition;ship target detection applications;high sea conditions;frequency 1 GHz to 4 GHz;frequency 8 GHz to 12 GHz","","","","","","22 Sep 2021","","","IET","IET Conferences"
"A Gaussian Process Regression Approach for Fusion of Remote Sensing Images for Oil Spill Segmentation","F. S. Longman; L. Mihaylova; L. Yang","Department of Automatic Control and System Engineering, University of Sheffield, UK; Department of Automatic Control and System Engineering, University of Sheffield, UK; Department of Electrical and Computer Engineering, University of Canterbury, Chrischurch, New Zealand","2018 21st International Conference on Information Fusion (FUSION)","6 Sep 2018","2018","","","62","69","Synthetic Aperture Radar (SAR) satellite systems are very efficient in oil spill monitoring due to their capability to operate under all weather conditions. This paper presents a framework using Gaussian process (GP) to fuse SAR images of different modalities and to segment dark areas (assumed oil spill) for oil spill detection. A new covariance function; a product of an intrinsically sparse kernel and a Rational Quadratic Kernel (RQK) is used to model the prior of the estimated image allowing information to be transferred. The accuracy performance evaluation demonstrates that the proposed framework has 37% less RMSE per pixel and a compelling enhancement visually when compared with existing methods.","","978-0-9964527-6-2","10.23919/ICIF.2018.8455304","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455304","Oil Spill;Synthetic Aperture Radar (SAR);Registration;Image Fusion;Segmentation;Gaussian Processes","Oils;Kernel;Synthetic aperture radar;Image resolution;Gaussian processes;Feature extraction;Image segmentation","Gaussian processes;geophysical image processing;image segmentation;radar imaging;regression analysis;remote sensing;synthetic aperture radar","gaussian process regression approach;remote sensing images;oil spill segmentation;Synthetic Aperture Radar satellite systems;oil spill monitoring;weather conditions;SAR images;Rational Quadratic Kernel;intrinsically sparse kernel;covariance function;oil spill detection","","1","","35","","6 Sep 2018","","","IEEE","IEEE Conferences"
"Fusing Landsat-8, Sentinel-1, and Sentinel-2 Data for River Water Mapping Using Multidimensional Weighted Fusion Method","Q. Liu; S. Zhang; N. Wang; Y. Ming; C. Huang","College of Urban and Environmental Sciences and the Shaanxi Key Laboratory of Earth Surface System and Environmental Carrying Capacity, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China; College of Urban and Environmental Sciences and the Shaanxi Key Laboratory of Earth Surface System and Environmental Carrying Capacity, Northwest University, Xi’an, China; Institute of Earth Surface System and Hazards, Northwest University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","12 Jul 2022","2022","60","","1","12","River water extent is critical for understanding river discharge or its hydrological conditions. Although numerous methods have been proposed to map river water from either optical or synthetic aperture radar (SAR) remotely sensed images, uncertainties still exist broadly. In this study, we developed an image fusion method that integrates Landsat-8, Sentinel-1, and Sentinel-2 images simultaneously for river water mapping with two major steps. Firstly, a posterior probability support vector machine (SVM) model was adopted to generate water probability maps from each individual image; and second, a multidimensional weighted fusion method (MDWFM) was developed to fuse these probability maps. Four reaches with different characteristics were selected as case study sites. High-resolution aerial images were acquired and used as the reference to evaluate our results. We found that the fusion process not only improves the quality of river water mapping but also excludes the cloud interference. The fused river water maps become more reliable after the conflicts from difference images being solved by the proposed MDWFM method that contains a proportional conflict redistribution rule. The weighted root mean square difference was reduced to 0.066, and the area under the ROC curve reached up to 0.984. The critical success index (CSI), kappa coefficient (KC), and F-measure reached up to 0.810, 0.836, and 0.895, respectively. These stable and accurate river extent mapping results obtained through fusing multiple images with high spatial resolution (SR) (10 m) and short revisit interval (0.4–4.4 days) are of great significance for enriching the data and methodology of hydrological studies.","1558-0644","","10.1109/TGRS.2022.3187154","National Natural Science Foundation of China(grant numbers:U2243205); National Key Research and Development Program of China(grant numbers:2019YFC1510503); Shaanxi Natural Science Foundation(grant numbers:2021JM-314); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810288","Image fusion;multidimensional weighted fusion;posterior probability;support vector machine (SVM);synthetic aperture radar (SAR)","Rivers;Earth;Remote sensing;Artificial satellites;Uncertainty;Indexes;Fuses","geophysical image processing;hydrological techniques;image classification;image fusion;radar imaging;remote sensing by radar;rivers;support vector machines;synthetic aperture radar","Landsat-8;Sentinel-1;Sentinel-2 data;river water mapping;multidimensional weighted fusion method;river water extent;river discharge;image fusion method;Sentinel-2 images;posterior probability support vector machine model;water probability maps;high-resolution aerial images;fused river water maps;difference images;stable river extent;synthetic aperture radar remotely sensed images;support vector machine model;cloud interference;critical success index;kappa coefficient;F-measure;ROC curve;spatial resolution;hydrological studies;redistribution rule","","","","90","IEEE","29 Jun 2022","","","IEEE","IEEE Journals"
"A Comparative Study on Image Registration Techniques for SAR Images","G. Sreeja; O. Saraniya","Department of ECE, Government College of Technology, Coimbatore, India; Department of ECE, Government College of Technology, Coimbatore, India","2019 5th International Conference on Advanced Computing & Communication Systems (ICACCS)","6 Jun 2019","2019","","","947","953","High resolution Synthetic Aperture Radar (SAR) images are extensively employed in many applications like object tracking, object detection, image fusion, image mosaicing. Image registration is mandatory process for all these applications. To register SAR images, feature based registration methods have been successfully deployed in recent years. State of art detectors like Harris Corner, SIFT, SURF, BRIEF, ORB, etc. have been applied to align SAR images. Among all feature detectors, SIFT and SURF algorithm proved to give better solutions for the SAR image registration problem due to its invariance and robustness. So in this paper, the attempt is made to give a detailed survey of SIFT and SURF based SAR image alignment.","2575-7288","978-1-5386-9533-3","10.1109/ICACCS.2019.8728390","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8728390","Synthetic aperture radar (SAR);Image registration;Feature descriptors;SIFT;SURF;Feature Matching.","Feature extraction;Radar polarimetry;Detectors;Synthetic aperture radar;Image edge detection;Optical filters;Feature detection","image registration;object detection;object tracking;synthetic aperture radar","image registration techniques;High resolution Synthetic Aperture Radar images;object tracking;object detection;image fusion;image mosaicing;feature based registration methods;SAR image registration problem;SAR image alignment","","2","","38","IEEE","6 Jun 2019","","","IEEE","IEEE Conferences"
"A Novel Recognition Method of ISAR Image Fusion Based on Botnet and Embracenet","W. Lin; X. Gao; Q. Shen","College of Electronic Science and Technology, National University of Defence Technology, Changsha, China; College of Electronic Science and Technology, National University of Defence Technology, Changsha, China; College of Electronic Science and Technology, National University of Defence Technology, Changsha, China","2021 IEEE 6th International Conference on Signal and Image Processing (ICSIP)","28 Jan 2022","2021","","","841","845","Compared with single ISAR image recognition, multiple ISAR images contain more target information, which is more conducive to target recognition. Simultaneously considering the non-cooperation and complexity of target motion, ISAR image usually has low-resolution or even cannot be generated, this paper proposes a feature fusion model for ISAR images which takes account of both recognition rate and robustness. In this model, feature information is extracted based on Botnet and multi-sensor features are fused based on Embracenet. The experimental results based on the simulation data set show that in the five types of target recognition tasks under different elevation angles, the classification accuracy of the proposed method based on the fusion features of two ISAR images is better than that of single ISAR image. In the absence of one ISAR image, the recognition rate decreased slightly, and was still higher than that of the single ISAR image method.","","978-1-6654-3904-6","10.1109/ICSIP52628.2021.9688885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9688885","Multi-sensor fusion;Feature fusion;ISAR target recognition;BotNet;EmbraceNet","Image recognition;Target recognition;Botnet;Radar;Radar imaging;Feature extraction;Data models","feature extraction;image fusion;image recognition;image resolution;radar imaging;sensor fusion;synthetic aperture radar","Botnet;multisensor features;Embracenet;target recognition tasks;fusion features;one ISAR image;recognition rate;single ISAR image method;novel recognition method;ISAR image fusion;single ISAR image recognition;multiple ISAR images;target information;target motion;feature fusion model","","","","15","IEEE","28 Jan 2022","","","IEEE","IEEE Conferences"
"On the Fusion Strategies of Sentinel-1 and Sentinel-2 Data for Local Climate Zone Classification","J. Gawlikowski; M. Schmitt; A. Kruspe; X. X. Zhu","Institute of Data Science, German Aerospace Center (DLR), Jena, Germany; Signal Processing in Earth Observation, Technical University of Munich (TUM), Munich, Germany; Institute of Data Science, German Aerospace Center (DLR), Jena, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Germany","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2081","2084","Local Climate Zone (LCZ) classification is the most commonly used scheme to analyze how local urban morphology affects the climate of local areas. Classification methods are often based on remote sensing data or on a fusion of several data sources. In this study, the effects of different fusion strategies of optical and synthetic aperture radar (SAR) data on the accuracy of LCZ classifications are investigated. The data processing is implemented with a convolutional neural network (CNN), where until a fusion layer, separate data sources are processed separately on branches. Strategies of splitting the data into branches and the effects of different fusion stages are compared, together with approaches based on sums of independent classifiers. For our setting, the stage of fusion does not seem to have a big influence on the accuracy. The results of this study contribute to a better understanding of cooperative usage of multispectral and SAR data.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324234","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324234","Local Climate Zone Classification;Data Fusion;Fusion Network","Training;Meteorology;Synthetic aperture radar;Optical imaging;Task analysis;Remote sensing;Vegetation mapping","convolutional neural nets;geophysical image processing;image classification;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","multispectral SAR data;fusion stages;data sources;fusion layer;convolutional neural network;data processing;LCZ classifications;fusion strategies;remote sensing data;classification methods;local urban morphology;local climate zone classification;sentinel-2 data;sentinel-1","","7","","10","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"A Dynamic End-to-End Fusion Filter for Local Climate Zone Classification Using SAR and Multi-Spectrum Remote Sensing Data","P. Feng; Y. Lin; G. He; J. Guan; J. Wang; H. Shi","State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; Group of Intelligent Signal Processing, Harbin Engineering University, Harbin, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; Group of Intelligent Signal Processing, Harbin Engineering University, Harbin, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China; State Key Laboratory of Space-Ground Integrated Information Technology, CAST, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","4231","4234","Local Climate Zone (LCZ) classification is potentially popular because of its extensive applications. Recently, data from different remote sensors including synthetic aperture radar (SAR) and multi-spectrum are employed for LCZ classification. However, different bands in SAR and multi-spectrum are difficult to fuse because of their various physical properties. In this paper, an dynamic end-to-end fusion filter is proposed. Firstly, a convolutional neural network (CNN) based dynamic filter network (DFN) is introduced to integrate different bands in SAR and multi-spectrum data, which enhances the fusion accuracy by a flexible dynamic operation. Then the filter is used for feature extraction, hence improve the performance of the classifier. The proposed method is evaluated using Sentinel-1 and Sentinel-2 dataset and the improvement of accuracy shows the superiority of the proposed dynamic data fusion approach.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324427","National Natural Science Foundation of China(grant numbers:61806018,41801291); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324427","Dynamic filter network;Local Climate Zone Classification;Multi-spectrum;End-to-end;Fusion","Feature extraction;Synthetic aperture radar;Meteorology;Remote sensing;Fuses;Convolution;Vegetation","atmospheric techniques;atmospheric temperature;convolutional neural nets;feature extraction;geophysical image processing;image classification;image filtering;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","SAR;LCZ classification;dynamic end-to-end fusion filter;dynamic filter network;flexible dynamic operation;dynamic data fusion approach;multispectrum remote sensing data;Local Climate Zone classification;remote sensors;feature extraction;Sentinel-1 and Sentinel-2 dataset","","1","","13","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Flood Mapping with SAR and Multi-Spectral Remote Sensing Images Based on Weighted Evidential Fusion","X. Chen; Y. Cui; C. Wen; M. Zheng; Y. Gao; J. Li","School of Earth and Space Sciences, Peking University, Beijing, China; School of Earth and Space Sciences, Peking University, Beijing, China; Information Center of Ministry of Civil Affairs of the People's Republic of China, Beijing, China; Information Center of Ministry of Civil Affairs of the People's Republic of China, Beijing, China; Information Center of Ministry of Civil Affairs of the People's Republic of China, Beijing, China; Faculty of Geographical Science, Beijing Normal University, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2519","2522","Synthetic Aperture Radar (SAR) and Multi-spectral (MS) remote sensing images are commonly used for flood mapping. SAR images can provide valid backscattering measurements of inundated areas through cloud cover, while MS data is able to monitor the spectral changes of ground surface, but usually affected by clouds. The complementary characteristics of the two data indicate the potential of their combining application for flood monitoring in emergency. This paper proposes a novel weighted evidential fusion method to take full advantages of the SAR and MS data for change detection during the flood. First, pre-processing and classification are performed with the SAR and MS data, independently. Second, a modified PCR6 rule for evidential fusion is proposed, which introduces the confusion matrixes to calculate the weight of evidences so that the conflicting degree in the fusion process can be reduced. Then, the flood inundating, standing and receding patterns are identified, which can be used to describe the flooding process in details. Practically, the proposed method is applied to flood mapping of the Typhoon Rumbia in 2018, in Shouguang City, China. The experiments show that the proposed fusion scheme efficiently use both of the SAR and MS data, and improve the flood mapping accuracy.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9324158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324158","SAR;multi-spectral;change detection;evidential fusion;flood monitoring;remote sensing","Floods;Synthetic aperture radar;Remote sensing;Land surface;Earth;Radar polarimetry;Monitoring","floods;geophysical image processing;hydrological techniques;image classification;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar","modified PCR6 rule;confusion matrixes;AD 2018;Typhoon Rumbia;China;Shouguang City;flooding process;flood inundating;fusion process;evidential fusion method;flood monitoring;spectral changes;cloud cover;backscattering measurements;SAR images;multispectral remote sensing images;flood mapping accuracy;MS data;fusion scheme","","","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Video SAR Image Fusion Using the Effective Reflection Coefficient","D. Song; R. Tharmarasa; K. Han; G. Li; T. Kirubarajana; M. McDonald","College of Information and Communication, National University of Defense Technology, Xi'an, China; Department of Electrical and Computer Engineering, McMaster University, Hamilton, Canada; College of Information and Communication, National University of Defense Technology, Xi'an, China; College of Information and Communication, National University of Defense Technology, Xi'an, China; Department of Electrical and Computer Engineering, McMaster University, Hamilton, Canada; Defence Research and Development Canada, Ottawa, Canada","2021 CIE International Conference on Radar (Radar)","8 Feb 2023","2021","","","1200","1204","The video synthetic aperture radar (ViSAR), as a mode for sensing in an ever-wider synthetic aperture by sequentially forming SAR images on a series of contiguous or overlapping sub-apertures, has the promising capability to capture wide-angle scattering behavior of objects. In this paper, a novel method for ViSAR image fusion is proposed. First, the ViSAR imaging mode is analyzed and modeled. Based on this model, the reflection coefficient (RC) is then estimated and the significant RC is detected in each sub-aperture image. These detection results are then fused across sub-aperture images to detect scatterers in the area of interest. The performance of the proposed method is evaluated using a simulated scenario and compared with that of the conventional ViSAR image fusion method based on the generalized likelihood ratio test. Numerical results demonstrate the advantages of the proposed method in capturing the aspect-dependent scattering characteristics as well as the spatial structure of objects.","2640-7736","978-1-6654-9814-2","10.1109/Radar53847.2021.10028338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10028338","Aspect-dependent scattering;video synthetic aperture radar (ViSAR);image fusion","Scattering;Imaging;Radar scattering;Radar polarimetry;Reflection;Reflection coefficient;Sensors","","","","","","16","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"Disaster risk reduction using image fusion of optical and SAR data before and after tsunami","Y. Kwak; A. Yorozuya; Y. Iwami","Dokuritsu Gyosei Hojin Doboku Kenkyujo, Tsukuba, Ibaraki, JP; Dokuritsu Gyosei Hojin Doboku Kenkyujo, Tsukuba, Ibaraki, JP; Dokuritsu Gyosei Hojin Doboku Kenkyujo, Tsukuba, Ibaraki, JP","2016 IEEE Aerospace Conference","30 Jun 2016","2016","","","1","11","This study applied supervised change detection to identify and estimate damaged urban surface conditions before and after a tsunami event in order to provide more accurate information for the implementation and enhancement of disaster risk reduction policies and strategies. Advanced remote sensing is crucial to support cost-effective emergency response activities for disaster risk assessment and management. This preliminary study, as an effort to propose a good case study in risk management, suggested that three main steps, i.e., filtering, fusing and classifying, should be adopted to perform change detection before and after a natural disaster. We fused very high-spatial-resolution multi-temporal optical images (0.6 m spatial resolution) and X-band SAR images (2.5 m spatial resolution). The study also revealed that the decision-level method, i.e. morphological transform, was the most promising in image fusion of filtered images to classify urban surfaces in tsunami damage assessment, compared with the pixel-level method, i.e. wavelet transform, and feature-level method, i.e. segmentation extraction. This paper reports, coupled with the results from the image fusion, that the preliminary results are good enough to obtain urban impervious surface estimation of a wide disaster risk area but not good enough to make clear amplitude images to identify individual buildings of dwelling zone. The proposed normalized change index (NCI), an important indicator for detecting changes, was found capable of providing better estimation of urban impervious surfaces, such as transport-related land (e.g., bridges and parking lots) and building roof tops in residential and industrial areas over a coastal zone in Rikuzen-takada City, devastated in the 2011 Great East Japan Earthquake.","","978-1-4673-7676-1","10.1109/AERO.2016.7500520","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500520","","Risk management;Tsunami;Synthetic aperture radar;Earthquakes;Optical imaging;Optical sensors;Optical surface waves","disasters;earthquakes;geophysical image processing;image classification;image filtering;image fusion;image resolution;remote sensing by radar;risk management;synthetic aperture radar;tsunami","image fusion;optical data;SAR data;supervised change detection;damaged urban surface conditions;tsunami event;disaster risk reduction policies;disaster risk reduction strategies;remote sensing;cost-effective emergency response activities;disaster risk assessment;disaster risk management;natural disaster;high-spatial-resolution multitemporal optical images;spatial resolution;X-band SAR images;decision- level method;morphological transform;filtered images;urban surfaces;tsunami damage assessment;pixel-level method;wavelet transform;feature-level method;segmentation extraction;urban impervious surface estimation;disaster risk area;amplitude images;individual building dwelling zone;normalized change index;urban impervious surfaces;transport-related land;building roof;residential area;industrial area;coastal zone;Rikuzen-takada City;Great East Japan Earthquake;AD 2011","","3","","35","IEEE","30 Jun 2016","","","IEEE","IEEE Conferences"
"A Topological Data Analysis Guided Fusion Algorithm: Mapper-Regularized Manifold Alignment","J. Hu; D. Hong; Y. Wang; X. X. Zhu","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany; Signal Processing in Earth Observation (SiPEO), Technische Universität München (TUM), Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2822","2825","Hyperspectral images and polarimetric synthetic aperture radar (PolSAR) data are two important data sources, yet they barely appear under the same scope, even though multi-modal data fusion is attracting more and more attention. To our best knowledge, this paper investigates for the first time semi-supervised manifold alignment (SSMA) for the fusion of the hyperspectral image and PolSAR data. The SSMA searches a latent space where different data sources are aligned, which is accomplished by using the label information and the topological structure of the data. This paper is the first attempt to apply topological data analysis (TDA), a recent mathematic sub-field of data analysis, in remote sensing. It aims to reveal relevant information from the shape of a data in its feature space, and has been proven powerful in medicine. The paper also proposes a novel algorithm, MAPPER-regularized manifold alignment, which embeds the TDA into a semi-supervised manifold alignment for the fusion of the hyper-spectral image and PolSAR data. The proposed algorithm exhibits superior performance in fusing a simulated EnMAP data set and a Sentinel-1 data set for an image of Berlin.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898471","Classification;data fusion;EnMAP;hyperspectral image;land cover;land use;manifold alignment;MAPPER;PolSAR;semi-supervised learning;Sentinel-1;topological data analysis (TDA)","Manifolds;Hyperspectral imaging;Data integration;Data analysis;Feature extraction","data analysis;hyperspectral imaging;image fusion;mathematical analysis;radar polarimetry;remote sensing by radar;synthetic aperture radar","Sentinel-1 data;topological data analysis guided fusion algorithm;mapper-regularized manifold alignment;polarimetric synthetic aperture radar data;PolSAR;multimodal data fusion;semisupervised manifold alignment;SSMA;hyperspectral image fusion;EnMAP data simulation;TDA;MAPPER-regularized manifold alignment","","1","","17","EU","14 Nov 2019","","","IEEE","IEEE Conferences"
"An image registration method based on the combination of multiple image features","G. -k. Wang; H. -p. Xu; H. Zhang","School of Electronics & Information Engineering, Beihang University, China; School of Electronics & Information Engineering, Beihang University, China; School of Electronics & Information Engineering, Beihang University, China","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","2803","2806","The image registration is one of the key steps to achieve three-dimensional (3D) localization and the other image fusion processes. This article presents a registration method based on the combination of edge feature and corner feature. The processing steps include image segmentation, corner detection, edge detection, extraction of interested region, and correspondence points matching. The algorithm flowchart and implementation of every step are presented. The proposed method makes the image registration with the multiple features of two images and avoids the inaccuracy of the SAR image information. Therefore, it has a better robustness and registration precision. A TerraSAR-X SAR image and a GeoEye-1 optical image of the Water Cube in Beijing are processed and the results validate the effectiveness of the proposed method.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729724","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729724","Image registration;combination;multiple image features;corner matching;edge matching","Image edge detection;Optical imaging;Image registration;Image segmentation;Transforms;Feature extraction;Image matching","geophysical techniques;image fusion;image registration;image segmentation;synthetic aperture radar","image registration method;multiple image features;three-dimensional localization;image fusion processes;edge feature;corner feature;image segmentation;corner detection;edge detection;correspondence point matching;algorithm flowchart;SAR image information;synthetic aperture radar;TerraSAR-X SAR image;GeoEye-1 optical image;water cube;Beijing;China","","2","","19","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Kohonen-Based Credal Fusion of Optical and Radar Images for Land Cover Classification","I. Hammami; J. Dezert; G. Mercier","Tunis Sciences Faculty, University of Tunis el Manar, Tunis, Tunisia; ONERA/DTIM/MSDA, The French Aerospace Lab, Palaiseau, France; CTO, eXo maKina, Digital Technologies, Paris, France","2018 21st International Conference on Information Fusion (FUSION)","6 Sep 2018","2018","","","1623","1630","This paper presents a Credal algorithm to perform land cover classification from a pair of optical and radar remote sensing images. SAR (Synthetic Aperture Radar) /optical multispectral information fusion is investigated in this study for making the joint classification. The approach consists of two main steps: 1) relevant features extraction applied to each sensor in order to model the sources of information and 2) a Kohonen map-based estimation of Basic Belief Assignments (BBA) dedicated to heterogeneous data. This framework deals with co-registered images and is able to handle complete optical data as well as optical data affected by missing value due to the presence of clouds and shadows during observation. A pair of SPOT-5 and RADARSAT-2 real images is used in the evaluation, and the proposed experiment in a farming area shows very promising results in terms of classification accuracy and missing optical data reconstruction when some data are hidden by clouds.","","978-0-9964527-6-2","10.23919/ICIF.2018.8455272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455272","Land cover classification;remote sensing;image fusion;Kohonen map;belief functions","Optical imaging;Optical sensors;Adaptive optics;Self-organizing feature maps;Synthetic aperture radar;Laser radar;Feature extraction","feature extraction;geophysical image processing;image classification;image fusion;image reconstruction;image registration;land cover;remote sensing by radar;self-organising feature maps;synthetic aperture radar","Basic Belief Assignments;co-registered images;Kohonen-based Credal fusion;optical data;optical data reconstruction;SAR-optical multispectral information fusion;features extraction;SPOT-5 images;RADARSAT-2 images;Kohonen map-based estimation;Synthetic Aperture Radar;optical radar remote sensing images;Credal algorithm;land cover classification","","2","","29","","6 Sep 2018","","","IEEE","IEEE Conferences"
"Mapping Forest and Their Spatial–Temporal Changes From 2007 to 2015 in Tropical Hainan Island by Integrating ALOS/ALOS-2 L-Band SAR and Landsat Optical Images","B. Chen; X. Xiao; H. Ye; J. Ma; R. Doughty; X. Li; B. Zhao; Z. Wu; R. Sun; J. Dong; Y. Qin; G. Xie","Danzhou Investigation and Experiment Station of Tropical Crops, Chinese Academy of Tropical Agricultural Sciences, Danzhou, China; Department of Microbiology and Plant Biology, and the Center for Spatial Analysis, University of Oklahoma, Norman, OK, USA; Key Laboratory of Digital Earth Science, Chinese Academy of Sciences, Beijing, China; Ministry of Education Key Laboratory for Biodiversity Science and Ecological Engineering, Fudan University, Shanghai, China; Department of Microbiology and Plant Biology, University of Oklahoma, Norman, OK, USA; Ministry of Education Key Laboratory for Biodiversity Science and Ecological Engineering, Fudan University, Shanghai, China; Ministry of Education Key Laboratory for Biodiversity Science and Ecological Engineering, Fudan University, Shanghai, China; Danzhou Investigation and Experiment Station of Tropical Crops, Chinese Academy of Tropical Agricultural Sciences, Danzhou, China; Danzhou Investigation and Experiment Station of Tropical Crops, Chinese Academy of Tropical Agricultural Sciences, Danzhou, China; Department of Microbiology and Plant Biology, University of Oklahoma, Norman, OK, USA; Department of Microbiology and Plant Biology, University of Oklahoma, Norman, OK, USA; Danzhou Investigation and Experiment Station of Tropical Crops, Chinese Academy of Tropical Agricultural Sciences, Danzhou, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Mar 2018","2018","11","3","852","867","Accurately monitoring forest dynamics in the tropical regions is essential for ecological studies and forest management. In this study, images from phase-array L-band synthetic aperture radar (PALSAR), PALSAR-2, and Landsat in 2006-2010 and 2015 were combined to identify tropical forest dynamics on Hainan Island, China. Annual forest maps were first mapped from PALSAR and PALSAR-2 images using structural metrics. Those pixels with a high biomass of sugarcane or banana, which are widely distributed in the tropics and subtropics and have similar structural metrics as forests, were excluded from the SAR-based forest maps by using phenological metrics from time series Landsat imagery. The optical-SAR-based forest maps in 2010 and 2015 had high overall accuracies (OA) of 92-97% when validated with ground reference data. The resultant forest map in 2010 shows good spatial agreement with public optical-based forest maps (OA = 88-90%), and the annual forest maps (2007-2010) were spatiotemporally consistent and more accurate than the PALSAR-based forest map from the Japan Aerospace Exploration Agency (OA = 82% in 2010). The areas of forest gain, loss, and net change on Hainan Island from 2007 to 2015 were 415 000 ha (+2.17% yr-1), 179 000 ha (-0.94% yr -1), and 236 000 ha (+1.23% yr-1), respectively. About 95% of forest gain and loss occurred in those areas with an elevation less than 400 m, where deciduous rubber, eucalyptus plantations, and urbanization expanded rapidly. This study demonstrates the potential of PALSAR/PALSAR-2/Landsat image fusion for monitoring annual forest dynamics in the tropical regions.","2151-1535","","10.1109/JSTARS.2018.2795595","Hainan Provincial Department of Science and Technology(grant numbers:ZDKJ2016021); National Natural Science Foundation of China(grant numbers:41571408,41701510); Earmarked Fund for China Agriculture Research System(grant numbers:CARS-34-GW5); Fundamental Research Funds of Rubber Research Institute; Chinese Academy of Tropical Agricultural Sciences(grant numbers:1630022017016,1630022015012); U.S. NASA Land Use; Land Cover Change program(grant numbers:NNX14AD78G); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8307088","Forest loss and gain;high biomass crops;image data fusion;land surface water index (LSWI);normalized difference vegetation index (NDVI)","Remote sensing;Earth;Artificial satellites;Synthetic aperture radar;Agriculture;Biomedical optical imaging;Optical imaging","geophysical image processing;image classification;image fusion;remote sensing by radar;synthetic aperture radar;time series;vegetation;vegetation mapping","annual forest maps;forest gain;PALSAR/PALSAR-2/Landsat image fusion;annual forest dynamics;tropical regions;mapping forest;spatial-temporal changes;tropical Hainan Island;integrating ALOS/ALOS-2 L-band SAR;Landsat optical images;accurately monitoring forest dynamics;forest management;phase-array L-band synthetic aperture radar;tropical forest dynamics;PALSAR-2 images;subtropics;similar structural metrics;time series Landsat imagery;optical-SAR-based forest maps;forest map","","32","","61","IEEE","6 Mar 2018","","","IEEE","IEEE Journals"
"Structure Consistency-Based Graph for Unsupervised Change Detection With Homogeneous and Heterogeneous Remote Sensing Images","Y. Sun; L. Lei; X. Li; X. Tan; G. Kuang","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","3 Dec 2021","2022","60","","1","21","Change detection (CD) of remote sensing (RS) images is one of the important problems in earth observation, which has been extensively studied in recent years. However, with the development of RS technology, the specific characteristics of remotely sensed images, including sensor characteristics, resolutions, noises, and distortions in imagery, make the CD more complex. In this article, we propose a structure consistency-based method for CD, which detects changes by comparing the structures of two images, rather than comparing the pixel values of images. Because the image structure is imaging modality-invariant and not sensitive to noise, illumination, and other interference factors, the proposed method can be applied to a variety of CD scenarios and has strong robustness. Structural comparison is realized by constructing and mapping an improved nonlocal patch-based graph (NLPG) to avoid the data leakage of two images. First, we demonstrate the effectiveness of the method in homogeneous and heterogeneous CD, which shows that the proposed method can be used as a unified CD framework. Second, we extend the method to the heterogeneous CD with multichannel synthetic aperture radar (SAR) image, which can provide a reference for future research as the heterogeneous CD with multichannel SAR is rarely studied. Third, through the decomposition and in-depth analysis of NLPG, we modify the graph construction process, structure difference calculation, and the difference image fusion to make it more robust and accurate. Experiments on six scenarios 12 data sets demonstrate the effectiveness of the proposed method.","1558-0644","","10.1109/TGRS.2021.3053571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344836","Graph;heterogeneous data;nonlocal similarity;structure consistency;unsupervised change detection (CD)","Synthetic aperture radar;Radar polarimetry;Image resolution;Optical variables measurement;Optical imaging;Adaptive optics;Optical sensors","geographic information systems;geophysical image processing;image fusion;radar imaging;remote sensing;synthetic aperture radar","image structure;illumination;CD scenarios;structural comparison;improved nonlocal patch-based graph;heterogeneous CD;unified CD framework;multichannel synthetic aperture radar image;graph construction process;structure difference calculation;difference image fusion;structure consistency-based graph;unsupervised change detection;heterogeneous remote sensing images;earth observation;RS technology;remotely sensed images;sensor characteristics;structure consistency-based method","","24","","79","IEEE","2 Feb 2021","","","IEEE","IEEE Journals"
"A Fast and Accurate Basis Pursuit Denoising Algorithm With Application to Super-Resolving Tomographic SAR","Y. Shi; X. X. Zhu; W. Yin; R. Bamler","Chair of Remote Sensing Technology, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Department of Mathematics, University of California at Los Angeles, Los Angeles, CA, USA; Chair of Remote Sensing Technology, Technical University of Munich, Munich, Germany","IEEE Transactions on Geoscience and Remote Sensing","26 Sep 2018","2018","56","10","6148","6158"," $L_{1}$  regularization is used for finding sparse solutions to an underdetermined linear system. As sparse signals are widely expected in remote sensing, this type of regularization scheme and its extensions have been widely employed in many remote sensing problems, such as image fusion, target detection, image super-resolution, and others, and have led to promising results. However, solving such sparse reconstruction problems is computationally expensive and has limitations in its practical use. In this paper, we proposed a novel efficient algorithm for solving the complex-valued  $L_{1}$  regularized least squares problem. Taking the high-dimensional tomographic synthetic aperture radar (TomoSAR) as a practical example, we carried out extensive experiments, both with the simulation data and the real data, to demonstrate that the proposed approach can retain the accuracy of the second-order methods while dramatically speeding up the processing by one or two orders. Although we have chosen TomoSAR as the example, the proposed method can be generally applied to any spectral estimation problems.","1558-0644","","10.1109/TGRS.2018.2832721","H2020 European Research Council(grant numbers:ERC-2016-StG-714087); Helmholtz Association(grant numbers:VH-NG-1018); Munich Aerospace e.V. Fakultat fur Luft-und Raumfahrt; Bavaria California Technology Center through the project Large-Scale Problems in Earth Observation; Gauss Centre for Supercomputing e.V.; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8412239","Basis pursuit denoising (BPDN);L₁ regularization;proximal gradient (PG);second-order cone programming (SOCP);TomoSAR","Synthetic aperture radar;Image reconstruction;Image resolution;Signal to noise ratio;Estimation;Remote sensing;Sensors","image denoising;image fusion;image reconstruction;image resolution;least squares approximations;radar imaging;remote sensing by radar;synthetic aperture radar","super-resolving tomographic SAR;underdetermined linear system;sparse signals;remote sensing problems;image fusion;target detection;image super-resolution;sparse reconstruction problems;high-dimensional tomographic synthetic aperture radar;TomoSAR;simulation data;spectral estimation problems;regularized least squares problem","","21","","23","OAPA","17 Jul 2018","","","IEEE","IEEE Journals"
"PolSAR Target Detection via Reflection Symmetry and a Wishart Classifier","M. Gu; H. Liu; Y. Wang; D. Yang","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Access","9 Jun 2020","2020","8","","103317","103326","Detection of man-made targets using polarimetric synthetic aperture radar (PolSAR) data has become a promising research area. The reflection symmetry is gradually being applied to man-made target detection algorithms as a physical property that can distinguish between man-made targets and natural clutter. However, the two terms related to the reflection symmetry property in the polarimetric coherency matrix, namely, the C12 and C23 terms, are not fully exploited by the traditional methods. To fully exploit the polarization information of the two terms, an image fusion strategy based on the position and scale information of the scale-invariant feature transform (SIFT) key points is proposed in this paper. Then, a new Wishart classifier based on the patch-level Wishart distance is used to realize automatic target detection of the fused image. The experimental results on measured data show that the proposed method can enhance the contrast between targets and clutter. In addition, the detection performance of the proposed method under different target-to-clutter ratios (TCRs) are verified on the synthetic data and measured data.","2169-3536","","10.1109/ACCESS.2020.2999472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106405","Target detection;reflection symmetry;polarimetric SAR;complex Wishart distribution","Object detection;Radar polarimetry;Synthetic aperture radar;Marine vehicles;Scattering;Detectors;Clutter","feature extraction;geophysical image processing;image classification;image fusion;object detection;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","PolSAR target detection;Wishart classifier;man-made targets;polarimetric synthetic aperture radar data;target detection algorithms;physical property;natural clutter;reflection symmetry property;polarimetric coherency matrix;polarization information;image fusion strategy;scale information;patch-level Wishart distance;automatic target detection;detection performance;target-to-clutter ratios;synthetic data","","2","","29","CCBY","2 Jun 2020","","","IEEE","IEEE Journals"
"Fusion of SAR and Multispectral Satellite Images Using Multiscale Analysis and Dempster-Shafer Theory for Flood Extent Extraction","M. O. Sghaier; M. Hadzagic; J. Patera","Centre de recherches math´ematiques, Universit´e de Montr´eal, Montréal, Québec, Canada; OODA Technologies Inc., Montréal, Québec, Canada; Centre de recherches math´ematiques, Universit´e de Montr´eal, Montréal, Québec, Canada","2019 22th International Conference on Information Fusion (FUSION)","27 Feb 2020","2019","","","1","8","Monitoring flood extent by means of Synthetic Aperture Radar (SAR) images has become a very common practice among decision makers and planners in disaster management as these images provide wide area coverage in extreme weather conditions. However, due to the satellite revisit time, their availability hinders their efficient use in disaster management. To capitalize on SAR images characteristics, this work considers both SAR and optical multispectral (MS) images, and proposes a novel method for SAR and optical image fusion in application to flood extent monitoring, which is based on two main steps: 1- Extraction of water pixels from the pre- and post-flooding images using a Modified Water Index (MWI) for water bodies identification from optical MS images and the Structural Feature Set (SFS) texture measurement for homogeneous areas extraction from SAR images, and 2- Applying the Max-Tree structure to estimate mass functions based on the multiscale and the multishape analysis of the input features map which are subsequently incorporated into the fusion module using Dempster-Shafer theory (DST). The results obtained in the evaluation of the proposed fusion method for three flood events characterized by different satellite image scenarios demonstrate the benefits of the multiscale DST fusion strategy in terms of chosen metrics in the classification of water body and monitoring of flood extent.","","978-0-9964527-8-6","10.23919/FUSION43075.2019.9011209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9011209","SAR;multispectral satellite images;Dempster-Shafer fusion;multiscale image analysis;flood extent extraction","Synthetic aperture radar;Feature extraction;Optical imaging;Indexes;Radiometry;Monitoring;Sensors","floods;geophysical image processing;image classification;image fusion;optical images;radar imaging;synthetic aperture radar;terrain mapping","Dempster-Shafer theory;fusion method;flood events;multiscale DST fusion strategy;multispectral satellite images;multiscale analysis;flood extent Extraction;Synthetic Aperture Radar images;decision makers;disaster management;wide area coverage;extreme weather conditions;satellite revisit time;SAR images characteristics;optical multispectral images;optical image fusion;flood extent monitoring;water pixels;post-flooding images;Modified Water Index;water bodies identification;optical MS images;Structural Feature Set texture measurement;homogeneous areas extraction;fusion module;Max-Tree structure;satellite image scenarios","","","","24","","27 Feb 2020","","","IEEE","IEEE Conferences"
"Fusion of RADARSAT-2 imagery with LANDSAT-8 multispectral data for improving land cover classification performance using SVM","C. Sukawattanavijit; J. Chen","School of Electronics and Information Engineering, Beihang University Beijing, CHINA; School of Electronics and Information Engineering, Beihang University Beijing, CHINA","2015 IEEE 5th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","29 Oct 2015","2015","","","567","572","Study of the land cover classification using multi-source data are very important for eco-environment monitoring, land use planning and climatic change detection. In this study, the utility of multi-source RADARSAT-2 and LANDSAT-8 multi-spectral images for improving land cover classification performance using Support Vector Machine (SVM) classifier. HH polarized C band RADARSAT-2 images were fused with the three band (6, 5, and 4) LANDSAT-8 multispectral image for land cover classification. Wavelet-based fusion (WT) techniques are implemented in the data fusion process. The Radial Basic Function (RBF) kernel function were used for SVM classifier in order to classify land cover types in the study area. The results of the SVM classification were compared with those using standard method Maximum Likelihood (ML) classifier, and it demonstrates a higher accuracy. Finally, it was indicated by the study that the fusion of SAR and optical images can significantly improve the classification accuracy with respect to use single dataset, and the SVM classifier could clearly outperform the standard method the ML classifier.","","978-1-4673-7297-8","10.1109/APSAR.2015.7306273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306273","image fusion;RADARSAT-2;LANDSAT-8;land cover classification;Support Vector Machine (SVM)","Support vector machines;Remote sensing;Satellites;Earth;Accuracy;Kernel;Training","climatology;environmental monitoring (geophysics);geophysical image processing;image classification;image fusion;land cover;land use planning;radar imaging;radial basis function networks;remote sensing by radar;satellite communication;support vector machines;wavelet transforms","HH polarized C band RADARSAT-2 imagery fusion;LANDSAT-8 multispectral data;land cover classification performance improvement;SVM classifier;support vector machine classifier;multisource data;eco-environment monitoring;land use planning;climatic change detection;wavelet-based fusion techniques;WT techniques;data fusion process;radial basic function kernel function;RBF kernel function","","5","","27","IEEE","29 Oct 2015","","","IEEE","IEEE Conferences"
"Research on Spatial Target Recognition Method Based on Multi-source Sensor Fusion","Y. Zhang; H. Yuan; Q. Tan; Y. Lu; C. Liu","Harbin Institute of Technology, Harbin, China; Harbin Institute of Technology, Harbin, China; Beijing Institute of Spacecraft System Engineering, CAST; Beijing Institute of Spacecraft System Engineering, CAST; Harbin Institute of Technology, Harbin, China","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","30 Mar 2020","2019","","","1","5","Radar, optical and infrared sensors are the main loads in space-based detection system. In this paper, multisensor information fusion method is applied to space target recognition. SVM, RFM and CNN is used in the recognition method. In this paper, the feature level fusion method of multisensor image is compared with extracting the features without fusion. The advantage of fusion for improving the recognition success rate is highlighted. Finally, the D-S evidence theory method is used in decision level fusion.","2474-2333","978-1-7281-2912-9","10.1109/APSAR46974.2019.9048272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048272","spatial target recognition;multi-source sensor information fusion;feature-level image fusion;D-S evidence theory","","convolutional neural nets;feature extraction;image fusion;inference mechanisms;infrared detectors;object recognition;optical sensors;random forests;support vector machines","spatial target recognition method;multisource sensor fusion;infrared sensors;space-based detection system;multisensor information fusion method;space target recognition;feature level fusion method;multisensor image;recognition success rate;D-S evidence theory method;decision level fusion;optical sensors;radar sensors;SVM;CNN","","","","12","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"On the possibility of conditional adversarial networks for multi-sensor image matching","N. Merkle; P. Fischer; S. Auer; R. Müller","German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany; German Aerospace Center (DLR), Remote Sensing Technology Institute Oberpfaffenhofen, Germany","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","2633","2636","A major research area in remote sensing is the problem of multi-sensor data fusion. Especially the combination of images acquired by different sensor types, e.g. active and passive, is a difficult task. Over the last years deep learning methods have proven their high potential for remote sensing applications. In this paper we will show how a deep learning method can be valuable for the problem of optical and SAR image matching. We investigate the possible of conditional generative adversarial networks (cGANs) for the generation of artificial templates. Contrary to common template generation approaches for image matching, the generation of templates using cGANs does not require the extraction of features. Our results show the possibility of realistic SAR-like template generation from optical images through cGANs and the potential of these templates for enhancing the matching of optical and SAR images by means of reliability and accuracy.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127535","conditional GANs;deep learning;image matching;multi-sensor;artificial template generation","Optical imaging;Adaptive optics;Optical sensors;Feature extraction;Synthetic aperture radar;Image matching;Gallium nitride","feature extraction;image fusion;image matching;learning (artificial intelligence);radar imaging;remote sensing;sensor fusion;synthetic aperture radar","artificial templates;optical images;multisensor image matching;research area;multisensor data fusion;remote sensing applications;deep learning method;optical SAR image matching;conditional generative adversarial networks;template generation;cGAN","","21","","10","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Change detection in heterogeneous remote sensing images based on the fusion of pixel transformation","Z. -g. Liu; L. Zhang; G. Li; Y. He","School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2017 20th International Conference on Information Fusion (Fusion)","14 Aug 2017","2017","","","1","6","A new change detection method for heterogeneous remote sensing images (i.e. SAR & optics) has been proposed via pixel transformation. It is difficult to directly compare the pixels from heterogeneous images for detecting changes. We propose to transfer the pixels in different images to a common feature space for convenience of comparison. For each pixel in the 1st image, it will be transferred to the 2nd feature space associated with the 2nd image according to the given unchanged pixel pairs. In fact, this transformation is done assuming that the pixel is not affected by the events. Then the difference value between the estimation of transferred pixel and the actual one in the same location of the 2nd image can be calculated. The bigger difference value, the higher possibility of change happening. We can similarly do the opposite transformation from the 2nd image to the 1st image, and one more difference value is obtained in the 1st feature space. Change occurrences will be detected using Fuzzy C-means clustering method based on the sum of two difference values. The flood detection in the SAR and optical images is given in the experiments, and it shows that the proposed method is able to efficiently detect changes.","","978-0-9964-5270-0","10.23919/ICIF.2017.8009656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009656","","Remote sensing;Synthetic aperture radar;Feature extraction;Clustering algorithms;Training;Neurons;Self-organizing feature maps","feature extraction;fuzzy set theory;geophysical image processing;image fusion;image resolution;pattern clustering;remote sensing;synthetic aperture radar","change detection;heterogeneous remote sensing images;pixel transformation fusion;heterogeneous images;feature space;unchanged pixel pairs;difference value;fuzzy c-means clustering method;flood detection;SAR;optical images","","10","","20","","14 Aug 2017","","","IEEE","IEEE Conferences"
"Classification of oyster habitats by combining wavelet-based texture features and polarimetric SAR descriptors","O. Regniers; L. Bombrun; I. Ilea; V. Lafon; C. Germain","Laboratoire IMS, Université de Bordeaux, Talence, France; Laboratoire IMS, Université de Bordeaux, Talence, France; Laboratoire IMS, Université de Bordeaux, Talence, France; Géo-Transfert, Pessac, France; Laboratoire IMS, Université de Bordeaux, Talence, France","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","3890","3893","In this study, we propose to evaluate the potential of combining very high resolution optical and SAR images for the classification of oyster habitats in tidal flats. To describe the classes of interest in both data, features are extracted by using wavelet-based texture features and polarimetric inter-band dependencies. A multisensor fusion scheme is then applied by adopting a maximum probability rule based on the outputs of SVM classifiers. Classification results show higher accuracies of detection of cultivated and abandoned oyster fields in comparison to classifications obtained using only texture features. This demonstrate the benefit of using both optical and SAR data for oyster habitats mapping in tidal flats.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7326674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7326674","texture;multi-sensor fusion;wavelet;SVM;classification;very high resolution;oyster habitats","Feature extraction;Support vector machines;Data mining;Synthetic aperture radar;Production;Tides;Training","feature extraction;geophysical image processing;image classification;image fusion;image texture;oceanographic techniques;radar imaging;radar polarimetry;support vector machines;synthetic aperture radar;tides","wavelet-based texture features;polarimetric SAR descriptor;oyster habitat classification;high resolution optical image;polarimetric interband dependency;SVM classifier;oyster habitat mapping;tidal flat;maximum probability rule;multisensor fusion scheme","","4","","9","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Multiscale multisensor decision level data fusion for urban mapping","A. Salentinig; P. Gamba","Department of Industrial and Information Engineering, University of Pavia, Pavia, Italy; Department of Industrial and Information Engineering, University of Pavia, Pavia, Italy","2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","29 Aug 2016","2016","","","67","71","The combination of multiple maps of urban area extents is becoming more and more common, as the availability of multiple EO data sets is increasing constantly, and different teams develop efficient techniques to extract human settlements from SAR and optical data. The next challenge is the need to combine all of these maps using an approach suitable to multi-source but also multi-resolution data sets. This work provides a first step towards this aim, by considering a “pixel aggregation” technique which proves to be more effective than standard techniques to resample all data sets on a common grid at the intended spatial resolution. Results on Beijing using SAR data from three different sensors and multispectral data from Landsat are offered to prove the effectiveness of the proposed procedure.","","978-1-5090-1479-8","10.1109/EORSA.2016.7552768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552768","urban mapping;multiscale data fusion;SAR;multispectral data","Spatial resolution;Synthetic aperture radar;Remote sensing;Urban areas;Earth;Satellites","geophysical image processing;image fusion;image resolution;image sampling;radar imaging;synthetic aperture radar;terrain mapping","multiscale multisensor decision level data fusion;urban mapping;urban area extent;human settlement extraction;SAR data;optical data;multiresolution data set;pixel aggregation technique;data set resampling;spatial resolution;Beijing;China;multispectral data;Landsat","","3","","12","IEEE","29 Aug 2016","","","IEEE","IEEE Conferences"
"A Manifold Learning Approach of Land Cover Classification for Optical and SAR Fusing Data","X. Tan; S. Jiang; Z. Zheng; P. Zhang; M. Zhu; Y. He; Z. Yu; N. Wang; L. Jiang; G. Zhou; H. Zhang; J. Li","Yunnan Electric Power Research Institute, Yunnan Power Grid Co., Ltd., Kunming, Yunnan, PRC; State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and the Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, PRC; State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and the Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, PRC; State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and the Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, PRC; Land and Resources Department of Sichuan Province, Chengdu, Sichuan, PRC; Sichuan Institute of Geo-Environment Monitoring, Chengdu, Sichuan, PRC; Center for Information and Geoscience, University of Electronic Science and Technology of China, Chengdu, Sichuan, PRC; State Key Laboratory of Remote Sensing Science, Jointly Sponsored by Beijing Normal University and the Institute of Remote Sensing and Digital Earth of Chinese Academy of Sciences, Beijing, PRC; Center for Information and Geoscience, University of Electronic Science and Technology of China, Chengdu, Sichuan, PRC; Guangxi Key Laboratory for Spatial Information and Geomatics, Guilin University of Technology, Guilin, Guangxi, PRC; Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Shatin, New Territories, Hong Kong; Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","3567","3570","In the field of remote sensing, data acquired from a single sensor usually can't meet the needs of some special applications, because the information extracted from the data are often incomplete and limited. Data fusing can solve this problem, but it will lead to the redundant information. In this paper, we proposed a novel manifold learning approach to perform dimensionality reduction for the fusing optical and SAR data. And three typical manifold learning models, namely, ISOMAP, local linear embedding (LLE) and principle component analysis (PCA), were utilized to test the robustness of our method by comparing with the land cover classification results. Our experimental results showed that our proposed method obtained the best land cover classification results among these approaches for the fusing optical and SAR data.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8517967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8517967","Land cover classification;Optical and SAR data;dimension reduction;manifold learning","Synthetic aperture radar;Optimization;Optical sensors;Manifolds;Remote sensing;Adaptive optics;Optical imaging","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);principal component analysis;remote sensing by radar;synthetic aperture radar","manifold learning models;land cover classification results;principle component analysis;SAR data;redundant information;data fusing;special applications;single sensor;remote sensing;SAR fusing data;manifold learning approach","","1","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Unsupervised change detection from multitemporal SAR images based on a detail preserving approach and a robust threshold estimation","B. Chabira; T. Skanderi; A. Belhadj Aissa","Dept. of telecommunications FEI, University of sciences and technology Houari Boumediene USTHB, Algiers, Algeria; Dept. of telecommunications FEI, University of sciences and technology Houari Boumediene USTHB, Algiers, Algeria; Dept. of telecommunications FEI, University of sciences and technology Houari Boumediene USTHB, Algiers, Algeria","2017 5th International Conference on Electrical Engineering - Boumerdes (ICEE-B)","14 Dec 2017","2017","","","1","5","In environment monitoring and disaster management, Synthetic aperture radars (SARs) have shown their great efficiency due to the fact that they provide short revisit times and they can operate in day and/or night and their independence of weather conditions. Different applications have been addressed a lot in recent years, but the one that receives a lot of attention is the change detection of the observed earth surface by exploiting the multitemporal SAR images. In this paper, we propose an unsupervised method for the change detection from multitemporal SAR images that does not require any speckle filtering. This method is based on: i) generating a multiresolution set of the single-channel log ratio image using stationary wavelet transform (SWT); ii) applying the T-point algorithm for all the images of the multiresolution sets; and iii) fusing the obtained images at the optimum reliable scale to generate the change map. The proposed method was experimentally validated using semisimulated and real SAR images acquired by RADARSAT-2 satellite in the region of Algiers.","","978-1-5386-0686-5","10.1109/ICEE-B.2017.8192061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8192061","Change detection;SAR;multi-temporal SAR images;speckle;single-channel;SWT;T-point algorithm;robust threshold estimation","Image resolution;Speckle;Robustness;Histograms;Synthetic aperture radar;Error analysis","geophysical image processing;image fusion;image resolution;object detection;remote sensing by radar;speckle;synthetic aperture radar;wavelet transforms","unsupervised change detection;multitemporal SAR images;robust threshold estimation;Synthetic aperture radars;short revisit times;unsupervised method;single-channel log ratio image;change map;semisimulated SAR images;real SAR images;detail preserving approach;environment monitoring;disaster management;weather conditions;change detection;observed earth surface;speckle filtering;stationary wavelet transform;T-point algorithm;multiresolution sets;optimum reliable scale;RADARSAT-2 satellite;Algiers","","","","8","IEEE","14 Dec 2017","","","IEEE","IEEE Conferences"
"Automatic alignment of high resolution optical and SAR images for urban areas","S. Auer; M. Schmitt; P. Reinartz","Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR); Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR)","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5466","5469","This paper presents the basics and functionality of SimGeoI, a simulation-based framework for the automated interpretation and alignment of optical and SAR remote sensing data. SimGeoI has been developed in order to align optical and SAR data based on given geometric information about objects represented by digital surface models. Thereby, the analysis of urban scenes is possible with independence of sensor type and perspective. After a brief introduction of the processor environment, possible applications of the framework are indicated with results of a case study for Istanbul (WorldView-2 and TerraSAR-X data). In this context, opportunities in the context of a joint analysis of high resolution optical and SAR data are addressed, i.e. concerning data fusion, change detection, and machine learning tasks.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128241","Optical Data;SAR Data;Data Fusion;Simulation;Interpretation;Urban Areas;Ray Tracing","Buildings;Optical imaging;Optical sensors;Synthetic aperture radar;Optical distortion;Sun;Satellites","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","urban scenes;sensor type;TerraSAR-X data;data fusion;automatic alignment;urban areas;functionality;SimGeoI;automated interpretation;optical SAR remote sensing data;align optical SAR data;digital surface models;geometric information;WorldView-2","","","","6","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Evaluate the contribution of SAR data in improving the impervious surfaces extraction at varying scales","Ru Xu; Hongsheng Zhang; Hui Lin","Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China; Chinese University of Hong Kong, Hong Kong, China","2016 4th International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","29 Aug 2016","2016","","","280","283","Multiple sources of remote sensing data have an increasingly wide use as an effective and comprehensive approach in many remote sensing applications, such as the estimation of impervious surface. Impervious surface is an important indicator of most urban-related environment, such as non-point source pollution, urban heat island and urban climate. However, accurate mapping of urban impervious surfaces is still challenging due the diversity of land cover materials. Among various studies, the synergistic use of optical and SAR data is one of the most challenging issues due to the differences of optical and SAR remote sensing techniques. Numerous researches have shown the good outcomes of ISE from fusion of optical and SAR data, while the potential of combining optical and SAR data was still not fully explored. In this study, the scale effects of combining optical and SAR data for ISE were investigated. Experiments will be carried out with satellite data of TM, TM+ASAR and TM + TerraSAR-X data obtained in November 2008 in the city of Shenzhen, China. The processing steps include: 1) Classifying the remote sensing image into 6 types, including water, soil, vegetation, shadow, dark impervious surface (DIS) and bright impervious surface (BIS) using TM, TM + ASAR and TM+TerraSAR-X data respectively; 2) Combining the DIS and BIS as impervious surfaces; 3) Calculating the impervious surfaces percentage (ISP) using grids with different sizes (30, 60, 90, 120….6000 m respectively), and 4) Evaluating the ISE with the indicator of correlation coefficient (CC) to compare the difference of results between different data type. The CC was employed to quantitatively evaluate the ISE accuracy based on different scales (grid sizes) from data combination of optical and SAR data. Preliminary experiments indicated that the ISE results derived from fusion data is better compared with single optical data and the ISP was different under different grid sizes.","","978-1-5090-1479-8","10.1109/EORSA.2016.7552813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552813","classification;impervious surface;scale effect","Image resolution;Earth;Synthetic aperture radar;Google;Manuals;Satellites;Remote sensing","feature extraction;image fusion;radar imaging;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar","impervious surfaces extraction;remote sensing data;urban-related environment;urban impervious surfaces;optical data;SAR data;optical remote sensing techniques;SAR remote sensing techniques;ISE;satellite data;remote sensing image;dark impervious surface;DIS;bright impervious surface;BIS;impervious surfaces percentage;ISP;correlation coefficient;fusion data","","","","7","IEEE","29 Aug 2016","","","IEEE","IEEE Conferences"
"Performance Assessment Metrics for Line-Infrastructure Monitoring with Multi-Sensor SAR Data","L. Chang; R. P. B. J. Dollevoet; R. F. Hanssen",University of Twente; Delft University of Technology; Delft University of Technology,"IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","4423","4426","Satellite radar interferometry (InSAR) has been used to monitor the structural health of line-infrastructure (e.g. railways, bridges, dams and dikes) in recent years. This enables the retrieval of millimeter-level changes in the line-infrastructure geometry on a bi-weekly basis. However, InSAR is an opportunistic method for which the location of the measurements (coherent scatterers) cannot be guaranteed, and the quality of the InSAR products vary from one case to another. Particularly, this is due to the orientation of the line-infrastructure relative to the satellite position, and its expected deformation magnitude and direction. Hence, the InSAR applicability and performance quality is not uniform. In operational situations, this tends to make asset managers skeptical about the potential of InSAR application on these assets. In this work, following [1] we develop new standard InSAR products for line-infrastructure monitoring, provide tools for predicting optimal multi-sensor SAR data combinations, and propose generic a priori performance assessment metrics for line-infrastructure. These products and metrics are tested on the Dutch railway line-infrastructure asset.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518364","Line-infrastructure;multi-sensor SAR","Strain;Satellites;Sensitivity;Monitoring;Rail transportation;Synthetic aperture radar","condition monitoring;image fusion;radar imaging;radar interferometry;railways;remote sensing by radar;synthetic aperture radar","generic a priori performance assessment metrics;Dutch railway line-infrastructure asset;line-infrastructure monitoring;satellite radar interferometry;line-infrastructure geometry;performance quality;InSAR application;standard InSAR products;millimeter-level changes retrieval;structural health monitoring;optimal multisensor SAR data combination prediction","","","","20","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Wide-angle ISAR Imaging of Highly Maneuvering Targets using Compressive Sensing","B. -H. Ryu; M. -J. Lee; K. -T. Kim; B. -S. Kang","Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang, South Korea; Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang, South Korea; Department of Electrical Engineering, Pohang University of Science and Technology (POSTECH), Pohang, South Korea; Agency for Defense Development (ADD), Daejeon, South Korea","2019 8th Asia-Pacific Conference on Antennas and Propagation (APCAP)","5 Jul 2021","2019","","","120","121","In practical wide-angle ISAR imaging scenarios, any Fourier based ISAR imaging algorithms cannot focus the ISAR images. To address the problems, we propose a new ISAR imaging framework by implementing compressive sensing theory and image fusion. In experiments, we observed that the proposed framework provides for high-quality ISAR images even for highly maneuvering targets.","2642-9179","978-1-6654-0054-1","10.1109/APCAP47827.2019.9472145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9472145","Inverse synthetic aperturer radar (ISAR) imaging;compressive sensing (CS);image fusion;wide-angle ISAR imaging","Imaging;Radar;Radar imaging;Radar antennas;Aperture antennas;Sensors;Compressed sensing","compressed sensing;image fusion;radar imaging;synthetic aperture radar;target tracking","ISAR imaging framework;compressive sensing theory;image fusion;high-quality ISAR images;highly maneuvering targets;wide-angle ISAR imaging scenarios;Fourier based ISAR imaging algorithms","","","","3","IEEE","5 Jul 2021","","","IEEE","IEEE Conferences"
"A Deep Learning Framework for Fusion of Sar and Optical Satellite Imagery","N. Gupta; H. S. Srivastava; T. Sivasankar; P. Patel","NIIT University, Neemrana, India; Indian Institute of Remote Sensing, ISRO, Dehradun, India; NIIT University, Neemrana, India; Space Applications Centre, ISRO, Ahmedabad, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","488","491","In remote sensing, image fusion is the process of converting information from various source images to a single image such that the features of the source are preserved and relevant information is being highlighted. Through this research work, we propose an unsupervised deep learning Generative Adversarial Network (GAN) for the fusion process of SAR and optical Images. For SAR image, we chose VV, VH, VV-VH bands and for optical image we did Principal Component Analysis (PCA) on its image bands to extract the top three principal components and compose an image out of it. Images were then converted into HSV space. The GAN is primarily trained to capture the maximum gradient features from both the images and secondarily to capture other noticeable features. Experimental results on both training and test samples indicate that the proposed method is able to preserve gradient features and other details of the images with respect to input images.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9792062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792062","SAR;optical;image fusion;deep learning;GAN","Deep learning;Training;Neural networks;Optical computing;Optical imaging;Generative adversarial networks;Optical sensors","deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image colour analysis;image fusion;principal component analysis;remote sensing;synthetic aperture radar;unsupervised learning","remote sensing;image fusion;GAN;optical Images;SAR image;VV-VH bands;optical image;principal component analysis;image bands;maximum gradient features;optical satellite imagery;unsupervised deep learning generative adversarial network;PCA","","","","18","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Fusion of Spaceborne and Airborne SAR Images Using Saliency and Fuzzy Logic for Vessel Detection","D. Zhu; X. Wang; G. Li; X. -P. Zhang","School of Electron. Inf. & Commun., Huazhong University of Science and Technology, China; Department of Electronic Engineering, Tsinghua University, China; Department of Electronic Engineering, Tsinghua University, China; Department of Electrical, Computer & Biomedical Engineering, Ryerson University, Canada","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4468","4471","In the paper, we propose a new method based on multi-order superpixel-level saliency and fuzzy logic (MSSFL) to fuse spaceborne and airborne SAR images for vessel detection. First, we generate a new global regional contrast map (GRCM) by exploiting the multi-order superpixel-level saliency (MSS). In the generated GRCM, the vessel targets are well restored and the backgrounds are suppressed. Next, a new fuzzy logic approach is presented to fuse the MSS information provided by the GRCMs. This GRCM-based fuzzy fusion can further enhance the vessel target regions and filter out the inshore interference regions. Experimental results using Gaofen-3 satellite and unmanned aerial vehicle (UAV) SAR images show that the proposed MSSFL method yields higher target-to-cluster ratio (TCR) of fused images and improved detection performance compared with the commonly utilized image fusion approaches.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554207","National Natural Science Foundation of China(grant numbers:61790551,61901244,61925106); China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554207","Fuzzy logic;saliency;spaceborne and airborne SAR image fusion;vessel target detection","Fuzzy logic;Satellites;Fuses;Object detection;Interference;Information filters;Radar polarimetry","autonomous aerial vehicles;fuzzy logic;image classification;image colour analysis;image fusion;image representation;image segmentation;object detection;radar imaging;remotely operated vehicles;sensor fusion;synthetic aperture radar","airborne SAR images;vessel detection;multiorder superpixel-level saliency;global regional contrast map;generated GRCM;vessel targets;fuzzy logic approach;GRCM-based fuzzy fusion;vessel target regions;inshore interference regions;MSSFL method yields higher target-to-cluster ratio;improved detection performance;commonly utilized image fusion approaches","","","","11","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Analysis of the Multispectral and SAR Image","S. Zhang; Y. Zhang; Y. Chou; Z. Wang; Y. Shi; Z. Sun","Simulation Technology and Service CO., Ltd AVIC International, Shanghai, China; Institute of Electronic, Engineering Technology Harbin Institute of Technology, Haerbin, China; Simulation Technology and Service CO., Ltd AVIC International, Shanghai, China; Department of electrical and computer engineering, Northeastern University, Boston, USA; Simulation Technology and Service CO., Ltd AVIC International, Shanghai, China; Simulation Technology and Service CO., Ltd AVIC International, Shanghai, China","2021 IEEE 6th International Conference on Computer and Communication Systems (ICCCS)","21 Jun 2021","2021","","","312","315","The SAR image and the multispectral image are both used for dynamic monitoring, mineral resources investigation, urban and rural monitoring and evaluation, traffic network exploration, forest resources investigation, desertification monitoring, and so on. The multi-spectral and SAR image fusion to improve the classify quality is discussed in this paper, compared the common fusion algorithms of the SAR image and multi spectral images, that is standard color transform (Brovey) method, phase recovery (Gram-Schmidt) method and color space transform (HSV) method, principal component transformation super resolution (PCA) method and Bias method (Pansharp), by which the fused image is more relative with the multi-spectral and SAR.","","978-1-6654-1256-8","10.1109/ICCCS52626.2021.9449213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9449213","SAR;multi spectral;image fusion","Image resolution;Image color analysis;Transforms;Distortion;Visual effects;Radar polarimetry;Classification algorithms","image colour analysis;image fusion;image resolution;principal component analysis;radar imaging;synthetic aperture radar;transforms","dynamic monitoring;traffic network exploration;forest resources investigation;desertification monitoring;SAR image fusion;multispectral images;standard color transform method;mineral resources investigation;rural monitoring;urban monitoring;phase recovery method;color space transform method;principal component transformation super resolution method;Bias method","","","","3","IEEE","21 Jun 2021","","","IEEE","IEEE Conferences"
"Navigation and SAR focusing with map aiding","Z. Sjanic; F. Gustafsson","Division of Automatic Control, Linköping University, Linköping, Sweden; Division of Automatic Control, Linköping University, Linköping, Sweden","IEEE Transactions on Aerospace and Electronic Systems","28 Sep 2015","2015","51","3","1652","1663","A method for fusing synthetic aperture radar (SAR) images with optical aerial images is presented. This is done in a navigation framework, in which the absolute position and orientation of the flying platform, as computed from the inertial navigation system, is corrected based on the aerial image coordinates taken as ground truth. The method is suitable for new low-price SAR systems for small unmanned vehicles. The primary application is surveillance, and to some extent it can be applied to remote sensing, where the SAR image provides complementary information by revealing reflectivity to microwave frequencies. The method is based on first applying an edge detection algorithm to the images and then optimising the most important navigation states by matching the two binary images. To get a measure of the estimation uncertainty, we embed the optimisation in a least squares framework, in which an explicit method to estimate the (relative) size of the errors is presented. The performance is demonstrated on real SAR and aerial images, leading to an error of only a few pixels (around 4 m in our case), which is a quite satisfactory performance for applications like surveillance and navigation.","1557-9603","","10.1109/TAES.2015.130397","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272820","","Synthetic aperture radar;Navigation;Optical imaging;Image edge detection;Trajectory;Detectors;Optical sensors","autonomous aerial vehicles;edge detection;geophysical image processing;image fusion;image matching;inertial navigation;least mean squares methods;optimisation;radar imaging;remote sensing by radar;search radar;synthetic aperture radar","synthetic aperture radar;SAR navigation;SAR focusing;SAR image fusion;optical aerial image coordinates;absolute position estimation;orientation estimation;inertial navigation system;small unmanned vehicles;survelliance;remote sensing;edge detection algorithm;navigation state optimisation;binary image matching;least square method;error size estimation","","15","","31","IEEE","28 Sep 2015","","","IEEE","IEEE Journals"
"Flood monitoring and change detection based on unsupervised image segmentation and fusion in multitemporal SAR imagery","J. Avendano; S. F. Mora; J. E. Vera; J. A. Torres; F. A. Prieto","Grupo de Investigacion y Desarrollo TECnologico Aplicado (INDETECA), Universidad Escuela Colombiana de Carreras Industriales, Bogota, Colombia; Grupo de Investigacion y Desarrollo TECnologico Aplicado (INDETECA), Universidad Escuela Colombiana de Carreras Industriales Bogota, Colombia; Grupo de Investigacion y Desarrollo TECnologico Aplicado (INDETECA), Universidad Escuela Colombiana de Carreras Industriales, Bogota, Colombia; Grupo de Investigacion y Desarrollo TECnologico Aplicado (INDETECA), Universidad Escuela Colombiana de Carreras Industriales, Bogota, Colombia; Grupo de Automatica (GAUNAL), Universidad Nacional de Colombia, Bogota, Colombia","2015 12th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE)","17 Dec 2015","2015","","","1","6","This paper presents an unsupervised method for change detection and analysis of the behavior of floods using multitemporal SAR (Synthetic Aperture Radar) images. First, images were filtered using the Enhanced Frost Filter in order to reduce the effect of speckle noise. Fuzzy Clustering Means (FCM) and k-means algorithms were used for unsupervised segmentation, and both results were fused using PCA. Subsequently, a Boolean image was created from the change information using a thresholding algorithm. Finally, the area of changes in the scene was calculated with spatial resolution information from the images. For the experiment phase, synthetic images were first created with varying levels of speckle noise, making it possible to evaluate the performance of the proposed method. The results showed an overall accuracy of approximately 99% and a kappa index of 0.76 for images whose Equivalent Number of Looks (ENL) equals 0.7. This shows that the proposed method is efficient in detecting changes in SAR images with an ENL greater than or equal to 0.7. Finally, two SAR images were tested, one before and one after a flood that covered an area of the Magdalena River in Colombia called Plato-Magdalena. Our method found that the river flooded approximately 131.51 hectares of terrain in the case of the studied images.","","978-1-4673-7839-0","10.1109/ICEEE.2015.7357982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357982","SAR;Unsupervised Segmentation;Multitemporal Data Fusion;Change Detection;Flood Analysis","Image segmentation;Speckle;Change detection algorithms;Synthetic aperture radar;Histograms;Principal component analysis;Fuses","floods;fuzzy set theory;geophysical image processing;hydrological techniques;image fusion;image resolution;image segmentation;principal component analysis;radar imaging;remote sensing by radar;rivers;speckle;synthetic aperture radar","unsupervised method;change detection;change analysis;flood monitoring;multitemporal SAR images;synthetic aperture radar;enhanced frost filter;speckle noise;fuzzy clustering means algorithm;k-means algorithm;unsupervised segmentation;PCA;Boolean image;thresholding algorithm;spatial resolution information;experiment phase;synthetic images;kappa index;equivalent number of looks;Magdalena River;Colombia;Plato-Magdalena;unsupervised image segmentation;image fusion;multitemporal SAR imagery","","3","","21","IEEE","17 Dec 2015","","","IEEE","IEEE Conferences"
"A General Framework for Change Detection Using Multimodal Remote Sensing Data","S. Chirakkal; F. Bovolo; A. R. Misra; L. Bruzzone; A. Bhattacharya","Advanced Microwave and Hyperspectral Techniques Development Group, Space Applications Center, Indian Space Research Organisation, Ahmedabad, India; Center for Information and Communication Technology, Fondazione Bruno Kessler, Trento, Italy; Advanced Microwave and Hyperspectral Techniques Development Group, Space Applications Center, Indian Space Research Organisation, Ahmedabad, India; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Center of Studies in Resources Engineering, Indian Institute of Technology Bombay, Mumbai, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Nov 2021","2021","14","","10665","10680","A general framework for change detection is proposed to analyze multimodal remotely sensed data utilizing the Kronecker product between two data representations (vectors or matrices). The proposed method is sensor independent and provides comparable results to techniques that exist for specific sensors. The proposed fusion technique is a pixel-level approach that incorporates inputs from different modalities, rendering enriched multimodal data representation. Thus, the proposed hybridization procedure helps to assimilate multisensor information in a meaningful manner. A novel change index ($\zeta$) is defined for the general multimodal case. This index is then used to quantify the change in bitemporal remotely sensed data. This article explores the usability, consistency, and robustness of the proposed multimodal fusion framework, including the change index, with proper validation on two multimodal cases: 1) the dual-frequency ($C$- and $L$-band) fully polarimetric Danish EMISAR data and 2) the dual-polarimetric synthetic aperture radar and Sentinel-2 multispectral data. Detailed analysis and validation using extensive ground-truth data are presented to establish the proposed framework.","2151-1535","","10.1109/JSTARS.2021.3119358","India–Trento Program for Advanced Research; Department of Science and Technology of the Indian Government; Provincia Autonoma di Trento; Trentino Cultural Institute, now Bruno Kessler Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568726","Change detection (CD);dual-frequency PolSAR;Kronecker product of matrices;multimodal data;synthetic aperture radar (SAR) optical fusion","Synthetic aperture radar;Remote sensing;Indexes;Optical sensors;Data integration;Optical imaging;Radar polarimetry","geophysical image processing;image fusion;radar imaging;radar polarimetry;remote sensing;sensor fusion;synthetic aperture radar","matrices;specific sensors;fusion technique;pixel-level approach;enriched multimodal data representation;novel change index;general multimodal case;bitemporal remotely sensed data;multimodal fusion framework;multimodal cases;Sentinel-2 multispectral data;extensive ground-truth data;change detection;multimodal remote sensing data;Kronecker product;vectors","","2","","55","CCBY","12 Oct 2021","","","IEEE","IEEE Journals"
"Radar and Optical Image Fusion using Airborne Sensor Data from the Heligoland Island","H. Anglberger; J. Fischer; D. Frommholz","DLR, Microwaves and Radar Institute, Oberpfaffenhofen, Wessling, Germany; DLR, Microwaves and Radar Institute, Oberpfaffenhofen, Wessling, Germany; DLR, Institute of Optical Sensor Systems, Berlin, Germany","2018 19th International Radar Symposium (IRS)","30 Aug 2018","2018","","","1","7","An accurate geometrical alignment of remote sensing data is the basis for higher-level image processing techniques used to extract information. Fusing radar image data with other sensor data sources states a special case because the coordinate system is based on the measured range which causes ambiguous regions due to layover effects. An accurate 3D representation of the scene is essential to find a fitting geometrical transformation between the respective sensor image spaces. This paper applies a method that accurately maps detailed 3D information of the German island of Heligoland to the slant-range-based coordinate system of radar images imaged by DLR's airborne F-SAR sensor. The highly accurate 3D information along with optical imagery has been acquired by DLR's airborne optical sensor system MACS.","2155-5753","978-3-7369-9545-1","10.23919/IRS.2018.8448211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8448211","","Optical sensors;Laser radar;Radar imaging;Optical imaging;Optical scattering;Three-dimensional displays","geophysical image processing;image fusion;image representation;optical images;radar imaging;radar receivers;remote sensing by radar;synthetic aperture radar","coordinate system;ambiguous regions;fitting geometrical transformation;radar images;optical imagery;airborne sensor data;Heligoland island;remote sensing data;higher-level image processing techniques;radar image data;3D representation;sensor image spaces;DLR airborne F-SAR sensor;DLR airborne optical sensor system MACS;geometrical alignment","","1","","6","","30 Aug 2018","","","IEEE","IEEE Conferences"
"Ship Detection Based on Deep Convolutional Neural Networks for Polsar Images","F. Zhou; W. Fan; Q. Sheng; M. Tao","Key Laboratory of Electronic Information Counter Measure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Electronic Information Counter Measure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Electronic Information Counter Measure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi'an, China","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","681","684","In this paper, we proposed a ship detection method based on deep convolutional neural networks for PolSAR images. The proposed ship detector firstly segments PolSAR images into sub-samples using a sliding window of fixed size to effectively extract translational-invariant spatial features. Further, the modified faster region based convolutional neural network (Faster-RCNN) method is utilized to realize ship detection for ships with different sizes and fusion the detection result. Finally, the proposed method was validated using real measured NASAlJPL AIRSAR datasets by comparing the performance with the modified constant false alarm rate (CFAR) detector. The comparison results demonstrate the validity and generality of the proposed detection algorithm.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518589","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518589","Deep convolutional neural networks;polarimetric synthetic aperture radar (PolSAR);ship detection","Marine vehicles;Proposals;Detectors;Feature extraction;Convolutional neural networks;Image segmentation;Object detection","feature extraction;feedforward neural nets;geophysical image processing;image fusion;image segmentation;object detection;radar detection;radar imaging;radar polarimetry;remote sensing by radar;ships;synthetic aperture radar","modified constant false alarm rate detector;detection algorithm;deep convolutional neural networks;ship detection method;PolSAR image segmentation;translational-invariant spatial feature extraction;faster region based convolutional neural network method;faster-RCNN method;NASAlJPL AIRSAR datasets;constant false alarm rate detector;CFAR detector","","8","","8","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Multi Scale Ship Detection Based on Attention and Weighted Fusion Model for High Resolution SAR Images","L. Zhang; Z. Chu; B. Zou","Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","631","634","Ship detection in SAR images is a challenging problem. CNN-based ship detection method in SAR images has achieved remarkable results. Due to the multi scale of the ships and interference from complex sea conditions or nearshore background in SAR images, many false alarms and missed detections can occur in ship detection. To solve these problems, a multi-scale ship detection network in SAR images based on attention and weighted fusion is proposed in this paper. First, a higher-resolution detect head is added based on the YOLOv5 framework for detecting tiny-scale ships in SAR images. Then, the coordinate attention block is introduced to refine the location features of ship targets and suppress the interference of complex background. Finally, in the feature fusion stage, adaptive weighted feature fusion is used to reduce feature redundancy. Experiments on the SSDD dataset show the effectiveness of the proposed method.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883844","National Natural Science Foundation of China(grant numbers:61871158); Aeronautical Science Foundation of China(grant numbers:20182077008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883844","Synthetic Aperture Radar (SAR);ship detection;multi -scale;coordinate attention;weighted feature fusion","Head;Image resolution;Redundancy;Semantics;Geoscience and remote sensing;Interference;Detectors","feature extraction;image fusion;object detection;radar imaging;ships;synthetic aperture radar","weighted fusion model;high resolution sar images;CNN-based ship detection method;missed detections;multiscale ship detection network;higher-resolution detect head;tiny-scale ships;ship targets","","1","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Mapping urban impervious surfaces by fusing optical and SAR data at decision level","Y. Bai; G. Sun; Y. Ge; Y. Zhang; Y. Li","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; National Astronomical Observatories, Chinese Academy of Sciences, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","6336","6339","The extraction of urban impervious surface information plays a key role in the studies of urbanization and its related environmental issues. Optical and SAR remote sensing provides complementary information to improve the accuracy of impervious mapping. However, the fusing of information acquired by different sensors is challenging. Optical and SAR features have distinct characteristics, and require different classification strategy and classification types. In this study, a strategy of fusing multi-spectral optical and polarimetric SAR data at decision-level is proposed. Features are extracted from optical and SAR data, then staked auto-encoder is applied to achieve the land use and land cover classification separately. D-S evidence theory is used to fuse the classification result and the imperious surface map is derived. The experiment was conducted in a highly complex urban area of Hong Kong and the results proves the soundness of the method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898039","impervious surface;decision-level fusion;land use and land cover;multi-spectrum;synthetic aperture radar","","geophysical image processing;geophysical signal processing;image classification;image fusion;radar polarimetry;remote sensing;remote sensing by radar;synthetic aperture radar;terrain mapping","related environmental issues;complementary information;impervious mapping;different classification strategy;classification types;polarimetric SAR data;decision-level;optical SAR data;land use;land cover classification;classification result;imperious surface map;highly complex urban area;urban impervious surfaces;decision level;urban impervious surface information","","","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Challenges and Opportunities of Multimodality and Data Fusion in Remote Sensing","M. Dalla Mura; S. Prasad; F. Pacifici; P. Gamba; J. Chanussot; J. A. Benediktsson","Université Grenoble Alpes, GIPSA-Lab, Grenoble, France; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA; DigitalGlobe Inc., CO, Boulder, CO, USA; University of Pavia, Pavia, Italy; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland; Faculty of Electrical and Computer Engineering, University of Iceland, Reykjavik, Iceland","Proceedings of the IEEE","20 Aug 2015","2015","103","9","1585","1601","Remote sensing is one of the most common ways to extract relevant information about Earth and our environment. Remote sensing acquisitions can be done by both active (synthetic aperture radar, LiDAR) and passive (optical and thermal range, multispectral and hyperspectral) devices. According to the sensor, a variety of information about the Earth's surface can be obtained. The data acquired by these sensors can provide information about the structure (optical, synthetic aperture radar), elevation (LiDAR), and material content (multispectral and hyperspectral) of the objects in the image. Once considered together their complementarity can be helpful for characterizing land use (urban analysis, precision agriculture), damage detection (e.g., in natural disasters such as floods, hurricanes, earthquakes, oil spills in seas), and give insights to potential exploitation of resources (oil fields, minerals). In addition, repeated acquisitions of a scene at different times allows one to monitor natural resources and environmental variables (vegetation phenology, snow cover), anthropological effects (urban sprawl, deforestation), climate changes (desertification, coastal erosion), among others. In this paper, we sketch the current opportunities and challenges related to the exploitation of multimodal data for Earth observation. This is done by leveraging the outcomes of the data fusion contests, organized by the IEEE Geoscience and Remote Sensing Society since 2006. We will report on the outcomes of these contests, presenting the multimodal sets of data made available to the community each year, the targeted applications, and an analysis of the submitted methods and results: How was multimodality considered and integrated in the processing chain? What were the improvements/new opportunities offered by the fusion? What were the objectives to be addressed and the reported solutions? And from this, what will be the next challenges?","1558-2256","","10.1109/JPROC.2015.2462751","European project(grant numbers:ERC-2012-AdG-320684-CHESS); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194740","Change detection (CD);classification;data fusion (DF);pansharpening;remote sensing;Change detection (CD);classification;data fusion (DF);pansharpening;remote sensing","Remote sensing;Data integration;Spatial resolution;Laser radar;Synthetic aperture radar;Optical sensors;Multimodal sensors;Climate change","data acquisition;geophysical image processing;hyperspectral imaging;image fusion;optical radar;remote sensing by laser beam;remote sensing by radar;synthetic aperture radar","oil field exploitation;mineral exploitation;natural resource monitoring;environmental variable;vegetation phenology;snow cover;anthropological effect;urban sprawl;deforestation;climate change;desertification;coastal erosion;multimodal data exploitation;Earth observation;IEEE Geoscience and Remote Sensing Society;method analysis;resource exploitation;oil spill;earthquake;hurricane;flood;natural disaster;damage detection;precision agriculture;urban analysis;land use characterization;optical radar;Earth surface;hyperspectral imaging;multispectral imaging;thermal range;optical range;passive devices;LiDAR;synthetic aperture radar;remote sensing acquisition;environmental information extraction;Earth information extraction;data fusion;remote sensing multimodality","","132","","90","IEEE","13 Aug 2015","","","IEEE","IEEE Journals"
"Deep Learning for SAR-Optical Image Matching","L. H. Hughes; N. Merkle; T. Bürgmann; S. Auer; M. Schmitt","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; Airbus Defence and Space GmbH, Immenstaad, Germany; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","4877","4880","The automatic matching of corresponding regions in remote sensing imagery acquired by synthetic aperture radar (SAR) and optical sensors is a crucial pre-requesite for many data fusion endeavours such as target recognition, image registration, or 3D-reconstruction by stereogrammetry. Driven by the success of deep learning in conventional optical image matching, we have carried out extensive research with regard to deep matching for SAR-optical multi-sensor image pairs in the recent past. In this paper, we summarize the achieved findings, including different concepts based on (pseudo-)siamese convolutional neural network architectures, hard negative mining, alternative formulations of the underlying loss function, and creation of artificial images by generative adversarial networks. Based on data from state-of-the-art remote sensing missions such as TerraSAR-X, Prism, Worldview-2, and Sentinel-1/2, we show what is already possible today, while highlighting challenges to be tackled by future research endeavors.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898635","Deep Learning;Image Matching;Optical Images;SAR Images;Data Fusion","Optical imaging;Optical sensors;Image matching;Optical fiber networks;Training data;Training;Deep learning","convolutional neural nets;image fusion;image matching;learning (artificial intelligence);optical images;optical sensors;radar imaging;remote sensing;synthetic aperture radar","remote sensing imagery;optical sensors;data fusion;deep learning;SAR-optical multisensor image pairs;artificial images;SAR-optical image matching;synthetic aperture radar;Siamese convolutional neural network architectures","","23","","12","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Dual-Stream High Resolution Network for Multi-Source Remote Sensing Image Segmentation","B. Ren; S. Ma; B. Hou; D. Hong","School of Artificial Intelligence, Xidian University, Xian, China; School of Artificial Intelligence, Xidian University, Xian, China; School of Artificial Intelligence, Xidian University, Xian, China; Remote Sensing Technology Institute (IMF), German Aerospace Center (DLR), Wessling, Germany","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","3440","3443","Recently, the image segmentation has been a significant research direction in the field of optical remote sensing data processing. However, due to the limitation of the optical imaging mechanism, traditional image segmentation methods are not efficient for processing the optical remote sensing images, especially influencing by the complex weather conditions. In order to ensure the classification performance, synthetic aperture radar (SAR) data are employed as complementary to the data procedure for enhancing the capability of land cover interpretation. Then a dual-stream high-resolution network (HRNet) is proposed to combine two types of heterogeneous data (SAR and optical image), and a multi-modal squeeze-and-excitation (SE) module is exploited to make feature maps fused. Experiments show that the proposed method has excellent performance on the remote sensing data acquired by GF2 and GF3 satellites.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553947","National Natural Science Foundation of China(grant numbers:62001355); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553947","Images Segmentation;Heterogeneous Data Fusion;Multi-modalities","Image segmentation;Satellites;Optical fiber networks;Optical imaging;Adaptive optics;Optical sensors;Task analysis","geophysical image processing;geophysical techniques;image fusion;image resolution;image segmentation;optical images;radar imaging;remote sensing;synthetic aperture radar","dual-stream high resolution network;multisource remote sensing image segmentation;significant research direction;optical remote sensing data processing;optical imaging mechanism;traditional image segmentation methods;optical remote sensing images;complex weather conditions;synthetic aperture radar data;data procedure;high-resolution network;heterogeneous data;optical image","","3","","5","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multistage fusion and dissimilarity regularization for deep learning","Y. -R. Cho; S. Shin; S. -H. Yim; H. -W. Cho; W. -J. Song","Pohang University of Science and Technology, Pohang, Gyeongbuk-do, Korea; Pohang University of Science and Technology, Pohang, Gyeongbuk-do, Korea; Pohang University of Science and Technology, Pohang, Gyeongbuk-do, Korea; Pohang University of Science and Technology, Pohang, Gyeongbuk-do, Korea; Pohang University of Science and Technology, Pohang, Gyeongbuk-do, Korea","2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)","11 Dec 2017","2017","","","586","591","We propose a multistage fusion stream (MFS) and dissimilarity regularization (DisReg) for deep learning. The degree of similarity between the feature maps of a single-sensor stream is estimated using DisReg. DisReg is applied to the learning problems of each single-sensor stream, so they have distinct types of feature map. Each stage of the MFS fuses the feature maps extracted from single-sensor streams. The proposed scheme fuses information from heterogeneous sensors by learning new patterns that cannot be observed using only the feature map of a single-sensor stream. The proposed method is evaluated by testing its ability to automatically recognize targets in a synthetic aperture radar and infrared images. The superiority of the proposed fusion scheme is demonstrated by comparison with conventional algorithm.","","978-1-5090-6064-1","10.1109/MFI.2017.8170385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170385","","Streaming media;Sensor fusion;Machine learning;Feature extraction;Fuses;Periodic structures","feature extraction;image classification;image fusion;infrared imaging;learning (artificial intelligence);radar imaging;sensor fusion;synthetic aperture radar","dissimilarity regularization;deep learning;DisReg;single-sensor stream;learning problems;synthetic aperture radar;infrared images","","2","","24","IEEE","11 Dec 2017","","","IEEE","IEEE Conferences"
"SOFNet: SAR-Optical Fusion Network for Land Cover Classification","D. Zhang; M. Gade; J. Zhang","Universität Hamburg, Fachbereich Informatik, Hamburg, Germany; Universität Hamburg, Institut für Meereskunde, Hamburg, Germany; Universität Hamburg, Fachbereich Informatik, Hamburg, Germany","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2409","2412","The objective of this research is to realize automatic land cover classification from synthetic aperture radar (SAR) and multispectral remote sensing imagery. We develop a SAR-optical fusion network (SOFNet) with the symmetric cross entropy (SCE) loss to utilize both the SAR and optical information in a novel deep neural network. The proposed framework has been trained on the public SEN12MS dataset and tested on the 2020 IEEE-GRSS Data Fusion Contest (DFC2020) dataset. Experimental results show that our approach takes full advantage of multimodal information and outperforms the state-of-the-art convolutional architectures.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554070","land cover classification;SAR;multispectral;multimodal fusion;deep learning","Optical losses;Ultraviolet sources;Semantics;Data integration;Optical fiber networks;Optical imaging;Adaptive optics","deep learning (artificial intelligence);entropy;geophysical image processing;image classification;image fusion;land cover;radar imaging;remote sensing by radar;synthetic aperture radar","automatic land cover classification;synthetic aperture radar;multispectral remote sensing imagery;SAR-optical fusion network;symmetric cross entropy loss;optical information;deep neural network;public SEN12MS dataset;2020 IEEE-GRSS Data Fusion Contest;SOFNet;SCE;DFC2020;multimodal information","","2","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Comparative analysis of the hyperspectral vegetatation index and radar vegetation index: A novel fusion vegetation index","Y. -H. Kim; J. -H. Oh; J. -W. Choi; Y. -I. Kim","Department of Civil and Environmental Engineering, Seoul National University, Republic of Korea; Department of Civil Engineering, Chonnam National University, Republic of Korea; Chungbuk National University, Cheongju, Chungcheongbuk-do, KR; Department of Civil and Environmental Engineering, Seoul National University, Republic of Korea","2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","23 Oct 2017","2015","","","1","4","The hyperspectral vegetation index (HVI) has shown promise in vegetation fields, but its relationship to the radar vegetation index (RVI) is not known in the context of various land covers. This work presents a comparative analysis of the HVI data derived from the AISA sensor and RVI values originating from the RADARSAT-2 quad-polarimetric synthetic aperture radar (SAR) data. Six types of land cover (buildings, forest, salt pond, tidal flat, ocean, and paddy field) were compared, and the patterns were investigated. In the RVI, forest areas show higher separability than the other types of land cover without exception. Also, in the HVIs, the forest areas indicate high values, without exception. The statistics of the region of interest (ROI) demonstrate that the RVI patterns of the six land-cover types are highly similar to those of the HVI. Thus, during bad weather conditions and at night, the RVI data could serve as an alternative to the HVI data. In addition to comparative analysis, we propose a novel fusion vegetation index (FVI) using the RVI and normalized difference vegetation index (NDVI). The proposed FVI creates obvious vegetation separation, more so than other land covers. Using the FVI, more effective vegetation monitoring could be possible in various vegetation monitoring fields.","2158-6276","978-1-4673-9015-6","10.1109/WHISPERS.2015.8075434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075434","Hyperspectral vegetation index;radar vegetation index;comparative analysis;fusion vegetation index","Vegetation mapping;Indexes;Hyperspectral imaging;Radar;Monitoring","geophysical image processing;hyperspectral imaging;image fusion;radar polarimetry;remote sensing by radar;synthetic aperture radar;vegetation","hyperspectral vegetation index;radar vegetation index;fusion vegetation index;vegetation fields;land cover;forest areas;land-cover types;normalized difference vegetation index;vegetation monitoring;RADARSAT-2 quadpolarimetric SAR data;synthetic aperture radar;vegetation separation;buildings;forest;salt pond;tidal flat;paddy field;ocean","","1","","16","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Exploring the Fusion of Sentinel-1 SAR and Sentinel-2 MSI Data for Built-Up Area Mapping Using Deep Learning","S. Hafner; Y. Ban; A. Nascetti","Division of Geoinformatics, KTH Royal Institute of Technology; Division of Geoinformatics, KTH Royal Institute of Technology; Division of Geoinformatics, KTH Royal Institute of Technology","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4720","4723","This research explores the potential of combining Sentinel-1 C-band Synthetic Aperture Radar (SAR) and Sentinel-2 MultiSpectral Instrument (MSI) data for Built-Up Area (BUA) mapping using deep learning. A lightweight U-Net model is trained using openly available building footprint reference data in North America and tested in four cities across three additional continents. The best test performance in terms of F1 score was achieved by the joint use of SAR and multispectral data (0.676), followed by multi-spectral (0.611) and SAR data (0.601). The developed fusion approach is particularly promising to distinguish BUA in low-density residential neighborhoods. Furthermore, our fusion approach compares favorably to the state-of-the-art in BUA mapping in the selected cities. However, associated with the diverse characteristics of human settlements around the world, considerable differences in accuracy among the test cities were observed. This indicates the need for more sophisticated fusion techniques to improve CNN model generalization and for adding more diverse training data.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553448","Swedish National Space Agency; ESA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553448","Sentinel-1;Sentinel-2;Built-up area mapping;data fusion;deep learning","Deep learning;Instruments;Urban areas;Training data;Optical imaging;Data models;Internet","convolutional neural nets;geophysical image processing;image fusion;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","Sentinel-1 SAR;Sentinel-2 MSI data;Built-Up Area mapping;deep learning;SAR data;BUA mapping;fusion techniques;diverse training data;building footprint reference data;Sentinel-1 C-band synthetic aperture radar;lightweight U-Net model;Sentinel-2 multispectral instrument data;F1 score","","1","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Automate Lithological Classification of the Amotape Tahuin Metamorphic Complex in Ecuador using Random Forest and a Multi-Sensor Satellite Imagery Approach","M. Erith; E. Jhonatan; T. Daniel; B. Marielisa; B. Franz; S. Carmen; P. Victor; L. Aracely; Z. Alfonso","The Food and Agriculture Organization of United Nations (FAO), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Instituto de Investigación Geológico y Energético (IIGE), Quito-Ecuador; Universidad Tecnológica Metropolitana (UTEM), Santiago de Chile-Chile","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2093","2096","In this paper, a methodological approach to classify lithology features using random forest and multisensor satellite imagery is presented. The incorporation of remote sensing techniques into regional geology mapping is important in order to make possible the implementation of monitoring strategies aimed to enhance national capabilities to provide information for natural disasters management, sustainable exploitation of natural resources, etc. In this context, the use of both optical/infrared and multi-wavelength synthetic aperture radar satellite data was evaluated in order to train a random forest algorithm, taking as reference field data, to generate a lithological map. Preliminary results have shown the capability of this approach to lithological classification with accuracies values higher than 80% in some conditions to map sedimentary, igneous, and metamorphic lithological classes.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323942","Landsat;Random Forest;Sentinel 1;Alos Palsar;Geology","Earth;Remote sensing;Artificial satellites;Satellites;Rocks;Radio frequency;Training","disasters;geology;geophysical image processing;geophysical prospecting;geophysical techniques;image fusion;random forests;remote sensing by radar;rocks;synthetic aperture radar;terrain mapping","multiwavelength synthetic aperture radar satellite data;random forest algorithm;reference field data;lithological map;map sedimentary;metamorphic lithological classes;automate lithological classification;amotape tahuin metamorphic complex;ecuador;multisensor satellite imagery approach;methodological approach;lithology features;remote sensing techniques;regional geology mapping;monitoring strategies;national capabilities;natural disasters management;sustainable exploitation;natural resources","","","","8","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"SAR image edge detection based on fuzzy theory and information fusion","L. Jiang; X. Ling; J. Geng; Y. Cheng","Beijing Institute of Space Long March Vehicle, China; Beijing Institute of Space Long March Vehicle, China; Beijing Institute of Space Long March Vehicle, China; Beijing Institute of Space Long March Vehicle, China","IET International Radar Conference 2015","21 Apr 2016","2015","","","1","7","Synthetic Aperture Radar (SAR) image edge detection is one of the important steps for earth observation and target recognition,and it is recognized a difficult problem by domestic and foreign researchers because of the presence of speckle noise. A study was undertaken in this paper to speckle noise suppression and edge detail holding for SAR image edge detection, and we present an intelligent edge detection approach based on fuzzy theory and information fusion. Some typical SAR images were selected for edge detection using this approach in order to verify the result of relevant speckle noise suppression and edge detail holding. Results shows that this method can effectively inhibit the SAR image speckle noise and has smart edge detection ability. Unlike classical SAR image edge detection methods, this approach is a comprehensive integrated of different intelligent information processing methods, and it shows good robustness and universality.","","978-1-78561-039-4","10.1049/cp.2015.1399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7455621","SAR image;Fuzzy theory;Speckle noise suppression;Edge detection","","edge detection;fuzzy set theory;image fusion;interference suppression;radar imaging;radar interference;speckle;synthetic aperture radar","SAR image edge detection;fuzzy theory;information fusion;synthetic aperture radar;earth observation;target recognition;speckle noise suppression;intelligent edge detection approach;intelligent information processing method","","","","","","21 Apr 2016","","","IET","IET Conferences"
"Multisource Shadow-Based Fuzzy Set (MSFS) Approach for Impervious Surfaces Mapping from Optical and SAR Data","Y. Lin; H. Zhang; P. Ma; Y. Li","Institute of Space and Earth Information Science, The Chinese University of Hong Kong, Hong Kong, China; Department of Geography, The University of Hong Kong, Hong Kong, China; Shenzhen Research Institute, The Chinese University of Hong Kong, Shenzhen, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","6817","6820","Urban impervious surfaces (UIS) indicate the environmental and socioeconomic influences of rapid urbanization. Synthetic aperture radar (SAR) reflects the scattering behaviors of different land covers while multispectral data demonstrate their physicochemical properties. Numerous studies reported that the incorporation of SAR and optical data supplement each other for better extracting UIS, nevertheless, the shadow and layover effects remain unclear, especially in very high-resolution observations. This study analyzed the shadow and layover influences from both optical and SAR data for fine resolution UIS estimation. Given the SAR shadow and layover distribution, we proposed a multisource shadow-based fuzzy set (MSFS) approach for fusing optical and SAR in optical shadow areas using decision fusion. SAR layovers showed effectiveness in UIS extraction. MSFS delivered 3% and 7% improvement in overall accuracy compared with SVM and RF using feature fusion respectively.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554229","National Natural Science Foundation of China(grant numbers:42022061,42071390,41971278); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554229","Urban impervious surface;SAR;shadows;layovers;fuzzy set theory","Support vector machines;Fuzzy sets;Optical imaging;Feature extraction;Adaptive optics;Optical sensors;Optical scattering","fuzzy set theory;geophysical image processing;image fusion;remote sensing by radar;synthetic aperture radar;terrain mapping","multisource shadow-based fuzzy set approach;SAR data;rapid urbanization;synthetic aperture radar;land cover;multispectral data;optical data supplement;fine resolution UIS estimation;optical shadow areas;physicochemical properties;urban impervious surface extraction;decision fusion;optical data","","","","15","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Quantitative Evaluation of Algae Detection Based on Deep Neural Network Multi-Source Data Fusion","L. Gao; X. Li; Y. Guo; J. Qi; B. Zhang","Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China; Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China; Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China; Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China; Institute of Oceanography, Chinese Academy of Sciences, Qingdao, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","7561","7563","This study developed a deep-learning-based model for macroalgae detection in optical Moderate Resolution Imaging Spectroradiometer (MODIS) and microwave synthetic aperture radar (SAR) images. The model reached the accuracy of 97.51(99.37)% and a mean Intersection over Union (IoU) of 42.62(86.22)% for the MODIS(SAR) images, based on the labeled 1055/4071 pairs of MODIS/SAR samples. For macroalgae detection, we designed the model based on the U-Net model with a specific-tailored modification: overfitting processing based on algae image features. We applied the model to images acquired in the Yellow Sea region frequently affected by the modifications blooming and obtained one new scientific discovery: SAR image with high-resolution has more powerful detection capabilities than MODIS image with coarse-resolution.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554235","Macroalgae detection;deep-learning model;MODIS;SAR","Laser radar;Algae;Geoscience and remote sensing;Optical imaging;Feature extraction;Radar polarimetry;Optical sensors","deep learning (artificial intelligence);feature extraction;geophysical image processing;image fusion;neural nets;oceanographic regions;oceanographic techniques;radar imaging;remote sensing by radar;synthetic aperture radar","MODIS image;deep neural network multisource data fusion;macroalgae detection;U-Net model;algae image features;SAR image;optical Moderate Resolution Imaging Spectroradiometer;microwave synthetic aperture radar;MODIS images;SAR images;Yellow Sea region;deep-learning-based model","","","","16","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Assessing Buildings Damage from Multi-Temporal Sar Images Fusion using Semantic Change Detection","L. Pang; F. Zhang; L. Li; Q. Huang; Y. Jiao; Y. Shao","Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China; Deqing Academy of Satellite Applications, Laboratory of Target Microwave Properties, Huzhou, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6292","6295","A prompt and accurate assessment of buildings' damage is critical for disaster management and emergency response. With the development of high-resolution synthetic aperture radar (SAR) and deep-learning methods, more efficient damage assessment techniques based on building-units are possible. This paper proposes a new building damage assessment method using high-resolution SAR images based on semantic change detection. It utilizes a Siamese-based module for damage change detection together with an attention mechanism-based module for semantic segmentation of the damage maps. To evaluate the proposed model, a new damage assessment dataset is constructed from the SAR imagery originated from the battle of Aleppo, Syria, for model training and testing. The experiments performed on this dataset show an overall accuracy of 88.3%. The proposed method effectively identifies the damaged areas of the buildings and grade the damage condition.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884915","National Natural Science Foundation of China(grant numbers:41671359,61471358); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884915","SAR;Buildings damage;Semantic change detection;Siamese network","Training;Open Access;Buildings;Semantics;Geoscience and remote sensing;Disaster management;Emergency services","geophysical image processing;image fusion;image segmentation;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","damage change detection;attention mechanism-based module;semantic segmentation;damage maps;damage assessment dataset;SAR imagery;damaged areas;damage condition;buildings damage;multitemporal sar images fusion;semantic change detection;prompt assessment;disaster management;emergency response;high-resolution synthetic aperture radar;deep-learning methods;efficient damage assessment techniques;building-units;building damage assessment method;high-resolution SAR images;Siamese-based module","","","","16","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Polarimetric-spatial classification of PolSAR images based on composite kernel feature fusion","X. Wang; J. Feng; Z. Cao; R. Min","Center for Information Geoscience, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Center for Information Geoscience, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Center for Information Geoscience, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Center for Information Geoscience, University of Electronic Science and Technology of China, Chengdu, Sichuan, China","2017 IEEE Radar Conference (RadarConf)","8 Jun 2017","2017","","","1455","1459","Composite kernel feature fusion is proposed in this paper for solving the classification of polarimetric synthetic aperture radar (PolSAR) images problem. The main idea is that the method of composite kernel encodes diverse information within a new kernel matrix and tunes the contribution of different type of features. The proposed approach is tested on Flevoland PolSAR data set. Experimental results verify the benefits of using both of polarimetric and spatial information by composite kernel feature fusion for the classification of PolSAR images.","2375-5318","978-1-4673-8823-8","10.1109/RADAR.2017.7944436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944436","PolSAR;Image Classification;Composite Kernel;Feature Fusion","Kernel;Stacking;Image classification;Support vector machines;Buildings;Training;Scattering","image classification;image fusion;radar imaging;radar polarimetry;synthetic aperture radar","polarimetric-spatial image classification;PolSAR images;composite kernel feature fusion;polarimetric synthetic aperture radar;Flevoland PolSAR data set;polarimetric information;spatial information","","","","15","IEEE","8 Jun 2017","","","IEEE","IEEE Conferences"
"Generative Adversarial Network for SAR-to-Optical Image Translation with Feature Cross-Fusion Inference","J. Wei; H. Zou; L. Sun; X. Cao; M. Li; S. He; S. Liu","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","6025","6028","The translation of synthetic aperture radar (SAR) to optical images provides a new solution for the interpretation of SAR images. Most of the existing translation networks are based on generative adversarial networks and use 9-residual blocks or U-Net structures in the feature inference phase. Both structures cause a large amount of information lost during the conversion of SAR image features to optical features, making the outline of the translated image blurred or semantic information lost. Aiming at this problem, this paper proposes a cross-fusion inference network structure, which preserves both high-resolution features and low-resolution features in the whole process of feature inference. Our proposed method broadens the network horizontally while deepening it vertically and improving the image translation performance. The experiments conducted on the public dataset sen1-2 show that the proposed method is superior to other networks.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884166","National Natural Science Foundation of China(grant numbers:62071474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884166","SAR-to-optical image translation;Generative adversarial network (GAN);Cross-fusion inference structure","Optical losses;Laser radar;Semantics;Optical fiber networks;Optical imaging;Generative adversarial networks;Radar polarimetry","feature extraction;image fusion;image resolution;image restoration;inference mechanisms;neural nets;optical images;radar imaging;radar resolution;synthetic aperture radar","generative adversarial network;SAR-to-optical image translation;synthetic aperture radar;optical images;U-Net structures;SAR image features;cross-fusion inference network structure;high-resolution features;low-resolution features;translated image blurred;semantic information lost","","","","11","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Robust Pol-ISAR Target Recognition Based on ST-MC-DCNN","X. Bai; X. Zhou; F. Zhang; L. Wang; R. Xue; F. Zhou","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","22 Nov 2019","2019","57","12","9912","9927","Although the deep convolutional neural network (DCNN) has been successfully applied to automatic target recognition (ATR) of ground vehicles based on synthetic aperture radar (SAR), most of the available techniques are not suitable for inverse synthetic aperture radar (ISAR) because they cannot tackle the inherent unknown deformation (e.g., translation, scaling, and rotation) among the training and test samples. To achieve robust polarimetric-ISAR (Pol-ISAR) ATR, this paper proposes the spatial transformer-multi-channel-deep convolutional neural network, i.e., ST-MC-DCNN. In this structure, we adopt the double-layer spatial transformer network (STN) module to adjust the image deformation of each polarimetric channel and then perform a robust hierarchical feature extraction by MC-DCNN. Finally, we carry out feature fusion in the concatenation layer and output the recognition result by the softmax classifier. The proposed network is end-to-end trainable and could learn the optimal deformation parameters automatically from training samples. For the fully Pol-ISAR image database generated from electromagnetic (EM) echoes of four satellites, the proposed structure achieves higher recognition accuracy than traditional DCNN and MC-DCNN. Additionally, it has shown robustness to image scaling, rotation, and combined deformation.","1558-0644","","10.1109/TGRS.2019.2930112","National Natural Science Foundation of China(grant numbers:61522114,61631019); Foundation for the Author of National Excellent Doctoral Dissertation of the People's Republic of China(grant numbers:201448); Fund for Foreign Scholars in University Research and Teaching Programs (the 111 Project)(grant numbers:B18039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804365","Automatic target recognition (ATR);deep convolutional neural network (DCNN);image deformation;inverse synthetic aperture radar (ISAR)","Feature extraction;Scattering;Strain;Target recognition;Shape;Azimuth;Image recognition","convolutional neural nets;feature extraction;image classification;image fusion;image recognition;radar imaging;radar polarimetry;radar target recognition;synthetic aperture radar;telecommunication computing","robust Pol-ISAR target recognition;ST-MC-DCNN;automatic target recognition;inverse synthetic aperture radar;inherent unknown deformation;polarimetric-ISAR ATR;double-layer spatial transformer network module;image deformation;polarimetric channel;robust hierarchical feature extraction;concatenation layer;recognition result;optimal deformation parameters;training samples;Pol-ISAR image database;image scaling;electromagnetic echoes;feature fusion;spatial transformer-multichannel-deep convolutional neural network","","32","","62","IEEE","16 Aug 2019","","","IEEE","IEEE Journals"
"Incoherent fusion of 3D InISAR images using multi-temporal and multi-static data","F. Salvetti; E. Giusti; D. Staglianò; M. Martorella","CNIT - RaSS, Pisa, Italy; CNIT - RaSS, Pisa, Italy; Department of Information Engineering, CNIT - RaSS/University of Pisa, Pisa, Italy; Department of Information Engineering, CNIT - RaSS/University of Pisa, Pisa, Italy","2016 IEEE Radar Conference (RadarConf)","9 Jun 2016","2016","","","1","6","It has recently been demonstrated that Interferometric ISAR is able to reconstruct 3D point-like target images. However, such 3D reconstructions are generally composed of a small amount of points as the number of scatterers that can be accurately extracted from the received signal and placed in a 3D space is limited by the SNR associated to each of them. This lack of information may cause difficulties for targets identification and classification. To overcome this issue, a 3D reconstruction method based on the use of multi-temporal and multistatic data that makes use of an incoherent 3D image fusion technique is presented in this paper.","2375-5318","978-1-5090-0863-6","10.1109/RADAR.2016.7485133","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7485133","","Three-dimensional displays;Radar imaging;Image reconstruction;Radar antennas;Image fusion;Sensors","feature extraction;image classification;image fusion;image reconstruction;radar imaging;radar interferometry;radar target recognition;stereo image processing;synthetic aperture radar","interferometric ISAR;3D InISAR images fusion;multitemporal data;multistatic data;3D point-like target images reconstruction;targets identification;targets classification","","8","","4","IEEE","9 Jun 2016","","","IEEE","IEEE Conferences"
"Classification on the Monogenic Scale Space: Application to Target Recognition in SAR Image","G. Dong; G. Kuang","School of Electronics Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronics Science and Engineering, National University of Defense Technology, Changsha, China","IEEE Transactions on Image Processing","11 May 2015","2015","24","8","2527","2539","This paper introduces a novel classification strategy based on the monogenic scale space for target recognition in Synthetic Aperture Radar (SAR) image. The proposed method exploits monogenic signal theory, a multidimensional generalization of the analytic signal, to capture the characteristics of SAR image, e.g., broad spectral information and simultaneous spatial localization. The components derived from the monogenic signal at different scales are then applied into a recently developed framework, sparse representation-based classification (SRC). Moreover, to deal with the data set, whose target classes are not linearly separable, the classification via kernel combination is proposed, where the multiple components of the monogenic signal are jointly considered into a unifying framework for target recognition. The novelty of this paper comes from: the development of monogenic feature via uniformly downsampling, normalization, and concatenation of the components at various scales; the development of score-level fusion for SRCs; and the development of composite kernel learning for classification. In particular, the comparative experimental studies under nonliteral operating conditions, e.g., structural modifications, random noise corruption, and variations in depression angle, are performed. The comparative experimental studies of various algorithms, including the linear support vector machine and the kernel version, the SRC and the variants, kernel SRC, kernel linear representation, and sparse representation of monogenic signal, are performed too. The feasibility of the proposed method has been successfully verified using Moving and Stationary Target Acquiration and Recognition database. The experimental results demonstrate that significant improvement for recognition accuracy can be achieved by the proposed method in comparison with the baseline algorithms.","1941-0042","","10.1109/TIP.2015.2421440","National Natural Science Foundation of China(grant numbers:61201338,61401477); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083742","The monogenic signal;sparse representation;SAR target recognition;composite kernel learning;score-level fusion;monogenic scale-space;The monogenic signal;sparse representation;SAR target recognition;composite kernel learning;score-level fusion;monogenic scale-space","Kernel;Synthetic aperture radar;Encoding;Training;Target recognition;Transforms;Hilbert space","image classification;image fusion;image representation;image sampling;radar imaging;support vector machines;synthetic aperture radar","monogenic scale space classification;target recognition application;SAR imaging;synthetic aperture radar;monogenic signal theory;multidimensional analytic signal generalization;spatial localization;spectral information;sparse representation-based classification;SRC;kernel combination;score-level fusion development;composite kernel learning version;structural modification;random noise corruption;depression angle variation;linear support vector machine;kernel linear representation;sparse representation;moving and stationary target acquiration and recognition database","","106","","52","IEEE","9 Apr 2015","","","IEEE","IEEE Journals"
"Multimodal Bilinear Fusion Network With Second-Order Attention-Based Channel Selection for Land Cover Classification","X. Li; L. Lei; Y. Sun; M. Li; G. Kuang","State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China; State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Mar 2020","2020","13","","1011","1026","As two different tools for earth observation, the optical and synthetic aperture radar (SAR) images can provide complementary information of the same land types for better land cover classification. However, because of the different imaging mechanisms of optical and SAR images, how to efficiently exploit the complementary information becomes an interesting and challenging problem. In this article, we propose a novel multimodal bilinear fusion network (MBFNet), which is used to fuse the optical and SAR features for land cover classification. The MBFNet consists of three components: the feature extractor, the second-order attention-based channel selection module (SACSM), and the bilinear fusion module. First, in order to avoid the network parameters tempting to ingratiate dominant modality, the pseudo-siamese convolutional neural network (CNN) is taken as the feature extractor to extract deep semantic feature maps of optical and SAR images, respectively. Then, the SACSM is embedded into each stream, and the fine channel-attention maps with second-order statistics are obtained by bilinear integrating the global average-pooling and global max-pooling information. The SACSM can not only automatically highlight the important channels of feature maps to improve the representation power of networks, but also uses the channel selection mechanism to reconfigure compact feature maps with better discrimination. Finally, the bilinear pooling is used as the feature-level fusion method, which establishes the second-order association between two compact feature maps of the optical and SAR streams to obtain the low-dimension bilinear fusion features for land cover classification. Experimental results on three broad coregistered optical and SAR datasets demonstrate that our method achieves more effective land cover classification performance than the state-of-the-art methods.","2151-1535","","10.1109/JSTARS.2020.2975252","National Natural Science Foundation of China(grant numbers:61971426); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9019866","Attention mechanism;bilinear pooling model;convolutional neural network (CNN);feature fusion;land cover classification;multimodal learning","Optical imaging;Feature extraction;Synthetic aperture radar;Optical sensors;Machine learning;Adaptive optics;Optical scattering","convolutional neural nets;feature extraction;geophysical image processing;higher order statistics;image classification;image fusion;image representation;land cover;learning (artificial intelligence);optical radar;radar imaging;remote sensing by radar;synthetic aperture radar","pseudosiamese convolutional neural network;feature extractor;SACSM;global max-pooling information;compact feature maps;feature-level fusion method;low-dimension bilinear fusion features;effective land cover classification performance;multimodal bilinear fusion network;complementary information;second-order attention-based channel selection module;bilinear fusion module;network parameters;earth observation;synthetic aperture radar;SAR images;optical radar;optical imaging mechanisms;MBFNet;SAR feature fusion;optical feature fusion;dominant modality;CNN;deep semantic feature map extraction;channel-attention maps;global average-pooling;second-order statistics;network representation power;second-order association;SAR datasets;coregistered optical datasets","","25","","59","CCBY","2 Mar 2020","","","IEEE","IEEE Journals"
"Runway Detection in SAR Images Based on Fusion Sparse Representation and Semantic Spatial Matching","W. Lv; K. Dai; L. Wu; X. Yang; W. Xu","Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN; Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN; Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN; Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN; Zhejiang Sci-Tech University, Hangzhou, Zhejiang, CN","IEEE Access","13 Jun 2018","2018","6","","27984","27992","In this paper, a novel algorithm is presented for runway detection in synthetic aperture radar images. It involves two steps: runway assessment and confirmation. In the first step, the primary runway (PR) and the auxiliary runway (AR) of the airport are assessed by using a sparse representation fusion frame. A set of residuals, for each feature from PR or AR, is first generated by performing sparse reconstructions over two training dictionaries constructed by a set of discriminative features. Based on all residuals for all types of features, two residual sequences for PR and AR are, respectively, built. To improve the assessment performance, these two residual sequences are normalized and further linearly fused. An assessment criterion is applied to the fusion result to infer an optimal target estimate. In the second step, the histogram of oriented gradient feature descriptors of PR, AR, and the entire runway region constructed by PR, AR, and taxiways are first generated. Afterward, two semantic spatial rules are developed to verify each candidate region of interest. If a perfect match is achieved, the candidate target can be confirmed. Since the PR and AR are selected based on the residual fusion related with the concatenation of multiple features, the presented algorithm has a good representation ability to the runway. By introducing semantic spatial relationships into the confirmation scheme, this algorithm can well discriminate other runway-like targets. The test results using real scene data demonstrate that the presented method has superiority to some state-of-the-art alternatives.","2169-3536","","10.1109/ACCESS.2018.2839025","National Natural Science Foundation of China(grant numbers:61601410,61374020); Natural Science Foundation of Zhejiang Province(grant numbers:LY16F010018,LY16F010017,LR15F010002,LQ12F01009); Science Foundation of Zhejiang Sci-Tech University(grant numbers:15032085-Y); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8361411","Runway detection;sparse representation;multifeature fusion;histogram of oriented gradient;semantic spatial matching","Semantics;Feature extraction;Airports;Histograms;Synthetic aperture radar;Dictionaries;Training","image fusion;image matching;image representation;image sequences;object detection;radar imaging;synthetic aperture radar","runway detection;SAR images;semantic spatial matching;synthetic aperture radar images;runway assessment;primary runway;PR;auxiliary runway;sparse representation fusion frame;sparse reconstructions;discriminative features;residual sequences;assessment performance;oriented gradient feature descriptors;semantic spatial rules;residual fusion;multiple features;semantic spatial relationships;runway-like targets;real scene data;runway confirmation","","8","","28","OAPA","21 May 2018","","","IEEE","IEEE Journals"
"Self-Supervised SAR-Optical Data Fusion of Sentinel-1/-2 Images","Y. Chen; L. Bruzzone","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Transactions on Geoscience and Remote Sensing","17 Feb 2022","2022","60","","1","11","The effective combination of the complementary information provided by huge amount of unlabeled multisensor data (e.g., synthetic aperture radar (SAR) and optical images) is a critical issue in remote sensing. Recently, contrastive learning methods have reached remarkable success in obtaining meaningful feature representations from multiview data. However, these methods only focus on image-level features, which may not satisfy the requirement for dense prediction tasks such as land-cover mapping. In this work, we propose a self-supervised framework for SAR-optical data fusion and land-cover mapping tasks. SAR and optical images are fused by using a multiview contrastive loss at image level and super-pixel level according to one of those possible strategies: in the early, intermediate, and late strategies. For the land-cover mapping task, we assign each pixel a land-cover class by the joint use of pretrained features and spectral information of the image itself. Experimental results show that the proposed approach not only achieves a comparable accuracy but also reduces the dimension of features with respect to the image-level contrastive learning method. Among three fusion strategies, the intermediate fusion strategy achieves the best performance. The combination of the pixel-level fusion approach and the self-training on spectral indices leads to further improvements in the land-cover mapping task with respect to the image-level fusion approach, especially with sparse pseudo labels. The code to reproduce our results will be found at https://github.com/yusin2it/SARoptical_fusion.","1558-0644","","10.1109/TGRS.2021.3128072","China Scholarship Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9614157","Data fusion;land-cover mapping;pixel level;remote sensing;self-supervised learning;Sentinel-1/-2","Task analysis;Synthetic aperture radar;Optical imaging;Optical sensors;Deep learning;Training;Fuses","feature extraction;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);radar imaging;remote sensing;synthetic aperture radar","multiview contrastive loss;image level;super-pixel level;early strategies;late strategies;land-cover mapping task;land-cover class;pretrained features;image-level contrastive learning method;fusion strategies;intermediate fusion strategy;pixel-level fusion approach;image-level fusion approach;self-supervised SAR-optical data fusion;unlabeled multisensor data;synthetic aperture radar;meaningful feature representations;multiview data;image-level features;dense prediction tasks;optical images","","7","","46","IEEE","12 Nov 2021","","","IEEE","IEEE Journals"
"A Novel Multi-Feature Joint Learning Method for Fast Polarimetric SAR Terrain Classification","J. Shi; H. Jin; X. Li","School of Computer Science and Technology, Xi’an University of Technology, Xi’an, China; School of Computer Science and Technology, Xi’an University of Technology, Xi’an, China; School of Computer Science and Technology, Xi’an University of Technology, Xi’an, China","IEEE Access","19 Feb 2020","2020","8","","30491","30503","Polarimetric synthetic aperture radar (PolSAR) image classification is one of the most important study areas for PolSAR image processing. Many kinds of PolSAR features can be extracted for PolSAR image classification, such as the scattering, polarimetric or image features. However, it is difficult to improve the classification accuracy of PolSAR images by using all these low-level features directly, since they may conflict with each other for classification. Hence, how to joint learn these low-level features to obtain high-level discriminating features is a challenging task. To solve this problem, a novel fast multi-feature joint learning method(fMF-JLC) is proposed for PolSAR image classification. The proposed method extract three kinds of low-level features of PolSAR data at first. Then, a multi-feature joint sparse representation model(MF-JSR) is proposed by designing joint sparse constraints on the extracted features above. Moreover, the joint sparse features are further compressed to overcome the dimension curse and acquire semantic features by the topic model. By this way, the low-level features are fused and discriminating high-level features are acquired. However, the pixel-wise feature learning method is time consuming. To speed the proposed method, a superpixel-based fast learning method is designed by involving the contextual relationship. Experiments are taken on three sets of real PolSAR data with different sensors and bands, and several compared methods are used to verify the effectiveness of the proposed method. The experimental results illustrate that the proposed method can obtain better performance than the state-of-art methods, especially for the heterogenous areas.","2169-3536","","10.1109/ACCESS.2020.2973246","National Basic Research Program of China (973 Program)(grant numbers:2013CB329402); National Natural Science Foundation of China(grant numbers:61113090,61472319,61703333,61502382); Natural Science Foundation of Shaanxi Province(grant numbers:2018JQ6055,2019JQ-740); Research Project of Shaanxi Provincial Education Department(grant numbers:19JK0566); Technology Innovation Project of Xi’an University of Technology(grant numbers:112/256081620); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8994163","Polarimetric SAR classification;joint multi-feature sparse representation;joint multi-feature learning;fast classification method","Feature extraction;Scattering;Learning systems;Data mining;Semantics;Synthetic aperture radar;Matrix decomposition","feature extraction;geophysical image processing;image classification;image fusion;image representation;learning (artificial intelligence);radar imaging;radar polarimetry;synthetic aperture radar","fast polarimetric SAR terrain classification;polarimetric synthetic aperture radar;PolSAR image processing;PolSAR features;PolSAR image classification;image features;PolSAR images;low-level features;high-level discriminating features;PolSAR data;multifeature joint sparse representation model;joint sparse features;semantic features;high-level features;pixel-wise feature learning method;superpixel-based fast learning method;multifeature joint learning method;MF-JSR;fMF-JLC","","7","","55","CCBY","11 Feb 2020","","","IEEE","IEEE Journals"
"Terrain Matching by Fusing HOG With Zernike Moments","K. Wang; H. Wang; J. Wang","Beihang University, Beijing, China; Beihang University, Beijing, China; University of New South Wales Australia, Sydney, Australia","IEEE Transactions on Aerospace and Electronic Systems","10 Apr 2020","2020","56","2","1290","1300","A new matching algorithm is proposed by fusing the histogram of oriented gradients' and the Zernike moments' descriptors extracted from the real-time synthetic aperture radar (SAR) images and the real-time elevation maps (REMs), respectively. Both the SAR images and the REMs are acquired by an on-board interferometric SAR simultaneously. The two reciprocal descriptors are fused by weighting their Canberra distances. The numerical experiments demonstrate the advantage of the proposed matching algorithm preliminarily.","1557-9603","","10.1109/TAES.2019.2930016","National Natural Science Foundation of China(grant numbers:41774014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770163","Histogram of oriented gradient (HOG);scene matching;terrain-aided navigation (TAN);terrain matching;Zernike moments (ZMs).","Feature extraction;Histograms;Radar polarimetry;Synthetic aperture radar;Real-time systems;Classification algorithms;Detectors","feature extraction;image fusion;image matching;radar imaging;radar interferometry;remote sensing by radar;synthetic aperture radar;terrain mapping","Canberra distances;histogram of oriented gradient fusion;HOG fusion;SAR images;real-time elevation maps;real-time synthetic aperture radar images;Zernike moment descriptor;terrain matching;matching algorithm;reciprocal descriptors;on-board interferometric SAR;REMs","","4","","36","IEEE","23 Jul 2019","","","IEEE","IEEE Journals"
"A sparse reconstruction and fusion technique for building footprint extraction from multi-aspect high-resolution SAR data","Di Feng; Xi Chen; T. Wang; Weidong Chen","Key Laboratory of Electromagnetic Space Information of Chinese Academy of Sciences, University of Science and Technology of China, Hefei, Anhui, P. R. China; Key Laboratory of Electromagnetic Space Information of Chinese Academy of Sciences, University of Science and Technology of China, Hefei, Anhui, P. R. China; Key Laboratory of Electromagnetic Space Information of Chinese Academy of Sciences, University of Science and Technology of China, Hefei, Anhui, P. R. China; Key Laboratory of Electromagnetic Space Information of Chinese Academy of Sciences, University of Science and Technology of China, Hefei, Anhui, P. R. China","2015 IEEE Radar Conference (RadarCon)","25 Jun 2015","2015","","","0420","0425","Traditional synthetic aperture radar (SAR) image processing for building detection and reconstruction faces the problems of sidelobe interference, geometric distortions and surrounding environment aliasing. In this paper, a sparse reconstruction and fusion technique is proposed for building footprint extraction from multi-aspect high-resolution SAR data. It can process different kinds of buildings including flat-roof buildings and gable-roof buildings. The building wall-ground double-bounce is mainly taken into account for its strong scattering properties and sparsity. First, double-bounce is extracted by compressive sensing (CS)-based sparse reconstruction and Hough transform. Then, multi-aspect data are projected and fused, and L-structures are extracted afterward. Finally, qualification by scores of confidence is proposed for the test of footprint candidates, and building footprint map is derived. Numerical experiment and comparison with traditional methods are carried out to show the effectiveness and performance improvement of the proposed method, with far fewer echo samples used.","2375-5318","978-1-4799-8232-5","10.1109/RADAR.2015.7131036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131036","","Buildings;Scattering;Image reconstruction;Data mining;Synthetic aperture radar;Geometry;Backscatter","compressed sensing;feature extraction;Hough transforms;image fusion;image reconstruction;image resolution;object detection;radar detection;radar imaging;synthetic aperture radar","CS sparse reconstruction technique;sparse fusion technique;building footprint extraction;multiaspect high-resolution SAR data;synthetic aperture radar image processing;building detection face;building reconstruction face;sidelobe interference problem;geometric distortion problem;surrounding environment aliasing problem;flat-roof buildings;gable-roof buildings;building wall-ground double-bounce;scattering properties;compressive sensing;Hough transform","","","","12","IEEE","25 Jun 2015","","","IEEE","IEEE Conferences"
"Fusion Detection of Closed Water in Medium-Low Resolution Remote Sensing Imagery","Y. Ning; Y. You; J. Cao; F. Liu; Q. Yan; Y. Zhang","School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications; School of Artificial Intelligence, Beijing University of Posts and Telecommunications","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4027","4030","Aiming at the closed water detection in remote sensing imagery at medium-low resolution, this paper proposes a novel method for closed water detection based on fusion detection which conducts detection via informative fused images blended by Synthetic Aperture Radar (SAR) and optical images. Firstly, it utilizes SAR and optical image pairs containing the same closed water object to generate aligned image pairs according to latitude and longitude information. Next, generative adversarial network (GAN) is adopted to fuse two categories of images. At last, a target detection network driven by optical image samples is used to detect the closed water on the fused image. The experiment result on Sentinel-1&2 shows that the proposed method can effectively make up for the shortage of SAR image in closed water detection and improve the detection performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553554","Beijing Natural Science Foundation, China(grant numbers:4214058); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553554","Closed water detection;Image fusion;GAN","Image resolution;Object detection;Optical fiber networks;Optical imaging;Generative adversarial networks;Adaptive optics;Radar polarimetry","geophysical image processing;geophysical techniques;image fusion;image resolution;object detection;optical images;radar imaging;remote sensing;synthetic aperture radar","target detection network;optical image samples;fused image;SAR image;closed water detection;detection performance;fusion detection;medium-low resolution remote sensing imagery;informative fused images;optical images;optical image pairs;closed water object;aligned image pairs","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Multimodal Classification of Remote Sensing Images: A Review and Future Directions","L. Gómez-Chova; D. Tuia; G. Moser; G. Camps-Valls","Image Processing Laboratory (IPL), Universitat de Valéncia, Valéncia, Spain; Department of Geography, University of Zürich, Zürich, Switzerland; Department of Electrical, Electronic, Telecommunications Engineering and Naval Architecture (DITEN), University of Genoa, Genova, Italy; Image Processing Laboratory (IPL), Universitat de Valéncia, Valéncia, Spain","Proceedings of the IEEE","21 Aug 2015","2015","103","9","1560","1584","Earth observation through remote sensing images allows the accurate characterization and identification of materials on the surface from space and airborne platforms. Multiple and heterogeneous image sources can be available for the same geographical region: multispectral, hyperspectral, radar, multitemporal, and multiangular images can today be acquired over a given scene. These sources can be combined/fused to improve classification of the materials on the surface. Even if this type of systems is generally accurate, the field is about to face new challenges: the upcoming constellations of satellite sensors will acquire large amounts of images of different spatial, spectral, angular, and temporal resolutions. In this scenario, multimodal image fusion stands out as the appropriate framework to address these problems. In this paper, we provide a taxonomical view of the field and review the current methodologies for multimodal classification of remote sensing images. We also highlight the most recent advances, which exploit synergies with machine learning and signal processing: sparse methods, kernel-based fusion, Markov modeling, and manifold alignment. Then, we illustrate the different approaches in seven challenging remote sensing applications: 1) multiresolution fusion for multispectral image classification; 2) image downscaling as a form of multitemporal image fusion and multidimensional interpolation among sensors of different spatial, spectral, and temporal resolutions; 3) multiangular image classification; 4) multisensor image fusion exploiting physically-based feature extractions; 5) multitemporal image classification of land covers in incomplete, inconsistent, and vague image sources; 6) spatiospectral multisensor fusion of optical and radar images for change detection; and 7) cross-sensor adaptation of classifiers. The adoption of these techniques in operational settings will help to monitor our planet from space in the very near future.","1558-2256","","10.1109/JPROC.2015.2449668","Generalitat Valenciana; Swiss National Science Foundation(grant numbers:PP00P2_150593); Spanish Ministry of Economy and Competitiveness (MINECO); Italian Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7182258","Classification;fusion;multiangular;multimodal image analysis;multisource;multitemporal;remote sensing;Classification;fusion;multiangular;multimodal image analysis;multisource;multitemporal;remote sensing","Remote sensing;Spatial resolution;Sensors;Satellites;Image fusion;Synthetic aperture radar","geophysical image processing;geophysical techniques;image classification;image fusion;remote sensing","remote sensing image;image multimodal classification;Earth observation;material identification;material characterization;space platforms;airborne platforms;heterogeneous image sources;satellite sensors;material classification;multimodal image fusion;signal processing;machine learning;sparse methods;kernel-based fusion;Markov modeling;manifold alignment;multiresolution fusion;multispectral image classification;multitemporal image fusion;multidimensional interpolation;optical images;radar images","","247","","160","IEEE","7 Aug 2015","","","IEEE","IEEE Journals"
"Polar-Spatial Feature Fusion Learning With Variational Generative-Discriminative Network for PolSAR Classification","Z. Wen; Q. Wu; Z. Liu; Q. Pan","Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding, Ministry of Education, Xidian University, Xi’an, China; Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China; Key Laboratory of Information Fusion Technology of Ministry of Education, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","30 Oct 2019","2019","57","11","8914","8927","Feature learning-based polarimetric synthetic aperture radar (PolSAR) classification model will generally suffer from the challenge of deficient labeled pixels. In this paper, we propose a novel generative-discriminative network for PolSAR polar-spatial feature fusion learning and classification, which comprises of a deep generative network and a discriminative network with their bottom layers shared. With this architecture, it enables to make use of both labeled and unlabeled pixels in a PolSAR image for model learning in a semisupervised way. Moreover, the proposed network imposes a Gaussian random field prior and a conditional random field posterior on the learned fusion features and the output label configuration, respectively. Without the need of the complicated recurrent iterations, our network can still efficiently produce the structured fusion feature as well as a smoothed classification map by involving some auxiliary variables, and it is specifically optimized via variational inference within an alternating direction method of multipliers iteration scheme. Extensive experiments on different benchmark PolSAR imageries demonstrate the effectiveness and superiority of the proposed network. Compared with other state-of-the-art algorithms of PolSAR feature learning and classification, our model can achieve a much better performance in terms of the visual quality of the label map and overall classification accuracy, facilitating the much less labeling pixels.","1558-0644","","10.1109/TGRS.2019.2923738","National Natural Science Foundation of China(grant numbers:61806165,61790552); Northwestern Polytechnical University(grant numbers:31020180QD040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765386","Deep learning;discriminative model;feature fusion;generative model;polarimetric synthetic aperture radar (PolSAR) classification;variational inference","Feature extraction;Data models;Scattering;Task analysis;Covariance matrices;Adaptation models;Deep learning","image classification;image fusion;iterative methods;neural nets;radar computing;radar imaging;radar polarimetry;synthetic aperture radar","variational generative-discriminative network;PolSAR classification;deficient labeled pixels;PolSAR polar-spatial feature fusion learning;deep generative network;unlabeled pixels;PolSAR image;Gaussian random field;conditional random field;learned fusion features;output label configuration;structured fusion feature;smoothed classification map;benchmark PolSAR imagery;feature learning;polarimetric synthetic aperture radar;complicated recurrent iterations;alternating direction method of multipliers iteration","","16","","47","IEEE","17 Jul 2019","","","IEEE","IEEE Journals"
"High Spatio-Temporal Resolution Deformation Time Series With the Fusion of InSAR and GNSS Data Using Spatio-Temporal Random Effect Model","N. Liu; W. Dai; R. Santerre; J. Hu; Q. Shi; C. Yang","School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; Center for Research in Geomatics, Laval University, Quebec, Canada; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China; School of Geosciences and Info-Physics, Central South University, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","24 Dec 2018","2019","57","1","364","380","High spatio-temporal resolution deformation series can be used to improve the understanding of deformation mechanism, thereby contributing to prevention and control of geological disasters such as mine subsidence, landslide, and earthquake. Among ground deformation monitoring technologies, global navigation satellite system has high temporal resolution but low spatial resolution, and interferometric synthetic aperture radar (InSAR) has high spatial resolution but low temporal resolution. Fusing these two data may generate high spatio-temporal resolution deformation series. Existing fusion methods usually use the bi-direction interpolation, which does not consider the spatio-temporal cross correlation and is computationally extensive. We propose a dynamic filtering fusion model based on the spatio-temporal random effect (a spatio-temporal Kalman filter) model. Experiments with simulated data and real data from the Los Angeles area are conducted to validate this method. Simulated experimental results are compared with truth data and the Los Angeles experiment data results are verified using the leave-one InSAR image-out validation method. The RMS results for them are around 13.8 and 5 mm, respectively, indicating that the proposed method can achieve high accuracy and high spatial-temporal resolution deformation time series.","1558-0644","","10.1109/TGRS.2018.2854736","National Natural Science Foundation of China(grant numbers:41674011); State Key Development Program of Basic Research of China(grant numbers:2013CB733303); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8439087","Deformation;fusion;global navigation satellite system (GNSS);high spatio-temporal resolution;interferometric synthetic aperture radar (InSAR)","Global navigation satellite system;Strain;Spatial resolution;Computational modeling;Mathematical model;Data models;Geologic measurements","disasters;geophysical techniques;image filtering;image fusion;image resolution;interpolation;Kalman filters;radar imaging;radar interferometry;radar resolution;remote sensing by radar;satellite navigation;synthetic aperture radar","temporal resolution;spatial resolution;InSAR data fusion;GNSS data fusion;geological disasters;mine subsidence;landslide;earthquake;global navigation satellite system;leave-one InSAR image-out validation method;Los Angeles;dynamic filtering fusion model;bidirection interpolation;interferometric synthetic aperture radar;spatio-temporal Kalman filter;spatio-temporal cross correlation;ground deformation monitoring technologies;spatio-temporal random effect model;high spatio-temporal resolution deformation time series","","11","","32","IEEE","17 Aug 2018","","","IEEE","IEEE Journals"
"Superpixel-Oriented Unsupervised Classification for Polarimetric SAR Images Based on Consensus Similarity Network Fusion","H. Zou; M. Li; N. Shao; X. Qin","College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; School of Information and Navigation, Air Force Engineering University, Xi’an, China","IEEE Access","24 Jun 2019","2019","7","","78347","78366","Unsupervised polarimetric synthetic aperture radar (PolSAR) image classification is an important task in PolSAR automatic image analysis and interpretation. Generally, a group of features is insufficient to effectively classify PolSAR images, especially in multiple terrain scenarios. Therefore, multiple features need to be extracted for PolSAR image classification. However, how to combine and integrate these features effectively to fully utilize each feature's information and discriminability need to be determined. Such integrated work has traditionally received little attention. In this paper, a novel unsupervised classification framework for PolSAR images is proposed. First, a PolSAR image is oversegmented via a fast superpixel segmentation method. Second, five feature vectors are extracted from PolSAR images via superpixels, resulting in five corresponding similarity matrices that are constructed by using Gaussian kernels. Third, consensus similarity network fusion (CSNF), originally proposed and widely used for biomedical sciences, is employed to combine and integrate the five similarity matrices to obtain a fused similarity matrix. Fourth, spectral clustering method, based on the fused similarity matrix, is used to cluster the PolSAR image. Finally, a novel classification postprocessing procedure is presented and exploited to smooth the initial clusters and correct some misclassified pixels. The extensive experimental results conducted on one simulated and two real-world PolSAR images demonstrate the feasibility and superiority of the proposed method compared with five other state-of-the-art classification approaches.","2169-3536","","10.1109/ACCESS.2019.2922473","National Natural Science Foundation of China(grant numbers:61331015,41601436); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736007","Polarimetric synthetic aperture radar (PolSAR) images;unsupervised classification;superpixels segmentation;consensus similarity network fusion (CSNF);spectral clustering","Feature extraction;Image segmentation;Image classification;Kernel;Image color analysis;Speckle;Computational efficiency","feature extraction;Gaussian processes;image classification;image fusion;image segmentation;matrix algebra;pattern clustering;radar computing;radar imaging;radar polarimetry;synthetic aperture radar;unsupervised learning","superpixel-oriented unsupervised classification;polarimetric SAR images;consensus similarity network fusion;unsupervised polarimetric synthetic aperture radar image classification;PolSAR image classification;novel unsupervised classification framework;fused similarity matrix;real-world PolSAR images;PolSAR automatic image interpretation;fast superpixel segmentation method;feature vector extraction;biomedical sciences;spectral clustering method;classification postprocessing procedure","","4","","46","OAPA","12 Jun 2019","","","IEEE","IEEE Journals"
"Self-Distillation Feature Learning Network for Optical and SAR Image Registration","D. Quan; H. Wei; S. Wang; R. Lei; B. Duan; Y. Li; B. Hou; L. Jiao","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; School of Telecommunications, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education of China, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","25 May 2022","2022","60","","1","18","Optical and synthetic aperture radar (SAR) image registration is important for multimodal remote sensing image information fusion. Recently, deep matching networks have shown better performances than traditional methods of image matching. However, due to significant differences between optical and SAR images, the performances of existing deep learning methods still need to be further improved. This article proposes a self-distillation feature learning network (SDNet) for optical and SAR image registration, improving performance from network structure and network optimization. First, we explore the impact of different weight-sharing strategies on optical and SAR image matching. Then, we design a partially unshared feature learning network for multimodal image feature learning. It has fewer parameters than the fully unshared network and has more flexibility than the fully shared network. In addition, the limited binary supervised information (matching or nonmatching) is insufficient to train the deep matching networks for optical-SAR image registration. Thus, we propose a self-distillation feature learning method to exploit more similarity information for deep network optimization enhancement, such as the similarity ordering between a series of nonmatching patch pairs. The exploited rich similarity information will significantly enhance network training and improve matching accuracy. Finally, existing deep learning methods brute-force make the matching features of the optical and SAR image patches similar, which will lead to the loss of discriminative information and degeneration of the matching performances. Thus, we build an auxiliary task reconstruction learning to optimize the feature learning network to keep more discriminative information. Extensive experiments demonstrate the effectiveness of our proposed method on multimodal image registration.","1558-0644","","10.1109/TGRS.2022.3173476","Key Research and Development Program of Shaanxi(grant numbers:2021ZDLGY01-06,2022ZDLGY0112); National Key Research and Development Program of China(grant numbers:2021ZD0110404); National Natural Science Foundation of China(grant numbers:62171347); Key Scientific Technological Innovation Research Project by Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770793","Deep matching;image registration;multimodal image;optical image;self-distillation learning;synthetic aperture radar (SAR) image","Feature extraction;Optical imaging;Optical sensors;Image registration;Radar polarimetry;Adaptive optics;Representation learning","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;image matching;image registration;learning (artificial intelligence);radar imaging;remote sensing;synthetic aperture radar","deep network optimization enhancement;exploited rich similarity information;network training;matching accuracy;deep learning methods brute-force;matching features;optical SAR image patches;discriminative information;matching performances;multimodal image registration;self-distillation feature learning network;synthetic aperture radar image registration;multimodal remote sensing image information fusion;deep matching networks;SAR images;network structure;different weight-sharing strategies;SAR image matching;partially unshared feature learning network;multimodal image feature;fully unshared network;fully shared network;optical-SAR image registration","","3","","48","IEEE","9 May 2022","","","IEEE","IEEE Journals"
"DFAF-Net: A Dual-Frequency PolSAR Image Classification Network Based on Frequency-Aware Attention and Adaptive Feature Fusion","Y. Cao; Y. Wu; M. Li; W. Liang; X. Hu","Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; National Key Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China; Remote Sensing Image Processing and Fusion Group, School of Electronic Engineering, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","31 Mar 2022","2022","60","","1","18","Multifrequency (MF) polarization synthetic aperture radar (PolSAR) systems can obtain more abundant and continuous earth resource information than single-frequency ones and have been widely used in the remote sensing community. However, there are relatively a few researches for the fine MF PolSAR image classification, which is an important part of remote sensing image interpretation. The main focus currently is on the single-frequency part. Therefore, for dual-frequency PolSAR image classification, this article proposes the dual-frequency attention fusion network (DFAF-Net). It is based on frequency-aware attention and adaptive feature fusion to improve classification performance. First, the dual-frequency PolSAR data is input into the joint feature extraction (JFE) module to obtain the joint feature representation. Meanwhile, two frequency-aware attention block (FAB) modules with the same structure are constructed, which, respectively, generate frequency-specific attention masks based on the guidance of different frequency data. Subsequently, these masks are used to weigh the joint features to highlight the description of different frequency-aware importance. The activated frequency-aware features can fully mine and utilize the complementary information provided by different frequencies, thereby enhancing the discrimination of similar landcover categories. Finally, the adaptive feature fusion block (AFFB) module is utilized to adaptively aggregate different frequency-aware features multiple times, which can effectively eliminate information differences. The obtained fusion features are more compact within classes and separable between classes, thereby effectively improving the classification performance. Experiments on three measured spaceborne and airborne dual-frequency PolSAR datasets verify that DFAF-Net can better perceive frequency characteristics and fully mine the complementary. Therefore, the classification accuracy is effectively enhanced, and the inaccuracy of single-frequency classification can be eliminated. Meanwhile, due to the introduction of the attention module, the classification performance of the proposed DFAF-Net is more competitive than the related deep learning networks. Quantitatively, the overall accuracy of DFAF-Net on the three datasets is respectively 98.30%, 97.42%, and 99.45%, which is better than other methods.","1558-0644","","10.1109/TGRS.2022.3152854","Natural Science Foundation of China(grant numbers:62172321,61871312); Civil Space Thirteen Five Years Pre-Research Project(grant numbers:D040114); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9717232","Adaptive feature fusion;complementary information;dual-frequency polarization synthetic aperture radar (PolSAR) image classification;frequency-aware attention","Feature extraction;Remote sensing;Frequency measurement;Radar polarimetry;Data mining;Visualization;Spaceborne radar","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);radar imaging;radar polarimetry;remote sensing;remote sensing by radar;synthetic aperture radar","classification performance;dual-frequency PolSAR data;joint feature extraction module;joint feature representation;frequency-aware attention block modules;frequency-specific attention masks;different frequency data;joint features;different frequency-aware importance;activated frequency-aware features;adaptive feature fusion block module;different frequency-aware features;information differences;fusion features;dual-frequency PolSAR datasets;DFAF-Net;frequency characteristics;classification accuracy;single-frequency classification;attention module;dual-frequency PolSAR image classification network;multifrequency polarization synthetic aperture radar systems;abundant earth resource information;continuous earth resource information;single-frequency ones;remote sensing community;fine MF PolSAR image classification;remote sensing image interpretation;single-frequency part;dual-frequency attention fusion network","","3","","40","IEEE","18 Feb 2022","","","IEEE","IEEE Journals"
"Scene Segmentation of Multi-Band ISAR Fusion Imaging Based on MB-PCSBL","X. Zhu; B. Guo; W. Hu; L. Shi; J. Ma; D. Xue","Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang Campus, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang Campus, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang Campus, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang Campus, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang Campus, Shijiazhuang, China; Department of Electronic and Optical Engineering, Army Engineering University, Shijiazhuang Campus, Shijiazhuang, China","IEEE Sensors Journal","7 Jan 2021","2021","21","3","3520","3532","We consider the problem of achieving multi-band inverse synthetic aperture radar (ISAR) fusion imaging of block structure targets with unknown block partition and develop a block-sparse recovering method based on matrix block pattern-coupled sparse Bayesian learning algorithm. Based on the sparse representation of multi-band ISAR fusion imaging model, a pattern-coupled hierarchical Gaussian prior is proposed to characterize the pattern relevance of scattering coefficients. The sparsity of each coefficient is controlled not only by its own hyperparameter, but also by the hyperparameters corresponding to its eight neighboring coefficients in the data matrix. The correlations between the coefficients in rows and columns are determined by different parameters, respectively. The proposed prior model can increase the model flexibility and promote the generation of block structures. Moreover, the whole observation scene is segmented into multiple sub-scenes to reduce the memory storage space and the computational complexity. Parameters and the fusion image result of each sub-scene are derived by the expectation-maximization method. The multi-band ISAR fusion image result of the whole scene is obtained through the stitching of the sub-scenes imaging results. Experimental results demonstrate the effectiveness and superiority of the proposed algorithm.","1558-1748","","10.1109/JSEN.2020.3026109","National Natural Science Foundation of China(grant numbers:61601496); Natural Science Foundation of Hebei Province of China(grant numbers:F2019506031,F2019506037,F2020506036); Frontier Innovation Program of Army Engineering University(grant numbers:KYSZJQZL2005); Research Innovation Development Funding of Army Engineering University, Shijiazhuang Campus(grant numbers:KYSZJQZL1902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204730","Inverse synthetic aperture radar (ISAR);multi-band fusion;sparse Bayesian learning;block-sparse signal;expectation-maximization","Imaging;Radar imaging;Partitioning algorithms;Bayes methods;Correlation;Sparse matrices","Bayes methods;expectation-maximisation algorithm;Gaussian processes;image fusion;image segmentation;learning (artificial intelligence);radar imaging;synthetic aperture radar","block-sparse recovering method;matrix block pattern-coupled sparse Bayesian learning algorithm;sparse representation;multiband ISAR fusion imaging model;pattern-coupled hierarchical Gaussian;pattern relevance;scattering coefficients;hyperparameter;neighboring coefficients;data matrix;model flexibility;block structure;observation scene;sub-scenes imaging results;scene segmentation;MB-PCSBL;multiband inverse synthetic aperture radar fusion imaging;multiband ISAR fusion image;expectation-maximization method","","2","","30","IEEE","23 Sep 2020","","","IEEE","IEEE Journals"
"Region-Level SAR Image Segmentation Based on Edge Feature and Label Assistance","R. Shang; M. Liu; L. Jiao; J. Feng; Y. Li; R. Stolkin","Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, School of Artificial Intelligence, Xidian University, Shanxi, Xi’an, China; Extreme Robotics Laboratory, University of Birmingham, Birmingham, U.K","IEEE Transactions on Geoscience and Remote Sensing","8 Nov 2022","2022","60","","1","16","This article proposes a novel segmentation algorithm for synthetic aperture radar (SAR) images. The algorithm performs region-level segmentation based on edge feature and label assistance. It demonstrates improved performance in terms of segmentation accuracy while better preserving image edges. First, an edge detection scheme is implemented, which fuses information from two advanced edge detection methods, thereby obtaining a more precise edge strength map (ESM). Second, a Canny algorithm is performed to divide the SAR image into edge regions and homogeneous regions, and different smoothing templates are selected according to pixel positions. Therefore, an anisotropic smoothing on the SAR image can be achieved, aiming at suppressing the noise within targets while also accurately maintaining the target boundaries. Third, the K-means clustering is applied to the smoothed result to generate an initial set of labels. Using ESM and the initial labels as inputs, a watershed transformation and a majority voting strategy are employed to realize an initial segmentation at the region level. Finally, a label-aided region merging (LaRM) strategy is used to correctly segment the wrongly labeled regions to give the final segmentation result. The LaRM, with merging rules based on the label rather than gray characteristics, can avoid the need for calculating a large number of complex formulas, thus accelerating the region merging. Results are presented of experiments on both simulated and real SAR images, in which the proposed region-level SAR image segmentation algorithm based on edge feature and label assistance (REFLA) method is compared against six state-of-the-art algorithms from the literature. REFLA achieves higher accuracy while better retaining the image edges.","1558-0644","","10.1109/TGRS.2022.3217053","National Natural Science Foundation of China(grant numbers:62176200,61773304,61871306); Natural Science Basic Research Program of Shaanxi(grant numbers:2022JC-45,2022JQ-616); Open Research Projects of Zhejiang Laboratory(grant numbers:2021KG0AB03); Higher Education Discipline Innovation Project; National Key Research and Development Program of China; Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110686); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930138","Anisotropic smoothing;edge feature;label-aided region merging (LaRM);synthetic aperture radar (SAR) image;unsupervised segmentation","Image edge detection;Image segmentation;Radar polarimetry;Merging;Smoothing methods;Speckle;Feature extraction","edge detection;feature extraction;image denoising;image fusion;image segmentation;radar imaging;smoothing methods;synthetic aperture radar","edge feature;advanced edge detection methods;precise edge strength map;ESM;Canny algorithm;anisotropic smoothing;region-level SAR image segmentation;label assistance;synthetic aperture radar;image edges;pixel positions;noise supression;watershed transformation;majority voting strategy;label-aided region merging;REFLA method","","1","","40","IEEE","25 Oct 2022","","","IEEE","IEEE Journals"
"Joint Estimation of Absolute Attitude and Size for Satellite Targets Based on Multi-Feature Fusion of Single ISAR Image","J. Wang; Y. Li; M. Song; M. Xing","Academy of Advanced Interdisciplinary Research, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; Beijing Institute of Space Long March Vehicle, Beijing, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Transactions on Geoscience and Remote Sensing","12 Apr 2022","2022","60","","1","20","It is challenging to estimate satellite targets’ absolute attitude and size with limited observational data. This article proposes an innovative way to jointly estimate satellite targets’ absolute attitude and size in the 3-D stable coordinates based on inverse synthetic aperture radar (ISAR) image interpretation with only one image. By taking advantage of the rectangular solar panels commonly equipped on satellites, this article extracts solar panel’s principal components, line features, and phase features of single ISAR imagery with principal component analysis (PCA), radon transform (RT), and minimum-entropy (ME)-based autofocus method, respectively. The projection relationship between these features and the absolute attitude and size of the satellite are established separately. Through multi-features fusion, a joint parameter estimation optimization function is established. This optimization is solved iteratively by the quasi-Newton method. The attitude and size parameters can be estimated simultaneously and rapidly, realizing the satellite state estimation under limited observation data. The excellent performance of the proposed algorithm is verified through different experiments.","1558-0644","","10.1109/TGRS.2022.3159345","National Key Research and Development Program of China(grant numbers:2018YFB2202500); National Natural Science Foundation of China(grant numbers:62171337,62101396); Key Research and Development Program of Shaanxi Province(grant numbers:2017KW-ZD-12); Shaanxi Province Funds for Distinguished Young Youths(grant numbers:S2020-JC-JQ-0056); Fundamental Research Funds for the Central Universities(grant numbers:XJS212205); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733900","Attitude estimation;inverse synthetic aperture radar (ISAR) image interpretation;multi-features fusion;satellite targets;size estimation","Satellites;Estimation;Radar imaging;Radar;Spaceborne radar;Space vehicles;Three-dimensional displays","estimation theory;image fusion;minimum entropy methods;Newton method;optimisation;parameter estimation;principal component analysis;radar imaging;Radon transforms;state estimation;synthetic aperture radar","absolute attitude estimation;satellite targets;multifeature fusion;inverse synthetic aperture radar image interpretation;rectangular solar panels;solar panel;single ISAR imagery;Radon transform;joint parameter estimation optimization function;satellite state estimation;RT;ME-based autofocus method;minimum-entropy-based autofocus method;quasiNewton method;PCA;principal component analysis","","1","","38","IEEE","14 Mar 2022","","","IEEE","IEEE Journals"
"Cloud Removal Based on SAR-Optical Remote Sensing Data Fusion via a Two-Flow Network","R. Mao; H. Li; G. Ren; Z. Yin","School of Resource and Environmental Engineering, Wuhan University of Technology, Wuhan, China; School of Resource and Environmental Engineering, Wuhan University of Technology, Wuhan, China; School of Resource and Environmental Engineering, Wuhan University of Technology, Wuhan, China; School of Resource and Environmental Engineering, Wuhan University of Technology, Wuhan, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","15 Sep 2022","2022","15","","7677","7686","Optical remote sensing imagery plays an important role in observing the Earth's surface today. However, it is not easy to obtain complete multitemporal optical remote sensing images because of the cloud cover, how reconstructing cloud-free optical images has become a big challenge task in recent years. Inspired by the remote sensing fusion methods based on the convolutional neural network model, we propose a two-flow network to remove clouds from optical images. In the proposed method, synthetic aperture radar images are used as auxiliary data to guide optical image reconstruction, which is not influenced by cloud cover. In addition, a novel loss function called content loss is introduced to improve image quality. The ablation experiment of the loss function also proves that content loss is indeed effective. To be more in line with a real situation, the network is trained, tested, and validated on the SEN12MS-CR dataset, which is a global real cloud-removal dataset. The experimental results show that the proposed method is better than other state-of-the-art methods in many indicators (RMSE, SSIM, SAM, and PSNR).","2151-1535","","10.1109/JSTARS.2022.3203508","Key Research & Developmental Program of Shandong Province(grant numbers:2019JZZY0314); National Natural Science Foundation of China(grant numbers:42071358,42171415); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9873922","Cloud removal;data fusion;deep learning;optical;remote sensing;synthetic aperture radar (SAR)","Remote sensing;Clouds;Optical imaging;Optical sensors;Convolutional neural networks;Radar polarimetry;Image reconstruction","geophysical image processing;geophysical techniques;image fusion;image reconstruction;image resolution;neural nets;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","cloud cover;novel loss function;content loss;image quality;cloud-removal dataset;cloud removal;SAR-optical remote sensing data fusion;two-flow network;optical remote sensing imagery;Earth's surface today;complete multitemporal optical remote sensing images;reconstructing cloud-free;big challenge task;remote sensing fusion methods;convolutional neural network model;optical images;synthetic aperture radar images;auxiliary data;optical image reconstruction","","1","","52","CCBY","1 Sep 2022","","","IEEE","IEEE Journals"
"Automatic Extraction of Layover From InSAR Imagery Based on Multilayer Feature Fusion Attention Mechanism","X. Cai; L. Chen; J. Xing; X. Xing; R. Luo; S. Tan; J. Wang","School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Engineering, Newcastle University, Newcastle upon Tyne, U.K.; School of Traffic and Transportation Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China; School of Electrical and Information Engineering, Changsha University of Science and Technology, Changsha, China","IEEE Geoscience and Remote Sensing Letters","29 Dec 2021","2022","19","","1","5","Layover is a kind of geometric distortion in radar systems with side-look imaging, especially in mountainous and dense urban areas. It causes phase distortion and alters target characteristics in the acquired images, which directly hinders the application of radar images. In this letter, the multilayer feature fusion attention mechanism (MF2AM) is proposed to extract layover from interferometric synthetic aperture radar (InSAR) imagery automatically. First, the SAR image, the corresponding coherence map, and interferometric phases are channel-fused to enhance semantic information of layover areas. Then, the fused image is fed into MF2AM to extract the essential features of layover. Finally, the detection results are produced via MF2AM. MF2AM consists of the encoder and the decoder. The encoder contains three parts: the resnet101, attention-based atrous spatial pyramid (AASP), and the semantic embedding branch (SEB). In the decoder, step decoding is used to better fuse high- and low-level features and improve the effect of edge segmentation. To verify the proposed method, the images of millimeter wave InSAR system are used for the experiment, and the performance is compared with DeepLabV3+ and Geospatial Contextual Attention Mechanism (GCAM). The results show that the MF2AM has achieved obvious performance advantages. The average pixel accuracy and average intersection over union (IOU) are 0.9601 and 0.9310, respectively, and the average test time is only 7.97 s.","1558-0571","","10.1109/LGRS.2021.3105722","National Natural Science Foundation of China(grant numbers:42101468,42074033); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524811","Attention mechanism;deep learning;interferometric synthetic aperture radar (InSAR);layover;semantic segmentation","Feature extraction;Convolution;Semantics;Decoding;Radar polarimetry;Data mining;Image edge detection","image fusion;radar imaging;radar interferometry;remote sensing by radar;synthetic aperture radar","radar images;multilayer feature fusion attention mechanism;MF2AM;interferometric synthetic aperture radar imagery;SAR image;interferometric phases;layover areas;fused image;attention-based atrous spatial pyramid;low-level features;millimeter wave InSAR system;Geospatial Contextual Attention Mechanism;automatic extraction;InSAR imagery;geometric distortion;radar systems;side-look imaging;mountainous areas;dense urban areas;phase distortion;alters target characteristics;time 7.97 s","","1","","22","IEEE","30 Aug 2021","","","IEEE","IEEE Journals"
"Block-sparse 3-D ISAR image reconstruction in a non-coherent multistatic scenario","S. Brisken; J. Ender","Fraunhofer Institute for High Frequency Physics and Radar Techniques FHR, Wachtberg, Germany; Fraunhofer Institute for High Frequency Physics and Radar Techniques FHR, Wachtberg, Germany","2015 IEEE Radar Conference (RadarCon)","25 Jun 2015","2015","","","0265","0269","This paper introduces a new technique for multistatic/multiband ISAR image fusion. Developing this technique, special regard was given to the facts that coherence between separated sensors cannot be assumed automatically and that the fusion has to be performed in all three spatial dimensions. The result is a 3-D ISAR image that mitigates the problem of aspect angle dependence of ISAR images used for classification. The presented method is furthermore a solution to fuse heterogeneous signals (e.g. multiband). A simulation illustrates the functionality of the technique.","2375-5318","978-1-4799-8232-5","10.1109/RADAR.2015.7131007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131007","","Radar imaging;Image reconstruction;Coherence;Sensor fusion;Noise","image classification;image fusion;image reconstruction;radar imaging;synthetic aperture radar","block sparse 3D ISAR image reconstruction;noncoherent multistatic scenario;multistatic ISAR image fusion;multiband ISAR image fusion;heterogeneous signals;image classification","","7","1","11","IEEE","25 Jun 2015","","","IEEE","IEEE Conferences"
"Framework for multi-sensor data fusion using template based matching","G. Palubinskas","German Aerospace Center DLR, Remote Sensing Technology Institute Oberpfaffenhofen, Wessling, Germany","2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","12 Nov 2015","2015","","","621","624","A general approach or framework is proposed for multi-sensor data fusion using template based matching (TBM). The main advantage of TBM is that it allows defining features/templates using a priori information from the scene/image. The approach works as follows: first quite complex features e.g. roundabouts/junctions are extracted in the optical image, then these features are simulated in SAR and finally they are compared or matched on patch/area basis with the SAR image. The proposed framework is illustrated for common tie points extraction in very high resolution satellite optical WorldView-2 and radar TerraSAR-X imagery.","2153-7003","978-1-4799-7929-5","10.1109/IGARSS.2015.7325840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325840","Optical;SAR;image fusion;registration;template extraction;simulation;matching","Optical imaging;Adaptive optics;Feature extraction;Optical sensors;Synthetic aperture radar;Data mining;Accuracy","geophysical image processing;geophysical techniques;image fusion","multisensor data fusion;template based matching;roundabouts-junctions;optical image;SAR;radar TerraSAR-X imagery;very high resolution satellite optical WorldView-2","","","2","18","IEEE","12 Nov 2015","","","IEEE","IEEE Conferences"
"Multi-mode Fusion and Classification Method for Space Targets Based on Convolutional Neural Network","Y. Li; W. Fan; X. Shi; F. Zhou","Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China; Key Laboratory of Electronic Information Countermeasure and Simulation Technology of Ministry of Education, Xidian University, Xi'an, China","2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)","30 Mar 2020","2019","","","1","6","The investigation and classification of space targets are significant issues in spatial situational awareness, and the radar plays an important role due to its superior real-time performance, luxuriant receiving information and all-weather, all-day capabilities. Electromagnetic characteristics and classification methods of regular scatters are the foundational problems for space targets investigation and classification. Aiming at the deficiencies of complex feature extracting, local parameters sharing and difficult network expanding in the existing methods, this paper proposed a multi-mode fusion and classification method for space targets based on convolutional neural network (CNN). Firstly, a multi-mode database of regular scatterers is constructed. Secondly, the spatial domain-based fusion is performed to fuse various characteristics. Then, feature vectors are generated through CNN. Finally, the classifier and the loss function are designed to classify space targets. Experiments show that the proposed method achieves the highest classification accuracy compared to other fusion and none-fusion methods.","2474-2333","978-1-7281-2912-9","10.1109/APSAR46974.2019.9048288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9048288","radar target;convolutional neural network (CNN);fusion;classification","","convolutional neural nets;feature extraction;image classification;image fusion;radar target recognition","convolutional neural network;spatial situational awareness;regular scatters;space targets investigation;classification method;multimode database;regular scatterers;spatial domain-based fusion;none-fusion methods;multimode fusion;electromagnetic characteristics","","1","","6","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"A Deep Learning-Based Heterogeneous Spatio-Temporal-Spectral Fusion: SAR and Optical Images","M. Jiang; J. Li; H. Shen","School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","1252","1255","Image fusion is a powerful means to integrate complementary spatio-temporal-spectral information among multi-source remote sensing images. The existing remote sensing image fusion is mostly limited to the fusion between optical images, and most of them are limited to the fusion between two sensors. Based on this, this paper proposes a heterogeneous spatio-temporal-spectral fusion method based on deep learning. Specifically, it combines the low-spatial-resolution (LR) cloudy image with the high-spatial-resolution (HR) SAR images and the HR cloud-free optical image to remove the clouds and improve the spatial resolution of the LR cloudy image. The SAR image is acquired at the same date as the LR cloudy image, while the HR cloud-free image is acquired at another date. Experiments are performed on the images of Landsat 8, Sentinel-1, and Sentinel-2. The experimental results show that the proposed method can effectively achieve the joint goal of spatial resolution improvement and cloud removal of the Landsat image.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554031","National Natural Science Foundation of China(grant numbers:62071341); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554031","Heterogeneous;SAR;optical;cloud-removal;spatial resolution improvement","Earth;Artificial satellites;Clouds;Optical imaging;Radar polarimetry;Optical sensors;Spatial resolution","deep learning (artificial intelligence);geophysical image processing;geophysical techniques;image fusion;image resolution;optical images;remote sensing;synthetic aperture radar","Sentinel-1;Sentinel-2;Landsat image;spatial resolution improvement;HR cloud-free image;SAR image;LR cloudy image;HR cloud-free optical image;high-spatial-resolution SAR images;low-spatial-resolution cloudy image;heterogeneous spatio-temporal-spectral fusion method;remote sensing image fusion;multisource remote sensing images;complementary spatio-temporal-spectral information;optical images;deep learning-based heterogeneous spatio-temporal-spectral fusion","","3","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Joint Detection of Airplane Targets Based on Sar Images and Optical Images","J. Qin; H. Qu; H. Chen; W. Chen","Institute of Software, Liaoning Technical University, Huludao, China; Institute of Software, Liaoning Technical University, Huludao, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China; Dept. of Information Engineering, Harbin Institute of Technology, Harbin, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","1366","1369","In airplane target detection, there was the drawback of weak recognition ability for dark targets and high false alarm rate for detected targets. In order to address the problem, we proposed a detection method based on SAR and optical image feature fusion. It extracted texture, moment and backscattering characteristics from SAR images and combined with optical features. Moreover, the novel airplane edge templates incorporating SAR and optical images were created to acquire saliency map. During the process of detection, first, the saliency map and the One-Class-SVM (OCSVM) classifier were used to initially recognize the suspected airplane targets. Then, the combination features were adopted to further identify the misidentified airplane target. The experimental results showed that the Precision of the proposed method was 61.82% and the False Alarm Rate was 20%, which was better than the HIS-based detection method.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900167","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900167","SAR Image;Optical Image;Airplane Object Detection;Image fusion","","feature extraction;image fusion;image texture;object detection;radar imaging;support vector machines;synthetic aperture radar","sar images;optical images;airplane target detection;weak recognition ability;dark targets;high false alarm rate;detected targets;detection method;optical image feature fusion;backscattering characteristics;saliency map;One-Class-SVM classifier;suspected airplane targets;combination features;misidentified airplane target;novel airplane edge templates;OCSVM classifier","","1","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"SAR target recognition based on texture feature and contour feature fusion","H. Liu; W. Chen; F. Li; T. Long","School of Information and Electronics, Beijing Institute of Technology, Radar Research Laboratory, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Radar Research Laboratory, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Radar Research Laboratory, Beijing 100081, China; School of Information and Electronics, Beijing Institute of Technology, Radar Research Laboratory, Beijing 100081, China","IET International Radar Conference (IET IRC 2020)","22 Sep 2021","2020","2020","","571","575","This paper proposes a SAR image recognition method that combines contour features and texture features. The moment feature is selected as the contour feature, and the HOG feature is selected as the texture feature. Considering that the single feature recognition method has limitations and the accuracy of multiple classifications is poor, the feature fusion method is used for optimization. This paper proposes a new feature-level fusion idea, try to use Fisher scoring method to select features, at the same time use FS as the fusion weight, and reduce the dimensionality of the moment features by PCA according to the weight value to obtain the fusion feature matrix. The recognition performance is better than ordinary weighted fusion algorithm. At the same time, the idea of heterogeneous feature fusion is given, that is, decision-level fusion.","","","10.1049/icp.2021.0729","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9545900","","","decision theory;feature selection;image classification;image fusion;image texture;optimisation;principal component analysis;radar imaging;radar target recognition;synthetic aperture radar","feature-level fusion;fusion feature matrix;heterogeneous feature fusion;SAR target recognition;texture feature fusion;SAR image recognition method;HOG feature;single feature recognition method;feature fusion method;contour feature fusion;moment feature selection;optimization;dimensionality reduction;PCA;decision-level fusion","","","","","","22 Sep 2021","","","IET","IET Conferences"
"Triplet Attention Feature Fusion Network for SAR and Optical Image Land Cover Classification","Z. Xu; J. Zhu; J. Geng; X. Deng; W. Jiang","School of Electronics And Information, Northwestern Polytechnical University, Xi'an, China; Chinese Academy of Sciences, Aerospace Information Research Institute, Beijing, China; School of Electronics And Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics And Information, Northwestern Polytechnical University, Xi'an, China; School of Electronics And Information, Northwestern Polytechnical University, Xi'an, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4256","4259","With recent advances in remote sensing, abundant multimodal data are available for applications. However, considering the redundancy and the huge domain differences among multimodal data, how to effectively integrate these data is becoming important and challenging. In this paper, we proposed a triplet attention feature fusion network (TAFFN) for SAR and optical image fusion classification. Specifically, spatial attention module and spectral attention module based on self-attention mechanism are developed to extract spatial and spectral long-range information from the SAR image and optical image respectively, at the same time, cross-attention mechanism is proposed to capture the long-range interactive representation. Triplet attentions are concatenated to further integrate the complementary information of SAR and optical images. Experiments on a SAR and optical multimodal dataset demonstrate that the proposed method can achieve the state-of-the-arts performance.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555126","National Natural Science Foundation of China(grant numbers:61901376); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555126","Feature fusion;attention mechanism;land cover classification;SAR image","Integrated optics;Redundancy;Optical fiber networks;Optical imaging;Feature extraction;Radar polarimetry;Optical sensors","feature extraction;geophysical image processing;geophysical signal processing;image classification;image fusion;optical images;radar imaging;remote sensing;sensor fusion;synthetic aperture radar;terrain mapping;visual perception","fusion network;SAR;optical image fusion classification;spatial attention module;spectral attention module;self-attention mechanism;long-range information;cross-attention mechanism;long-range interactive representation;triplet attention;optical images;optical multimodal dataset;sar;optical image land cover classification;remote sensing;abundant multimodal data;huge domain differences","","1","","8","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Cloud removal with fusion of SAR and Optical Images by Deep Learning","J. Gao; H. Zhang; Q. Yuan","School of Geodesy and Geomatics, Wuhan University, Wuhan, China; School of Resource and Environmental Sciences, Wuhan University, Wuhan, China; School of Geodesy and Geomatics, Wuhan University, Wuhan, China","2019 10th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)","14 Oct 2019","2019","","","1","3","Due to the different imaging methods of SAR image and optical image, it is difficult to establish the corresponding relationship between them by traditional methods. However, with the development of deep learning, there are many researches on the transformation from SAR image to optical image based on GAN, which prove that the mapping between SAR image and optical image can be achieved. Based on this, this work will transform the SAR image into optical image and fuse to fill the cloud area of the optical image. This work will provide a method of heterogeneous image fusion to remove cloud, and get a good effect.","","978-1-7281-4615-7","10.1109/Multi-Temp.2019.8866939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866939","Deep Learning;GAN;Cloud removal","Optical imaging;Optical sensors;Optical distortion;Radar polarimetry;Optical reflection;Adaptive optics;Gallium nitride","clouds;geophysical image processing;image fusion;optical images;radar imaging;synthetic aperture radar","optical image;deep learning;imaging methods;SAR image;heterogeneous image fusion;GAN","","1","","7","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Construction of Yunnan's Agricultural Ecological Civilization Based on Intelligent UAV and SAR Image Analysis","W. Li; X. Chen; G. Li; Y. Bi","College of Humanities and Social Sciences, Yunnan Agricultural University, Kunming, Yunnan, China; College of Economics and Management, Yunnan Agricultural University, Kunming, Yunnan, China; Yunnan Plateau Characteristic Agricultural Industry Research Institute, Kunming, Yunnan, China; Yunnan Agricultural University, Kunming, Yunnan, China","2022 4th International Conference on Smart Systems and Inventive Technology (ICSSIT)","25 Feb 2022","2022","","","1639","1642","This paper studies the construction of Yunnan's agricultural ecological civilization based on the analysis of intelligent drones and SAR images. This article deeply discusses several key problems faced by multi-rotor UAV SAR imaging processing from the aspects of imaging algorithm, motion compensation and auto-focusing, and puts forward an effective imaging processing scheme. The SIFT algorithm is used to extract the image feature points, and the BBF algorithm calculates and generates a matching point set. According to the matching point set, the inter-image perspective transformation model is calculated to complete the image registration, and the wavelet transform algorithm is used to realize the registration image fusion. Based on SAR image analysis and dimensionality reduction of remote sensing data, dimensionality reduction, extraction and classification of remote sensing images of agricultural ecology in Yunnan are carried out.","","978-1-6654-0118-0","10.1109/ICSSIT53264.2022.9716302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9716302","Yunnan;Agricultural Ecological Civilization;Intelligent UAV;SAR Image","Dimensionality reduction;Wavelet transforms;Satellites;Imaging;Reconnaissance;Feature extraction;Radar polarimetry","autonomous aerial vehicles;feature extraction;geophysical image processing;image fusion;image matching;image processing;image registration;motion compensation;radar imaging;remote sensing;remotely operated vehicles;synthetic aperture radar;transforms;wavelet transforms","image registration;wavelet transform algorithm;registration image fusion;SAR image analysis;remote sensing images;agricultural ecology;Yunnan's agricultural ecological civilization;intelligent UAV;intelligent drones;SAR images;multirotor UAV SAR imaging processing;motion compensation;auto;effective imaging processing scheme;SIFT algorithm;image feature points;BBF algorithm;matching point;inter-image perspective transformation model","","","","24","IEEE","25 Feb 2022","","","IEEE","IEEE Conferences"
"SAR and Optical Image Registration Method Based on Quantum Particle Swarm Optimization","X. Ji","Nanjing Vocational College of Information Technology, Nanning, China","2018 10th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)","11 Nov 2018","2018","01","","365","368","Image registration is the most important step of SAR and optical image fusion. In order to improve the registration accuracy and efficiency of the strategic targets, a new image registration method based on quantum particle swarm optimization is presented. The proposed method consists of three steps in sequence. Firstly, it decomposes the optical image into the high frequency detail components and the low frequency approximation component with the wavelet transform, and extracts feature corner points with Harris corner detection algorithm for the low frequency component. Secondly, it constructs the similarity measure criterion by migrating the pixel feature points to SAR image, and searches the most optimal registration parameter for the SAR image using quantum particle swarm optimization algorithm with individual optimal selection constraint. Finally, it completes affine transform according to the optimal registration parameters, and finishes image registration through interpolation resampling. The experimental results prove that the proposed method is effective and feasible, it can achieve high accuracy of sub-pixel level for the large strategic targets.","","978-1-5386-5836-9","10.1109/IHMSC.2018.00091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530348","Image registration;SAR image;optical image;quantum particle swarm optimization","Man-machine systems;Cybernetics","affine transforms;feature extraction;image fusion;image registration;interpolation;optical images;particle swarm optimisation;synthetic aperture radar;wavelet transforms","feature corner points;interpolation resampling;similarity measure criterion;wavelet transform;optical image registration;affine transform;individual optimal selection constraint;quantum particle swarm optimization algorithm;optimal registration parameter;SAR image;pixel feature points;low frequency component;Harris corner detection algorithm;low frequency approximation component;high frequency detail components;image registration method;strategic targets;optical image fusion","","","","8","IEEE","11 Nov 2018","","","IEEE","IEEE Conferences"
"Heterogeneous image matching based on phase consistency","M. Liu; X. Mu; X. He","Rocket Force University of Engineering, Xi’an, China; Rocket Force University of Engineering, Xi’an, China; Beijing Institute of Remote Sensing Equipment, Beijing, China","2021 IEEE Conference on Telecommunications, Optics and Computer Science (TOCS)","28 Jan 2022","2021","","","864","868","The Image matching is an important step in image registration and image fusion. To solve the problem that SAR images and optical image matching are sensitive to radiation distortion, a novel heterogeneous image matching algorithm is proposed based on image frequent-domain wavelet decomposition based on phase congruency. Firstly, morphological denoising was carried out for image pairs, and then wavelet decomposition was performed by log-Gabor filter to obtain phase consistency of pixel points (PC graph). Then, feature descriptors were established based on the maximum index graph. Double-point matching was used as the search strategy for feature matching, and the random sampling consensus algorithm was used to eliminate false matching. Compare with traditional SIFT method. Experimental results show that the proposed algorithm has better matching effect than SIFT method and is an effective heterogeneous image matching algorithm","","978-1-6654-2498-1","10.1109/TOCS53301.2021.9689044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689044","Heterogeneous image matching;Image morphology;Phase consistency;The Log Gabor filter","Optical filters;Image matching;Optical distortion;Feature extraction;Optical imaging;Radar polarimetry;Adaptive optics","feature extraction;Gabor filters;image fusion;image matching;image registration;synthetic aperture radar;wavelet transforms","false matching;matching effect;effective heterogeneous image matching algorithm;phase consistency;image registration;image fusion;SAR images;optical image matching;novel heterogeneous image matching algorithm;image frequent-domain wavelet decomposition;phase congruency;image pairs;double-point matching;feature matching;random sampling consensus algorithm","","","","11","IEEE","28 Jan 2022","","","IEEE","IEEE Conferences"
"CNN-Based Fusion and Classification of Multi-Temporal Sentinel-1 & -2 Satellite Data","A. Shakya; M. Biswas; M. Pal","Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Computer Engineering Department, National Institute of Technology, Kurukshetra, India; Civil Engineering Department, National Institute of Technology, Kurukshetra, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","57","60","SAR and optical data are widely used in image fusion to provide the complimentary information of each other and obtain the spatial and spectral features for improved classifications. This paper proposes to use multi-temporal data form Sentinel-1 (VV & VH polarization) and Sentinel-2 sensors for the fusion and classification over an agricultural area. Convolutional Neural Network (CNN)- based Pyramid method for fusion and Bayesian Optimized 2-D CNN for classification of fused multi-temporal data was used to extract spatial-spectral information. Results in terms of classification accuracy suggests slightly better performance by VV polarized fused images than the VH and also suggests an improved performance by multi-temporal data in comparison to the single date data over the study area.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9791998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791998","Fusion;Classification;Convolutional Neural Network (CNN);Bayesian Optimization","Satellites;Optical polarization;Neural networks;Sensor fusion;Optical imaging;Bayes methods;Convolutional neural networks","agriculture;convolutional neural nets;geophysical image processing;image classification;image fusion;remote sensing;synthetic aperture radar","multitemporal Sentinel-1 satellite data;multitemporal Sentinel-2 satellite data;Bayesian optimized 2D CNN;Sentinel-2 sensors;VV & VH polarization;spectral features;spatial features;complimentary information;image fusion;optical data;CNN-based fusion;single date data;VV polarized fused images;classification accuracy;spatial-spectral information;fused multitemporal data;convolutional neural network","","","","10","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Comparative Analysis of the Fusion Methods Based on GF-3 Radar and GF-1 Multispectral Data","Z. Fu; J. Qi; D. Zhang; J. Wang; W. Zhang; X. Han","China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China; China Aero Geophysical Survey and Remote Sensing Center for Land and Resources, Beijing, China","2018 Fifth International Workshop on Earth Observation and Remote Sensing Applications (EORSA)","3 Jan 2019","2018","","","1","5","To make better use of the advantages of radar and promote the application of image fusion based on radar data, the author uses different fusion methods based on GF-3 SAR and GF-1 MSS, and evaluates the fusion results by analyzing mean, variance, information entropy, average gradient, spectral distortion and correlation coefficient. The results show that HSV and GS transforms have the best performances in overall. PC is recognized as the third, while it is still remarkable that it has the best ability of spectral retention. And the specialty in NIR band makes PC more conducive for extraction of vegetation. Brovey and Multiplicative transforms are not effective in comparison.","","978-1-5386-6642-5","10.1109/EORSA.2018.8598556","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598556","GF-3;multispectral image;SAR;fusion;remote sensing","Radar imaging;Transforms;Distortion;Correlation coefficient;Information entropy;Remote sensing","entropy;geophysical image processing;image fusion;remote sensing;sensor fusion;synthetic aperture radar;transforms;vegetation mapping","comparative analysis;GF-3 Radar;GF-1 Multispectral Data;image fusion;radar data;different fusion methods;GF-3 SAR;GF-1 MSS;fusion results;information entropy;average gradient;spectral distortion;correlation coefficient;spectral retention","","","","9","IEEE","3 Jan 2019","","","IEEE","IEEE Conferences"
"When to fuse what? random forest based fusion of low-, mid-, and high-level information for land cover classification from optical and SAR images","R. Hänsch; O. Hellwich","Computer Vision & Remote Sensing, Technische Universität Berlin, Germany; Computer Vision & Remote Sensing, Technische Universität Berlin, Germany","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","3587","3590","With increasing availability of different sensors for earth observation, data fusion gained more and more importance. While previous publications focussed on new sensor combinations, new fusion techniques, or new applications, this work investigates at which stage of the image analysis pipeline the fusion process is most beneficial. The fusion of an optical and a SAR image for the task of land cover classification serves as an example. The experimental results indicate, that although the fusion of complementary data is generally advantageous, it is most helpful at later stages of the classification process.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7729929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729929","Data fusion;Random Forests;SAR","Optical imaging;Adaptive optics;Synthetic aperture radar;Optical sensors;Data integration;Feature extraction","geophysical image processing;image classification;image fusion;land cover","random forest;Earth observation sensor;data fusion;image fusion technique;image analysis pipeline;optical image;SAR image;land cover classification;high-level information;midlevel information;low-level information","","1","","9","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Image Information Processing System","J. Lu",NA,"Design Technology of Synthetic Aperture Radar","","2019","","","249","305","This chapter begins with the application requirements for synthetic aperture radar (SAR) image information. It discusses the technologies of SAR image target detection, target change detection, target recognition, and multisource SAR image fusion. The chapter also proposes many typical information processing technologies, and presents up‐to‐date information processing results of SAR images. SAR systems are operated with the use of multiband, multipolarization, multiresolution, and multimode. Earth observation in all dimensions has been achieved. A huge amount of image data is produced every day. As a SAR image is quite different from an optical image, information processing is much more difficult than that of conventional optical images, due to influences of speckle noise, shadow, foreshortening, top and bottom inversion, and other geometric features. SVM is a new pattern recognition technology developed in the early 1990s.","","9781119564638","10.1002/9781119564621.ch7","","https://ieeexplore.ieee.org/xpl/ebooks/bookPdfWithBanner.jsp?fileName=8826455.pdf&bkn=8826411&pdfType=chapter","","Radar polarimetry;Object detection;Marine vehicles;Synthetic aperture radar;Aircraft;Clutter;Target recognition","","","","","","","","9 Sep 2019","","","IEEE","Wiley-IEEE Press eBook Chapters"
"Registration of Multiresolution Remote Sensing Images Based on L2-Siamese Model","R. Fan; B. Hou; J. Liu; J. Yang; Z. Hong","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China; School of Water Resources and Hydro-Electric Engineering, Xi’an University of Technology, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Shaanxi Institute of Geological Survey, Xi’an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","7 Jan 2021","2021","14","","237","248","The registration of multiresolution optical remote sensing images has been widely used in image fusion, change detection, and image stitching. However, traditional registration methods achieve poor accuracy in the registration of multiresolution remote sensing images. In this study, we propose a framework for generating deep features via a deep residual encoder (DRE) fused with shallow features for multiresolution remote sensing image registration. Through an L2 normalization Siamese network (L2-Siamese) based on the DRE, the multiscale loss function is used to learn the attribute characteristics and distance characteristics of two key points and obtain the trained feature extractor. Finally, the DRE is used to extract the deep features of the key points and their neighbors, which are concatenated with the shallow features into a fusion feature vector to complete the image registration. We performed comprehensive experiments on four sets of multiresolution optical remote sensing images and two sets of synthetic aperture radar images. The results demonstrate that the proposed registration model can achieve subpixel registration. The relative registration accuracy improved by 1.6%–7.5%, whereas the overall performance improved by 4.5%–14.1%.","2151-1535","","10.1109/JSTARS.2020.3038922","Key Research and Development Plan of Shaanxi Province(grant numbers:D5140200023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264687","Deep descriptors;L2-Siamese;multiresolution image registration;residual encoder;satellite remote sensing;Siamese network","Image registration;Image resolution;Feature extraction;Optical imaging;Adaptive optics;Optical sensors;Remote sensing","feature extraction;geophysical image processing;image fusion;image registration;image resolution;learning (artificial intelligence);radar imaging;remote sensing by radar;synthetic aperture radar","registration model;synthetic aperture radar images;fusion feature vector;trained feature extractor;L2 normalization Siamese network;image registration;shallow features;DRE;deep features;traditional registration methods;image stitching;image fusion;multiresolution optical remote sensing images;L2-Siamese Model;multiresolution remote sensing images;relative registration accuracy;subpixel registration","","11","","41","CCBY","19 Nov 2020","","","IEEE","IEEE Journals"
"Change Detection in Heterogeneous Optical and SAR Remote Sensing Images Via Deep Homogeneous Feature Fusion","X. Jiang; G. Li; Y. Liu; X. -P. Zhang; Y. He","Institute of Information Fusion, Naval Aeronautical University, Yantai, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Institute of Information Fusion, Naval Aeronautical University, Yantai, China; Computer and Biomedical Engineering, Department of Electrical, Ryerson University, Toronto, ON, Canada; Institute of Information Fusion, Naval Aeronautical University, Yantai, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 May 2020","2020","13","","1551","1566","Change detection in heterogeneous remote sensing images is crucial for disaster damage assessment. Recent methods use homogenous transformation, which transforms the heterogeneous optical and synthetic aperture radar (SAR) remote sensing images into the same feature space, to achieve change detection. Such transformations mainly operate on the low-level feature space and may corrupt the semantic content, deteriorating the performance of change detection. To solve this problem, this article presents a new homogeneous transformation model termed deep homogeneous feature fusion (DHFF) based on image style transfer (IST). Unlike the existing methods, the DHFF method segregates the semantic content and the style features in the heterogeneous images to perform homogeneous transformation. The separation of the semantic content and the style in the homogeneous transformation prevents the corruption of image semantic content, especially in the regions of change. In this way, the detection performance is improved with accurate homogeneous transformation. Furthermore, we present a new iterative IST strategy, where the cost function in each IST iteration measures and thus maximizes the feature homogeneity in additional new feature subspaces for change detection. After that, change detection is accomplished accurately on the original and the transformed images that are in the same feature space. Real remote sensing images acquired by SAR and optical satellites are utilized to evaluate the performance of the proposed method. The experiments demonstrate that the proposed DHFF method achieves significant improvement for change detection in heterogeneous optical and SAR remote sensing images in terms of both accuracy rate and Kappa index.","2151-1535","","10.1109/JSTARS.2020.2983993","National Natural Science Foundation of China(grant numbers:61790551,61925106); Civil Space Advance Research Program of China(grant numbers:D010305); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9057421","Change detection;heterogeneous;image style transfer (IST);remote sensing","Feature extraction;Synthetic aperture radar;Optical sensors;Remote sensing;Optical imaging;Semantics;Cost function","feature extraction;geophysical image processing;image fusion;object detection;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","IST iteration measures;deep homogeneous feature fusion;homogeneous transformation model;low-level feature space;homogenous transformation;heterogeneous remote sensing images;SAR remote sensing images;transformed images;change detection;feature homogeneity;image semantic content;DHFF method;image style transfer","","25","","42","CCBY","6 Apr 2020","","","IEEE","IEEE Journals"
"AM³Net: Adaptive Mutual-Learning-Based Multimodal Data Fusion Network","J. Wang; J. Li; Y. Shi; J. Lai; X. Tan","School of Intelligent Systems Engineering, Sun Yat-sen University, Guangzhou, China; School of Intelligent Systems Engineering, Sun Yat-sen University, Guangzhou, China; School of Intelligent Systems Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, and the Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Sun Yat-sen University, Guangzhou, China; Southern Marine Science and Engineering Guangdong Laboratory (Zhuhai), Zhuhai, China","IEEE Transactions on Circuits and Systems for Video Technology","3 Aug 2022","2022","32","8","5411","5426","Multimodal data fusion, e.g., hyperspectral image (HSI) and light detection and ranging (LiDAR) data fusion, plays an important role in object recognition and classification tasks. However, existing methods pay little attention to the specificity of HSI spectral channels and the complementarity of HSI and LiDAR spatial information. In addition, the utilized feature extraction modules tend to consider the feature transmission processes among different modalities independently. Therefore, a new data fusion network named AM3Net is proposed for multimodal data classification; it includes three parts. First, an involution operator slides over the input HSI’s spectral channels, which can independently measure the contribution rate of the spectral channel of each pixel to the spectral feature tensor construction. Furthermore, the spatial information of HSI and LiDAR data is integrated and excavated in an adaptively fused, modality-oriented manner. Second, a spectral-spatial mutual-guided module is designed for the feature collaborative transmission among spectral features and spatial information, which can increase the semantic relatedness connection through adaptive, multiscale, and mutual-learning transmission. Finally, the fused spatial-spectral features are embedded into a classification module to obtain the final results, which determines whether to continue updating the network weights. Experimental evaluations on HSI-LiDAR datasets indicate that AM3Net possesses a better feature representation ability than the state-of-the-art methods. Additionally, AM3Net still maintains considerable performance when its input is replaced with multispectral and synthetic aperture radar data. The result indicates that the proposed data fusion framework is compatible with diversified data types.","1558-2205","","10.1109/TCSVT.2022.3148257","Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B090921003); Southern Marine Science and Engineering Guangdong Laboratory (Zhuhai)(grant numbers:SML2020SP011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698196","Involution networks;adaptive mutual-learning;data fusion;and multimodal data classification","Feature extraction;Laser radar;Convolution;Kernel;Data integration;Convolutional neural networks;Adaptive systems","feature extraction;geophysical image processing;image classification;image fusion;learning (artificial intelligence);object recognition;optical radar;sensor fusion;synthetic aperture radar;tensors","adaptive mutual-learning-based multimodal data fusion network;hyperspectral image;light detection;object recognition;classification tasks;HSI spectral channels;LiDAR spatial information;utilized feature extraction modules;feature transmission processes;multimodal data classification;involution operator slides;input HSI's spectral channels;spectral channel;spectral feature tensor construction;LiDAR data;modality-oriented manner;spectral-spatial mutual-guided module;feature collaborative transmission;spatial-spectral features;classification module;network weights;HSI-LiDAR datasets;3 Net possesses;feature representation ability;multispectral aperture radar data;synthetic aperture radar data;data fusion framework;diversified data types","","3","","52","IEEE","31 Jan 2022","","","IEEE","IEEE Journals"
"Optical-and-Radar Image Fusion for Dynamic Estimation of Spin Satellites","Y. Zhou; L. Zhang; Y. Cao; Y. Huang","Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; School of Electronics and Communication Engineering, Sun Yat-Sen University, Guangzhou, China; Collaborative Innovation Center of Information Sensing and Understanding, Xidian University, Xi’an, China; State Key Laboratory of Millimeter Waves, Southeast University, Nanjing, China","IEEE Transactions on Image Processing","28 Jan 2020","2020","29","","2963","2976","As more and more satellites are launched into the space, dynamic estimation of spin satellites has become a critical component of the space situation awareness application. Some explored studies using exterior measurements from different sensors such as optical device and inverse synthetic aperture radar (ISAR) to estimate dynamic parameters of spin satellites. As a single sensor normally provides two-dimensional observation, three-dimensional estimations resulting from these algorithms are strictly related to the prior knowledge of targets characteristics. As a result, it is difficult to expand these methods to other satellites. In order to support the dynamic estimation of most spin satellites, this paper presents a novel dynamic estimation approach which employs synchronized optical-and-radar images. The optical-and-radar fusion strategy has demonstrated its superiority in image analysis field, and breaks down the dynamic estimation of spin satellites into two sub-problems: target attitude estimation and spin parameters estimation. In this work, the proposed algorithm deduces two explicit expressions of target dynamic parameters under the imaging projection model of the joint optical-and-radar observation. Through the particle swarm optimization (PSO), target dynamic parameters are determined in two stages. This paper presents some experiments illustrating the feasibility of the proposed method and subsequent conclusions, which reflect advantages of the joint optical-and-radar observation mode in image interpretation.","1941-0042","","10.1109/TIP.2019.2955248","National Natural Science Foundation of China(grant numbers:61771372,61771367); Natural Science Foundation of Shanghai(grant numbers:1428700); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8917816","Dynamic estimation;spin satellites;optical-and-radar fusion;image interpretation","Satellites;Optical imaging;Radar imaging;Optical sensors;Spaceborne radar;Estimation","estimation theory;image fusion;optical information processing;optical radar;parameter estimation;particle swarm optimisation;radar imaging;spaceborne radar","image interpretation;PSO;spin satellite estimation;particle swarm optimization;ISAR;optical-radar observation mode;optical-radar image fusion strategy;three-dimensional dynamic estimation approach;optical-radar fusion strategy;parameter estimation;attitude estimation;inverse synthetic aperture radar","","17","","46","IEEE","28 Nov 2019","","","IEEE","IEEE Journals"
"Explore Better Network Framework for High-Resolution Optical and SAR Image Matching","H. Zhang; L. Lei; W. Ni; T. Tang; J. Wu; D. Xiang; G. Kuang","Northwest Institute of Nuclear Technology, Xi’an, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; Northwest Institute of Nuclear Technology, Xi’an, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China; Northwest Institute of Nuclear Technology, Xi’an, China; Beijing Advanced Innovation Center for Soft Matter Science and Engineering and the Interdisciplinary Research Center for Artificial Intelligence, Beijing University of Chemical Technology, Beijing, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, China","IEEE Transactions on Geoscience and Remote Sensing","15 Feb 2022","2022","60","","1","18","To fully explore the complementary information from optical and synthetic aperture radar (SAR) imageries, they need first to be coregistered with high accuracy. Due to the vast radiometric and geometric disparity, the problem to match high-resolution optical and SAR images is quite challenging. The present deep learning-based methods have shown advantages over the traditional approaches, but the performance increment is not significant. In this article, we explore a better network framework for high-resolution optical and SAR image matching from three aspects. First, we propose an effective multilevel feature fusion method, which helps to take advantage of both the low-level fine-grained features for precious feature location and the high-level semantic features for better discriminative ability. Second, a feature channel excitation procedure is conducted using a novel multifrequency channel attention module, which is able to make image features of different types and multiple levels effectively collaborate with each other and produce image matching features with high diversity. Third, the self-adaptive weighting loss is introduced, with which, each sample is assigned with an adaptive weighting factor, and therefore, information buried in all nearby samples can be better exploited. Under a pseudo-Siamese architecture, the proposed optical and SAR image matching network (OSMNet) is trained and tested on a large and diverse high-resolution optical and SAR dataset. Extensive experiments demonstrate that each component of the proposed deep framework helps to improve the matching accuracy. Also, the OSMNet shows overwhelming superior to the state-of-the-art handcrafted approaches on imageries of different land-cover types.","1558-0644","","10.1109/TGRS.2021.3126939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609993","Convolutional neural networks (CNNs);feature fusion;high resolution;image matching;multi-frequency channel attention;optical;remote sensing;self-adaptive weighting loss (SAW);synthetic aperture radar (SAR)","Optical imaging;Adaptive optics;Radar polarimetry;Optical sensors;Nonlinear optics;Image matching;Spatial resolution","feature extraction;image fusion;image matching;learning (artificial intelligence);radar imaging;synthetic aperture radar","network framework;vast radiometric disparity;geometric disparity;deep learning-based methods;effective multilevel feature fusion method;low-level fine-grained features;precious feature location;high-level semantic features;feature channel excitation procedure;image features;image matching features;proposed optical SAR image;large resolution optical;diverse high-resolution optical;SAR dataset;matching accuracy","","4","","54","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Learning a Joint Embedding of Multiple Satellite Sensors: A Case Study for Lake Ice Monitoring","M. Tom; Y. Jiang; E. Baltsavias; K. Schindler","Chair of Glaciology and Geomorphodynamics, University of Zurich, Zürich, Switzerland; Chair of Photogrammetry and Remote Sensing, Swiss Federal Institute of Technology (ETH Zürich), Zürich, Switzerland; Chair of Photogrammetry and Remote Sensing, Swiss Federal Institute of Technology (ETH Zürich), Zürich, Switzerland; Chair of Photogrammetry and Remote Sensing, Swiss Federal Institute of Technology (ETH Zürich), Zürich, Switzerland","IEEE Transactions on Geoscience and Remote Sensing","21 Oct 2022","2022","60","","1","15","Fusing satellite imagery acquired with different sensors has been a long-standing challenge of Earth observation, particularly across different modalities such as optical and synthetic aperture radar (SAR) images. Here, we explore the joint analysis of imagery from different sensors in the light of representation learning: we propose to learn a joint embedding of multiple satellite sensors within a deep neural network. Our application problem is the monitoring of lake ice on Alpine lakes. To reach the temporal resolution requirement of the Swiss Global Climate Observing System (GCOS) office, we combine three image sources: Sentinel-1 SAR (S1-SAR), Terra moderate resolution imaging spectroradiometer (MODIS), and Suomi-NPP visible infrared imaging radiometer suite (VIIRS). The large gaps between the optical and SAR domains and between the sensor resolutions make this a challenging instance of the sensor fusion problem. Our approach can be classified as a late fusion that is learned in a data-driven manner. The proposed network architecture has separate encoding branches for each image sensor, which feed into a single latent embedding, i.e., a common feature representation shared by all inputs, such that subsequent processing steps deliver comparable output irrespective of which sort of input image was used. By fusing satellite data, we map lake ice at a temporal resolution of <1.5 days. The network produces spatially explicit lake ice maps with pixelwise accuracies >91% [respectively, mean per-class Intersection-over-Union (mIoU) scores >60%] and generalizes well across different lakes and winters. Moreover, it sets a new state-of-the-art for determining the important ice-on and ice-off dates for the target lakes, in many cases meeting the GCOS requirement.","1558-0644","","10.1109/TGRS.2022.3211184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9906117","Deep neural network;lake ice;moderate resolution imaging spectroradiometer (MODIS);satellite embedding learning;sensor fusion;Sentinel-1 SAR (S1-SAR);synthetic aperture radar (SAR);visible infrared imaging radiometer suite (VIIRS)","Lakes;Optical sensors;Spatial resolution;Ice;Satellites;Optical imaging;Integrated optics","geophysical image processing;ice;image classification;image fusion;infrared imaging;lakes;learning (artificial intelligence);radar imaging;radiometers;radiometry;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","Terra moderate resolution imaging spectroradiometer;S1-SAR;Sentinel-1 SAR;image sources;Swiss Global Climate Observing System office;temporal resolution requirement;Alpine lakes;application problem;deep neural network;representation learning;Earth observation;satellite imagery;lake ice monitoring;multiple satellite sensors;joint embedding;target lakes;important ice-on;winters;different lakes;spatially explicit lake ice maps;satellite data;input image;single latent embedding;image sensor;sensor fusion problem;sensor resolutions;optical SAR domains;Suomi-NPP visible infrared imaging radiometer suite;time 1.5 d","","1","","58","IEEE","30 Sep 2022","","","IEEE","IEEE Journals"
"SDCAFNet: A Deep Convolutional Neural Network for Land-Cover Semantic Segmentation With the Fusion of PolSAR and Optical Images","B. Chu; J. Chen; J. Chen; X. Pei; W. Yang; F. Gao; S. Wang","Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China; Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China; Key Laboratory of Aerospace Information Applications of CETC, Shijiazhuang, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Oct 2022","2022","15","","8928","8942","Due to the different imaging mechanisms between optical and polarimetric synthetic aperture radar (PolSAR) images, determining how to effectively use such complementary information has become an interesting and challenging problem. Convolutional neural networks (CNNs) and other deep neural networks have achieved good experimental results in remote sensing land-cover semantic segmentation. However, the CNN convolution structure can extract only the features within the receptive field in the spatial dimension without focusing on the relationship between multiple channels; therefore, it is impossible to realize fusion and complementarity between multiple channels. In this article, we propose a novel spatial dense channel attention fusion network (SDCAFNet), which takes optical and PolSAR images as different inputs and completes feature fusion and semantic segmentation within a neural network. First, SDCAFNet uses a two-stream siamese CNN network to realize the preliminary feature coding of optical and PolSAR images. Then, a spatial dense channel attention module (SDCAM) is proposed. The channel activation values obtained at different positions are combined in the spatial dense matrix, which can describe the attention in the feature fusion process. Finally, we introduce the fused features into the symmetric skip-connection decoder composed of multiple symmetric decoder blocks to realize end-to-end land-cover semantic segmentation. Experimental results show that SDCAFNet can effectively learn the correlation between optical and PolSAR channels and has a better segmentation accuracy than other methods.","2151-1535","","10.1109/JSTARS.2022.3213601","S&T Program of Hebei(grant numbers:21340302D); Foundation of the CETC Key Laboratory of Aerospace Information Applications(grant numbers:SCX20629T007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9916086","Channel attention;feature fusion;land-cover semantic segmentation;optical image;polarimetric synthetic aperture radar (PoLSAR)","Optical imaging;Feature extraction;Optical sensors;Adaptive optics;Task analysis;Decoding;Optical reflection","feature extraction;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;object detection;radar imaging;radar polarimetry;synthetic aperture radar","SDCAFNet;deep convolutional neural network;different imaging mechanisms;interesting problem;convolutional neural networks;deep neural networks;good experimental results;remote sensing land-cover semantic segmentation;CNN convolution structure;spatial dimension;multiple channels;novel spatial dense channel attention fusion network;optical images;completes;two-stream siamese CNN network;preliminary feature coding;optical PolSAR images;spatial dense channel attention module;channel activation values;spatial dense matrix;feature fusion process;fused features;multiple symmetric decoder blocks;end-to-end land-cover semantic segmentation;optical PolSAR channels;segmentation accuracy","","","","48","CCBYNCND","11 Oct 2022","","","IEEE","IEEE Journals"
"Fusion of Spaceborne and Airborne SAR Images via Target Proposal and Polarization Information Exploitation for Vessel Detection","D. Zhu; X. Wang; Y. Cheng; G. Li","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; School of Electronics and Information Engineering, Harbin Institute of Technology, Harbin, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2021 CIE International Conference on Radar (Radar)","8 Feb 2023","2021","","","2058","2061","In this paper, we focus on the vessel detection via fusion of synthetic aperture radar (SAR) images acquired from spaceborne-airborne collaborative observations, and propose a new method based on target proposal and polarization information exploitation (TPPIE). First, a new triple-state proposal matrix (TSPM) is generated by combing the normed gradient-based target proposal and the edge-based morphological candidate map. Second, we present a new polarization feature, named absolute polarization ratio (APR), to exploit the intensity information of dual-polarization SAR images. Third, the final fused image with enhanced targets and suppressed backgrounds, i.e., improved target-to-cluster ratio (TCR), is attained by the Hadamard product of the intersected TSPM from multi-sources and the composite map exploiting APR feature. Experimental results using Gaofen-3 satellite and unmanned aerial vehicle (UAV) SAR images show that the proposed TPPIE fusion method yields higher TCRs of fused images and better detection performance of vessel targets than the commonly used image fusion approaches.","2640-7736","978-1-6654-9814-2","10.1109/Radar53847.2021.10028255","National Natural Science Foundation of China(grant numbers:61901244,61790551,61925106,61901242,62101303); Central Universities (Huazhong University of Science and Technology)(grant numbers:BX20200195); China Postdoctoral Science Foundation(grant numbers:2020M680561); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10028255","Image fusion;polarization information;spaceborne-airborne collaboration;synthetic aperture radar (SAR) image;target proposal;vessel detection","Satellites;Spaceborne radar;Image edge detection;Radar detection;Collaboration;Radar imaging;Autonomous aerial vehicles","","","","","","11","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"Pixel-Wise Cloud Dictionary Learning for Fusing Optical and SAR Data","J. Ling; H. Zhang","The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China; The University of Hong Kong Shenzhen Institute of Research and Innovation, Shenzhen, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5840","5843","Urban land cover (ULC) is a fundamental indicator of urbanization, while cloud cover hinders accurate and timely ULC monitoring. The fusion of synthetic aperture radar (SAR) and cloud-free optical data has shown good performance in previous studies, while there is a lack of investigation in cloud-prone areas where optical data is contaminated by clouds. This study proposes a cloud-oriented framework for fusing the two data sources for ULC classification in cloud-prone areas. For alleviating cloud interference, the framework proposes a cloud probability weighting strategy and a pixel-wise cloud dictionary learning algorithm considering the interference difference in different cloud probability levels. Experiments show that all algorithms using fused data improve the overall accuracy (OA) of above 6% and 20% compared with using single SAR and optical data, respectively. Compared with traditional SVM, RF, and dictionary learning methods which ignore cloud interference and directly concatenate optical and SAR features, the proposed method shows a significant improvement of 3% in OA. It improves almost all land covers' producer accuracy (PA) and user accuracy (UA), up to 9%. Further experiments with three cloud probability level samples find that the higher the cloud probability, the lower the classification accuracy of the sample. At each probability level, the proposed pixel-wise cloud dictionary learning method improves more than 2% in OA, improves up to 4% to 10% in PA and UA.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9884125","National Natural Science Foundation of China(grant numbers:42022061,42071390); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9884125","Cloud;SAR;ULC;dictionary learning","Support vector machines;Radio frequency;Laser radar;Soft sensors;Machine learning;Interference;Adaptive optics","clouds;geophysical image processing;image classification;image fusion;land cover;radar imaging;remote sensing by radar;support vector machines;synthetic aperture radar","fusing optical;SAR data;urban land cover;cloud cover hinders;cloud-prone areas;optical data;cloud-oriented framework;data sources;ULC classification;alleviating cloud interference;cloud probability weighting strategy;pixel-wise cloud dictionary learning algorithm;interference difference;different cloud probability levels;fused data;single SAR;directly concatenate optical SAR features;land covers;cloud probability level samples;pixel-wise cloud dictionary learning method","","","","8","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Improving Pixel-Based Change Detection Accuracy Using an Object-Based Approach in Multitemporal SAR Flood Images","J. Lu; J. Li; G. Chen; L. Zhao; B. Xiong; G. Kuang","School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; Department of Geography and Environmental Management, University of Waterloo, Waterloo, ON, Canada; Department of Geography and Earth Sciences, University of North Carolina at Charlotte, Charlotte, NC, USA; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China; School of Electronic Science and Engineering, National University of Defense Technology, Changsha, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 May 2017","2015","8","7","3486","3496","Most of existing change detection methods could be classified into three groups, the traditional pixel-based change detection (PBCD), the object-based change detection (OBCD), and the hybrid change detection (HCD). Nevertheless, both PBCD and OBCD have disadvantages, and classical HCD methods belong to intuitive decision-level fusion schemes of PBCD and OBCD. There is no optimum HCD method as of yet. Analyzing the complementarities of PBCD and OBCD method, we propose a new unsupervised algorithm-level fusion scheme (UAFS-HCD) in this paper to improve the accuracy of PBCD using spatial context information through: 1) getting the preliminary change mask with PBCD at first to estimate some parameters for OBCD; 2) deriving the unchanged area mask to eliminate the areas without changes, reducing error amplification phenomenon of OBCD; and 3) obtaining the final change mask by means of OBCD method. Taking flood detection with multitemporal SAR data as an example, we compared the new scheme with some classical methods, including PBCD, OBCD, and HCD method and supervised manual trial-and-error procedure (MTEP). The experimental results of flood detection showed that the new scheme was efficient and robust, and its accuracy sometimes can even exceed MTEP.","2151-1535","","10.1109/JSTARS.2015.2416635","China Scholarship Council(grant numbers:201304740123); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081753","Change detection;object-based;pixel-based;SAR images;unsupervised change detection;Change detection;object-based;pixel-based;SAR images;unsupervised change detection","Change detection algorithms;Synthetic aperture radar;Histograms;Accuracy;Feature extraction;Backscatter;Surface waves","floods;geophysical image processing;hydrological techniques;image fusion;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","pixel-based change detection method;object-based change detection method;optimum hybrid change detection method;intuitive decision-level fusion schemes;unsupervised algorithm-level fusion scheme;spatial context information;preliminary change mask;unchanged area mask;error amplification phenomenon;multitemporal SAR data;supervised manual trial-and-error procedure;multitemporal SAR flood images","","56","","51","IEEE","8 Apr 2015","","","IEEE","IEEE Journals"
"Combining SAR-Based and Multispectral-Based Extractions to Map Urban Areas at Multiple Spatial Resolutions","A. Salentinig; P. Gamba","Dipartimento di Ingegneria Industriale e dell'Informazione, Universitá di Pavia, Pavia, Italy; Dipartimento di Ingegneria Industriale e dell'Informazione, Universitá di Pavia, Pavia, Italy","IEEE Geoscience and Remote Sensing Magazine","30 Sep 2015","2015","3","3","100","112","Urban remote sensing and data fusion are intimately connected, and this paper discusses recent developments in urban extent extraction using remotely sensed data with data sets or algorithms working at the global level. To achieve the results presented in this paper, several data fusion techniques at the raw data level, the feature level, and the decision level are designed and exploited. Specifically, multi-resolution data fusion techniques to exploit SAR data acquired in different modes, as well as fusion of information extracted from SAR and multispectral data are considered and evaluated.","2168-6831","","10.1109/MGRS.2015.2430874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284788","","Urban areas;Spatial resolution;Synthetic aperture radar;Data mining;Remote sensing;Feature extraction","feature extraction;image fusion;image resolution;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","multispectral-based extraction;urban area mapping;multiple spatial resolution;urban remote sensing;urban extent extraction;multiresolution data fusion technique;SAR","","22","","53","IEEE","30 Sep 2015","","","IEEE","IEEE Magazines"
"Dual-Polarization SAR Ship Target Recognition Based on Mini Hourglass Region Extraction and Dual-Channel Efficient Fusion Network","G. Xiong; Y. Xi; D. Chen; W. Yu","School of Electronic Information and Electrical Engineering, Institute of Sensing and Navigation, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Institute of Sensing and Navigation, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Institute of Sensing and Navigation, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Institute of Sensing and Navigation, Shanghai Jiao Tong University, Shanghai, China","IEEE Access","22 Feb 2021","2021","9","","29078","29089","A novel Dual-polarization SAR ship target recognition method based on feature and loss fusion deep network is proposed in this paper, to improve the generalization ability, the recognition accuracy of imbalance samples, and the real-time performance of the general deep learning network. The proposed combined network is composed of two parts. The first part is the target region extraction network based on lightweight mini Hourglass network, to eliminate the impact of data imbalance and background noise on the identification accuracy. The second part is a two-channel feature/loss function fusion network based on Efficient B2 backbone network, aiming to solve the problem of dual-polarization image feature fusion and iterative convergence acceleration. The proposed method is tested on SAR image ship slices from the OpenSAR data sets. The experimental results indicates that, the proposed method achieves a recognition rate of 110FPS with recognition accuracy of 87.72%, and exceeds the SOTA by recognition accuracy 3.72% with convergence speed improved by 75.68%. The proposed method can be applied to SAR target recognition with dual polarization, imbalance samples and low-resolution condition for reference.","2169-3536","","10.1109/ACCESS.2021.3058188","National Natural Science Foundation of China (NSFC)(grant numbers:62071293,61571294); Equipment Pre-research Area Fund Project(grant numbers:61404130221); Key Laboratory Fund Project(grant numbers:61421060105); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9350622","Dual-polarization SAR;ship target recognition;dual-channel efficient network;fusion loss function","Target recognition;Marine vehicles;Synthetic aperture radar;Feature extraction;Radar polarimetry;Image recognition;Training","deep learning (artificial intelligence);feature extraction;image fusion;image recognition;radar imaging;ships;synthetic aperture radar","mini Hourglass region extraction;Dual-channel Efficient fusion network;novel Dual-polarization SAR ship target recognition method;loss fusion deep network;generalization ability;recognition accuracy;imbalance samples;general deep learning network;target region extraction network;lightweight mini Hourglass network;data imbalance;background noise;identification accuracy;Efficient B2 backbone network;dual-polarization image;iterative convergence acceleration;SAR image ship slices;recognition rate;SAR target recognition;dual polarization","","7","","29","CCBY","9 Feb 2021","","","IEEE","IEEE Journals"
"MSCDUNet: A Deep Learning Framework for Built-Up Area Change Detection Integrating Multispectral, SAR, and VHR Data","H. Li; F. Zhu; X. Zheng; M. Liu; G. Chen","Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Guangdong Provincial Key Laboratory of Urbanization and Geo-Simulation, School of Geography and Planning, Sun Yat-sen University, Guangzhou, China; Division of Landscape Architecture, Faculty of Architecture, The University of Hong Kong, Hong Kong SAR, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","6 Jul 2022","2022","15","","5163","5176","Built-up area change detection (CD) plays an important role in city management, which always uses very high spatial resolution (VHR) remote sensing data to extract refined spatial information. Recently, many CD models based on deep learning with VHR data have been proposed. However, due to the complex background information and natural landscape changes, VHR with optical RGB features is hard to extract changes exactly. To this end, we tend to explore the abundant channel information of multispectral and SAR data as a supplement to the refined spatial features of VHR images. We propose a new deep learning framework called multisource CD UNet++ (MSCDUNet), integrating multispectral, SAR, and VHR data for built-up area CD. First, we label and reform two new built-up area CD datasets containing multispectral, SAR, and VHR data: multisource built-up change (MSBC) and multisource OSCD (MSOSCD) datasets. Second, a feature selection method based on random forest is introduced to choose effective features from multispectral and SAR images. Finally, a multilevel heterogeneous feature fusion module is embedded in MSCDUNet to combine multifeatures for CD. Experiments are conducted on both the MSOSCD and the MSBC datasets. Compared to other CD methods based on VHR images, our proposal achieves the highest accuracy on both datasets and proves the effectiveness of multispectral, SAR, and VHR data fusion for CD. The dataset in the article will be available for download from the following link.1","2151-1535","","10.1109/JSTARS.2022.3181155","National Natural Science Foundation of China(grant numbers:61976234); Guangzhou Applied Basic Research Project; Natural Science Foundation of Guangdong Province(grant numbers:2019A1515011057); Fundamental Research Funds for the Central Universities; Sun Yat-sen University(grant numbers:22qntd2001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9791854","Benchmark dataset;built-up;change detection (CD);deep learning (DL);multispectral data fusion;very high resolution (VHR)","Feature extraction;Synthetic aperture radar;Data mining;Spatial resolution;Remote sensing;Task analysis;Semantics","convolutional neural nets;deep learning (artificial intelligence);feature extraction;feature selection;geophysical image processing;geophysical techniques;hyperspectral imaging;image classification;image colour analysis;image fusion;image resolution;object detection;random forests;remote sensing by radar;synthetic aperture radar","very high spatial resolution images;multispectral data fusion;multilevel heterogeneous feature fusion module;multispectral images;random forest;feature selection method;MSOSCD datasets;MSBC datasets;multisource OSCD datasets;multisource built-up change datasets;optical RGB feature extraction;complex background information;multisource CD UNet++;deep learning framework;VHR images;refined spatial features;natural landscape changes;complex background information;CD models;high spatial resolution remote sensing data;SAR;area change detection;MSCDUNet;VHR data fusion","","2","","68","CCBY","8 Jun 2022","","","IEEE","IEEE Journals"
"Geometric and Polarimetric Sharpening of SAR Images by Kennaugh- and Schmittlet-based Multi-frequency Data Fusion","A. Schmitt; A. Wendleder",NA; NA,"Proceedings of EUSAR 2016: 11th European Conference on Synthetic Aperture Radar","5 Sep 2016","2016","","","1","4","The joint use of diverse sensors is of major interest in the remote sensing community. As each sensor has its own characteristics in terms of geometric, radiometric and polarimetric resolution, it provides certain advantages and disadvantages for special applications. This contribution presents a way of image sharpening both in the geometric and polarimetric domain by joining the acquisitions of TerraSAR-X and RADARSAT-2. The image fusion is performed by the help of the Kennaugh element framework and the Schmittlet image enhancement. An unexpected high richness of detail characterizes the combined image and facilitates the following image interpretation like land cover classification.","","978-3-8007-4228-8","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559483","","","","","","","","","","5 Sep 2016","","","VDE","VDE Conferences"
"THz Radar Security Screening Method for Walking Human Torso With Multi-Angle Synthetic Aperture","S. Gui; Y. Yang; J. Li; F. Zuo; Y. Pi","School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; Key Laboratory of Electronic and Information Engineering, Southwest Minzu University, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China (Qingshuihe Campus), Chengdu, China; AVIC Leihua Electronic Technology Research Institute, Wuxi, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China (Qingshuihe Campus), Chengdu, China","IEEE Sensors Journal","13 Aug 2021","2021","21","16","17962","17972","Terahertz (THz) radar imaging has widely adopted for security screening of standoff human body in recent years, owing to its capacities of high resolution, harmless, contactless, and penetrate clothing. However, THz radar security screening for walking human is still a challenge due to the non-rigid motion of walking. To figure it out, a THz radar security screening method for walking human torso is proposed with multi-angle synthetic aperture. In this method, the echo data of walking human torso component is assumed as a rigid motion model within a short time. Particularly, the torso component has the maximum ratio of surface area and echo energy, which means that its motion phase error can be detected after fractional Fourier transform processing. Besides, after the correction and imaging processing, the imaging results under different angles are fused by coherent integration with registration processing to improve the integrity of scattering information, which is incomplete within a short aperture due to self-occlusion and the changes in incident angles. The simulation experiments are conducted to verify the imaging quality and feasibility of the proposed method. Moreover, the real small aperture echo data of moving torso collected by our THz radar (that operates at 330GHz with the bandwidth of 30GHz and the output peak power of 5mW) is processed via the proposed method to indicate the significance of multi-angle imaging.","1558-1748","","10.1109/JSEN.2021.3083816","Fundamental Research Funds for the Central Universities(grant numbers:2021NQNCZ18); Science and technology research program of Chongqing Education Commission(grant numbers:KJQN201900603); Fundamental and Frontier Research Project of Chongqing(grant numbers:cstc2019jcyjmsxmX0635,cstc2019jcyj-msxmX0635); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440943","Walking human imaging;THz radar;synthetic aperture radar;phase error correction;security screening","Radar imaging;Radar;Legged locomotion;Imaging;Security;Apertures;Synthetic aperture radar","image fusion;image motion analysis;image registration;microwave imaging;millimetre wave imaging;millimetre wave radar;radar imaging;submillimetre wave imaging;terahertz wave imaging","fractional Fourier transform processing;walking human torso;THz radar imaging;multiangle imaging processing;rigid motion model;human torso component;terahertz radar imaging;multiangle synthetic aperture;THz radar security screening method;frequency 330.0 GHz;power 5.0 mW;bandwidth 30.0 GHz","","3","","36","IEEE","26 May 2021","","","IEEE","IEEE Journals"
"Fully Complex-valued Fully Convolutional Multi-feature Fusion Network(FC2MFN) for Building Segmentation of InSAR images","A. Sikdar; S. Udupa; S. Sundaram; N. Sundararajan","Robert Bosch Centre for Cyber Physical Systems, Indian Institute of Science, Bengaluru, India; Department of Aerospace Engineering, Indian Institute of Science, Bengaluru, India; Department of Aerospace Engineering, Indian Institute of Science, Bengaluru, India; Department of Aerospace Engineering, Indian Institute of Science, Bengaluru, India","2022 IEEE Symposium Series on Computational Intelligence (SSCI)","30 Jan 2023","2022","","","581","587","Building segmentation in high-resolution InSAR images is a challenging task that can be useful for large-scale surveillance. Although complex-valued deep learning networks perform better than their real-valued counterparts for complexvalued SAR data, phase information is not retained throughout the network, which causes a loss of information. This paper proposes a Fully Complex-valued, Fully Convolutional Multifeature Fusion Network $(FC^{2}\mathbf{MFN})$ for building semantic segmentation on InSAR images using a novel, fully complex-valued learning scheme. $FC^{2}$ MFN learns multi-scale features, performs multi-feature fusion, and has a complex-valued output. For the particularity of complex-valued InSAR data, a new complexvalued pooling layer is proposed that compares complex numbers considering their magnitude and phase. This helps the network retain the phase information even through the pooling layer. Experimental results on the simulated InSAR dataset [1] show that $FC^{2}$ MFN achieves better results compared to other state-of theart methods in terms of segmentation performance and model complexity.","","978-1-6654-8768-9","10.1109/SSCI51031.2022.10022109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10022109","Semantic segmentation;fully complex-valued convolutional neural network;interferometric synthetic aperture radar (InSAR)","Deep learning;Semantic segmentation;Surveillance;Buildings;Semantics;Complexity theory;Convolutional neural networks","deep learning (artificial intelligence);feature extraction;image fusion;image segmentation;radar computing;radar imaging;radar interferometry;synthetic aperture radar;video surveillance","$FC^{2}$MFN;building segmentation;complex numbers;complex valued deep learning networks;complex valued InSAR data;complex valued pooling layer;complex-valued output;complexvalued SAR data;FC 2 MFN;fully convolutional multifeature fusion network;high resolution InSAR images;large scale surveillance;model complexity;multiscale features;performs multifeature fusion;phase information;segmentation performance;semantic segmentation;simulated InSAR dataset","","","","25","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Hierarchical multinomial latent model with G0 distribution for remote sensing image semantic segmentation","Y. Duan; X. Tao; C. Han; J. Lu","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","8 Mar 2018","2017","","","254","258","Considering the scattering statistics and multi-scale characteristics of the remote sensing images, this paper presents a hierarchical multinomial latent model with G0 distribution (HML-G0) for remote sensing image semantic segmentation. In the proposed approach, hierarchical multinomial latent model is proposed to capture the multi-scale information of the remote sensing images. Moreover, the flexibility of G0 distribution is plugged into the hierarchical multinomial latent model for the segmentation of various types of land covers. Then, the developed Bayesian inference on the quadtree is incorporated in our approach, and the semantic segmentation map is achieved by bottom-up and top-down probability computation. Experimental results demonstrate that our proposed hierarchical scheme produces the semantic segmentation maps, and the exhibiting performance improvements in terms of labeling consistency and the detail preservation.","","978-1-5090-5990-4","10.1109/GlobalSIP.2017.8308643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8308643","Remote sensing images;semantic segmentation;hierarchical multinomial latent model;G0;distribution;Bayesian inference","Image segmentation;Remote sensing;Semantics;Synthetic aperture radar;Context modeling;Bayes methods;Microsoft Windows","Bayes methods;geophysical image processing;image classification;image fusion;image representation;image segmentation;inference mechanisms;probability;quadtrees;remote sensing","top-down probability computation;semantic segmentation maps;labeling consistency;detail preservation;scattering statistics;remote sensing image semantic segmentation;hierarchical multinomial latent model;remote sensing images","","4","","15","IEEE","8 Mar 2018","","","IEEE","IEEE Conferences"
"Multimodal-Temporal Fusion: Blending Multimodal Remote Sensing Images to Generate Image Series With High Temporal Resolution","X. Liu; C. Deng; B. Zhao; J. Chanussot","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Univ. Grenoble Alpes, CNRS, Grenoble INP GIPSA-lab, Grenoble, France","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","10083","10086","This paper aims to tackle a general but interesting cross-modality problem in remote sensing community: can multimodal images help to generate synthetic images in time series and improve temporal resolution? To this end, we explore multimodal-temporal fusion, in which we attempt to leverage the availability of additional cross-modality images to simulate the missing images in time series. We propose a multimodal-temporal fusion framework, and mainly focus on two kinds of information for the simulation: intra-modal cross-modality information and inter-modal temporal information. To exploit the cross-modality information, we adopt available paired images and learn a mapping between different modality images using a deep neural network. Considering temporal dependency among time-series images, we formulate a temporal constraint in the learning to encourage temporal consistent results. Experiments are conducted on two cross-modality image simulation applications (SAR to visible and visible to SWIR), and both visual and quantitative results demonstrate that the proposed model can successfully simulate missing images with cross-modality data.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898453","Multimodal-Temporal Fusion;Cross-modality Image Translation;Image Time Series;Temporal Resolution;Deep Neural Networks","Time series analysis;Remote sensing;Image resolution;Neural networks;Data models;Synthetic aperture radar;Simulation","geophysical image processing;image fusion;image resolution;learning (artificial intelligence);neural nets;remote sensing;time series","time-series images;cross-modality data;temporal resolution;cross-modality problem;multimodal-temporal fusion;intramodal cross-modality information;intermodal temporal information;multimodal remote sensing image blending;cross-modality image simulation;deep neural network","","3","","5","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Mapping Cropland Extent by Asynchronous Fusion of Optical and Active Microwave Imagery","S. Chakrabarti; T. Cormier; N. Malizia; D. Potere; D. Sulla-Menashe; K. Zmijewski; M. Friedl","Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA; Telluslabs, Inc., Somerville, Massachusetts, USA","IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium","4 Nov 2018","2018","","","5319","5321","In this study, a machine learning based algorithm is developed for dynamically classifying and determining area of agricultural land-use using optical and active microwave remote sensing. It includes a gradient boosted machine trained in the US using labels derived from the United States Department of Agriculture National Agricultural Statistics Service crop land data layers (CDL). This is then applied to the state of Mato Grosso in Brazil with a hidden Markov model applied to temporally stabilize the land-cover predictions. High resolution remote sensing products such as land surface temperature, normalized difference vegetation index, nadir adjusted bidirectional reflectance, land-cover and radar backscatter were used to develop the classification model. The results in the US were validated using a hold-out set with CDL labels. The results in Brazil were validated using the harvested area statistics reported by Companhia Nacional de Abastecimento.","2153-7003","978-1-5386-7150-4","10.1109/IGARSS.2018.8518892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8518892","Crop Mapping;Land Cover;Machine Learning","Agriculture;Remote sensing;Synthetic aperture radar;Biomedical optical imaging;Optical imaging;Optical sensors;MODIS","agriculture;crops;geophysical image processing;hidden Markov models;image classification;image fusion;land surface temperature;learning (artificial intelligence);terrain mapping;vegetation mapping","hidden Markov model;land-cover predictions;high resolution remote sensing products;land surface temperature;normalized difference vegetation index;nadir adjusted bidirectional reflectance;US;CDL labels;Brazil;harvested area statistics;cropland extent;asynchronous fusion;optical microwave imagery;active microwave imagery;machine learning;agricultural land-use;optical microwave remote sensing;active microwave remote sensing;Mato Grosso;United States Department of Agriculture;National Agricultural Statistics Service crop land data layers","","","","9","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"Estimation of Ship Dynamics with a Multiplatform Radar Imaging System","F. Santi; D. Pastina; M. Bucciarelli","Sapienza University of Rome, Rome, Italy; Sapienza University of Rome, Rome, Italy; Sapienza University of Rome, Rome, Italy","IEEE Transactions on Aerospace and Electronic Systems","7 Dec 2017","2017","53","6","2769","2788","Distributed inverse synthetic aperture radar (ISAR) exploits the data acquired by multiple radar sensors carried by multiple platforms working in formation to increase the cross range resolution with respect to the value achievable by single platform systems. In this frame, the paper addresses the problem of the estimation of the ship dynamics, i.e., yaw, pitch, and roll rotation motions, exploiting the signals collected by such multiplatform radar imaging systems providing angular diversity in order to enable the focusing of the distributed ISAR images. Specifically, in this work a multi-angle formation of sensors is considered and the corresponding maximum likelihood estimator and Cramer Rao bound are derived. Then, a new Doppler matching based technique is proposed as a suboptimal approach exploiting only the linear component of the phase of the received signals. The performance analysis proves the effectiveness of the proposed techniques to separately estimate the horizontal, radial, and vertical components of the rotation vector, therefore making possible both the focusing and accurate cross-range scaling of the distributed ISAR products (as well as of the low-resolution ISAR images regarding the different sensors) and, as a further advantage, providing knowledge of the orientation of the corresponding image projection planes. The analysis of experimental multisensor datasets confirms the feasibility of the proposed techniques.","1557-9603","","10.1109/TAES.2017.2714960","Selex Galileo CTO - Innovative Projects Analysis & Demonstrations Group, Pomezia, Italy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7947191","Radar imaging;Inverse SAR (ISAR);multiplatform;multistatic;motion estimation","Sensors;Estimation;Marine vehicles;Image resolution;Radar imaging;Three-dimensional displays","distributed sensors;Doppler radar;image fusion;image matching;image sensors;marine radar;maximum likelihood estimation;motion measurement;radar imaging;radar receivers;ships;synthetic aperture radar;vectors","ship dynamic estimation;multiplatform radar imaging system;distributed inverse synthetic aperture radar;data acquisition;multiple radar sensor;cross range resolution;single platform system;distributed ISAR imaging;maximum likelihood estimator;Cramer Rao bound;Doppler matching based technique;horizontal component estimation;radial component estimation;vertical component estimation;rotation vector;cross-range scaling;low-resolution ISAR imaging;image projection plane;multisensor dataset","","15","","51","IEEE","13 Jun 2017","","","IEEE","IEEE Journals"
"Multisensor Time Synchronization Error Modeling and Compensation Method for Distributed POS","J. Li; L. Jia; G. Liu","Science and Technology on Inertial Laboratory and the Key Laboratory of Fundamental Science for National Defense-Novel Inertial Instrument and Navigation System Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China; Science and Technology on Inertial Laboratory and the Key Laboratory of Fundamental Science for National Defense-Novel Inertial Instrument and Navigation System Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China; Science and Technology on Inertial Laboratory and the Key Laboratory of Fundamental Science for National Defense-Novel Inertial Instrument and Navigation System Technology, School of Instrument Science and Opto-Electronic Engineering, Beihang University, Beijing, China","IEEE Transactions on Instrumentation and Measurement","7 Oct 2016","2016","65","11","2637","2645","An airborne distributed position and orientation system (POS) is high-precision measurement equipment that can accurately provide multinode time-spatial reference for novel remote sensing system as multitask imaging sensors and array antenna synthetic aperture radar. However, it is difficult for multisensor to precisely acquire information at the same moment and result in data fusion error. Thus, the measurement precision is severely degraded. To solve the problem, a multisensor time synchronization error modeling and compensation method is proposed. Based on the component and operation principles of distributed POS, the time synchronization mechanism is analyzed. Multisensor time synchronization error models that include time delay error, random error, and time-varying error are established. A time synchronization error compensation method of the distributed POS is proposed. The experiment results show that the proposed method can accurately calibrate and compensate for the time synchronization error, and improve the measurement precision of the distributed POS. It verified the validity of the proposed method.","1557-9662","","10.1109/TIM.2016.2598020","National Natural Science Foundation of China(grant numbers:61571030); National High Technology Research and Development Program of China (863 Program)(grant numbers:2015AA124001); Beijing Youth Elite Project; CALT Innovation Foundation(grant numbers:CALT201505); Basic Scientific Research(grant numbers:YWF-15-YQGD-001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7547391","Distributed position and orientation system (POS);modeling and compensation;multisensor;time synchronization error","Synchronization;Clocks;Delay effects;Sensors;Global Positioning System;Imaging;Antenna arrays","compensation;distributed sensors;image fusion;image sensors;position measurement;radar antennas;remote sensing;sensor arrays;synchronisation;synthetic aperture radar;time measurement","multisensor time synchronization error modeling;compensation method;distributed POS;airborne distributed position and orientation system;high-precision measurement equipment;multinode time-spatial reference;remote sensing system;multitask imaging sensor;array antenna synthetic aperture radar;data fusion error;time delay error;random error;time-varying error;calibration","","14","","21","IEEE","18 Aug 2016","","","IEEE","IEEE Journals"
"Polarimetric SAR Image Classification Based on Ensemble Dual-Branch CNN and Superpixel Algorithm","W. Hua; C. Zhang; W. Xie; X. Jin","Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China; Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China; Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China; Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, School of Computer Science and Technology, Xi'an University of Posts and Telecommunications, Xi'an, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","19 Apr 2022","2022","15","","2759","2772","Recently, convolutional neural networks (CNNs) have been successfully utilized in polarimetric synthetic aperture radar (PolSAR) image classification and obtained promising results. However, most CNN-based classification methods require a large number of labeled samples and it is difficult to obtain sufficient labeled samples. For this reason, an ensemble dual-branch CNN (EDb-CNN) is proposed for PolSAR image classification with small samples. First, to solve the problem of the small sample in PolSAR image classification, a new data enhancement method based on the superpixel algorithm is proposed to expand the number of labeled samples. Second, to obtain different scales of features from PolSAR images, a Db-CNN model is proposed. This model contains two parallel CNN structures. One CNN branch is used to extract the polarization features from the complex coherency matrix. The other branch is utilized to extract the spatial features based on weighted spatial neighborhood. On the top of these two branches, a feature fusion model is adopted to combine these two deep features, and a weighted loss function is employed to improve the learning procedure. Then, the ensemble learning algorithm is used for each CNN branch and Db-CNN network to obtain the better classification results. Finally, a postprocess algorithm based on the superpixel algorithm is proposed to improve the consistency of classification results. Experiments on two PolSAR datasets show that the proposed method achieves a much better performance than other classification methods, especially when only a few labeled samples are available.","2151-1535","","10.1109/JSTARS.2022.3162953","National Natural Science Foundation of China(grant numbers:61901368); Natural Science Foundation of Shaanxi Province(grant numbers:2019JQ-377); Key Special project of China High Resolution Earth Observation System Young Scholar Innovation Fund(grant numbers:GFZX04061502); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744483","Dual-branch convolutional neural network (Db-CNN);polarimetric SAR;superpixels;terrain classification","Feature extraction;Image classification;Licenses;Classification algorithms;Covariance matrices;Remote sensing;Image recognition","convolutional neural nets;deep learning (artificial intelligence);feature extraction;geophysical image processing;image classification;image fusion;image resolution;radar imaging;radar polarimetry;remote sensing by radar;synthetic aperture radar","Db-CNN network;superpixel algorithm;PolSAR datasets;polarimetric SAR image classification;ensemble dual-branch CNN;convolutional neural networks;polarimetric synthetic aperture radar image classification;CNN-based classification methods;labeled samples;EDb-CNN;PolSAR image classification;data enhancement method;PolSAR images;Db-CNN model;parallel CNN structures;CNN branch;polarization features;spatial features;feature fusion model;ensemble learning algorithm;complex coherency matrix","","","","43","CCBY","29 Mar 2022","","","IEEE","IEEE Journals"
"Automatic Area-Based Registration of Optical and SAR Images Through Generative Adversarial Networks and a Correlation-Type Metric","L. Maggiolo; D. Solarna; G. Moser; S. B. Serpico","University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy; University of Genoa, Genoa, Italy","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","2089","2092","The automatic registration of multisensor remote sensing images is a highly challenging task due to the inherently different physical, statistical, and textural properties of the input data. In the present paper, this problem is addressed in the case of optical-SAR images by proposing a novel method based on deep learning and area-based registration concepts. The method integrates a conditional generative adversarial network (cGAN), an area-based cross-correlation-type l2 similarity metric, and the COBYLA constrained maximization algorithm. Whereas correlation-type metrics are typically ineffective in the application to multisensor registration, the proposed approach allows exploiting the image translation capabilities of cGAN architectures to enable the use of an l2 similarity metric, which favors high computational efficiency. Experiments with Sentinel-1 and Sentinel-2 data suggest the effectiveness of this strategy and the capability of the proposed method to achieve accurate registration.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323235","European Space Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323235","Multisensor image registration;conditional generative adversarial network;$\ell^{2}$ similarity;COBYLA","Radar polarimetry;Optical imaging;Measurement;Optical sensors;Feature extraction;Training;Generative adversarial networks","geophysical image processing;geophysical signal processing;image classification;image fusion;image registration;radar imaging;remote sensing;remote sensing by radar;sensor fusion;synthetic aperture radar","automatic area-based registration;generative adversarial networks;correlation-type metric;automatic registration;multisensor remote sensing images;textural properties;optical-SAR images;deep learning;area-based registration concepts;conditional generative adversarial network;area-based cross-correlation-type l;image translation capabilities;Sentinel-2 data","","4","","14","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"GEPATAR: A geotechnical based PS-InSAR toolbox for architectural conservation in Belgium","M. Shimoni; J. Lopez; J. Walstra; P. . -Y. Declercq; L. Bejarano-Urrego; E. Verstrynge; D. Derauw; R. Hayen; K. Van Balen","Signal and Image Centre, Belgian Royal military Academy (SIC-RMA), Brussels, Belgium; Signal and Image Centre, Belgian Royal military Academy (SIC-RMA), Brussels, Belgium; Geological Survey of Belgium, Royal Belgian Institute of Natural Sciences (RBINS), Brussels, Belgium; Geological Survey of Belgium, Royal Belgian Institute of Natural Sciences (RBINS), Brussels, Belgium; Building Materials and Building Technology Division, KU Leuven, Belgium; Building Materials and Building Technology Division, KU Leuven, Belgium; Central Spatial of Liege (CSL), Angleur, Belgium; Royal Institute for Cultural Heritage (KIK-IRPA), Brussels, Belgium; Building Materials and Building Technology Division, KU Leuven, Belgium","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5555","5558","Ground displacements that cause structural damage to heritage buildings are precipitating cultural and economic value losses. The GEPATAR project (GEotechnical and Patrimonial Archives Toolbox for ARchitectural conservation in Belgium) aims creating an online interactive geoinformation tool that allows the user to view and to be informed about the Belgian heritage buildings at risk due to differential ground movements. In the last decade, Persistent Scatterer SAR interferometry (PS-InSAR) has proven to be a powerful technique for analysing earth surface deformation. In order to identify the level of risk at national and local scales, this information is integrated with the Belgian heritage data by means of a GIS environment interactive toolbox and fusion modules. This paper presents a description of the methodology implemented in the project together with the case study of Saint-Vincent church, located in Zolder in a former colliery zone, for which damage is assessed.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128263","PS-InSAR;differential ground movements;stability damage;historical buildings;GeoWeb toolbox","Strain;Buildings;Cultural differences;Interferometry;Monitoring;Time series analysis;Geology","buildings (structures);deformation;geographic information systems;geomorphology;geophysical image processing;image fusion;radar interferometry;remote sensing by radar;synthetic aperture radar","Belgium;ground displacements;structural damage;cultural value losses;economic value losses;GEPATAR project;online interactive geoinformation tool;Belgian heritage buildings;fusion modules;ground movements;Earth surface deformation;geotechnical-based PS-InSAR toolbox;Geotechnical and Patrimonial Archives Toolbox for Architectural Conservation;persistent scatterer SAR interferometry;Zolder;colliery zone","","4","","9","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Multi-Modal Fusion Architecture Search for Land Cover Classification Using Heterogeneous Remote Sensing Images","X. Li; L. Lei; G. Kuang","State Key Laboratory of Complex Electromagnetic Environment Effects on Electronics and Information System; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5997","6000","Optical and SAR modalities can provide the complementary information on land properties for better land cover classification. Most of existing multi-modal land cover classification methods based on two-streams convolutional neural networks (CNNs), which obtained fusion features by merging optical and SAR features that come from manually selective layer of different streams. However, they ignored different semantic between manually selective optical and SAR features, which might result in suboptimal fusion features. We tackle the problem of finding good fusion architectures for multimodal land cover classification inspired by the network architecture search (NAS), and introduces the multi-modal fusion architecture search network (M2PASNet). Extensive experimental results show superior performances of our work on a broad co-registered optical and SAR dataset.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9555029","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555029","Land cover classification;convolutional neural networks (CNNs);network architecture search (NAS);multi-modal fusion","Semantics;Neural networks;Optical computing;Optical fiber networks;Network architecture;Optical imaging;Search problems","feature extraction;geophysical image processing;image classification;image fusion;neural nets;radar imaging;remote sensing;sensor fusion;synthetic aperture radar","different streams;manually selective optical SAR features;suboptimal fusion features;good fusion architectures;network architecture search;multimodal fusion architecture search network;heterogeneous remote sensing;land properties;existing multimodal land cover classification methods;two-streams convolutional neural networks;manually selective layer","","3","","14","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"SAR/optical data fusion for flood detection","A. D'Addabbo; A. Refice; G. Pasquariello; F. Lovergine","CNR-ISSIA, Bari, Italy; CNR-ISSIA, Bari, Italy; CNR-ISSIA, Bari, Italy; CNR-ISSIA, Bari, Italy","2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","7631","7634","In precision flood monitoring it is important to follow the temporal evolution of an event. Often, however, sufficient temporal coverage of events spanning several days can be attained only by recurring to multi-sensor data, due to different acquisition characteristics and schedules of different types of sensors. We present an example of a successful fusion of data coming from both SAR (COSMO-SkyMed stripmap, 3-m resolution) and optical (RapidEye, multispectral, 5 m-resolution) data, covering a flood event in southern Italy. The data fusion is performed through a Bayesian network approach, a reliable means to infer probabilistic information from heterogeneous sources. Results show accordance with independent model-based flood maps reaching accuracies of up to 96%.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7730990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7730990","","Floods;Remote sensing;Image resolution;Optical imaging;Optical sensors","Bayes methods;data acquisition;floods;geophysical image processing;image fusion;remote sensing by radar;synthetic aperture radar;terrain mapping","precision flood monitoring;temporal evolution;temporal coverage;multisensor data;data acquisition characteristics;SAR-optical data fusion;flood event;southern Italy;COSMO-SkyMed stripmap;RapidEye data;multispectral data;Bayesian network approach;probabilistic infomation;heterogeneous sources;independent model-based flood maps;flood detection","","3","","10","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"Fusion of TanDEM-X and Cartosat-1 DEMS using TV-norm regularization and ANN-predicted weights","H. Bagheri; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany; Signal Processing in Earth Observation, Technical University of Munich, Munich, Germany","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","3369","3372","This paper deals with TanDEM-X and Cartosat-1 DEM fusion over urban areas with support of weight maps predicted by an artificial neural network (ANN). Although the TanDEM-X DEM is a global elevation dataset of unprecedented accuracy (following HRTI-3 standard), its quality decreases over urban areas because of artifacts intrinsic to the SAR imaging geometry. DEM fusion techniques can be used to improve the TanDEM-X DEM in problematic areas. In this investigation, Cartosat-1 elevation data were fused with the TanDEM-X DEM by weighted averaging and total variation (TV)-based regularization, resorting to weight maps derived by a specifically trained ANN. The results show that the proposed fusion strategy can significantly improve the final DEM quality.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8127720","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8127720","Data fusion;L1 norm total variation;weight map;Artificial Neural Network;TanDEM-X DEM;Cartosat-1 DEM","Urban areas;Training;Measurement;Neural networks;Buildings;Laser radar;Remote sensing","digital elevation models;geophysical image processing;image fusion;neural nets;remote sensing by radar;synthetic aperture radar","Cartosat-1 DEM fusion;urban areas;artificial neural network;fusion techniques;Cartosat-1 elevation data;DEM quality;TanDEM-X DEM fusion;TV-norm regularization;ANN-predicted weights;SAR imaging geometry;total variation-based regularization","","3","","12","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Dilated Residual Convolutional Neural Networks for Low-Dose CT Image Denoising","N. Thanh Trung; D. -H. Trinh; N. Linh Trung; T. Thi Thuy Quynh; M. -H. Luu","University of Information and Communication Technology, Thai Nguyen University, Thai Nguyen, Vietnam; VIBOT ERL CNRS 6000 / ImViA, University of Bourgogne, France; VNU University of Engineering and Technology, Vietnam National University, Hanoi, Vietnam; VNU University of Engineering and Technology, Vietnam National University, Hanoi, Vietnam; VNU University of Engineering and Technology, Vietnam National University, Hanoi, Vietnam","2020 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)","29 Dec 2020","2020","","","189","192","X-ray computed tomography (CT) imaging, which uses X-ray to acquire image data, is widely used in medicine. High X-ray doses may be harmful to the patient's health. Therefore, X-ray doses are often reduced at the expense of reduced quality of CT images. This paper presents a convolutional neural network model for low-dose CT image denoising, inspired by a recently introduced dialated residual network for despeckling of synthetic aparture radar images (SAR-DRN). In particular, batch normalization is added to some layers of SAR-DRN in order to adapt SAR-DRN for low-dose CT denoising. In addition, a preprocessing layer and a post-processing one are added in order to improve the receptive field and to reduce computational time. Moreover, the perceptual loss combined with MSE one are used in the training phase so that the proposed denoising model can preserve more subtle details of denoised images. Experimental results show that the proposed model can denoise low-dose CT images efficiently as compared to some state-of-the-art methods.","","978-1-7281-9396-0","10.1109/APCCAS50809.2020.9301693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301693","Computer tomography;low dose imaging;medical image denoising;dilated residual network;convolutional neural network;perceptual loss","Computed tomography;Noise reduction;Training;Image denoising;X-ray imaging;Residual neural networks;Image reconstruction","computerised tomography;image classification;image colour analysis;image denoising;image fusion;medical image processing;neural nets;radar imaging;radial basis function networks;remote sensing by radar;synthetic aperture radar;transforms","dilated residual convolutional neural networks;low-dose CT image denoising;X-ray computed tomography imaging;image data;high X-ray doses;convolutional neural network model;residual network;synthetic aparture radar images;SAR-DRN;low-dose CT denoising;denoised images;low-dose CT images;MSE","","1","","14","IEEE","29 Dec 2020","","","IEEE","IEEE Conferences"
"Development of Method for Change Detection Based on Information Fusion for PALSAR-2 Data","A. Jain; D. Singh","Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Roorkee, India; Department of Electronics and Communication Engineering, Indian Institute of Technology Roorkee, Roorkee, India","2019 URSI Asia-Pacific Radio Science Conference (AP-RASC)","20 Jun 2019","2019","","","1","4","This paper presents an approach for change detection which helps in detecting change information to achieve final change map. The different techniques are available to generate difference image but there is a need to explore a technique for threshold detection. Therefore, the work presented in this paper explored Expectation Maximization (EM) algorithm which is used for threshold selection for change detection map. Further, information fusion is performed in the approach to achieve resultant fused change map. The proposed algorithm is implemented on fully polarimetric PALSAR-2 data.","","978-908-25987-5-9","10.23919/URSIAP-RASC.2019.8738679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8738679","","Change detection algorithms;Data integration","expectation-maximisation algorithm;geophysical image processing;image fusion;radar polarimetry;remote sensing by radar;synthetic aperture radar","fused change map;Expectation Maximization;fully polarimetric PALSAR-2 data;change detection map;threshold detection;final change map;change information;information fusion","","1","","14","","20 Jun 2019","","","IEEE","IEEE Conferences"
"A Novel Fusion Framework without Pooling for Noisy SAR Image Classification","J. Zhao; J. Yang; Z. Yuan; Q. Lin","School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","3531","3536","Due to the particularity of SAR image, existing SAR image classification models often lack strong robustness against noise. Moreover, SAR images are naturally prone to speckle noise and sensitive to observed azimuth. To solve these problems, in this paper, we propose a novel fusion framework in which the convolutional layer with increased stride is used to replace the max pooling layer. Unlike max pooling layer roughly extracts the maximum pixel value in one region as its main feature, which is easy to introduce noise, convolution operation can update the weights and learn features more rationally by back-propagation. It also can achieve the same purpose of down sampling as pooling layers. In order to make full use of feature maps from different layers, our framework fuses the feature vectors extracted from different layers, which helps improve the performance of our classification model. For the problem of overfitting caused by the small MSTAR dataset of SAR images, we replace fully connected layers with convolution layers to relieve the overfitting of the convolution layers by reducing the number of parameters. In order to improve the robustness against observed azimuth angles of the dataset, we adopt the multi-channel calibration and superposition as model’s input, which can be used in real flight platform. The extensive experiments conducted on the MSTAR dataset have clearly demonstrated that our framework achieves higher classification accuracy, stronger robustness against noise than other existing methods, as well as its excellent classification performance for the targets of the same category and different subcategories, which is more difficult to be classified.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9282977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282977","Fusion network;No pooling;Multi-channel","Convolution;Azimuth;Speckle;Radar imaging;Feature extraction;Robustness;Radar polarimetry","feature extraction;image classification;image fusion;radar imaging;synthetic aperture radar","azimuth angles;convolution layers;MSTAR dataset;classification model;feature vectors;feature maps;pooling layers;convolution operation;noisy SAR image classification;fusion framework","","1","","15","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"Earthquake-Induced Building Damage Assessment on SAR Multi-Texture Feature Fusion","Y. Du; L. Gong; Q. Li; F. Wu","Institute of Crustal Dynamics, China Earthquake Administration, Beijing, P R China; Institute of Crustal Dynamics, China Earthquake Administration, Beijing, P R China; Institute of Crustal Dynamics, China Earthquake Administration, Beijing, P R China; Institute of Remote Sensing and Digital Earth, Chinese Academy of Sciences, Beijing, China","IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium","17 Feb 2021","2020","","","6608","6610","Multi-temporal RS data is not often available in time after an earthquake, so it is useful to assess the building situation with a single post-event SAR. Aiming at the problem that the single texture feature extraction method has inadequate information in the collapsed buildings classification, this paper proposes a SAR texture feature classification method that combines multiple features. Taking the 2016 Kumamoto earthquake as an example, the four methods based on gray histogram, GLCM, LBP, and Gabor filter are used to extract texture features and fused, then random forest classification is applied to obtain the collapse information of earthquake-damaged buildings. In addition, it is compared with the classification results of 26 texture features after principal component analysis. The results of two sets of experiments show that the extraction accuracy based on multi-feature fusion is higher than that of a single texture feature extraction method, and the multi-feature fusion classification result after principal component analysis improves the accuracy while improving the recognition efficiency.","2153-7003","978-1-7281-6374-1","10.1109/IGARSS39084.2020.9323644","China Earthquake Administration(grant numbers:ZDJ-2018-14,ZDJ-2019-32); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9323644","Earthquake;Building damage assessment;SAR;Multi-texture feature","Feature extraction;Buildings;Principal component analysis;Earthquakes;Gabor filters;Histograms;Data mining","earthquake engineering;earthquakes;feature extraction;geophysical image processing;image classification;image fusion;image texture;synthetic aperture radar","earthquake-induced building damage assessment;SAR multitexture feature;multitemporal RS data;single post-event SAR;single texture feature extraction method;collapsed buildings classification;SAR texture feature classification method;Kumamoto earthquake;random forest classification;earthquake-damaged buildings;texture features;multifeature fusion classification result;AD 2016","","1","","6","IEEE","17 Feb 2021","","","IEEE","IEEE Conferences"
"Building Detection Using Very High Resolution SAR Images With Multi-Direction Based on Weighted-Morphological Indexes","F. Amjadipour; H. Ghassemian; M. Imani","Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran; Image Processing and Information Analysis Lab, Tarbiat Modares University, Tehran, Iran","2022 International Conference on Machine Vision and Image Processing (MVIP)","22 Mar 2022","2022","","","1","6","Today, technological advancement in production of radar images can be seen with high spatial resolution and also the availability of these images’ significant growth in interpretation and processing of high-resolution radar images. The building extraction from urban areas is one of the most challenging applications in VHR SAR image, which is used to estimate the population and urban development. Detection of individual buildings in the urban context is highly considered by researchers due to complexity of interpreting radar images in these fields. On the other hand, one of the main issues in the complexity of the scatters received from buildings is change in direction of the building relative to the horizon, which is correlated with the look angle. Other influential parameters are geometric distortions, which include layover and shadow effects. In some cases, the effect of shadow is an auxiliary parameter in detection of these targets that increases accuracy of the detection. In this paper, we intend to extract the building from high spatial resolution SAR images using fuzzy fusion of two morphological indicators, SI and DI, which represent the shadow and bright area, respectively. Due to the effect of SAR imaging geometry on ground targets, different sizes and directions of structural elements were applied to the image. The use of indicators weights with different sizes is proposed in this work. The Detection Ratio of experiment of TerraSAR-X image has a result of 95.3%.","2166-6784","978-1-6654-1216-2","10.1109/MVIP53647.2022.9738776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9738776","Building extraction;top-hat transform;morphology index;building orientation;fuzzy system","Geometry;Buildings;Urban areas;Radar;Radar imaging;Radar polarimetry;Complexity theory","fuzzy set theory;geophysical image processing;image fusion;image resolution;radar imaging;remote sensing by radar;synthetic aperture radar","high spatial resolution SAR images;SAR imaging geometry;building detection;weighted-morphological indexes;high-resolution radar images;building extraction;urban areas;VHR SAR image;urban development;layover;shadow effects","","1","","19","IEEE","22 Mar 2022","","","IEEE","IEEE Conferences"
"Research on fusion strategy of ascending and descending multi-baseline multi-frequency insar results to generate high quality DEM","Z. Qiming; Z. Xiaojie; J. Jian","Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China; Institute of Remote Sensing and Geographic Information System, Peking University, Beijing, China","2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","4 Dec 2017","2017","","","5685","5688","Compared with DEM generated from single frequency or single baseline repeat pass inteferometric SAR data, the quality of DEM fused with multi-baseline multi-frequency InSAR has been improved effectively. However, the quality of fused DEM by using same orbital and illuminating direction has still been constrained by some factors, among of which the layover and shadow effects are the most important ones, especially in areas where topography reliefs change rapidly. In this paper, we proposed an innovation method to fuse ascending and descending multi-baseline multi-frequency InSAR results to generate higher quality DEMs, which included how to access the effective information in the DEMs generated by using ascending and descending interferometric data, and to complement each other to eliminate potential deficient in layover and shadow areas. The key point of the fusion strategy was to set fusion weight for each resolution element according to the reliabilities, effective mean coherences and resolution ratios of ascending and descending DEMs to be fused. Experiments with real SAR data proved that the proposed method could improve the DEM quality effectively.","2153-7003","978-1-5090-4951-6","10.1109/IGARSS.2017.8128298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8128298","Fusion;Ascending;Descending;DEM;InSAR","Indexes;5G mobile communication;Azimuth;Economic indicators","digital elevation models;geophysical image processing;image fusion;radar interferometry;remote sensing by radar;synthetic aperture radar;topography (Earth)","fusion strategy;fused DEM;DEM quality;inteferometric SAR data;multibaseline multifrequency InSAR","","1","","9","IEEE","4 Dec 2017","","","IEEE","IEEE Conferences"
"Multi-source Collaborative Target Classification Based on ISAR and Infrared Image","C. Han; Y. Lu; D. Yang; H. Wang; L. Li; X. Wang","Xi'an Research Institute of Navigation Technology, Shaanixi, China; Xi'an Research Institute of Navigation Technology Xi'an, Shaanixi, China; Xi'an Research Institute of Navigation Technology Xi'an, Shaanixi, China; National Key Laboratory of Aerospace Flight Dynamics School of Astronautics Northwestern Polytechnical University Xi'an, Shaanixi, China; National Key Laboratory of Aerospace Flight Dynamics School of Astronautics Northwestern Polytechnical University Xi'an, Shaanixi, China; National Key Laboratory of Aerospace Flight Dynamics School of Astronautics Northwestern Polytechnical University Xi'an, Shaanixi, China","2021 6th International Conference on Image, Vision and Computing (ICIVC)","14 Sep 2021","2021","","","149","153","With the development of information technology, in order to improve the accuracy of target recognition, information collaboration technology plays an increasingly important role. However, cooperating with ISAR images and infrared images for target recognition is invisible. In this paper, we propose for the first time to fuse the sample feature information of ISAR images and infrared images for target recognition. Initially, the same preprocessing and feature extraction are performed on ISAR and infrared images respectively. Subsequently, their own features are weighted by the weight coefficient to complete the feature-level fusion. Finally, BP neural network is used for image classification. The experimental results confirm that the multi-source collaborative classification has higher accuracy than single-source.","","978-1-6654-4368-5","10.1109/ICIVC52351.2021.9526971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9526971","information collaboration;ISAR and infrared images;feature-level fusion;target recognition","Image recognition;Target recognition;Fuses;Neural networks;Collaboration;Feature extraction;Information technology","backpropagation;feature extraction;groupware;image classification;image fusion;infrared imaging;neural nets;radar imaging;radar target recognition;synthetic aperture radar;target tracking","infrared image;information technology;target recognition;information collaboration technology;ISAR images;sample feature information;image classification;multisource collaborative classification;multisource collaborative target classification","","","","30","IEEE","14 Sep 2021","","","IEEE","IEEE Conferences"
"Forward-looking InSAR image pixel-level fusing for unmanned ground vehicle unconstructed field perception","Q. Song; J. Wang; Y. Li; Z. Zhou","College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China","2016 Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)","1 Sep 2016","2016","","","179","183","We have present a forward-looking InSAR sensor for Unmanned Ground Vehicle (UGV) to sense the complex unconstructed roads which are challenge environments face by UGVs. The forward-looking InSAR can provide a scattering image, a coherence image and a Digital Terrain Model (DTM) of the same scene ahead the radar during each scan. Each type of image can highlight some unique features of an obstacle and the features are complementary. In the scattering image, a positive obstacle features a strong leading edge and a weak shadow; while a negative obstacle features a weak body and a strong trailing edge. In the coherence image, the weak areas are emphasized and are easy to be segmented. In the DTM, the height profiles of the visible parts of the positive and negative obstacle can be extracted for target discrimination. Pixel-level image fusing results have demonstrated the potential to boost topography mapping fidelity and to improve discrimination performance between positive and negative obstacles.","","978-1-5090-1362-3","10.1109/ACIRS.2016.7556209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7556209","unmanned ground vehicle;InSAR;positive obstcleg;negative obstacle;image fusing","Coherence;Radar imaging;Spaceborne radar;Radar scattering;Antenna arrays","collision avoidance;image fusion;radar imaging;remotely operated vehicles;synthetic aperture radar","forward-looking InSAR image pixel-level fusing;unmanned ground vehicle;unconstructed field perception;UGV;coherence image;digital terrain model;DTM;negative obstacle features","","","","18","IEEE","1 Sep 2016","","","IEEE","IEEE Conferences"
"Optical SAR Fusion of Sentinel-2 Images for Mapping High Resolution Land Cover","Yuhendra; E. Yulianti; J. Na'am","Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia; Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia; Informatics Engineering Department, Padang Institute of Technology, Padang-West Sumatera, Indonesia","2018 International Conference on System Science and Engineering (ICSSE)","4 Nov 2018","2018","","","1","4","Sentinel-2 is a very new programme of the European Space Agency (ESA) that is designed for fine spatial resolution global monitoring. Land cover-land use (LCLU) classification tasks can take advantage of the fusion of radar and optical remote sensing data, leading generally to increase mapping accuracy. Here we propose a methodological approach to fuse information from the new European Space Agency Sentinel-1 and Sentinel-2 imagery for accurate land cover mapping of a portion of the South Solok region, West Sumatera. Data pre-processing was carried out using the European Space Agency's Sentinel Application Platform and the SEN2COR toolboxes. The two main objectives of this study are to evaluate the potential use and synergetic effects of ESA Sentinel-1A C-band SAR and Sentinel-2A Optical data for classification and mapping of LCLU. As a result of the research, two main advantages. First, the pre-processing chain supported by sensor-specific toolboxes developed by ESA represents a reliable and fast approach for the preparation of ready-to-process imagery. Second, investigation to derive a methodological framework to integrate Sentinel-1 and Sentinel-2 imagery for land cover mapping by integrating of radar and optical imagery have been set up and tested.","2325-0925","978-1-5386-6285-4","10.1109/ICSSE.2018.8520099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520099","Sentinel-1;Sentinel-2;SAR;land cover mapping;data fusion;segmentation;South Solok","Remote sensing;Optical imaging;Spatial resolution;Laser radar;Image segmentation;Optical sensors;Earth","geophysical image processing;geophysical techniques;image classification;image fusion;land cover;radar imaging;remote sensing by radar;synthetic aperture radar;terrain mapping","land cover mapping;South Solok region;West Sumatera;optical imagery;ready-to-process imagery;pre-processing chain;Optical data;Sentinel-2A;ESA Sentinel-1A;SEN2COR toolboxes;European Space Agency's Sentinel Application Platform;data pre-processing;Sentinel-2 imagery;European Space Agency Sentinel-1;mapping accuracy;optical remote sensing data;land cover-land use classification tasks;fine spatial resolution global monitoring;mapping high resolution land cover;Optical SAR fusion","","","","6","IEEE","4 Nov 2018","","","IEEE","IEEE Conferences"
"HFGAN: A Heterogeneous Fusion Generative Adversarial Network for Sar-to-Optical Image Translation","N. Yu; A. Ma; Y. Zhong; X. Gong","State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; State Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University, Wuhan, China; Beijing Institute of Remote Sensing Information, Beijing, China","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","2864","2867","Due to the influence of the imaging mechanism of SAR images, it is difficult to interpret ground information through SAR images without expert knowledge. On the contrary, optical images have rich spatial and color information, so it is necessary to conduct research on the translation of SAR to optical remote sensing images. In this end, we propose a heterogeneous fusion generative adversarial network (HFGAN) for SAR-to-optical image translation. There are two main improvements: (1) Complementary generation of global structure and texture information. A heterogeneous fusion generator and a multi-scale discriminator are proposed to ensure that the global and detailed features of the generated image are more accurate and rich. (2) Color fidelity. Chromatic aberration loss are introduced to reduce the color difference between the generated image and the real optical image. Through qualitative and quantitative experiments, it is proved that the proposed method not only obtains better visual effects, but also has certain progress in the evaluation metrics, which proves that the proposed method is superior to the previous advanced methods in SAR-to-optical image translation.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883519","Remote sensing;Generative Adversarial Network;Image tranlation","Optical losses;Image color analysis;Optical imaging;Generative adversarial networks;Visual effects;Generators;Radar polarimetry","aberrations;geophysical image processing;image classification;image colour analysis;image fusion;image resolution;image texture;optical images;radar imaging;remote sensing;synthetic aperture radar","optical remote sensing images;heterogeneous fusion generative adversarial network;sar-to-optical image translation;heterogeneous fusion generator;imaging mechanism;SAR images;rich spatial;color information","","","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"A fusion and target detection method based on SAR and optical images","X. Xie; Y. Wang; X. Tuo; Y. Zhang; X. Zhao; J. Pei","University of Electronic Science and Technology of China, China; University of Electronic Science and Technology of China, China; University of Electronic Science and Technology of China, China; University of Electronic Science and Technology of China, China; University of Electronic Science and Technology of China, China; University of Electronic Science and Technology of China, China","2019 International Conference on Control, Automation and Information Sciences (ICCAIS)","23 Apr 2020","2019","","","1","6","How to effectively and accurately detect target information in the formed images has long been an area that we attach great importance to. For example, we not only need the imaging results not to be affected by the weather and other environments, but also hope that the edge information of the imaging results is very clear and the image resolution is very high. Therefore, a single sensor cannot achieve this effect at the same time. In this case, the fusion rule (nonsubsampled contourlet transform) proposed in this paper solves the problem of insufficient imaging information of a single sensor and removes redundant information. The target is pre-screened to make the target area more accurate. Through spatial morphological filtering, false alarm rate is reduced to improve the success rate of target matching.","2475-7896","978-1-7281-2311-0","10.1109/ICCAIS46528.2019.9074687","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9074687","","Optical imaging;Integrated optics;Redundancy;Stability criteria;Automation","image filtering;image fusion;image matching;image resolution;object detection;radar detection;radar imaging;radar receivers;radar resolution;synthetic aperture radar;transforms","optical images;edge information;image resolution;fusion rule;imaging information;target matching;target detection method;SAR;spatial morphological filtering;false alarm rate;nonsubsampled contourlet transform","","","","4","IEEE","23 Apr 2020","","","IEEE","IEEE Conferences"
"A Remote Sensing Image Registration Method Based on Multi-features","K. Xie; J. Chen; M. Yang","Guangxi Key Laboratory of Trusted Software of Guilin, University of Electronic Technology, Guilin, China; Guangxi Key Laboratory of Cryptography and Information Security of Guilin, University of Electronic Technology, Guilin, China; Key Laboratory of Pattern Recognition, Chinese academy of sciences, Beijing, China","2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC)","6 Feb 2020","2019","","","134","138","Remote sensing image registration technology has important significance in the field of image processing. The registration technology of SAR images has been a hot research in this field, and it is also a challenge. Aiming at this difficulty, this paper proposes a fusion feature algorithm, which combines gradient, grayscale, texture, geometry and other features in heterogeneous remote sensing images for comprehensive matching. Compared with the traditional single feature, the improved detector has good performance and registration accuracy in SAR image and visible light image registration.","","978-1-7281-2325-7","10.1109/ICIVC47709.2019.8981069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8981069","remote sensing image registration;fusion features;SAR images;visible light images","","feature extraction;geophysical image processing;image fusion;image registration;remote sensing;synthetic aperture radar","remote sensing image registration method;multifeatures;sensing image registration technology;important significance;image processing;SAR images;hot research;fusion feature algorithm;geometry;heterogeneous remote sensing images;traditional single feature;registration accuracy;visible light image registration","","","","9","IEEE","6 Feb 2020","","","IEEE","IEEE Conferences"
"Road Detection in High-resolution SAR Images with Improved Multiple Feature Fusion","J. Chen; Z. Ding; Y. Wei; Q. Gao; Y. Li","Radar Research Lab, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Radar Research Lab, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Radar Research Lab, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Radar Research Lab, School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Radar Research Lab, School of Information and Electronics, Beijing Institute of Technology, Beijing, China","2019 International Radar Conference (RADAR)","27 Apr 2020","2019","","","1","6","In this paper, we propose a novel method for road region detection in high-resolution SAR images based on the fusion of multiple features. Compared with traditional SAR road detection methods with feature fusion, we exploit more useful features such as the standard deviation of directional radiance for distinguishing between roads and buildings or flatland. Then, the features are binarized with dynamic thresholds related to the cumulative possibility distribution of features. Finally, we define a membership parameter to fuse the binarized features and select the road candidate regions according to their geometric features, thereby ensuring better detection rate and lower false alarm rate. Experimental results of GF-3 SAR images show the effectiveness of the proposed method in the detection of both urban and suburban road regions.","2640-7736","978-1-7281-2660-9","10.1109/RADAR41533.2019.171332","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079137","road region detection;multiple features;dynamic thresholds;GF-3 SAR images","","image fusion;image segmentation;object detection;radar imaging;synthetic aperture radar","road region detection;high-resolution SAR images;traditional SAR road detection methods;buildings;binarized features;road candidate regions;geometric features;detection rate;GF-3 SAR images;urban road regions;suburban road regions;improved multiple feature fusion;cumulative possibility distribution","","","","8","IEEE","27 Apr 2020","","","IEEE","IEEE Conferences"
"Multi-frequency and multi-resolution EO images for Smart Asset Management","A. Brunetti; M. Gaeta; P. Mazzanti","NHAZCA S.r.l., startup of “Sapienza” University of Rome, Rome, Italy; NHAZCA S.r.l., startup of “Sapienza” University of Rome, Rome, Italy; NHAZCA S.r.l., startup of “Sapienza” University of Rome, Rome, Italy","IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium","28 Sep 2022","2022","","","5192","5195","This paper describes some of the data fusion methods developed by NHAZCA S.r.l. in the frame of the project “MUSAR”, funded by ASI, for the integration of data from multi-sensor/multiband satellite images. The aim of MUSAR is to extend the exploitation of EO data in the research area of natural hazard, with a specific focus on their interaction and interference with structures and infrastructures. The proposed methods are based on the post-processing of results achieved from InSAR and A-DInSAR analyses and by Photomonitoring techniques. Some preliminary results on real case studies are also presented.","2153-7003","978-1-6654-2792-0","10.1109/IGARSS46834.2022.9883325","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9883325","","Solid modeling;Three-dimensional displays;Satellites;Space missions;Licenses;Asset management;Pressure measurement","asset management;geophysical image processing;geophysical techniques;image fusion;radar imaging;radar interferometry;synthetic aperture radar","photomonitoring techniques;A-DInSAR analysis;multisensor satellite images;multiband satellite images;MUSAR;data fusion methods;smart asset management;multiresolution EO images;multifrequency;interference;natural hazard;EO data;ASI","","","","6","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Multimodal and Multitemporal Spatial Data Analysis in Google Earth Engine Cloud Computing Platform to Detect Human Settlements Without Electricity: A Case Study of Bangalore City","M. B. Ujjinakoppa; U. Kumar; R. Thottolil; A. Dasgupta","Samsung R&D Institute, Bangalore, India; Spatial Computing Laboratory, Center for Data Sciences, International Institute of Information Technology Bangalore (IIITB), Bangalore, India; Spatial Computing Laboratory, Center for Data Sciences, International Institute of Information Technology Bangalore (IIITB), Bangalore, India; Spatial Computing Laboratory, Center for Data Sciences, International Institute of Information Technology Bangalore (IIITB), Bangalore, India","2021 IEEE International India Geoscience and Remote Sensing Symposium (InGARSS)","13 Jun 2022","2021","","","238","241","Accurate information about the human settlements without electricity is essential for monitoring the areas deprived of access to electricity and to end the darkness. Motivated by the 2021 IEEE GRSS Data Fusion Contest organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society (GRSS), the objective of this research was to assess the human settlements without electricity for areas in and around Bangalore City. We used multimodal and multitemporal data of the year 2019 with 27 layers such as Landsat-8 OLI bands, Sentinel-1 C Band (SAR data) with VV and VH polarization, spectral indices (EVI, NDVI, MNDWI, NDBI, NDMI, BSI, SAVI, IBI, BuEI and SoEI), Texture parameters (DISS, Entropy and Angular Second Moment), Topological data (slope and elevation), and land surface temperature to detect land use map with urban builtup, vegetation, water and barren land classes with a spatial resolution of 30 m using object-based Random Forest algorithm. To overcome the computational limitations, all the analyses were carried out in Google Earth Engine (GEE) cloud-based platform that has planetary-scale analysis capabilities. Overall, 39 experiments on classification were carried out with various combinations of feature vectors to obtain the most accurate land use map with 4 classes. Composite of Landsat bands and advantages of other spectral indices with thresholds rendered the highest classification accuracy of 93.49%. The final mapping results of human settlements without electricity was obtained by comparing binary classified maps with resampled VIIRS night-time light imagery. The results revealed that the total area of human settlements without electricity in Bangalore City is approximately 36.57 sq. km. accounting for 6.2% of the total study area.","","978-1-6654-4249-7","10.1109/InGARSS51564.2021.9792041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792041","Urban builtup;Google Earth Engine;multimodal;multitemporal;remote sensing;machine learning","Earth;Cloud computing;Temperature distribution;Artificial satellites;Urban areas;Data integration;Vegetation mapping","cloud computing;data analysis;geophysical image processing;geophysical signal processing;geophysical techniques;image classification;image fusion;land surface temperature;remote sensing;sensor fusion;synthetic aperture radar;terrain mapping","Google Earth Engine cloud computing platform;Bangalore City;2021 IEEE GRSS Data Fusion Contest;Image Analysis;Data Fusion Technical Committee;multimodal data;multitemporal data;Landsat-8 OLI bands;SAR data;spectral indices;land surface temperature;barren land classes;object-based Random Forest algorithm;Google Earth Engine cloud-based platform;planetary-scale analysis capabilities;land use map;Landsat bands","","","","7","IEEE","13 Jun 2022","","","IEEE","IEEE Conferences"
"Design and implementation of multi-feature fusion kernel correlation filtering algorithm based on HLS","P. Cong; M. Xie; K. Yang; X. Zhang; H. Su; X. Fu","School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China; Science and Technology on Millimeter-wave Laboratory, Beijing Institute of Remote-sensing Equipment, Beijing, China; Science and Technology on Millimeter-wave Laboratory, Beijing Institute of Remote-sensing Equipment, Beijing, China; School of Information and Electronics, Beijing Institute of Technology, Beijing, China","IET International Radar Conference (IET IRC 2020)","22 Sep 2021","2020","2020","","645","649","In order to balance the accuracy and real-time performance of the moving target tracking system, an optimized design and implementation method based on high-level synthesis (HLS) of multi-feature fusion with kernel correlation filtering algorithms on FPGA is designed. This design improves the KCF algorithm with LBP and HOG features, and proposes a new dimensionality reduction method for LBP, which enhances the real-time performance while maintaining effective extraction of target features. The algorithm is implemented with FPGA, and a well acceleration effect is obtained on the basis of high precision. In test, the frame rate reaches 35 frames per second. Finally, it is verified through simulation that this feature extraction method can be used to process various image data such as infrared detection and SAR radar imaging, and has a wide range of applications.","","","10.1049/icp.2021.0761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9545666","","","feature extraction;field programmable gate arrays;filtering theory;image fusion;object detection;radar imaging;synthetic aperture radar;target tracking","multifeature fusion kernel correlation;HLS;real-time performance;moving target tracking system;optimized design;high-level synthesis;FPGA;KCF algorithm;LBP;HOG features;dimensionality reduction method;effective extraction;target features;acceleration effect;feature extraction method","","","","","","22 Sep 2021","","","IET","IET Conferences"
"Multistatic Radar Fusion imaging based on Beidou navigation signal","X. Peng; X. Li; D. Ding; R. Chen","School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China","2020 Cross Strait Radio Science & Wireless Technology Conference (CSRSWTC)","11 Mar 2021","2020","","","1","2","This paper studies the multistatic radar fusion imaging based on Beidou navigation signal. For Beidou navigation system, the transmitter and receiver are represented by different navigation satellites, forming a dual-station radar system. Due to the sparseness of the target signal received by the radar, based on the two-dimensional radar scatter echo sparse representation model and data fusion as a means, the bistatic radar can be expanded into a multistatic radar to broaden the frequency band and viewing angle range And then improve the range and azimuth resolution.","","978-1-7281-8181-3","10.1109/CSRSWTC50769.2020.9372610","Natural Science(grant numbers:61890541,61731001,61871443); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372610","multistatic radar;navigation signal;fusion imaging","Image resolution;Imaging;Radar imaging;Radar scattering;Satellite navigation systems;Multistatic radar;Signal resolution","image fusion;radar cross-sections;radar imaging;remote sensing by radar;satellite navigation;synthetic aperture radar","multistatic radar fusion imaging;Beidou navigation signal;Beidou navigation system;different navigation satellites;dual-station radar system;target signal;two-dimensional radar scatter;sparse representation model;data fusion;bistatic radar","","","","3","IEEE","11 Mar 2021","","","IEEE","IEEE Conferences"
"Texture and Intensity Fusion based SAR Image Change Detection","J. Wu; L. Yu; M. Liu; M. Ma","School of Computer Science Shaanxi Normal University, Xi'an, China; School of Computer Science Shaanxi Normal University, Xi'an, China; School of Computer Science Shaanxi Normal University, Xi'an, China; School of Computer Science Shaanxi Normal University, Xi'an, China","2021 SAR in Big Data Era (BIGSARDATA)","26 Oct 2021","2021","","","1","4","Due to the coherent imaging method of the SAR system, SAR images are always accompanied by multiplicative speckle. Therefore, it makes the change detection based on SAR images are challenging. Aiming at the problem of the construction and the analysis of the difference image in SAR change detection, we propose a new SAR image change detection algorithm. In this method, the texture information and intensity information of difference image are fused adaptively according to the Bayesian formula and the local consistency evaluation. In the experiments, two real SAR image sets are used for performance analysis, from which the proposed method shows a better performance.","","978-1-6654-0123-4","10.1109/BIGSARDATA53212.2021.9574306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9574306","SAR imagery;Bayesian;Local consistency;Iterative fusion","Imaging;Speckle;Big Data;Radar polarimetry;Performance analysis;Bayes methods;Iterative methods","geophysical image processing;image fusion;image texture;object detection;radar imaging;speckle;synthetic aperture radar","SAR image sets;coherent imaging method;SAR system;SAR images;difference image;SAR change detection;SAR image change detection algorithm;texture information;intensity information","","","","13","IEEE","26 Oct 2021","","","IEEE","IEEE Conferences"
"Multitemporal Sar And Map Fusion For Extracting Persitent Scatterers On Roads","T. Tanaka; D. Ikefuji; O. Hoshuyama","Data Science Research Laboratories, NEC Corporation; Data Science Research Laboratories, NEC Corporation; Data Science Research Laboratories, NEC Corporation","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","182","185","This paper proposes a fusion method of multitemporal SAR images and a map to extract persistent scatterers (PSs) on roads, which are special SAR pixels used to monitor millimetric displacement of roads. The proposed method evaluates phase correlation of PSs in addition to shape similarity between their distribution in the SAR image and corresponding roads in the map. Since uncorrelated phase implies that PSs are on different objects, which have different heights and displacements, the proposed method can extract PSs matching to the road shape while excluding those on neighboring buildings even under severe distortion caused by layover. Evaluation results using real SAR images and map show that the proposed fusion method can find PSs corresponding to roads in an urban area highly affected by layover.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899061","PS;InSAR;Road Extraction,","Roads;Correlation;Shape;Radar polarimetry;Feature extraction;Data mining;Monitoring","image fusion;radar imaging;radar interferometry;remote sensing by radar;roads;synthetic aperture radar","persitent scatterers;fusion method;multitemporal SAR images;PSs;special SAR pixels;millimetric displacement;phase correlation;uncorrelated phase;road shape;urban area","","","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Small ship detection in SAR images based on modified SSD","L. Han; W. Ye; J. Li; D. Ran","Space Engineering University, Beijing, China; Space Engineering University, Beijing, China; Naval Aeronautical University, Yantai, China; Space Engineering University, Beijing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","Small object detection is a nodus for CNNs (Convolutional Neural Networks) that take advantages of feature pyramid for object detection, and it is also a hotspot which researchers focus on, especially small ship detection in SAR images. Based on SSD (Single Shot Multibox Detector), which is a classical CNN detector and a typical one-stage method for object detection, and inspired by the ideas of object enhancement and feature fusion, the paper uses deconvolution to enhance representation of small object in feature pyramid. Thereby we expect to improve detecting precision of SSD. The feasibility and effectiveness of proposed method are proved by our experiments. Experimental results on SSDD show that the proposed method greatly improves detecting precision of small ships in SAR images.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173268","small ship detection;deconvolution;SSD;SAR image","","convolutional neural nets;feature extraction;image enhancement;image fusion;object detection;radar imaging;ships;synthetic aperture radar","small ship detection;SAR images;modified SSD;object detection;convolutional neural networks;feature pyramid;single shot multibox detector;CNN detector;object enhancement;feature fusion","","","","8","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Unsupervised Image Change Detection Based on Ground-Based Imaging Radar","H. Ren; P. Huang; W. Tan; W. Xu; F. Liu; M. Zhang","Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; Inner Mongolia Key Laboratory of Radar Technology and Application, Hohhot, China; China Geological Environment Monitoring Institute, Beijing, China","2019 IEEE International Conference on Signal, Information and Data Processing (ICSIDP)","21 Aug 2020","2019","","","1","5","Multitemporal ground-based imaging radar images have been successfully used for the detection of different types of terrain changes. The image change detection of the ground-based imaging radar is used to estimate the interference change information in the monitoring area within a short time baselines to avoid misjudging the radar deformation image. Many scholars have proposed many effective detection methods for spaceborne radar images, but there are few researches on the change detection method of ground-based imaging radar images. In this paper, a change detection method for ground-based imaging radar image is proposed. To obtain the change information of ground-based imaging radar, the difference map and coherence coefficient map were used to obtain the change region. Experiment results show that the proposed method is effective for ground-based imaging radar image change detection in terms of smooth the background noise and enhance the contrast between the changed and unchanged portions.","","978-1-7281-2345-5","10.1109/ICSIDP47821.2019.9173172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173172","Ground-based imaging radar;Change detection;Gaussian operators;Coherence coefficient;K-means","","geophysical image processing;geophysical techniques;image fusion;radar imaging;remote sensing by radar;spaceborne radar;synthetic aperture radar","unsupervised image change detection;ground-based imaging radar;multitemporal ground-based;radar deformation image;spaceborne radar images;change detection method","","","","15","IEEE","21 Aug 2020","","","IEEE","IEEE Conferences"
"Multi-scale Features Fusion Network for Unsupervised Change Detection in Heterogeneous Optical and SAR Images","J. Shi; Z. Zhang; T. Wu; X. Li; D. Zhou; Y. Lei","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, Shaanxi, China","2021 IEEE 7th International Conference on Cloud Computing and Intelligent Systems (CCIS)","14 Apr 2022","2021","","","270","274","Change detection (CD) in heterogeneous remote sensing image applications has become an issue of increasing concern in, as they cannot be compared directly with traditional homogenous CD methods. To solve feature loss problem and generating better representations to accommodate regions of various sizes in heterogeneous images CD, a multi-scale features fusion network (MFFN) is proposed. Firstly, multi-scale representative deep features can be extracted to distinguish difference in high-dimension feature space. Then, hierarchical features from the original image pairs can be fuse to generate a difference image with more explicit semantic information owing to the strategy of multi-scale features fusion, which can better adapt different scale of changes in heterogeneous remote sensing images. It is noteworthy that the experimental results on both heterogeneous and homogeneous data set confirm the effectiveness of the proposed method.","","978-1-6654-4149-0","10.1109/CCIS53392.2021.9754667","National Natural Science Foundation of China; Natural Science Foundation of Shaanxi Province; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754667","Change detection;Multi-scale feature fusion;Heterogeneous images;Neural network","Semantics;Radar detection;Radar;Radar imaging;Feature extraction;Optical imaging;Radar polarimetry","feature extraction;geophysical image processing;image fusion;image segmentation;radar imaging;remote sensing;synthetic aperture radar","multiscale features fusion network;unsupervised change detection;heterogeneous optical images;sar images;heterogeneous remote sensing image applications;traditional homogenous CD methods;feature loss problem;heterogeneous images CD;multiscale representative deep features;high-dimension feature space;hierarchical features;original image pairs;difference image;heterogeneous remote sensing images;heterogeneous data;homogeneous data","","","","15","IEEE","14 Apr 2022","","","IEEE","IEEE Conferences"
"Data Fusion and Remote Sensing: An ever-growing relationship","M. Schmitt; X. X. Zhu",TUM; TUM,"IEEE Geoscience and Remote Sensing Magazine","16 Dec 2016","2016","4","4","6","23","Characterized by a certain focus on the heavily discussed topic of image fusion in its beginnings, sensor data fusion has played a significant role in the remote sensing research community for a long time. With this article, we aim to provide a short overview of established definitions, targeting a generalized understanding of the topic. In addition, a review of the state of the art of remote sensing data fusion research is given. By bringing together the conventional view expressed in the classical data fusion community and a review of current activities in the field of Earth observation, this article provides a holistic view of generic data fusion concepts and their applicability to the remote sensing domain.","2168-6831","","10.1109/MGRS.2016.2561021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740215","","Data fusion;Remote sensing;Synthetic aperture radar;Data integration;Optical imaging;Laser radar","image fusion;remote sensing","sensor data fusion;image fusion;remote sensing research community;remote sensing data fusion research;classical data fusion community;Earth observation;generic data fusion concepts;remote sensing domain","","161","","185","IEEE","16 Dec 2016","","","IEEE","IEEE Magazines"
"V-NIIRS fusion modeling for EO/IR systems","E. Blasch; B. Kahler","Air Force Research Laboratory, Rome, NY; Leidos, Dayton, OH","2015 National Aerospace and Electronics Conference (NAECON)","31 Mar 2016","2015","","","19","26","The Video National Imagery Interpretability Rating Scale (V-NIIRS) is an emerging standard of the Motion Imagery Standards Board (MISB). V-NIIRS extends NIIRS to from image-based scene characterization to streaming video for image quality assessment of object recognition. To apply V-NIIRS for image fusion, there is a need to understand the operating conditions of the sensor type, environmental phenomenon, and target behavior (SET). In this paper, we explore V-NIIRS as related to resolution, ground sampling distance, and probability of detection, recognition, and identification success. In a modeling analysis, we determine the issues and capabilities of using V-NIIRS video quality ratings to determine task success. Scenarios are provided that allow one to determine the V-NIIRS requirement for a given operational parameter.","2379-2027","978-1-4673-7565-8","10.1109/NAECON.2015.7443033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443033","V-NIIRS;target tracking;performance modeling","Target tracking;Target recognition;Image quality;Vehicles;Synthetic aperture radar;Mathematical model;Image resolution","image fusion;image resolution;object recognition;probability;video signal processing;video streaming","V-NIIRS fusion modeling;EO-IR systems;video national imagery interpretability rating scale;motion imagery standard board;video streaming;image quality assessment;object recognition;image fusion;sensor type operating conditions;environmental phenomenon;target behavior;SET;ground sampling distance;probability of detection;image resolution;identification success;V-NIIRS video quality ratings;image-based scene characterization","","5","","102","IEEE","31 Mar 2016","","","IEEE","IEEE Conferences"
"Landslide Detection Based on Improved YOLOv5 and Satellite Images","T. Wang; M. Liu; H. Zhang; X. Jiang; Y. Huang; X. Jiang","State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China; State Key Laboratory of Geohazard Prevention and Geoenvironment Protection, Chengdu University of Technology, China, Chengdu, China","2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)","4 Oct 2021","2021","","","367","371","Since the 21st century, due to the repeated deterioration of the natural climate and the increasing impact of human production activities on the ecological environment, landslides have become a common high-hazard natural phenomenon. Traditional landslide detection is mainly done through field detection, synthetic aperture radar, and other technologies. With the increase in the accuracy of satellite imagery and the rapid development of deep learning, the use of deep learning to realize landslide detection has gradually become a trend. Our work mainly includes two parts: landslide data set production and model performance improvement. We have produced a complete general landslide dataset based on the high-resolution remote sensing images obtained from open satellites and the annotations of professional researchers. We will publish our dataset later. Based on the research of previous researchers and based on the basic framework of YOLOv5, we improved the feature splicing method of YOLOv5 by adding Adaptively Spatial Feature Fusion (ASFF), and fused feature information of different scales to improve the model. To better mine shallow feature information, we introduced the Convolutional Block Attention Module (CBAM) module to improve the performance of the model. Experiments have proved that our model has a 1.64% performance improvement.","","978-1-6654-1322-0","10.1109/PRAI53619.2021.9551067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551067","landslide detection;YOLOv5;ASFF;Attention mechanism","Deep learning;Landslides;Adaptation models;Satellites;Biological system modeling;Splicing;Production","deep learning (artificial intelligence);feature extraction;geomorphology;geophysical image processing;image fusion;object detection;remote sensing","convolutional block attention module;high-hazard natural phenomenon;YOLOv5 feature splicing method;satellite images;adaptively spatial feature fusion;high-resolution remote sensing images;landslide data set production;deep learning;satellite imagery;synthetic aperture radar;landslide detection;human production activities","","6","","16","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"The Time Variable in Data Fusion: A Change Detection Perspective","F. Bovolo; L. Bruzzone","Fondazione Bruno Kessler, Center for Information and Communication Technologies, Povo, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Povo, Trento, Italy","IEEE Geoscience and Remote Sensing Magazine","30 Sep 2015","2015","3","3","8","26","This paper presents an overview on the image fusion concept in the context of multitemporal remote sensing image processing. In the remote sensing literature, multitemporal image analysis mainly deals with the detection of changes and land-cover transitions. Thus the paper presents and analyses the most relevant literature contributions on these topics. From the perspective of change detection and detection of land-cover transitions, multitemporal image analysis techniques can be divided into two main groups: those based on the fusion of the multitemporal information at feature level, and those based on the fusion of the multitemporal information at decision level. The former mainly exploit multitemporal image comparison techniques, which aim at highlighting the presence/absence of changes by generating change indices. These indices are then analyzed by unsupervised algorithms for extracting the change information. The latter rely mainly on classification and include both supervised and semi/partially-supervised/unsupervised methods. The paper focuses the attention on both standard (and largely used) methods and techniques proposed in the recent literature. The analysis is conducted by considering images acquired by optical and SAR systems at medium, high and very high spatial resolution.","2168-6831","","10.1109/MGRS.2015.2443494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284786","","Feature extraction;Optical sensors;Optical imaging;Remote sensing;Nonlinear optics;Data integration;Data mining","geophysical image processing;image classification;image fusion;image resolution;land cover;optical radar;radar imaging;radar resolution;remote sensing by radar;synthetic aperture radar","data fusion;image fusion concept;multitemporal remote sensing image processing;land-cover transition;multitemporal image analysis technique;change information extraction;semipartially-supervised-unsupervised method;image classification;SAR system;image resolution;optical system","","110","","145","IEEE","30 Sep 2015","","","IEEE","IEEE Magazines"
"First experimental results on multi-angle DVB-S based passive ISAR exploiting multipolar data","F. Santi; I. Pisciottano; D. Pastina; D. Cristallini","Department of Information Engineering, Electronics and Telecommunications (DIET) – Sapienza University, Rome, Italy; Fraunhofer Institute for High Frequency Physics and Radar Techniques (FHR), Wachtberg, Germany; Department of Information Engineering, Electronics and Telecommunications (DIET) – Sapienza University, Rome, Italy; Fraunhofer Institute for High Frequency Physics and Radar Techniques (FHR), Wachtberg, Germany","2021 IEEE Radar Conference (RadarConf21)","18 Jun 2021","2021","","","1","6","This work investigates the potentialities of a passive multidimensional ISAR imaging based on DVB-S transmitters of opportunity. Image fusion methods exploiting diversities in both the spatial and polarimetric domains are introduced, aiming at achieving an image with better quality than the images acquired by the individual observation angles and/or polarimetric channels. An analysis using experimental datasets comprising a passive receiver able to collect signals in both the horizontal and vertical polarizations, a cooperative turning ferry, and an Astra satellite has been provided.","2375-5318","978-1-7281-7609-3","10.1109/RadarConf2147009.2021.9455209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9455209","passive radar;passive ISAR;DVB-S based passive ISAR;passive polarimetry;multidimensional imaging","Passive radar;Image segmentation;Satellites;Spaceborne radar;Imaging;Lighting;Receivers","digital video broadcasting;image fusion;radar imaging;synthetic aperture radar","passive multidimensional ISAR;image fusion methods;spatial domains;polarimetric domains;individual observation angles;polarimetric channels;experimental datasets;passive receiver;first experimental results;multiangle DVB-S based passive ISAR;multipolar data","","2","","17","IEEE","18 Jun 2021","","","IEEE","IEEE Conferences"
"Complex number domain SAR image fusion based on Laplacian pyramid","Y. He; Y. Zhang; P. Chen; J. Wang","School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beihang University, Beijing, China","2021 CIE International Conference on Radar (Radar)","8 Feb 2023","2021","","","306","309","Aiming at the problem of low azimuth resolution due to the limited azimuth observation angle of a single synthetic aperture radar (SAR) image, a multi-view SAR image fusion method is proposed. According to the characteristics of complex number domain SAR images in the MSTAR dataset, this method uses complex image domain data for processing. First, the morphological opening and closing operations are used to suppress the background noise, and then the Canny operator is used to extract the target contour, and the target area is separated from the original image according to the contour. Then the angle information in the image data is used to complete the image registration, and finally the multiple targets are merged through the Laplacian pyramid, and the obtained image contains more information and has an improved resolution. The simulation and real data processing results show that the multi-view SAR image fusion algorithm can better reduce the influence of external errors and improve the resolution.","2640-7736","978-1-6654-9814-2","10.1109/Radar53847.2021.10028153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10028153","SAR image resolution;Laplacian pyramid;complex image domain","Image registration;Image resolution;Laplace equations;Azimuth;Image edge detection;Radar imaging;Data processing","","","","","","8","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"YOLOrs: Object Detection in Multimodal Remote Sensing Imagery","M. Sharma; M. Dhanaraj; S. Karnam; D. G. Chachlakis; R. Ptucha; P. P. Markopoulos; E. Saber","Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Chester F. Carlson Center for Imaging Science, Rochester Institute of Technology, Rochester, NY, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Jan 2021","2021","14","","1497","1508","Deep-learning object detection methods that are designed for computer vision applications tend to underperform when applied to remote sensing data. This is because contrary to computer vision, in remote sensing, training data are harder to collect and targets can be very small, occupying only a few pixels in the entire image, and exhibit arbitrary perspective transformations. Detection performance can improve by fusing data from multiple remote sensing modalities, including red, green, blue, infrared, hyperspectral, multispectral, synthetic aperture radar, and light detection and ranging, to name a few. In this article, we propose YOLOrs: a new convolutional neural network, specifically designed for real-time object detection in multimodal remote sensing imagery. YOLOrs can detect objects at multiple scales, with smaller receptive fields to account for small targets, as well as predict target orientations. In addition, YOLOrs introduces a novel mid-level fusion architecture that renders it applicable to multimodal aerial imagery. Our experimental studies compare YOLOrs with contemporary alternatives and corroborate its merits.","2151-1535","","10.1109/JSTARS.2020.3041316","National Geospatial-Intelligence Agency(grant numbers:# HM0476-19-1-2014); National Science Foundation(grant numbers:OAC-1 808 582); Air Force Office of Scientific Research(grant numbers:FA9550-20-1-0039); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9273212","Aerial imagery;fusion;multimodal;object detection;remote sensing (RS)","Feature extraction;Remote sensing;Object detection;Computer architecture;Real-time systems;Head;Computer vision","computer vision;convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image fusion;object detection;remote sensing","YOLOrs;real-time object detection;multimodal remote sensing imagery;multimodal aerial imagery;deep-learning object detection;computer vision;remote sensing data;training data;arbitrary perspective transformations;detection performance;remote sensing modalities;hyperspectral aperture radar;multispectral aperture radar;synthetic aperture radar;light detection;mid-level fusion architecture","","10","","50","CCBY","30 Nov 2020","","","IEEE","IEEE Journals"
"Data Fusion Technique Using Wavelet Transform and Taguchi Methods for Automatic Landslide Detection From Airborne Laser Scanning Data and QuickBird Satellite Imagery","B. Pradhan; M. N. Jebur; H. Z. M. Shafri; M. S. Tehrany","Department of Civil Engineering, Geospatial Information Science Research Center (GISRC), Faculty of Engineering, Universiti Putra Malaysia, Serdang, Malaysia; Department of Civil Engineering, Geospatial Information Science Research Center (GISRC), Faculty of Engineering, Universiti Putra Malaysia, Serdang, Malaysia; Department of Civil Engineering, Geospatial Information Science Research Center (GISRC), Faculty of Engineering, Universiti Putra Malaysia, Serdang, Malaysia; Department of Civil Engineering, Geospatial Information Science Research Center (GISRC), Faculty of Engineering, Universiti Putra Malaysia, Serdang, Malaysia","IEEE Transactions on Geoscience and Remote Sensing","29 Feb 2016","2016","54","3","1610","1622","Landslide mapping is indispensable for efficient land use management and planning. Landslide inventory maps must be produced for various purposes, such as to record the landslide magnitude in an area and to examine the distribution, types, and forms of slope failures. The use of this information enables the study of landslide susceptibility, hazard, and risk, as well as of the evolution of landscapes affected by landslides. In tropical countries, precipitation during the monsoon season triggers hundreds of landslides in mountainous regions. The preparation of a landslide inventory in such regions is a challenging task because of rapid vegetation growth. Thus, enhancing the proficiency of landslide mapping using remote sensing skills is a vital task. Various techniques have been examined by researchers. This study uses a robust data fusion technique that integrates high-resolution airborne laser scanning data (LiDAR) with high-resolution QuickBird satellite imagery (2.6-m spatial resolution) to identify landslide locations in Bukit Antarabangsa, Ulu Klang, Malaysia. This idea is applied for the first time to identify landslide locations in an urban environment in tropical areas. A wavelet transform technique was employed to achieve data fusion between LiDAR and QuickBird imagery. An object-oriented classification method was used to differentiate the landslide locations from other land use/covers. The Taguchi technique was employed to optimize the segmentation parameters, whereas the rule-based technique was used for object-based classification. In addition, to assess the impact of fusion in classification and landslide analysis, the rule-based classification method was also applied on original QuickBird data which have not been fused. Landslide locations were detected, and the confusion matrix was used to examine the proficiency and reliability of the results. The achieved overall accuracy and kappa coefficient were 90.06% and 0.84, respectively, for fused data. Moreover, the acquired producer and user accuracies for landslide class were 95.86% and 95.32%, respectively. Results of the accuracy assessment for QuickBird data before fusion showed 65.65% and 0.59 for overall accuracy and kappa coefficient, respectively. It revealed that fusion made a significant improvement in classification results. The direction of mass movement was recognized by overlaying the final landslide classification map with LiDAR-derived slope and aspect factors. Results from the tested site in a hilly area showed that the proposed method is easy to implement, accurate, and appropriate for landslide mapping in a tropical country, such as Malaysia.","1558-0644","","10.1109/TGRS.2015.2484325","Putra Research Grant(grant numbers:GP-I/2014/9439200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322237","Fusion;landslide;laser scanning data (LiDAR);Malaysia;remote sensing;rule-based;Taguchi;wavelet transform;Fusion;landslide;laser scanning data (LiDAR);Malaysia;remote sensing;rule-based;Taguchi;wavelet transform","Terrain factors;Satellites;Wavelet transforms;Laser radar;Synthetic aperture radar;Image segmentation","geomorphology;geophysical techniques;image fusion","data fusion technique;wavelet transform;Taguchi methods;automatic landslide detection;airborne laser scanning data;quickbird satellite imagery;landslide mapping;efficient land use management;efficient land use planning;landslide inventory;landslide susceptibility;rapid vegetation growth;remote sensing;Bukit Antarabangsa;Ulu Klang;Malaysia;object-oriented classification method;LiDAR-derived slope","","69","","73","IEEE","9 Nov 2015","","","IEEE","IEEE Journals"
"An Energy-Based Model Encoding Nonlocal Pairwise Pixel Interactions for Multisensor Change Detection","R. Touati; M. Mignotte","Vision Laboratory, Université de Montréal, Montréal, QC, Canada; Vision Laboratory, Université de Montréal, Montréal, QC, Canada","IEEE Transactions on Geoscience and Remote Sensing","26 Jan 2018","2018","56","2","1046","1058","Image change detection (CD) is a challenging problem, particularly when images come from different sensors. In this paper, we present a novel and reliable CD model, which is first based on the estimation of a robust similarity-feature map generated from a pair of bitemporal heterogeneous remote sensing images. This similarity-feature map, which is supposed to represent the difference between the multitemporal multisensor images, is herein defined, by specifying a set of linear equality constraints, expressed for each pair of pixels existing in the before-and-after satellite images acquired through different modalities. An estimation of this overconstrained problem, also formulated as a nonlocal pairwise energy-based model, is then carried out, in the least square sense, by a fast linear-complexity algorithm based on a multidimensional scaling mapping technique. Finally, the fusion of different binary segmentation results, obtained from this similarity-feature map by different automatic thresholding algorithms, allows us to precisely and automatically classify the changed and unchanged regions. The proposed method is tested on satellite data sets acquired by real heterogeneous sensor, and the results obtained demonstrate the robustness of the proposed model compared with the best existing state-of-the-art multimodal CD methods recently proposed in the literature.","1558-0644","","10.1109/TGRS.2017.2758359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070964","Change detection (CD);energy-based model;FastMap;fusion of binary segmentations;heterogeneous sensors;multidimensional scaling (MDS) mapping;MDS;multimodal remote sensing;multisensors;multisource data;pairwise pixel interactions","Image sensors;Estimation;Remote sensing;Synthetic aperture radar;Optical sensors;Robustness","feature extraction;geophysical image processing;image classification;image coding;image fusion;image motion analysis;image segmentation;image sensors;object detection;remote sensing","unchanged regions;satellite data sets;heterogeneous sensor;multimodal CD methods;multisensor change detection;image change detection;bitemporal heterogeneous remote sensing images;multitemporal multisensor images;linear equality constraints;satellite images;overconstrained problem;fast linear-complexity algorithm;multidimensional scaling mapping technique;changed regions;nonlocal pairwise pixel interaction encoding;nonlocal pairwise energy-based model;automatic thresholding algorithms;binary segmentation;least square sense;robust similarity-feature map estimation","","39","","44","IEEE","18 Oct 2017","","","IEEE","IEEE Journals"
"Global Land-Cover Mapping With Weak Supervision: Outcome of the 2020 IEEE GRSS Data Fusion Contest","C. Robinson; K. Malkin; N. Jojic; H. Chen; R. Qin; C. Xiao; M. Schmitt; P. Ghamisi; R. Hänsch; N. Yokoya","AI for Good Research Lab, Microsoft Research, Redmond, WA, USA; Department of Mathematics, Yale University, New Haven, CT, USA; Microsoft Research, Redmond, WA, USA; Department of Civil, Environmental and Geodetic Engineering and the Environmental Science Graduate Program, The Ohio State University, Columbus, OH, USA; Department of Civil, Environmental and Geodetic Engineering, the Department of Electrical, and Computer Engineering, and the Translational Data Analytics Institute, The Ohio State University, Columbus, OH, USA; Department of Civil, Environmental and Geodetic Engineering, The Ohio State University, Columbus, OH, USA; Department of Geoinformatics, Munich University of Applied Sciences, München, Germany; Institute of Advanced Research in Artificial Intelligence, Vienna, Austria; German Aerospace Center, Weßling, Germany; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","26 Mar 2021","2021","14","","3185","3199","This article presents the scientific outcomes of the 2020 Data Fusion Contest (DFC2020) organized by the Image Analysis and Data Fusion Technical Committee of the IEEE Geoscience and Remote Sensing Society. The 2020 Contest addressed the problem of automatic global land-cover mapping with weak supervision, i.e., estimating high-resolution semantic maps while only low-resolution reference data are available during training. Two separate competitions were organized to assess two different scenarios: 1) high-resolution labels are not available at all; and 2) a small amount of high-resolution labels are available additionally to low-resolution reference data. In this article, we describe the DFC2020 dataset that remains available for further evaluation of corresponding approaches and report the results of the best-performing methods during the contest.","2151-1535","","10.1109/JSTARS.2021.3063849","Office of Naval Research(grant numbers:N000141712928); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369830","Convolutional neural networks (CNNs);deep learning;image analysis and data fusion;land-cover mapping;multimodal;random forests (RFs);weak supervision","Earth;Data integration;Remote sensing;Satellites;Training;Tensors;Synthetic aperture radar","geophysical image processing;geophysical techniques;image fusion;image resolution;land cover;remote sensing;terrain mapping","DFC2020 dataset;high-resolution labels;low-resolution reference data;high-resolution semantic maps;automatic global land-cover mapping;2020 IEEE GRSS Data Fusion Contest","","19","","41","CCBY","4 Mar 2021","","","IEEE","IEEE Journals"
"Framework for Fusion of Ascending and Descending Pass TanDEM-X Raw DEMs","R. Deo; C. Rossi; M. Eineder; T. Fritz; Y. S. Rao","Centre of Studies in Resources Engineering, Indian Institute of Technology Bombay, Mumbai, India; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Remote Sensing Technology Institute, German Aerospace Center (DLR), Wessling, Germany; Centre of Studies in Resources Engineering, Indian Institute of Technology Bombay, Mumbai, India","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","12 Aug 2015","2015","8","7","3347","3355","A novel method for calculating optimum incidence angle for the TanDEM-X system using any available digital elevation model (DEM) for the given area is proposed in this study. This method includes the plotting of slopes and aspect of the test area in a statistical way and applying mathematical approach using acquisition geometry in ascending and descending pass TanDEM-X data to optimize the incidence angle for obtaining precise DEM. Furthermore, the TanDEM-X raw DEMs in ascending and descending pass over Mumbai, India are combined using a simple weighted fusion algorithm and the quality of fused DEM thus generated is enhanced. The method adopted for fusion is just an experimental study. The problem of optimum weight selection for fusion has been addressed using height error map and a robust layover shadow mask calculated in “Integrated TanDEM-X Processor” (ITP) during TanDEM-X DEM generation. The height error map is calculated from the interferometric coherence with geometrical considerations and the robust layover and shadow map is calculated using TanDEM-X DEM and the corresponding slant range. Results show a significant reduction in the number of invalid pixels after fusion. In the fused DEM, invalids are only 2.14%, while ascending and descending pass DEMs have 5.02% and 6.34%, respectively. Statistical analysis shows a slight improvement in standard deviation of the error in fused DEM by 8% in urban area and about 5% for the whole scene. Only slight improvement in accuracy of fused DEM can be attributed to the coarse resolution of the SRTM-X DEM used as reference.","2151-1535","","10.1109/JSTARS.2015.2431433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112083","Fusion;layover;optimum incidence angle;TanDEM-X digital elevation model (DEM);weightage;Fusion;layover;optimum incidence angle;TanDEM-X digital elevation model (DEM);weightage","Coherence;Accuracy;Satellites;Synthetic aperture radar;Standards;Image color analysis","digital elevation models;geophysical image processing;image fusion;remote sensing","ascending pass TanDEM-X raw DEM;descending pass TanDEM-X raw DEM;TanDEM-X system;digital elevation model;statistical way;TanDEM-X data;Mumbai;India;fused DEM quality;height error map;Integrated TanDEM-X Processor;layover shadow mask;interferometric coherence;statistical analysis;urban area","","17","","28","IEEE","22 May 2015","","","IEEE","IEEE Journals"
"Vessel Detection From Nighttime Remote Sensing Imagery Based on Deep Learning","J. Shao; Q. Yang; C. Luo; R. Li; Y. Zhou; F. Zhang","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","16 Dec 2021","2021","14","","12536","12544","The continuous and rapid detection of sea vessels on a large scale is of great importance in marine traffic management, resource protection, and rights maintenance. Nighttime remote sensing can reflect human activities during the night with a wide swath and high efficiency, which is unique for vessel detection. Deep learning algorithms have already demonstrated superior performance in many fields, but it is confronted with some problems when applied to vessel detection with nighttime remote sensing imagery, including the lack of labeled dataset, the missed detection of small vessels, and false alarms of land targets. In this article, first, the nighttime remote sensing imagery was collected and the sea vessels in it were labeled. Second, to enhance the detection performance of small vessels, a modified YOLOv5 algorithm—TASFF-YOLOv5 was proposed, which was supplemented with a tiny target detection layer and a four-layer adaptively spatial feature fusion network to obtain a better feature fusion. Third, a land mask operation based on the sea–land prior database was performed to eliminate the false alarms of the land lights. The experimental results showed that the proposed TASFF-YOLOv5 could effectively improve the precision, recall, and mAP0.5 on the vessel dataset, achieving 95.2%, 93.1%, and 94.9% respectively.","2151-1535","","10.1109/JSTARS.2021.3125834","National Natural Science Foundation of China(grant numbers:62171016,61871413); Fundamental Research Funds for the Central Universities(grant numbers:buctrc202001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606559","Adaptively spatial feature fusion;nighttime remote sensing;sea–land mask;vessel detection;YOLOv5","Remote sensing;Feature extraction;Object detection;Deep learning;Synthetic aperture radar;Optical sensors;Optical imaging","feature extraction;geophysical image processing;image fusion;learning (artificial intelligence);object detection;remote sensing","missed detection;nighttime remote sensing imagery;sea vessels;detection performance;modified YOLOv5 algorithm-TASFF-YOLOv5;tiny target detection layer;vessel dataset;vessel detection;continuous detection;deep learning algorithms","","6","","38","CCBY","8 Nov 2021","","","IEEE","IEEE Journals"
"Sentinel-1-Based Water and Flood Mapping: Benchmarking Convolutional Neural Networks Against an Operational Rule-Based Processing Chain","M. Helleis; M. Wieland; C. Krullikowski; S. Martinis; S. Plank","German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany; German Remote Sensing Data Center (DFD), German Aerospace Center (DLR), Oberpfaffenhofen, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","8 Mar 2022","2022","15","","2023","2036","In this study, the effectiveness of several convolutional neural network architectures (AlbuNet-34/FCN/DeepLabV3+/U-Net/U-Net++) for water and flood mapping using Sentinel-1 amplitude data is compared to an operational rule-based processor (S-1FS). This comparison is made using a globally distributed dataset of Sentinel-1 scenes and the corresponding ground truth water masks derived from Sentinel-2 data to evaluate the performance of the classifiers on a global scale in various environmental conditions. The impact of using single versus dual-polarized input data on the segmentation capabilities of AlbuNet-34 is evaluated. The weighted cross entropy loss is combined with the Lovász loss and various data augmentation methods are investigated. Furthermore, the concept of atrous spatial pyramid pooling used in DeepLabV3+ and the multiscale feature fusion inherent in U-Net++ are assessed. Finally, the generalization capacity of AlbuNet-34 is tested in a realistic flood mapping scenario by using additional data from two flood events and the Sen1Floods11 dataset. The model trained using dual polarized data outperforms the S-1FS significantly and increases the intersection over union (IoU) score by 5%. Using a weighted combination of the cross entropy and the Lovász loss increases the IoU score by another 2%. Geometric data augmentation degrades the performance while radiometric data augmentation leads to better testing results. FCN/DeepLabV3+/U-Net/U-Net++ perform not significantly different to AlbuNet-34. Models trained on data showing no distinct inundation perform very well in mapping the water extent during two flood events, reaching IoU scores of 0.96 and 0.94, respectively, and perform comparatively well on the Sen1Floods11 dataset.","2151-1535","","10.1109/JSTARS.2022.3152127","German Federal Ministry of Education and Research; Künstliche Intelligenz zur Analyse von Erdbeobachtungs- und Internetdaten zur Entscheidungsunterstützung im Katastrophenfall(grant numbers:13N15525); Helmholtz Artificial Intelligence Cooperation Unit; AI for Near Real Time Satellite-based Flood Response(grant numbers:ZT-IPF-5-39); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714780","Convolutional neural networks;data augmentation;semantic segmentation;Sen1Floods11;Sentinel-1;Sentinel-2;surface water monitoring","Floods;Data models;Synthetic aperture radar;Convolutional neural networks;Training;Sea surface;Thresholding (Imaging)","convolutional neural nets;emergency management;entropy;feature extraction;floods;geophysical image processing;image classification;image fusion;image segmentation;learning (artificial intelligence);neural nets;remote sensing","convolutional neural networks;operational rule-based processing chain;operational rule-based processor;S-1FS;globally distributed dataset;geometric data augmentation;Sentinel-1-based water mapping;flood mapping;AlbuNet-34;FCN;DeepLabV3+;U-Net;U-Net++;ground truth water masks;environmental conditions;dual-polarized input data;single-polarized input data;spatial pyramid pooling;multiscale feature fusion","","5","","72","CCBY","16 Feb 2022","","","IEEE","IEEE Journals"
"A Novel Region-Based Image Registration Method for Multisource Remote Sensing Images Via CNN","L. Zeng; Y. Du; H. Lin; J. Wang; J. Yin; J. Yang","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Science and Technology on Information System Engineering Laboratory, China Electronics Technology Group Corporation, Nanjing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","22 Jan 2021","2021","14","","1821","1831","The comprehensive utilization of images from various satellite sensors can significantly increase the performance of remote sensing applications and has, therefore, attracted extensive research attention. One of the essential challenges that research encounters comes from multisource image registration. This article proposes a novel region-based image registration method for multisource images. The proposed method exploits the region features of input images, which provide more consistent and common information of the multisource data. The image region features are extracted based on image semantic segmentation using the deep convolutional neural network approach. The final registration result is a pixel-level output corresponding to the input images. The proposed registration scheme overcomes the limits of traditional feature extraction methods (e.g., point feature) adopted in previous registration schemes. Results indicate that the proposed method has good performance for the multisource remote sensing image registration and can serve as a building block for the fusion of multisource images.","2151-1535","","10.1109/JSTARS.2020.3047656","National Natural Science Foundation of China(grant numbers:61771043,61701454); National Key Research and Development Program of China(grant numbers:2017YFB0502703); Nature Science Foundation for Young Scientists of Jiangsu Province(grant numbers:BK20160147); China Postdoctoral Science Foundation(grant numbers:2020M680554); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309405","Image registration;radar imaging","Image registration;Radar polarimetry;Feature extraction;Optical imaging;Optical sensors;Image segmentation;Synthetic aperture radar","convolutional neural nets;deep learning (artificial intelligence);feature extraction;image fusion;image registration;image segmentation;remote sensing","region-based image registration method;multisource remote sensing images;multisource image registration;image semantic segmentation;CNN;image region features extraction;deep convolutional neural network","","4","","52","CCBY","28 Dec 2020","","","IEEE","IEEE Journals"
"Fusion of Urban TanDEM-X Raw DEMs Using Variational Models","H. Bagheri; M. Schmitt; X. X. Zhu","Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany; Signal Processing in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","4 Jan 2019","2018","11","12","4761","4774","Recently, a new global digital elevation model (DEM) with pixel spacing of 0.4 arcsec and relative height accuracy finer than 2 m for flat areas (slopes <; 20%) and better than 4 m for rugged terrain (slopes > 20%) was created through the TanDEM-X mission. One important step of the chain of global DEM generation is to mosaic and fuse multiple raw DEM tiles to reach the target height accuracy. Currently, weighted averaging (WA) is applied as a fast and simple method for TanDEM-X raw DEM fusion, in which the weights are computed from height error maps delivered from the Integrated TanDEM-X Processor (ITP). However, evaluations show that WA is not the perfect DEM fusion method for urban areas, especially in confrontation with edges such as building outlines. The main focus of this paper is to investigate more advanced variational approaches such as TV-L1 and Huber models. Furthermore, we also assess the performance of variational models for fusing raw DEMs produced from data takes with different baseline configurations and height of ambiguities. The results illustrate the high efficiency of variational models for TanDEM-X raw DEM fusion in comparison to WA. Using variational models could improve the DEM quality by up to 2m, particularly in inner city subsets.","2151-1535","","10.1109/JSTARS.2018.2878608","European Research Council (ERC); European Union's Horizon 2020 research and innovation(grant numbers:ERC-2016-StG-714087); Young Investigators Group “SiPEO”(grant numbers:VH-NG-1018); Bavarian Academy of Sciences and Humanities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540396","Data fusion;Huber model; $ L_1$  norm total variation;TanDEM-X DEM;weight map","Urban areas;Synthetic aperture radar;Remote sensing;Data models;Data fusion;Data integration","digital elevation models;image fusion;radar interferometry;terrain mapping","TanDEM-X mission;global DEM generation;multiple raw DEM tiles;target height accuracy;TanDEM-X raw DEM fusion;height error maps;Integrated TanDEM-X Processor;perfect DEM fusion method;urban areas;Huber models;variational models;raw DEMs;DEM quality;urban TanDEM-X;global digital elevation model;relative height accuracy finer","","2","","36","IEEE","18 Nov 2018","","","IEEE","IEEE Journals"
"Iterative Classifiers Combination Model for Change Detection in Remote Sensing Imagery","R. Hedjam; M. Kalacska; M. Mignotte; H. Ziaei Nafchi; M. Cheriet","Department of Geography, McGill University, Montréal, QC, Canada; Department of Geography, McGill University, Montréal, QC, Canada; Department of Computer Science and Operations Research (DIRO), University of Montréal, Montréal, QC, Canada; École de Technologie Supérieure, Montréal, QC, Canada; École de Technologie Supérieure, Montréal, QC, Canada","IEEE Transactions on Geoscience and Remote Sensing","30 Sep 2016","2016","54","12","6997","7008","In this paper, we propose a new unsupervised change detection method designed to analyze multispectral remotely sensed image pairs. It is formulated as a segmentation problem to discriminate the changed class from the unchanged class in the difference images. The proposed method is in the category of the committee machine learning model that utilizes an ensemble of classifiers (i.e., the set of segmentation results obtained by several thresholding methods) with a dynamic structure type. More specifically, in order to obtain the final “change/no-change” output, the responses of several classifiers are combined by means of a mechanism that involves the input data (the difference image) under an iterative Bayesian-Markovian framework. The proposed method is evaluated and compared to previously published results using satellite imagery.","1558-0644","","10.1109/TGRS.2016.2593982","Natural Sciences and Engineering Research Council of Canada; Fonds de Recherche du Quebec - Nature et technologies (FRQNT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7548313","Change detection;classifiers combination;image fusion;multispectral multitemporal image analysis;remote sensing imagery","Image segmentation;Remote sensing;Estimation;Training;Synthetic aperture radar;Iterative methods;Indexes","geophysical image processing;image classification;image segmentation;remote sensing","iterative classifiers combination model;change detection;remote sensing imagery;unsupervised change detection method;multispectral remotely sensed image pairs;segmentation problem difference images;committee machine learning model;final change-no-change output;iterative Bayesian-Markovian framework;satellite imagery","","16","","51","IEEE","24 Aug 2016","","","IEEE","IEEE Journals"
"Adversarial Deep Domain Adaptation for Multi-Band SAR Images Classification","W. Zhang; Y. Zhu; Q. Fu","National Key Laboratory of Science and Technology on ATR, Institution of Electronic Science, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, Institution of Electronic Science, National University of Defense Technology, Changsha, China; National Key Laboratory of Science and Technology on ATR, Institution of Electronic Science, National University of Defense Technology, Changsha, China","IEEE Access","25 Jun 2019","2019","7","","78571","78583","Deep convolutional neural networks (CNNs) have made a breakthrough on supervised SAR images classification. However, SAR imaging is considerably affected by the frequency band. That means a neural network trained on a SAR image set of one band is not suitable for the classification of another band images. As manually labeling the training samples of each band is always time-consuming, we propose an unsupervised multi-level domain adaptation method based on adversarial learning to solve the problem of multi-band SAR images classification. First, we train a discriminative CNN using samples of one frequency band data set that contains labels to map the data to a latent feature space. Then, we adjust the trained CNN to map the unlabeled samples of another frequency band data set to the same feature space through alternately optimizing two adversarial loss functions. Thus, the features of these two band images are fused and can be classified by the same classifier. We checked the performance of our method using both simulated data and measured data. Our method made a breakthrough in the classification of multi-band images with accuracies of 99% on both data sets. The results are even very close to the supervised CNN trained using a large number of labeled samples.","2169-3536","","10.1109/ACCESS.2019.2922844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736260","Convolutional neural network(CNN);domain adaptation;multi-band SAR images classification;adversarial learning","Radar polarimetry;Neural networks;Training;Adaptation models;Feature extraction;Deep learning;Optical imaging","convolutional neural nets;image classification;image fusion;optimisation;radar imaging;synthetic aperture radar","adversarial deep domain adaptation;frequency band data;deep convolutional neural networks;unsupervised multilevel domain adaptation method;supervised SAR image classification;multiband SAR image classification;discriminative CNN","","12","","37","OAPA","13 Jun 2019","","","IEEE","IEEE Journals"
"Feature Extraction Method for DCP HRRP-Based Radar Target Recognition via  $m-\chi$  Decomposition and Sparsity-Preserving Discriminant Correlation Analysis","J. Wang; Z. Liu; L. Ran; R. Xie","National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China; National Laboratory of Radar Signal Processing, Xidian University, Xi’an, China","IEEE Sensors Journal","18 Mar 2020","2020","20","8","4321","4332","Dual-circular polarimetric (DCP) high-resolution range profile (HRRP) provides similar target information to the full polarimetric HRRP and has the same data volume as the dual-linear polarimetric HRRP, thus it is significant to investigate the capability of DCP HRRP for target recognition. In this paper, a novel feature extraction method is proposed for the DCP HRRP-based target recognition. First, due to the good capability of characterizing the target polarimetric structure, the m-χ decomposition is exploited to obtain three scattering components of targets corresponding to the odd-bounce, even-bounce and randomly polarized scattering mechanisms along the radar line-of-sight (LOS), respectively. However, these three scattering components are infeasible to be directly utilized for target recognition due to the high dimensionality and redundancy. Considering this, a novel feature fusion method named as sparsity-preserving discriminant correlation analysis (SPDCA) is proposed to fuse the three scattering components. The SPDCA method can obtain the low-dimensional projection feature with good class separability by preserving both the global structure and local sparsity property of the original data. Besides, the redundancy of the three scattering components is eliminated by the SPDCA method. The results of experiments conducted on the simulated data of 10 civilian vehicles and real data of 3 military vehicles demonstrate the effectiveness and robustness of the proposed feature extraction method.","1558-1748","","10.1109/JSEN.2019.2962573","Equipment Pre-Research Field Foundation(grant numbers:61404150102); Fundamental Research Funds for the Central Universities(grant numbers:JB190206); National Postdoctoral Program for Innovative Talents(grant numbers:BX20180240); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943385","Radar automatic target recognition (RATR);high-resolution range profile (HRRP);dual-circular polarization;feature extraction;m ??? ?? decomposition;sparsity-preserving discriminant correlation analysis (SPDCA)","Feature extraction;Correlation;Radar;Target recognition;Data mining;Radar scattering","feature extraction;image fusion;military vehicles;radar computing;radar imaging;radar polarimetry;radar resolution;radar target recognition;synthetic aperture radar","military vehicles;civilian vehicles;feature extraction;dual-linear polarimetric HRRP;DCP HRRP-based target recognition;target polarimetric structure;m-χ decomposition;scattering components;randomly polarized scattering mechanisms;radar line-of-sight;feature fusion method;sparsity-preserving discriminant correlation analysis;SPDCA method;low-dimensional projection feature;local sparsity property;DCP HRRP-based radar target recognition;dual-circular polarimetric high-resolution range profile;target information","","11","","35","IEEE","26 Dec 2019","","","IEEE","IEEE Journals"
"Real-Time Multiview SAR Imaging Using a Portable Microwave Camera With Arbitrary Movement","J. Laviada; M. T. Ghasr; M. López-Portugués; F. Las-Heras; R. Zoughi","Department of Electrical Engineering, University of Oviedo, Gijón, Spain; Applied Microwave Nondestructive Testing Laboratory, Missouri University of Science and Technology, Rolla, MO, USA; Department of Electrical Engineering, University of Oviedo, Gijón, Spain; Applied Microwave Nondestructive Testing Laboratory, Missouri University of Science and Technology, Rolla, MO, USA; Applied Microwave Nondestructive Testing Laboratory, Missouri University of Science and Technology, Rolla, MO, USA","IEEE Transactions on Antennas and Propagation","29 Nov 2018","2018","66","12","7305","7314","This paper presents the first demonstrator of a portable, multiview, high-resolution, 3-D, and real-time microwave imaging system. The system is based on a recently developed real-time 3-D microwave camera, which performs quasi-monostatic acquisitions, equipped with an optical depth camera, providing target surface profile information. In addition, the entire system can be arbitrarily moved along the target performing microwave and depth camera synchronized acquisitions from different views with a twofold purpose, namely: 1) enabling a coverage area much larger than that possible with a static imaging system and b) allowing for incorporation of several tilt angles (or views) to enhance capturing specular reflection imaging data to improve the overall image quality. At each scanning position, the imaging data from the microwave camera are processed to build a local 3-D microwave image. The information is then merged, using recently proposed techniques for multiview synthetic aperture imaging, to compose the global image. The synchronized optical camera depth acquisitions enable tracking the entire imager movements so that the position and attitude are known. Moreover, the data acquired by the depth camera are also use to build a complementary 3-D outer surface profile model of the target, producing a combined and realistic image of the internal and external geometries of the target. Finally, the performance of the combined system is evaluated using several examples related to hidden contraband covered by clothing (i.e., people screening).","1558-2221","","10.1109/TAP.2018.2870485","Ministerio de Economía y Competitividad(grant numbers:TEC2014-55290-JIN); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466794","3-D microwave camera;depth camera;multiview imaging;synthetic aperture imaging","Cameras;Microwave imaging;Three-dimensional displays;Microwave theory and techniques;Apertures;Optical imaging","airborne radar;cameras;electromagnetic wave scattering;image fusion;image reconstruction;image resolution;image sensors;microwave imaging;object recognition;radar imaging;radar receivers;radar transmitters;synthetic aperture radar","entire imager movements;complementary 3-D outer surface profile model;combined image;realistic image;combined system;SAR imaging;portable microwave camera;arbitrary movement;portable resolution;multiview resolution;high-resolution;real-time microwave;optical depth camera;target surface profile information;target performing microwave;static imaging system;image quality;imaging data;multiview synthetic aperture imaging;global image;synchronized optical camera depth acquisitions;3D microwave image;quasimonostatic acquisition","","11","","35","IEEE","16 Sep 2018","","","IEEE","IEEE Journals"
"Ocean Eddy Recognition in SAR Images With Adaptive Weighted Feature Fusion","Y. Du; J. Liu; W. Song; Q. He; D. Huang","College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China; College of Information Technology, Shanghai Ocean University, Shanghai, China","IEEE Access","25 Oct 2019","2019","7","","152023","152033","Automatic recognition of ocean eddies has become one of the hotspots in the field of physical oceanography. Traditional methods based on either physical parameters or geometry features require manual parameter adjustment, and cannot adapt to the dynamic changes of ocean eddies caused by complicated ocean environments. To address these issues, we propose a new eddy recognition method in SAR images with adaptive weighted multi-feature fusion. Specially, to better characterize eddies, we first extract texture, shape and corner features using global Gray Level Co-occurrence Matrix (GLCM), detailed Fourier Descriptor (FD) and local salient Harris features respectively. Secondly, considering the different importance of features for eddy recognition, we propose an adaptive weighted feature fusion method with multiple kernel learning (MKL). Here, a combined kernel is derived to fuse three selected kernels for the three types of features with the weights trained by MKL. Finally, we design a SVM classifier with the combined kernel to realize the eddy recognition. The experimental results show that: 1) our proposed method can reach 93.42% of eddy recognition accuracy, which is much higher than the methods with only one single feature; 2) adaptive weighted fusion plays an important role in improving the accuracy. Our proposed method with MKL gains a 8.36% accuracy increase than the method without MKL. Through adaptive weighted fusion, our method avoids the manual parameter adjustment and is more robust and general. Experimental results have proven that our method is effective and applicable to recognize eddies.","2169-3536","","10.1109/ACCESS.2019.2946852","National Natural Science Foundation of China(grant numbers:41671431); National Natural Science Foundation of China(grant numbers:41906179); Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning(grant numbers:TP2016038); National Key Research and Development Program of China(grant numbers:2016YFC1401902); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8865024","Multi-feature fusion;adaptive weighted fusion;multiple kernel learning;ocean eddies;image recognition;SAR images","Oceans;Kernel;Feature extraction;Image recognition;Radar polarimetry;Fuses;Shape","feature extraction;Fourier analysis;geophysical image processing;image classification;image colour analysis;image fusion;image texture;learning (artificial intelligence);matrix algebra;oceanographic techniques;radar imaging;support vector machines;synthetic aperture radar","local salient Harris features;FD;Fourier descriptor;GLCM;global gray level cooccurrence matrix;shape feature extraction;texture feature extraction;corner feature extraction;ocean eddy recognition method;adaptive weighted multifeature fusion method;MKL;multiple kernel learning;complicated ocean environments;manual parameter adjustment;geometry features;oceanography;automatic recognition;SAR imaging","","5","","44","CCBY","14 Oct 2019","","","IEEE","IEEE Journals"
"High-resolution ISAR imaging with sparse subband based on waveform fusion dictionary","J. Ma; M. Gao; M. Xia; W. Hu; Z. Gao","Ordnance Engineering College, Shijiazhuang, Hebei, China; School of Information and Electronics, Beijing Institute of Technology, Beiiing, China; Ordnance Engineering College, Shijiazhuang, Hebei, China; Ordnance Engineering College, Shijiazhuang, Hebei, China; Ordnance Engineering College, Shijiazhuang, Hebei, China","2017 7th IEEE International Conference on Electronics Information and Emergency Communication (ICEIEC)","23 Oct 2017","2017","","","385","390","In this paper, a new high-resolution ISAR Imaging method by using sparse subband measurements is developed. It requires no resampling the irregularly measurements onto a uniform frequency grid. Firstly, a one-dimensional waveform dictionary for LFM signal after dechirping is constructed, and the principle of dictionary fusion is illustrated. Then, the two-dimensional waveform fusion dictionary is proposed. Secondly, the fusion imaging method based on Bayesian framework is analyzed, and a hierarchical form of the Laplace prior is used to sparse modeling of the high-resolution ISAR image. Finally, we provide experimental results with one-dimensional and two-dimensional fusion imaging, which illustrated the effectiveness and the superiority of the proposed fusion method over the existing algorithms.","2377-844X","978-1-5090-3025-5","10.1109/ICEIEC.2017.8076588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8076588","ISAR;Bayesian;Signal-Fusion;High-resolution","Dictionaries;Two dimensional displays;Imaging;Bayes methods;Scattering;Radar imaging;Estimation","Bayes methods;FM radar;image fusion;image resolution;radar imaging;synthetic aperture radar","dictionary fusion;fusion imaging method;sparse subband measurements;LFM signal;2D waveform fusion dictionary;2D fusion imaging;1D waveform dictionary;Bayesian framework;Laplace prior;linear frequemcy modulated signal;high-resolution ISAR imaging method","","","","15","IEEE","23 Oct 2017","","","IEEE","IEEE Conferences"
"Mapping Paddy Rice Area and Yields Over Thai Binh Province in Viet Nam From MODIS, Landsat, and ALOS-2/PALSAR-2","K. Guan; Z. Li; L. N. Rao; F. Gao; D. Xie; N. T. Hien; Z. Zeng","National Center for Supercomputing Applications, University of Illinois at Urbana Champaign, Urbana, IL, USA; School for the Environment, University of Massachusetts Boston, Boston, MA, USA; Asian Development Bank, Manila, Philippines; U. S. Department of Agriculture, Agricultural Research Service, Hydrology and Remote Sensing Laboratory, Beltsville, MD, USA; State Key Laboratory of Remote Sensing Science, Beijing Normal University, Beijing, China; Center for Informatics and Statistics, Ministry of Agriculture and Rural Development, Hanoi, Viet Nam; Department of Civil and Environmental Engineering, Princeton University, Princeton, NJ, USA","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","25 Jul 2018","2018","11","7","2238","2252","This study uses multiple satellite datasets to map paddy rice areas and yields for the Thai Binh Province, Viet Nam, over the summer growing season of 2015. The major datasets used are: first, surface reflectance and vegetation indices (VI) by fusing the optical observations from the Landsat sensors and the MODerate Resolution Imaging Spectroradiometer; and second, the L-band radar data from the PALSAR-2 sensor onboard the Advanced Land Observing Satellite 2. We find that although the fused VI time series are not necessarily beneficial for paddy rice mapping, the fusion datasets reduce observational gaps and allow us to better identify peak VI values and derive their empirical relationships with crop-cutting yield data (R2 = 0.4 for all the rice types, and R2 = 0.69 for the dominant rice type -58% of all the sampled fields). The L-band radar data have slightly lower performance in rice mapping than the optical satellite data, while it has much less contribution to yield estimation than the optical data. Furthermore, our study suggests the geolocation errors of satellite images be taken into account when selecting small sample are as for crop cutting. This practice will ensure the representativeness of crop-cutting sample areas with regard to satellite observations and thus better linkages between field data and satellite pixels for yield modeling. We also highlight the need of crop-cutting data from multiple years and/or at different regions to account for the spatial and temporal variations of harvest index to improve the spatially explicit rice yield estimates through satellite observations.","2151-1535","","10.1109/JSTARS.2018.2834383","Asian Development Bank; NASA New Investigator(grant numbers:NNX16AI56G); ACES Office of International; University of Illinois at Urbana Champaign; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8375090","Advanced Land Observing Satellite 2 (ALOS-2);agriculture;crop yield;image fusion;Landsat;moderate resolution imaging spectroradiometer (MODIS);paddy rice;Viet Nam","Agriculture;Satellites;Optical sensors;Remote sensing;Production;Earth;MODIS","agriculture;crops;geophysical image processing;remote sensing;remote sensing by radar;synthetic aperture radar;time series;vegetation;vegetation mapping","satellite pixels;crop-cutting yield data;rice types;dominant rice type;optical satellite data;optical data;satellite images;crop-cutting sample areas;field data;peak VI values;observational gaps;fusion datasets;paddy rice mapping;fused VI time series;Advanced Land Observing Satellite 2;PALSAR-2 sensor;L-band radar data;MODerate Resolution Imaging Spectroradiometer;Landsat sensors;optical observations;vegetation indices;surface reflectance;summer growing season;map paddy rice areas;multiple satellite datasets;ALOS-2/PALSAR-2;Viet Nam;Thai Binh Province;mapping paddy rice area;satellite observations;spatially explicit rice yield;crop-cutting data;yield modeling","","18","","72","IEEE","7 Jun 2018","","","IEEE","IEEE Journals"
"GAF-Net: Improving the Performance of Remote Sensing Image Fusion using Novel Global Self and Cross Attention Learning","A. Jha; S. Bose; B. Banerjee","Indian Institute of Technology Bombay, India; Technical University of Munich, Germany; Indian Institute of Technology Bombay, India","2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","6 Feb 2023","2023","","","6343","6352","The notion of self and cross-attention learning has been found to substantially boost the performance of remote sensing (RS) image fusion. However, while the self-attention models fail to incorporate the global context due to the limited size of the receptive fields, cross-attention learning may generate ambiguous features as the feature extractors for all the modalities are jointly trained. This results in the generation of redundant multi-modal features, thus limiting the fusion performance. To address these issues, we propose a novel fusion architecture called Global Attention based Fusion Network (GAF-Net), equipped with novel self and cross-attention learning techniques. We introduce the within-modality feature refinement module through global spectral-spatial attention learning using the query-key-value processing where both the global spatial and channel contexts are used to generate two channel attention masks. Since it is non-trivial to generate the cross-attention from within the fusion network, we propose to leverage two auxiliary tasks of modality-specific classification to produce highly discriminative cross-attention masks. Finally, to ensure non-redundancy, we propose to penalize the high correlation between attended modality-specific features. Our extensive experiments on five benchmark datasets, including optical, multispectral (MS), hyperspectral (HSI), light detection and ranging (LiDAR), synthetic aperture radar (SAR), and audio modalities establish the superiority of GAF-Net concerning the literature.","2642-9381","978-1-6654-9346-8","10.1109/WACV56688.2023.00629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030780","Applications: Remote Sensing;Image recognition and understanding (object detection;categorization;segmentation;scene modeling;visual reasoning)","Representation learning;Laser radar;Limiting;Benchmark testing;Feature extraction;Optical imaging;Optical sensors","","","","","","54","IEEE","6 Feb 2023","","","IEEE","IEEE Conferences"
"Multi-Sensor Image Feature Fusion via Subspace-based approach using $\ell _{1}$-Gradient Regularization","H. Vargas; J. Ramírez; S. Pinilla; J. I. Martínez-Torre","Department of Electronic Engineering, Universidad Manuela Beltrán, Campus Edificio Académico, Bogotá, Colombia; Department of Computer Science, Universidad Rey Juan Carlos, Campus Móstoles, Madrid, Spain; University of Manchester at Harwell Science and Innovation campus, Oxfordshire, United Kingdom; Department of Computer Science, Universidad Rey Juan Carlos, Campus Móstoles, Madrid, Spain","IEEE Journal of Selected Topics in Signal Processing","","2022","PP","99","1","13","Image fusion is a technique of combining two or more images into a single image which is more informative from an interpretation point of view. With the rapid development of different synthetic aperture radar sensing satellites capturing information from the earth by measuring energy in different portions of the electromagnetic spectrum (narrow/wide-band), complementary information about the area captured by different satellites is available (e.g. high-resolution spectral and RGB images). However, the estimation of the full-resolution image may not be necessary for inference approaches, including the pixel-based classification. Instead, it is desirable to extract the relevant information embedded in the available data to improve the inference capabilities. This work proposes a computational framework to estimate features with high-spatial-resolution and appropriate spectral content by combining information from a multi-sensor system. The considered multi-sensor setup is a hyperspectral imaging system with a complementary RGB sensor. The proposed framework first extracts spatial features from the RGB image using morphological profiles. Then, the fusion model assumes that the extracted features, and the hyperspectral measurements, lie in different subspaces matrices. In addition, this work developed a joint optimization scheme to solve the feature fusion problem by integrating the alternating direction method of multipliers with the block coordinate descent method. The alternating optimization method estimates the spatial features in the fusion model by penalizing the $\ell _{1}$-norm of the spatial gradient magnitudes. The quality of extracted features is measured in terms of supervised pixel-based classification methods. Extensive simulations show that the proposed approach outperforms other state-of-the-art methods in terms of classification accuracy.","1941-0484","","10.1109/JSTSP.2022.3219357","European Union's Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie(grant numbers:754382); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9937076","hyperspectral imaging system;pixel-based classification;feature fusion;alternating optimization;subspace-based method; $\ell _{1}$ -gradient regularization;synthetic aperture radar","Feature extraction;Sensors;Spatial resolution;Imaging;Sensor systems;Optimization;Optical sensors","","","","","","","IEEE","3 Nov 2022","","","IEEE","IEEE Early Access Articles"
"WHDA-FCM: Wolf Hunting-Based Dragonfly With Fuzzy C-Mean Clustering For Change Detection In SAR Images","J. T. Kumar; Y. M. Reddy; B. P. Rao","Jawaharlal Nehru Technological University, Kakinada, 533003 Andhra Pradesh, India; kumarthrisul9@gmail.com; Vasireddy Venkatadri Institute of Technology, Guntur, 522508 Andhra Pradesh, India; Jawaharlal Nehru Technological University, Kakinada, 533003 Andhra Pradesh, India","The Computer Journal","1 Jun 2020","2020","63","1","308","321","For the past few years, the automated addressing of changes in remote sensing images plays a significant role. However, the change detection (CD) model often suffers from the issue of speckle noise. More investigations have been proceeded to overcome this obstacle. This paper also considers the same issue and proposes a new CD model in synthetic aperture radar (SAR) images. Here, two SAR images that are captivated at different times will be considered as the input of the detection process. At first, discrete wavelet transform is incurred for image fusion, where the coefficients are optimally selected through a hybrid model that hybridizes the gray wolf optimization and dragonfly (DA) optimization. At last, the fused images after inverse transform are clustered via the fuzzy c-mean (FCM) clustering approach, and a similarity measure is performed between the segmented image and the ground truth image. The proposed model, wolf hunting-based DA with FCM, compares its performance over other conventional methods in terms of measures like accuracy, specificity, sensitivity, precision, negative predictive value, F1 score and Matthews correlation coefficient. Similarly, the negative measures are false positive rate, false negative rate and false discovery rate, and the betterment is proven.","1460-2067","","10.1093/comjnl/bxz130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9104624","SAR image;discrete wavelet transform;fuzzy c-mean;filter coefficient;wolf hunting-based dragonfly with FCM","","","","","","","","","1 Jun 2020","","","OUP","OUP Journals"
"[Copyright notice]","",,"2015 7th Conference on Information and Knowledge Technology (IKT)","5 Oct 2015","2015","","","1","1","The following topics are dealt with: wireless sensor network; Internet traffic classification; intrusion detection system; mobile RFID system; image steganography; particle swarm optimization; OFDM cellular networks; e-marketplaces; image watermarking scheme; recommender systems; cloud computing; backpropagation neural networks; social networks; emotion recognition; image segmentation; intelligent tutoring system; PolSAR images; brain multispectral MR image fusion; mobile e-healthcare system; and named entity recognition.","","978-1-4673-7485-9","10.1109/IKT.2015.7288664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7288664","","","backpropagation;biomedical MRI;cellular radio;cloud computing;electronic commerce;emotion recognition;image fusion;image segmentation;image watermarking;intelligent tutoring systems;Internet;medical computing;medical image processing;mobile computing;neural nets;OFDM modulation;particle swarm optimisation;pattern classification;radar imaging;radar polarimetry;radiofrequency identification;recommender systems;security of data;social networking (online);steganography;synthetic aperture radar;wireless sensor networks","wireless sensor network;Internet traffic classification;intrusion detection system;mobile RFID system;image steganography;particle swarm optimization;OFDM cellular networks;e-marketplaces;image watermarking scheme;recommender systems;cloud computing;backpropagation neural networks;social networks;emotion recognition;image segmentation;intelligent tutoring system;PolSAR images;brain multispectral MR image fusion;mobile e-healthcare system;named entity recognition","","","","","IEEE","5 Oct 2015","","","IEEE","IEEE Conferences"
"[Title page]","",,"2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)","3 Nov 2016","2016","","","i","i","The following topics are dealt with: SAR tomography of distributed media; hyperspectral band selection and dimensionality reduction; SAR image feature extraction and filtering; image processing techniques of detecting; ship detection; observations by the NASA Soil Moisture Active Passive Mission; multi-source remote sensing approaches to vegetation monitoring; Earth observing data science; remote sensing of land surface evapotranspiration; international spaceborne imaging spectroscopy missions; L-band microwave radiometry; radio frequency interference in microwave remote sensing and radio astronomy; TANDEM-X; atmosphere remote sensing and its application in air pollution; status and development of chinese meteorological and oceanographic series satellites; aperture synthesis radiometry; building features detection; classification of hyperspectral image; clouds and precipitation; data management and systems; detection with high resolution images; disaster and anomaly detection; feature extraction and detection algorithm; forest monitoring by radar and lidar; high resolution images classification; land cover mapping; land targets; microwave and optical calibration; microwave radiometer calibration and emerging techniques; microwave radiometry; multi-source images fusion and classification; object detection and recognition with SAR images; region based image classification; SAR and sonar image analysis and classification; SAR imaging techniques; satellite missions; urban targets and roads; vegetation monitoring by MODIS; vehicle and aircraft detection; advances on spaceborne SAR imaging; advanced interferometric processing and multidimensional SAR imaging techniques; lidar feature extraction and analysis; learning based image classification; SMAP soil moisture; remote sensing of vegetation traits and function; radar forestry; remote sensors and sensing of urban areas; geographic information science; IEEE GRSS data fusion contest; student paper contest finalists; physical models in microwave remote sensing; calibration, validation and related topics in support of spaceborne imaging spectroscopy missions; thermal and hyperspectral sensors and mapping; remote sensing using GNSS-like signals and other sensors; radiometer cross-calibration; COSMO-SKYMED mission; bistatic and digital beamforming SAR; numerical weather prediction and data assimilation; sar remote sensing for ocean applications; remote sensing of high winds; ocean surface winds; aerial images analysis and applications; analysis of multitemporal optical images; change detection applications; coastal zones; data fusion techniques; electromagnetic theory; multitemporal InSAR analysis; noise reduction techniques; optical sensors and calibration; soil properties; vegetation and tree remote sensing; pseudo pixel based image classification; Water Cycle Observation Mission (WCOM); remote sensing for ecology; advancing interoperability for geoscience information systems; lunar-based earth observation; advances in bathymetric and oceanographic lidar studies; DRAGON 3 cooperation; the Mexican perspective to the understanding of our planet through remote sensing; ALOS-2; SENTINEL-1 constellation mission; atmospheric sounding sensors; ocean waves and currents; ocean temperature and salinity; aerosols and atmospheric chemistry; air pollution; disaster assessment; droughts; earthquake; estimation and regression applications; floods; geographic information science; hazards and disasters; land cover dynamics; land use applications; ocean altimetry; POL and POLINSAR; sea ice; snow cover; neural network based classification; high resolution image analysis; global change study; forest biomass; digital terrain models; minerals and hydrocarbons; remote sensing data and policy decisions; earth remote sensing with small satellites; the China France Oceanography Satellite; agricultural parameters; big data in geoscience; differential SAR interferometry; ice sheets and glaciers; inland waters; interdisciplinary topics; remote sensing for crop yield and classification; remote sensing in mining; subsurface sensing; spectral unmixing techniques; UAV systems and sensors; super-resolution; oil spill, and; urbanization and environmental change.","2153-7003","978-1-5090-3332-4","10.1109/IGARSS.2016.7728982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728982","","","aerosols;air pollution;atmospheric chemistry;autonomous underwater vehicles;bathymetry;crops;disasters;earthquakes;ecology;evaporation;feature extraction;floods;geophysical image processing;geophysical signal processing;glaciology;hydrology;ice;image classification;image denoising;image fusion;land cover;marine pollution;meteorology;minerals;mining;object detection;optical radar;radiometry;rain;remote sensing by laser beam;remote sensing by radar;ships;snow;sonar;synthetic aperture radar;transpiration;vegetation mapping","IEEE International Geoscience and Remote Sensing Symposium;precipitation;disaster detection;anomaly detection;radar forest monitoring;lidar forest monitoring;land cover mapping;land targets;calibration;multisource images fusion;multisource images classification;object detection;object recognition;sonar image analysis;urban targets;urban roads;MODIS;vehicle detection;aircraft detection;SMAP soil moisture;forestry;geographic information science;GNSS like signals;COSMO-SKYMED mission;change detection;coastal zones;data fusion;noise reduction;Water Cycle Observation Mission;ecology;bathymetric lidar;oceanographic lidar;DRAGON 3 cooperation;ALOS-2;clouds;hyperspectral image classification;building features detection;aperture synthesis radiometry;air pollution;atmosphere remote sensing;TANDEM-X;radio astronomy;radio frequency interference;microwave radiometry;international spaceborne imaging spectroscopy missions;land surface evapotranspiration;Earth observing data science;vegetation monitoring;multisource remote sensing;NASA Soil Moisture Active Passive Mission;ship detection;image processing techniques;SAR image filtering;SAR image feature extraction;dimensionality reduction;hyperspectral band selection;SAR tomography;IGARSS;SENTINEL-1 constellation mission;atmospheric sounding sensors;ocean waves;ocean currents;ocean temperature;ocean salinity;aerosols;atmospheric chemistry;disaster assessment;droughts;earthquake;floods;hazards;disasters;land cover dynamics;land use;ocean altimetry;POLINSAR;sea ice;snow cover;neural network based classification;forest biomass;digital terrain models;minerals;hydrocarbons;China France Oceanography Satellite;differential SAR interferometry;ice sheets;glaciers;crop yield;oil spill","","","","","IEEE","3 Nov 2016","","","IEEE","IEEE Conferences"
"[Copyright notice]","",,"2016 IEEE 12th International Colloquium on Signal Processing & Its Applications (CSPA)","21 Jul 2016","2016","","","ii","ii","The following topics are dealt with: template-based search; scene analysis; deceptive speech detection; sparse representations; Wingler method; multicomponent exponential analysis; nonequispaced data; current waveform extraction; low-complexity CORDIC-based VLSI design; FPGA prototype; CI-OFDMA; particle filter; least square fitting prediction; spatial relationship based multiview elimination; 3D volleyball players tracking; visible light beacon-based positioning systems; smartphone; hesitant dynamic postural control detection; DVT cuffs monitoring; long-term operation; fuzzy approach; content based mode; depth skipping; sharp edges; directional edges; screen content coding intraprediction; fast enhancement layer intracoding; interchannel correlations; TU depth correlation; SHVC; feature level fusion; biometric verification; two-lead ECG signals; model-following servo controller design; integral-type state equation; visual anomaly analysis; ST-based multiresolution VEP decomposition; disturbance removal controller design; active disturbance rejection control; PID controller design; reset compensator; Clegg integrator; model-following control design ; controller inversion technique; respiratory sound analysis; continuous positive airway pressure machines; PZT driver development; acoustic energy transfer applications; nonparametric Bayesian models; ARX identification; predictive models construction; bicycle riding comfort evaluation; electromyogram; electroencephalogram; multiple-input-multiple-output systems; automatic alignment method; projection mapping waveform optimization techniques; bistatic cognitive radars; multiutility-meter; load profiling; waveform capture logger applications; capacitive power transfer; impedance matching network; file transfer system; hidden ID signaling; camera; computer-smart devices file sharing; accompaniment signal extraction method; stereo type noise suppression method; hybrid TOA; RSS-based factor graph; wireless geolocation technique; FWM based optical delay performance enhancement; FWM based optical delay optimization; perturbation signal comparison; ill-conditioned systems; system characteristics; source characteristics; speaker verification; limited abnormal sound analytical surveillance system; microcontroller; noise reduction methods; terrain phase estimation; inSAR images; path loss model; outdoor environment; single image superresolution; no-reference image quality index optimization; PCA subspace; visual based trespasser; faint detection; human silhouette integration; posture recognition; ultrawideband propagation; narrow band propagation; indoor positioning; square-root-free QR-based adaptive filtering algorithms; LS performance analysis; MMSE; channel estimation; VLC systems; UAV onboard positioning; ground control point establishment; object classification; object recognition; bows model; local binary pattern; LBP; variant object detection; vulnerability map visualization; dengue incidences; CORDIC-based FFT real-time processing design; power-throughput Blowfish algorithm development; abdominal aortic aneurysm automatic detection; abdominal aortic aneurysm segmentation; abdominal aortic aneurysm classification; orthogonal matching pursuit; fuzzy fractional-order PI-PD controller design; automatic fire detection; warning system; home video surveillance; flood prediction modelling; NNARX structure; sound amplitude characterization; force feedback; tele-haptic operation; Arabic phonemes acoustic analysis; model predictive control; pneumatic actuator system; direct thermal desorption extraction; DTD; agarwood incense analysis; Sabah electricity Sdn Bhd; SESB; route transmission line; AHP; ARX model parameters; vertical position electro-hydraulic actuator; near infrared spectroscopy applications; NIRS; noninvasive leukemia screening; raw broiler shear force prediction; visible near infrared spectroscopy; short wave near infrared spectroscopy; verification test; tree height quantification; mangrove forest; bioshield; Malaysia; steam distillation system nonlinear modeling; high dimensional data; hidden neuron variation; multilayer perceptron; flood water level prediction; Kusial station; embedded force sensor calibration and robotic hand manipulation.","","978-1-4673-8780-4","10.1109/CSPA.2016.7515768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515768","","","active disturbance rejection control;adaptive filters;analytic hierarchy process;autonomous aerial vehicles;Bayes methods;biometrics (access control);cameras;channel estimation;control system synthesis;data visualisation;digital arithmetic;diseases;electrocardiography;electroencephalography;electrohydraulic control equipment;electromyography;fast Fourier transforms;feature extraction;field programmable gate arrays;fires;floods;force feedback;force sensors;forestry;frequency division multiple access;fuzzy control;fuzzy set theory;haptic interfaces;image classification;image coding;image fusion;image resolution;image retrieval;image segmentation;impedance matching;infrared spectroscopy;intelligent sensors;least squares approximations;manipulators;microcontrollers;multilayer perceptrons;natural language processing;object detection;object recognition;object tracking;OFDM modulation;particle filtering (numerical methods);PD control;peer-to-peer computing;PI control;pneumatic actuators;power transmission lines;predictive control;principal component analysis;radar interferometry;servomechanisms;signal detection;signal representation;smart phones;speaker recognition;speech processing;synthetic aperture radar;three-term control;video surveillance;VLSI","template-based search;scene analysis;deceptive speech detection;sparse representations;Wingler method;multicomponent exponential analysis;nonequispaced data;current waveform extraction;low-complexity CORDIC-based VLSI design;FPGA prototype;CI-OFDMA;particle filter;least square fitting prediction;spatial relationship based multiview elimination;3D volleyball players tracking;visible light beacon-based positioning systems;smartphone;hesitant dynamic postural control detection;DVT cuffs monitoring;long-term operation;fuzzy approach;content based mode;depth skipping;sharp edges;directional edges;screen content coding intraprediction;fast enhancement layer intracoding;interchannel correlations;TU depth correlation;SHVC;feature level fusion;biometric verification;two-lead ECG signals;model-following servo controller design;integral-type state equation;visual anomaly analysis;ST-based multiresolution VEP decomposition;disturbance removal controller design;active disturbance rejection control;PID controller design;reset compensator;Clegg integrator;model-following control design;controller inversion technique;respiratory sound analysis;continuous positive airway pressure machines;PZT driver development;acoustic energy transfer applications;nonparametric Bayesian models;ARX identification;predictive models construction;bicycle riding comfort evaluation;electromyogram;electroencephalogram;multiple-input-multiple-output systems;automatic alignment method;projection mapping waveform optimization techniques;bistatic cognitive radars;multiutility-meter;load profiling;waveform capture logger applications;capacitive power transfer;impedance matching network;file transfer system;hidden ID signaling;camera;computer-smart devices file sharing;accompaniment signal extraction method;stereo type noise suppression method;hybrid TOA;RSS-based factor graph;wireless geolocation technique;FWM based optical delay performance enhancement;FWM based optical delay optimization;perturbation signal comparison;ill-conditioned systems;system characteristics;source characteristics;speaker verification;limited abnormal sound analytical surveillance system;microcontroller;noise reduction methods;terrain phase estimation;inSAR images;path loss model;outdoor environment;single image superresolution;no-reference image quality index optimization;PCA subspace;visual based trespasser;faint detection;human silhouette integration;posture recognition;ultrawideband propagation;narrow band propagation;indoor positioning;square-root-free QR-based adaptive filtering algorithms;LS performance analysis;MMSE;channel estimation;VLC systems;UAV onboard positioning;ground control point establishment;object classification;object recognition;bows model;local binary pattern;LBP;variant object detection;vulnerability map visualization;dengue incidences;CORDIC-based FFT real-time processing design;power-throughput Blowfish algorithm development;abdominal aortic aneurysm automatic detection;abdominal aortic aneurysm segmentation;abdominal aortic aneurysm classification;orthogonal matching pursuit;fuzzy fractional-order PI-PD controller design;automatic fire detection;warning system;home video surveillance;flood prediction modelling;NNARX structure;sound amplitude characterization;force feedback;tele-haptic operation;Arabic phonemes acoustic analysis;model predictive control;pneumatic actuator system;flood water level prediction;Kusial station;embedded force sensor calibration;robotic hand manipulation;frequency 416 kHz;time 7 hour;multilayer perceptron;hidden neuron variation;high dimensional data;steam distillation system nonlinear modeling;Malaysia;bioshield;mangrove forest;tree height quantification;verification test;short wave near infrared spectroscopy;visible near infrared spectroscopy;raw broiler shear force prediction;noninvasive leukemia screening;NIRS;near infrared spectroscopy applications;vertical position electro-hydraulic actuator;ARX model parameters;AHP;route transmission line;SESB;Sabah electricity Sdn Bhd;agarwood incense analysis;DTD;direct thermal desorption extraction","","","","","IEEE","21 Jul 2016","","","IEEE","IEEE Conferences"
