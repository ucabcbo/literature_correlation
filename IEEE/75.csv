"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Drawgan: Text to Image Synthesis with Drawing Generative Adversarial Networks","Z. Zhang; J. Zhou; W. Yu; N. Jiang","Graduate School of Science and Engineering, Hosei University; Graduate School of Science and Engineering, Hosei University; Graduate School of Science and Engineering, Hosei University; Graduate School of Science and Engineering, Hosei University","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","4195","4199","In this paper, we propose a novel drawing generative adversarial networks (DrawGAN) for text-to-image synthesis. The whole model divides the image synthesis into three stages by imitating the process of drawing. The first stage synthesizes the simple contour image based on the text description, the second stage generates the foreground image with detailed information, and the third stage synthesizes the final result. Through the step by step synthesis process from simple to complex and easy to difficult, the model can draw the corresponding results step by step and finally achieve the higher-quality image synthesis effect. Our method is validated on the Caltech-UCSD Birds 200 (CUB) dataset and the Microsoft Common Objects in Context (MS COCO) dataset. The experimental results demonstrate the effectiveness and superiority of our method. In terms of both subjective and objective evaluation, our method’s results surpass the existing state-of-the-art methods.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414166","Text-to-Image Synthesis;Computer Vision;Deep Learning;Generative Adversarial Networks","Image synthesis;Conferences;Signal processing;Generative adversarial networks;Birds;Acoustics;Speech processing","image texture;neural nets","generative adversarial networks;text-to-image synthesis;simple contour image;foreground image;higher-quality image synthesis effect;Caltech-UCSD Birds 200 dataset;drawGAN;CUB dataset;Microsoft common objects in context dataset;MS COCO dataset","","3","","20","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Remote Sensing Image Synthesis via Graphical Generative Adversarial Networks","G. Wang; G. Dong; H. Li; L. Han; X. Tao; P. Ren","College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China; College of Information and Control Engineering, China University of Petroleum, Qingdao, China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","10027","10030","We explore the use of graphical generative adversarial networks (Graphical-GAN) for synthesizing remote sensing images. The model is probabilistic graphical based generative adversarial networks (GAN). It pairs a generative network G with a recognition network R. Both of them are adversarially trained with a discriminative network D. Particularly, R is employed to infer the underlying causal relationships among both observed and latent variables from real remote sensing images. The advantages of the Graphical-GAN for synthesizing multiple categories of remote sensing images are two fold. Firstly, it considers the underlying causal relationships and captures the true data distribution of remote sensing images. Secondly, the adversarial learning generates synthetic sensing images that are similar to real ones with slight differences. Our remote sensing image synthesis scheme paves a promising way for remote sensing dataset augmentation, which is an effective means of improving the accuracy of learning models. Experimental results with high Inception Scores (IS) validate the effectiveness of the Graphical-GAN for remote sensing image synthesis.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8898915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8898915","Remote Sensing Image Synthesis;Graphical Generative Adversarial Networks","Remote sensing;Image synthesis;Training;Generative adversarial networks;Bayes methods;Neural networks;Probabilistic logic","inference mechanisms;learning (artificial intelligence);remote sensing","graphical generative adversarial networks;Graphical-GAN;remote sensing images;generative network;recognition network;causal relationships;synthetic sensing images;remote sensing image synthesis scheme paves;remote sensing dataset augmentation;discriminative network","","5","","11","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Text To Image Synthesis With Erudite Generative Adversarial Networks","Z. Zhang; W. Yu; N. Jiang; J. Zhou",Hosei Unviersity; Southwest University of Science and Technology; Southwest University of Science and Technology; Hosei Unviersity,"2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","2438","2442","In this paper, an Erudite Generative Adversarial Networks (EruditeGAN) is proposed for the text-to-image synthesis task. By introducing additional image distribution related to the original image into the network structure, the entire network can learn more about the image distribution and become more knowledgeable. In this case, it can be more clear about the distribution of the image that needs to be synthesized and finally synthesize high-quality results. Experiments well validate our method’s effectiveness and demonstrate the different effects of different distribution situations on the final results. According to the quantitative results of Fréchet Inception Distance (FID) and R-precision, our method’s comprehensive score is the best, which reflects our results are closer to the real image effect.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506487","Hosei University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506487","Text-to-Image Synthesis;Computer Vision;Deep Learning;Generative Adversarial Networks","Image synthesis;Conferences;Generative adversarial networks;Data models;Task analysis","image reconstruction;image resolution;inference mechanisms;learning (artificial intelligence);security of data","network structure;image effect;Erudite Generative Adversarial Networks;text-to-image synthesis task;image distribution","","","","23","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Fast-Converging Conditional Generative Adversarial Networks for Image Synthesis","C. Li; Z. Wang; H. Qi","Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","2132","2136","Building on top of the success of generative adversarial networks (GANs), conditional GANs attempt to better direct the data generation process by conditioning with certain additional information. Inspired by the most recent AC-GAN, in this paper we propose a fast-converging conditional GAN (FC-GAN). In addition to the real/fake classifier used in vanilla GANs, our discriminator has an advanced auxiliary classifier which distinguishes each real class from an extra `fake' class. The `fake' class avoids mixing generated data with real data, which can potentially confuse the classification of real data as AC-GAN does, and makes the advanced auxiliary classifier behave as another real/fake classifier. As a result, FC-GAN can accelerate the process of differentiation of all classes, thus boost the convergence speed. Experimental results on image synthesis demonstrate our model is competitive in the quality of images generated while achieving a faster convergence rate.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451161","Generative adversarial networks;conditioning;fast convergence;image synthesis","Gallium nitride;Generators;Generative adversarial networks;Convergence;Image generation;Visualization;Training","artificial intelligence;convergence;image classification;neural nets","data classification;fake classifier;real classifier;data generation;auxiliary classifier;fast-converging conditional GAN;fast-converging conditional generative adversarial networks;image synthesis","","19","1","25","IEEE","6 Sep 2018","","","IEEE","IEEE Conferences"
"Utilization of Generative Adversarial Networks in Face Image Synthesis for Augmentation of Face Recognition Training Data","A. R. Revanda; C. Fatichah; N. Suciati","Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia; Department of Informatics, Institut Teknologi Sepuluh Nopember, Surabaya, Indonesia","2020 International Conference on Computer Engineering, Network, and Intelligent Multimedia (CENIM)","24 Dec 2020","2020","","","396","401","Face recognition has become a popular research field in computer vision and is widely applied in various sectors. The challenge with face recognition is that if the training data is limited, the face recognition rate will be less effective. Generative Adversarial Networks (GANs) is a deep learning method that can create synthesis images with high quality. This research aims to utilize GANs in synthesizing face images as a form of augmentation in face recognition training data. Initially, the latent space representation of the face image will be made using GANs, then adding styles to the face image using the latent direction method. In the experiment of making latent space representation, the loss value was able to reach 0.15. In the experiment of face recognition, the addition of face image synthesis was able to increase the accuracy of the face recognition classifier model from 0.74 to 0.89.","","978-1-7281-8283-4","10.1109/CENIM51130.2020.9297899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9297899","generative adversarial networks;face image synthesis;face recognition","Face recognition;Generators;Feature extraction;Computer architecture;Computational modeling;Generative adversarial networks;Training data","deep learning (artificial intelligence);face recognition;image classification;image representation","deep learning method;computer vision;face recognition training data augmentation;face recognition classifier model;latent space representation;GAN;face image synthesis;generative adversarial networks","","","","18","IEEE","24 Dec 2020","","","IEEE","IEEE Conferences"
"Bottleneck Sharing Generative Adversarial Networks for Unified Multi-Contrast MR Image Synthesis","O. Dalmaz; B. Sağlam; K. Gönç; S. U. Dar; T. Çukur","Ulusal Manyetik Rezonans Araştırma Merkezi (UMRAM), Bilkent Üniversitesi, Ankara, Türkiye; Elektrik ve Elektronik Mühendisliği Bölümü, Bilkent Üniversitesi, Ankara, Türkiye; Bilgisayar Mühendisliği Bölümü, Bilkent Üniversitesi, Ankara, Türkiye; Ulusal Manyetik Rezonans Araştırma Merkezi (UMRAM), Bilkent Üniversitesi, Ankara, Türkiye; Sinirbilim Programı, Mühendislik ve Fen Bilimleri Enstitüsü, Bilkent Üniversitesi, Ankara, Türkiye","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","Magnetic Resonance Imaging (MRI) is the favored modality in multi-modal medical imaging due to its safety and ability to acquire various different contrasts of the anatomy. Availability of multiple contrasts accumulates diagnostic information and, therefore, can improve radiological observations. In some scenarios, acquiring all contrasts might be challenging due to reluctant patients and increased costs associated with additional scans. That said, synthetically obtaining missing MRI pulse sequences from the acquired sequences might prove to be useful for further analyses. Recently introduced Generative Adversarial Network (GAN) models offer state-of-the-art performance in learning MRI synthesis. However, the proposed generative approaches learn a distinct model for each conditional contrast to contrast mapping. Learning a distinct synthesis model for each individual task increases the time and memory demands due to the increased number of parameters and training time. To mitigate this issue, we propose a novel unified synthesis model, bottleneck sharing GAN (bsGAN), to consolidate learning of synthesis tasks in multi-contrast MRI. bsGAN comprises distinct convolutional encoders and decoders for each contrast to increase synthesis performance. A central information bottleneck is employed to distill hidden representations. The bottleneck, based on residual convolutional layers, is shared across contrasts to avoid introducing many learnable parameters. Qualitative and quantitative comparisons on a multi-contrast brain MRI dataset show the effectiveness of the proposed method against existing unified synthesis methods.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864880","unified;MRI synthesis;bottleneck;parameter-sharing;generative adversarial networks","Training;Costs;Convolution;Image synthesis;Magnetic resonance imaging;Generative adversarial networks;Safety","biomedical MRI;brain;image segmentation;learning (artificial intelligence);medical image processing","unified synthesis model;bottleneck sharing GAN;bsGAN;synthesis tasks;multicontrast MRI;distinct convolutional encoders;synthesis performance;central information bottleneck;multicontrast brain MRI;image synthesis;magnetic resonance imaging;multimodal medical imaging;diagnostic information;radiological observations;MRI pulse sequences;generative adversarial network models;learning MRI synthesis;distinct model;conditional contrast;contrast mapping;distinct synthesis model;bottleneck sharing generative adversarial networks","","","","29","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Skip Connections for Medical Image Synthesis with Generative Adversarial Networks","M. U. Mirza; O. Dalmaz; T. Çukur","Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey; Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey; Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","Magnetic Resonance Imaging (MRI) is an imaging technique used to produce detailed anatomical images. Acquiring multiple contrast MRI images requires long scan times forcing the patient to remain still. Scan times can be reduced by synthesising unacquired contrasts from acquired contrasts. In recent years, deep generative adversarial networks have been used to synthesise contrasts using one-to-one mapping. Deeper networks can solve more complex functions, however, their performance can decline due to problems such as overfitting and vanishing gradients. In this study, we propose adding skip connections to generative models to overcome the decline in performance with increasing complexity. This will allow the network to bypass unnecessary parameters in the model. Our results show an increase in performance in one-to-one image synthesis by integrating skip connections.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864939","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864939","Medical image synthesis;Magnetic resonance imaging (MRI);Multi-contrast MRI;Generative adversarial network;Skip connections","Image synthesis;Magnetic resonance imaging;Signal processing;Generative adversarial networks;Complexity theory;Biomedical imaging","biomedical MRI;medical image processing","skip connections;medical image synthesis;magnetic resonance imaging;imaging technique;anatomical images;multiple contrast MRI images;deep generative adversarial networks;complex functions;generative models","","","","20","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Microscopic Fluorescence In Situ Hybridization (FISH) Image Synthesis with Generative Adversarial Networks","G. Dursun; U. Özkaya","Elektrik-Elektronik Mühendisliği, Süleyman Demirel Üniversitesi, Isparta, Türkiye; Elektrik-Elektronik Mühendisliği, Süleyman Demirel Üniversitesi, Isparta, Türkiye","2021 29th Signal Processing and Communications Applications Conference (SIU)","19 Jul 2021","2021","","","1","4","One of the most important problems in biomedical image analysis is the low amount of data and the cost of accessing to the marked data by researchers. In order to provide a solution to this problem, microscopic fluorescence in situ hybridization (FISH) images are synthesized with generative adversarial network in this paper. The generative adversarial network is trained to synthesize FISH images from mask images. The trained model was implemented on 150 test images and the performance of the model both was presented with visual results and evaluated quantitatively by calculating the performance metrics. By evaluating the synthesized FISH images in terms of image quality and structural features, it is observed that they can be used to provide a solution to the problem of the lack of data.","2165-0608","978-1-6654-3649-6","10.1109/SIU53274.2021.9477999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477999","generative adversarial networks;image synthesis;fluorescence in situ hybridization","Fish;Generative adversarial networks;Biomedical imaging;Microscopy;Image synthesis;Fluorescence;Image segmentation","biomedical optical imaging;fluorescence;image segmentation;medical image processing;neural nets;optical microscopy","generative adversarial network;biomedical image analysis;marked data;mask images;test images;synthesized FISH images;image quality;microscopic fluorescence in situ hybridization image synthesis;performance metrics;structural features","","","","","IEEE","19 Jul 2021","","","IEEE","IEEE Conferences"
"Image Synthesis of Warli Tribal Stick figures using Generative Adversarial Networks","N. Das; S. Kundu; S. Deb","Dept. of CSE, NIT, Agartala, India; Dept. of CSE, NIT, Agartala, India; Dept. of CSE, NIT, Agartala, India","2021 IEEE 6th International Conference on Computing, Communication and Automation (ICCCA)","10 Jan 2022","2021","","","266","271","With the advancement in deep learning methods, Generative Adversarial Neural Networks(GANs) have been successful in the application of Computer Vision, Natural Language Processing, and medical. It is observed to have great research potential of using GAN for Image Synthesis. GANs can generate meaningful samples from complex patters or distributions. We present a method to generate Asian Tribal human synthetic stick figures using GAN. With variety of applications the output are limited to low resolution and are unrealistic. The work includes image to image translation where the inputs are low resolution images and as the training progresses, addition of new layers lead to better or high resolution output. For this purpose, it was mandatory to train the generator and discriminator following the principles governing the concept of transfer learning and to fetch evidence of how the generated output resembles the original data set. We also extend additional visualizations of our approach and support our GAN model with images of our generated output. This piece of work includes customized data set as input, where the input images are constructed based on laws of coordinate geometry and spherical polar coordinates and final generated images to illustrate its resemblance with the actual input images. The generated images were quite efficient in faking the discriminator, contributing towards robustness, precision and viability of the stated architecture.","2642-7354","978-1-6654-1473-9","10.1109/ICCCA52192.2021.9666257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9666257","Deep Learning;discriminator;fake images;GAN;generative adversarial networks;generator;image synthesis;real images;warli stick figure","Training;Image resolution;Image color analysis;Image synthesis;Graphics processing units;Data visualization;Computer architecture","computer vision;image resolution;learning (artificial intelligence);natural language processing;neural nets","Image synthesis;warli tribal stick figures;generative adversarial networks;deep learning methods;Natural Language Processing;great research potential;Image Synthesis;complex patters;Asian Tribal human synthetic stick figures;image translation;low resolution images;high resolution output;generated output;GAN model;actual input images","","2","","21","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Text-to-Painting on a Large Variance Dataset with Sequential Generative Adversarial Networks","A. C. Özgen; O. A. Aghdam; H. K. Ekenel","Department of Computer Engineering Istanbul Technical University, SiMiT Lab, Istanbul, Turkey; Department of Computer Engineering Istanbul Technical University, SiMiT Lab, Istanbul, Turkey; Department of Computer Engineering Istanbul Technical University, SiMiT Lab, Istanbul, Turkey","2020 28th Signal Processing and Communications Applications Conference (SIU)","7 Jan 2021","2020","","","1","4","Converting text descriptions to images using Generative Adversarial Networks has become a popular research area. Visually appealing images were generated in recent years successfully. We investigated the generation of artistic images on a custom-built large variance dataset, which includes training images with variations, for example, in shape, color, and content. These variations in images provide originality, which is an important factor for artistic essence. One major characteristic of our work is that we used keywords as image descriptions, instead of sentences. As a network architecture, we proposed a sequential Generative Adversarial Network model, which utilizes several techniques like Wasserstein loss, spectral normalization, and minibatch discrimination to have stable training curves. Ultimately, we were able to generate painting images, which have a variety of styles. We evaluated the quality of generated paintings by using Fréchet Inception Distance score.","2165-0608","978-1-7281-7206-4","10.1109/SIU49456.2020.9302112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302112","Text-to-Image synthesis;Generative Adversarial Networks (GANs);Sequential GANs;Painting generation","Generators;Training;Generative adversarial networks;Gallium nitride;Painting;Image synthesis;Image resolution","feature extraction;image colour analysis;learning (artificial intelligence);neural nets;text analysis","text-to-painting;variance dataset;sequential Generative Adversarial Networks;converting text;popular research area;artistic images;custom-built;artistic essence;image descriptions;network architecture;sequential Generative Adversarial Network model;stable training curves;painting images;generated paintings","","","","31","IEEE","7 Jan 2021","","","IEEE","IEEE Conferences"
"Sub-Axial Vertebral Column Fracture CT Image Synthesis by Progressive Growing Generative Adversarial Networks (PGGANs)","D. Sindhura; R. M. Pai; S. N. Bhat; M. Manohara Pai","Department of Information and Communication Technology, Manipal Academy Of Higher Education, Manipal Institute of Technology, Manipal, India; Department of Information and Communication Technology, Manipal Academy Of Higher Education, Manipal Institute of Technology, Manipal, India; Department of Orthopaeadics Kasturba Medical College,Manipal, Manipal Academy Of Higher Education, Manipal, India; Department of Information and Communication Technology, Manipal Academy Of Higher Education, Manipal Institute of Technology, Manipal, India","2022 International Conference on Distributed Computing, VLSI, Electrical Circuits and Robotics ( DISCOVER)","12 Dec 2022","2022","","","311","315","Orthopaedicians need the assistance of the Deep Learning (DL) model for easy Vertebral Column Fracture Type identification. Deep Learning models require large datasets. Due to the non-availability of large annotated data sets, the DL model needs intensive data augmentation methods. In this proposed research work, Progressive Growing Generative Adversarial Networks (PGGANs) are used to generate synthetic Vertebral Column Fracture (VCF) CT images. The synthetic CT images of VCF generated by PGGANs are high resolution, realistic yet wholly different from the real images. The PGGANs is a multi-stage generative model that generates 512 X 512 CT images that increases the accuracy of the VCF Type identification system. A total of375 vertebral column CT images were utilized for training the model, which were collected from the Spine Clinic, Orthopaedics Department, Kasturba Medical College, Manipal, Manipal Academy of Higher Education, Manipal. Among 375 images, 275 Chance fractures and 100 posterior tension band disruption fracture images were present. To analyse the effect of PGGAN augmentation on VCF type identification, lately VGG16 pre-trained model is implemented. The VGG16 model with PGGAN augmentation got an accuracy of 87.01%, which is more when compared to the model without augmentation. In conclusion, PGGAN generated VCF images are realistic and can be used for data augmentation without privacy restrictions and in VCF type identification DL models for increased performance.","","978-1-6654-8716-0","10.1109/DISCOVER55800.2022.9974676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9974676","CT Images;Deep learning (DL) model;Data Augmentation;Progressive Growing Generative Adversarial Networks (PGGANs);Sub-Axial;Vertebral column Fracture (VCF)","Deep learning;Training;Computed tomography;Computational modeling;Surgery;Very large scale integration;Generative adversarial networks","bone;computerised tomography;deep learning (artificial intelligence);fracture;image segmentation;learning (artificial intelligence);medical image processing;orthopaedics","annotated data sets;Chance fractures;deep learning;DL model;intensive data augmentation methods;Kasturba Medical College;Manipal;multistage generative model;orthopaedics department;PGGAN augmentation;posterior tension band disruption fracture images;progressive growing generative adversarial networks;spine clinic;sub-axial vertebral column fracture CT image synthesis;synthetic CT images;synthetic vertebral column fracture CT images;VCF images;VCF Type identification system;vertebral column fracture type identification;VGG16 model pretraining","","","","16","IEEE","12 Dec 2022","","","IEEE","IEEE Conferences"
"edaGAN: Encoder-Decoder Attention Generative Adversarial Networks for Multi-contrast MR Image Synthesis","O. Dalmaz; B. Saglam; K. Gönç; T. Çukur","National Magnetic Resonance Research Center, Bilkent University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey; Department of Computer Science, Bilkent University, Ankara, Turkey; Neuroscience Program, Sabuncu Brain Research Center, Bilkent University, Ankara, Turkey","2022 9th International Conference on Electrical and Electronics Engineering (ICEEE)","16 May 2022","2022","","","320","324","Magnetic resonance imaging (MRI) is the preferred modality among radiologists in the clinic due to its superior depiction of tissue contrast. Its ability to capture different contrasts within an exam session allows it to collect additional diagnostic information. However, such multi-contrast MRI exams take a long time to scan, resulting in acquiring just a portion of the required contrasts. Consequently, synthetic multi-contrast MRI can improve subsequent radiological observations and image analysis tasks like segmentation and detection. Because of this significant potential, multi-contrast MRI synthesis approaches are gaining popularity. Recently, generative adversarial networks (GAN) have become the de facto choice for synthesis tasks in medical imaging due to their sensitivity to realism and high-frequency structures. In this study, we present a novel generative adversarial approach for multi-contrast MRI synthesis that combines the learning of deep residual convolutional networks and spatial modulation introduced by an attention gating mechanism to synthesize high-quality MR images. We show the superiority of the proposed approach against various synthesis models on multi-contrast MRI datasets.","","978-1-6654-6754-4","10.1109/ICEEE55327.2022.9772555","NVIDIA; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772555","MRI;synthesis;attention;generative;adversarial","Image segmentation;Sensitivity;Image analysis;Image synthesis;Magnetic resonance imaging;Modulation;Generative adversarial networks","biomedical MRI;image segmentation;medical image processing","encoder-decoder attention generative adversarial networks;multicontrast MR image synthesis;magnetic resonance imaging;preferred modality;superior depiction;tissue contrast;different contrasts;exam session;additional diagnostic information;multicontrast MRI exams;required contrasts;subsequent radiological observations;image analysis tasks;multicontrast MRI synthesis approaches;synthesis tasks;medical imaging;generative adversarial approach;deep residual convolutional networks;high-quality MR images;synthesis models;multicontrast MRI datasets","","","","32","IEEE","16 May 2022","","","IEEE","IEEE Conferences"
"FA-GAN: Feature-Aware GAN for Text to Image Synthesis","E. Jeon; K. Kim; D. Kim","Dept. of Computer Science and Engineering, Pohang University of Science and Technology, Korea; Dept. of Computer Science and Engineering, Pohang University of Science and Technology, Korea; Dept. of Computer Science and Engineering, Pohang University of Science and Technology, Korea","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","2443","2447","Text-to-image synthesis aims to generate a photo-realistic image from a given natural language description. Previous works have made significant progress with Generative Adversarial Networks (GANs). Nonetheless, it is still hard to generate intact objects or clear textures (Fig 1). To address this issue, we propose Feature-Aware Generative Adversarial Network (FA-GAN) to synthesize a high-quality image by integrating two techniques: a self-supervised discriminator and a feature-aware loss. First, we design a self-supervised discriminator with an auxiliary decoder so that the discriminator can extract better representation. Secondly, we introduce a feature-aware loss to provide the generator more direct supervision by employing the feature representation from the self-supervised discriminator. Experiments on the MSCOCO dataset show that our proposed method significantly advances the state-of-the-art FID score from 28.92 to 24.58.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506172","Text-to-Image Synthesis;Generative Adversarial Networks;Feature-Aware GAN","Image synthesis;Conferences;Natural languages;Generative adversarial networks;Feature extraction;Generators;Decoding","feature extraction;image retrieval;image segmentation;image texture;learning (artificial intelligence);natural language processing;natural languages;realistic images;recurrent neural nets","Feature-Aware Generative Adversarial Network;high-quality image;self-supervised discriminator;feature-aware loss;feature representation;FA-GAN: feature-aware GAN;text-to-image synthesis;photo-realistic image;given natural language description;Generative Adversarial Networks;GANs;intact objects;clear textures;Fig 1","","2","","21","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Evolution and effectiveness of loss functions in generative adversarial networks","A. S. Saqlain; F. Fang; T. Ahmad; L. Wang; Z. -u. Abidin","School of Control and Computer Engineering, North China Electric Power University, Beijing, China; School of Control and Computer Engineering, North China Electric Power University, Beijing, China; School of Control and Computer Engineering, North China Electric Power University, Beijing, China; Department of Computer Science, Portland State University, Portland, OR, USA; School of Information and Communication Engineering, South West Jiaotong University, Chengdu, China","China Communications","1 Nov 2021","2021","18","10","45","76","Recently, the evolution of Generative Adversarial Networks (GANs) has embarked on a journey of revolutionizing the field of artificial and computational intelligence. To improve the generating ability of GANs, various loss functions are introduced to measure the degree of similarity between the samples generated by the generator and the real data samples, and the effectiveness of the loss functions in improving the generating ability of GANs. In this paper, we present a detailed survey for the loss functions used in GANs, and provide a critical analysis on the pros and cons of these loss functions. First, the basic theory of GANs along with the training mechanism are introduced. Then, the most commonly used loss functions in GANs are introduced and analyzed. Third, the experimental analyses and comparison of these loss functions are presented in different GAN architectures. Finally, several suggestions on choosing suitable loss functions for image synthesis tasks are given.","1673-5447","","10.23919/JCC.2021.10.004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9597636","loss functions;deep learning;machine learning;unsupervised learning;generative adversarial networks (GANs);image synthesis","Generators;Generative adversarial networks;Training;Linear programming;Task analysis;Computer architecture;Games","learning (artificial intelligence);neural nets","GANs;suitable loss functions;generative adversarial networks;generating ability;artificial intelligence;computational intelligence;training mechanism;image synthesis tasks","","4","","","","1 Nov 2021","","","IEEE","IEEE Magazines"
"Research on Generative Adversarial Networks and Their Applications in Image Generation","J. Zhou","Beijing Aidi School, Beijing, China","2022 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)","18 Oct 2022","2022","","","1144","1147","This paper first introduces the basic principles, model structures, and advantages and disadvantages of generative adversarial networks. Then we give a detailed introduction to three application areas of generative adversarial networks in image generation: medical imaging, 3D reconstruction, and image fusion. Finally, the development trend of generative adversarial networks and their applications in the field of image generation prospects.","","978-1-6654-8090-1","10.1109/AEECA55500.2022.9918833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918833","Generative Adversarial Networks;Image Generation;Application Research","Training;Heating systems;Three-dimensional displays;Image synthesis;Focusing;Generative adversarial networks;Market research","image fusion;image reconstruction;stereo image processing","generative adversarial networks;image generation;medical imaging;3D reconstruction;image fusion","","","","5","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"Stacked Generative Adversarial Networks for Image Generation based on U-Net discriminator","W. Feng; Z. Meng; L. Wang","College of Computer, Electronics and Information, Guangxi University, Nanning, Guangxi; College of Computer, Electronics and Information, Guangxi University, Nanning, Guangxi; College of Computer, Electronics and Information, Guangxi University, Nanning, Guangxi","2022 Asia Conference on Algorithms, Computing and Machine Learning (CACML)","19 Aug 2022","2022","","","762","768","Although Generative Adversarial Networks (GANs) are powerful generative models and have shown remarkable success in various tasks recently but suffers from generating high-quality images. In this paper, we proposed a U-Net-based discriminator structure on the network structure of the two-stage Stacked Generative Adversarial Networks (StackGAN++), aiming to generate high-resolution images with actual shapes and textures. To gain more insight from limited datasets, we focused on improving the discriminator's ability to discriminate the real from the fake. The discriminator based on U-Net architecture allows providing details per-pixels and global feedback to the generator to maintain the global coherence of synthetic images and the realistic of local shape and textures. In addition, for the problem that the training effect on a small number of sample datasets is not ide-al, we further improve the quality of the generated samples by transfer learning of model parameters. Compared with the StackGAN++ baseline, experiments show that we have significantly improved the IS and FID evaluation indicators of the ImageNet subset.","","978-1-6654-8290-5","10.1109/CACML55074.2022.00132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852598","component;Generative Adversarial Networks (GANs);U-net;Image Generation;Transfer learning","Training;Machine learning algorithms;Image synthesis;Shape;Transfer learning;Generative adversarial networks;Feature extraction","feature extraction;image representation;learning (artificial intelligence);neural nets","high-quality images;discriminator structure;network structure;high-resolution images;actual shapes;U-Net architecture;synthetic images;local shape;image generation;U-Net discriminator;powerful generative models;two-stage stacked generative adversarial networks;StackGAN++","","","","25","IEEE","19 Aug 2022","","","IEEE","IEEE Conferences"
"Traffic Sign Image Synthesis with Generative Adversarial Networks","H. Luo; Q. Kong; F. Wu","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Chinese Academy of Sciences, Beijing, China","2018 24th International Conference on Pattern Recognition (ICPR)","29 Nov 2018","2018","","","2540","2545","Deep convolutional neural networks (CNN) has achieved state-of-the-art result on traffic sign classification, which plays a key role in intelligent transportation system. However, it usually requires a large number of labeled training data, which is not always available, to guarantee a good performance. In this paper, we propose to synthesize traffic sign images by generative adversarial networks (GANs). It takes a standard traffic sign template and a background image as input to the generative network in GANs, where the template defines which class of traffic sign to include and the background image controls the visual appearance of the synthetic images. Experiments show that our method could generate more realistic traffic sign images than the conventional image synthesis method. Meanwhile, by adding the synthesis images to train a typical CNN for traffic sign classification, we obtained a better accuracy.","1051-4651","978-1-5386-3788-3","10.1109/ICPR.2018.8545787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8545787","","Gallium nitride;Standards;Lighting;Training;Visualization;Image generation;Generative adversarial networks","feedforward neural nets;image classification;learning (artificial intelligence);object detection;traffic engineering computing","traffic sign image synthesis;generative adversarial networks;convolutional neural networks;traffic sign classification;intelligent transportation system;labeled training data;standard traffic sign template;background image;generative network;template defines which class;synthetic images;realistic traffic sign images;conventional image synthesis method;GAN","","6","","19","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"FittingGAN: Fitting image Generation Based on Conditional Generative Adversarial Networks","Y. Li; J. Wang; X. Zhang; Y. Cao","College of Information and Management Science, Henan Agricultural University, Zhengzhou, China; College of Information and Management Science, Henan Agricultural University, Zhengzhou, China; College of Information and Management Science, Henan Agricultural University, Zhengzhou, China; School of Software Engineering, Zhengzhou University, Zhengzhou, China","2019 14th International Conference on Computer Science & Education (ICCSE)","23 Sep 2019","2019","","","741","745","Recent studies have shown remarkable success in image generations using generative adversarial networks (GANs). However, how to deal with the fitting image generation, which is a task that generates a reasonable dressing image containing the input clothes is still an open problem. In this paper, we propose a condition generation model named FittingGAN which can achieve the generation of fitting scenes. The results show that It can generate fitting images with high resolution and realistic details, and FittingGAN have achieved good results in both qualitative and quantitative evaluations.","2473-9464","978-1-7281-1846-8","10.1109/ICCSE.2019.8845499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845499","Generative adversarial networks;Domain Transfer;Image Generation","Generators;Image synthesis;Feature extraction;Generative adversarial networks;Task analysis;Decoding;Convolution","image resolution;neural nets","FittingGAN;fitting image generation;conditional generative adversarial networks;image generations;condition generation model;dressing image","","","","33","IEEE","23 Sep 2019","","","IEEE","IEEE Conferences"
"Research on the Application of Generative Adversarial Networks in Aerial Image Generation","H. X. Cai; X. Y. Zhu; P. C. Wen; L. T. Bai; R. Q. Li; W. Han","AVIC Xi’an Aeronautics Computing Technique Research Institute, Xi’an, China; AVIC Xi’an Aeronautics Computing Technique Research Institute, Xi’an, China; AVIC Xi’an Aeronautics Computing Technique Research Institute, Xi’an, China; AVIC Xi’an Aeronautics Computing Technique Research Institute, Xi’an, China; AVIC Xi’an Aeronautics Computing Technique Research Institute, Xi’an, China; AVIC Xi’an Aeronautics Computing Technique Research Institute, Xi’an, China","2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)","12 Jan 2023","2022","","","416","420","Computer vision is one of the important areas and directions of deep learning research, which requires different approaches to be chosen for different fields due to the complexity and diversity of vision tasks. In the field of aviation, the existing image resources are still far from the real needs due to the influence and constraints of realistic scenes and difficulties of image acquisition. More detailed and comprehensive images can better provide reliable technical support and basis for applications, and then make more accurate decisions on problems, which requires generating more effective images to expand the data. Generative Adversarial Networks (GAN) are the fastest growing and most effective generation method in recent years, so this experiment investigates the application of GAN on aviation data, taking images of airplanes, cars and ships as examples to conduct quantitative research. The effect on the effect of GAN is studied from the perspective of image size, number of images, number of iterations, and different categories of images, in order to obtain better parameter settings for generating effective images, which provides a theoretical and experimental basis for the subsequent application of GAN in the aviation field to generate more images with similar characteristics and solve the problem of insufficient data.","","978-1-6654-6468-0","10.1109/ICICML57342.2022.10009730","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009730","Generative Adversarial Networks;image generation;aerial applications;quantitative research","Deep learning;Computer vision;Image synthesis;Computer network reliability;Generative adversarial networks;Complexity theory;Reliability","aerospace computing;computer vision;deep learning (artificial intelligence)","aerial image generation;aviation data;aviation field;computer vision;deep learning;GAN;generative adversarial networks;image acquisition;image resources;image size;realistic scenes","","","","18","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Comparison on Generative Adversarial Networks –A Study","A. Sharma; N. Jindal; A. Thakur","Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India; Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India; Department of Electronics and Communication, Thapar Institute of Engineering & Technology, Patiala, Punjab, India","2018 First International Conference on Secure Cyber Computing and Communication (ICSCCC)","2 May 2019","2018","","","391","396","Various new deep learning models have been invented, among which generative adversarial networks have gained exceptional prominence in last four years due to its property of image synthesis. GANs have been utilized in diverse fields ranging from conventional areas like image processing, biomedical signal processing, remote sensing, video generation to even off beat areas like sound and music generation. In this paper, we provide an overview of GANs along with its comparison with other networks, as well as different versions of Generative Adversarial Networks.","","978-1-5386-6373-8","10.1109/ICSCCC.2018.8703267","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703267","Machine learning;Generative Adversarial Networks","Generative adversarial networks;Training;Generators;Gallium nitride;Loss measurement;Computational modeling","image processing;learning (artificial intelligence)","deep learning models;image synthesis;GANs;image processing;biomedical signal processing;video generation;music generation;sound generation;generative adversarial networks","","3","","22","IEEE","2 May 2019","","","IEEE","IEEE Conferences"
"Human Sketch Recognition using Generative Adversarial Networks and One-Shot Learning","D. Wadhwa; U. Maharana; D. Shah; V. Yadav; P. Pandey","Department of Information Technology, GGSIPU, New Delhi, India; Department of Information Technology, GGSIPU, New Delhi, India; Department of Information Technology, GGSIPU, New Delhi, India; Department of Information Technology, GGSIPU, New Delhi, India; Department of Information Technology, GGSIPU, New Delhi, India","2019 Twelfth International Conference on Contemporary Computing (IC3)","19 Sep 2019","2019","","","1","6","We introduce a model for face recognition from sketches. The system follows a multi-layered approach and is built by combining sketch to image generation, and face recognition methods. First, sketch to photo generation is achieved by employing cGAN based pix2pix model. Then face recognition is done by One Shot Learning using FaceNet. The sketch recognition model developed through this research is able to yield good results on multiple datasets with the generated images performing with an accuracy close to that with the original images. The average difference from the recognition accuracy as compared to with original images was approximately three percent on the datasets used. Real time recognition using sketches as inputs is also effectively explored through the combination of these two layers of approach.","2572-6129","978-1-7281-3591-5","10.1109/IC3.2019.8844885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844885","generative adversarial networks;one shot learning;sketch recognition;sketch to image generation;conditional generative adversarial network","Face recognition;Training;Image synthesis;Image segmentation;Generative adversarial networks;Task analysis;Generators","face recognition;learning (artificial intelligence)","face recognition;multilayered approach;image generation;photo generation;cGAN based pix2pix model;human sketch recognition;generative adversarial networks;one-shot learning","","2","","34","IEEE","19 Sep 2019","","","IEEE","IEEE Conferences"
"Synthetic Vertebral Column Fracture Image Generation by Deep Convolution Generative Adversarial Networks","S. D. N; R. M. Pai; S. N. Bhat; M. P. M. M","Department of Information and Communication Technology, Manipal Institute of Technology, Manipal Academy Of Higher Education, Manipal, India; Department of Information and Communication Technology, Manipal Institute of Technology, Manipal Academy Of Higher Education, Manipal, India; Department of Orthopaeadics, Kasturba Medical College, Manipal Academy Of Higher Education, Manipal, India; Department of Information and Communication Technology, Manipal Institute of Technology, Manipal Academy Of Higher Education, Manipal, India","2021 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)","7 Dec 2021","2021","","","1","4","In the field of medical imaging, the challenging objective is to generate synthetic, realistic images which resembles the original images. The generated synthetic images would enhance the accuracy of the computer-assisted classification, Decision Support System, which aid the doctor in diagnosis of diseases. The Generative Adversarial Networks (GANs), is a method of data augmentation which can be used to generate synthetic realistic looking images, however low quality images are generated. For AI models, it is challenging tasks to do classification using this low quality images. In this work, generation of high quality synthetic medical image using Deep Convolutional Generative Adversarial Networks (DCGANs) is presented. Data augmentation method by DCGANs is illustrated on the limited dataset of CT (Computed Tomography) images of vertebral column fracture. A total of 340 CT scan images were taken for the study, which comprises of complete burst fracture scans of vertebral column. The evaluation of the generated images was done with Visual Turing Test.","2766-2101","978-1-6654-2849-1","10.1109/CONECCT52877.2021.9622527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622527","Deep Convolutional Generative Adversarial Network;data augmentation;Image synthesis;vertebral column fracture;computed tomography;AI models;Visual Turing Test","Visualization;Image synthesis;Computed tomography;Computational modeling;Training data;Medical services;Generative adversarial networks","biomechanics;bone;computerised tomography;decision support systems;diseases;fracture;image segmentation;medical image processing","synthetic vertebral column fracture image generation;Deep convolution Generative Adversarial Networks;medical imaging;synthetic images;realistic images;original images;computer-assisted classification;synthetic realistic looking images;low quality images;high quality synthetic medical image;Deep Convolutional Generative Adversarial Networks;data augmentation method;CT images;340 CT scan images","","","","12","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"MsCGAN: Multi-scale Conditional Generative Adversarial Networks for Person Image Generation","W. Tang; G. Li; X. Bao; F. Nian; T. Li","Anhui University, Hefei, China; Anhui University, Hefei, China; Anhui University, Hefei, China; Hefei University, Hefei, China; Anhui University, Hefei, China","2020 Chinese Control And Decision Conference (CCDC)","11 Aug 2020","2020","","","1440","1445","To synthesize high-quality person images with arbitrary poses is challenging. In this paper, we propose a novel Multi-scale Conditional Generative Adversarial Networks (MsCGAN), aiming to convert the input conditional person image to a synthetic image of any given target pose, whose appearance and the texture are consistent with the input image. MsCGAN is a multi-scale adversarial network consisting of two generators and two discriminators. One generator transforms the conditional person image into a coarse image of the target pose globally, and the other is to enhance the detailed quality of the synthetic person image through a local reinforcement network. The outputs of the two generators are then merged into a synthetic, discriminant and high-resolution image. On the other hand, the synthetic image is downsampled to multiple resolutions as the input to multi-scale discriminator networks. The proposed multi-scale generators and discriminators handling different levels of visual features can benefit to synthesizing high-resolution person images with realistic appearance and texture. Experiments are conducted on the Market-1501 and DeepFashion datasets to evaluate the proposed model, and both qualitative and quantitative results demonstrate the superior performance of the proposed MsCGAN.","1948-9447","978-1-7281-5855-6","10.1109/CCDC49329.2020.9164755","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164755","person image generation;multi-scale discriminators;generative adversarial networks;image synthesis","Generators;Gallium nitride;Image generation;Generative adversarial networks;Image resolution;Pose estimation;Visualization","feature extraction;image classification;image representation;image resolution;image texture;statistical analysis","MsCGAN;person image generation;high-quality person images;arbitrary poses;multiscale conditional generative adversarial networks;input conditional person image;synthetic image;input image;multiscale adversarial network;coarse image;synthetic person image;local reinforcement network;synthetic resolution image;discriminant resolution image;high-resolution image;multiscale discriminator networks;multiscale generators;synthesized high-resolution person images;Market-1501 dataset;DeepFashion dataset","","4","","31","IEEE","11 Aug 2020","","","IEEE","IEEE Conferences"
"Improving Generative Adversarial Networks with Adaptive Control Learning","X. Ma; R. Jin; K. -A. Sohn; J. Paik; J. Sun; T. -S. Chung","Dept. of Software, Ajou University, Suwon, Korea; School of Computer Science and Software Engineering, Tianjin Polytechnic University, Tianjin, China; Dept. of Software, Ajou University, Suwon, Korea; School of Computer Science and Software Engineering, Tianjin Polytechnic University, Tianjin, China; Dept. of Software, Ajou University, Suwon, Korea; Dept. of Software, Ajou University, Suwon, Korea","2018 IEEE Visual Communications and Image Processing (VCIP)","25 Apr 2019","2018","","","1","4","Generative adversarial networks (GANs) are well known both for being unstable to train and for the problem of mode collapse, particularly when trained on data collections containing a diverse set of visual objects. This study introduces an adaptive hyper-parameter learning procedure for GANs as an alternative to the existing static approach. The proposed procedure is designed to mitigate the impact of instability and saturation in the original by dynamically adjusting the ratio of the training steps of both the generator and discriminator. To accomplish this, we track and analyze stable training curves of relatively narrow datasets and use them as the target fitting lines when training more diverse data collections. Experimental results show that the proposed model improves the stability and generates more realistic images.","1018-8770","978-1-5386-4458-4","10.1109/VCIP.2018.8698669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698669","Generative adversarial networks;image synthesis;adaptive algorithm","Training;Gallium nitride;Generators;Generative adversarial networks;Adaptive control;Adaptation models;Stability analysis","adaptive control;learning (artificial intelligence)","generative adversarial networks;adaptive control learning;mode collapse;diverse set;visual objects;adaptive hyper-parameter learning procedure;training steps;generator;discriminator;stable training curves;diverse data collections;GAN","","3","","14","IEEE","25 Apr 2019","","","IEEE","IEEE Conferences"
"Optimizations of Ternary Generative Adversarial Networks","K. Nakamura; H. Nakahara","Information and Communications Engineering, Tokyo Institute of Technology, Tokyo, Japan; Information and Communications Engineering, Tokyo Institute of Technology, Tokyo, Japan","2022 IEEE 52nd International Symposium on Multiple-Valued Logic (ISMVL)","24 Jun 2022","2022","","","158","163","Generative adversarial networks (GANs), which can generate and transform data, have been attracting attention. However, the model must be lightweight and fast when applied in the field. In terms of model weight reduction, B-DCGAN, which restricts the value of the weights to {−1, +1}, has already been proposed. We propose a ternary GAN using the ternary representation {−1, 0, +1} and an $\alpha$-layer which makes the learning between generator and discriminator more competitive. It succeeded in improving the quality of the output image considerably while maintaining the almost same memory usage as that of B-DCGAN.","2378-2226","978-1-6654-2395-3","10.1109/ISMVL52857.2022.00031","Energy and Industrial Technology Development Organization (NEDO); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9797400","Ternary;alpha layer;layer;GAN;image synthesis;TinyML;machine learning;quantization;generative adversarial networks","Degradation;Memory management;Transforms;Generative adversarial networks;Generators;Optimization","image processing;neural nets;optimisation;ternary logic","GANs;model weight reduction;B-DCGAN;ternary GAN;ternary representation;optimizations;ternary generative adversarial networks","","1","","12","IEEE","24 Jun 2022","","","IEEE","IEEE Conferences"
"Evaluation of Generative Adversarial Network for Human Face Image Synthesis","I. Marin; S. Gotovac; M. Russo","Faculty of Science, University of Split, Split, Croatia; Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture, University of Split, Split, Croatia; Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture, University of Split, Split, Croatia","2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","28 Oct 2020","2020","","","1","6","Meaningful and objective evaluation metric for fair model comparison is crucial for further scientific progress in the field of deep generative modeling. Despite the significant progress and impressive results obtained by Generative Adversarial Networks in recent years, the problem of their objective evaluation remains open. In this paper, we give an overview of qualitative and quantitative evaluation measures most frequently used to assess the quality of generated images and learned representations of an adversarial network together with the empirical comparison of their performance on the problem of human face image synthesis. It is shown that evaluation scores of the two most widely accepted quantitative metrics, Inception Score (IS) and Fréchet Inception Distance (FID), do not correlate. The IS is not an appropriate evaluation metric for a given problem, but FID shows good performance that correlates well with a visual inspection of generated samples. The qualitative evaluation can be used to complement results obtained with quantitative evaluation - to gain further insight into the learned data representation and detect possible overfitting.","1847-358X","978-953-290-099-6","10.23919/SoftCOM50211.2020.9238203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238203","Generative Adversarial Networks;Evaluation;Inception Score;Fréchet Inception Distance;Latent Space Exploration","Measurement;Visualization;Image synthesis;Computer architecture;Benchmark testing;Generative adversarial networks;Telecommunications","face recognition;image representation;learning (artificial intelligence);neural nets","Generative Adversarial network;human face image synthesis;objective evaluation;fair model comparison;scientific progress;deep generative modeling;Generative Adversarial Networks;quantitative evaluation;empirical comparison;evaluation scores;appropriate evaluation metric;qualitative evaluation","","1","","22","","28 Oct 2020","","","IEEE","IEEE Conferences"
"Self-Attention Generative Adversarial Networks for Times Series VHR Multispectral Image Generation","F. Chaabane; S. Réjichi; F. Tupin","COSIM laboratory, SUP'COM, Carthage University, Tunisia; COSIM laboratory, SUP'COM, Carthage University, Tunisia; Department of Image and Signal Processing, Telecom ParisTech, France","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4644","4647","Recently classical deep learning approaches are commonly used to perform spatial and temporal classification especially for Very High Resolution (VHR) images. They learn from existing low resolution or undersized datasets because of the availability and prices of VHR remote sensing images. Thus, they have witnessed a conspicuous success because it is quite challenging to classify high-dimensional multispectral time series data with few labeled samples. It is also difficult to simulate high quality samples having the same features as the real ones. It goes without saying that the introduction of GANs (Generative Adversarial Network) models as an unsupervised learning method, has allowed the extraction of accurate representations of the data via latent codes and backpropagation techniques. However, it is difficult to acquire high-quality samples with unwanted noises and uncontrolled divergences. To generate high-quality multispectral time series samples, a Self-Attention Generative Adversarial Network (SAGAN) is proposed in this work. SAGAN allows attention-driven, long-range dependency modeling for VHR Multispectral time series image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations which improves training dynamics. The proposed SAGAN performs better than traditional GANs, boosting the best inception score. The main contribution of this work is the use of one of the new generation of learning techniques, SAGAN, for Times Series VHR Multispectral Image Generation. SAGAN has been recently used only for single image generation.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553597","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553597","Self-Attention Generative Adversarial Networks;Multispectral VHR time series;deep learning techniques;etc.","Training;Image synthesis;Time series analysis;Generative adversarial networks;Feature extraction;Spatial resolution;Task analysis","backpropagation;feature extraction;geophysical image processing;geophysical signal processing;image classification;image resolution;learning (artificial intelligence);remote sensing;time series;unsupervised learning","GANs models;Generative Adversarial Network;unsupervised learning method;high-quality samples;high-quality multispectral time series samples;SAGAN;VHR Multispectral time series image generation tasks;high-resolution details;lower-resolution feature maps;Times Series VHR Multispectral Image Generation;single image generation;Generative Adversarial networks;classical deep learning approaches;High Resolution images;VHR remote sensing images;high-dimensional multispectral time series data;high quality samples","","1","","10","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Improving Image Synthesis Quality in Multi-Contrast MRI Using Transfer Learning via Autoencoders","S. Y. Selçuk; O. Dalmaz; S. U. H. Dar; T. Çukur","National Magnetic Resonance Research Center, Bilkent University, Ankara, Turkey; National Magnetic Resonance Research Center, Bilkent University, Ankara, Turkey; National Magnetic Resonance Research Center, Bilkent University, Ankara, Turkey; Neuroscience Program, Sabuncu Brain Research Center, Bilkent University, Ankara, Turkey","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","The capacity of magnetic resonance imaging (MRI) to capture several contrasts within a session enables it to obtain increased diagnostic information. However, such multi-contrast MRI tests take a long time to scan, resulting in acquiring just a part of the essential contrasts. Synthetic multi-contrast MRI has the potential to improve radiological observations and consequent image analysis activities. Because of its ability to generate realistic results, generative adversarial networks (GAN) have recently been the most popular choice for medical imaging synthesis. This paper proposes a novel generative adversarial framework to improve the image synthesis quality in multi-contrast MRI. Our method uses transfer learning to adapt pre-trained autoencoder networks to the synthesis task and enhances the image synthesis quality by initializing the training process with more optimal network parameters. We demonstrate that the proposed method outperforms competing synthesis models by 0.95 dB on average on a well-known multi-contrast MRI dataset.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864750","Multi-contrast MRI;autoencoder;transfer learning;generative adversarial networks","Training;Measurement;Image synthesis;Magnetic resonance imaging;Transfer learning;Signal processing;Generative adversarial networks","biomedical MRI;image segmentation;learning (artificial intelligence);medical image processing;neural nets","medical imaging synthesis;image synthesis quality;transfer learning;multicontrast MRI dataset;magnetic resonance imaging;multicontrast MRI tests;consequent image analysis;generative adversarial networks;pretrained autoencoder networks","","","","22","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"Synthesizing Images from Hand-Drawn Sketches using Conditional Generative Adversarial Networks","B. Kuriakose; T. Thomas; N. E. Thomas; S. J. Varghese; V. A. Kumar","Dept. of Computer Science and Engineering, Saintgits College of Engineering, Kottayam, Kerala, India; Dept. of Computer Science and Engineering, Saintgits College of Engineering, Kottayam, Kerala, India; Dept. of Computer Science and Engineering, Saintgits College of Engineering, Kottayam, Kerala, India; Dept. of Computer Science and Engineering, Saintgits College of Engineering, Kottayam, Kerala, India; Dept. of Computer Science and Engineering, Saintgits College of Engineering, Kottayam, Kerala, India","2020 International Conference on Electronics and Sustainable Communication Systems (ICESC)","4 Aug 2020","2020","","","774","778","Today, Technology have remarkable charm in the area of Computer Graphics and Vision. Producing absolute images from the poor hand-drawn sketches is a very demanding and laborious task in this area. Hand-drawn sketch recognition is widely used in sketch based image and video retrieval, manipulations and reorganizations. In S ketch to image synthesis, the sketches are translated to realistic images with the use of a generative model. An image is put forward to image translation network that involves in producing a synthesized image from the input sketch via an adversarial process. A novel Conditional Generative Adversarial Network (cGANs) which is an extension of Generative Adversarial Networks (GANs) is used to produce the images with some sort of conditions or attributes. In this work, the implementation of cGANs for synthesizing the images from hand-drawn sketches gives a remarkable output. The performance of the proposed sketch to image translation network was excellent and appreciable.","","978-1-7281-4108-4","10.1109/ICESC48915.2020.9155550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9155550","Adversarial process;Generative Adversarial Networks;Image synthesis","Gallium nitride;Generators;Generative adversarial networks;Image generation;Training;Computational modeling;Neural networks","computer graphics;feature extraction;image recognition;realistic images;video retrieval","cGAN;Microsoft Xbox;generative model;realistic images;image synthesis;sketch based image;hand-drawn sketch recognition;absolute images;image translation network;conditional generative adversarial network;adversarial process;input sketch;synthesized image","","1","","15","IEEE","4 Aug 2020","","","IEEE","IEEE Conferences"
"Single Image HDR Reconstruction Using Generative Adversarial Networks","Z. Wei; J. Xu","State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China","2021 International Conference on Culture-oriented Science & Technology (ICCST)","13 Dec 2021","2021","","","134","138","The advances in GANs have paved the way for various methods of high dynamic range (HDR) image reconstruction. In this paper, we use the structural advantages of GAN to infer natural HDR images and reconstruct missing information from a single exposure low dynamic range(LDR) image in an end-to-end fashion, which extends the dynamic range of a given image to generate HDR image. Furthermore, we propose a novel dense feedback model and the feedback mechanism, which can make use of the high-level information to refine the shallow information in the top-down feedback stream through the global feedback and the local feedback connection. The dense connections in the forward-pass enable feature-reuse and comprehensively learn complex nonlinear relationships from LDR to HDR mapping. Experiment results demonstrate proposed method produces superior performance compared to existing state-of-the-art approaches.","","978-1-6654-4254-1","10.1109/ICCST53801.2021.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9637664","deep learning;high dynamic range imaging;inverse tone mapping;generative adversarial networks","Multiplexing;Degradation;Image synthesis;Dynamic range;Streaming media;Generative adversarial networks;Image restoration","feature extraction;image reconstruction;image resolution;learning (artificial intelligence);neural nets","HDR mapping;generative adversarial networks;GANs;high dynamic range image reconstruction;single exposure low dynamic range image;HDR image;image HDR reconstruction;LDR mapping;dense feedback model;feature-reuse","","","","14","IEEE","13 Dec 2021","","","IEEE","IEEE Conferences"
"Cyber Attacks Detection Based on Generative Adversarial Networks","A. Shi","Department of Electronics and Communications Engineering, Beijing Jiaotong University, Beijing, China","2021 2nd Asia Conference on Computers and Communications (ACCC)","24 Jan 2022","2021","","","111","114","The development of the Internet of things (IoT) and other modern technologies brings not only great convenience and intelligence to society but also the severe problem of cyber security. In recent years, a large number of network attacks, especially botnet and distributed denial of service (DDoS) attacks which are difficult to detect, have seriously affected the normal operation of social work. Solutions and methods based on artificial intelligence techniques are therefore proposed to help detect cyberattacks. In this paper, the application of GAN in the field of network security is discussed. GAN has been one of the most significant deep learning models in recent years. It consists of a generative model and a discriminative model which can be combined to produce a dynamic game system. GAN has been applied in various fields such as image generation, speech processing, data enhancement, and cyberattack detection. This paper demonstrates the basic working principle and infrastructure of GAN, and also focuses on how the GAN model assists with cyber intrusion detection.","","978-1-6654-0743-4","10.1109/ACCC54619.2021.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681469","generative adversarial networks (GAN);deep learning model;cyber-attack detection;anomaly detection","Deep learning;Image synthesis;Intrusion detection;Games;Network security;Generative adversarial networks;Denial-of-service attack","computer crime;deep learning (artificial intelligence);invasive software","distributed denial of service attacks;artificial intelligence;network security;deep learning;generative model;discriminative model;GAN;cyber intrusion detection;cyber attacks detection;generative adversarial networks;cyber security;botnet;DDoS attacks","","","","12","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"DepthwiseGANs: Fast Training Generative Adversarial Networks for Realistic Image Synthesis","M. Ngxande; J. -R. Tapamo; M. Burke","CSIR Defence, Peace, Safety and Security, Optronic Sensor Systems, Pretoria, South Africa; School of Computer Engineering, University of Kwa-Zulu Natal, Durban, South Africa; Mobile Intelligent Autonomous Systems, Council for Scientific and Industrial Research, Pretoria, South Africa","2019 Southern African Universities Power Engineering Conference/Robotics and Mechatronics/Pattern Recognition Association of South Africa (SAUPEC/RobMech/PRASA)","2 May 2019","2019","","","111","116","Recent work has shown significant progress in the direction of synthetic data generation using Generative Adversarial Networks (GANs). GANs have been applied in many fields of computer vision including text-to-image conversion, domain transfer, super-resolution, and image-to-video applications. In computer vision, traditional GANs are based on deep convolutional neural networks. However, deep convolutional neural networks can require extensive computational resources because they are based on multiple operations performed by convolutional layers, which can consist of millions of trainable parameters. Training a GAN model can be difficult and it takes a significant amount of time to reach an equilibrium point In this paper, we investigate the use of depthwise separable convolutions to reduce training time while maintaining data generation performance. Our results show that a DepthwiseGAN architecture can generate realistic images in shorter training periods when compared to a StarGan architecture, but that model capacity still plays a significant role in generative modelling. In addition, we show that depthwise separable convolutions perform best when only applied to the generator. For quality evaluation of generated images, we use the Fréchet Inception Distance (FID), which compares the similarity between the generated image distribution and that of the training dataset.","","978-1-7281-0369-3","10.1109/RoboMech.2019.8704766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8704766","Synthetic Data;GANs;Depthwise Separable Convolution;FID","Generators;Training;Face;Generative adversarial networks;Computational modeling;Computer architecture","computer vision;convolutional neural nets;realistic images","realistic image synthesis;synthetic data generation;image-to-video applications;deep convolutional neural networks;convolutional layers;GAN model;depthwise separable convolutions;data generation performance;realistic images;generative modelling;generated images;generated image distribution;generative adversarial networks;computer vision;depthwiseGAN architecture;text-to-image conversion","","3","","31","IEEE","2 May 2019","","","IEEE","IEEE Conferences"
"Synthesis of Images by Two-Stage Generative Adversarial Networks","Q. Huang; P. J. B. Jackson; M. D. Plumbley; W. Wang","Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK; Centre for Vision, Speech and Signal Processing, University of Surrey, Guildford, UK","2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 Sep 2018","2018","","","1593","1597","In this paper, we propose a divide-and-conquer approach using two generative adversarial networks (GANs) to explore how a machine can draw colorful pictures (bird) using a small amount of training data. In our work, we simulate the procedure of an artist drawing a picture, where one begins with drawing objects' contours and edges and then paints them different colors. We adopt two GAN models to process basic visual features including shape, texture and color. We use the first GAN model to generate object shape, and then paint the black and white image based on the knowledge learned using the second GAN model. We run our experiments on 600 color images. The experimental results show that the use of our approach can generate good quality synthetic images, comparable to real ones.","2379-190X","978-1-5386-4658-8","10.1109/ICASSP.2018.8461984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8461984","Generative adversarial networks;conditional;image generation","Gallium nitride;Birds;Generative adversarial networks;Shape;Image color analysis;Training;Visualization","feature extraction;image colour analysis;object detection;visual databases","color images;divide-and-conquer approach;Image synthesis;object contour;object edge;visual features;synthetic image quality;knowledge learning;two-stage generative adversarial networks;white image;black image;object shape;GAN model","","2","","26","IEEE","13 Sep 2018","","","IEEE","IEEE Conferences"
"A State-of-the-Art Review on Image Synthesis With Generative Adversarial Networks","L. Wang; W. Chen; W. Yang; F. Bi; F. R. Yu","Mine Digitization Engineering Research Center of the Ministry of Education, China University of Mining and Technology, Xuzhou, China; Information Engineering College, Beijing Institute of Petrochemical Technology, Beijing, China; Mine Digitization Engineering Research Center of the Ministry of Education, China University of Mining and Technology, Xuzhou, China; Mine Digitization Engineering Research Center of the Ministry of Education, China University of Mining and Technology, Xuzhou, China; School of Information Technology, Carleton University, Ottawa, Canada","IEEE Access","14 Apr 2020","2020","8","","63514","63537","Generative Adversarial Networks (GANs) have achieved impressive results in various image synthesis tasks, and are becoming a hot topic in computer vision research because of the impressive performance they achieved in various applications. In this paper, we introduce the recent research on GANs in the field of image processing, including image synthesis, image generation, image semantic editing, image-to-image translation, image super-resolution, image inpainting, and cartoon generation. We analyze and summarize the methods used in these applications which have improved the generated results. Then, we discuss the challenges faced by GANs and introduce some methods to deal with these problems. We also preview some likely future research directions in the field of GANs, such as video generation, facial animation synthesis and 3D face reconstruction. The purpose of this review is to provide insights into the research on GANs and to present the various applications based on GANs in different scenarios.","2169-3536","","10.1109/ACCESS.2020.2982224","National Natural Science Foundation of China(grant numbers:51874300,51874299); National Natural Science Foundation of China and Shanxi Provincial People’s Government Jointly Funded Project of China for Coal Base and Low Carbon(grant numbers:U1510115); Open Research Fund of Key Laboratory of Wireless Sensor Network and Communication, Shanghai Institute of Microsystem and Information Technology, Chinese Academy of Sciences(grant numbers:20190902,20190913); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043519","Generative adversarial networks;image synthesis;image-to-image translation;image editing;cartoon generation","Image synthesis;Generative adversarial networks;Training;Face;Task analysis;Generators","computer vision;image resolution;image restoration;neural nets;stereo image processing","GANs;image synthesis tasks;computer vision research;image processing;image generation;image semantic editing;image-to-image translation;image super-resolution;image inpainting;cartoon generation;generative adversarial networks","","52","1","140","CCBY","20 Mar 2020","","","IEEE","IEEE Journals"
"Image Synthesis and Editing with Generative Adversarial Networks (GANs): A Review","W. Li","Department of Computer Science, George Mason University, Fairfax, VA, US","2021 Fifth World Conference on Smart Trends in Systems Security and Sustainability (WorldS4)","19 Aug 2021","2021","","","65","70","Recently, as many deep learning models are emerging, deep learning has achieved great success in the field of artificial intelligence(AI). Especially, the Generative adversarial networks (GANs) based on zero-sum game theory has become a new research hot spot in the field of deep learning. The significance of the GAN model is that it can generate realistic data through unsupervised learning. Based on the conceptual and theoretical framework of the generative adversarial network, GANs models and their application result in tremendous success among different areas, especially in image synthesis and editing. This paper visualizes the data structures of various kinds of GANs models in 3D and discusses the variational GAN models with respect to their improvements in the applications. As the GANs have superior learning ability, strong plasticity, great potential for improvement, and a wide application range, this paper prospects the possible applications of the GANs in the near future.","","978-1-6654-0096-1","10.1109/WorldS451998.2021.9514052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9514052","Image Synthesis;Image Editing;Neural Networks;Deep Learning;Generative Adversarial Networks","Deep learning;Solid modeling;Three-dimensional displays;Image synthesis;Generative adversarial networks;Data models;Security","deep learning (artificial intelligence);game theory;image processing;unsupervised learning","editing;deep learning models;generative adversarial networks;zero-sum game theory;unsupervised learning;image synthesis;variational GAN models;superior learning ability;3D","","","","24","IEEE","19 Aug 2021","","","IEEE","IEEE Conferences"
"GAN-Based Realistic Gastrointestinal Polyp Image Synthesis","A. Sams; H. H. Shomee","Bangladesh University of Engineering and Technology, Dhaka; Brac University, Dhaka","2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)","26 Apr 2022","2022","","","1","4","Polyps in the gastrointestinal (GI) tract in the human body are one of the most significant symptoms of gastric and colorectal cancer and some other diseases. This paper proposes Generative Adversarial Networks (GANs) based methods that first use a StyleGAN2-ada to generate random polyp masks, which are used to create composite images with healthy GI images. Then a conditional GAN is used to translate these composite images into synthetic polyp images. The proposed approach can produce a high amount of realistic GI polyp images and can increase F1-score and IoU in polyp detection by around 4% when used in the training phase of the YOLOv4 object detector.","1945-8452","978-1-6654-2923-8","10.1109/ISBI52829.2022.9761447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761447","Detection;polyp;generative adversarial networks;synthesis","Training;Image synthesis;Detectors;Generative adversarial networks;Gastrointestinal tract;Cancer;Biomedical imaging","biological organs;cancer;computerised tomography;diseases;endoscopes;gallium compounds;learning (artificial intelligence);medical image processing","GAN-based realistic gastrointestinal polyp image synthesis;polyps;gastrointestinal tract;human body;colorectal cancer;Generative Adversarial Networks based methods;GANs;StyleGAN2-ada;random polyp masks;composite images;healthy GI images;conditional GAN;synthetic polyp images;realistic GI polyp images;polyp detection","","","","11","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"High Resolution SAR Image Synthesis with Hierarchical Generative Adversarial Networks","H. Huang; F. Zhang; Y. Zhou; Q. Yin; W. Hu","College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, P.R. China","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","2782","2785","Generative adversarial network (GAN) is an artificial neural network based on unsupervised learning method. Due to its powerful model representation capabilities, GAN has been introduced to synthesize synthetic aperture radar (SAR) image data, for the real sample is difficult to acquire. Large-scale, high-resolution SAR images play an important role in promoting SAR applications, such as automatic target recognition and image interpretation. However, on account of the difficult training problem of GAN network, especially for SAR images with speckle noise, it is difficult to obtain high-resolution SAR images by simply transfer the net from optical image. Recent studies in other image fields have shown that hierarchical structure is an effective and useful way to decompose a generation task into several smaller subtasks. How to obtain more high-resolution SAR images from limited original samples through GAN is the target of our research. Therefore, in this paper, we introduce a hierarchical GAN network model to generate SAR images, through the multi-stage network, gradually improve the quality of the generated image, and finally obtain high-resolution images. The type and aspect of generated images are determined by the input of condition vectors in the last two stages. In addition, we introduce the triple loss, in which the background loss is used to imitating background clutter noise of SAR image, the condition loss is to make the generated images' type and aspect become controllable, and the global loss for getting higher image generation quality. The generated images show high similarity with the real samples.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8900494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8900494","Generative adversarial network(GAN);synthetic aperture radar (SAR);SAR simulator;automatic target recognition (ATR);triple loss","Generative adversarial networks;Radar polarimetry;Gallium nitride;Image resolution;Synthetic aperture radar;Solid modeling;Data models","image resolution;neural nets;radar imaging;synthetic aperture radar;unsupervised learning","high resolution SAR image synthesis;hierarchical generative adversarial networks;generative adversarial network;artificial neural network;synthetic aperture radar image data;high-resolution SAR images;SAR applications;automatic target recognition;image interpretation;optical image;image fields;hierarchical GAN network model;high-resolution images;generated images;image generation quality;unsupervised learning method","","10","","8","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Semi-Supervised Generative Learning with Extended Distribution Matching for Class-Conditional Image Synthesis","X. Huo; G. Deng; S. Wu; Z. Yu; P. Li","School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; Guangdong Zhile Technology Company Limited","2022 IEEE International Conference on Multimedia and Expo (ICME)","26 Aug 2022","2022","","","1","6","Generative Adversarial Network (GAN)-based models have made remarkable progress in high-fidelity image synthesis. However, the performance of class-conditional image synthesis may significantly deteriorate for the case where a limited number of labeled training samples are available. To reduce the dependence on labeled data, we propose a semi-supervised GAN with Extended Distribution Matching, and our model is referred to as EDM-GAN. To prevent a class-conditional discriminator from overfitting the limited labeled data, we perform a transformation of random regional replacement on both real and synthesized samples. By matching the extended distributions, the discriminator is encouraged to focus more on the spatial regions that contain certain objects, while at the same time a class-conditional generator is induced to capture precise class semantics. The adversarial training process can be effectively stabilized and converges to a better solution. Our experimental results on multiple standard benchmarks demonstrate consistent performance gains in synthesis quality and class-semantic accuracy.","1945-788X","978-1-6654-8563-0","10.1109/ICME52920.2022.9859759","National Natural Science Foundation of China(grant numbers:62072189); Natural Science Foundation of Guangdong Province(grant numbers:2020A1515010484); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859759","Image synthesis;generative adversarial networks;semi-supervised learning;distribution matching","Training;Representation learning;Image synthesis;Semantics;Performance gain;Benchmark testing;Generative adversarial networks","image classification;semi-supervised learning (artificial intelligence)","high-fidelity image synthesis;class-conditional image synthesis;semisupervised GAN;EDM-GAN;class-conditional discriminator;class-conditional generator;class semantics;adversarial training process;semisupervised generative learning;generative adversarial network-based models;extended distribution matching","","","","31","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Teacher-Supervised Generative Adversarial Networks","Y. Gan; T. Xiang; H. Liu; M. Ye; D. Liu","Chongqing University, Chongqing, China; Chongqing University, Chongqing, China; Chongqing University, Chongqing, China; University of Electronic Science and Technology of China, Chengdu, China; University of Shanghai for Science and Technology, Shanghai, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Although generative adversarial networks (GANs) show impressive effects on image generation, existing GANs suffer an unstable training process, and thus result in poor image quality sometimes. To solve this problem, we first introduce a supervision mechanism into GANs and propose a teacher-supervised GAN (GAN-T) model. Specifically, we design a teacher supervision mechanism to inspect whether the features of generated images are as real as those of real images. If not, we add action into the generator. The action takes the encoding of the real image as prior knowledge to guide the generation of samples. We then apply our proposed method to existing GANs to show its compatibility with them. Finally, we conduct extensive experiments on the tasks of noise-to-image generation and image translation, and experimental results show that our proposed method can significantly stabilize the training process of the generator and improve the quality of generated images.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428290","National Natural Science Foundation of China; Natural Science Foundation of Chongqing; China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428290","GANs;image synthesis;teacher supervision mechanism;prior knowledge","Training;Image quality;Image coding;Image synthesis;Conferences;Knowledge based systems;Generative adversarial networks","deep learning (artificial intelligence);image resolution;supervised learning","teacher-supervised GAN model;GAN-T;noise-to-image generation;image translation;teacher-supervised generative adversarial networks;image quality","","1","","23","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"Generating High-Resolution Fire Images with Controllable Attributes via Generative Adversarial Networks","N. Q. Dung; H. Kim","Electrical and Computer Engineering, Inha University, Korea; Electrical and Computer Engineering, Inha University, Korea","2022 22nd International Conference on Control, Automation and Systems (ICCAS)","9 Jan 2023","2022","","","348","353","Obtaining realistic fire images using deep learning models and recent versions of Generative adversarial networks (GAN) has been proven to be a difficult task due to the unnatural appearance of the generated results. This paper provides a novel approach based on StarGANv2 to generate fire kernels from any input provided as a reference. In addition, a deep learning-based image blending technique performs the migration of the fire kernels to the target scenes. By using any input as a reference, the generated fire image could be controlled to accommodate different environmental factors, resulting in a diverse but equally pseudo-real synthetic dataset. The proposed method generates images that achieve better FID and LPIPS values than StarGANv2 for both a public dataset (AI Hub) and a privately-owned dataset (Visionin). In addition, YOLOv4 is used as a fire detection model to evaluate the synthetic data on improving the performance of the detected network. Compared to the model trained on the real data, the model trained on the combined dataset outperforms 2%~14% higher.","2642-3901","978-89-93215-24-3","10.23919/ICCAS55662.2022.10003687","K2; National Research Foundation of Korea; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003687","Attention;Generative adversarial network;Image blending;Image synthesis","Training;Shape;Image synthesis;Generative adversarial networks;Data models;Environmental factors;Kernel","image classification;image coding;image segmentation;learning (artificial intelligence);statistical analysis","controllable attributes;deep learning models;deep learning-based image blending technique;detected network;fire detection model;fire kernels;generated fire image;generated results;generating high-resolution fire images;Generative adversarial networks;pseudoreal synthetic dataset;realistic fire images;StarGANv2","","","","23","","9 Jan 2023","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks Based on Dynamic Word-Level Update for Text-to-Image Synthesis","Z. Wang; L. Liu; H. Zhang; Y. Ma; H. Cui; Y. Chen; H. Kong","School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; Institute of Information Science and Electrical Engineering, Shandong Jiaotong University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China","2022 7th International Conference on Image, Vision and Computing (ICIVC)","19 Sep 2022","2022","","","641","647","The traditional fine-grained generation adversarial networks pay less attention to the importance evaluation of word-level information for text-to-image synthesis. Incorrect importance rating of word-level information may skew image generation to less critical direction and affect the generation of key features. In this paper, a novel generative adversarial network based on dynamic word-level update is proposed to solve the above problem by dynamically updating the different importance of each word in the image generation stage. The assignment module with dynamic weights is designed to update text features and image features by communicating the information of text and image. This module enables the importance rating of words to be updated. In addition, the mixed zero-center gradient penalty function and visual loss function are proposed to optimize generative adversarial networks based on dynamic word-level update. The mixed zero-center gradient penalty function allows the generator to generate image with high semantic consistency and ensures the stability of the training process. The visual loss function further improves the visual effect of the generated image by narrowing the difference between the real image and the generated image. Extensive experiments on public benchmark datasets demonstrate that the proposed method outperforms the state-of-the-art methods.","","978-1-6654-6734-6","10.1109/ICIVC55077.2022.9886095","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9886095","text-to-image synthesis;deep feature learning;dynamic weighting network;hierarchical image generation","Training;Image synthesis;Semantics;Benchmark testing;Generative adversarial networks;Visual effects;Generators","feature extraction;gradient methods;learning (artificial intelligence);text analysis","text-to-image synthesis;fine-grained generation adversarial networks;word-level information;incorrect importance rating;novel generative adversarial network;dynamic word-level update;image generation stage;text features;image features;zero-center gradient penalty function;visual loss function;generative adversarial networks","","","","19","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Multi-Modality Generative Adversarial Networks with Tumor Consistency Loss for Brain MR Image Synthesis","B. Xin; Y. Hu; Y. Zheng; H. Liao","Institute of Biomedical Engineering, Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Tencent Youtu Lab, Shenzhen, China; Tencent Youtu Lab, Shenzhen, China; Institute of Biomedical Engineering, Graduate School at Shenzhen, Tsinghua University, Shenzhen, China","2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)","22 May 2020","2020","","","1803","1807","Magnetic Resonance (MR) images of different modalities can provide complementary information for clinical diagnosis, but whole modalities are often costly to access. Most existing methods only focus on synthesizing missing images between two modalities, which limits their robustness and efficiency when multiple modalities are missing. To address this problem, we propose a multi-modality generative adversarial network (MGAN) to synthesize three high-quality MR modalities (FLAIR, T1 and T1ce) from one MR modality T2 simultaneously. The experimental results show that the quality of the synthesized images by our proposed methods is better than the one synthesized by the baseline model, pix2pix. Besides, for MR brain image synthesis, it is important to preserve the critical tumor information in the generated modalities, so we further introduce a multi-modality tumor consistency loss to MGAN, called TC-MGAN. We use the synthesized modalities by TC-MGAN to boost the tumor segmentation accuracy, and the results demonstrate its effectiveness.","1945-8452","978-1-5386-9330-8","10.1109/ISBI45749.2020.9098449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098449","Image synthesis;Generative Adversarial Networks;Brain tumor segmentation;Multi-modality","Tumors;Image segmentation;Generators;Training;Image generation;Gallium nitride;Biomedical imaging","biomedical MRI;brain;cancer;image classification;image segmentation;medical image processing;tumours","multimodality generative adversarial network;brain MR image synthesis;Magnetic Resonance images;synthesizing missing images;multiple modalities;synthesized images;MR brain image synthesis;critical tumor information;generated modalities;multimodality tumor consistency loss;synthesized modalities","","8","","18","IEEE","22 May 2020","","","IEEE","IEEE Conferences"
"Dog Image Generation using Deep Convolutional Generative Adversarial Networks","Z. Shangguan; Y. Zhao; W. Fan; Z. Cao","Department of Electrical and Computer Engineering, University of Rochester, Rochester, US; Department of Electrical and Computer Engineering, University of Rochester, Rochester, US; Department of Electrical Engineering, Columbia University, New York, US; Department of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, US","2020 5th International Conference on Universal Village (UV)","11 May 2021","2020","","","1","6","Generative Adversarial Networks (GAN) serve as an important position of the data generation models, providing possibility for generating nonexistent images, style transfer, back-ground masking, alternative faces, etc. However, the generated images are becoming more and more realistic, which has raised the concern of people's privacy. In this paper, we implemented a Deep Convolutional Generative Adversarial Network (DCGAN) to show how to generate novel dog images from noise. We improved the performance of the basic DCGAN by applying different tricks, including adding noise to the training images, excute input normalization and batch normalization, comparing different activation functions, and using soft labels. The purpose of all these tricks is to synchronize the learning process between generator and discriminator as well as introduce stochasticity. The performance evaluation is based on Memorization-informed Frechet Inception Distance (MiFID) and results show that the MiFID value of our model reached outstanding performance, which is 95.85.","","978-1-7281-9523-0","10.1109/UV50937.2020.9426213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426213","Generative Adversarial Networks;privacy;Deep Convolutional Generative Adversarial Network;Memorization-informed Frechet Inception Distance","Training;Performance evaluation;Data privacy;Image synthesis;Dogs;Generative adversarial networks;Generators","convolutional neural nets;deep learning (artificial intelligence);image processing","dog image generation;deep convolutional generative adversarial network;data generation models;nonexistent images;training images;style transfer;background masking;batch normalization;activation functions;soft labels;Memorization-informed Frechet Inception Distance","","1","","27","IEEE","11 May 2021","","","IEEE","IEEE Conferences"
"Font Design Method Based on Multi-scale CycleGAN","Y. Pan; G. Liu; X. Wu; C. Chen; Z. Zhou; X. Liu","School of Computer, Hubei University of Technology, Wuhan, China; School of Computer, Hubei University of Technology, Wuhan, China; School of Computer, Hubei University of Technology, Wuhan, China; School of Computer, Hubei University of Technology, Wuhan, China; School of Computer, Hubei University of Technology, Wuhan, China; School of Computer, Hubei University of Technology, Wuhan, China","2022 IEEE 2nd International Conference on Information Communication and Software Engineering (ICICSE)","18 Jul 2022","2022","","","121","125","Font design is an important research direction in art design and has high commercial value. It requires professionals to design fonts, which is not only time-consuming and costly, but also inefficient. Font-to-font translation is a commonly used font design method. Font-to-font translation is essentially the problem of image synthesis. Currently, generative adversarial networks (GANs) have been used for image synthesis and achieved some results. However, for the task of font-to-font translation the existing methods based on GANs generally have low-quality visual effects, such as incomplete fonts and distortion of font details. In order to solve the above problems, we propose a more effective multi-scale CycleGAN for font-to-font translation and the proposed method can obtain the font images with better visual quality. The proposed method is called MSM-CycleGAN. In MSM-CycleGAN, a U-net with multiple outputs (UM) is used as the generator. UM outputs the generated images of multiple scales. And then the outputs of UM are fed into the multi-scale discriminator. Our model uses the unsupervised learning method. This multi-scale discrimination method effectively improves the detailed information of the generated image. Experimental results show that our method performs better than other state-of-the-art image synthesis methods, and can obtain the font images with higher visual quality.","","978-1-6654-8220-2","10.1109/ICICSE55337.2022.9828945","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9828945","u-net;multi-scale;generative adversarial networks;image synthesis","Image synthesis;Design methodology;Conferences;Visual effects;Generative adversarial networks;Distortion;Generators","character sets;image processing;neural nets;unsupervised learning","font images;font design method;font-to-font translation;font details;multiscale CycleGAN;generative adversarial networks;image synthesis;U-net;multiscale discriminator;unsupervised learning method;visual quality","","","","18","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"Image Generation by Residual Block Based Generative Adversarial Networks","K. -H. Liu; C. -C. Lin; T. -J. Liu","National Taichung University of Science and Technology, Taichung, Taiwan; National Taichung University of Science and Technology, Taichung, Taiwan; National Chung Hsing University, Taichung, Taiwan","2022 IEEE International Conference on Consumer Electronics (ICCE)","15 Mar 2022","2022","","","1","4","Generative adversarial network is a popular deep learning technique for solving artificial intelligence tasks, and it has been widely studied and applied for processing images, voices, texts and so on. Especially, generative adversarial network is adopted in the field of image processing, such as image style transfer, image restoration, image super-resolution and so on. Although generative adversarial networks show remarkable success in image generation, training process is usually unstable and trained models collapse where many of the generated images may contain the same color or texture pattern. In this paper, the network of generator and discriminator are modified, and the residual block is added to the generative adversarial network architecture to learn better image features. To reduce the loss of image feature during training and get more features to stabilize image generation, we use feature matching to minimize feature loss between the real and generated images for stable training. In the experiment, performance improvement can be obtained by adopting our proposed method, which is also better than some state-of-the-art methods.","2158-4001","978-1-6654-4154-4","10.1109/ICCE53296.2022.9730533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730533","Artificial intelligence;deep learning;image gen-eration;generative adversarial networks;residual block","Training;Image synthesis;Image color analysis;Superresolution;Learning (artificial intelligence);Generative adversarial networks;Generators","artificial intelligence;deep learning (artificial intelligence);feature extraction;image matching;image resolution;image restoration;image texture","real image generation;generative adversarial network architecture;image feature;image processing;image style;image restoration;image super-resolution;feature matching;artificial intelligence","","","","22","IEEE","15 Mar 2022","","","IEEE","IEEE Conferences"
"A Comparative Study of COVID-19 CT Image Synthesis using GAN and CycleGAN","K. W. Lee; R. K. Yin Chin","Faculty of Engineering, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia; Faculty of Engineering, Universiti Malaysia Sabah, Kota Kinabalu, Malaysia","2022 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET)","9 Nov 2022","2022","","","1","6","Generative adversarial networks (GANs) have been very successful in many applications of medical image synthesis, which hold great clinical value in diagnosis and analysis tasks, especially when data is scarce. This study compares the two most adopted generative modelling algorithms in recent medical image synthesis tasks, namely the traditional Generative Adversarial Network (GAN) and Cycle-consistency Generative Adversarial Network (CycleGAN) for COVID-19 CT image synthesis. Experiments show that very plausible synthetic COVID-19 images with a clear vision of artificially generated ground glass opacity (GGO) can be generated with CycleGAN when trained using an identity loss constant at 0.5. Moreover, it is found that the synthesis of the synthetic GGO features is generalized across images with different chest and lung structures, which suggests that diverse patterns of GGO can be synthesized using a conventional Image-to- Image translation setting without additional auxiliary conditions or visual annotations. In addition, similar experiment setting achieves encouraging perceptual quality with a Fréchet Inception Distance score of 0.347, which outperforms GAN at 0.383 and CycleGAN at 0.380 with an identity loss constant of 0.005. The experiment outcomes postulate a negative correlation between the strength of the identity loss and the significance of the synthetic instances manifested on the generated images, which highlights an interesting research path to improve the quality of generated images without compromising the significance of synthetic instances upon the image translation.","","978-1-6654-6837-4","10.1109/IICAIET55139.2022.9936810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9936810","generative adversarial networks;medical image synthesis;COVID-19;image-to-image translation;chest computerized tomography","COVID-19;Adaptation models;Correlation;Image synthesis;Computed tomography;Generative adversarial networks;Loss measurement","computerised tomography;image segmentation;lung;medical image processing","COVID-19 CT image synthesis;plausible synthetic COVID-19 images;artificially generated ground glass opacity;CycleGAN;identity loss;synthetic GGO features;conventional Image-to- Image translation setting;GAN;COVID-19 CT Image synthesis;Generative adversarial networks;analysis tasks;adopted generative modelling algorithms;recent medical image synthesis tasks;traditional Generative Adversarial Network;Cycle-consistency Generative Adversarial Network","","","","28","IEEE","9 Nov 2022","","","IEEE","IEEE Conferences"
"A Novel Framework to Synthesize Arterial Spin Labeling Images using Difference Images","F. LI; P. ZHANG; W. HUANG","Department of Computer Science, Nanchang University, Nanchang, China; School of Computer Science Northwestern Polytechnical University, Xi'an, China; Informatization Office Nanchang University, Nanchang, China","2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)","4 Oct 2021","2021","","","27","31","Arterial spin labeling (ASL) images that are capable to quantitatively measure the cerebral blood flow receive increasing research attention in recent dementia diseases diagnosis studies. However, this important and relatively new imaging modality is unfortunately not commonly seen in many well-established image-based dementia datasets, including the ADNI-1/2/3/Go datasets. Hence, synthesizing ASL images to supplement this important modality is valuable. In this study, a new framework based on a cascade of generative adversarial networks (GANs) and difference images generated from a Laplacian pyramid is proposed. This framework is novel as it is the first attempt to incorporate difference images for synthesizing medical images. Experimental results based on a 355-demented patient dataset and ADNI-1 dataset suggest that, this new framework outperforms all state-of-the-arts in ASL image synthesis. Also, synthesized ASL images obtained by this new framework are capable to significantly improve the accuracy of dementia diseases diagnosis performance.","","978-1-6654-3960-2","10.1109/ICCEAI52939.2021.00005","National Natural Science Foundation of China(grant numbers:61862043,61971352); Natural Science Foundation of Jiangxi Province(grant numbers:20204BCJ22011); Natural Science Foundation of Shaanxi Province(grant numbers:2018JM6015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9544364","Image synthesis;Difference image;Generative adversarial networks","Laplace equations;Image synthesis;Generative adversarial networks;Medical diagnosis;Labeling;Artificial intelligence;Medical diagnostic imaging","biomedical MRI;blood vessels;brain;diseases;haemodynamics;haemorheology;image classification;medical image processing;neurophysiology;patient diagnosis","ASL images;important modality;generative adversarial networks;incorporate difference images;synthesizing medical images;patient dataset;ADNI-1 dataset;ASL image synthesis;dementia diseases diagnosis performance;arterial spin labeling images;cerebral blood flow;recent dementia diseases diagnosis studies;important imaging modality;relatively new imaging modality;well-established image-based dementia datasets","","","","10","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"A Review on Generative Adversarial Networks used for Image Reconstruction in Medical imaging","R. Kumar; R. Malik","Computer Science Engineering, Lovely Professional University, Punjab, India; Computer Science Engineering, Lovely Professional University, Punjab, India","2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)","15 Nov 2021","2021","","","1","5","In computer vision, Because of its capacity to generate data, generative adversarial networks have gotten much interest. Generative Adversarial Network (GANs) opens different ways to overcome difficulties in medical image investigations like de-noising of the captured image, reconstruction of captured medical image, segmentation of the captured medical image, synthesis of the captured image, detection and classification of processed medical image. GANs great potentials in image restoration or reconstruction for different anatomy open a new research area. However, image reconstruction for specific anatomy is still facing the challenge. This article consists of a recent literature review on the applications of GAN for the reconstruction of medical images. Different reconstruction methods in the field of medical imaging were explored in this paper thoroughly. We have studied the foremost relevant publications.","","978-1-6654-1703-7","10.1109/ICRITO51393.2021.9596487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9596487","Generative adversarial network;GAN model;Medical imaging Anatomy;Reconstruction","Image segmentation;Image synthesis;Noise reduction;Reconstruction algorithms;Generative adversarial networks;Market research;Reliability","biomedical imaging;computer vision;image reconstruction;image restoration;medical image processing","Generative Adversarial Network;medical image investigations;captured image;captured medical image;classification;processed medical image;image restoration;image reconstruction;medical images;different reconstruction methods;medical imaging;generative adversarial networks","","","","23","IEEE","15 Nov 2021","","","IEEE","IEEE Conferences"
"Generation of Artificial CT Images using Patch-based Conditional Generative Adversarial Networks","M. Habijan; I. Galić","Faculty of Electrical Engineering, Computer Science and Information Technology Osijek, J. J. Strossmayer University of Osijek, Osijek, Croatia; Faculty of Electrical Engineering, Computer Science and Information Technology Osijek, J. J. Strossmayer University of Osijek, Osijek, Croatia","2022 7th International Conference on Smart and Sustainable Technologies (SpliTech)","19 Aug 2022","2022","","","1","5","Deep learning has a great potential to alleviate diagnosis and prognosis for various clinical procedures. However, the lack of a sufficient number of medical images is the most common obstacle in conducting image-based analysis using deep learning. Due to the annotations scarcity, semi-supervised techniques in the automatic medical analysis are getting high attention. Artificial data augmentation and generation techniques such as generative adversarial networks (GANs) may help overcome this obstacle. In this work, we present an image generation approach that uses generative adversarial networks with a conditional discriminator where segmentation masks are used as conditions for image generation. We validate the feasibility of GAN-enhanced medical image generation on whole heart computed tomography (CT) images and its seven substructures, namely: left ventricle, right ventricle, left atrium, right atrium, myocardium, pulmonary arteries, and aorta. Obtained results demonstrate the suitability of the proposed adversarial approach for the accurate generation of high-quality CT images. The presented method shows great potential to facilitate further research in the domain of artificial medical image generation.","","978-1-6654-8828-0","10.23919/SpliTech55088.2022.9854249","Croatian Science Foundation(grant numbers:UIP-2017-05-4968); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854249","conditional generative adversarial networks;CT;deep learning;generative adversarial networks;medical image generation;unsupervised deep learning","Deep learning;Training;Heart;Image segmentation;Image synthesis;Computed tomography;Lung","blood vessels;cardiology;computerised tomography;deep learning (artificial intelligence);image segmentation;medical image processing;neural nets","automatic medical analysis;artificial data augmentation;image generation approach;GAN-enhanced medical image generation;adversarial approach;high-quality CT images;artificial medical image generation;artificial CT images;patch-based conditional generative adversarial networks;deep learning;medical images;image-based analysis;semisupervised techniques;whole heart computed tomography images","","","","19","","19 Aug 2022","","","IEEE","IEEE Conferences"
"StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks","H. Zhang; T. Xu; H. Li; S. Zhang; X. Wang; X. Huang; D. N. Metaxas","Department of Computer Science, Rutgers University, Piscataway, NJ, USA; Department of Computer Science and Engineering, Lehigh University, Bethlehem, PA, USA; Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA; Department of Electronic Engineering, The Chinese University of Hong Kong, Shatin, N. T., Hong Kong; Department of Computer Science and Engineering, Lehigh University, Bethlehem, PA, USA; Department of Computer Science, Rutgers University, Piscataway, NJ, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","1 Jul 2019","2019","41","8","1947","1962","Although Generative Adversarial Networks (GANs) have shown remarkable success in various tasks, they still face challenges in generating high quality images. In this paper, we propose Stacked Generative Adversarial Networks (StackGANs) aimed at generating high-resolution photo-realistic images. First, we propose a two-stage generative adversarial network architecture, StackGAN-v1, for text-to-image synthesis. The Stage-I GAN sketches the primitive shape and colors of a scene based on a given text description, yielding low-resolution images. The Stage-II GAN takes Stage-I results and the text description as inputs, and generates high-resolution images with photo-realistic details. Second, an advanced multi-stage generative adversarial network architecture, StackGAN-v2, is proposed for both conditional and unconditional generative tasks. Our StackGAN-v2 consists of multiple generators and multiple discriminators arranged in a tree-like structure; images at multiple scales corresponding to the same scene are generated from different branches of the tree. StackGAN-v2 shows more stable training behavior than StackGAN-v1 by jointly approximating multiple distributions. Extensive experiments demonstrate that the proposed stacked generative adversarial networks significantly outperform other state-of-the-art methods in generating photo-realistic images.","1939-3539","","10.1109/TPAMI.2018.2856256","Air Force Office of Scientific Research; National Science Foundation(grant numbers:1763523,1747778,1733843,1703883); NSF(grant numbers:ABI-1661280,CNS-1629913); General Research Fund(grant numbers:CUHK14213616,CUHK14206114,CUHK14205615,CUHK14203015,CUHK14239816,CUHK419412,CUHK14207814,CUHK14208417,CUHK14202217); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411144","Generative models;generative adversarial networks (GANs);multi-stage GANs;multi-distribution approximation;photo-realistic image generation;text-to-image synthesis","Gallium nitride;Generators;Image resolution;Training;Image generation;Task analysis;Computational modeling","image colour analysis;image resolution;image texture;realistic images;solid modelling;trees (mathematics)","unconditional generative tasks;multiple generators;stacked generative adversarial networks;realistic image synthesis;high quality images;high-resolution photo-realistic images;two-stage generative adversarial network architecture;text-to-image synthesis;low-resolution images;StackGAN-v2;stage-II GAN;stage-I GAN;multistage generative adversarial network architecture;conditional generative tasks;tree-like structure","","379","","57","IEEE","16 Jul 2018","","","IEEE","IEEE Journals"
"Pose and Color-Gamut Guided Generative Adversarial Network for Pedestrian Image Synthesis","X. Liu; X. Liu; G. Li; S. Bi","Information Science and Technology College, Dalian Maritime University, Dalian 116026, China.; Information Science and Technology College, Dalian Maritime University, Dalian 116026, China.; SIASUN Robot & Automation Company, Ltd., Shenyang, China.; Information Science and Technology College, Dalian Maritime University, Dalian 116026, China.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","13","Tremendous transfer requirements in pedestrian reidentification (Re-ID) tasks have greatly promoted the remarkable success in pedestrian image synthesis, to relieve the inconsistency in poses and lighting. However, existing approaches are confined to transferring in a particular domain and are difficult to combine, since pose and color variables locate in two independent domains. To facilitate the research toward conquering this issue, we propose a pose and color-gamut guided generative adversarial network (PC-GAN) that performs joint-domain pedestrian image synthesis conditioned on certain pose and color-gamut through a delicate supervision design. The generator of the network comprises a sequence of cross-domain conversion subnets, where the local displacement estimator, color-gamut transformer, and pose transporter coordinate their learning pace to progressively synthesize images in desired pose and color-gamut. Ablation studies have demonstrated the efficacy and efficiency of the proposed network both qualitatively and quantitatively on Market-1501 and DukeMTMC. Furthermore, the proposed architecture can generate training images for person Re-ID, alleviating the data insufficiency problem.","2162-2388","","10.1109/TNNLS.2022.3171245","National Natural Science Foundation of China(grant numbers:61802044); China Postdoctoral Science Foundation(grant numbers:2019M661078); Natural Science Foundation of Liaoning Province(grant numbers:2020BS074); Dalian Science and Technology Innovation Fund(grant numbers:2020JJ27SN104); Fundamental Research Fund for the Central Universities(grant numbers:3132021235); Major Science and Technology Innovation Engineering Projects of Shandong Province(grant numbers:2019JZZY010128); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777843","Color-gamut transformation;generative adversarial networks (GANs);joint-domain transformation;pedestrian image synthesis.","Image color analysis;Generative adversarial networks;Task analysis;Image synthesis;Lighting;Generators;Cameras","","","","","","","IEEE","18 May 2022","","","IEEE","IEEE Early Access Articles"
"Performance Study of Image Data Augmentation by Generative Adversarial Networks","K. Wada; B. Chakraborty","Dept. of Software & Information Science, Iwate Prefectural University, Takizawa, Iwate, Japan; Faculty of Software & Information Science, Iwate Prefectural University, Takizawa, Iwate, Japan","2021 IEEE 12th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)","6 Dec 2021","2021","","","1022","1026","Convolutional Neural Networks (CNN) are becoming increasingly popular among machine learning techniques for image classification problems. But these networks are highly dependent on large set of data for proper learning to avoid overfitting in the context of supervised classification. Contrary to numerical data, image data poses difficulty for data augmentation by artificial increase of training data. Recently Generative Adversarial Networks (GAN) have been developed and are increasingly used for generation of image data. It has been proved that GAN has higher potential in generating image data for training purpose compared to traditional methods of data augmentation in the area of image processing. In this work we have studied the performance of different GAN developed recently in terms of their effectiveness in classification problems compared to baseline CNN without GAN. We have also done the comparative study on the qualities of fake image generation by different GAN in terms of IS (Inception Score) and FID (Fréchet Inception Distance). In this work we have studied the performance of Lightweight GAN, Data-efficient GAN and StyleGAN-ADA, in addition to the most conventional Deep Convolutional Generative Adversarial Networks (DCGAN). Simulation experiments have been done using bench mark data set Food-101. The results show that Lightweight GAN is the most efficient for data augmentation.","2644-3163","978-1-6654-0066-4","10.1109/IEMCON53756.2021.9623117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623117","Data augmentation;image classification;generative adversarial network","Training;Image synthesis;Training data;Machine learning;Generative adversarial networks;Mobile communication;Convolutional neural networks","convolutional neural nets;deep learning (artificial intelligence);image classification","image data augmentation;convolutional neural networks;image classification problems;training data;image processing;fake image generation;CNN;machine learning technique;inception score;FID;Fréchet inception distance;data-efficient GAN;StyleGAN-ADA;conventional deep convolutional generative adversarial networks;DCGAN;Food-101 benchmark dataset","","","","18","IEEE","6 Dec 2021","","","IEEE","IEEE Conferences"
"Butterfly Image Generation and Recognition Based on Improved Generative Adversarial Networks","F. Min; W. Xiong","Hubei Province Intelligent Robotics Key Laboratory, Wuhan institute of Technology, Wuhan, China; Hubei Province Intelligent Robotics Key Laboratory, Wuhan institute of Technology, Wuhan, China","2021 4th International Conference on Robotics, Control and Automation Engineering (RCAE)","14 Dec 2021","2021","","","40","44","As one of the important methods for sample generation, Generative Adversarial Networks (GAN) can generate samples based on the data distribution in any given dataset. However, it suffers from problems such as blurred texture, unstable training process and collapse of patterns in the actual training process. To address these problems, a sample generation method based on Deep Convolutional Generative Adversarial Networks (DCGAN) combined with Residual Networks (ResNet) is designed. The method uses residual and convolutional networks to construct generative and discriminative models, the deep residual network can recover rich image textures and effectively alleviate the instability of adversarial network training and the occurrence of pattern collapse. The performance of the algorithm is tested on the Butterfly Image Classification 50 species datasets with loss function curves and FID scores, then the network model is applied to the butterfly-like insect datasets for data augmentation. And we use the two classic deep convolutional neural network models to train and identify the samples. The results show that the proposed method can improve the accuracy of the two models in classification by 1.84% and 2.78% on the butterfly class datasets. The experiment results confirm that the improved DCGAN can effectively generate butterfly-like image data and improve the accuracy of the deep neural network model in classifying butterfly images.","","978-1-6654-2730-2","10.1109/RCAE53607.2021.9638915","Natural Science Foundation of Hubei Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9638915","butterfly recognition;DCGAN;ResNet;FID","Training;Image texture;Image recognition;Image synthesis;Insects;Superresolution;Generative adversarial networks","biology computing;convolutional neural nets;deep learning (artificial intelligence);image classification;image texture","data distribution;sample generation method;deep convolutional generative adversarial networks;generative models;discriminative models;deep residual network;rich image textures;adversarial network training;pattern collapse;Butterfly Image Classification 50 species datasets;butterfly-like insect datasets;classic deep convolutional neural network models;butterfly class datasets;DCGAN;butterfly-like image data;butterfly image generation;butterfly image recognition;loss function curves;FID scores;data augmentation","","","","18","IEEE","14 Dec 2021","","","IEEE","IEEE Conferences"
"Data Augmentation For Deep Learning Using Generative Adversarial Networks","D. Yorioka; H. Kang; K. Iwamura","Department of Electrical Engineering, Tokyo University of Science, Tokyo, Japan; Department of Electrical Engineering, National Institute of Technology Tokyo College, Tokyo, Japan; Department of Electrical Engineering, Tokyo University of Science, Tokyo, Japan","2020 IEEE 9th Global Conference on Consumer Electronics (GCCE)","21 Dec 2020","2020","","","516","518","Deep learning requires the use of several labeled images as training data. However, in practice, it is difficult to obtain a sufficient number of appropriate images, and it is particularly difficult to obtain diverse classes of images with labels. This poses a challenge in the training of convolutional neural networks (CNNs). Data augmentation, a technique for generating more data from pre-existing training data, plays an essential role in addressing the dearth of appropriate images. Thus, in this study, we propose a data augmentation method using image generation and a generative adversarial network (GAN) with geometric deformation. In the proposed method, a small dataset is repeatedly augmented via geometric deformation and used as training data for the auxiliary classifier-GAN. Finally, the performance of the CNN trained on the dataset generated by the proposed method is evaluated. Although the quality and accuracy of the results achieved were insufficient, it would be remiss to dismiss the potential of this approach. The challenge now lies in improving the quality of the generated images, thereby improving the accuracy of the CNN.","2378-8143","978-1-7281-9802-6","10.1109/GCCE50665.2020.9291963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9291963","Deep neural network;Generative adversarial networks;Data augmentation","Deep learning;Training;Image synthesis;Neural networks;Training data;Generative adversarial networks;Strain","convolutional neural nets;deep learning (artificial intelligence);image classification","generative adversarial network;geometric deformation;deep learning;labeled images;appropriate images;data augmentation;image generation;CNN;GAN;auxiliary classifier","","6","","7","IEEE","21 Dec 2020","","","IEEE","IEEE Conferences"
"A Study on the Collision of Artificial Intelligence and Art Based on Generative Adversarial Networks (GAN)","S. Guo; Y. Wang; W. Yang","School of Design and Art, Shenyang Jianzhu University, Shenyang, China; Chaoyang Quality Education Practice, School for Primary and Secondary Schools, Chaoyang, China; Graduate School of Shenyang Ligong University, Shenyang, China","2022 International Conference on 3D Immersion, Interaction and Multi-sensory Experiences (ICDIIME)","24 Nov 2022","2022","","","27","31","The cross-collision of artificial intelligence and art has attracted significant attention in related fields, such as the migration and integration of painting styles. Due to the inherent differences of different painting techniques, direct application of existing methods does not bring satisfactory results. This paper proposes a multi-layer Pyramid Generative Adversarial Network (MPGAN), an end-to-end Generative Adversarial Network(GAN) based architecture, whose advantages are in image quality and better-scaled fusion of painting styles compared with state-of-the-art algorithms demonstrate the effectiveness of the method. The method's effectiveness is demonstrated by its advantages in image quality compared to the state-of-the-art algorithms and its better scaling fusion of painting styles.","","978-1-6654-9009-2","10.1109/ICDIIME56946.2022.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9951580","artificial intelligence;painting style migration;multi-layer pyramids;generative adversarial networks;scale fusion","Image quality;Training;Art;Three-dimensional displays;Image synthesis;Semantic segmentation;Generative adversarial networks","art;artificial intelligence;computer vision;feature extraction;neural nets","artificial intelligence;drawing style information;image quality;interpixel difference;MPGAN;multilayer pyramid generative adversarial network;painting style;painting technique;self-attentive mechanism;semantic information extraction","","","","16","IEEE","24 Nov 2022","","","IEEE","IEEE Conferences"
"Introduction to Medical Image Synthesis Using Deep Learning:A Review","M. S. Meharban; M. K. Sabu; S. krishnan","Department of Computer Application, CUSAT, Kochi, Kerala, India; Department of Computer Application, CUSAT, Kochi, Kerala, India; DRDO-NPOL, Kochi, Kerala, India","2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS)","3 Jun 2021","2021","1","","414","419","Medical imaging performs a vital function in unique medical programs. But, because of multiple issues like price and radiation dose, the purchase of sure image modalities is moreover limited. Therefore, clinical photo synthesis is of excellent profit with the aid of estimating a desired imaging modality whereas no longer acquisition companion in nursing real experiment. Photograph synthesis has attracted people's attention due to its plethora of application in social media. The GANs have finished tremendous finally ends up inside the international of photo synthesis, like CycleGAN. This overview paper gives revolutionary growth in generative antagonistic networks-based clinical programs like clinical photo technology, and cross-modality synthesis.","2575-7288","978-1-6654-0521-8","10.1109/ICACCS51430.2021.9442041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442041","GANs:Generative Adversarial Networks;CycleGAN:Cycle Generative Adversarial Networks","Training;Three-dimensional displays;Social networking (online);Image synthesis;Communication systems;Medical services;Generative adversarial networks","deep learning (artificial intelligence);medical image processing","medical image synthesis;deep learning;medical programs;radiation dose;image modalities;clinical photo synthesis;imaging modality;photograph synthesis;generative antagonistic networks-based clinical programs;clinical photo technology;cross-modality synthesis;CycleGAN;nursing","","3","","28","IEEE","3 Jun 2021","","","IEEE","IEEE Conferences"
"SIMGAN: Photo-Realistic Semantic Image Manipulation Using Generative Adversarial Networks","S. Yu; H. Dong; F. Liang; Y. Mo; C. Wu; Y. Guo",Imperial College London; Imperial College London; University of Washington; Imperial College London; Zhejiang University; Imperial College London,"2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","734","738","Semantic image manipulation (SIM) aims to generate realistic images from an input source image and a target text description, such that the generated images not only match the content of the description, but also maintain text-irrelevant features of the source image. It requires to learn a good mapping between visual features and linguistic features. Previous works on SIM can only generate images of limited resolution that typically lack of fine and clear details. In this work, we aim to generate high-resolution photo-realistic images for SIM. Specifically, we propose SIMGAN, a generative adversarial networks (GAN) based architecture that is capable of generating images of size 256 × 256 for SIM. We demonstrate the effectiveness of SIMGAN and its superiority over existing methods via qualitative and quantitative evaluation on Caltech-200 and Oxford-102 datasets.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8804285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8804285","adversarial learning;generative model;image generation;semantic image manipulation","Semantics;Generators;Image synthesis;Generative adversarial networks;Image resolution;Feature extraction;Visualization","image resolution;image retrieval;image texture;learning (artificial intelligence);neural net architecture;realistic images;text analysis;text detection","visual features;linguistic features;SIM;high-resolution photo-realistic images;SIMGAN;generative adversarial networks;photo-realistic semantic image manipulation;target text description;text-irrelevant features;Caltech-200 dataset;Oxford-102 dataset","","3","","25","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Art Font Image Generation with Conditional Generative Adversarial Networks","Y. Yuan; Y. Ito; K. Nakano","Graduate School of Advanced Science and Engineering, Hiroshima University, Higashi-Hiroshima, JAPAN; Graduate School of Advanced Science and Engineering, Hiroshima University, Higashi-Hiroshima, JAPAN; Graduate School of Advanced Science and Engineering, Hiroshima University, Higashi-Hiroshima, JAPAN","2020 Eighth International Symposium on Computing and Networking Workshops (CANDARW)","22 Feb 2021","2020","","","151","156","An art font is an artistic font that has an impressive and attractive shape with decoration and it is widely used in advertisements and posters, among others. The main contribution of this paper is to propose an art font generation method using machine learning approach using conditional generative adversarial networks. The first idea of the proposed method is using the separated two networks, the typeface network and the ornament network. The typeface network changes shape of an input font and the ornament network adds effects to the font. The second idea is providing the skeletons and edges of the character as auxiliary input to the typeface network to avoid disrupting the shape of the font. The experimental results show that our method is better than direct transfer of font and texture to generate better-looking art fonts, and it can generate a large number of art fonts from a very small number of observed characters of the same style at one time, which has higher practicability.","","978-1-7281-9919-1","10.1109/CANDARW51189.2020.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355842","art font;machine learning;conditional GAN","Art;Shape;Image synthesis;Conferences;Machine learning;Generative adversarial networks;Skeleton","art;character recognition;computer graphics;edge detection;learning (artificial intelligence);neural nets","ornament network;input font;art font image generation;conditional generative adversarial networks;artistic font;impressive shape;attractive shape;art font generation method;typeface network;machine learning","","1","","18","IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Training of Generative Adversarial Networks using Particle Swarm Optimization Algorithm","K. G. Shreeharsha; C. G. Korde; M. H. Vasantha; Y. B. Nithin Kumar","Department of Electronics and Communication Engineering, National Institute of Technology, Goa, India; Department of Electronics and Communication Engineering, National Institute of Technology, Goa, India; Department of Electronics and Communication Engineering, National Institute of Technology, Goa, India; Department of Electronics and Communication Engineering, National Institute of Technology, Goa, India","2021 IEEE International Symposium on Smart Electronic Systems (iSES)","8 Feb 2022","2021","","","127","130","In this paper, a particle swarm optimization (PSO) based solution is proposed for the training of generative adversarial networks (GANs). Conventional GAN networks take around 5x times more number of iterations to generate plausible images compared to the proposed method, thereby increasing the simulation time and decreasing the Frechet Inception Distance (FID) score. To overcome the problems of non-convergence and mode collapse associated with the conventional GANs, proposed work uses a PSO algorithm to stabilize the inertia weights during the training duration followed by conventional optimization method for the remaining iterations. The proposed solution is implemented on Nvidia Tesla VI00-PCIE-16GB GPU, using tensorflow and keras. The efficiency of the proposed solution is verified using MNIST dataset. The results showed that the iteration at which images are generated for the proposed method is faster as compared to the conventional GAN architectures, quantified with lower FID score.","","978-1-7281-8753-2","10.1109/iSES52644.2021.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9701036","Generative adversarial network (GAN);particle swarm optimization (PSO);Frechet inception distance (FID);training of neural networks","Training;Image synthesis;Neural networks;Optimization methods;Graphics processing units;Generative adversarial networks;Stability analysis","image processing;learning (artificial intelligence);neural nets;particle swarm optimisation","Frechet inception distance score;training duration;Nvidia Tesla VI00-PCIE-16GB GPU;generative adversarial networks;particle swarm optimization;GAN networks;plausible images;FID score;inertia weights;tensorflow;keras;MNIST dataset;PSO;memory size 16.0 GByte","","","","15","IEEE","8 Feb 2022","","","IEEE","IEEE Conferences"
"Cantonese Porcelain Image Generation Using User-Guided Generative Adversarial Networks","S. S. -C. Chen; H. Cui; P. Tan; X. Sun; Y. Ji; H. Duh","Department of Computer Science and Information Technology, La Trobe University; Department of Computer Science and Information Technology, La Trobe University; School of Art and Design, Guangdong University of Technology; School of Art and Design, Guangdong University of Technology; School of Art and Design, Guangdong University of Technology; Department of Computer Science and Information Technology, La Trobe University","IEEE Computer Graphics and Applications","24 Aug 2020","2020","40","5","100","107","Automated image style transfer is of great interest given the recent advances in generative adversarial networks (GANs). However, it is challenging to generate synthesized images from abstract masks while preserving detailed patterns for certain kinds of art given small datasets. We propose an intelligent GAN-based system enhanced with user intent and prior knowledge for generating images styled as Cantonese porcelain using user-defined masks. Given a mask with specified objects, our system first generates a synthesized natural image. We then use a novel semantic user intent enhancement module to retrieve semantically relevant images from an image dataset. Objects in the retrieved image are used to refine local patterns in the synthesized image. Finally, the refined image is restyled in the Cantonese porcelain style. The system is trained by 454 pairs of natural images and semantic segmentation of 24 objects from the COCO dataset for synthesized image generation from masks, and 1445 Cantonese porcelain images for style transfer. Experimental results and ablation studies demonstrate that the synthesized and restyled images were improved with local details and enhanced contrast.","1558-1756","","10.1109/MCG.2020.3012079","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9175078","","Porcelain;Semantics;Image synthesis;Generative adversarial networks;Art;Image segmentation;Generators","history;image enhancement;image retrieval;image segmentation;image texture;natural language processing;neural nets;unsupervised learning","Cantonese porcelain image generation;user-guided generative adversarial networks;intelligent GAN-based system;synthesized natural image;semantic user intent enhancement module;image dataset;image retrieval;Cantonese porcelain style;synthesized image generation;automated image style transfer;COCO datase","","","","15","IEEE","24 Aug 2020","","","IEEE","IEEE Magazines"
"Relationship Between Electroencephalogram and Fine-art Generation using Generative Adversarial Networks","R. Takagi; K. Inagaki","Department of Robotic Science and Technology, Chubu University, Kasugai, Japan; Department of Robotic Science and Technology, Chubu University, Kasugai, Japan","2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech)","14 Apr 2022","2022","","","391","392","In recent years, the abilities of neural networks imitating human brain processing are gathering researchers’ attention. Generative Adversarial Networks (GANs), which learn features from numerous data and generate novel data based on acquired features, are beginning to be applied in many research and commercial areas. In the field of fine-art, it was reported that GAN successfully learned fine-arts and generate extremely similar images. In this study, we hypothesized that the GAN might learn also the way of the style in the fine-arts with considering emotional state and generate a novel fine-art. To investigate this hypothesis, we employed the Cycle-Consistent Adversarial network to learn the fine-arts and evaluate the relationship between human brain activities related to emotion when watching fine-arts and generation of them by CCAN.","","978-1-6654-1904-8","10.1109/LifeTech53646.2022.9754816","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754816","machine learning;EEG;art;emotion","Image segmentation;Brain;Image synthesis;Conferences;Generative adversarial networks;Electroencephalography;Life sciences","electroencephalography;learning (artificial intelligence);medical signal processing;neural nets","neural networks;generative adversarial networks;cycle-consistent adversarial network;electroencephalogram;human brain activities","","","","4","IEEE","14 Apr 2022","","","IEEE","IEEE Conferences"
"Semantic Image Synthesis via Conditional Cycle-Generative Adversarial Networks","X. Liu; G. Meng; S. Xiang; C. Pan","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation Chinese Academy of Sciences, Beijing, China","2018 24th International Conference on Pattern Recognition (ICPR)","29 Nov 2018","2018","","","988","993","Traditional approaches for semantic image synthesis mainly focus on text descriptions while ignoring the related structures and attributes in the original images. Therefore, some critical information, e.g., the style, backgrounds, objects shapes and pose, is missed in the generated images. In this paper, we propose a novel framework called Conditional Cycle-Generative Adversarial Network (CCGAN) to address this issue. Our model can generate photo-realistic images conditioned on the given text descriptions, while maintaining the attributes of the original images. The framework mainly consists of two coupled conditional adversarial networks, which are able to learn a desirable image mapping that can keep the structures and attributes in the images. We introduce a conditional cycle consistency loss to prevent the contradiction between two generators. This loss allows the generated images to retain most of the features of the original image, so as to improve the stability of network training. Moreover, benefiting from the mechanism of circular training, the proposed networks can learn the semantic information of the text much accurately. Experiments on Caltech-UCSD Bird dataset and Oxford-102 flower dataset demonstrate that the proposed method significantly outperforms the existing methods in terms of image details reconstruction and semantic information expression.","1051-4651","978-1-5386-3788-3","10.1109/ICPR.2018.8545383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8545383","","Birds;Generators;Generative adversarial networks;Semantics;Gallium nitride;Training;Image generation","image representation;realistic images;semantic networks","text descriptions;Oxford-102 flower dataset;Caltech-UCSD Bird dataset;circular training;conditional cycle consistency loss;desirable image mapping;conditional adversarial networks;photo-realistic images;Conditional Cycle-Generative Adversarial Network;semantic image synthesis;semantic information expression;image details reconstruction;network training","","5","","22","IEEE","29 Nov 2018","","","IEEE","IEEE Conferences"
"Fine-Grained Mri Reconstruction Using Attentive Selection Generative Adversarial Networks","J. Liu; M. Yaghoobi","IDCOM, School of Engineering, University of Edinburgh, UK; IDCOM, School of Engineering, University of Edinburgh, UK","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","1155","1159","Compressed sensing (CS) leverages the sparsity prior to provide the foundation for fast magnetic resonance imaging (fastMRI). However, iterative solvers for ill-posed problems hinder their adaption to time-critical applications. Moreover, such a prior can be neither rich to capture complicated anatomical structures nor applicable to meet the demand of high-fidelity reconstructions in modern MRI.Inspired by the state-of-the-art methods in image generation, we propose a novel attention-based deep learning framework to provide high-quality MRI reconstruction. We incorporate large-field contextual feature integration and attention selection in a generative adversarial network (GAN) framework. We demonstrate that the proposed model can produce superior results compared to other deep learning-based methods in terms of image quality, and relevance to the MRI reconstruction in an extremely low sampling rate diet.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414981","MRI Reconstruction;GAN-based Framework;Attention Selection","Learning systems;Image quality;Deep learning;Image synthesis;Magnetic resonance imaging;Signal processing;Generative adversarial networks","biomedical MRI;compressed sensing;image reconstruction;iterative methods;learning (artificial intelligence);medical image processing","attentive selection generative adversarial networks;compressed sensing;fast magnetic resonance imaging;iterative solvers;ill-posed problems;time-critical applications;complicated anatomical structures;high-fidelity reconstructions;image generation;attention-based deep learning framework;high-quality MRI reconstruction;large-field contextual feature integration;generative adversarial network framework;deep learning-based methods;image quality;fine-grained MRI reconstruction","","3","","22","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Semantically Consistent Hierarchical Text to Fashion Image Synthesis with an Enhanced-Attentional Generative Adversarial Network","K. E. Ak; J. H. Lim; J. Y. Tham; A. Kassim","National University of Singapore, Singapore, Singapore; National University of Singapore, Singapore; ESP xMedia Pte. Ltd., Singapore; National University of Singapore, Singapore, Singapore","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3121","3124","In this paper, we present the enhanced Attentional Generative Adversarial Network (e-AttnGAN) with improved training stability for text-to-image synthesis. e-AttnGAN's integrated attention module utilizes both sentence and word context features and performs feature-wise linear modulation (FiLM) to fuse visual and natural language representations. In addition to multimodal similarity learning for text and image features of AttnGAN, cosine and feature matching losses of real and generated images are included while employing a classification loss for ""significant attributes"". In order to improve the stability of the training and solve the issue of model collapse, spectral normalization and two-time scale update for the discriminator are used together with instance noise. Our experiments show that e-AttnGAN outperforms state-of-the-art methods on the FashionGen and DeepFashion-Synthesis datasets.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022076","text to image;image synthesis;generative adversarial networks","Training;Generators;Visualization;Gallium nitride;Generative adversarial networks;Feature extraction;Computer vision","feature extraction;learning (artificial intelligence);text detection;video signal processing","improved training stability;text-to-image synthesis;word context features;visual language representations;natural language representations;image features;cosine losses;consistent hierarchical text;fashion image synthesis;enhanced-attentional generative adversarial network;e-AttnGAN;feature-wise linear modulation;integrated attention module;enhanced attentional generative adversarial network","","13","","32","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks","Q. Cheng; K. Wen; X. Gu","Department of Electronic Engineering, Fudan University, Shanghai, China; Department of Electronic Engineering, Fudan University, Shanghai, China; Department of Electronic Engineering, Fudan University, Shanghai, China","IEEE Transactions on Multimedia","","2022","PP","99","1","14","Text-to-image synthesis is an attractive but challenging task that aims to generate a photo-realistic and semantic consistent image from a specific text description. The images synthesized by off-the-shelf models usually contain limited components compared with the corresponding image and text description, which decreases the image quality and the textual-visual consistency. To address this issue, we propose a novel Vision-Language Matching strategy for text-to-image synthesis, named VLMGAN*, which introduces a dual vision-language matching mechanism to strengthen the image quality and semantic consistency. The dual vision-language matching mechanism considers textual-visual matching between the generated image and the corresponding text description, and visual-visual consistent constraints between the synthesized image and the real image. Given a specific text description, VLMGAN* firstly encodes it into textual features and then feeds them to a dual vision-language matching-based generative model to synthesize a photo-realistic and textual semantic consistent image. Besides, the popular evaluation metrics for text-to-image synthesis are borrowed from simple image generation, which mainly evaluate the reality and diversity of the synthesized images. Therefore, we introduce a metric named Vision-Language Matching Score (VLMS) to evaluate the performance of text-to-image synthesis which can consider both the image quality and the semantic consistency between the synthesized image and the description. The proposed dual multi-level vision-language matching strategy can be applied to other text-to-image synthesis methods. We implement this strategy on two popular baselines, which are marked with ${\rm{VLMGAN}_{+\rm{AttnGAN}}}$ and ${\rm{VLMGAN}_{+\rm{DFGAN}}}$ . The experimental results on two widely-used datasets show that the model achieves significant improvements over other state-of-the-art methods.","1941-0077","","10.1109/TMM.2022.3217384","National Natural Science Foundation of China(grant numbers:62176062); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930667","Text-to-image synthesis;Generative Adversarial Networks;vision-language matching","Semantics;Image synthesis;Visualization;Task analysis;Measurement;Generative adversarial networks;Image quality","","","","","","","IEEE","26 Oct 2022","","","IEEE","IEEE Early Access Articles"
"A survey of image synthesis and editing with generative adversarial networks","X. Wu; K. Xu; P. Hall","Tsinghua University, Beijing, Beijing, CN; Tsinghua University, Beijing, Beijing, CN; University of Bath, Bath, Somerset, GB","Tsinghua Science and Technology","14 Dec 2017","2017","22","6","660","674","This paper presents a survey of image synthesis and editing with Generative Adversarial Networks (GANs). GANs consist of two deep networks, a generator and a discriminator, which are trained in a competitive way. Due to the power of deep networks and the competitive training manner, GANs are capable of producing reasonable and realistic images, and have shown great capability in many image synthesis and editing applications. This paper surveys recent GAN papers regarding topics including, but not limited to, texture synthesis, image inpainting, image-to-image translation, and image editing.","1007-0214","","10.23919/TST.2017.8195348","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8195348","image synthesis;image editing;constrained image synthesis;generative adversarial networks;image-to-image translation","Gallium nitride;Image generation;Generators;Image resolution;Feature extraction;Training;Linear programming","image restoration;image texture;neural nets","image synthesis;texture synthesis;image inpainting;image-to-image translation;image editing;generative adversarial networks;GANs;deep networks;GAN","","88","","","","14 Dec 2017","","","TUP","TUP Journals"
"Neural Architecture Search for Generative Adversarial Networks: A Review","V. U. Buthgamumudalige; T. Wirasingha","Informatics Institute of Technology No 57, Ramakrishna Road, Colombo 6, Sri Lanka; Informatics Institute of Technology No 57, Ramakrishna Road, Colombo 6, Sri Lanka","2021 10th International Conference on Information and Automation for Sustainability (ICIAfS)","10 Nov 2021","2021","","","246","251","Achieving competent image generation results in Generative Adversarial Networks (GANs) has been observed to be weighed down by barriers such as mode collapse and being error-prone, due to the time consumption, effort and domain expertise required to arduously design novel Neural Network Architectures through trial-and-error. Neural Architecture Search (NAS) which is a subfield of Automated Machine Learning (AutoML), has gained recent attention for its ability to automate the process of Neural Network architecture generation. NAS has achieved promising results in many computer-vision fields such as image classification, image segmentation, object detection and has now found its way into image generation tasks of Generative Adversarial Networks. Recent works on NAS systems that generate novel GAN architectures have proven this direction of applying NAS is promising by producing results that rival current state-of-the-art human-made architectures. The existing works related to NAS for GANs along with their approaches, performance and image generation results has been explored in this paper to assist and inspire research in the domain of Image Synthesis via NAS for GANs.","2151-1810","978-1-6654-4429-3","10.1109/ICIAfS52090.2021.9605804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9605804","Neural Architecture Search;Generative Adversarial Networks;Automated Machine Learning;Image Generation;Image Synthesis","Knowledge engineering;Image synthesis;Neural networks;Computer architecture;Object detection;Machine learning;Generative adversarial networks","image classification;image segmentation;learning (artificial intelligence);neural nets;object detection","NAS;Neural Network architecture generation;image generation tasks;generative adversarial networks;novel GAN architectures;rival current state-of-the-art human-made architectures;GANs;Neural architecture search;competent image generation results;mode collapse;being error;novel Neural Network Architectures;Neural Architecture Search","","1","","33","IEEE","10 Nov 2021","","","IEEE","IEEE Conferences"
"Virtual Face Animation Generation Based on Conditional Generative Adversarial Networks","J. Zeng; X. He; S. Li; L. Wu; J. Wang","Key Laboratory of China’s Ethnic Languages and Information Technology of Ministry of Education, Northwest Minzu University Key Laboratory of China's Ethnic Languages and Intelligent Processing of Gansu Province, Northwest Minzu University, Lanzhou, Gansu, China; Key Laboratory of China’s Ethnic Languages and Information Technology of Ministry of Education, Northwest Minzu University Key Laboratory of China's Ethnic Languages and Intelligent Processing of Gansu Province, Northwest Minzu University, Lanzhou, Gansu, China; Key Laboratory of China’s Ethnic Languages and Information Technology of Ministry of Education, Northwest Minzu University Key Laboratory of China's Ethnic Languages and Intelligent Processing of Gansu Province, Northwest Minzu University, Lanzhou, Gansu, China; Key Laboratory of China’s Ethnic Languages and Information Technology of Ministry of Education, Northwest Minzu University Key Laboratory of China's Ethnic Languages and Intelligent Processing of Gansu Province, Northwest Minzu University, Lanzhou, Gansu, China; Key Laboratory of China’s Ethnic Languages and Information Technology of Ministry of Education, Northwest Minzu University Key Laboratory of China's Ethnic Languages and Intelligent Processing of Gansu Province, Northwest Minzu University, Lanzhou, Gansu, China","2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)","12 Jan 2023","2022","","","580","583","Face animation is to generate facial animation video with rich expression according to the target face image. Virtual face animation generation has always been a hot topic in the field of computer vision. How to generate real and natural virtual face animation has always been a key issue in this field. In this paper, facial feature points are used to describe different facial shapes, which are used as guidance conditions for generating target facial animations. By improving the Conditional Generation Adversarial Network(CGAN), high quality virtual facial animations are generated. At the same time, the MOS evaluation verifies the effectiveness of the model in generating characters by virtual face animation.","","978-1-6654-6468-0","10.1109/ICICML57342.2022.10009693","Northwest University; Fundamental Research Funds for the Central Universities; Experiment; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009693","Face animation generation;Conditional generative adversarial network;Face feature points;Image generation","Computer vision;Shape;Image synthesis;Computational modeling;Character generation;Machine learning;Generative adversarial networks","computer animation;face recognition;feature extraction","Conditional generative Adversarial networks;facial animation video;facial feature points;high quality virtual facial animations;natural virtual face animation;target face image;target facial animations;virtual face animation generation","","","","9","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Bi-Modality Medical Image Synthesis Using Semi-Supervised Sequential Generative Adversarial Networks","X. Yang; Y. Lin; Z. Wang; X. Li; K. -T. Cheng","School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Department of Radiology, Union Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong","IEEE Journal of Biomedical and Health Informatics","5 Mar 2020","2020","24","3","855","865","In this paper, we propose a bi-modality medical image synthesis approach based on sequential generative adversarial network (GAN) and semi-supervised learning. Our approach consists of two generative modules that synthesize images of the two modalities in a sequential order. A method for measuring the synthesis complexity is proposed to automatically determine the synthesis order in our sequential GAN. Images of the modality with a lower complexity are synthesized first, and the counterparts with a higher complexity are generated later. Our sequential GAN is trained end-to-end in a semi-supervised manner. In supervised training, the joint distribution of bi-modality images are learned from real paired images of the two modalities by explicitly minimizing the reconstruction losses between the real and synthetic images. To avoid overfitting limited training images, in unsupervised training, the marginal distribution of each modality is learned based on unpaired images by minimizing the Wasserstein distance between the distributions of real and fake images. We comprehensively evaluate the proposed model using two synthesis tasks based on three types of evaluate metrics and user studies. Visual and quantitative results demonstrate the superiority of our method to the state-of-the-art methods, and reasonable visual quality and clinical significance. Code is made publicly available at https://github.com/hust- linyi/Multimodal-Medical-Image-Synthesis.","2168-2208","","10.1109/JBHI.2019.2922986","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFA0700402); National Natural Science Foundation of China(grant numbers:61502188); Natural Science Foundation of Hubei Province(grant numbers:ZRMS2017000375); Wuhan Science and Technology Bureau(grant numbers:2017010201010111); Fundamental Research Funds for the Central Universities(grant numbers:2019kfyRCPY118); HUST Acadamic Frontier Youth Team; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8736809","Bi-modality;generative adversarial learning;semi-supervised learning;medical image synthesis","Biomedical imaging;Image synthesis;Gallium nitride;Complexity theory;Training;Task analysis;Generative adversarial networks","image reconstruction;learning (artificial intelligence);medical image processing;neural nets;statistical distributions","unpaired images;fake images;semisupervised sequential generative adversarial networks;semisupervised learning;generative modules;sequential order;synthesis complexity;synthesis order;sequential GAN;supervised training;paired images;real images;synthetic images;unsupervised training;bimodality images;bimodality medical image synthesis;reconstruction losses;marginal distribution;Wasserstein distance;visual quality","Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Male;Multimodal Imaging;Neural Networks, Computer;Prostate;Retina;Supervised Machine Learning","17","","48","IEEE","14 Jun 2019","","","IEEE","IEEE Journals"
"Staged Sketch-to-Image Synthesis via Semi-supervised Generative Adversarial Networks","Z. Li; C. Deng; E. Yang; D. Tao","School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; UBTECH Sydney Artificial Intelligence Centre and the School of Information Technologies, the Faculty of Engineering and Information Technologies, The University of Sydney, Darlington, NSW, Australia","IEEE Transactions on Multimedia","24 Aug 2021","2021","23","","2694","2705","Sketch-based image synthesis is a challenging problem in computer graphics and vision. Existing approaches either require exact edge maps or rely on the retrieval of existing photographs, which limits their applications in real-world scenarios. Accordingly in this work, we propose a staged semi-supervised generative adversarial networks based method for sketch-to-image synthesis, which can directly generate realistic images from novice sketches. More specifically, we first adopt a conditional generative adversarial network (CGAN) to extract class-wise representations from unpaired images. These class-wise representations are then exploited and incorporated with another CGAN, which are used to generate realistic images from sketches. By incorporating the class-wise representations, our method can leverage both the general class information from unpaired images and the targeted object information from input sketches. Additionally, this network architecture also enables us to take full advantage of widely available unpaired images and learn more accurate class representations. Extensive experiments demonstrate, compared with state-of-the-art image translation methods, our approach can achieve more promising results and synthesize images with significantly better Inception Scores and Fréchet Inception Distance.","1941-0077","","10.1109/TMM.2020.3015015","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFE0104100,2016YFE0200400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163292","Gan;image generation;sketch","Gallium nitride;Generative adversarial networks;Image generation;Training;Image edge detection;Task analysis;Image retrieval","image representation;learning (artificial intelligence);neural net architecture","input sketches;network architecture;class representations;image translation methods;sketch-to-image synthesis;semisupervised generative adversarial networks;sketch-based image synthesis;exact edge maps;conditional generative adversarial network;class-wise representations;general class information","","10","","49","IEEE","10 Aug 2020","","","IEEE","IEEE Journals"
"Semi-Supervised Pseudo-Healthy Image Synthesis via Confidence Augmentation","Y. Du; Q. Quan; H. Han; S. K. Zhou","George Mason University, Fairfax, USA; University of Chinese Academy of Sciences, Beijing, China; Peng Cheng Laboratory, Shenzhen, China; Medical Imaging, Robotics, and Analytic Computing Laboratory and Engineering (MIRACLE), School of Biomedical Engineering & Suzhou Institute for Advance Research, University of Science and Technology of China, Suzhou, China","2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)","26 Apr 2022","2022","","","1","4","Pseudo-healthy image synthesis, which computationally synthesizes a pathology-free image from a pathological one, has been proved valuable in many downstream medical image analysis tasks, from lesion detection, data augmentation to clinical surgery suggestion. Thanks to the advancement of generative adversarial networks (GANs), recent studies have made steady progress to synthesize realistic-looking pseudohealthy images with the perseverance of the structure identity as well as the healthy-looking appearance. Nevertheless, it is challenging to generate high-quality pseudo-healthy images in the absence of the lesion segmentation mask. In this paper, we aim to alleviate the needs of a large amount of lesion segmentation labeled data when synthesizing pseudo-healthy images. We propose a semi-supervised pseudo-healthy image synthesis framework which leverages unlabeled pathological image data for efficient pseudo-healthy image synthesis based on a novel confidence augmentation trick. Furthermore, we re-design the network architecture which takes advantage of previous studies and allows for more flexible applications. Extensive experiments have demonstrated the effectiveness of the proposed method in generating realistic-looking pseudo-healthy images and improving downstream task performances.","1945-8452","978-1-6654-2923-8","10.1109/ISBI52829.2022.9761522","Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761522","Generative adversarial networks;semi-supervised learning;medical image synthesis","Image segmentation;Pathology;Image synthesis;Surgery;Network architecture;Semisupervised learning;Space exploration","image segmentation;medical image processing;surgery;visual databases","pathology-free image;downstream medical image analysis tasks;high-quality pseudohealthy images;synthesizing pseudohealthy images;semisupervised pseudohealthy image synthesis framework;unlabeled pathological image data;efficient pseudohealthy image synthesis;improving downstream task performances","","","","12","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"Leaves image synthesis using generative adversarial networks with regularization improvement","M. E. Purbaya; N. A. Setiawan; T. B. Adji","Department of Electrical Engineering and Information Technology, UGM Yogyakarta, Indonesia; Department of Electrical Engineering and Information Technology, UGM Yogyakarta, Indonesia; Department of Electrical Engineering and Information Technology, UGM Yogyakarta, Indonesia","2018 International Conference on Information and Communications Technology (ICOIACT)","30 Apr 2018","2018","","","360","365","Diversity of leaves form a special feature in a plant that can be done research such as image segmentation. However, the thing that is the main issue is the quantity of labeled data. Through image synthesis or image segmentation we are able to add leaf shape needed to use Generative Adversarial Networks (GAN). To train the GAN requires the choice of architecture, initialization parameters and more accurate selection as it often becomes a GAN challenge. Therefore appropriate regularization techniques are needed to address the problem. In the end we were able to segment the 3 leaf shape images using conventional GANs that we have modified using attention to the optimal regularizer parameters. Elastic Net or a combination of L1 and L2 regularizer that we tested on the second model gives error rate 0,105% for discriminator and 20,95% for generator.","","978-1-5386-0954-5","10.1109/ICOIACT.2018.8350780","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8350780","Generative Adversarial Network;GAN;deep learning;leaves;regularizer;image segmentation","Generators;Image segmentation;Gallium nitride;Training;Shape;Manifolds;Data models","image classification;image recognition;image representation;image segmentation;learning (artificial intelligence)","generative adversarial networks;regularization improvement;initialization parameters;optimal regularizer parameters;GAN;leaf shape image segmentation;leaf image synthesis;elastic net;L1 regularizer;L2 regularizer","","2","","32","IEEE","30 Apr 2018","","","IEEE","IEEE Conferences"
"Making a Batik Dataset for Text to Image Synthesis Using Generative Adversarial Networks","A. N. Amalia; A. F. Huda; D. R. Ramdania; M. Irfan","Informatic Engineering Department, UIN Sunan Gunung Djati, Bandung, Indonesia; Mathematic Department, UIN Sunan Gunung Djati, Bandung, Indonesia; Informatic Engineering Department, UIN Sunan Gunung Djati, Bandung, Indonesia; Informatic Engineering Department, UIN Sunan Gunung Djati, Bandung, Indonesia","2019 IEEE 5th International Conference on Wireless and Telematics (ICWT)","3 Feb 2020","2019","","","1","7","Batik is a cultural heritage as well as the identity of the Indonesian nation that needs to be preserved. The use of deep learning allows the process of making batik patterns done by computer through the mechanism of text-to-image synthesis without humans needing to make it directly. The main contribution of this research is to produce a synthetic batik pattern that is similar to the original without removing the characteristics possessed by each batik pattern. This process of text synthesis to images uses the Generative Adversarial Networks (GAN) by first creating a system that can learn from a datasets. A varied and structured dataset can make it easier for the system to learn faster. In this study, a batik dataset was created for the synthesis of text into images.","","978-1-7281-4796-3","10.1109/ICWT47785.2019.8978233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978233","","","art;image colour analysis;image texture;learning (artificial intelligence);neural nets;text analysis","batik dataset;generative adversarial networks;cultural heritage;Indonesian nation;deep learning;text-to-image synthesis;synthetic batik pattern;text synthesis","","1","","13","IEEE","3 Feb 2020","","","IEEE","IEEE Conferences"
"Simulating Bruise and Defects on Mango images using Image-to-Image Translation Generative Adversarial Networks","M. H. bin Ismail; T. R. Razak; R. A. J. M. Gining; S. S. M. Fauzi","Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM), Arau, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM), Arau, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM), Arau, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA (UiTM), Arau, Malaysia","2022 3rd International Conference on Artificial Intelligence and Data Sciences (AiDAS)","26 Oct 2022","2022","","","110","114","A well-balanced dataset is essential for every computer vision task. However, the process of gathering data from various sources is laborious and time-consuming. The lack of samples and class imbalance will reduce the reliability of the CNN model in image classification and recognition tasks. In this research, we examine the use of Image-to-Image translation with conditional GAN for producing synthetic mango images with bruises. We introduce a conditional GAN for producing mango images with controlled surface defects, which is suited for dataset augmentation tasks within the fruit classification problem domain. The findings shows that our networks is able to generate mango images with bruises that are very close to the ground truth with FID value of 37.0","","978-1-6654-9164-8","10.1109/AiDAS56890.2022.9918816","Dana Pembudayaan Penyelidikan Dalaman (DPPD), Universiti Teknologi MARA(grant numbers:600-TNCPI 5/3/DDN (09) (013/2020)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918816","Deep Learning;cGAN;Generative Adversatial Network;mango;image synthesis","Training;Image resolution;Image recognition;Shape;Image color analysis;Machine learning;Generative adversarial networks","agricultural products;computer vision;image classification;learning (artificial intelligence);pattern classification","bruise;Image-to-Image translation generative adversarial networks;computer vision task;image classification;conditional GAN;synthetic mango images;dataset augmentation tasks","","","","24","IEEE","26 Oct 2022","","","IEEE","IEEE Conferences"
"Anime Characters Generation with Generative Adversarial Networks","S. Ruan","China-UK college Haicheng Road No.243, Bao’an District, Shenzhen, Guangdong, China","2022 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA)","18 Oct 2022","2022","","","1332","1335","Automatic anime character generation has the potential to motivate professionals to create new characters while also lowering the cost of creating animation. Adversarial generative networks (GANs) have yielded impressive results in the field of image synthesis. There have been some attempts to apply the GAN model to produce anime character facial images. The increasing number of contributions in recent years requires a systematic summary and analysis of new findings to speed up future research. This paper presents a comprehensive analysis of the GAN-based generation of anime characters. This paper introduces the concept of generative adversarial networks at first, including the adversarial idea as well as its algorithms. Then, the author presents GANs’ application in anime character generation, including the ideas and new findings in this field. Finally, the author also offers a summary of the open challenges for anime character generation.","","978-1-6654-8090-1","10.1109/AEECA55500.2022.9918869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918869","deep learning;generative adversarial networks;anime character generation","Electrical engineering;Systematics;Image resolution;Costs;Image synthesis;Character generation;Computer applications","computer animation;neural nets","automatic anime character generation;adversarial generative networks;anime character facial images;GAN-based generation;systematic summary;image synthesis","","","","11","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"Improving Text Encoding for Retro-Remote Sensing","M. B. Bejiga; G. Hoxha; F. Melgani","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","IEEE Geoscience and Remote Sensing Letters","24 Mar 2021","2021","18","4","622","626","A recent work on retro-remote sensing (converting ancient text descriptions into images) was proposed using a multilabel encoding scheme in which an input text description is represented by a binary vector indicating the presence or absence of specific objects. However, this kind of encoding disregards information such as object attributes and spatial relationship between multiple objects in a description, resulting in images that do not semantically (fully) conform to the input description. In this letter, we propose an improved text-encoding mechanism that takes into account different levels of information available from an input text. The encoded text is then used as conditional information to guide the image synthesis process using generative adversarial networks (GANs). Besides, we present a modified GAN architecture intending to improve the semantic content of the generated images. Both the qualitative and quantitative results obtained indicate that the proposed method is particularly promising.","1558-0571","","10.1109/LGRS.2020.2983851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066830","Generative adversarial networks (GANs);multimodal learning;retro-remote sensing;text-to-image synthesis","Gallium nitride;Training;Generators;Generative adversarial networks;Context modeling;Encoding;Sensors","encoding;image coding;neural net architecture;remote sensing;text analysis","input text description;binary vector;object attributes;spatial relationship;encoded text;conditional information;image synthesis;text encoding;retro-remote sensing;ancient text;text-encoding mechanism;multilabel encoding;generative adversarial networks;GAN architecture","","4","","20","IEEE","14 Apr 2020","","","IEEE","IEEE Journals"
"A Comparison between AttnGAN and DF GAN: Text to Image Synthesis","P. Sumi; S. Sindhuja; S. Sureshkumar","Department of Computer Science and Engineering, Bannari Amman Institute of Technology, Sathyamangalam; Department of Computer Science and Engineering, Bannari Amman Institute of Technology, Sathyamangalam; Department of Computer Science and Engineering, Bannari Amman Institute of Technology, Sathyamangalam","2021 3rd International Conference on Signal Processing and Communication (ICPSC)","15 Jun 2021","2021","","","615","619","Nowadays conversion from text to high resolution image is a challenging task due to its wide variety of application area. For text to image conversion almost all systems use Generative Adversarial Networks as the basic part of the system and GAN guarantees semantic consistency between the text input and the generated image output. In this paper we are comparing two algorithms that is used for generating image from text. The first algorithm is the AttnGAN and the second one is the DF-GAN. AttnGAN builds on top of StackGAN by using attention network which allows it to capture word level information along with the broader sentence level information. The second algorithm is the DF-GAN, which uses single generator and discriminator model to synthesize high resolution images and also uses Matching-Aware Gradient Penalty (MA-GP) to get real images with real description. The model contains a Deep text-image Fusion Block (DFBlock) to generate image features from text. Both algorithms work efficiently for image generation from text but DF-GAN generates the perfect output than AttnGAN. The AttnGAN always focus on the textual part to generate output image but DF-GAN also focuses on background of image.","","978-1-6654-2864-4","10.1109/ICSPC51351.2021.9451789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9451789","AttnGAN;DF GAN;BLSTM;CNN","Image resolution;Image synthesis;Fuses;Semantics;Signal processing algorithms;Signal processing;Generative adversarial networks","deep learning (artificial intelligence);feature extraction;image fusion;image resolution;text analysis","DF-GAN;AttnGAN;text to image synthesis;image conversion;generative adversarial networks;GAN;text input;image output;sentence level information;Deep text-image Fusion Block;image features;image generation;matching-aware gradient penalty;MA-GP;StackGAN;attention network","","1","","10","IEEE","15 Jun 2021","","","IEEE","IEEE Conferences"
"Medical MR Image Synthesis using DCGAN","S. Divya; L. P. Suresh; A. John",TKM College of Engineering APJ Abdul Kalam Technological University; Baselios Mathew II College of Engineering; TKM College of Engineering,"2022 First International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)","10 May 2022","2022","","","01","04","Generative Adversarial Networks (GANs) have been extensively gained considerable attention since 2014. Irrefutably saying, their most remarkable success has been made in domains such as computer vision and medical image processing. Despite the noteworthy success attained to date, applying GANs to real world problems still posses significant challenges, one among which is diversity of image generation and detection of fake images from real ones. Focusing on the extend to which various GAN models have made headway against these challenges, this study provides an overview of DCGAN architecture and its application as a synthetic data generator and act an a binary classifier, which detects real or fake images using brain tumorous Magnetic Resonance Imaging (MRI) dataset.","","978-1-6654-3647-2","10.1109/ICEEICT53079.2022.9768647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9768647","GAN;medical image processing;synthetic data;brain tumor;MRI","Computer vision;Image synthesis;Magnetic resonance imaging;Computational modeling;Focusing;Computer architecture;Generative adversarial networks","biomedical MRI;brain;image segmentation;medical image processing;tumours","brain tumorous Magnetic Resonance Imaging dataset;medical MR image synthesis;Generative Adversarial Networks;GANs;computer vision;medical image processing;noteworthy success;image generation;fake images;GAN models;DCGAN architecture;synthetic data generator","","","","16","IEEE","10 May 2022","","","IEEE","IEEE Conferences"
"SemGAN: Text to Image Synthesis from Text Semantics using Attentional Generative Adversarial Networks","A. Nasr; R. Mutasim; H. Imam","Department of Electrical and Electronic Engineering, University of Khartoum, Khartoum, Sudan; Department of Electrical and Electronic Engineering, University of Khartoum, Khartoum, Sudan; Department of Electrical and Electronic Engineering, University of Khartoum, Khartoum, Sudan","2020 International Conference on Computer, Control, Electrical, and Electronics Engineering (ICCCEEE)","17 May 2021","2021","","","1","6","Text to Image Synthesis is the procedure of automatically creating a realistic image from a particular text description. There are numerous innovative and practical applications for text to image synthesis, including image processing and compute-raided design. Using Generative Adversarial Networks (GANs) alongside the Attention mechanism has led to huge improvements lately. The fine-grained attention mechanism, although powerful, does not preserve the general description information well in the generator since it only attends to the text description at word-level (fine-grained). We propose incorporating the whole sentence semantics when generating images from captions to enhance the attention mechanism outputs. According to experiments, on our model produces more robust images with a better semantic layout. We use the Caltech birds dataset to run experiments on both models and validate the effectiveness of our proposal. Our model boosts the original AttnGAN Inception score by +4.13% and the Fréchet Inception Distance score by +13.93%. Moreover, an empirical analysis is carried out on the objective and subjective measures to: (i) address and overcome the limitations of these metrics (ii) verify that performance improvements are due to fundamental algorithmic changes rather than initialization and fine-tuning as with GANs models.","","978-1-7281-9111-9","10.1109/ICCCEEE49695.2021.9429602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9429602","","Measurement;Digital control;Image synthesis;Computational modeling;Semantics;Layout;Generative adversarial networks","image processing;image segmentation;natural language processing;RAID;text analysis","realistic image;particular text description;numerous innovative applications;image synthesis;image processing;fine-grained attention mechanism;general description information;generating images;robust images;text semantics;attentional Generative Adversarial Networks","","","","23","IEEE","17 May 2021","","","IEEE","IEEE Conferences"
"SphericGAN: Semi-supervised Hyper-spherical Generative Adversarial Networks for Fine-grained Image Synthesis","T. Chen; Y. Zhang; X. Huo; S. Wu; Y. Xu; H. S. Wong","School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; Peng Cheng Laboratory; Communication and Computer Network Laboratory of Guangdong; Department of Computer Science, City University of Hong Kong","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","9991","10000","Generative Adversarial Network (GAN)-based models have greatly facilitated image synthesis. However, the model performance may be degraded when applied to finegrained data, due to limited training samples and subtle distinction among categories. Different from generic GAN-s, we address the issue from a new perspective of discovering and utilizing the underlying structure of real data to explicitly regularize the spatial organization of latent space. To reduce the dependence of generative models on labeled data, we propose a semi-supervised hyper-spherical GAN for class-conditional fine-grained image generation, and our model is referred to as SphericGAN. By projecting random vectors drawn from a prior distribution onto a hyper-sphere, we can model more complex distributions, while at the same time the similarity between the resulting latent vectors depends only on the angle, but not on their magnitudes. On the other hand, we also incorporate a mapping network to map real images onto the hyper-sphere, and match latent vectors with the underlying structure of real data via real-fake cluster alignment. As a result, we obtain a spatially organized latent space, which is useful for capturing class-independent variation factors. The experi-mental results suggest that our SphericGAN achieves state-of-the-art performance in synthesizing high-fidelity images with precise class semantics.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00976","China Scholarship Council(grant numbers:62072188,62072189); Natural Science Foundation of Guangdong Province(grant numbers:2020A1515010484); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879126","Self-& semi-& meta- Image and video synthesis and generation","Training;Computer vision;Image synthesis;Semantics;Training data;Organizations;Generative adversarial networks","Bayes methods;image classification;image matching;learning (artificial intelligence);vectors","training samples;subtle distinction;generic GAN;spatial organization;generative models;semisupervised hyper-spherical GAN;class-conditional fine-grained image generation;SphericGAN;random vectors;hyper-sphere;resulting latent vectors;mapping network;map real images;spatially organized latent space;high-fidelity images;semisupervised hyper-spherical generative Adversarial networks;fine-grained image synthesis;Generative Adversarial Network-based models;finegrained data","","1","","41","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"ResFPA-GAN: Text-to-Image Synthesis with Generative Adversarial Network Based on Residual Block Feature Pyramid Attention","J. Sun; Y. Zhou; B. Zhang","Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Communication, University of China, China","2019 IEEE International Conference on Advanced Robotics and its Social Impacts (ARSO)","6 Jan 2020","2019","","","317","322","Text-to-image synthesis based on generative adversarial networks (GAN) is a challenging task. The developed methods have show prominent progress on visual quality of the synthesized images, but it still face challenge in the image synthesis of details. In this paper, we introduce an image synthesis algorithm based on semantic description and propose a residual block feature pyramid attention generative adversarial network, called ResFPA-GAN. This network introduces multiscale feature fusion by embedding feature pyramid structure to achieve the fine-grained image synthesis. The quality of the image synthesis can be improved via the iterative training of GAN, while the reference of attention can enhance the network's learning of the details of image texture. Through extensive experimental comparison on the CUB dataset, our method can achieve significant improvement on the variety and authenticity for the synthesised images.","2162-7576","978-1-7281-3176-4","10.1109/ARSO46408.2019.8948717","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948717","","Training;Visualization;Image synthesis;Semantics;Generative adversarial networks;Task analysis;Robots","feature extraction;image fusion;image texture;iterative methods;learning (artificial intelligence);text analysis","multiscale feature fusion;feature pyramid structure;fine-grained image synthesis;image texture;text-to-image synthesis;synthesized images;image synthesis algorithm;residual block feature pyramid attention generative adversarial network;ResFPA-GAN;CUB dataset","","3","","24","IEEE","6 Jan 2020","","","IEEE","IEEE Conferences"
"Controllable Image Dataset Construction Using Conditionally Transformed Inputs in Generative Adversarial Networks","F. Makhmudkhujaev; J. Kwon; I. K. Park","Department of Information and Communication Engineering, Inha University, Incheon, South Korea; School of Computer Science and Engineering, Chung-Ang University, Seoul, South Korea; Department of Information and Communication Engineering, Inha University, Incheon, South Korea","IEEE Access","29 Oct 2021","2021","9","","144699","144712","In this paper, we tackle the well-known problem of dataset construction from the point of its generation using generative adversarial networks (GAN). As semantic information of the dataset should have a proper alignment with images, controlling the image generation process of GAN comes to the first position. Considering this, we focus on conditioning the generative process by solely utilizing conditional information to achieve reliable control over the image generation. Unlike the existing works that consider the input (noise or image) in conjunction with conditions, our work considers transforming the input directly to the conditional space by utilizing the given conditions only. By doing so, we reveal the relations between conditions to determine their distinct and reliable feature space without the impact of input information. To fully leverage the conditional information, we propose a novel architectural framework (i.e., conditional transformation) that aims to learn features only from a set of conditions for guiding a generative model by transforming the input to the generator. Such an approach enables controlling the generator by setting its inputs according to the specific conditions necessary for semantically correct image generation. Given that the framework operates at the initial stage of generation, it can be plugged into any existing generative models and trained in an end-to-end manner together with the generator. Extensive experiments on various tasks, such as novel image synthesis and image-to-image translation, demonstrate that the conditional transformation of inputs facilitates solid control over the image generation process and thus shows its applicability for use in dataset construction.","2169-3536","","10.1109/ACCESS.2021.3122834","National Research Foundation of Korea (NRF); Korean Government through the MSIT(grant numbers:NRF-2019R1A2C1006706); Institute of Information and Communications Technology Planning and Evaluation (IITP); Korean Government through the MSIT (Artificial Intelligence Convergence Research Center, Inha University)(grant numbers:2020-0-01389); Inha University Research Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585485","Dataset construction;conditional image generation;generative adversarial networks;conditional transformation","Generators;Image synthesis;Generative adversarial networks;Task analysis;Process control;Reliability;Transforms","feature extraction;neural nets","controllable image dataset construction;conditionally transformed inputs;generative adversarial networks;GAN;semantic information;image generation process;generative process;conditional information;reliable control;conditional space;distinct feature space;reliable feature space;conditional transformation;generative model;image generation;image synthesis;image-to-image translation;solid control","","","","68","CCBY","26 Oct 2021","","","IEEE","IEEE Journals"
"Imagination with Generative Adversarial Networks for Object Detection","A. Srichandan; B. Gupta; P. P; N. M. V","School of Electronics Engineering, Vellore Institute of Technology, Vellore, India; School of Electronics Engineering, Vellore Institute of Technology, Vellore, India; School of Electronics Engineering, Vellore Institute of Technology, Vellore, India; School of Electronics Engineering, Vellore Institute of Technology, Vellore, India","2021 Innovations in Power and Advanced Computing Technologies (i-PACT)","8 Feb 2022","2021","","","1","6","Assistive and home service robots have been an important as well as lucrative applications in the burgeoning robotics technology. In this paper we introduce an approach for home service mobile robots to assist and fetch objects based on speech commands given by the user, from its surroundings. We have tried to make the robot learn to fetch using imaginative learning (TI) by making use of text to image GANs in the object detection pipeline. The robot will fetch the object from different rooms in the environment by imagining the object based on the user's speech command and compare it with real ones. We use deep learning techniques and models such as speech to text, text to image deep learning, deep image similarity, classical computer vision, SLAM and path planning of robot to achieve the overall fetching object pipeline and finally compare similarity score of generated images and detected images using Deep Image Similarity by the robot.","","978-1-6654-2691-6","10.1109/i-PACT52855.2021.9696929","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696929","Generative Adversarial Networks;Text to image synthesis;Mobile robots;Object Detection","Deep learning;Technological innovation;Simultaneous localization and mapping;Service robots;Pipelines;Object detection;Reinforcement learning","computer vision;learning (artificial intelligence);mobile robots;object detection;object recognition;path planning;service robots;SLAM (robots)","fetching object pipeline;deep image similarity;imagination;generative adversarial networks;home service robots;lucrative applications;burgeoning robotics technology;home service mobile robots;speech command;imaginative learning;image GANs;object detection pipeline;different rooms;deep learning techniques;image deep learning","","","","19","IEEE","8 Feb 2022","","","IEEE","IEEE Conferences"
"Progressive Semantic Image Synthesis via Generative Adversarial Network","K. Yue; Y. Li; H. Li","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China","2019 IEEE Visual Communications and Image Processing (VCIP)","23 Jan 2020","2019","","","1","4","Semantic image synthesis via text description is a desirable and challenging task, which requires more protection of the text irrelevant content in the original image. Existing methods directly modify the original image, which become more difficult when encountering high resolution image, and the generated images are also blurred and lack in detail. This paper presents a novel network architecture to progressively manipulate an image starting from low-resolution, while introducing the original image of corresponding size at different stages with our proposed union module to avoid losing of detail. And the progressive design of the network allows us to modify the image from coarse into fine. Compared with the previous methods, our new method can successfully manipulate a high resolution image and generate a new image with background protection and fine details. The experimental results on CUB-200-2011 dataset show that the proposed approach outperforms existing methods in terms of image detail, background protection and high resolution generation.","2642-9357","978-1-7281-3723-0","10.1109/VCIP47243.2019.8966069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966069","Generative adversarial network;Semantic image synthesis;Progressive","Image resolution;Shape;Feature extraction;Image synthesis;Training;Generative adversarial networks;Visual effects","image resolution;neural net architecture;text analysis","text irrelevant content;high resolution image;background protection;high resolution generation;progressive semantic image synthesis;generative adversarial network;text description;CUB-200-2011 dataset;network architecture;original image;union module","","1","","16","IEEE","23 Jan 2020","","","IEEE","IEEE Conferences"
"Pix2Pix GAN Image Synthesis To Detect Electric Vehicle License Plate","A. Kalpana.; A. John","Central Research Laboratory Bharat Electronics Limited, Bengaluru, India; Central Research Laboratory Bharat Electronics Limited, Bengaluru, India","2022 IEEE 4th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)","22 Dec 2022","2022","","","439","443","The area of image processing is more intensive in development and research activities for decades. The role of image processing is huge in modeling, analytics, communication, computation, information security, information forensics and smart city application. Images are ubiquitous in day to day life and images or videos play dominant role in monitoring applications. But when it comes to development of specific application, collection of data is a very challenging task. Nowadays deep learning plays a significant role for generation of data. Robust technologies like Generative Adversarial Network (GAN) and Cycle GAN play a crucial role for generating realistic images with super resolution. GAN and its associated methods used for image synthesis improve the accuracy of deep learning models. In this paper, we analyze challenges of license plate recognition in realistic situation and experiments demonstrate that GAN can generate realistic images to improve the accuracy of license plate recognition.","","978-1-6654-6246-4","10.1109/ICCCMLA56841.2022.9989063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989063","Generative Adversarial Network;Image synthesis;Generator;License Plate Recognition;Deep Learning","Deep learning;Image synthesis;Smart cities;Information security;Generative adversarial networks;Electric vehicles;Task analysis","deep learning (artificial intelligence);electric vehicles;gallium compounds;image recognition;image resolution;realistic images;traffic engineering computing","Cycle GAN;deep learning models;electric vehicle license plate;generative adversarial network;image processing;information forensics;information security;license plate recognition;monitoring applications;pix2pix GAN image synthesis;realistic images;smart city application;super resolution","","","","21","IEEE","22 Dec 2022","","","IEEE","IEEE Conferences"
"Multi-Sentence Auxiliary Adversarial Networks for Fine-Grained Text-to-Image Synthesis","Y. Yang; L. Wang; D. Xie; C. Deng; D. Tao","School of Computer Science and Technology, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Electronic Engineering, Xidian University, Xi’an, China; School of Computer Science, Faculty of Engineering, The University of Sydney at Darlington, Darlington, NSW, Australia","IEEE Transactions on Image Processing","12 Feb 2021","2021","30","","2798","2809","Due to the development of Generative Adversarial Networks (GANs), significant progress has been achieved in text-to-image synthesis task. However, most previous works have only focus on learning the semantic consistency between paired images and sentences, without exploring the semantic correlation between different yet related sentences that describe the same image, which leads to significant visual variation among the synthesized images. Accordingly, in this article, we propose a new method for text-to-image synthesis, dubbed Multi-sentence Auxiliary Generative Adversarial Networks (MA-GAN); this approach not only improves the generation quality but also guarantees the generation similarity of related sentences by exploring the semantic correlation between different sentences describing the same image. More specifically, we propose a Single-sentence Generation and Multi-sentence Discrimination (SGMD) module that explores the semantic correlation between multiple related sentences in order to reduce the variation between their generated images and enhance the reliability of the generated results. Moreover, a Progressive Negative Sample Selection mechanism (PNSS) is designed to mine more suitable negative samples for training, which can effectively promote detailed discrimination ability in the generative model and facilitate the generation of more fine-grained results. Extensive experiments on Oxford-102 and CUB datasets reveal that our MA-GAN significantly outperforms the state-of-the-art methods.","1941-0042","","10.1109/TIP.2021.3055062","National Natural Science Foundation of China(grant numbers:62071361); National Key Research and Development Program of China(grant numbers:2017YFE0104100); China Research Project(grant numbers:6141B07270429); Australian Research Council(grant numbers:FL-170100117,DP-180103424); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9345477","Conditional image synthesis;generative adversarial networks;attention mechanism;negative sample learning","Semantics;Task analysis;Visualization;Training;Generative adversarial networks;Correlation;Birds","image processing;natural language processing;neural nets;text analysis","CUB datasets;Oxford-102;PNSS;progressive negative sample selection;SGMD;multisentence auxiliary adversarial networks;multisentence auxiliary generative adversarial networks;single-sentence generation;multisentence discrimination;generation similarity;generation quality;MA-GAN;visual variation;semantic correlation;paired images;semantic consistency;text-to-image synthesis","","14","","48","IEEE","2 Feb 2021","","","IEEE","IEEE Journals"
"Dual-Cycle Constrained Bijective Vae-Gan For Tagged-To-Cine Magnetic Resonance Image Synthesis","X. Liu; F. Xing; J. L. Prince; A. Carass; M. Stone; G. E. Fakhri; J. Woo","Dept. of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Dept. of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Dept. of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Dept. of Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA; Dept. of Neural and Pain Sciences, University of Maryland School of Dentistry, Baltimore, MD, USA; Dept. of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Dept. of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA","2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)","25 May 2021","2021","","","1448","1452","Tagged magnetic resonance imaging (MRI) is a widely used imaging technique for measuring tissue deformation in moving organs. Due to tagged MRI’s intrinsic low anatomical resolution, another matching set of cine MRI with higher resolution is sometimes acquired in the same scanning session to facilitate tissue segmentation, thus adding extra time and cost. To mitigate this, in this work, we propose a novel dual-cycle constrained bijective VAE-GAN approach to carry out tagged-to-cine MR image synthesis. Our method is based on a variational autoencoder backbone with cycle reconstruction constrained adversarial training to yield accurate and realistic cine MR images given tagged MR images. Our framework has been trained, validated, and tested using 1,768, 416, and 1,560 subject-independent paired slices of tagged and cine MRI from twenty healthy subjects, respectively, demonstrating superior performance over the comparison methods. Our method can potentially be used to reduce the extra acquisition time and cost, while maintaining the same workflow for further motion analyses.","1945-8452","978-1-6654-1246-9","10.1109/ISBI48211.2021.9433852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433852","Tagged MRI;image synthesis;deep learning;Generative adversarial networks","Training;Image segmentation;Image resolution;Image synthesis;Magnetic resonance imaging;Motion segmentation;Magnetic resonance","biological tissues;biomechanics;biomedical MRI;cardiology;image segmentation;medical image processing","tissue segmentation;VAE-GAN approach;tagged-to-cine MR image synthesis;cycle reconstruction;cine MR images;tagged MR images;dual-cycle constrained bijective vae-gan;magnetic resonance image synthesis;tagged magnetic resonance imaging;imaging technique;tissue deformation;MRI intrinsic low-anatomical resolution","","6","","20","IEEE","25 May 2021","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks based image generation of insulator","Z. Liu; H. Qiu; L. Weng; M. Luo; X. Zhu; Y. Zhang","State Grid Hangzhou Xiaoshan Power Supply Company, Hangzhou, China; State Grid Hangzhou Xiaoshan Power Supply Company, Hangzhou, China; State Grid Hangzhou Xiaoshan Power Supply Company, Hangzhou, China; Zhejiang Zhongxin Power Engineering Construction Co., Ltd, Hangzhou, China; Zhejiang Zhongxin Power Engineering Construction Co., Ltd, Hangzhou, China; Zhejiang Zhongxin Power Engineering Construction Co., Ltd, Hangzhou, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","547","552","In recent years, the combination of artificial intelligence and power systems has been increasing. However, the collection of image datasets of power equipment is limited by places and environments, so the number of collected datasets is relatively small, which makes it unable to provide sufficient data support for specific applications. We propose to use Generative Adversarial Networks to generate images of electrical equipment from existing sparse datasets, thereby increasing the size of the electrical equipment dataset. In this paper, three generative adversarial networks with different structures are used to generate images of insulator and to assess the caliber of images produced by the three models, the analysis of loss function image and FID score of generated images is performed.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023599","Generative Adversarial Network;insulator;image generation","Training;Analytical models;Automation;Image synthesis;Insulators;Generative adversarial networks;Generators","artificial intelligence;image processing;insulators;neural nets;power apparatus;power engineering computing","artificial intelligence;data support;electrical equipment dataset;FID score;generative adversarial network;image datasets;image generation;insulator;loss function image;power equipment;power systems;sparse datasets","","","","25","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
"Image Synthesis in Multi-Contrast MRI With Conditional Generative Adversarial Networks","S. U. Dar; M. Yurt; L. Karacan; A. Erdem; E. Erdem; T. Çukur","NationalMagnetic Resonance Research Center, Bilkent University, Ankara, Turkey; NationalMagnetic Resonance Research Center, Bilkent University, Ankara, Turkey; Department of Computer Engineering, Hacettepe University, Ankara, Turkey; Department of Computer Engineering, Hacettepe University, Ankara, Turkey; Department of Computer Engineering, Hacettepe University, Ankara, Turkey; Neuroscience Program, Sabuncu Brain Research Center, Bilkent University, Ankara, Turkey","IEEE Transactions on Medical Imaging","1 Oct 2019","2019","38","10","2375","2388","Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1- and T2- weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.","1558-254X","","10.1109/TMI.2019.2901750","European Molecular Biology Organization(grant numbers:IG3028); Türkiye Bilimsel ve Teknolojik Araştirma Kurumu(grant numbers:118E256); BAGEP fellowship awarded; Nvidia; separate TUBA GEBIP fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653423","Generative adversarial network;image synthesis;multi-contrast MRI;pixel-wise loss;cycleconsistency loss","Magnetic resonance imaging;Image generation;Generative adversarial networks;Gallium nitride;Feature extraction;Task analysis;Generators","biomedical MRI;image reconstruction;image registration;medical image processing;neural nets","T1-weighted images;neural networks;intermediate-to-high frequency;T2-weighted images;unregistered images;cycle-consistency loss;registered multicontrast images;perceptual losses;pixel-wise losses;multicontrast MRI synthesis;target images;nonlinear intensity transformation;multicontrast synthesis;diagnostic utility;unacquired contrasts;scan time limitations;diagnostic information;conditional generative adversarial networks;image synthesis","Brain;Brain Neoplasms;Glioma;Humans;Image Interpretation, Computer-Assisted;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Neural Networks, Computer","213","","80","IEEE","26 Feb 2019","","","IEEE","IEEE Journals"
"Cross-Modal Semantic Matching Generative Adversarial Networks for Text-to-Image Synthesis","H. Tan; X. Liu; B. Yin; X. Li","School of Mathematical Sciences, Dalian University of Technology, Dalian, China; School of Mathematical Sciences, Dalian University of Technology, Dalian, China; Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; School of Electrical Engineering & Computer Science, and Center for Computation & Technology, Louisiana State University, Baton Rouge, LA, USA","IEEE Transactions on Multimedia","8 Feb 2022","2022","24","","832","845","Synthesizing photo-realistic images based on text descriptions is a challenging image generation problem. Although many recent approaches have significantly advanced the performance of text-to-image generation, to guarantee semantic matchings between the text description and synthesized image remains very challenging. In this paper, we propose a new model, Cross-modal Semantic Matching Generative Adversarial Networks (CSM-GAN), to improve the semantic consistency between text description and synthesized image for a fine-grained text-to-image generation. Two new modules are proposed in CSM-GAN: Text Encoder Module (TEM) and Textual-Visual Semantic Matching Module (TVSMM). TVSMM is aimed at making the distance of the pairs of synthesized image and its corresponding text description closer, in global semantic embedding space, than those of mismatched pairs. This improves the semantic consistency and consequently, the generalizability of CSM-GAN. In TEM, we introduce Text Convolutional Neural Networks (Text_CNNs) to capture and highlight local visual features in textual descriptions. Thorough experiments on two public benchmark datasets demonstrated the superiority of CSM-GAN over other representative state-of-the-art methods.","1941-0077","","10.1109/TMM.2021.3060291","Ministry of Science and Technology of the Peopleapos;s Republic of China(grant numbers:2018AAA0102003); National Natural Science Foundation of China(grant numbers:61976040); National Natural Science Foundation of China(grant numbers:U19B2039); National Science Foundation(grant numbers:OIA-1946231); Science and Technology Foundation of Dalian(grant numbers:2018J11CY010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359527","Cross-modal semantic matching;generative adversarial network (GAN);text-to-image synthesis;text _CNNs","Semantics;Task analysis;Generative adversarial networks;Generators;Gallium nitride;Feature extraction;Visualization","convolutional neural nets;feature extraction;image matching;realistic images;text analysis","Text_CNNs;CSM-GAN;text-to-image synthesis;semantic consistency;fine-grained text-to-image generation;text description;semantic embedding space;text convolutional neural networks;cross-modal semantic matching generative adversarial networks;photo-realistic images synthesis;text encoder module;textual-visual semantic matching module","","4","","50","IEEE","19 Feb 2021","","","IEEE","IEEE Journals"
"EndoVAE: Generating Endoscopic Images with a Variational Autoencoder","D. E. Diamantis; P. Gatoula; D. K. Iakovidis","Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece; Department of Computer Science and Biomedical Informatics, University of Thessaly, Lamia, Greece","2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)","11 Jul 2022","2022","","","1","5","The generalization performance of deep learning models is closely associated with the number and diversity of data available upon training. While in many applications there is a large number of data available in public, in domains such as medical image analysis, the data availability is limited. This can be largely attributed to data privacy legislations, including the General Data Protection Regulation (GDPR), and the cost of data annotation by experts. Aiming to address this issue, data augmentation approaches employing deep generative models have emerged. Existing augmentation techniques are primarily based on Generative Adversarial Networks (GANs). However, ill-posed training issues of GANs such as nonconvergence, mode collapse and instability in conjunction with their demand for large scale training datasets, complicate their use in medical imaging modalities. Motivated by these issues, this paper investigates the performance of alternative generative models i.e., Variational Autoencoders (VAEs) in endoscopic image synthesis tasks. Contrary to the conventional GAN-based approaches that aiming at augmenting the existing endoscopic datasets the proposed methodology constitutes feasible the complete substitution of medical imaging datasets from real individuals with artificially generated ones. The experimental results obtained validate the effectiveness of the proposed methodology over the state-of-art.","","978-1-6654-7822-9","10.1109/IVMSP54334.2022.9816329","European Regional Development Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9816329","Wireless Capsule Endoscopy;Variational Autoencoders;Medical Image Synthesis","Training;Wireless communication;Solid modeling;Image synthesis;Generative adversarial networks;Data models;Task analysis","data privacy;data protection;endoscopes;learning (artificial intelligence);legislation;medical image processing;security of data","EndoVAE;generating endoscopic images;Variational autoencoder;generalization performance;deep learning models;medical image analysis;data availability;data privacy legislations;General Data Protection Regulation;data annotation;data augmentation;deep generative models;augmentation techniques;Generative Adversarial Networks;GANs;ill-posed training issues;mode collapse;instability;scale training datasets;medical imaging modalities;alternative generative models i.e;Variational Autoencoders;endoscopic image synthesis tasks;conventional GAN-based approaches;existing endoscopic datasets;medical imaging datasets;artificially generated ones","","","","24","IEEE","11 Jul 2022","","","IEEE","IEEE Conferences"
"Adversarial Network for Photographic Image Synthesis from Fine-grained Captions","P. Desai; C. Sujatha; R. Shanbhag; R. Gotur; R. Hebbar; P. Kurtkoti","Computer Science and Engineering, KLE Technological University, Hubballi, India; Computer Science and Engineering, KLE Technological University, Hubballi, India; Computer Science and Engineering, KLE Technological University, Hubballi, India; Computer Science and Engineering, KLE Technological University, Hubballi, India; Computer Science and Engineering, KLE Technological University, Hubballi, India; Computer Science and Engineering, KLE Technological University, Hubballi, India","2021 International Conference on Intelligent Technologies (CONIT)","4 Aug 2021","2021","","","1","5","Automatic synthesis of realistic images from a given caption of text is a fascinating research idea and useful in many applications such as image in-painting, photo-editing, computer-aided design, etc. Current AI techniques are still exploring in this direction. However, researchers are currently developing text-to-image synthesis networks focused on the learning of discriminative text and image features using continuous and generalized robust neural network architectures. Many applications use deep convolution Generative Adversarial Networks (GANs) to construct extremely persuasive representations of explicit categories like faces, album covers, birds, creatures, and room interiors, among others. With the progress of generative models, neural networks can not only recognize images but are used to generate audio and realistic images as well. In the proposed work, the authors have used GAN-CLS architecture to create images from given text descriptions/captions. The experiment uses the CUB-200 dataset, which contains 11,788 bird images from 200 categories, as well as the Oxford-102 dataset, which contains 8,189 flower images from 102 categories. The proposed system's performance is assessed and compared to that of other systems.","","978-1-7281-8583-5","10.1109/CONIT51480.2021.9498513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9498513","photographic;GAN;text;captions;image synthesis","Visualization;Image recognition;Image synthesis;System performance;Neural networks;Computer architecture;Birds","deep learning (artificial intelligence);feature extraction;image classification;image colour analysis;image recognition;image segmentation;object recognition;text analysis;visual databases","Oxford-102 dataset;CUB-200 dataset;flower images;bird images;GAN-CLS architecture;neural networks;deep convolution generative adversarial networks;generalized robust neural network architectures;text-to-image synthesis networks;AI techniques;realistic images;fine-grained captions;photographic image synthesis","","1","","27","IEEE","4 Aug 2021","","","IEEE","IEEE Conferences"
"Whole-body PET Image Synthesis from Low-Dose Images Using Cycle-consistent Generative Adversarial Networks","A. Sanaat; I. Shiri; H. Arabi; I. Mainta; R. Nkoulou; H. Zaidi","Division of Nuclear Medicine &#x0026; Molecular Imaging, Geneva University Hospital, Geneva, Switzerland; Division of Nuclear Medicine &#x0026; Molecular Imaging, Geneva University Hospital, Geneva, Switzerland; Division of Nuclear Medicine &#x0026; Molecular Imaging, Geneva University Hospital, Geneva, Switzerland; Division of Nuclear Medicine &#x0026; Molecular Imaging, Geneva University Hospital, Geneva, Switzerland; Geneva University Neurocenter, Geneva University, Geneva, Switzerland; Geneva University Neurocenter, Geneva University, Geneva, Switzerland","2020 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)","12 Aug 2021","2020","","","1","3","This work sets out to investigate the performance of full-dose (FD) PET prediction from fast or low-dose (LD) whole-body (WB) PET scans using convolutional neural networks. One hundred patients who underwent WB PET/CT examinations were retrospectively used to develop LD to FD PET conversion models. The patients underwent two separate WB examinations lasting ~27 min (regular scan) and ~3 min (fast or LD scan) acquisition times. The fast (3 min) WB PET examinations are equivalent to 1/8<sup>th</sup> of the full-dose PET scan. A residual neural network (ResNet) and a modified cycle-consistent generative adversarial network (CycleGAN) architecture were employed to model the LD to FD PET conversion. The quality of synthetic PET images produced by ResNet and CycleGAN models, referred to as RNET and CGAN, respectively, were evaluated by two nuclear medicine physicians using a five-point scoring. Quantitative metrics, including structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), mean square error (MSE) and standardized uptake value (SUV) bias were calculated within the left/right lung, brain, liver, and one hundred hot spots (malignant lesions) in PET images predicted using CGAN and RNET models. The physicians assigned scores of 3.88 and 4.92 (out of 5) to the CGAN-predicted FD PET images for the neck & trunk and brain regions, respectively. Considering the PSNR and SSIM metrics, the CGAN model exhibited superior performance with PSNR=39.08±3.56 and SSIM=0.97±0.02 compared to the RNET model with PSNR=34.91±1.50 and SSIM=0.93±0.04. Overall, the CGAN model outperformed the RNET model exhibiting lower SUV bias and higher image quality in the predicted FD PET images.","2577-0829","978-1-7281-7693-2","10.1109/NSS/MIC42677.2020.9507947","Swiss National Science foundation(grant numbers:SNFN 320030_176052); Private Foundation of Geneva University Hospitals(grant numbers:RC-06-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507947","","Measurement;Nuclear medicine;PSNR;Whole-body PET;Medical services;Predictive models;Brain modeling","brain;cancer;computerised tomography;liver;lung;mean square error methods;medical image processing;neural nets;positron emission tomography;radiation therapy;tumours","SSIM;CGAN model;RNET model;higher image quality;body PET image synthesis;low-dose images;cycle-consistent generative adversarial networks;full-dose PET prediction;whole-body PET scans;convolutional neural networks;FD PET conversion models;separate WB examinations;regular scan;PET examinations;full-dose PET scan;residual neural network;ResNet;modified cycle-consistent;synthetic PET images;CycleGAN models;nuclear medicine physicians;peak signal-to-noise ratio;mean square error;CGAN-predicted FD PET images;PSNR=39;PSNR=34;time 27.0 min;time 3.0 min","","3","","10","IEEE","12 Aug 2021","","","IEEE","IEEE Conferences"
"Medical Image Synthesis with Generative Adversarial Networks for Tissue Recognition","Q. Zhang; H. Wang; H. Lu; D. Won; S. W. Yoon","Department of Systems Science and Industrial Engineering, State University of New York at Binghamton, Binghamton, United States; Department of Systems Science and Industrial Engineering, State University of New York at Binghamton, Binghamton, United States; Department of Systems Science and Industrial Engineering, State University of New York at Binghamton, Binghamton, United States; Department of Systems Science and Industrial Engineering, State University of New York at Binghamton, Binghamton, United States; NA","2018 IEEE International Conference on Healthcare Informatics (ICHI)","26 Jul 2018","2018","","","199","207","This paper presents an adversarial learning-based approach to synthesize medical images for medical image tissue recognition. The performance of medical image recognition models highly depends on the representativeness and sufficiency of training samples. The high expense of collecting large amounts of practical medical images leads to a demand of synthesizing image samples. In this research, generative adversarial networks (GANs), which consist of a generative network and a discriminative network, are applied to develop a medical image synthesis model. Specifically, deep convolutional GANs (DCGANs), Wasserstein GANs (WGANs), and boundary equilibrium GANs (BEGANs) are implemented and compared to synthesize medical images in this research. Convolutional neural networks (CNNs) are applied in the GAN models, which can capture feature representations that describe a high level of image semantic information. Then synthetic images are generated by employing the generative network mapping from random noise. The effectiveness of the generative network is validated by a discriminative network, which is trained to detect the synthetic images from real images. Through a minimax two-player game, the generative and discriminative networks can train each other. The generated synthetic images are used to train a CNN classification model for tissue recognition. Through the experiments with the synthetic images, the tissue recognition accuracy achieves 98.83%, which reveals the effectiveness and applicability of synthesizing medical images through the GAN models.","2575-2634","978-1-5386-5377-7","10.1109/ICHI.2018.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419363","generative adversarial networks;image synthesis;Wasserstein distance;tissue recognition","Gallium nitride;Training;Image recognition;Medical diagnostic imaging;Solid modeling;Generators","edge detection;feature extraction;image classification;image denoising;image representation;learning (artificial intelligence);medical image processing;neural nets","generative adversarial networks;adversarial learning-based approach;medical image tissue recognition;medical image recognition models;medical image synthesis model;deep convolutional GANs;boundary equilibrium GANs;convolutional neural networks;GAN models;image semantic information;synthetic images;generative network mapping;generative networks;discriminative networks;tissue recognition accuracy","","18","1","20","IEEE","26 Jul 2018","","","IEEE","IEEE Conferences"
"A Reconfigurable Accelerator for Generative Adversarial Network Training Based on FPGA","T. Yin; W. Mao; J. Lu; Z. Wang","School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China","2021 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)","23 Aug 2021","2021","","","144","149","In recent years, generative adversarial networks (GANs) have been widely applied in various tasks, demonstrating outstanding performance, such as image generation, style transfer, and video generation. However, due to their high computation complexity and large amount of intermediate data to be stored, the on-device learning that trains GANs on embedded platforms remains a very challenging problem. In this work, we propose an FPGA-based reconfigurable accelerator for efficient GAN training. Firstly, the cascaded fast FIR algorithm (CFFA) is opti-mized towards GAN training, and a fast convolution processing element (FCPE) based on the optimized algorithm is introduced to support various computation patterns during GAN training. Secondly, a well optimized architecture on the basis of FCPEs is presented, which is flexible to support forward, backward, and weight gradient phases of GAN training. Finally, training of a prevailing network (DCGAN) is implemented on Xilinx VCU108 platform with our methods. Experimental results show that our design achieves 315.18 GOPS and 83.87 GOPS/W in terms of throughput and energy efficiency, respectively. Our accelerator achieves 4.0× improvement over the state-of-the-art design in energy efficiency.","2159-3477","978-1-6654-3946-6","10.1109/ISVLSI51109.2021.00036","National Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516779","Generative adversarial networks;hardware accelerator;training accelerator;reconfigurable design;FPGA","Training;Convolution;Image synthesis;Computer architecture;Very large scale integration;Generative adversarial networks;Throughput","convolution;field programmable gate arrays;FIR filters","reconfigurable accelerator;generative adversarial network training;generative adversarial networks;GANs;image generation;video generation;high computation complexity;efficient GAN training;cascaded fast FIR algorithm;fast convolution processing element;prevailing network","","1","","22","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Semantic Image Synthesis from Inaccurate and Coarse Masks","K. Katsumata; H. Nakayama","Graduate School of Information Science and Technology, The University of Tokyo; Graduate School of Information Science and Technology, The University of Tokyo","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","2285","2289","Semantic image synthesis is an image-to-image translation problem where the goal is to learn mapping from semantic segmentation masks to corresponding photorealistic images. However, conventional semantic image synthesis methods require numerous pairs of correct semantic masks and real images, and collecting these pairs is not always possible. To address this issue, we propose a smoothing method, which we call local label smoothing (LLS), that incorporates label smoothing per small patch of an input mask to learn mapping from masks to images even when semantic masks are inaccurate. Furthermore, we also propose an extended method for coarse masks. We demonstrate the advantage of the proposed methods over existing methods to deal with noisy masks on several datasets.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414521","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414521","Semantic image synthesis;Generative adversarial networks;Deep learning;Computational imaging","Image segmentation;Smoothing methods;Image synthesis;Semantics;Imaging;Signal processing;Information filters","image segmentation;smoothing methods","coarse masks;image-to-image translation problem;semantic segmentation masks;photorealistic images;conventional semantic image synthesis methods;input mask;noisy masks;local label smoothing","","","","21","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Semantic Regularized Class-Conditional GANs for Semi-Supervised Fine-Grained Image Synthesis","T. Chen; S. Wu; X. Yang; Y. Xu; H. -S. Wong","School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Communication and Computer Network Laboratory of Guangdong, Guangdong, China; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong","IEEE Transactions on Multimedia","9 Jun 2022","2022","24","","2975","2985","Learning effective generative models for natural image synthesis is a promising way to reduce the dependence of deep models on massive training data. This work focuses on Fine-Grained Image Synthesis (FGIS) in the semi-supervised setting where a small number of training instances are labeled. Different from generic image synthesis tasks, the available fine-grained data may be inadequate, and the differences among the object categories are typically subtle. To address these issues, we propose a Semantic Regularized class-conditional Generative Adversarial Network, which is referred to as SReGAN. We incorporate an additional discriminator and classifier into the generator-discriminator minimax game. Competing with two discriminators enforces the generator to model both marginal and class-conditional data distributions, which alleviates the problem of limited training data and labels. However, the discriminators may overlook the class separability. To induce the generator to discover the distinctions between classes, we construct semantically congruent and incongruent pairs in the generation process, and further regularize the generator by encouraging high similarities of congruent pairs, while penalizing that of incongruent ones in the classifier’s feature space. We have conducted extensive experiments to verify the capability of SReGAN in generating high-fidelity images on a variety of FGIS benchmarks.","1941-0077","","10.1109/TMM.2021.3091859","National Natural Science Foundation of China(grant numbers:62072188,62072189); Research Grants Council of the Hong Kong Special Administration Region(grant numbers:CityU 11201220); Natural Science Foundation of Guangdong Province(grant numbers:2019A050510010,2020A1515010484); China Postdoctoral Science Foundation(grant numbers:2021M691682); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9466403","Semi-supervised learning;fine-grained image synthesis;generative adversarial networks;semantic regularization","Generators;Semantics;Image synthesis;Training;Task analysis;Generative adversarial networks;Data models","feature extraction;image classification;learning (artificial intelligence);minimax techniques;neural nets","FGIS benchmark;object categories;classifier feature space;generator-discriminator minimax game;discriminators;class-conditional data distributions;class separability;generation process;high-fidelity images;generative model learning;natural image synthesis;deep models;generic image synthesis tasks;semantic regularized class-conditional GAN;semisupervised fine-grained image synthesis;semantic regularized class-conditional generative adversarial network;SReGAN;marginal data distributions","","","","54","IEEE","28 Jun 2021","","","IEEE","IEEE Journals"
"Colonoscopic Image Synthesis For Polyp Detector Enhancement Via Gan And Adversarial Training","F. He; S. Chen; S. Li; L. Zhou; H. Zhang; H. Peng; X. Huang","Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University; Institute of Image Processing and Pattern Recognition, Shanghai Jiao Tong University; Digestive Endoscopy Center, Tongren Hospital, Shanghai Jiao Tong University, School of Medicine; Digestive Endoscopy Center, Tongren Hospital, Shanghai Jiao Tong University, School of Medicine; Digestive Endoscopy Center, Tongren Hospital, Shanghai Jiao Tong University, School of Medicine; Institute of Medical Robotics, Shanghai Jiao Tong University","2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)","25 May 2021","2021","","","1887","1891","Computer-aided polyp detection system powered by deep neural networks has achieved high performance but also suffers from data insufficiency. To address this problem, recent researches focus on synthesizing new colonoscopic images by Generative Adversarial Network (GAN). However, the synthesized images follow the same distribution as that of the training dataset, which limits the performance of the detectors re-trained on it. Recent studies show that adversarial examples can expand the data distribution and thus adversarial training can effectively improve the robustness of deep neural networks. Inspired by these two factors, this paper proposes a data augmentation framework to directly produce false negative colonoscopic images via GAN and the adversarial attack. The synthesized polyps are natural and experiments on three popular detectors show that compared with using GAN alone, producing false negative images by the adversarial attack can further improve the performance of the re-trained detectors.","1945-8452","978-1-6654-1246-9","10.1109/ISBI48211.2021.9434050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9434050","Image synthesis;Adversarial training;Colonoscopic images;Polyp detection","Training;Image synthesis;Neural networks;Deep architecture;Detectors;Generative adversarial networks;Robustness","biological organs;computerised tomography;learning (artificial intelligence);medical image processing;neural nets","colonoscopic image synthesis;polyp detector enhancement;gan;computer-aided polyp detection system;deep neural networks;data insufficiency;recent researches focus;Generative Adversarial Network;GAN;synthesized images;training dataset;detectors re-trained;adversarial examples;data distribution;adversarial training;data augmentation framework;false negative colonoscopic images;adversarial attack;synthesized polyps;popular detectors;false negative images;re-trained detectors","","4","","21","IEEE","25 May 2021","","","IEEE","IEEE Conferences"
"Skip Attention GAN for Remote Sensing Image Synthesis","K. Deng; K. Zhang; P. Yao; S. Cheng; P. He","Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences; Institute of Computing Technology, Chinese Academy of Sciences","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","2305","2309","High-quality remote sensing images are difficult to obtain due to limited conditions and high cost for data acquisition. With the development of machine vision and deep learning, some image generation methods (e.g., GANs) are introduced into this field, but it’s still hard to generate images with good texture details and structural dependencies. We establish Skip Attention Mechanism to deal with this problem, which learns dependencies between local points on low-resolution feature maps, and then upsample the attention map and combine it with high-resolution feature maps. With this method, long-range dependencies learned from low-resolution are used for generating remote sensing images with more structural details. We name this method as Skip Attention GAN, which is the first method applying cross-scale attention mechanism for unsupervised remote sensing image generation. Experiments show that our method outperforms previous methods under several metrics. Visual and ablation results of attention layers show that Skip Attention has learned long-distance structural dependencies between similar targets.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414701","Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414701","Attention Mechanism;Remote Sensing Image Synthesis;Generative Adversarial Network","Measurement;Deep learning;Visualization;Image synthesis;Machine vision;Generative adversarial networks;Sensors","computer vision;data acquisition;edge detection;geophysical image processing;image classification;image resolution;image texture;learning (artificial intelligence);remote sensing;robot vision","high-quality remote sensing images;data acquisition;machine vision;deep learning;image generation methods;GANs;good texture details;Skip Attention Mechanism;low-resolution feature maps;attention map;high-resolution feature maps;long-range dependencies;Skip Attention GAN;cross-scale attention mechanism;unsupervised remote sensing image generation;attention layers;long-distance structural dependencies;remote sensing image synthesis","","1","","24","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"Text-To-Image Synthesis Using Modified GANs","L. S. Hanne; R. Kundana; R. Thirukkumaran; Y. V. Parvatikar; K. Madhura","Department of CSE., New Horizon College of Engineering, Bengaluru, India; Department of CSE., New Horizon College of Engineering, Bengaluru, India; Department of CSE., New Horizon College of Engineering, Bengaluru, India; Department of CSE., New Horizon College of Engineering, Bengaluru, India; Department of CSE., New Horizon College of Engineering, Bengaluru, India","2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)","15 Apr 2022","2022","","","1","7","Synthesis of high-resolution images for the input textual descriptions is a prominent field of research that needs to be looked into, in this world of computer vision and animations. The task of image generation has its own challenges such as, working on the main object and background elements simultaneously, considering the ambiguity of natural language and reducing the noise that may be collected along with the input. It is also equally important to determine the importance of words given in the input textual description and ranking them according to their degree of relevance to desired image. Generative Adversarial Networks (GANs) are one of the newest developments for synthesis of images from text. In our literature review article, we have provided a consolidated idea of GAN s and how their variants are helpful in producing high resolution images.","","978-1-6654-9529-5","10.1109/ACCAI53970.2022.9752641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9752641","Text-to Image synthesis;high resolution images;Generative Adversarial Networks (GANs);natural language processing;Generator;Discriminator;Semantic correlation","Measurement;Image resolution;Correlation;Image synthesis;Semantics;Natural languages;Generative adversarial networks","computer vision;image resolution;natural language processing;neural nets;text analysis","background elements;natural language;input textual description;generative adversarial networks;high resolution images;text-to-image synthesis;modified GAN;high-resolution images;computer vision;animations;image generation;main object","","","","18","IEEE","15 Apr 2022","","","IEEE","IEEE Conferences"
"Generation of images based on emotions evoked by music using Generative Adversarial Networks","G. Katayose; I. Shimizu","Graduate School of Engineering, Tokyo University of Agriculture and Technology; Graduate School of Engineering, Tokyo University of Agriculture and Technology","2022 IEEE 11th Global Conference on Consumer Electronics (GCCE)","18 Jan 2023","2022","","","907","909","In recent years, video production is very popular among non-professional people. One of the key technologies supporting video production is image generation. There are many methods for generating images which are strongly associated by given textual descriptions or audio data. On the other hand, there has been very few researches on image generation from music. In this paper, we propose a method for image generation based on emotions evoked by music using Generative Adversarial Networks(GAN). We employ MidiBERT for feature extraction from music to estimate the emotions evoked by music and StackGAN for image generation based on the extracted features. By training an image generation network on our dataset, we show that it is possible to generate similar images for similar musical pieces.","2378-8143","978-1-6654-9232-4","10.1109/GCCE56475.2022.10014145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10014145","Image Generation;Music;Generative Adversarial Network","Training;Image synthesis;Music;Production;Feature extraction;Generative adversarial networks;Consumer electronics","data visualisation;feature extraction;music","image generation network;music;video production","","","","8","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"Trilateral GAN with Channel Attention Residual for Semantic Image Synthesis","A. Ke; G. Liu; J. Chen; X. Wu","Hubei University of Technology, Wuhan, China; Hubei University of Technology, Wuhan, China; Hubei University of Technology, Wuhan, China; Hubei University of Technology, Wuhan, China","2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI)","28 Feb 2022","2021","","","1123","1129","In this paper, a novel method for synthesizing photo-realistic images from semantic label maps using GANs is proposed, which is an ideal and challenging task in computer vision and image synthesis. Due to the sparsity of the information contained in semantic label maps, it is difficult for some existing methods to achieve satisfactory synthesis effect. This paper proposes a trilateral generative adversarial network to support multi-directional transmission between images of different resolutions, called TrilateralGAN. Compared with the traditional single-directional transmission, the design of our TrilateralGAN network can better retain the information in the original image to avoid loss of details. In addition, we further propose a new channel attention residual as the main part of the TrilateralGAN network. This part can enhance the retained information to varying degrees, which can make the image synthesized by TrilateralGAN have clearer edges and richer details. The experimental results on Cityscapes and ADE20K datasets demonstrate the advantage of TrilateralGAN over the state-of-the-art approaches, regarding both visual quality and the representative evaluating criteria.","","978-1-6654-0692-5","10.1109/CISAI54367.2021.00223","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9718914","image synthesis;Generative Adversarial Networks;semantic label;channel attention residual","Visualization;Information science;Computer vision;Image resolution;Image synthesis;Image edge detection;Semantics","computer vision;edge detection;image texture;realistic images","trilateral GAN;channel attention residual;semantic image synthesis;photo-realistic images;semantic label maps;GANs;ideal task;computer vision;satisfactory synthesis effect;trilateral generative adversarial network;multidirectional transmission;called TrilateralGAN;single-directional transmission;TrilateralGAN network;retained information","","","","22","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Mask-based Style-Controlled Image Synthesis Using a Mask Style Encoder","J. Cho; W. Shimoda; K. Yanai","Department of Informatics, The University of Electro-Communications, Tokyo, Japan; Department of Informatics, The University of Electro-Communications, Tokyo, Japan; Department of Informatics, The University of Electro-Communications, Tokyo, Japan","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","5176","5183","In recent years, the advances in Generative Adversarial Networks (GANs) have shown impressive results for image generation and translation tasks. In particular, the image-to-image translation is a method of learning mapping from a source domain to a target domain and synthesizing an image. Image-to-image translation can be applied to a variety of tasks, making it possible to quickly and easily synthesize realistic images from semantic segmentation masks. However, in the existing image-to-image translation method, there is a limitation on controlling the style of the translated image, and it is not easy to synthesize an image by controlling the style of each mask element in detail. Therefore, we propose an image synthesis method that controls the style of each element by improving the existing image-to-image translation method. In the proposed method, we implement a mask style encoder that extracts style features for each mask element. The extracted style features are concatenated to the semantic mask in the normalization layer, and used the style-controlled image synthesis of each mask element. In the experiments, we performed style-controlled images synthesis using the datasets consisting of semantic segmentation masks and real images. The results show that the proposed method has excellent performance for style-controlled images synthesis for each element.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412647","","Image segmentation;Image synthesis;Semantics;Feature extraction;Generative adversarial networks;Pattern recognition;Task analysis","feature extraction;image segmentation;language translation;learning (artificial intelligence);realistic images;speech synthesis","semantic segmentation masks;style-controlled images synthesis;mask-based style-controlled image synthesis;mask style;image generation;translation tasks;synthesize realistic images;existing image-to-image translation method;translated image;mask element;image synthesis method;extracts style features;extracted style features","","","","43","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"BORDE: Boundary and Sub-Region Denormalization for Semantic Brain Image Synthesis","I. N. Chaparro-Cruz; J. A. Montoya-Zegarra","Department of Computer Science, Universidad Católica San Pablo, Arequipa, Perú; Institute for Biomedical Engineering, ETH Zurich, Zurich, Switzerland","2021 34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)","20 Dec 2021","2021","","","81","88","Medical images are often expensive to acquire and offer limited use due to legal issues besides the lack of consistency and availability of image annotations. Thus, the use of medical datasets can be restrictive for training deep learning models. The generation of synthetic images along with their corresponding annotations can therefore aid to solve this issue. In this paper, we propose a novel Generative Adversarial Network (GAN) generator for multimodal semantic image synthesis of brain images based on a novel denormalization block named BOundary and sub-Region DEnormalization (BORDE). The new architecture consists of a decoder generator that allows: (i) an effectively sequential propagation of a-priori semantic information through the generator, (ii) noise injection at different scales to avoid mode-collapse, and (iii) the generation of rich and diverse multimodal synthetic samples along with their contours. Our model generates very realistic and plausible synthetic images that when combined with real data helps to improve the accuracy in brain segmentation tasks. Quantitative and qualitative results on challenging multimodal brain imaging datasets (BraTS 2020 [1] and ISLES 2018 [2]) demonstrate the advantages of our model over existing image-agnostic state-of-the-art techniques, improving segmentation and semantic image synthesis tasks. This allows us to prove the need for more domain-specific techniques in GANs models.","2377-5416","978-1-6654-2354-0","10.1109/SIBGRAPI54419.2021.00020","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643093","Brain Imaging;generative adversarial networks;normalizationlayers;semantic image synthesis","Training;Neuroimaging;Image segmentation;Image synthesis;Semantics;Brain modeling;Generative adversarial networks","biomedical MRI;brain;image classification;image segmentation;learning (artificial intelligence);medical image processing","BORDE;sub-Region denormalization;semantic brain image synthesis;medical images;legal issues;image annotations;medical datasets;deep learning models;synthetic images;corresponding annotations;novel Generative Adversarial Network generator;multimodal semantic image synthesis;brain images;novel denormalization block;sub-Region DEnormalization;decoder generator;effectively sequential propagation;a-priori semantic information;rich synthetic samples;diverse multimodal synthetic samples;brain segmentation tasks;multimodal brain imaging datasets;image-agnostic state-of-the-art techniques;semantic image synthesis tasks;GANs models","","1","","33","IEEE","20 Dec 2021","","","IEEE","IEEE Conferences"
"Sharp-GAN: Sharpness Loss Regularized GAN for Histopathology Image Synthesis","S. Butte; H. Wang; M. Xian; A. Vakanski","Department of Computer Science, University of Idaho, Idaho, USA; Department of Computer Science, University of Idaho, Idaho, USA; Department of Computer Science, University of Idaho, Idaho, USA; Department of Computer Science, University of Idaho, Idaho, USA","2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)","26 Apr 2022","2022","","","1","5","Existing deep learning-based approaches for histopathology image analysis require large annotated training sets to achieve good performance; but annotating histopathology images is slow and resource-intensive. Conditional generative adversarial networks have been applied to generate synthetic histopathology images to alleviate this issue, but current approaches fail to generate clear contours for overlapped and touching nuclei. In this study, We propose a sharpness loss regularized generative adversarial network to synthesize realistic histopathology images. The proposed network uses normalized nucleus distance map rather than the binary mask to encode nuclei contour information. The proposed sharpness loss enhances the contrast of nuclei contour pixels. The proposed method is evaluated using four image quality metrics and segmentation results on two public datasets. Both quantitative and qualitative results demonstrate that the proposed approach can generate realistic histopathology images with clear nuclei contours.","1945-8452","978-1-6654-2923-8","10.1109/ISBI52829.2022.9761534","Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761534","Histopathology image synthesis;GAN;Nuclei segmentation","Training;Measurement;Image quality;Image segmentation;Histopathology;Image synthesis;Biological system modeling","cancer;image segmentation;learning (artificial intelligence);medical image processing","sharp-GAN;sharpness loss regularized GAN;histopathology image synthesis;deep learning-based approaches;histopathology image analysis;annotated training;conditional generative adversarial networks;synthetic histopathology images;clear contours;overlapped nuclei;touching nuclei;generative adversarial network;realistic histopathology images;nuclei contour information;nuclei contour pixels;image quality metrics;clear nuclei contours","","","","20","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"CcGL-GAN: Criss-Cross Attention and Global-Local Discriminator Generative Adversarial Networks for text-to-image synthesis","X. Ye; L. Lu","South China University of Technology, School of Computer Science and Engineering, Guangzhou, China; South China University of Technology, School of Computer Science and Engineering, Guangzhou, China","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","8","Text-to-image synthesis aims to generate a visually realistic image according to a linguistic text description. Visual quality and semantic consistency are two key objectives. Although remarkable progress has been made in improving visual resolutions leveraging Generative Adversarial Networks (GANs), guaranteeing the semantic conformity remains challenging. In this paper, we address it by proposing a novel Criss-Cross Attention and Global-Local Discriminator Generative Adversarial Networks(CcGL-GAN). CcGL-GAN exploits a Criss-Cross Attention mechanism to capture the variation of contextual description, which enables back generators to generate images more efficiently. Moreover, it utilizes Global-Local discriminators to project low-resolution images onto global linguistic representations, and high-resolution images onto local linguistic representations, which ensures that our model narrows the gap between images and descriptions. Experiments conducted on two publicly available datasets, the CUB and Oxford-102, demonstrate the effectiveness of the proposed CcGL-GAN model.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533396","","Visualization;Image resolution;Semantics;Neural networks;Linguistics;Generative adversarial networks;Generators","computational linguistics;image representation;image resolution;natural language processing;neural nets;realistic images;text analysis","text-to-image synthesis;visually realistic image;linguistic text description;visual quality;visual resolutions;generators;low-resolution images;global linguistic representations;high-resolution images;local linguistic representations;CcGL-GAN model;criss-cross attention mechanism;global-local discriminator generative adversarial networks","","","","29","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Fire Image Generation Based on ACGAN","Y. Zhikai; B. Leping; W. Teng; Z. Tianrui; W. Fen","Naval University of Engineering, Wuhan; Naval University of Engineering, Wuhan; Naval University of Engineering, Wuhan; NA; , Beijing","2019 Chinese Control And Decision Conference (CCDC)","12 Sep 2019","2019","","","5743","5746","In order to solve the problem that it is difficult to obtain fire image data in CNN training, this paper discusses the method of generating fire image by means of generative adversarial networks. How to generate the desired fire image according to the known observation variables is discussed. According to the structure of InfoGAN and ACGAN, a GAN structure for generating fire image is proposed. Fire area is selected as a known observation variable to generate the corresponding fire image. Experiments show that the network structure can generate the required images according to the values of a observed variables. And the quality of the generated image is related to the distribution of observed variables in the data set.","1948-9447","978-1-7281-0106-4","10.1109/CCDC.2019.8832678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832678","Fire Alarm;Generative Adversarial Networks;Fire Identification;Fire Characteristics","Generative adversarial networks;Training;Generators;Image synthesis;Linear programming;Mutual information;Gallium nitride","image processing;neural nets","fire image generation;ACGAN;fire image data;generative adversarial networks;GAN structure;fire area;network structure","","3","","11","IEEE","12 Sep 2019","","","IEEE","IEEE Conferences"
"Improved Generative Convolution Method for Image Generation","S. Park; M. -U. Yang; G. -H. Kim; J. -E. Im; K. -H. Kim; Y. -G. Shin","Biomedical Engineering, Chungbuk National University Hospital, Chungcheongbuk-do, Rep. of Korea; Medical AI Research Team, Chungbuk National University Hospital, Chungcheongbuk-do, Rep. of Korea; Medical AI Research Team, Chungbuk National University Hospital, Chungcheongbuk-do, Rep. of Korea; Medical AI Research Team, Chungbuk National University Hospital, Chungcheongbuk-do, Rep. of Korea; Medical AI Research Team, Chungbuk National University Hospital, Chungcheongbuk-do, Rep. of Korea; Department of Artificial Intelligence, Hannam University, Daejeon, Rep. of Korea","2022 13th International Conference on Information and Communication Technology Convergence (ICTC)","25 Nov 2022","2022","","","546","551","In our recent study, we introduced a novel convolution method, called GConv, which improves the performance of generative adversarial networks (GAN) by modulating the convolution kernels following the given latent vector. In this paper, we analyze the limitations of GConv, and propose an improved GConv to address those problems. While GConv modulates the convolution kernel equally at all pixels, the proposed method produces pixel-wise different kernels following not only the given latent vector but also the feature in each pixel. Even though the proposed method is a simple modification of GConv, it shows better performance compared to the standard convolution as well as GConv. To show the superiority of the proposed method, this paper provides experimental results on the CIFAR-10 and CIFAR-100 datasets. Quantitative evaluations reveal that the proposed method improves both GAN and conditional GAN (cGAN) performance in terms of Frechet inception distance (FID).","2162-1241","978-1-6654-9939-2","10.1109/ICTC55196.2022.9952596","Korea Health Industry Development Institute (KHIDI); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952596","Generative adversarial networks;Image generation;GConv;Convolution operation","Image resolution;Convolution;Image synthesis;Generative adversarial networks;Information and communication technology;Kernel;Standards","convolutional neural nets;image processing;learning (artificial intelligence)","CIFAR-10 datasets;CIFAR-100 datasets;conditional GAN performance;convolution kernel;Frechet inception distance;generative adversarial networks;generative convolution method;image generation;improved GConv;latent vector;pixel-wise different kernels;standard convolution","","1","","41","IEEE","25 Nov 2022","","","IEEE","IEEE Conferences"
"Adversarial Synthesis based Data Augmentation for Speech Classification","P. Shastri; C. Patil; P. Wanere; S. Mahajan; A. Bhatt","Dept. of Electronics and telecom., College of Engineering, Pune, India; Dept. of Electronics and telecom., College of Engineering, Pune, India; Software Developer Bajaj Finance Ltd.; Dept. of Electronics and telecom., College of Engineering, Pune, India; Dept. of Electronics and telecom., College of Engineering, Pune, India","2022 International Conference on Signal and Information Processing (IConSIP)","12 Jan 2023","2022","","","1","6","Speech classification plays a vital role in modern audio processing, with the rise in technologies like home assistants and speech-based control devices. Deep learning-based algorithms have played a big role in developing such technologies. Deep learning algorithms are data-hungry and need large labelled datasets for classification. However, finding such labelled datasets is rare in the real world. The datasets found in the open can be highly skewed, making the classification task hard. This study addresses the problem of data augmentation for the task of speech classification. To create examples for underrepresented classes, this method suggests conditioned data augmentation using generative adversarial networks (GAN). This work adapts and improves conditional GAN architecture to generate synthetic Mel spectrograms for the minority classes. The proposed GAN based approach is evaluated on the Speech Commands “Zero” through “Nine” (Sc09) dataset. Results demonstrate approximately 1.7% relative performance improvement in the accuracy, against a Convolutional Recurrent Neural Network (CRNN) baseline adapted to classify the speech commands.","","978-1-7281-6885-2","10.1109/ICoNSIP49665.2022.10007491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007491","Mel spectrogram;Generative Adversarial Networks;Data augmentation;Classification;Convolutional Neural Networks","Training;Representation learning;Recurrent neural networks;Image synthesis;Process control;Generative adversarial networks;Classification algorithms","gallium compounds;learning (artificial intelligence);neural nets;pattern classification;recurrent neural nets","adversarial synthesis;classification task;data augmentation;data-hungry;deep learning algorithms;deep learning-based algorithms;GAN based approach;generative adversarial networks;labelled datasets;modern audio processing;speech classification;Speech Commands Zero through Nine dataset;speech-based control devices","","","","24","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"Example-Guided Identify Preserving Face Synthesis by Metric Learning","D. WEI; X. HU; K. CHEN; P. P. K. CHAN","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China","2019 International Conference on Wavelet Analysis and Pattern Recognition (ICWAPR)","2 Jan 2020","2019","","","1","6","Generative adversarial networks (GANs) are commonly applied to example-guided identify preserving face synthesis. A binary classifier is used as style consistency discriminator in GAN model in order to ensure the consistency of style. However, the over-fitting problem of a binary classifier downgrade its discrimination ability on style consistency. In this paper, we propose a style consistency discriminator based on metric learning, which performs better in keeping identity information and guaranteeing consistency in style between input examplar and result. Through separating the positive pairs form the negative, metric learning model can efficiently measure the similarity between the synthesis face and the genuine face. The experimental results indicate that the metric learning performs better than a binary classifier in terms of preserving style consistency.","2158-5709","978-1-7281-2996-9","10.1109/ICWAPR48189.2019.8946468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8946468","Generative adversarial networks;Identity preserving;Metric learning","Measurement;Training;Face recognition;Generative adversarial networks;Faces;Image synthesis;Semantics","face recognition;image classification;learning (artificial intelligence)","genuine face;face synthesis;generative adversarial networks;style consistency discriminator;GAN model;binary classifier;identity information;negative learning model;metric learning model;synthesis face","","","","32","IEEE","2 Jan 2020","","","IEEE","IEEE Conferences"
"Automatic Image Generation of Peking Opera Face using StyleGAN2","X. Xin; Y. Shen; R. Xiong; X. Lin; M. Yan; W. Jiang","School of Information and Communication Engineering, Communication University of China, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China; Center for Ethnic and Folk Literature and Art Development Ministry of Culture and Tourism of the P.R.C, Beijing, China; Key Laboratory of Acoustic Visual Technology and Intelligent Control System, Communication University of China, Beijing, China; Key Laboratory of Acoustic Visual Technology and Intelligent Control System, Communication University of China, Beijing, China","2022 International Conference on Culture-Oriented Science and Technology (CoST)","28 Sep 2022","2022","","","99","103","Image generation technology, which is often used in various applications of intelligent image generation, can learn the feature distribution of real images and sample from the distribution to obtain the generated images with high fidelity. This paper focuses on the feature extraction and intelligent generation techniques of Peking opera face with Chinese cultural characteristics. Based on the creation of a Peking opera face dataset, this paper compares the impact of different variants of a Style-based generator architecture for Generative Adversarial Networks (StyleGAN2) and different sizes of datasets on the quality of face generation. The experimental results verify that the synthetic images generated by StyleGAN2 with the addition of the Adaptive Discriminator Augmentation (ADA) module are visually better and have good local randomness when the dataset is small and unbalanced in distribution.","","978-1-6654-6248-8","10.1109/CoST57098.2022.00030","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9898526","Peking Opera face;Image generation;Generative Adversarial Networks;StyleGAN2","Costs;Image synthesis;Generative adversarial networks;Feature extraction;Generators;Cultural differences;Faces","face recognition;feature extraction;neural nets","automatic image generation;StyleGAN2;image generation technology;intelligent image generation;feature distribution;feature extraction;intelligent generation techniques;Chinese cultural characteristics;Peking opera face dataset;style-based generator architecture;generative adversarial networks;face generation;synthetic images;Adaptive Discriminator Augmentation module","","","","17","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Sketch-to-Color Image with GANs","W. Zhang","Arts, Sciences and Engineering University of Rochester, Rochester, United States","2020 2nd International Conference on Information Technology and Computer Application (ITCA)","7 May 2021","2020","","","322","325","Unsupervised Learning is a trending research field of artificial intelligence, which aims to interpret and understand the hidden structure of the data. However, the development of deep learning with Generative Adversarial Networks (GANs) creates more possibilities for unsupervised learning. GAN is a category of Neural Networks, which are mostly applied to generating images. In this paper, how GAN was implemented to help with sketch-to-color translation is illustrated. In order to achieve this goal, data-preprocessing is implemented first. Then, the model is trained for 65 epochs, and the performance of the model is improved by virtue of loss functions and optimizers. In the end, a proper User Interface (GUI) is designed to have a full application, and people could turn any sketch picture they want into a colored image.","","978-1-6654-0378-8","10.1109/ITCA52113.2020.00075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9422042","Colorization;Generative Adversarial Networks;Deep Learning","Deep learning;Image synthesis;Computational modeling;Neural networks;Manuals;Computer applications;Generative adversarial networks","data encapsulation;deep learning (artificial intelligence);graphical user interfaces;image colour analysis;unsupervised learning","sketch-to-color image;GAN;unsupervised learning;artificial intelligence;deep learning;generative adversarial networks;neural networks;image generation;sketch-to-color translation;data-preprocessing;data hidden structure;user interface","","","","10","IEEE","7 May 2021","","","IEEE","IEEE Conferences"
"Asymmetric Generative Adversarial Networks with a New Attention Mechanism","J. Chen; G. Liu; A. Ke","School of Computer Science, Hubei University of Technology, Wuham, China; School of Computer Science, Hubei University of Technology, Wuham, China; School of Computer Science, Hubei University of Technology, Wuham, China","2022 Asia Conference on Algorithms, Computing and Machine Learning (CACML)","19 Aug 2022","2022","","","186","192","In this paper, a new residual decoding network is proposed to solve the problem that semantic label maps are transformed into real images in image processing tasks, which is a very challenging and difficult task. Since the input semantic label map lacks rich detailed information, it will generate blurry, low-detailed, and color-distorted images during con-version. We propose a new residual decoding network to solve the above problems, calling AsymmetricGAN. Compared with the traditional upsampling network, our proposed new residual module with skip and attention connections can better preserve the information in the original image to avoid the loss of details. We also propose a new module with channel and spatial attention mechanism as the main component of the network which can better retain useful information and make the edges of the synthesized image clearer and more abundant in the details. The experimental results on Cityscapes and ADE20K datasets demonstrate the advantage of AsymmetricGAN over the state-of-the-art approaches, regarding both visual quality and the representative evaluating criteria.","","978-1-6654-8290-5","10.1109/CACML55074.2022.00038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852613","image synthesis;Generative Adversarial Net-works;semantic label;channel attention residual","Training;Visualization;Machine learning algorithms;Image synthesis;Image edge detection;Semantics;Machine learning","decoding;image coding;image colour analysis;image enhancement;image restoration;image sampling;neural nets","asymmetric generative adversarial networks;residual decoding network;semantic label maps;image processing tasks;semantic label map;color-distorted images;upsampling network;residual module;attention connections;spatial attention mechanism;AsymmetricGAN;Cityscapes;ADE20K datasets;synthesized image","","","","20","IEEE","19 Aug 2022","","","IEEE","IEEE Conferences"
"Automating Generative Adversarial Networks using Neural Architecture Search: A Review","V. V. V. Ganepola; T. Wirasingha","Department of Computing, Informatics Institute of Technology, Colombo, Sri Lanka; Department of Computing, Informatics Institute of Technology, Colombo, Sri Lanka","2021 International Conference on Emerging Smart Computing and Informatics (ESCI)","9 Apr 2021","2021","","","577","582","Generative Adversarial Networks (GANs) has emerged a great success in image processing and computer vision. Neural Architecture Search (NAS), a process of automating architectural engineering, was applied in GANs to improve backbone architectures. Currently, image generation and GAN model compression are the key tasks applied NAS in GANs. This study analysed the NAS literature on search spaces, search strategies, and performance estimation strategies applied in GANs. The results of the analysis reveal that cell-based and chain/entire structured search spaces have been used, whereas cell-based structure is the most used search space type in GANs. Reinforcement learning (RL), gradient-based, and evolutionary algorithms have been applied as search strategies to search for the optimal GAN architectures. Weight-sharing has been used as the performance estimation strategy in most of the NAS in GANs research. Furthermore, multi-objective architecture search for GANs approach, which is based on an evolutionary algorithm search strategy, was found to achieve the most outstanding results with NAS in GANs in both supervised and unsupervised image generation. Results were analysed using CIFAR-10 and STL-10 datasets in terms of Inception Score (IS) and Frechet Inception Distance (FID) score. This paper is a review study of NAS in GANs, and it outlines possible future works to stimulate other researchers to examine NAS in GANs.","","978-1-7281-8519-4","10.1109/ESCI50559.2021.9396991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9396991","Neural Architecture Search (NAS);Generative Adversarial Networks (GANs);GAN automation","Image synthesis;Estimation;Computer architecture;Evolutionary computation;Generative adversarial networks;Search problems;Task analysis","computer vision;evolutionary computation;gradient methods;learning (artificial intelligence);neural net architecture;search problems","gradient-based algorithm;reinforcement learning;unsupervised image generation;supervised image generation;evolutionary algorithm search strategy;multiobjective architecture search;optimal GAN architectures;search space type;performance estimation strategy;search strategies;GAN model compression;backbone architectures;automating architectural engineering;NAS;neural architecture search;computer vision;image processing;generative adversarial networks","","4","","36","IEEE","9 Apr 2021","","","IEEE","IEEE Conferences"
"Automatic Synthetic Document Image Generation using Generative Adversarial Networks: Application in Mobile-Captured Document Analysis","Q. A. Bui; D. Mollard; S. Tabbone","PurchEase company, Paris, France; PurchEase company, Paris, France; LORIA UMR 5073, Université de Lorraine, Nancy, France","2019 International Conference on Document Analysis and Recognition (ICDAR)","3 Feb 2020","2019","","","393","400","In this paper, we propose a method using Generative Adversarial Networks for automatically synthesizing document images that are similar to real printed documents captured by mobile phone's camera in unconstrained environment. We focus on the simulation of image defects for unconstrained mobile image acquisition procedure (non-uniform illumination, defocusing, optical and mechanical deformations, vibrations, noise in electronic components,...). Our approach is proven to be low-cost as it only requires a collection of real document images without any annotation. Experimental results show the effectiveness of our approach to improve OCR (Optical Character Recognition) recognition rate in a mobile-captured document images framework. Although in this paper, we focus on modern printed document images, our proposed approach could be extended to another type of documents, including historical one.","2379-2140","978-1-7281-3014-9","10.1109/ICDAR.2019.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978011","synthetic document image generation;Generative Adversarial Networks;mobile-captured documents","Generators;Neural networks;Generative adversarial networks;Text analysis;Image synthesis;Degradation;Data models","cameras;document image processing;image annotation;neural nets;optical character recognition","modern printed document images;automatic synthetic document image generation;generative adversarial networks;mobile-captured document analysis;automatically synthesizing document images;printed documents;mobile phone;unconstrained environment;image defects;unconstrained mobile image acquisition procedure;nonuniform illumination;OCR recognition rate;Optical Character Recognition;mobile-captured document images framework","","2","","28","IEEE","3 Feb 2020","","","IEEE","IEEE Conferences"
"A Mongolian Handwritten Word Images Generation Approach Based on Generative Adversarial Networks","S. Sun; H. Wei","National and Local Joint Engineering Research Center of Mongolian Information Processing Technology, Hohhot, China; National and Local Joint Engineering Research Center of Mongolian Information Processing Technology, Hohhot, China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","The particular word formation manner of the Mongolian language makes its vocabulary reach millions. It is tough to manually collect a Mongolian handwritten word images dataset covering all words. In order to realize the automatic generation of Mongolian handwritten word images, an approach based on generative adversarial networks has been proposed in this paper, which is called Mongolian Generator (MG). The proposed MG generates Mongolian handwritten word images with better writing details and more accurate text content. Also, it can generate handwritten word images based on a specific writing style or text content. Moreover, the perceptual adversarial loss has been integrated into the MG, which guarantees that the generated handwritten word images appear more realistic. The experimental results indicate that the performance of the proposed MG outperforms several baselines and state-of-the-art models.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892917","Generative Adversarial Networks;Perceptual Adversarial Loss;Mongolian Handwritten Words;Image Generation","Vocabulary;Handwriting recognition;Image synthesis;Data security;Neural networks;Writing;Generative adversarial networks","handwritten character recognition;linguistics;natural languages;text analysis;vocabulary","generated handwritten word images;Mongolian handwritten word images generation approach;generative adversarial networks;particular word formation manner;Mongolian language;Mongolian handwritten word images dataset;automatic generation;Mongolian Generator","","1","","27","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"A Survey of Generative Adversarial Networks","K. Zhu; X. Liu; H. Yang","Beijing Polytechnic, College of Telecom. Eng., Beijing, China; The State Key Laboratory of Management and Control for Complex Systems, Qingdao Academy of Intelligent Industries, Qingdao, Beijing, China; Beijing Polytechnic, College of Telecom. Eng., Beijing, China","2018 Chinese Automation Congress (CAC)","24 Jan 2019","2018","","","2768","2773","Generative adversarial networks(GANs) coming from the game theory allow machines to learn deep representations without extra training data. By training two adversarial networks, including a generator and a discriminator, GANs could get the distribution of the real samples. This capability makes it a prospect learning method in image synthesis, image recognition, image translation etc. In this paper, we survey the state of the art of GANs by categorizing the GANs into four classifications on the basis of GANs' functions and list two application domains: vision computing & natural language processing(NLP) regarding to GANs' applications.","","978-1-7281-1312-8","10.1109/CAC.2018.8623645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8623645","generative adversarial networks;vision computing;natural language processing(NLP)","Gallium nitride;Training;Face;Generators;Three-dimensional displays;Generative adversarial networks;Image generation","game theory;learning (artificial intelligence)","generative adversarial networks;GAN applications;GAN functions;image translation;image recognition;image synthesis;extra training data;deep representations;game theory","","5","","119","IEEE","24 Jan 2019","","","IEEE","IEEE Conferences"
"Controllable Image Synthesis With Attribute-Decomposed GAN","G. Pu; Y. Men; Y. Mao; Y. Jiang; W. -Y. Ma; Z. Lian","School of Software and Microelectronics, Peking University, Beijing, China; DAMO Academy, Alibaba Group, Beijing, China; AI-Lab, ByteDance Inc, Beijing, China; Ads Engineering Business Unit, Alibaba Group, Hangzhou, China; Institute for AI Industry Research, Tsinghua University, Beijing, China; Wangxuan Institute of Computer Technology, Peking University, Beijing, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","6 Jan 2023","2023","45","2","1514","1532","This paper proposes Attribute-Decomposed GAN (ADGAN) and its enhanced version (ADGAN++) for controllable image synthesis, which can produce realistic images with desired attributes provided in various source inputs. The core ideas of the proposed ADGAN and ADGAN++ are both to embed component attributes into the latent space as independent codes and thus achieve flexible and continuous control of attributes via mixing and interpolation operations in explicit style representations. The major difference between them is that ADGAN processes all component attributes simultaneously while ADGAN++ utilizes a serial encoding strategy. More specifically, ADGAN consists of two encoding pathways with style block connections and is capable of decomposing the original hard mapping into multiple more accessible subtasks. In the source pathway, component layouts are extracted via a semantic parser and the segmented components are fed into a shared global texture encoder to obtain decomposed latent codes. This strategy allows for the synthesis of more realistic output images and the automatic separation of un-annotated component attributes. Although the original ADGAN works in a delicate and efficient manner, intrinsically it fails to handle the semantic image synthesizing task when the number of attribute categories is huge. To address this problem, ADGAN++ employs the serial encoding of different component attributes to synthesize each part of the target real-world image, and adopts several residual blocks with segmentation guided instance normalization to assemble the synthesized component images and refine the original synthesis result. The two-stage ADGAN++ is designed to alleviate the massive computational costs required when synthesizing real-world images with numerous attributes while maintaining the disentanglement of different attributes to enable flexible control of arbitrary component attributes of the synthesized images. Experimental results demonstrate the proposed methods’ superiority over the state of the art in pose transfer, face style transfer, and semantic image synthesis, as well as their effectiveness in the task of component attribute transfer. Our code and data are publicly available at https://github.com/menyifang/ADGAN.","1939-3539","","10.1109/TPAMI.2022.3161985","Beijing Nova Program of Science and Technology(grant numbers:Z191100001119077); PKU-Baidu Fund(grant numbers:2020BD020); National Language Committee of China(grant numbers:ZDI135-130); Center For Chinese Font Design and Research; Key Laboratory of Science, Technology and Standard in Press Industry; State Key Laboratory of Media Convergence Production Technology and Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9741362","Image Synthesis;generative adversarial networks (GAN);attribute control;style transfer","Semantics;Task analysis;Image synthesis;Codes;Generative adversarial networks;Encoding;Skeleton","computer graphics;data mining;grammars;graph theory;image colour analysis;image segmentation;image texture;interpolation;learning (artificial intelligence);pattern clustering;realistic images","ADGAN++;arbitrary component attributes;attribute categories;Attribute-Decomposed GAN;component attribute transfer;controllable image synthesis;decomposed latent codes;desired attributes;numerous attributes;original ADGAN;realistic images;realistic output images;semantic image synthesis;semantic image synthesizing task;synthesized component images;synthesizing real-world images;target real-world image;un-annotated component attributes","","1","","80","IEEE","24 Mar 2022","","","IEEE","IEEE Journals"
"Abdomen MRI Synthesis Based on Conditional GAN","H. Yang; K. Xia; B. Anqi; P. Qian; M. R. Khosravi","School of Digital Media, Jiangnan University, Wuxi, P.R. China; The affiliated changshu hospital of soochow university, Changshu, China; School of Computer Science and Engineering, Changshu Institute of Technology, Changshu, P.R. China; School of Digital Media, Jiangnan University, Wuxi, P.R. China; Department of Electrical and Electronic Engineering, Telecommunications Group, Shiraz University of Technology, Shiraz, Iran","2019 International Conference on Computational Science and Computational Intelligence (CSCI)","20 Apr 2020","2019","","","1021","1025","Obtaining different contrast images of the same anatomical location of the same subject can increase the diversity of diagnostic information. However, due to the cost constraints, it is often not possible to obtain sufficient different contrast images, and images in some contrasts may be corrupted by noise and artifacts. In this case, if these images with different contrasts can be generated from existing images, the diagnostic accuracy will be greatly improved. Here, we explore an end-to-end deep convolutional neural networks for multi-contrast MRI synthesis based on conditional generative adversarial networks (cGAN). The proposed approach preserves high-frequency details via an adversarial loss and a pixel-wise loss, to transform abdomen CT into abdomen MR. Paired MR-CT volumes of 10 patients are analyzed. A quantitative evaluation showed that the approach is capable to synthesize MR images that closely similar to reference MR images.","","978-1-7281-5584-5","10.1109/CSCI49370.2019.00195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071043","multi-contrast;image synthesis;conditional generative adversarial networks","Computed tomography;Magnetic resonance imaging;Training;Generative adversarial networks;Fats;Abdomen;Gallium nitride","biomedical MRI;computerised tomography;convolutional neural nets;medical image processing","abdomen MRI synthesis;conditional GAN;end-to-end deep convolutional neural networks;multicontrast MRI synthesis;generative adversarial networks;adversarial loss;paired MR-CT volumes;abdomen MR images;transform abdomen CT","","1","","10","IEEE","20 Apr 2020","","","IEEE","IEEE Conferences"
"Learning Rotation Invariant Features For Cryogenic Electron Microscopy Image Reconstruction","K. Bibas; G. Weiss-Dicker; D. Cohen; N. Cahan; H. Greenspan","Faculty of Engineering, Tel Aviv University, Israel; Faculty of Engineering, Tel Aviv University, Israel; Faculty of Engineering, Tel Aviv University, Israel; Faculty of Engineering, Tel Aviv University, Israel; Faculty of Engineering, Tel Aviv University, Israel","2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)","25 May 2021","2021","","","563","566","Cryo-Electron Microscopy (Cryo-EM) is a Nobel prizewinning technology for determining the 3D structure of particles at near-atomic resolution. A fundamental step in the recovering of the 3D single-particle structure is to align its 2D projections; thus, the construction of a canonical representation with a fixed rotation angle is required. Most approaches use discrete clustering which fails to capture the continuous nature of image rotation, others suffer from low-quality image reconstruction. We propose a novel method that leverages the recent development in the generative adversarial networks. We introduce an encoder-decoder with a rotation angle classifier. In addition, we utilize a discriminator on the decoder output to minimize the reconstruction error. We demonstrate our approach with the Cryo-EM 5HDB and the rotated MNIST datasets showing substantial improvement over recent methods.","1945-8452","978-1-6654-1246-9","10.1109/ISBI48211.2021.9433789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433789","Cryo-EM;5HDB;Rotated MNIST;Deep learning;Image synthesis;Generative adversarial networks","Three-dimensional displays;Image resolution;Magnetic resonance imaging;Computed tomography;Cryogenics;Generative adversarial networks;Decoding","biological techniques;biology computing;convolutional neural nets;electron microscopy;feature extraction;image classification;image reconstruction;image representation;learning (artificial intelligence);molecular biophysics","rotation angle classifier;reconstruction error;MNIST datasets;rotation invariant features;near-atomic resolution;3D single-particle structure;canonical representation;fixed rotation angle;discrete clustering;image rotation;low-quality image reconstruction;generative adversarial networks;cryogenic electron microscopy image reconstruction;Cryo-EM 5HDB datasets","","","","9","IEEE","25 May 2021","","","IEEE","IEEE Conferences"
"Artificial Computed Tomography Images with Progressively Growing Generative Adversarial Network","F. Asadi; J. A. O’Reilly","College of Biomedical Engineering Rangsit University, Pathumthani, Thailand; College of Biomedical Engineering Rangsit University, Pathumthani, Thailand","2021 13th Biomedical Engineering International Conference (BMEiCON)","1 Apr 2022","2021","","","1","5","Applications of artificial intelligence in medical imaging include classification, segmentation, and treatment planning. Using current deep-learning techniques, developing these systems requires large amounts of labelled training data. Obtaining this data is challenging due to costs, required expertise, inconsistency of imaging procedures and formatting, and patient privacy concerns. Generative adversarial networks (GANs) may alleviate some of these issues by supplying realistic artificial medical images. In this study, we trained progressively growing (PG)GAN to synthesize full-sized computed tomography (CT) images and succeeded. Performance of the PGGAN was evaluated using Fréchet Inception Distance (FID), Inception Score (IS), and Precision (P) and Recall (R). These metrics were calculated for generated, training, and validation images. The influence of dataset size was explored by varying the number of samples used to calculate each metric; this affected FID, P, and R, but not IS, which has obvious implications for comparing studies. The FID between artificial CT images from PGGAN and real validation images was 42; interestingly, FID between real training and validation images was 24. This suggests that a further reduction of 18 could be achieved by improving the generative model. Overall, artificial CT images generated by PGGAN were almost indistinguishable from real images to the human eye, although computational metrics could identify differences between them. In future work, GANs may be deployed to augment data for training medical AI systems.","2334-3052","978-1-6654-2627-5","10.1109/BMEiCON53485.2021.9745251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745251","generative adversarial networks;medical image synthesis;lung computed tomography;GAN evaluation metrics","Measurement;Training;Image segmentation;Image synthesis;Computed tomography;Training data;Generative adversarial networks","computerised tomography;deep learning (artificial intelligence);image classification;medical image processing;neural nets","artificial computed tomography images;generative adversarial network;artificial intelligence;medical imaging;labelled training data;patient privacy concerns;GANs;realistic artificial medical images;Frechet inception distance;FID;artificial CT images;generative model;computational metrics;medical AI systems","","1","","21","IEEE","1 Apr 2022","","","IEEE","IEEE Conferences"
"Review on Image Synthesis Techniques","M. F. Shirin; M. S. Meharban","Department of Computer Science and Engineering, Rajagiri School of Engineering and Technology, Ernakulam, India; Department of Computer Science and Engineering, Rajagiri School of Engineering and Technology, Ernakulam, India","2019 5th International Conference on Advanced Computing & Communication Systems (ICACCS)","6 Jun 2019","2019","","","13","17","Realistic Image Synthesis is the process of synthesizing images that constitutes the information of a real scene. Although they have strong relevance in multiple fields, realistic image synthesis from a given input is still a challenging problem. By making use of deep learning, more accurate methods were introduced that synthesize images in a more realistic manner. In this paper, a survey on image synthesis based on deep learning is carried out.","2575-7288","978-1-5386-9533-3","10.1109/ICACCS.2019.8728545","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8728545","Deep Learning;Recurrent Neural Network;Attention Mechanism;VAE;GAN","Image generation;Gallium nitride;Decoding;Training;Generative adversarial networks;Generators;Deep learning","learning (artificial intelligence);natural scenes;realistic images;recurrent neural nets;text analysis","realistic image synthesis;deep learning;real scene;recurrent neural network;attention mechanism;VAE;variational auto encoders;GAN;generative adversarial networks;text to image synthesis","","1","","8","IEEE","6 Jun 2019","","","IEEE","IEEE Conferences"
"Sign Language Video Synthesis using Skeleton Sequence","S. Gencoglu; H. Y. Keles","Bilgisayar Mühendisliği Bölümü, Ankara Üniversitesi, Ankara, Türkiye; Bilgisayar Mühendisliği Bölümü, Ankara Üniversitesi, Ankara, Türkiye","2020 28th Signal Processing and Communications Applications Conference (SIU)","7 Jan 2021","2020","","","1","4","Generative Adversarial Networks (GANs) enable generating realistic synthetic images. However, majority of the research in this domain focus on image-to-image synthesis problem. The aim of this study is to develop a model that encodes high quality video frames, with true motion dynamics, using only a reference image frame and a skeleton sequence. In this context, Ankara University Turkish Sign Language dataset is used to synthesize new sign videos using a given signer frame as a reference and a skeleton stream. To solve this challenging problem, a conditional generative adversarial network (GAN) is designed, where skeletal data is used as a condition. Using the trained model, we are able to generate sign video streams with the given signer, where the motion dynamics are successfully and fluently encoded in the video. Moreover, we evaluated the quality of the generated images using Fréchet Inception Distance (FID) metric; the FID score is 26.","2165-0608","978-1-7281-7206-4","10.1109/SIU49456.2020.9302436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302436","Generative adversarial networks;conditional generative adversarial networks;convolutional neural networks;video to video synthesis","Generative adversarial networks;Streaming media;Skeleton;Computational modeling;Assistive technology;Kernel;Information processing","image motion analysis;image sequences;image thinning;neural nets;sign language recognition;video coding;video streaming","skeleton stream;GAN;sign video streams;motion dynamics;skeleton sequence;image-to-image synthesis;reference image frame;sign videos;signer frame;Ankara University Turkish sign language dataset;sign language video synthesis;realistic synthetic images;generative adversarial network;skeletal data;Fréchet inception distance metric;FID score;high quality video frame encoding","","2","","","IEEE","7 Jan 2021","","","IEEE","IEEE Conferences"
"K-Means Clustering Guided Generative Adversarial Networks for SAR-Optical Image Matching","W. -L. Du; Y. Zhou; J. Zhao; X. Tian","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the People’s Republic of China, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau","IEEE Access","11 Dec 2020","2020","8","","217554","217572","Synthetic Aperture Radar and optical (SAR-optical) image matching is a technique of finding correspondences between SAR and optical images. SAR-optical image matching can be simplified to single-mode image matching through image synthesis. However, the existing SAR-optical image synthesis methods are unable to provide qualified images for SAR-optical image matching. In this work, we present a K-means Clustering Guide Generative Adversarial Networks (KCG-GAN) to improve the image quality of synthesizing by constraining spatial information synthesis. KCG-GAN uses k-means segmentations as one of the image generator's inputs and introduces feature matching loss, segmentation loss, and L1 loss to the objective function. Meanwhile, to provide repeatable k-means segmentations, we develop a straightforward 1D k-means algorithm. We compare KCG-GAN with a leading image synthesis method-pix2pixHD. Qualitative results illustrate that KCG-GAN preserves more spatial structures than pix2pixHD. Quantitative results show that, compared with pix2pixHD, images synthesized by KCG-GAN are more similar to original optical images, and SAR-optical image matching based on KCG-GAN obtains at most 3.15 times more qualified matchings. Robustness tests demonstrate that SAR-optical image matching based on KCG-GAN is robust to rotation and scale changing. We also test three SIFT-like algorithms on matching original SAR-optical image pairs and matching KCG-GAN synthesized optical-optical image pairs. Experimental results show that our KCG-GAN significantly improves the performances of the three algorithms on SAR-optical image matching.","2169-3536","","10.1109/ACCESS.2020.3042213","National Natural Science Foundation of China(grant numbers:61572505,62002360,61806206,61772530,U1610124); Six Talent Peaks Project in Jiangsu Province(grant numbers:2015-DZXX-010); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180639,BK20171192); China Postdoctoral Science Foundation(grant numbers:2018M642359); Science and Technology Development Fund of Macau(grant numbers:0038/2020/A1); Science and Technology Development Fund, Macu SAR under the open project of State Key Laboratory of Lunar and Planetary Science; Fundamental Research Funds for the Central Universities(grant numbers:2020ZDPY0305); Xuzhou Science and Technology Program(grant numbers:KC18061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9279214","Image matching;image synthesis;synthetic aperture radar (SAR);generative adversarial networks (GANs)","Optical imaging;Optical sensors;Nonlinear optics;Image segmentation;Image matching;Optical distortion;Adaptive optics","image matching;image segmentation;neural nets;optical information processing;pattern clustering;radar computing;radar imaging;synthetic aperture radar;transforms","KCG-GAN;image synthesis method;SAR-optical image matching;optical-optical image pairs;SAR-optical image synthesis methods;k-means clustering guide generative adversarial networks;synthetic aperture radar;constraining spatial information synthesis;k-means segmentations;1D k-means algorithm;pix2pixHD;SIFT-like algorithms","","7","","69","CCBY","3 Dec 2020","","","IEEE","IEEE Journals"
"Cartoon Face to Human Face Translation using Contour Loss based CycleGAN","M. Singhal; R. Agarwal","Department of Information Technology, Delhi Technological University, Delhi, India; Department of Information Technology, Delhi Technological University, Delhi, India","2021 2nd Global Conference for Advancement in Technology (GCAT)","9 Nov 2021","2021","","","1","6","Cartoon to Human Translation transforms a 2D vector cartoon face to a Real Human Face. The mapping is based on semantic similarity of both the input domains. This is an image$\rightarrow$mage translation problem that finds its applications in the entertainment and animation industry. Cartoon movies evolved from 2D animations in 1930 and became more lifelike with timeline. In image synthesis, audio, and other sorts of data, Generative Adversarial Networks have demonstrated promising outcomes. They also produce excellent results when translating images to images. In this research, a CycleGAN based methodology for generating target Human Faces from source Cartoon Faces is proposed, preserving the facial characteristics i.e. face shape, eyebrow alignment and hair style. In order to improve the mapping we have used contour loss along with cycle consistency loss in our model and patch discriminator is used with L2 norm.","","978-1-6654-1836-2","10.1109/GCAT52182.2021.9587703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9587703","Generative Adversarial Networks;CycleGAN;cycle consistency;patch discriminator;l2 norm","Training;Shape;Semantics;Transforms;Generative adversarial networks;Animation;Motion pictures","computer animation;face recognition;feature extraction;neural nets","translating images;CycleGAN based methodology;face shape;contour loss;cycle consistency loss;human face translation;human translation;2D vector cartoon face;semantic similarity;input domains;image→mage translation problem;entertainment;animation industry;image synthesis;generative adversarial networks;cartoon movies;source cartoon faces;target human faces","","","","15","IEEE","9 Nov 2021","","","IEEE","IEEE Conferences"
"Investigating the Potential of Auxiliary-Classifier Gans for Image Classification in Low Data Regimes","A. Dravid; F. Schiffers; Y. Wu; O. Cossairt; A. K. Katsaggelos","Department of Computer Science, Northwestern University, Evanston, IL, USA; Department of Computer Science, Northwestern University, Evanston, IL, USA; Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL, USA; Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL, USA; Department of Electrical and Computer Engineering, Northwestern University, Evanston, IL, USA","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","3318","3322","Generative Adversarial Networks (GANs) have shown promise in augmenting datasets and boosting convolutional neural network (CNN) performance on image classification tasks. But they introduce more hyperparameters to tune as well as the need for additional time and computational power to train, supplementary to the CNN. In this work, we examine the potential for Auxiliary-Classifier GANs (AC-GANs) as a ’one-stop-shop’ architecture for image classification, particularly in low data regimes. Additionally, we explore modifications to the typical AC-GAN framework, changing the generator’s latent space sampling scheme and employing a Wasserstein loss with gradient penalty to stabilize the simultaneous training of image synthesis and classification. Through experiments on images of varying resolutions and complexity, we demonstrate that AC-GANs show promise in image classification, achieving competitive performance with standard CNNs. These methods can be employed as an ’all-in-one’ framework with particular utility in the absence of large amounts of training data.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9747286","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9747286","Generative Adversarial Networks;Image Classification;Convolutional Neural Networks;Data Aug-mentation;Deep Learning","Training;Image resolution;Training data;Generative adversarial networks;Complexity theory;Convolutional neural networks;Task analysis","cellular neural nets;convolutional neural nets;gradient methods;image classification;image resolution;image sampling;learning (artificial intelligence)","low data regimes;generative adversarial networks;CNN;image classification tasks;typical AC-GAN framework;image synthesis;training data;convolutional neural network performance;auxiliary-classifier GAN;one-stop-shop architecture;generator latent space sampling scheme;Wasserstein loss;gradient penalty;image resolution","","","","28","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"GEAC: Generating and Evaluating Handwritten Arabic Characters Using Generative Adversarial Networks","T. Alkhodidi; L. Aljoudi; A. Fallatah; A. Bashy; N. Ali; N. Alqahtani; N. Almajnooni; A. Allhabi; T. Albarakati; T. Alafif; S. Sasi","Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer Science Department, Jamoum University College, Umm Al-Qura University, Jamoum, Saudi Arabia; Computer & Information Science Department, Gannon University, Erie, PA, US","2021 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE)","28 Apr 2021","2021","","","228","233","Generative Adversarial Network (GAN) has made a breakthrough and great success in many research areas in computer vision. Different GANs generate different outputs. In this research work, we apply different GANs to generate handwritten Arabic characters. A basic GAN, Vanilla GAN, Deep Convolutional GAN (DCGAN), Bidirectional GAN (BiGAN), and Wasserstein GAN (WGAN) are used. Then, the results of the generated images are evaluated using native-Arabic human and Fréchet Inception Distance (FID). The qualitative and quantitative results are provided for the images generation and evaluation. In experimental evaluation, WGAN achieves better results in FID with a value of 96.007. On the other hand, DCGAN achieves better results in native-Arabic human evaluation with a value of 35%.","","978-1-6654-2921-4","10.1109/ICCIKE51210.2021.9410746","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9410746","Handwritten Arabic character;image generation;evaluation;GAN;VanillaGAN;DCGAN;BiGAN;WGAN","Computer vision;Image synthesis;Generative adversarial networks;Computational intelligence","computer vision;convolutional neural nets;handwritten character recognition;neural nets","Deep Convolutional GAN;Bidirectional GAN;Wasserstein GAN;native-Arabic human Fréchet Inception Distance;images generation;native-Arabic human evaluation;handwritten Arabic characters;Generative Adversarial Network;basic GAN;Vanilla GAN;Fréchet Inception Distance;computer vision","","1","","8","IEEE","28 Apr 2021","","","IEEE","IEEE Conferences"
"ISAR Images Generation Via Generative Adversarial Networks","R. -Y. Zhou; Z. -L. Yang; F. Wang","Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China; Key laboratory for Information Science of Electromagnetic Waves (MoE), School of Information Science and Technology, Fudan University, Shanghai, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","5267","5270","One of the challenges faced by current intelligent target recognition tasks is the lack of samples, especially in the Inverse Synthetic Aperture Radar (ISAR) images understanding. In this paper, we proposed an ISAR objects generative network to generate multi-aspect ISAR images. A simulated ISAR dataset of six types of aircrafts is produced via, using bidirectional analytic ray tracing (BART) method. Then, the proposed generative network is trained with the simulated ISAR dataset. We evaluated the performance of the proposed network using structural similarity (SSIM). The experimental results show that the generated targets are very close to the real ISAR samples, and the SSIM between generated and real ISAR images of aircrafts is larger than 0.7.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9553814","National Natural Science Foundation of China(grant numbers:61901122); Natural Science Foundation of Shanghai(grant numbers:20ZR1406300); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9553814","Inverse Synthetic Aperture Radar (ISAR);Automatic Target Recognition (ATR);Generative Adversarial Nets (GANs)","Training;Target recognition;Image synthesis;Geoscience and remote sensing;Ray tracing;Generative adversarial networks;Aircraft","","","","1","","12","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks With Attention Mechanisms at Every Scale","F. Makhmudkhujaev; I. K. Park","Department of Information and Communication Engineering, Inha University, Incheon, South Korea; Department of Information and Communication Engineering, Inha University, Incheon, South Korea","IEEE Access","30 Dec 2021","2021","9","","168404","168414","Existing works in image synthesis have shown the efficiency of applying attention mechanisms in generating natural-looking images. Despite the great informativeness, current works utilize such mechanisms at a certain scale of generative and discriminative networks. Intuitively, the increased use of attention should lead to better performance. However, due to memory constraints, even moving a single attention mechanism to a higher scale of the network is infeasible. Motivated by the importance of attention in image generation, we tackle this limitation by proposing a generative adversarial network-based framework that readily incorporates attention mechanisms at every scale of its networks. A straightforward structure of attention mechanism enables direct plugging in a scale-wise manner and trains jointly with adversarial networks. As a result, networks are forced to focus on relevant regions of feature maps learned at every scale, thus improving their own image representation power. In addition, we exploit and show the usage of multiscale attention features as a complementary feature set in discriminator training. We demonstrate qualitatively and quantitatively that the introduction of scale-wise attention mechanisms benefits competitive networks, thus improving the performance compared with those of current works.","2169-3536","","10.1109/ACCESS.2021.3135637","National Research Foundation of Korea (NRF); Korea Government (MSIT)(grant numbers:NRF-2019R1A2C1006706); Institute of Information and Communications Technology Planning and Evaluation (IITP) grant; Korea Government (MSIT), Artificial Intelligence Convergence Research Center (Inha University)(grant numbers:2020-0-01389); Artificial Intelligence Innovation Hub(grant numbers:2021-0-02068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650851","Image synthesis;generative adversarial networks;attention;multiscale","Generators;Generative adversarial networks;Image synthesis;Training;Task analysis;Games;Reliability","feature extraction;image motion analysis;image representation;self-organising feature maps","image generation;generative adversarial network-based framework;multiscale attention features;scale-wise attention mechanisms benefits competitive networks;memory constraints;straightforward structure;feature maps;image representation power","","","","41","CCBY","14 Dec 2021","","","IEEE","IEEE Journals"
"Aesthetic Style Transfer through Text-to-image Synthesis and Image-to-image Translation","M. Kotera; R. Togo; T. Ogawa; M. Haseyama","School of Engineering, Hokkaido University, N-13, W-8, Kita-ku, Hokkaido, Sapporo, Japan; Faculty of Information Science and Technology, Hokkaido University, N-14, W-9, Kita-ku, Hokkaido, Sapporo, Japan; Faculty of Information Science and Technology, Hokkaido University, N-14, W-9, Kita-ku, Hokkaido, Sapporo, Japan; Faculty of Information Science and Technology, Hokkaido University, N-14, W-9, Kita-ku, Hokkaido, Sapporo, Japan","2019 IEEE 8th Global Conference on Consumer Electronics (GCCE)","27 Feb 2020","2019","","","483","484","This paper presents a style transfer method combining generative adversarial networks and style transfer networks. In the previous style transfer methods, transformation from one image to another has been proposed. On the other hand, our method enables style transfer from a text to an image. This will be helpful when there are no images that represent the desired style. Experimental results show the effectiveness of our method.","2378-8143","978-1-7281-3575-5","10.1109/GCCE46687.2019.9015508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9015508","","Computer vision;Generative adversarial networks;Semantics;Conferences;Image segmentation;Gallium nitride;Task analysis","image texture;text analysis","aesthetic style transfer;text-to-image synthesis;image-to-image translation;style transfer method;generative adversarial networks;style transfer networks;previous style transfer methods;desired style","","1","","10","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"VCL-GAN: A Variational Contrastive Learning Generative Adversarial Network for Image Synthesis","K. Liang; H. Lei; M. Wang","School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China; School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China; School of Computer and Information Engineering, Jiangxi Normal University, Nanchang, China","2022 9th International Conference on Digital Home (ICDH)","19 Dec 2022","2022","","","50","54","Generative Adversarial Networks (GANs) have worked well for image generation, but recent works have shown that their generated images lack diversity. In response to this problem, we propose an image generation network based on contrastive learning (CL) and Autoencoder (AE). Firstly, we try to let the network learn to extract different types of features by Siamese network (SN), which is a classic of contrastive learning. Secondly, we perform variational processing on the resulting codes, so that the GANs can generate high-quality images of multiple types by the codes. Finally, extensive experiments and comparisons with the state-of-the-art are conducted on the Tsinghua dog dataset that the method has made progress in generating realistic images of different types.","","978-1-6654-5478-0","10.1109/ICDH57206.2022.00015","National Natural Science Foundation of China; Jiangxi Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9978515","component;generative network;contrastive learning;feature extraction;image generation","Codes;Image resolution;Image synthesis;Dogs;Feature extraction;Generative adversarial networks","feature extraction;image representation;learning (artificial intelligence);neural nets;realistic images","AE;autoencoder;CL;contrastive learning;high-quality images;image generation network;image synthesis;realistic images;Siamese network;SN;variational contrastive learning generative adversarial network;variational processing;VCL-GAN","","","","29","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Image generation of traditional Chinese window grilles based on generative adversarial networks","C. Miao; J. Wu; J. Chen; S. Xiong; L. Wang; Q. Wang","School of Information and Communication Engineering, Communication University of China, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China; School of Information and Communication Engineering, Communication University of China, Beijing, China","2022 International Conference on Culture-Oriented Science and Technology (CoST)","28 Sep 2022","2022","","","232","236","Window grille is one of the expressions of traditional Chinese folk arts, which has unique stylistic characteristics and rich symbolic meanings. Studying how to extract the style characteristics of window grilles and generate new window grilles is beneficial to the inheritance and development of traditional Chinese arts. In recent years, the innovative development of generative adversarial networks (GANs) has made it possible to capture the intrinsic distribution of data and generate images that look like real ones. On the basis of researching existing three types of style-based generative adversarial networks (StyleGANs) and adaptive discriminator augmentation (ADA), we use StyleGAN2-ADA to train window grille datasets and generate new window grille images. Finally, multiple image quality evaluation metrics are used to analyze the generated images. The result shows that StyleGAN2-ADA has a good effect on the automatic generation of window grille images. In addition, by comparing the results of different size datasets, we found that the size of dataset has a significant impact on the quality of the generated window grilles.","","978-1-6654-6248-8","10.1109/CoST57098.2022.00055","Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9898534","window grille images generation;GANs;StyleGANs;ADA","Training;Measurement;Image quality;Art;Costs;Adaptive systems;Image synthesis","art;computer graphics;image processing;neural nets","window grille dataset training;image quality evaluation metrics;StyleGAN2-ADA;adaptive discriminator augmentation;traditional Chinese arts;stylistic characteristics;generative adversarial networks;traditional Chinese window grilles;automatic image generation","","","","14","IEEE","28 Sep 2022","","","IEEE","IEEE Conferences"
"Multimodal MR Image Synthesis Using Gradient Prior and Adversarial Learning","X. Liu; A. Yu; X. Wei; Z. Pan; J. Tang","Hubei Province Key Laboratory of Intelligent Information Processing, Real-Time Industrial System, Wuhan, China; Hubei Province Key Laboratory of Intelligent Information Processing, Real-Time Industrial System, Wuhan, China; Hubei Province Key Laboratory of Intelligent Information Processing, Real-Time Industrial System, Wuhan, China; Information Technology Center, Wenzhou Medical University, Wenzhou, China; College of Computing, Michigan Technological University, Houghton, USA","IEEE Journal of Selected Topics in Signal Processing","24 Sep 2020","2020","14","6","1176","1188","In magnetic resonance imaging (MRI), several images can be obtained using different imaging settings (e.g. T1, T2, DWI, and Flair). These images have similar anatomical structures but are with different contrasts, which provide a wealth of information for diagnosis. However, the images under specific imaging settings may not be available due to the limitation of scanning time or corruption caused by noises. It is attractive to derive missing images with some settings from the available MR images. In this paper, we propose a novel end-to-end multisetting MR image synthesis method. The proposed method is based on generative adversarial networks (GANs) - a deep learning model. In the proposed method, different MR images obtained by different settings are used as the inputs of a GANs and each image is encoded by an encoder. Each encoder includes a refinement structure which is used to extract a multiscale feature map from an input image. The multiscale feature maps from different input images are then fused to generate several desired target images under specific settings. Because the resultant images obtained with GANs have blurred edges, we fuse gradient prior information in the model to protect high frequency information such as important tissue textures of medical images. In the proposed model, the multiscale information is also adopted in the adversarial learning (not just in the generator or discriminator) so that we can produce high quality synthesized images. We evaluated the proposed method on two public datasets: BRATS and ISLES. Experimental results demonstrate that the proposed approach is superior to current state-of-the-art methods.","1941-0484","","10.1109/JSTSP.2020.3013418","National Natural Science Foundation of China(grant numbers:61403287,61472293,61572381); Wenzhou Municipal Science and Technology Bureau(grant numbers:2018ZG016); Natural Science Foundation of Zhejiang Province(grant numbers:LY16F030010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153921","Generative adversarial networks;gradient prior;image synthesis;magnetic resonance imaging","Medical diagnostic imaging;Magnetic resonance imaging;Generative adversarial networks;Image segmentation","biomedical MRI;brain;feature extraction;image fusion;image reconstruction;image segmentation;image texture;learning (artificial intelligence);medical image processing","adversarial learning;magnetic resonance imaging;anatomical structures;missing images;MR image synthesis method;generative adversarial networks;deep learning model;refinement structure;multiscale feature map;gradient prior information;high-frequency information;medical images;multiscale information","","33","","48","IEEE","31 Jul 2020","","","IEEE","IEEE Journals"
"HTGAN: An architecture for Hindi Text based Image Synthesis","A. S. Parihar; A. Kaushik; A. V. Choudhary; A. K. Singh","Dept. of Computer Engineering, Delhi Technological University, New Delhi, India; Dept. of Computer Engineering, Delhi Technological University, New Delhi, India; Dept. of Computer Engineering, Delhi Technological University, New Delhi, India; Dept. of Computer Engineering, Delhi Technological University, New Delhi, India","2021 5th International Conference on Computer, Communication and Signal Processing (ICCCSP)","29 Jun 2021","2021","","","273","279","Synthesis of high-quality images that are semantically consistent with their text descriptions has emerged as an essential and onerous problem in the areas of CV and NLP. Various significant steps have been taken in the task of generation of images using English textual descriptions, with Multimodal GANs being at the forefront of all these efforts. In this work we attempt to extend the existing English text to image generation (T2I) techniques to the novel task of Hindi T2I using language translation models. To achieve this, the input Hindi sentences were translated to English using a transformer based Neural Machine Translation module the output of which was then fed to the Generative Adversarial Networks based Image Generation Module. The outcomes of this approach have been evaluated using well established metrics such as Inception Score and BLEU Score which yield scores that indicate the generation of lifelike images which semantically align with the input text descriptions.","","978-1-6654-3277-1","10.1109/ICCCSP52374.2021.9465527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465527","Text to Image;Neural Machine Translation;Transformers;Generative Adversarial Networks","Measurement;Image synthesis;Computer architecture;Signal processing;Generative adversarial networks;Machine translation;Task analysis","language translation;natural language processing;text analysis","HTGAN;Hindi text;Image synthesis;high-quality images;text descriptions;essential problem;onerous problem;significant steps;English textual descriptions;Multimodal GANs;forefront;existing English text;Hindi T2I;language translation models;input Hindi sentences;Neural Machine Translation;Generative Adversarial Networks;Image Generation Module;lifelike images;input text","","","","30","IEEE","29 Jun 2021","","","IEEE","IEEE Conferences"
"TT2INet: Text to Photo-realistic Image Synthesis with Transformer as Text Encoder","J. Zhu; Z. Li; H. Ma","Guangxi Key Lab of Multi-source Information Mining and Security, Guangxi Normal University, Guilin, China; Guangxi Key Lab of Multi-source Information Mining and Security, Guangxi Normal University, Guilin, China; College of Computer Science and Engineering, Northwest Normal University, Lanzhou, China","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","8","A text-to-image (T2I) generation method is mainly evaluated from two aspects, one is the quality and diversity of the generated images, and the other is the semantic consistency between the generated images and the input sentences. The feature extraction of the text is a very important part. In this paper, we propose a Transformer based Text-to-Image Network (TT2INet). we use the pre-trained Transformer model (ALBERT) to extract the sentence feature vectors and word feature vectors of the input sentences as the basis for the Generative Adversarial Networks (GANs) to generate images. In addition, we also added self-attention mechanism and spectral normalization method to the model. Adding a self-attention mechanism can make the model pay attention to more local features when generating images. Using the spectral normalization method can make the training of GANs more stable. The Inception Scores of our method on Oxford-102, CUB and COCO datasets are 3.90, 4.89 and 26.53, and R-precision scores are 92.55, 87.72 and 92.29, respectively.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9534074","National Natural Science Foundation of China(grant numbers:61966004,61866004); Guangxi Natural Science Foundation(grant numbers:2019GXNSFDA245018); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534074","Transformer;Generative Adversarial Networks (GANs);spectral normalization;self-attention","Training;Image synthesis;Semantics;Neural networks;Transformers;Feature extraction;Generative adversarial networks","feature extraction;learning (artificial intelligence);natural language processing;realistic images;text analysis","TT2INet;photo-realistic Image synthesis;Text encoder;generation method;input sentences;feature extraction;Text-to-Image Network;pre-trained Transformer model;sentence feature vectors;Generative Adversarial Networks;self-attention mechanism;spectral normalization method;generating images","","","","20","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis","E. R. Chan; M. Monteiro; P. Kellnhofer; J. Wu; G. Wetzstein",Stanford University; Stanford University; Stanford University; Stanford University; Stanford University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","5795","5805","We have witnessed rapid progress on 3D-aware image synthesis, leveraging recent advances in generative visual models and neural rendering. Existing approaches how-ever fall short in two ways: first, they may lack an under-lying 3D representation or rely on view-inconsistent rendering, hence synthesizing images that are not multi-view consistent; second, they often depend upon representation network architectures that are not expressive enough, and their results thus lack in image quality. We propose a novel generative model, named Periodic Implicit Generative Adversarial Networks (π-GAN or pi-GAN), for high-quality 3D-aware image synthesis. π-GAN leverages neural representations with periodic activation functions and volumetric rendering to represent scenes as view-consistent radiance fields. The proposed approach obtains state-of-the-art results for 3D-aware image synthesis with multiple real and synthetic datasets.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577547","","Image quality;Visualization;Computer vision;Three-dimensional displays;Image synthesis;Network architecture;Rendering (computer graphics)","image representation;neural nets;rendering (computer graphics);stereo image processing","pi-GAN;generative visual models;view-inconsistent rendering;representation network architectures;image quality;generative model;high-quality 3D-aware image synthesis;π-GAN;Periodic Implicit Generative Adversarial Networks","","99","","69","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Multi-View Consistent Generative Adversarial Networks for 3D-aware Image Synthesis","X. Zhang; Z. Zheng; D. Gao; B. Zhang; P. Pan; Y. Yang","DAMO Academy, Alibaba Group; ReLER, AAII, University of Technology, Sydney; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; Zhejiang University","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","18429","18438","3D-aware image synthesis aims to generate images of objects from multiple views by learning a 3D representation. However, one key challenge remains: existing approaches lack geometry constraints, hence usually fail to generate multi-view consistent images. To address this challenge, we propose Multi-View Consistent Generative Adversarial Networks (MVCGAN) for high-quality 3D-aware image synthesis with geometry constraints. By leveraging the underlying 3D geometry information of generated images, i.e., depth and camera transformation matrix, we explicitly establish stereo correspondence between views to perform multi-view joint optimization. In particular, we enforce the photometric consistency between pairs of views and integrate a stereo mixup mechanism into the training process, encouraging the model to reason about the correct 3D shape. Besides, we design a two-stage training strategy with feature-level multi-view joint optimization to improve the image quality. Extensive experiments on three datasets demonstrate that MVCGAN achieves the state-of-the-art performance for 3D-aware image synthesis.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879976","Image and video synthesis and generation","Geometry;Training;Image quality;Solid modeling;Three-dimensional displays;Image synthesis;Shape","cameras;geometry;image matching;image reconstruction;image representation;learning (artificial intelligence);optimisation;stereo image processing","MultiView Consistent Generative Adversarial Networks;geometry constraints;multiview consistent images;high-quality 3D-aware image synthesis;underlying 3D geometry information;correct 3D shape;feature-level multiview joint optimization;image quality","","4","","66","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Bidirectional Mapping Generative Adversarial Networks for Brain MR to PET Synthesis","S. Hu; B. Lei; S. Wang; Y. Wang; Z. Feng; Y. Shen","Department of Computer Science, University of Chinese Academy of Sciences, Beijing, China; School of Biomedical Engineering, Shenzhen University, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; School of Automation, Central South University, Changsha, China; College of Intelligent Systems Science and Engineering, Harbin Engineering University, Harbin, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China","IEEE Transactions on Medical Imaging","30 Dec 2021","2022","41","1","145","157","Fusing multi-modality medical images, such as magnetic resonance (MR) imaging and positron emission tomography (PET), can provide various anatomical and functional information about the human body. However, PET data is not always available for several reasons, such as high cost, radiation hazard, and other limitations. This paper proposes a 3D end-to-end synthesis network called Bidirectional Mapping Generative Adversarial Networks (BMGAN). Image contexts and latent vectors are effectively used for brain MR-to-PET synthesis. Specifically, a bidirectional mapping mechanism is designed to embed the semantic information of PET images into the high-dimensional latent space. Moreover, the 3D Dense-UNet generator architecture and the hybrid loss functions are further constructed to improve the visual quality of cross-modality synthetic images. The most appealing part is that the proposed method can synthesize perceptually realistic PET images while preserving the diverse brain structures of different subjects. Experimental results demonstrate that the performance of the proposed method outperforms other competitive methods in terms of quantitative measures, qualitative displays, and evaluation metrics for classification.","1558-254X","","10.1109/TMI.2021.3107013","National Natural Science Foundations of China(grant numbers:62172403,61872351); International Science and Technology Cooperation Projects of Guangdong(grant numbers:2019A050510030); Distinguished Young Scholars Fund of Guangdong(grant numbers:2021B1515020019); Excellent Young Scholars of Shenzhen(grant numbers:RCYX20200714114641211); Shenzhen Key Basic Research Project(grant numbers:JCYJ20200109115641762); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521163","Medical image synthesis;generative adversarial network;bidirectional mapping mechanism","Generators;Generative adversarial networks;Three-dimensional displays;Positron emission tomography;Training;Image synthesis;Computed tomography","brain;image fusion;medical image processing;positron emission tomography","perceptually realistic PET images;diverse brain structures;Bidirectional Mapping Generative Adversarial Networks;fusing multimodality medical images;positron emission tomography;functional information;PET data;end-to-end synthesis network;image contexts;brain MR-to-PET synthesis;bidirectional mapping mechanism;high-dimensional latent space;Dense-UNet generator architecture;cross-modality synthetic images","Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Positron-Emission Tomography;Semantics","28","","58","IEEE","24 Aug 2021","","","IEEE","IEEE Journals"
"Cross Modal Facial Image Synthesis Using a Collaborative Bidirectional Style Transfer Network","N. U. Din; S. Bae; K. Javed; H. Park; J. Yi","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; National Centre of Artificial Intelligence (NCAI), Saudi Data and Artificial Intelligence Authority (SDAIA), Riyadh, Saudi Arabia; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Saudi Scientific Society for Cybersecurity, Riyadh, Saudi Arabia","IEEE Access","27 Sep 2022","2022","10","","99077","99087","In this paper, we present a novel collaborative bidirectional style transfer network based on generative adversarial network (GAN) for cross modal facial image synthesis, possibly with large modality gap. We think that representation decomposed into content and style can be effectively exploited for cross modal facial image synthesis. However, we have observed that unidirectional application of decomposed representation based style transfer in case of large modality gap does not work well for this purpose. Unlike existing image synthesis methods that typically formulate image synthesis as an unidirectional feed forward mapping, our network utilizes mutual interaction between two opposite mappings in a collaborative way to address complex image synthesis problem with large modality gap. The proposed bidirectional network aligns shape content from two modalities and exchanges their appearance styles using feature maps of the layers in the encoder space. This allows us to effectively retain the shape content and transfer style details for synthesizing each modality. Focusing on facial images, we consider facial photo, sketch, and color-coded semantic segmentation as different modalities. The bidirectional synthesis results for the pairs of these modalities show the effectiveness of the proposed approach. We further apply our network to style-content manipulation to generate multiple photo images with various appearance styles for a same content shape. The proposed method can be adopted for solving other cross modal image synthesis tasks. The dataset and source code are available at https://github.com/kamranjaved/Bidirectional-style-transfer-network.","2169-3536","","10.1109/ACCESS.2022.3207288","National Research Foundation of Korea (NRF)(grant numbers:2020R1F1A1048438); High-performance Computing (HPC) Support Project; Ministry of Science and ICT and National IT Industry Promotion Agency (NIPA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893785","Generative adversarial network;image synthesis;unidirectional style transfer network;bidirectional style transfer network;collaborative learning","Image synthesis;Image segmentation;Face recognition;Generative adversarial networks;Bidirectional control;Collaborative work","face recognition;feature extraction;image colour analysis;image representation;image segmentation","cross modal facial image synthesis;decomposed representation based style transfer;modality gap;image synthesis methods;complex image synthesis problem;bidirectional network;appearance styles;transfer style details;facial images;bidirectional synthesis results;style-content manipulation;cross modal image synthesis tasks;collaborative bidirectional style transfer network","","","","39","CCBY","16 Sep 2022","","","IEEE","IEEE Journals"
"FDA-GAN: Flow-based Dual Attention GAN for Human Pose Transfer","L. Ma; K. Huang; D. Wei; Z. -Y. Ming; H. Shen","Information and Electronic Engineering, Zhejiang University, 12377 Hangzhou, zhejiang, China, 310058 (e-mail: mlyarthur@zju.edu.cn); College of Information Science & Electronic Engineering, Zhejiang University, 12377 Hangzhou, China, 310058 (e-mail: huangkejie@zju.edu.cn); College of Information Science & Electronic Engineering, Zhejiang University, 12377 Hangzhou, Zhejiang, China, 310058 (e-mail: tracywei@zju.edu.cn); Institute of Computing Innovation, Zhejiang University, 12377 Hangzhou, China, 310058 (e-mail: mingzhaoyan@gmail.com); College of Information Science & Electronic Engineering, Zhejiang University, 12377 Hangzhou, Zhejiang, China, (e-mail: shen_hb@zju.edu.cn)","IEEE Transactions on Multimedia","","2021","PP","99","1","1","Human pose transfer aims at transferring the appearance of the source person to the target pose. Existing methods utilizing flow-based warping for non-rigid human image generation have achieved great success. However, they fail to preserve the appearance details in synthesized images since the spatial correlation between the source and target is not fully exploited. To this end, we propose the Flow-based Dual Attention GAN (FDA-GAN) to apply occlusion- and deformation-aware feature fusion for higher generation quality. Specifically, deformable local attention and flow similarity attention, constituting the dual attention mechanism, can derive the output features responsible for deformable- and occlusion-aware fusion, respectively. Besides, to maintain the pose and global position consistency in transferring, we design a pose normalization network for learning adaptive normalization from the target pose to the source person. Both qualitative and quantitative results show that our method outperforms state-of-the-art models in public iPER and DeepFashion datasets.","1941-0077","","10.1109/TMM.2021.3134157","National Natural Science Foundation of China(grant numbers:U19B2043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645200","Pose Transfer;Image Synthesis;Generative Adversarial Networks(GANs)","Image synthesis;Task analysis;Strain;Solid modeling;Three-dimensional displays;Generative adversarial networks;Feature extraction","","","","4","","","IEEE","9 Dec 2021","","","IEEE","IEEE Early Access Articles"
"HFGAN-CN: T2I Model via Text-Image Hierarchical Attention Fusion","J. -Y. Han; J. -W. Liu","Department of Automation, College of Information Science and Engineering, China University of Petroleum, Beijing, Beijing, China; Department of Automation, College of Information Science and Engineering, China University of Petroleum, Beijing, Beijing, China","2022 34th Chinese Control and Decision Conference (CCDC)","14 Feb 2023","2022","","","6112","6117","In this paper, we propose a simple and effective text-to-image generative adversarial network named Hierarchical Fusion via GAN with Capsule Network discriminator (HFGAN-CN) , which uses only one generator and one discriminator to generate high-resolution images directly from text. We introduce a text-image feature fusion module via spatial and channel attention mechanism into the generator, which performs hierarchical fusion on the features of the two modes in the image generation process, so as to guide the generator to generate high-quality images in line with text semantics. We propose a match-aware discriminator to synthesize text-semantic compliant images and employ a capsule network to reconfigure the network structure of the discriminator without introducing an additional network. Experiments on the benchmark dataset show that our method is superior to the existing techniques and can effectively generate high-resolution images conforming to the text description.","1948-9447","978-1-6654-7896-0","10.1109/CCDC55256.2022.10033800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10033800","Text to Image Synthesis;Attention Mechanism;Feature Fusion;Generative Adversarial Networks","Training;Image synthesis;Computational modeling;Semantics;Generative adversarial networks;Generators;Computational complexity","","","","","","10","IEEE","14 Feb 2023","","","IEEE","IEEE Conferences"
"Visible to Thermal Image Synthesis using Light Weight Pyramid Network","S. Imtiaz; I. A. Taj; R. Nawaz","Electrical Engineering Department, Capital University of Science & Technology, Islamabad, Pakistan; Electrical Engineering Department, Capital University of Science & Technology, Islamabad, Pakistan; VisPRS Research Group, Capital University of Science & Technology, Islamabad, Pakistan","2021 16th International Conference on Emerging Technologies (ICET)","31 Jan 2022","2021","","","1","5","We investigate Lightweight Pyramid Network as a general–purpose solution for image-to-image synthesis. Already employed techniques that are based upon deep convolutional neural networks (CNNs) have found reasonable success but with the trade-off of a large number of parameters that eventually result in high computational costs. In these methods, post processing of various types is incorporated as well to further refine the transformed image, thus making the whole process cumbersome and time taking. In this paper, we have made use of lightweight pyramid network (LPNet) for image synthesis that was primarily used for image deraining. We find that by using Laplacian-Gaussian image pyramid decomposition coupled with reconstruction and calculation of SSIM along with neural network, the heat signature in the resulting synthesized thermal image becomes much more enhanced and at the same time contours of various image objects stays prominent even without the use of any post processing techniques. The computations for training become less intensive due to the use of a shallow network. We further prove the efficacy of our approach by doing SSIM, PSNR and UQI Quantitative Analysis.","","978-1-6654-9437-3","10.1109/ICET54505.2021.9689909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689909","Deep convolutional neural network (CNN);pyramid;KAIST;CVC-09;CVC-14 lightweight networks;Residual Learning;Generative Adversarial Networks;Post Processing;LPNet;ResNet;MobileNet;StyleGAN2;CRN;ThermalNet","Training;Laplace equations;Image synthesis;Statistical analysis;Surveillance;Thermal decomposition;Neural networks","convolutional neural nets;deep learning (artificial intelligence);Gaussian processes;image denoising;image enhancement;image reconstruction;infrared imaging;transforms","thermal image synthesis;lightweight pyramid network;image-to-image synthesis;deep convolutional neural networks;image deraining;shallow network;Laplacian-Gaussian image pyramid decomposition;SSIM reconstruction;SSIM calculation;PSNR;UQI quantitative analysis;LPNet;deep CNN","","","","10","IEEE","31 Jan 2022","","","IEEE","IEEE Conferences"
"Visual Thinking of Neural Networks: Interactive Text to Image Synthesis","H. Lee; G. Kim; Y. Hur; H. Lim","Department of Computer Science and Engineering, Korea University, Seoul, South Korea; Department of Computer Science and Engineering, Korea University, Seoul, South Korea; Department of Computer Science and Engineering, Korea University, Seoul, South Korea; Department of Computer Science and Engineering, Korea University, Seoul, South Korea","IEEE Access","30 Apr 2021","2021","9","","64510","64523","Reasoning, a trait of cognitive intelligence, is regarded as a crucial ability that distinguishes humans from other species. However, neural networks now pose a challenge to this human ability. Text-to-image synthesis is a class of vision and linguistics, wherein the goal is to learn multimodal representations between the image and text features. Hence, it requires a high-level reasoning ability that understands the relationships between objects in the given text and generates high-quality images based on the understanding. Text-to-image translation can be termed as the visual thinking of neural networks. In this study, our model infers the complicated relationships between objects in the given text and generates the final image by leveraging the previous history. We define diverse novel adversarial loss functions and finally demonstrate the best one that elevates the reasoning ability of the text-to-image synthesis. Remarkably, most of our models possess their own reasoning ability. Quantitative and qualitative comparisons with several methods demonstrate the superiority of our approach.","2169-3536","","10.1109/ACCESS.2021.3074973","Ministry of Science and ICT (MSIT), South Korea, through the Information Technology Research Center (ITRC) Support Program Supervised by the Institute for Information and Communications Technology Planning and Evaluation (IITP)(grant numbers:IITP-2018-0-01405); IITP through the MSIT(grant numbers:IITP-2020-0-00368); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9410550","Generative adversarial networks;image generation;multimodal learning;multimodal representation;text-to-image synthesis","Cognition;Visualization;Neural networks;Generative adversarial networks;Image synthesis;Image registration;Text recognition","document image processing;learning (artificial intelligence);neural nets","visual thinking;neural networks;text-to-image translation;high-quality images;high-level reasoning ability;text features;text-to-image synthesis;interactive text","","3","","50","CCBY","22 Apr 2021","","","IEEE","IEEE Journals"
"Exploring Painting Synthesis with Diffusion Models","D. Yi; C. Guo; T. Bai","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China","2021 IEEE 1st International Conference on Digital Twins and Parallel Intelligence (DTPI)","22 Sep 2021","2021","","","332","335","As a significant composition of art, fine art painting is becoming a research hotspot in machine learning community. With unique aesthetic value, paintings have quite different representations from natural images, making them irreplaceable. Meanwhile, the lack of training data is common in painting-related machine learning tasks. Therefore, the synthesis of fine art painting is meaningful and challenging work. There are two main types of generative models for image synthesis: generative adversarial networks (GANs) and likelihood-based models. GAN-based models can obtain high-quality samples but usually sacrifice diversity and training stability. Diffusion models are a class of likelihood-based models and have recently been shown to achieve state-of-the-art quality on the image synthesis tasks. In this paper, we explore generating fine art paintings by using diffusion models. We carried out the experiments on the partial impression paintings from the Wikiart dataset. The results demonstrate that the diffusion model can generate high-quality samples, and it is easy to train to cover more target distribution than the GAN-based methods.","","978-1-6654-3337-2","10.1109/DTPI52967.2021.9540115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9540115","painting synthesis;image generation;diffusion models","Training;Art;Image synthesis;Digital twin;Training data;Machine learning;Generative adversarial networks","art;image processing;learning (artificial intelligence);neural nets","diffusion model;likelihood-based models;image synthesis;fine art painting;partial impression paintings;painting synthesis;painting-related machine learning tasks;generative models;generative adversarial networks;GAN-based models","","","","23","IEEE","22 Sep 2021","","","IEEE","IEEE Conferences"
"Multi-Instance Sketch to Image Synthesis With Progressive Generative Adversarial Networks","Z. -H. Wang; N. Wang; J. Shi; J. -J. Li; H. Yang","School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; School of Software, Qufu Normal University, Qufu, China; School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China; School of Software, Dalian University of Technology, Dalian, China","IEEE Access","10 May 2019","2019","7","","56683","56693","Real-world images usually contain multiple objects, as a result, generating an image from a multi-instance sketch is an attractive research topic. However, existing generative networks usually produce a similar texture on different instances for those methods focus on learning the distribution of the whole image. To address this problem, we propose a progressive instance texture reserved generative approach to generate more convincible images by decoupling the generation of the instances and the whole image. Specifically, we create an instance generator to synthesize the primitive color distribution and the detailed texture for each instance. Then, an image generator is designed to combine all of these instances to synthesize an image retaining texture and color. Besides, to generate more significant details, such as eyes, ears, and so on, we propose a novel technique called discriminative sketch augmentation, which can provide structural constraint by obtaining the sketch of the discriminative region. The extensive experiments demonstrate that our model not only generates convincing images but also achieves higher inception score and lower Fréchet Inception Distance on the MS-COCO dataset.","2169-3536","","10.1109/ACCESS.2019.2913178","National Natural Science Foundation of China(grant numbers:61472059,61772108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698864","Multi-instance sketch to image;image processing;image generation","Generators;Image generation;Image color analysis;Task analysis;Generative adversarial networks;Software;Semantics","image colour analysis;image texture;neural nets","primitive color distribution;image generator;image retaining texture;discriminative sketch augmentation;image synthesis;generative adversarial networks;Fréchet inception distance","","4","","40","OAPA","25 Apr 2019","","","IEEE","IEEE Journals"
"Generating Images in Compressed Domain Using Generative Adversarial Networks","B. Kang; S. Tripathi; T. Q. Nguyen","Research Center for Electrical and Information Technology, Seoul National University of Science and Technology, Seoul, South Korea; Intel Labs, San Diego, CA, USA; Department of Electrical and Computer Engineering, University of California at San Diego, La Jolla, CA, USA","IEEE Access","12 Oct 2020","2020","8","","180977","180991","In this article, we present a generative adversarial network framework that generates compressed images instead of synthesizing raw RGB images and compressing them separately. In the real world, most images and videos are stored and transferred in a compressed format to save storage capacity and data transfer bandwidth. However, since typical generative adversarial networks generate raw RGB images, those generated images need to be compressed by a post-processing stage to reduce the data size. Among image compression methods, JPEG has been one of the most commonly used lossy compression methods for still images. Hence, we propose a novel framework that generates JPEG compressed images using generative adversarial networks. The novel generator consists of the proposed locally connected layers, chroma subsampling layers, quantization layers, residual blocks, and convolution layers. The locally connected layer is proposed to enable block-based operations. We also discuss training strategies for the proposed architecture including the loss function and the decoding between its generator and its discriminator. The proposed method is evaluated using the publicly available CIFAR-10 dataset and LSUN bedroom dataset. The results demonstrate that the proposed method is able to generate compressed data with competitive qualities.","2169-3536","","10.1109/ACCESS.2020.3027800","NSF(grant numbers:IIS-1522125); Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:NRF-2019R1A6A1A03032119); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210026","Generative adversarial networks;image generation;image synthesis","Image coding;Gallium nitride;Generators;Transform coding;Generative adversarial networks;Quantization (signal);Decoding","data compression;decoding;image classification;image coding;image colour analysis;neural nets","compressed data;locally connected layer;JPEG compressed images;lossy compression methods;image compression methods;data transfer bandwidth;storage capacity;compressed format;raw RGB images;generative adversarial network framework;compressed domain","","2","","60","CCBYNCND","30 Sep 2020","","","IEEE","IEEE Journals"
"DCFGAN: Dynamic Convolutional Fusion Generative Adversarial Network for Text-to-Image Synthesis","M. Tao; S. Wu; X. Zhang; C. Wang","School of Automation, Nanjing University of Posts and Telecommunications, NanJing, China; School of Automation, Nanjing University of Posts and Telecommunications, NanJing, China; School of Automation, Nanjing University of Posts and Telecommunications, NanJing, China; School of Automation, Nanjing University of Posts and Telecommunications, NanJing, China","2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)","14 Dec 2020","2020","1","","1250","1254","Text-to-image synthesis is the task of synthesizing realistic and text-matching images according to given text descriptions. Most text-to-image generative networks consist of two modules: a pre-trained text-image encoder and a text-to-image generative adversarial network. In this paper, we propose a stronger text encoder which employs a text Transformer to extract semantically meaningful parts from text descriptions. With the stronger text encoder, the generator can obtain more meaningful text information to synthesize realistic and text-matching images. In addition, we propose a Dynamic Convolutional text-image Fusion Generative Adversarial Network (DCFGAN) which employs the Dynamic Convolutional Fusion Block to fuse text and image features efficiently. The Dynamic Convolutional Fusion block adjusts the parameters in the convolution layer according to different text descriptions to synthesize text-matching images. It improves the efficiency of fusing text features and image features in generator network. We evaluate the proposed DCF-GAN on two benchmark datasets, the CUB and the Oxford-102. The extensive experiments demonstrate that our proposed stronger text encoder and Dynamic Convolutional Fusion Layer can greatly promote the performance of text-to-image synthesis.","","978-1-7281-5224-0","10.1109/ICIBA50161.2020.9277299","Nanjing University of Posts and Telecommunications General School(grant numbers:NY220057); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277299","Text-to-Image synthesis;generative adversarial network;cross-modal;image-text matching;deep representation learning","Generators;Convolution;Generative adversarial networks;Computational modeling;Semantics;Feature extraction;Head","convolutional neural nets;feature extraction;image fusion;image matching;text analysis","text features;image features;generator network;stronger text encoder;text-to-image synthesis;text-matching images;given text descriptions;text-to-image generative networks;pre-trained text-image encoder;text-to-image generative adversarial network;text Transformer;meaningful text information;Dynamic Convolutional text-image Fusion Generative Adversarial Network;Dynamic Convolutional Fusion Block;Dynamic Convolutional Fusion block;different text descriptions;dynamic convolutional fusion generative adversarial network;dynamic convolutional fusion layer","","12","","24","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"Mode Seeking Generative Adversarial Networks for Diverse Image Synthesis","Q. Mao; H. -Y. Lee; H. -Y. Tseng; S. Ma; M. -H. Yang","Institute of Digital Media, Peking University; University of California, Merced; University of California, Merced; Peng Cheng Laboratory; Google Cloud","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","1429","1437","Most conditional generation tasks expect diverse outputs given a single conditional context. However, conditional generative adversarial networks (cGANs) often focus on the prior conditional information and ignore the input noise vectors, which contribute to the output variations. Recent attempts to resolve the mode collapse issue for cGANs are usually task-specific and computationally expensive. In this work, we propose a simple yet effective regularization term to address the mode collapse issue for cGANs. The proposed method explicitly maximizes the ratio of the distance between generated images with respect to the corresponding latent codes, thus encouraging the generators to explore more minor modes during training. This mode seeking regularization term is readily applicable to various conditional generation tasks without imposing training overhead or modifying the original network structures. We validate the proposed algorithm on three conditional image synthesis tasks including categorical generation, image-to-image translation, and text-to-image synthesis with different baseline models. Both qualitative and quantitative results demonstrate the effectiveness of the proposed regularization method for improving diversity without loss of quality.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953230","Image and Video Synthesis;Deep Learning","","image processing;learning (artificial intelligence)","regularization term;cGANs;conditional generation tasks;conditional image synthesis tasks;categorical generation;image-to-image translation;text-to-image synthesis;diverse image synthesis;single conditional context;conditional generative adversarial networks;input noise vectors;output variations;conditional information","","138","","32","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications","M. -Y. Liu; X. Huang; J. Yu; T. -C. Wang; A. Mallya","NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; Google, Mountain View, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA","Proceedings of the IEEE","30 Apr 2021","2021","109","5","839","862","The generative adversarial network (GAN) framework has emerged as a powerful tool for various image and video synthesis tasks, allowing the synthesis of visual content in an unconditional or input-conditional manner. It has enabled the generation of high-resolution photorealistic images and videos, a task that was challenging or impossible with prior methods. It has also led to the creation of many new applications in content creation. In this article, we provide an overview of GANs with a special focus on algorithms and applications for visual synthesis. We cover several important techniques to stabilize GAN training, which has a reputation for being notoriously difficult. We also discuss its applications to image translation, image processing, video synthesis, and neural rendering.","1558-2256","","10.1109/JPROC.2021.3049196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343694","Computer vision;generative adversarial networks (GANs);image and video synthesis;image processing;neural rendering","Computer vision;Generators;Training data;Generative adversarial networks;Linear programming;Visualization;Training data;Neural networks;Rendering (computer vision);Photorealism;Image synthesis","neural nets;rendering (computer graphics);video signal processing","content creation;visual synthesis;GAN training;image translation;image processing;generative adversarial network framework;visual content;unconditional input-conditional manner;high-resolution photorealistic images;image synthesis;neural rendering","","36","","257","IEEE","1 Feb 2021","","","IEEE","IEEE Journals"
"R2-B2: A Metric of Synthesized Image’s Photorealism by Regression Analysis based on Recognized Objects’ Bounding Box","S. Hattori; K. Aiba; M. Takahara","Faculty of Advanced Engineering, The University of Shiga Prefecture, Hikone-shi, Japan; Graduate School of Science and Engineering, Muroran Institute of Technology, Muroran-shi, Japan; Faculty of Advanced Science and Technology, Ryukoku University, Otsu-shi, Japan","2022 Joint 12th International Conference on Soft Computing and Intelligent Systems and 23rd International Symposium on Advanced Intelligent Systems (SCIS&ISIS)","4 Jan 2023","2022","","","1","8","In recent years, a lot of researches on AI (Artificial Intelligence) for Image Synthesis and Image Generation have been being conducted actively, and state of the art GANs (Generative Adversarial Networks) for text-to-image have been able to generate precise images with high photorealism for a text-based user query (but also no-good images). However, it is pointed out that the precision of all images generated for a query has been not always enough high. Therefore, for practical usages, they are required to be re-ranked and/or filtered based on some sort of metric(s). This paper proposes a novel metric, R2-B2 (RR-BB), on photorealism, especially “size balance” (i.e., balance between in-image objects’ size), of a manually or automatically synthesized image by Regression analysis based on multiple Recognized objects’ Bounding Box, i.e., the position $(x, y)$ and size (width, height, or area) of objects recognized in the image.","","978-1-6654-9924-8","10.1109/SCISISIS55246.2022.10001857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10001857","image evaluation;no-reference image quality assessment;image quality metrics;object recognition;object detection;regression analysis;correlation analysis","Measurement;Image quality;Photorealism;Image recognition;Image synthesis;Generative adversarial networks;Regression analysis","artificial intelligence;object recognition;query processing;regression analysis;text analysis","AI;art GANs;Artificial Intelligence;Generative Adversarial Networks;high photorealism;Image Generation;Image Synthesis;in-image objects;multiple Recognized objects;no-good images;practical usages;precise images;R2-B2;regression analysis;synthesized image;text-based user query;text-to-image","","","","40","IEEE","4 Jan 2023","","","IEEE","IEEE Conferences"
"DAM-GAN : Image Inpainting Using Dynamic Attention Map Based on Fake Texture Detection","D. Cha; D. Kim","Department of Computer Science and Engineering, Pohang University of Science and Technology, South Korea; Department of Computer Science and Engineering, Pohang University of Science and Technology, South Korea","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","4883","4887","Deep neural advancements have recently brought remarkable image synthesis performance to the field of image inpainting. The adaptation of generative adversarial networks (GAN) in particular has accelerated significant progress in high-quality image reconstruction. However, although many notable GAN-based networks have been proposed for image inpainting, still pixel artifacts or color inconsistency occur in synthesized images during the generation process, which are usually called fake textures. To reduce pixel inconsistency disorder resulted from fake textures, we introduce a GANbased model using dynamic attention map (DAM-GAN). Our proposed DAM-GAN concentrates on detecting fake texture and products dynamic attention maps to diminish pixel inconsistency from the feature maps in the generator. Evaluation results on CelebA-HQ and Places2 datasets with other image inpainting approaches show the superiority of our network.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746659","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746659","CNN;Computer Vision;GAN;Image Inpainting;Image Completion","Training;Image synthesis;Image color analysis;Dams;Signal processing;Generative adversarial networks;Feature extraction","image colour analysis;image resolution;image restoration;image texture;neural nets","DAM-GAN;dynamic attention map;fake texture detection;deep neural advancements;remarkable image synthesis performance;generative adversarial networks;high-quality image reconstruction;GAN-based networks;pixel artifacts;color inconsistency;pixel inconsistency disorder;products dynamic attention maps;image inpainting approaches","","","","23","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Towards the Efficiency of the Fusion Step in Language-Based Fashion Image Editing","M. Darvish; J. Shanbehzadeh; A. Mansouri","Department of Electrical and Computer Engineering, Faculty of Engineering, Kharazmi University, Tehran, Iran; Department of Electrical and Computer Engineering, Faculty of Engineering, Kharazmi University, Tehran, Iran; Department of Electrical and Computer Engineering, Faculty of Engineering, Kharazmi University, Tehran, Iran","2022 27th International Computer Conference, Computer Society of Iran (CSICC)","27 May 2022","2022","","","1","5","Text-to-image synthesis is a new research field of image generation. The generated images must be based on textual descriptions and of acceptable quality. Among the generative models, Generative Adversarial Networks (GANs) can generate higher quality images compared to other models. Most GAN-based text-to-image generation methods use simple datasets, including flower and bird images. For more complex datasets (such as the Fashion Synthesis dataset used in this paper), the issue of generating an image from a text becomes more challenging, as the images in this dataset are much richer in content than just the flower images or birds. Most of the methods proposed so far are only able to recognize the color of an object based on the text description and have difficulty in accurately identifying the location in the image where these changes are to be made. One way to solve this problem is to generate an image from the text based on editing this image. In this paper, the methods of combining text and image features in the generation of images are discussed and the effect of different fusing models on the process in terms of quality and accuracy of the description is investigated.","","978-1-6654-8027-7","10.1109/CSICC55295.2022.9780492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780492","GAN;Text-to-image;Fashion synthesis","Image recognition;Costs;Text recognition;Image synthesis;Image color analysis;Birds;Generative adversarial networks","feature extraction;image colour analysis;text analysis","language-based Fashion image editing;text-to-image synthesis;acceptable quality;generative models;Generative Adversarial Networks;higher quality images;GAN-based text-to-image generation methods;simple datasets;bird images;complex datasets;Fashion Synthesis dataset;flower images;text description;image features","","","","15","IEEE","27 May 2022","","","IEEE","IEEE Conferences"
"Abnormal Colon Polyp Image Synthesis Using Conditional Adversarial Networks for Improved Detection Performance","Y. Shin; H. A. Qadir; I. Balasingham","Intervention Centre, Oslo University Hospital, Oslo, Norway; Intervention Centre, Oslo University Hospital, Oslo, Norway; Intervention Centre, Oslo University Hospital, Oslo, Norway","IEEE Access","21 Oct 2018","2018","6","","56007","56017","One of the major obstacles in automatic polyp detection during colonoscopy is the lack of labeled polyp training images. In this paper, we propose a framework of conditional adversarial networks to increase the number of training samples by generating synthetic polyp images. Using a normal binary form of polyp mask which represents only the polyp position as an input conditioned image, realistic polyp image generation is a difficult task in a generative adversarial networks approach. We propose an edge filtering-based combined input conditioned image to train our proposed networks. This enables realistic polyp image generations while maintaining the original structures of the colonoscopy image frames. More importantly, our proposed framework generates synthetic polyp images from normal colonoscopy images which have the advantage of being relatively easy to obtain. The network architecture is based on the use of multiple dilated convolutions in each encoding part of our generator network to consider large receptive fields and avoid much contractions of a feature map size. An image resizing with convolution for upsampling in the decoding layers is considered to prevent artifacts on generated images. We show that the generated polyp images are not only qualitatively realistic, but also help to improve polyp detection performance.","2169-3536","","10.1109/ACCESS.2018.2872717","Norges Forskningsråd(grant numbers:225885/O70); Norges Forskningsråd(grant numbers:271542/O30); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478237","Colonoscopy;convolutional neural network;dilated convolution;generative adversarial networks;polyp detection","Generators;Generative adversarial networks;Gallium nitride;Colonoscopy;Image generation;Training;Decoding","biological organs;biomedical optical imaging;image filtering;medical image processing","abnormal colon polyp image synthesis;conditional adversarial networks;automatic polyp detection;synthetic polyp images;polyp mask;polyp position;generative adversarial networks approach;edge filtering-based combined input;colonoscopy image frames;generator network;polyp detection performance;polyp training images;polyp image generation","","49","","39","OAPA","30 Sep 2018","","","IEEE","IEEE Journals"
"DGattGAN: Cooperative Up-Sampling Based Dual Generator Attentional GAN on Text-to-Image Synthesis","H. Zhang; H. Zhu; S. Yang; W. Li","School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China; Department of Mathematics, Natural, Mathematical & Engineering Sciences, King’s College London, London, U.K; School of Information Science and Engineering, East China University of Science and Technology, Shanghai, China","IEEE Access","22 Feb 2021","2021","9","","29584","29598","Text-to-image synthesis task aims at generating images consistent with input text descriptions and is well developed by the Generative Adversarial Network (GAN). Although GAN based image generation approaches have achieved promising results, synthesizing quality is sometimes unsatisfied due to discursive generation of background and object. In this article, we propose a cooperative up-sampling based Dual Generator attentional GAN (DGattGAN) to generate high-quality images from text description. To achieve this, two generators with individual generation purpose are established to decouple object and background generation. In particular, we introduce a cooperative up-sampling mechanism to build cooperation between object and background generators during training. This strategy is potentially very useful as any dual generator architecture in GAN models can benefit from this mechanism. Furthermore, we propose an asymmetric information feeding scheme to distinguish two synthesis tasks, such that each generator only synthesizes based on semantic information they accept. Taking advantage of effective dual generator, the attention mechanism we incorporated on object generator could devote to fine-grained details generation on actual targeted objects. Experiments on Caltech-UCSD Bird (CUB) and Oxford-102 datasets suggest that generated images by the proposed model are more realistic and consistent with input text, and DGattGAN is competent compared to state-of-the-art methods according to Inception Score (IS) and R-precision metrics. Our codes are available at: https://github.com/ecfish/DGattGAN.","2169-3536","","10.1109/ACCESS.2021.3058674","National Nature Science Foundation of China(grant numbers:61872143); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9352788","Asymmetric information feeding;cooperative up-sampling;dual generator;generative adversarial networks;text-to-image synthesis","Generators;Image resolution;Generative adversarial networks;Gallium nitride;Task analysis;Image synthesis;Visualization","neural nets;object detection;text analysis","object generator;fine-grained details generation;DGattGAN;text-to-image synthesis task;discursive generation;text description;background generation;dual generator architecture;GAN models;dual generator attentional GAN;generative adversarial network;GAN based image generation;cooperative up-sampling;Caltech-UCSD Bird;Oxford-102 datasets","","6","","34","CCBY","11 Feb 2021","","","IEEE","IEEE Journals"
"Pose-Guided Person Image Synthesis for Data Augmentation in Pedestrian Detection","R. Zhi; Z. Guo; W. Zhang; B. Wang; V. Kaiser; J. Wiederer; F. B. Flohr","Department of Automated Driving System, Mercedes-Benz Research & Development Center, Daimler Greater China, Beijing, China; Department of Automated Driving System, Mercedes-Benz Research & Development Center, Daimler Greater China, Beijing, China; Department of Automated Driving System, Mercedes-Benz Research & Development Center, Daimler Greater China, Beijing, China; Department of Automated Driving System, Mercedes-Benz Research & Development Center, Daimler Greater China, Beijing, China; Environment Perception Department, Daimler AG, Stuttgart-Vaihingen, Germany; Environment Perception Department, Daimler AG, Stuttgart-Vaihingen, Germany; Environment Perception Department, Daimler AG, Stuttgart-Vaihingen, Germany","2021 IEEE Intelligent Vehicles Symposium (IV)","1 Nov 2021","2021","","","1493","1500","In this paper, we present a data augmentation framework for pedestrian detection using a pose-guided person image synthesis model. The proposed framework can boost the performance of state-of-the-art pedestrian detectors by generating new and unseen pedestrian training samples with controllable appearances and poses. This is achieved by a new latent-consistent adversarial variational auto-encoder (LAVAE) model, leveraging the advantages of conditional variational auto-encoders and conditional generative adversarial networks to disengage and reconstruct person images conditioned on target poses. An additional latent regression path is introduced to preserve appearance information and to guarantee a spatial alignment during transfer. LAVAE goes beyond existing works in restoring structural information and perceptual details with limited annotations and can further benefit the pedestrian detection task in automated driving scenarios. Extensive pedestrian detection and person image synthesis experiments are performed on the EuroCity Person dataset. We show that data augmentation using LAVAE improves the accuracy of state-of-the-art pedestrian detectors significantly. Furthermore, a competitive performance can be observed when we compare LAVAE with other generative models for person image synthesis.","","978-1-7281-5394-0","10.1109/IV48863.2021.9575574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9575574","","Training;Image synthesis;Intelligent vehicles;Annotations;Detectors;Generative adversarial networks;Data models","image classification;learning (artificial intelligence);object detection;pedestrians;pose estimation","data augmentation framework;pose-guided person image synthesis model;state-of-the-art pedestrian detectors;unseen pedestrian training samples;controllable appearances;latent-consistent adversarial variational auto-encoder model;LAVAE;conditional variational auto-encoders;conditional generative adversarial networks;person images;target poses;additional latent regression path;appearance information;pedestrian detection task;extensive pedestrian detection;person image synthesis experiments;EuroCity Person dataset;generative models","","","","49","IEEE","1 Nov 2021","","","IEEE","IEEE Conferences"
"Attentive Generative Adversarial Network To Bridge Multi-Domain Gap For Image Synthesis","M. Wang; C. Lang; L. Liang; G. Lyu; S. Feng; T. Wang","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China","2020 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2020","2020","","","1","6","Despite the significant progress on text-to-image synthesis, automatically generating realistic images remains a challenging task since the location and specific shape of object are not given in the text descriptions. To address these problems, we propose a novel attentive generative adversarial network with contextual loss (AGAN-CL) algorithm. More specifically, the generative network consists of two sub-networks: a contextual network for generating image contours, and a cycle transformation autoencoder for converting contours to realistic images. Our core idea is the injection of image contours into the generative network, which is the most critical part of our network, since it will guide the whole generative network to focus on object regions. In addition, we also apply contextual loss and cycle-consistent loss to bridge multi-domain gap. Comprehensive results on several challenging datasets demonstrate the advantage of the proposed method over the leading approaches, regarding both visual fidelity and alignment with input descriptions.","1945-788X","978-1-7281-1331-9","10.1109/ICME46284.2020.9102761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102761","text-to-image synthesis;attentive generative adversarial network;contextual loss;image contours","Generative adversarial networks;Image generation;Gallium nitride;Feature extraction;Generators;Shape;Loss measurement","image segmentation;neural nets;object detection;realistic images","visual fidelity;AGAN-CL;multidomain gap;attentive generative adversarial network;text-to-image synthesis;contextual loss;realistic images;image contours","","5","","23","IEEE","9 Jun 2020","","","IEEE","IEEE Conferences"
"KnHiGAN: Knowledge-enhanced Hierarchical Generative Adversarial Network for Fine-grained Text-to-Image Synthesis","N. Ge; Y. Zhu; X. Xiong; B. Zheng; J. Huang","Shanghai Film Academy, Shanghai University, Shanghai, China; Shanghai Film Academy, Shanghai University, Shanghai, China; Shanghai Film Academy, Shanghai University, Shanghai, China; Shanghai Film Academy, Shanghai University, Shanghai, China; Shanghai Film Academy, Shanghai University, Shanghai, China","2021 14th International Symposium on Computational Intelligence and Design (ISCID)","20 Jan 2022","2021","","","357","360","To generate fine-grained images with greater authenticity, in this paper, we propose a Knowledge-enhanced Hierarchical Generative Adversarial Network (KnHiGAN) for text-to-image synthesis. KnHiGAN sets up a Knowledge Enhancement Module to expand conditions for the limited text descriptions by combining with the knowledge graph, as a result, it can provide richer fine-grained details to the generative network. Moreover, a Hierarchical Generative Adversarial Network is designed to generate the foreground and background separately, and the two are integrated together to composite the final result. Experiments on CUB-200 and Oxford-102 datasets show that our KnHiGAN can not only generate the fine-grained images which are more like those that exist in the real world, but also can maintain a high degree of consistency with the original text input.","2473-3547","978-1-6654-3856-8","10.1109/ISCID52796.2021.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9679242","Text-to-image synthesis;Knowledge graph;Hierarchical structure;Generative Adversarial Network","Knowledge engineering;Fuses;Generative adversarial networks;Generators;Task analysis;Computational intelligence","image processing;neural nets;semantic networks","KnHiGAN;knowledge-enhanced hierarchical generative adversarial network;fine-grained text-to-image synthesis;knowledge graph;fine-grained details;knowledge enhancement module;text descriptions;Oxford-102 datasets;CUB-200 datasets","","","","22","IEEE","20 Jan 2022","","","IEEE","IEEE Conferences"
"Disentangled Facial Expressions Editing in Trained Latent Space","W. S. S. Khine; P. Siritanawan; K. Kotani","School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan; School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan; School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan","2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","18 Nov 2022","2022","","","1700","1706","In recent years, Generative Adversarial Networks (GANs) have gained attention in image synthesis mapping from the latent space onto image space. Trained latent space carries the visual semantics for generated images. Past studies observed that arithmetic operation and linear interpolation in latent space could change the visible facial attributes, such as beards and glasses, in image space. In this work, the visual concepts in the latent space are observed, allowing to change the emotion attribute per facial expressions in the image space. We observed interpolation of a sample while disentangling the emotional attributes to edit the emotion-related facial expressions in the synthesized images. For the experiment, the Deep Convolution Generative Adversarial Networks (DCGANs) are utilized for image synthesis, and Extended Cohn Kanade (CK +) facial expression dataset is applied as the input. Our results showed that manipulating the latent space of the well-trained GANs can edit the emotional aspects of the image space. Moreover, editing facial expressions in the latent space is helpful for the recognition task to improve accuracy. Empirical results showed that the facial expressions classifier improved its performance in the recognition sadness class from 20% to 80% on the imbalance dataset.","2577-1655","978-1-6654-5258-8","10.1109/SMC53654.2022.9945408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945408","Linear Interpolation;Disentanglement of emotional attribute;Deep Convolution Generative Adversarial Networks (DCGANs)","Visualization;Interpolation;Image synthesis;Convolution;Shape;Face recognition;Semantics","convolutional neural nets;deep learning (artificial intelligence);emotion recognition;face recognition;image classification;interpolation","arithmetic operation;DCGAN;deep convolution generative adversarial networks;emotion-related facial expressions;emotional attributes;extended Cohn Kanade facial expression dataset;facial expressions classifier;facial expressions editing;image space;image synthesis mapping;linear interpolation;trained latent space;visible facial attributes","","","","15","IEEE","18 Nov 2022","","","IEEE","IEEE Conferences"
"Exploring the Potential of Unsupervised Image Synthesis for SAR-Optical Image Matching","W. -L. Du; Y. Zhou; J. Zhao; X. Tian; Z. Yang; F. Bian","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of the People's Republic of China, Xuzhou, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau; DFH Satellite Company Ltd., Beijing, China; DFH Satellite Company Ltd., Beijing, China","IEEE Access","18 May 2021","2021","9","","71022","71033","We consider SAR-optical image matching problems, where correspondences are acquired from a pair of SAR and optical images. Recent methods for such a problem typically simplify the SAR-optical image matching to the SAR-SAR or optical-optical image matchings using supervised-image-synthesis methods. However, training supervised-image-synthesis needs plenty of aligned SAR-optical image pairs while gathering sufficient amounts of aligned multi-modal image pairs is challenging in remote sensing. In this work, we investigate the applicability of unsupervised-image-synthesis for SAR-optical image matching such that the unaligned SAR-optical images could be used. To this end, we apply feature matching loss to a well known unsupervised-image-synthesis method, i.e., CycleGAN, to enforce the feature matching consistency. Moreover, we develop a shared-matching-strategy to improve the results of SAR-optical image matching further. Qualitative comparisons against CycleGAN, StarGAN, and DualGAN demonstrate the superiority of our approach. Quantitative results show that, compared with CycleGAN, StarGAN, and DualGAN, our method obtains at least 2.6 times more qualified SAR-optical matchings.","2169-3536","","10.1109/ACCESS.2021.3079327","National Natural Science Foundation of China(grant numbers:62002360,61806206,61772530); opening fund of State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology (Macau FDCT)(grant numbers:119/2017/A3); Science and Technology Development Fund of Macau(grant numbers:0038/2020/A1); Fundamental Research Funds for the Central Universities(grant numbers:2020ZDPY0305); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201346,BK20180639); Six Talent Peaks Project in Jiangsu Province(grant numbers:2015-DZXX-010,2018-XYDXX-044); China Postdoctoral Science Foundation(grant numbers:2020M681765); Jiangsu Province Postdoctoral Research Foundation(grant numbers:2020Z178); Xuzhou Science and Technology Program(grant numbers:KC18061); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427486","Image matching;unsupervised-image-synthesis;synthetic aperture radar (SAR);generative adversarial networks (GANs)","Optical imaging;Image matching;Adaptive optics;Optical sensors;Radar polarimetry;Nonlinear optics;Image synthesis","geophysical image processing;image matching;image registration;optical images;radar imaging;remote sensing;synthetic aperture radar","unsupervised image synthesis;SAR-optical image matching problems;SAR-SAR;optical-optical image matchings;supervised-image-synthesis methods;training supervised-image-synthesis;aligned SAR-optical image pairs;aligned multimodal image pairs;unaligned SAR-optical images;known unsupervised-image-synthesis method;2.6 times more qualified SAR-optical matchings","","5","","57","CCBY","11 May 2021","","","IEEE","IEEE Journals"
"Cycle-Consistent Diverse Image Synthesis from Natural Language","Z. Chen; Y. Luo","School of Information Technology and Electrical Engineering, The University of Queensland; School of Information Technology and Electrical Engineering, The University of Queensland","2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","15 Aug 2019","2019","","","459","464","Text-to-image translation has become an attractive yet challenging task in computer vision. Previous approaches tend to generate similar, or even monotonous, images for distinctive texts and overlook the characteristics of specific sentences. In this paper, we aim to generate images from the given texts by preserving diverse appearances and modes of the objects or instances contained. To achieve that, a novel learning model named SuperGAN is proposed, which consists of two major components: an image synthesis network and a captioning model in a Cycle-GAN framework. SuperGAN adopts the cycle-consistent adversarial training strategy to learn an image generator where the feature distribution of the generated images complies with the distribution of the generic images. Meanwhile, a cycle-consistency loss is applied to constrain that the caption of the generated images is closed to the original texts. Extensive experiments on the benchmark dataset Oxford-flowers-102 demonstrate the validity and effectiveness of our proposed method. In addition, a new evaluation metric is proposed to measure the diversity of synthetic results.","","978-1-5386-9214-1","10.1109/ICMEW.2019.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795022","Image synthesis;Image captioning;generative adversarial networks;cycle-consistency loss","","computer vision;feature extraction;image annotation;learning (artificial intelligence);natural language processing;text analysis","text-to-image translation;computer vision;distinctive texts;SuperGAN;image synthesis network;cycle-consistent adversarial training strategy;image generator;learning model;image captioning model;cycle-GAN framework;cycle-consistent diverse image synthesis;oxford-flowers-102 dataset;natural language","","4","","17","IEEE","15 Aug 2019","","","IEEE","IEEE Conferences"
"Crack Image Synthesis and Segmentation using Paired Image Translation","A. Sekar; V. Perumal","Department of Computer Technology, Anna University, Chennai, India; Department of Computer Technology, Anna University, Chennai, India","2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)","15 Apr 2022","2022","","","1","7","We introduce a novel method for intelligent transportation systems to investigate the synthesising of crack images and crack image segmentation. Our model proposes Image-to- Image paired translation GAN for crack image synthesizing and U network with skip connections for segmentation. To achieve synthesis of crack images, we trained a paired imageto image translation model using generative adversarial network associated with perceptual loss function and for segmentation U-network is designed with skip connections. We conduct an extensive experiments and results for the generated crack images with ground truth images and also for crack region segmentation. The obtained result indicates that our proposed model generate more similar image and also segment crack regions accurately when compared with various existing approaches.","","978-1-6654-9529-5","10.1109/ACCAI53970.2022.9752524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9752524","Crack image generation;Crack image segmentation;Paired image translation;U-network model","Measurement;Image segmentation;Analytical models;Image synthesis;Computational modeling;Roads;Generative adversarial networks","cracks;image segmentation;intelligent transportation systems;neural nets","intelligent transportation systems;crack image segmentation;generative adversarial network;U-network;ground truth images;crack region segmentation;segment crack regions;crack image synthesis;image-to-image paired translation;GAN","","","","28","IEEE","15 Apr 2022","","","IEEE","IEEE Conferences"
"Domain-Adaptive generative adversarial networks for sketch-to-photo inversion","Y. -C. Liu; W. -C. Chiu; S. -D. Wang; Y. -C. F. Wang","Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan","2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP)","7 Dec 2017","2017","","","1","6","Generating photo-realistic images from multiple style sketches is one of challenging tasks in image synthesis with important applications such as facial composite for suspects. While machine learning techniques have been applied for solving this problem, the requirement of collecting sketch and face photo image pairs would limit the use of the learned model for rendering sketches of different styles. In this paper, we propose a novel deep learning model of Domain-adaptive Generative Adversarial Networks (DA-GAN). The design of DA-GAN performs cross-style sketch-to-photo inversion, which mitigates the difference across input sketch styles without the need to collect a large number of sketch and face image pairs for training purposes. In experiments, we show that our method is able to produce satisfactory results as well as performing favorably against state-of-the-art approaches.","","978-1-5090-6341-3","10.1109/MLSP.2017.8168181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8168181","Image Inversion;Deep Learning;Convolutional Neural Network;Generative Adversarial Network","Training;Gallium nitride;Face;Image generation;Machine learning;Training data","face recognition;learning (artificial intelligence);realistic images;rendering (computer graphics)","sketch-to-photo inversion;Domain-Adaptive generative adversarial networks;photo-realistic images;image synthesis;face photo image pairs;deep learning model;sketch styles;machine learning;sketch rendering","","5","","15","IEEE","7 Dec 2017","","","IEEE","IEEE Conferences"
"A Realistic Image Generation of Face From Text Description Using the Fully Trained Generative Adversarial Networks","M. Z. Khan; S. Jabeen; M. U. G. Khan; T. Saba; A. Rehmat; A. Rehman; U. Tariq","Alkhawarizmi Institute of Computer Sciences, UET Lahore, Lahore, Pakistan; Alkhawarizmi Institute of Computer Sciences, UET Lahore, Lahore, Pakistan; Department of Computer Science and Engineering, UET Lahore, Lahore, Pakistan; Artificial Intelligence and Data Analytics Lab, CCIS Prince Sultan University, Riyadh, Saudi Arabia; Department of Computer Science and Engineering, UET Lahore, Lahore, Pakistan; Artificial Intelligence and Data Analytics Lab, CCIS Prince Sultan University, Riyadh, Saudi Arabia; College of Computer Engineering and Science, Prince Sattam bin Abdulaziz University, Alkharj, Saudi Arabia","IEEE Access","5 Jan 2021","2021","9","","1250","1260","Text to face generation is a sub-domain of text to image synthesis. It has a huge impact on new research areas along with the wide range of applications in the public safety domain. Due to the lack of dataset, the research work focused on the text to face generation is very limited. Most of the work for text to face generation until now is based on the partially trained generative adversarial networks, in which the pre-trained text encoder has been used to extract the semantic features of the input sentence. Later, these semantic features have been utilized to train the image decoder. In this research work, we propose a fully trained generative adversarial network to generate realistic and natural images. The proposed work trained the text encoder as well as the image decoder at the same time to generate more accurate and efficient results. In addition to the proposed methodology, another contribution is to generate the dataset by the amalgamation of LFW, CelebA and locally prepared dataset. The dataset has also been labeled according to our defined classes. Through performing different kinds of experiments, it has been proved that our proposed fully trained GAN outperformed by generating good quality images by the input sentence. Moreover, the visual results have also strengthened our experiments by generating the face images according to the given query.","2169-3536","","10.1109/ACCESS.2020.3015656","National Center of Artificial Intelligence, Pakistan; Artificial Intelligence and Data Analytics Lab, Prince Sultan University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9163356","GAN;CNN;text to face;image generation;face synthesis;data augmentation;legal identity for all","Face;Image generation;Generative adversarial networks;Task analysis;Decoding;Gallium nitride;Semantics","face recognition;feature extraction;image coding;learning (artificial intelligence);text analysis","realistic image generation;text description;fully trained generative adversarial network;image synthesis;public safety domain;partially trained generative adversarial networks;pre-trained text encoder;semantic features;image decoder;realistic images;natural images;fully trained GAN;good quality images;face images","","22","","36","CCBY","10 Aug 2020","","","IEEE","IEEE Journals"
"Towards Generating Remote Sensing Images of the Far Past","M. B. Bejiga; F. Melgani","Department of Information Engineering and Computer science, University of Trento, Trento, Italy; Department of Information Engineering and Computer science, University of Trento, Trento, Italy","IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium","14 Nov 2019","2019","","","9502","9505","Text-to-image synthesis is a research topic that has not yet been addressed by the remote sensing community. It consists in learning a mapping from text description to image pixels. In this paper, we propose to address this topic for the very first time. More specifically, our objective is to convert ancient text descriptions of geographic areas written by past explorers into an equivalent remote sensing image. To this effect, we rely on generative adversarial networks (GANs) to learn the mapping. GANs aim to represent the distribution of a dataset using weights of a deep neural network, which are trained as an adversarial competition between two networks. We collected ancient texts dating back to 7 BC to train our network and obtained interesting results, which form the basis to highlight future research directions to advance this new topic.","2153-7003","978-1-5386-9154-0","10.1109/IGARSS.2019.8899834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8899834","GANs;text-to-image synthesis;remote sensing","Remote sensing;Generators;Generative adversarial networks;Training;Gallium nitride;Cost function;Mathematical model","geophysical image processing;learning (artificial intelligence);neural nets;remote sensing;text analysis;text detection","text-to-image synthesis;text description;image pixels;generative adversarial networks;GAN;deep neural network;remote sensing image generation","","","","18","IEEE","14 Nov 2019","","","IEEE","IEEE Conferences"
"Hierarchically-Fused Generative Adversarial Network for Text to Realistic Image Synthesis","X. Huang; M. Wang; M. Gong","Department of Computer Science, Memorial University of Newfoundland, St. John’s, Canada; Department of Computer Science, Memorial University of Newfoundland, St. John’s, Canada; Department of Computer Science, Memorial University of Newfoundland, St. John’s, Canada","2019 16th Conference on Computer and Robot Vision (CRV)","1 Aug 2019","2019","","","73","80","In this paper, we present a novel Hierarchically-fused Generative Adversarial Network (HfGAN) for synthesizing realistic images from text descriptions. While existing approaches on this topic have achieved impressive success, to generate 256×256 images from captions, they commonly resort to coarse-to-fine scheme and associate multiple discriminators in different stages of the networks. Such a strategy is both inefficient and prone to artifacts. Motivated by the above findings, we propose an end-to-end network that can generate 256×256 photo-realistic images with only one discriminator. We fully exploit the hierarchical information from different layers and directly generate the fine-scale images by adaptively fusing features from multi-hierarchical layers. We quantitatively evaluate the synthesized images with Inception Score, Visual-semantic Similarity and average training time on the CUB birds, Oxford-102 flowers, and COCO datasets. The results show that our model is more efficient and noticeably outperforms the previous state-of-the-art methods.","","978-1-7281-1838-3","10.1109/CRV.2019.00018","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8781630","Generative Adversarial Networks;text-to-image synthesis;hierarchical features fusion","Generative adversarial networks;Generators;Feature extraction;Training;Image resolution;Image synthesis;Birds","feature extraction;neural nets;text analysis","text descriptions;associate multiple discriminators;end-to-end network;hierarchical information;fine-scale images;multihierarchical layers;synthesized images;realistic image synthesis;photo-realistic images;hierarchically-fused generative adversarial network","","6","","29","IEEE","1 Aug 2019","","","IEEE","IEEE Conferences"
"Photorealism in Driving Simulations: Blending Generative Adversarial Image Synthesis With Rendering","E. Yurtsever; D. Yang; I. M. Koc; K. A. Redmill","Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA; Chongqing Changan Automobile Company Ltd., Chongqing, China; Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA; Department of Electrical and Computer Engineering, The Ohio State University, Columbus, OH, USA","IEEE Transactions on Intelligent Transportation Systems","6 Dec 2022","2022","23","12","23114","23123","Driving simulators play a large role in developing and testing new intelligent vehicle systems. The visual fidelity of the simulation is critical for building vision-based algorithms and conducting human driver experiments. Low visual fidelity breaks immersion for human-in-the-loop driving experiments. Conventional computer graphics pipelines use detailed 3D models, meshes, textures, and rendering engines to generate 2D images from 3D scenes. These processes are labor-intensive, and they do not generate photorealistic imagery. Here we introduce a hybrid generative neural graphics pipeline for improving the visual fidelity of driving simulations. Given a 3D scene, we partially-render only important objects of interest, such as vehicles, and use generative adversarial processes to synthesize the background and the rest of the image. To this end, we propose a novel image formation strategy to form 2D semantic images from 3D scenery consisting of simple object models without textures. These semantic images are then converted into photorealistic RGB images with a state-of-the-art Generative Adversarial Network (GAN) trained on real-world driving scenes. This replaces repetitiveness with randomly generated but photorealistic surfaces. Finally, the partially-rendered and GAN synthesized images are blended with a blending GAN. We show that the photorealism of images generated with the proposed method is more similar to real-world driving datasets such as Cityscapes and KITTI than conventional approaches. This comparison is made using semantic retention analysis and Frechet Inception Distance (FID) measurements.","1558-0016","","10.1109/TITS.2022.3193347","United States Department of Transportation (Mobility21 University Transportation Center)(grant numbers:69A3551747111); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9843916","Driving simulation;deep learning;generative adversarial networks;image synthesis","Three-dimensional displays;Semantics;Solid modeling;Rendering (computer graphics);Visualization;Generative adversarial networks;Image synthesis","computer vision;image colour analysis;image texture;intelligent transportation systems;pipeline processing;rendering (computer graphics)","2D semantic images;3D scenes;blending GAN;blending generative adversarial image synthesis;conventional computer graphics pipelines;driving simulations;driving simulators;generative adversarial network;generative adversarial processes;human driver experiments;human-in-the-loop driving experiments;hybrid generative neural graphics pipeline;image formation strategy;intelligent vehicle systems;low visual fidelity breaks;photorealistic RGB images;randomly generated but photorealistic surfaces;real-world driving datasets;real-world driving scenes;semantic images;simple object models;vision-based algorithms","","","","54","IEEE","27 Jul 2022","","","IEEE","IEEE Journals"
"Modality Disentangled Discriminator for Text-to-Image Synthesis","F. Feng; T. Niu; R. Li; X. Wang","School of Artificial Intelligence, Beijing University of Posts and Communications, and the Engineering Research Center of Information Networks, Ministry of Education, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Communications, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Communications, and the Engineering Research Center of Information Networks, Ministry of Education, Beijing, China; School of Artificial Intelligence, Beijing University of Posts and Communications, and the Engineering Research Center of Information Networks, Ministry of Education, Beijing, China","IEEE Transactions on Multimedia","6 Apr 2022","2022","24","","2112","2124","Text-to-image (T2I) synthesis aims at generating photo-realistic images from text descriptions, which is a particularly important task in bridging vision and language. Each generated image consists of two parts: the content part related to the text and the style part irrelevant to the text. The existing discriminator does not distinguish between the content part and the style part. This not only precludes the T2I synthesis models from generating the content part effectively but also makes it difficult to manipulate the style of the generated image. In this paper, we propose a modality disentangled discriminator that distinguishes between the content part and the style part at a specific layer. Specifically, we enforce the early layers of a certain number in the discriminator to become the disentangled representation extractor through two losses. The extracted common representation for the content part can make the discriminator more effective for capturing the text-image correlation, while the extracted modality-specific representation for the style part can be directly transferred to other images. The combination of these two representations can also improve the quality of the generated images. Our proposed discriminator is used to substitute the discriminator of each stage in the representative model AttnGAN and the SOTA model DM-GAN. Extensive experiments are conducted on three widely used datasets, i.e. CUB, Oxford-102, and COCO, for the T2I synthesis task, demonstrating the superior performance of the modality disentangled discriminator over the base models. Code for DM-GAN with our modality disentangled discriminator is available at https://github.com/FangxiangFeng/DM-GAN-MDD.","1941-0077","","10.1109/TMM.2021.3075997","National Key Research, and Development Program of China(grant numbers:2020YFF0305302); National Natural Science Foundation of China(grant numbers:61906018,62076032); Fundamental Research Funds for the Central Universities(grant numbers:2021RC36); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9417738","text-to-image synthesis;generative adversarial networks;multi-modal disentangled representation learning","Task analysis;Correlation;Image synthesis;Image reconstruction;Generative adversarial networks;Image representation;Visualization","computer vision;feature extraction;realistic images;rendering (computer graphics);text analysis","text-to-image synthesis;photo-realistic images;text descriptions;content part;style part;text-image correlation;extracted modality-specific representation;T2I;AttnGAN;SOTA model DM-GAN;CUB;Oxford-102;COCO","","","","62","IEEE","28 Apr 2021","","","IEEE","IEEE Journals"
"Exploring Global and Local Linguistic Representations for Text-to-Image Synthesis","R. Li; N. Wang; F. Feng; G. Zhang; X. Wang","School of Computer Science, and the Engineering Research Center of Information Networks, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Institute of Network Technology, and the Engineering Research Center of Information Networks, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China; School of Computer Science, and the Engineering Research Center of Information Networks, Ministry of Education, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Transactions on Multimedia","18 Nov 2020","2020","22","12","3075","3087","The task of text-to-image synthesis is to generate photographic images conditioned on given textual descriptions. This challenging task has recently attracted considerable attention from the multimedia community due to its potential applications. Most of the up-to-date approaches are built based on generative adversarial network (GAN) models, and they synthesize images conditioned on the global linguistic representation. However, the sparsity of the global representation results in training difficulties on GANs and a shortage of fine-grained information in the generated images. To address this problem, we propose cross-modal global and local linguistic representations-based generative adversarial networks (CGL-GAN) by incorporating the local linguistic representation into the GAN. In our CGL-GAN, we construct a generator to synthesize the target images and a discriminator to judge whether the generated images conform with the text description. In the discriminator, we construct the cross-modal correlation by projecting the image representations at high and low levels onto the global and local linguistic representations, respectively. We design the hinge loss function to train our CGL-GAN model. We evaluate the proposed CGL-GAN on two publicly available datasets, the CUB and the MS-COCO. The extensive experiments demonstrate that incorporating fine-grained local linguistic information with cross-modal correlation can greatly improve the performance of text-to-image synthesis, even when generating high-resolution images.","1941-0077","","10.1109/TMM.2020.2972856","National Key R&D Program of China(grant numbers:2019YFF0303300); Subject II(grant numbers:2019YFF0303302); National Natural Science Foundation of China(grant numbers:61802026,61906018); Science and Technology Program of the Headquarters of State Grid Corporation of China(grant numbers:5200-201918255A-0-0-00); Higher Education Discipline Innovation Project(grant numbers:B08004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8989803","Text-to-image synthesis;generative adversarial network (GAN);linguistic representation;cross-modal","Gallium nitride;Task analysis;Linguistics;Generators;Generative adversarial networks;Training data;Correlation","computational linguistics;computer vision;image representation;learning (artificial intelligence);neural nets;text analysis","CGL-GAN model;fine-grained local linguistic information;cross-modal correlation;text-to-image synthesis;high-resolution images;photographic images;textual descriptions;generative adversarial network models;local linguistic representations-based generative adversarial networks;target images;text description;image representations;multimedia community;synthesize images;cross-modal global linguistic representation;CUB;MS-COCO","","12","","42","IEEE","10 Feb 2020","","","IEEE","IEEE Journals"
"Conditional Convolution Projecting Latent Vectors on Condition-Specific Space","M. -C. Sagong; Y. -J. Yeo; Y. -G. Shin; S. -J. Ko","Department of Electrical Engineering, Korea University, Seoul 02841, South Korea.; Department of Electrical Engineering, Korea University, Seoul 02841, South Korea.; Division of Smart Interdisciplinary Engineering, Hannam University, Daejeon 34430, South Korea.; Department of Electrical Engineering, Korea University, Seoul 02841, South Korea.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","8","Despite rapid advancements over the past several years, the conditional generative adversarial networks (cGANs) are still far from being perfect. Although one of the major concerns of the cGANs is how to provide the conditional information to the generator, there are not only no ways considered as the optimal solution but also a lack of related research. This brief presents a novel convolution layer, called the conditional convolution (cConv) layer, which incorporates the conditional information into the generator of the generative adversarial networks (GANs). Unlike the most general framework of the cGANs using the conditional batch normalization (cBN) that transforms the normalized feature maps after convolution, the proposed method directly produces conditional features by adjusting the convolutional kernels depending on the conditions. More specifically, in each cConv layer, the weights are conditioned in a simple but effective way through filter-wise scaling and channel-wise shifting operations. In contrast to the conventional methods, the proposed method with a single generator can effectively handle condition-specific characteristics. The experimental results on CIFAR, LSUN, and ImageNet datasets show that the generator with the proposed cConv layer achieves a higher quality of conditional image generation than that with the standard convolution layer.","2162-2388","","10.1109/TNNLS.2022.3172512","Institute of Information and Communications Technology Promotion IITP MSIT Korean Government development of global multitarget tracking and event prediction techniques based on real-time large-scale video analysis(grant numbers:2014-3-00077); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9777844","Conditional image generation;deep learning;generative adversarial networks (GANs).","Generators;Convolution;Standards;Image synthesis;Generative adversarial networks;Learning systems;Visualization","","","","1","","","IEEE","18 May 2022","","","IEEE","IEEE Early Access Articles"
"Side-Scan Sonar Image Synthesis Based on Generative Adversarial Network for Images in Multiple Frequencies","Y. Jiang; B. Ku; W. Kim; H. Ko","School of Electrical Engineering, Korea University, Seoul, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea; Agency for Defense Development, Jinhae, South Korea; School of Electrical Engineering, Korea University, Seoul, South Korea","IEEE Geoscience and Remote Sensing Letters","27 Aug 2021","2021","18","9","1505","1509","The side-scan sonar (SSS) is a critical sensor device used to explore underwater environments in the deep sea. Gathering SSS data, however, is an expensive and time-consuming task because it requires sensor towing and involves complicated field operations. Recently, deep learning has been making advances rapidly in the field of computer vision. Benefiting from this development, generative adversarial networks (GANs) have been demonstrated to produce realistic synthetic data of various types, including images and acoustics signals. In this letter, we propose a GAN-based semantic image synthesis model based on GAN that can generate high-quality SSS images at a low cost in less time. We evaluate the proposed model using both shallow and deep water SSS data sets that include a diverse range of imaging conditions. such as high and low sonar operating frequencies and different landscapes. The experimental results show that the proposed method can effectively generate synthesized SSS data characterized by the shape and style of real data, thereby demonstrating its promising potential for SSS data augmentation in diverse SSS relevant machine learning tasks.","1558-0571","","10.1109/LGRS.2020.3005679","Agency for Defense Development of Korea(grant numbers:UD190005DD); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9137342","Generative adversarial network (GAN);image translation;semantic image synthesis;side-scan sonar (SSS)","Image segmentation;Semantics;Generators;Image synthesis;Feature extraction;Sonar;Measurement","geophysical image processing;learning (artificial intelligence);sonar imaging","expensive time-consuming task;sensor towing;complicated field operations;deep learning;computer vision;generative adversarial network;realistic synthetic data;GAN-based semantic image synthesis model;high-quality SSS images;shallow water SSS data;deep water SSS data;imaging conditions;high sonar operating frequencies;low sonar operating frequencies;SSS data augmentation;diverse SSS relevant machine learning tasks;scan sonar image synthesis;side-scan sonar;critical sensor device;underwater environments;deep sea;gathering SSS data","","2","","16","IEEE","9 Jul 2020","","","IEEE","IEEE Journals"
"Modularized Architecture of Attribute Generative Adversarial Network for Image Synthesis","K. -H. Chen; B. -S. Lin","Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan","2021 IEEE 4th International Conference on Knowledge Innovation and Invention (ICKII)","27 Oct 2021","2021","","","140","145","In recent years, neural network models have achieved great success in the application of image-to-image translation based on conditional or cyclic generative adversarial networks. A variety of extended models have been developed to perform the multi-task image translation to synthesize output images according to the target attributes. In this paper, we propose a novel network architecture with multiple attribute discriminators that are pre-trained independently with attribute-relevant data, instead of being trained as a part of the generative adversarial network. Discriminators of extra attributes are added into the network incrementally, and the network is trained much faster since it is unnecessary to train the whole network for all attributes from scratch. This new design methodology makes the network modularized and extensible for new attributes in image translation tasks after the network has been trained with given attributes. We verified the proposed approach on an image-translation task that changes the attributes of the emoji images. Experimental result shows that the quality of image with multiple attributes generated by this approach is compatible with that by the conventional network.","","978-1-6654-2307-6","10.1109/ICKII51822.2021.9574660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9574660","GAN;CGAN;Image translation;Modularized architecture","Training;Image quality;Technological innovation;Image synthesis;Neural networks;Transforms;Network architecture","image processing;neural nets","modularized architecture;attribute generative adversarial network;image synthesis;neural network models;image-to-image translation;conditional networks;cyclic generative adversarial networks;multitask image translation;target attributes;network architecture;multiple attribute discriminators;attribute-relevant data;emoji images","","","","15","IEEE","27 Oct 2021","","","IEEE","IEEE Conferences"
"Defect Information Synthesis via Latent Mapping Adversarial Networks","S. Song; J. -G. Baek","Department of Industrial and Management Engineering, Korea University, Seoul, South Korea; Department of Industrial and Management Engineering, Korea University, Seoul, South Korea","2022 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","1 Mar 2022","2022","","","017","022","This research presents a new image synthesis methodology for automated visual inspection (AVI) in steel manufacturing process. We develop a novel methodology, termed Latent Mapping Adversarial Networks. As the end product of the manufacturing process is directly linked to economic factors, various methods are being utilized to improve the quality of the product. Among them, the defect detection steps carried out in advance are important as it greatly impacts productivity. However, new challenges have emerged for several reasons. First, it requires prior knowledge of the expert to define the defect image and perform detection. To alleviate this problem, various companies have started utilizing AVI to reduce this dependence on domain knowledge. Secondly, defect detection is an arduous task since fewer defect images are available compared to normal images. This underlying problem leads to a classification model that is biased toward the majority class, which degrades the final performance. In this paper, we propose a method to synthesize defect images to solve the above-mentioned problems. Inspired by StyleGAN, we build mapping networks for latent space of the generator. Through this, we can synthesize defect images of various sizes in the manufacturing process. In addition, we experiment to find the most suitable loss function to solve the common problems of Generative Adversarial Networks (GAN). We also optimized the proposed method in terms of convergence and computation speed by estimating the size of optimal latent space. The experimental results using quantitative metrics illustrate the improved performance of the proposed methodology. As a result, it is now possible to solve the quality problem and increase productivity by reducing misclassification in the model through AVI experiments using the generated images","","978-1-6654-5818-4","10.1109/ICAIIC54071.2022.9722628","Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722628","Automated visual inspection;generative adversarial networks;latent mapping;mapping network;synthesize defect","Productivity;Measurement;Visualization;Manufacturing processes;Image synthesis;Inspection;Generative adversarial networks","automatic optical inspection;image classification;inspection;manufacturing processes;production engineering computing;quality control;steel manufacture","generative adversarial networks;optimal latent space;quality problem;productivity;defect information synthesis;image synthesis;automated visual inspection;AVI;steel manufacturing process;end product;defect detection steps;defect images;latent mapping adversarial networks;StyleGAN","","1","","22","IEEE","1 Mar 2022","","","IEEE","IEEE Conferences"
"Architectural Facade Recognition and Generation through Generative Adversarial Networks","Q. Yu; J. Malaeb; W. Ma","Department of Architecture, School of Design, Shanghai Jiao Tong University, Shanghai, China; Department of Architecture, School of Design, Shanghai Jiao Tong University, Shanghai, China; China Institute for Urban Governance, Shanghai, China","2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)","23 Apr 2021","2020","","","310","316","With the development of artificial intelligence technology, the ideas of machine learning have been introduced into the field of design in recent years. The research methods of “AI + Architecture” have brought new ideas for solving traditional problems. Generative Adversarial Network (GAN) is a machine learning model for image generation. Pix2pix is an improved version of GAN, which is specially designed to learn and generate pairs of image data with similar characteristics. In this study, Pix2pix is applied to the recognition and generation of building facade. The purpose is to explore the feasibility of using image generation technology to achieve rapid recognition and generation of building facade based on pix2pix. This paper also discusses the application scenarios of this technology. The existing building façade datasets and the self-made Chinese traditional building datasets are used to test and verify that pix2pix under different types of datasets can nicely identify and generate facade images. Then we summarize a set of working methods based on GAN to realize the overall or local reconstruction design of the facade, so as to provide new ideas for the improvement of the efficiency of related industries and the expansion of teaching tools.","","978-1-7281-9619-0","10.1109/ICBASE51474.2020.00072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403820","facade;GAN;image recognition;image generation","Industries;Image recognition;Image synthesis;Buildings;Education;Machine learning;Tools","architecture;buildings (structures);image recognition;learning (artificial intelligence);neural nets;structural engineering computing","building facade;Pix2pix;facade images;GAN;generative adversarial networks;machine learning;image generation technology;architectural facade recognition;architectural facade generation","","2","","7","IEEE","23 Apr 2021","","","IEEE","IEEE Conferences"
"Application for Generating Images Using Generative-Adversarial Networks","M. Bardina; I. Volodchenko; M. Petrova","Faculty of Digital Transformation, ITMO University, Saint Petersburg, Russia; School of Electronic Engineering and Computer Science, South Ural State University (national research university), Chelyabinsk, Russia; School of Electronic Engineering and Computer Science, South Ural State University (national research university), Chelyabinsk, Russia","2020 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon)","8 Dec 2020","2020","","","1","6","Nowadays there is an increasing interest in the field of image generation using neural networks. Images created by such neural networks can find practical applications in various fields. The article describes the stages of creating an application for generating images in a predetermined art style using neural network technologies. The review of scientific literature and analogues is given. The topology of the used generative adversarial network is also given. The process of creating a dataset is described.","","978-1-7281-6951-4","10.1109/FarEastCon50210.2020.9271543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271543","Convolutional Neural Networks;Computer vision;Analysis of images;Automatic painter classification;Deep learning","Neural networks;Training;Task analysis;Industrial engineering;Image synthesis;Convolution;Computer architecture","art;computer vision;neural nets","art style;neural network technologies;image generation;generative-adversarial networks","","1","","19","IEEE","8 Dec 2020","","","IEEE","IEEE Conferences"
"Auto-Embedding Generative Adversarial Networks For High Resolution Image Synthesis","Y. Guo; Q. Chen; J. Chen; Q. Wu; Q. Shi; M. Tan","School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Multimedia","24 Oct 2019","2019","21","11","2726","2737","Generating images via a generative adversarial network (GAN) has attracted much attention recently. However, most of the existing GAN-based methods can only produce lowresolution images of limited quality. Directly generating highresolution images using GANs is nontrivial, and often produces problematic images with incomplete objects. To address this issue, we develop a novel GAN called auto-embedding generative adversarial network, which simultaneously encodes the global structure features and captures the fine-grained details. In our network, we use an autoencoder to learn the intrinsic high-level structure of real images and design a novel denoiser network to provide photo-realistic details for the generated images. In the experiments, we are able to produce 512 × 512 images of promising quality directly from the input noise. The resultant images exhibit better perceptual photo-realism, that is, with sharper structure and richer details, than other baselines on several datasets, including Oxford-102 Flowers, Caltech-UCSD Birds (CUB), High-Quality Large-scale CelebFaces Attributes (CelebAHQ), Large-scale Scene Understanding (LSUN), and ImageNet.","1941-0077","","10.1109/TMM.2019.2908352","National Natural Science Foundation of China(grant numbers:61602185,61876208,61502177); Recruitment Program for Young Professionals; Guangdong Provincial Scientific and Technological Fund(grant numbers:2017B090901008,2017A010101011,2017B090910005); Pearl River S and T Nova Program of Guangzhou(grant numbers:201806010081); CCF-Tencent Open Research Fund(grant numbers:RAGR20170105); Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X183); Scientific and Technological Planning on Provincial Key Research and Development(grant numbers:2018B01010700); Guangdong Special Branch Plans Young Talent; Scientific and Technological Innovation(grant numbers:2016TQ03X445); Guangzhou Science and Technology Planning Project(grant numbers:2019-03-01-06-3002-0003); Guangzhou Tianhe District Science and Technology Planning(grant numbers:201702YH112); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676365","Generative models;adversarial learning;low-dimensional embedding;autoencoder","Image resolution;Generative adversarial networks;Gallium nitride;Training;Data mining;Feature extraction;Generators","image coding;image denoising;image resolution","intrinsic high-level structure;GAN-based methods;autoembedding generative adversarial network;high resolution image synthesis;low resolution image synthesis;denoiser network;autoencoder;Oxford-102 Flowers;Caltech-UCSD Birds;CUB;high-quality large-scale celebfaces attributes;CelebAHQ;large-scale scene understanding;LSUN;ImageNet","","42","","61","IEEE","29 Mar 2019","","","IEEE","IEEE Journals"
"Anycost GANs for Interactive Image Synthesis and Editing","J. Lin; R. Zhang; F. Ganz; S. Han; J. -Y. Zhu",Adobe Research; Adobe Research; Adobe Research; MIT; CMU,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","14981","14991","Generative adversarial networks (GANs) have enabled photorealistic image synthesis and editing. However, due to the high computational cost of large-scale generators (e.g., StyleGAN2), it usually takes seconds to see the results of a single edit on edge devices, prohibiting interactive user experience. In this paper, inspired by quick preview features in modern rendering software, we propose Anycost GAN for interactive natural image editing. We train the Anycost GAN to support elastic resolutions and channels for faster image generation at versatile speeds. Running subsets of the full generator produce outputs that are perceptually similar to the full generator, making them a good proxy for quick preview. By using sampling-based multi-resolution training, adaptive-channel training, and a generator-conditioned discriminator, the anycost generator can be evaluated at various configurations while achieving better image quality compared to separately trained models. Furthermore, we develop new encoder training and latent code optimization techniques to encourage consistency between the different sub-generators during image projection. Anycost GAN can be executed at various cost budgets (up to 10× computation reduction) and adapt to a wide range of hardware and la tency requirements. When deployed on desktop CPUs and edge devices, our model can provide perceptually similar previews at 6-12× speedup, enabling interactive image editing. The ${\color{RubineRed}{code}}$ and ${\color{RubineRed}{demo}}$ are publicly available.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01474","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577400","","Training;Adaptation models;Image synthesis;Image edge detection;Computational modeling;Generative adversarial networks;Rendering (computer graphics)","image resolution;image sampling;interactive systems;neural nets;optimisation;rendering (computer graphics);user experience;user interfaces","desktop CPU;latent code optimization technique;encoder training;sampling-based multiresolution training;modern rendering software;image projection;image quality;anycost generator;generator-conditioned discriminator;adaptive-channel training;image generation;elastic resolutions;interactive natural image editing;quick preview features;interactive user experience;single edit;large-scale generators;photorealistic image synthesis;generative adversarial networks;interactive image synthesis;edge devices;Anycost GAN","","10","","81","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Text to Image Synthesis With Bidirectional Generative Adversarial Network","Z. Wang; Z. Quan; Z. -J. Wang; X. Hu; Y. Chen","College of Information Science and Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China; College of Computer Science, Chongqing University, Chongqing, China; College of Information Science and Engineering, Hunan University, Changsha, China; College of Information Science and Engineering, Hunan University, Changsha, China","2020 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2020","2020","","","1","6","Generating realistic images from text descriptions is a challenging problem in computer vision. Although previous works have shown remarkable progress, guaranteeing semantic consistency between text descriptions and images remains challenging. To generate semantically consistent images, we propose two semantics-enhanced modules and a novel Textual-Visual Bidirectional Generative Adversarial Network (TVBi-GAN). Specifically, this paper proposes a semanticsenhanced attention module and a semantics-enhanced batch normalization module. These modules improve consistency of synthesized images by involving precisely semantic features. What's more, an encoder network is proposed to extract semantic features from images. During the adversarial process, the encoder could guide our generator to explore corresponding features behind descriptions. With extensive experiments on CUB and COCO datasets, we demonstrate that our TVBi-GAN outperforms state-of-the-art methods.","1945-788X","978-1-7281-1331-9","10.1109/ICME46284.2020.9102904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9102904","Text-to-Image Synthesis","Semantics;Generators;Feature extraction;Gallium nitride;Visualization;Image generation;Generative adversarial networks","computer vision;feature extraction;neural nets;realistic images","TVBi-GAN;text to image synthesis;realistic images;computer vision;semantics-enhanced batch normalization module;synthesized images;textual-visual bidirectional generative adversarial network","","7","","28","IEEE","9 Jun 2020","","","IEEE","IEEE Conferences"
"DivCo: Diverse Conditional Image Synthesis via Contrastive Generative Adversarial Network","R. Liu; Y. Ge; C. L. Choi; X. Wang; H. Li","CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong; CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong; NVIDIA AI Technology Center, NVIDIA, Hong Kong; CUHK-SenseTime Joint Laboratory, The Chinese University of Hong Kong; School of CST, Xidian University","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","16372","16381","Conditional generative adversarial networks (cGANs) target at synthesizing diverse images given the input conditions and latent codes, but unfortunately, they usually suffer from the issue of mode collapse. To solve this issue, previous works [47], [22] mainly focused on encouraging the correlation between the latent codes and their generated images, while ignoring the relations between images generated from various latent codes. The recent MSGAN [27] tried to encourage the diversity of the generated image but only considers ""negative"" relations between the image pairs.In this paper, we propose a novel DivCo framework to properly constrain both ""positive"" and ""negative"" relations between the generated images specified in the latent space. To the best of our knowledge, this is the first attempt to use contrastive learning for diverse conditional image synthesis. A novel latent-augmented contrastive loss is introduced, which encourages images generated from adjacent latent codes to be similar and those generated from distinct latent codes to be dissimilar. The proposed latent-augmented contrastive loss is well compatible with various cGAN architectures. Extensive experiments demonstrate that the proposed DivCo can produce more diverse images than state-of-the-art methods without sacrificing visual quality in multiple unpaired and paired image generation tasks. Training code and pretrained models are available at https://github.com/ruiliu-ai/DivCo.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578308","","Training;Visualization;Computer vision;Codes;Correlation;Image synthesis;Generative adversarial networks","feature extraction;image colour analysis;image reconstruction;learning (artificial intelligence)","DivCo;diverse conditional image synthesis;contrastive generative adversarial network;conditional generative adversarial networks target;synthesizing diverse images;input conditions;negative relations;image pairs;latent space;novel latent-augmented contrastive loss;adjacent latent codes;distinct latent codes;multiple unpaired paired image generation tasks","","9","","47","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"I2T2I: Learning text to image synthesis with textual data augmentation","H. Dong; J. Zhang; D. McIlwraith; Y. Guo","Data Science Institute, Imperial College London; Data Science Institute, Imperial College London; Data Science Institute, Imperial College London; Data Science Institute, Imperial College London","2017 IEEE International Conference on Image Processing (ICIP)","22 Feb 2018","2017","","","2015","2019","Translating information between text and image is a fundamental problem in artificial intelligence that connects natural language processing and computer vision. In the past few years, performance in image caption generation has seen significant improvement through the adoption of recurrent neural networks (RNN). Meanwhile, text-to-image generation begun to generate plausible images using datasets of specific categories like birds and flowers. We've even seen image generation from multi-category datasets such as the Microsoft Common Objects in Context (MSCOCO) through the use of generative adversarial networks (GANs). Synthesizing objects with a complex shape, however, is still challenging. For example, animals and humans have many degrees of freedom, which means that they can take on many complex shapes. We propose a new training method called Image-Text-Image (I2T2I) which integrates text-to-image and image-to-text (image captioning) synthesis to improve the performance of text-to-image synthesis. We demonstrate that I2T2I can generate better multi-categories images using MSCOCO than the state-of-the-art. We also demonstrate that I2T2I can achieve transfer learning by using a pre-trained image captioning module to generate human images on the MPII Human Pose dataset (MHP) without using sentence annotation.","2381-8549","978-1-5090-2175-8","10.1109/ICIP.2017.8296635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296635","Deep learning;GAN;Image Synthesis","Training;Image generation;Gallium nitride;Birds;Generators;Recurrent neural networks;Shape","computer vision;image annotation;learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis","complex shape;Image-Text-Image;I2T2I;image-to-text synthesis;text-to-image synthesis;human images;textual data augmentation;natural language processing;computer vision;image caption generation;recurrent neural networks;text-to-image generation;multicategory datasets;generative adversarial networks;image captioning module;artificial intelligence;information translation;Microsoft Common Objects in Context;object synthesis;transfer learning;MPII Human Pose dataset;sentence annotation","","24","","19","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"High-Quality Sonar Image Generation Algorithm Based on Generative Adversarial Networks","Z. Wang; Q. Guo; M. Lei; S. Guo; X. Ye","College of Intelligent Systems Science and Engineering, Harbin Engineering University, Harbin, China; No.710 R & D, CSIC, Yichang, China; No.710 R & D, CSIC, Yichang, China; Department of Intelligent Mechanical Systems Engineering, Faculty of Engineering, Kagawa University, Takamatsu, Japan; College of Intelligent Systems Science and Engineering, Harbin Engineering University, Harbin, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","3099","3104","In this paper, we propose a high-quality sonar image generation model based on generative adversarial networks, which is used to generate user-controlled high-resolution and high-quality sonar images, so that the targets in these images have more obvious features and can be further used in engineering applications such as target detection and image classification. As a result of the development of underwater detection technology, sonar target detection technology is widely used in various engineering fields. High resolution and high-quality underwater sonar images have become an indispensable part of underwater detection. However, due to the high cost of sonar and the high cost of marine experiments, sonar datasets are scarce. Even then, few sonar data can be obtained, and most images have no target and have more noise so that they cannot be used for underwater detection. In recent years, using generative adversarial networks to generate images has gradually become an important method of solving the problem of data scarcity. But there are still some shortcomings — the images produced are poor and the resolution is low. To overcome the issue of poor image quality and low resolution caused by gradient disappearing and sudden change in the training process, we, therefore, use a controllable multi-layer transposed convolutional layer and gradient correction term to improve image resolution and image quality. A simple network structure does not make the model difficult to train, and the user may change the size of the generated image by simply changing the convolution parameter i to achieve the objective of improving the resolution. Moreover, the gradient correction term added in the training phase will always restrict the gradient to a certain value. To a certain degree, the problems of gradient disappearing and gradient sudden change in model training can be overcome, thereby providing a guarantee for the production of high-quality images. Experiments demonstrate that this method will successfully increase the resolution and image quality of the GAN-based image and make the image target more apparent.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9550195","National Natural Science Foundation of China; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550195","Generative Adversarial Networks;Sonar Image Generation;Transposed Convolution;Gradient Correction Term;Underwater Detection","Training;Image quality;Image resolution;Costs;Image synthesis;Convolution;Object detection","convolutional neural nets;geophysical image processing;gradient methods;image classification;image resolution;object detection;oceanographic techniques;sonar imaging","image classification;underwater detection;sonar target detection;high-quality underwater sonar images;sonar datasets;generative adversarial networks;gradient correction term;image resolution;GAN-based image;image target;user-controlled high-resolution;high-quality sonar image generation;marine experiments;data scarcity;image quality;multilayer transposed convolutional layer;network structure;convolution parameter","","","","16","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Ea-GANs: Edge-Aware Generative Adversarial Networks for Cross-Modality MR Image Synthesis","B. Yu; L. Zhou; L. Wang; Y. Shi; J. Fripp; P. Bourgeat","School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; CSIRO Health and Biosecurity, Brisbane, QLD, Australia; CSIRO Health and Biosecurity, Brisbane, QLD, Australia","IEEE Transactions on Medical Imaging","28 Jun 2019","2019","38","7","1750","1762","Magnetic resonance (MR) imaging is a widely used medical imaging protocol that can be configured to provide different contrasts between the tissues in human body. By setting different scanning parameters, each MR imaging modality reflects the unique visual characteristic of scanned body part, benefiting the subsequent analysis from multiple perspectives. To utilize the complementary information from multiple imaging modalities, cross-modality MR image synthesis has aroused increasing research interest recently. However, most existing methods only focus on minimizing pixel/voxel-wise intensity difference but ignore the textural details of image content structure, which affects the quality of synthesized images. In this paper, we propose edge-aware generative adversarial networks (Ea-GANs) for cross-modality MR image synthesis. Specifically, we integrate edge information, which reflects the textural structure of image content and depicts the boundaries of different objects in images, to reduce this gap. Corresponding to different learning strategies, two frameworks are proposed, i.e., a generator-induced Ea-GAN (gEa-GAN) and a discriminator-induced Ea-GAN (dEa-GAN). The gEa-GAN incorporates the edge information via its generator, while the dEa-GAN further does this from both the generator and the discriminator so that the edge similarity is also adversarially learned. In addition, the proposed Ea-GANs are 3D-based and utilize hierarchical features to capture contextual information. The experimental results demonstrate that the proposed Ea-GANs, especially the dEa-GAN, outperform multiple state-of-the-art methods for cross-modality MR image synthesis in both qualitative and quantitative measures. Moreover, the dEa-GAN also shows excellent generality to generic image synthesis tasks on benchmark datasets about facades, maps, and cityscapes.","1558-254X","","10.1109/TMI.2019.2895894","Australian Research Council(grant numbers:ARC DE160100241); CCFTencent Open Research Fund(grant numbers:RAGR 20180114); National Natural Science Foundation of China(grant numbers:NSFC 61673203); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629301","Neural networks;machine learning;magnetic resonance imaging (MRI);brain","Image edge detection;Image generation;Generative adversarial networks;Generators;Imaging;Gallium nitride;Three-dimensional displays","","","Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Neural Networks, Computer","115","","64","IEEE","29 Jan 2019","","","IEEE","IEEE Journals"
"Thermal Face Image Generator","X. Cao; K. Lai; S. Yanushkevich; M. Smith","Department of Electrical and Software Engineering, University of Calgary, Alberta, Canada; Department of Electrical and Software Engineering, University of Calgary, Alberta, Canada; Department of Electrical and Software Engineering, University of Calgary, Alberta, Canada; Department of Electrical and Software Engineering, University of Calgary, Alberta, Canada","2021 IEEE International Conference on Autonomous Systems (ICAS)","6 Oct 2021","2021","","","1","5","This work addresses two image-to-image translation tasks. The first task is to convert a visible face image into a thermal face image (V2T) and the second task is to convert a thermal face image into another thermal face image with a given target temperature (T2T). We propose to use conditional generative adversarial networks to solve the two tasks. We train our models using Carl and SpeakingFaces Datasets, and use SSIM to measure the performance of our models. The SSIM of the generated thermal images reach 0.82 and 0.84 for the V2T and T2T tasks respectively.","","978-1-7281-7289-7","10.1109/ICAS49788.2021.9551148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9551148","Generative adversarial networks;image-to-image translation;thermal image generation","Temperature measurement;Temperature distribution;Image synthesis;Autonomous systems;Conferences;Generative adversarial networks;Generators","face recognition;infrared imaging;neural nets","T2T tasks;SSIM;Carl datasets;SpeakingFaces datasets;visible face image conversion;target temperature;image-to-image translation tasks;thermal face image generator","","2","","14","IEEE","6 Oct 2021","","","IEEE","IEEE Conferences"
"Latent Vector Prototypes Guided Conditional Face Synthesis","Q. Wei; X. Yang; T. Sang; H. Wang; Z. Xiaofeng; C. Zhongyao; Z. Ziyuan; Z. Zeng","School of Microelectronics, Shanghai University (SHU), China; Artificial Intelligence, Analytics And Informatics (AI3), A*STAR, Singapore; School of Microelectronics, Shanghai University (SHU), China; School of Electronic Information, Wuhan University (WHU), China; College of Computer Science and Electronic Engineering, Hunan University (HNU), China; Institute for Infocomm Research (I2R), A*STAR, Singapore; Artificial Intelligence, Analytics And Informatics (AI3), A*STAR, Singapore; Artificial Intelligence, Analytics And Informatics (AI3), A*STAR, Singapore","2022 IEEE International Conference on Image Processing (ICIP)","18 Oct 2022","2022","","","3898","3902","Recent advances in deep neural networks, especially in generative adversarial networks (GAN), have shown remarkable progress in face image generations. However, most of the existing face image generators can only synthesize random face images, but are not able to control the attributes of the generated face images. Though conditional GAN based methods can manipulate the attributes to some extent, but can only generate low-resolution face images up to 256 × 256. In this study, based on StyleGAN, one of the state-of-the-art image generators for synthesizing high-quality face images, we propose a simple but efficient approach to generate high-resolution and hyper-realistic face images with any desired attribute. By training an attribute classifier to assign attribute labels to given synthesized face images, we build the links between latent vectors and face attributes. In such a way, the latent vectors can be grouped into different clusters, one cluster corresponding to one face attribute, respectively. We then extract the prototypes for the clusters, which are used to control the attribute of the generated face image. Extensive experiments demonstrate the effectiveness of the proposed approach for high-quality face image generation with predefined attributes.","2381-8549","978-1-6654-9620-9","10.1109/ICIP46576.2022.9897502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897502","Conditional Face Generation;Generative Adversarial Networks;Prototype Clustering","Training;Deep learning;Image synthesis;Neural networks;Prototypes;Generative adversarial networks;Generators","","","","","","21","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"Cumulant GAN","Y. Pantazis; D. Paul; M. Fasoulakis; Y. Stylianou; M. A. Katsoulakis","Institute of Applied and Computational Mathematics, Foundation for Research & Technology-Hellas (FORTH), 70013 Heraklion, Greece; Department of Computer Science, University of Crete, 70013 Rethymno, Greece.; Institute of Computer Science, FORTH, 70013 Heraklion, Greece.; Department of Computer Science, University of Crete, 70013 Rethymno, Greece.; Department of Mathematics and Statistics, University of Massachusetts at Amherst, Amherst, MA 01003 USA.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","12","In this article, we propose a novel loss function for training generative adversarial networks (GANs) aiming toward deeper theoretical understanding as well as improved stability and performance for the underlying optimization problem. The new loss function is based on cumulant generating functions (CGFs) giving rise to Cumulant GAN. Relying on a recently derived variational formula, we show that the corresponding optimization problem is equivalent to Rényi divergence minimization, thus offering a (partially) unified perspective of GAN losses: the Rényi family encompasses Kullback-Leibler divergence (KLD), reverse KLD, Hellinger distance, and χ²-divergence. Wasserstein GAN is also a member of cumulant GAN. In terms of stability, we rigorously prove the linear convergence of cumulant GAN to the Nash equilibrium for a linear discriminator, Gaussian distributions, and the standard gradient descent ascent algorithm. Finally, we experimentally demonstrate that image generation is more robust relative to Wasserstein GAN and it is substantially improved in terms of both inception score (IS) and Fréchet inception distance (FID) when both weaker and stronger discriminators are considered.","2162-2388","","10.1109/TNNLS.2022.3161127","Hellenic Foundation for Research and Innovation HFRI through the Second Call for HFRI Research Projects to support Faculty Members and Researchers(grant numbers:4753); project Innovative Actions in Environmental Research and Development PErAn MIS 5002358 funded by the Operational Program Competitiveness Entrepreneurship and Innovation(grant numbers:NSRF 2014-2020); EU H2020 Research and Innovation Program(grant numbers:MSCA GA 67532 (the ENRICH network:www.enrich-etn.eu)); Stavros Niarchos-FORTH Post-Doctoral Fellowship for the project Advancing Young Researchers Human Capital in Cutting Edge Technologies in the Preservation of Cultural Heritage and the Tackling of Societal Challenges - ARCHERS; National Science Foundation NSF(grant numbers:DMS-2008970); HDR-TRIPODS Institute for Integrated Data Science: A Transdisciplinary Approach to Understanding Fundamental Trade-offs and Theoretical Foundations Program of NSF(grant numbers:CISE-1934846); Air Force Office of Scientific Research AFOSR(grant numbers:FA-9550-18-1-0214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9750393","Cumulant generating function (CGF);generative adversarial networks (GANs);image generation;Rényi divergence.","Generative adversarial networks;Training;Optimization;Standards;Minimization;Generators;Image synthesis","","","","","","","IEEE","6 Apr 2022","","","IEEE","IEEE Early Access Articles"
"DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation","Z. Zhang; L. Schomaker","Bernoulli Institute, University of Groningen, Groningen, The Netherlands; Bernoulli Institute, University of Groningen, Groningen, The Netherlands","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","8","Most existing text-to-image generation methods adopt a multi-stage modular architecture which has three significant problems: 1) Training multiple networks increases the run time and affects the convergence and stability of the generative model; 2) These approaches ignore the quality of early-stage generator images; 3) Many discriminators need to be trained. To this end, we propose the Dual Attention Generative Adversarial Network (DTGAN) which can synthesize high-quality and semantically consistent images only employing a single generator/discriminator pair. The proposed model introduces channel-aware and pixel-aware attention modules that can guide the generator to focus on text-relevant channels and pixels based on the global sentence vector and to fine-tune original feature maps using attention weights. Also, Conditional Adaptive Instance-Layer Normalization (CAdaILN) is presented to help our attention modules flexibly control the amount of change in shape and texture by the input natural-language description. Furthermore, a new type of visual loss is utilized to enhance the image resolution by ensuring vivid shape and perceptually uniform color distributions of generated images. Experimental results on benchmark datasets demonstrate the superiority of our proposed method compared to the state-of-the-art models with a multi-stage framework.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533527","text-to-image synthesis;attention mechanism;conditional normalization;visual loss","Training;Visualization;Image resolution;Shape;Image color analysis;Neural networks;Generative adversarial networks","data mining;feature extraction;image colour analysis;image representation;image resolution;image texture;learning (artificial intelligence);natural language processing;text analysis","DTGAN;Dual Attention Generative Adversarial networks;existing text-to-image generation methods;multistage modular architecture;generative model;early-stage generator images;Dual Attention Generative Adversarial Network;semantically consistent images;pixel-aware attention modules;text-relevant channels;attention weights;image resolution","","6","","33","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Label-Noise Robust Generative Adversarial Networks","T. Kaneko; Y. Ushiku; T. Harada",The University of Tokyo; The University of Tokyo; RIKEN,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","2462","2471","Generative adversarial networks (GANs) are a framework that learns a generative distribution through adversarial training. Recently, their class conditional extensions (e.g., conditional GAN (cGAN) and auxiliary classifier GAN (AC-GAN)) have attracted much attention owing to their ability to learn the disentangled representations and to improve the training stability. However, their training requires the availability of large-scale accurate class-labeled data, which are often laborious or impractical to collect in a real-world scenario. To remedy this, we propose a novel family of GANs called label-noise robust GANs (rGANs), which, by incorporating a noise transition model, can learn a clean label conditional generative distribution even when training labels are noisy. In particular, we propose two variants: rAC-GAN, which is a bridging model between AC-GAN and the label-noise robust classification model, and rcGAN, which is an extension of cGAN and solves this problem with no reliance on any classifier. In addition to providing the theoretical background, we demonstrate the effectiveness of our models through extensive experiments using diverse GAN configurations, various noise settings, and multiple evaluation metrics (in which we tested 402 conditions in total).","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954304","Image and Video Synthesis;Deep Learning;Representation Learning","Training;Computer vision;Image synthesis;Computational modeling;Generative adversarial networks;Stability analysis;Data models","image classification;learning (artificial intelligence)","rAC-GAN;label-noise robust classification model;diverse GAN configurations;noise settings;label-noise robust generative adversarial networks;adversarial training;class conditional extensions;training stability;large-scale accurate class-labeled data;noise transition model;clean label conditional generative distribution;training labels;label-noise robust GAN","","29","","77","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Feedback Adversarial Learning: Spatial Feedback for Improving Generative Adversarial Networks","M. Huh; S. -H. Sun; N. Zhang",UC Berkeley; University of Southern California; Vaitl Inc.,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","1476","1485","We propose feedback adversarial learning (FAL) framework that can improve existing generative adversarial networks by leveraging spatial feedback from the discriminator. We formulate the generation task as a recurrent framework, in which the discriminator’s feedback is integrated into the feedforward path of the generation process. Specifically, the generator conditions on the discriminator’s spatial output response, and its previous generation to improve generation quality over time – allowing the generator to attend and fix its previous mistakes. To effectively utilize the feedback, we propose an adaptive spatial transform layer, which learns to spatially modulate feature maps from its previous generation and the error signal from the discriminator. We demonstrate that one can easily adapt FAL to existing adversarial learning frameworks on a wide range of tasks, including image generation, image-to-image translation, and voxel generation.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954130","Image and Video Synthesis;Computational Photography;Deep Learning","Measurement;Computer vision;Image synthesis;Computational modeling;Transforms;Generative adversarial networks;Adversarial machine learning","feedback;feedforward;image resolution;learning (artificial intelligence)","generative adversarial networks;feedback adversarial learning framework;leveraging spatial feedback;generation task;recurrent framework;discriminator;generation process;generator conditions;generation quality;adaptive spatial;spatially modulate feature maps;image generation;voxel generation","","10","","68","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Constrained Generative Adversarial Networks for Interactive Image Generation","E. Heim","Air Force Research Laboratory Information Directorate, Rome, NY, USA","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","10745","10753","Generative Adversarial Networks (GANs) have received a great deal of attention due in part to recent success in generating original, high-quality samples from visual domains. However, most current methods only allow for users to guide this image generation process through limited interactions. In this work we develop a novel GAN framework that allows humans to be “in-the-loop” of the image generation process. Our technique iteratively accepts relative constraints of the form “Generate an image more like image A than image B”. After each constraint is given, the user is presented with new outputs from the GAN, informing the next round of feedback. This feedback is used to constrain the output of the GAN with respect to an underlying semantic space that can be designed to model a variety of different notions of similarity (e.g. classes, attributes, object relationships, color, etc.). In our experiments, we show that our GAN framework is able to generate images that are of comparable quality to equivalent unsupervised GANs while satisfying a large number of the constraints provided by users, effectively changing a GAN into one that allows users interactive control over image generation without sacrificing image quality.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953955","Deep Learning;Image and Video Synthesis;Representation Learning","Image quality;Visualization;Computer vision;Image synthesis;Image color analysis;Semantics;Generative adversarial networks","computer vision;content-based retrieval;data visualisation;neural nets","interactive image generation;generative adversarial networks;image generation process;novel GAN framework;unsupervised GANs;image quality","","6","","40","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks","V. S. Bhaskara; T. Aumentado-Armstrong; A. Jepson; A. Levinshtein",Samsung AI Centre Toronto; Vector Institute for AI; Samsung AI Centre Toronto; Samsung AI Centre Toronto,"2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","15 Feb 2022","2022","","","2432","2441","Modern generative adversarial networks (GANs) predominantly use piecewise linear activation functions in discriminators (or critics), including ReLU and LeakyReLU. Such models learn piecewise linear mappings, where each piece handles a subset of the input space, and the gradients per subset are piecewise constant. Under such a class of discriminator (or critic) functions, we present Gradient Normalization (GraN), a novel input-dependent normalization method, which guarantees a piecewise K-Lipschitz constraint in the input space. In contrast to spectral normalization, GraN does not constrain processing at the individual network layers, and, unlike gradient penalties, strictly enforces a piecewise Lipschitz constraint almost everywhere. Empirically, we demonstrate improved image generation performance across multiple datasets (incl. CIFAR-10/100, STL-10, LSUN bedrooms, and CelebA), GAN loss functions, and metrics. Further, we analyze altering the often untuned Lipschitz constant in several standard GANs, not only attaining significant performance gains, but also finding connections between and training dynamics, particularly in low-gradient loss plateaus, with the common Adam optimizer.","2642-9381","978-1-6654-0915-5","10.1109/WACV51458.2022.00249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707010","Deep Learning Deep Learning -> Neural Generative Models; Autoencoders; GANs","Training;Measurement;Computer vision;Image synthesis;Performance gain;Generative adversarial networks;Standards","gradient methods;image classification;learning (artificial intelligence);neural nets;optimisation;piecewise linear techniques","image generation performance;GAN loss functions;low-gradient loss plateaus;GraN-GAN;generative adversarial networks;piecewise linear activation functions;piecewise linear mappings;piecewise K-Lipschitz constraint;Adam optimizer;piecewise gradient normalization;input-dependent normalization","","3","","43","IEEE","15 Feb 2022","","","IEEE","IEEE Conferences"
"Up and Down Residual Blocks for Convolutional Generative Adversarial Networks","Y. Wang; X. Guo; P. Liu; B. Wei","Computing Center, Ocean University of China, Qingdao, China; The Institution for Maritime Development Studies, Ocean University of China, Qingdao, China; Computing Center, Ocean University of China, Qingdao, China; Shandong Key Laboratory of Digital Medicine and Computer Assisted Surgery, Qingdao, China","IEEE Access","15 Feb 2021","2021","9","","26051","26058","Most recent existing image generation methods have made great progress in creating high-quality images, mainly focusing on improving the generator or discriminator of convolutional generative adversarial networks (GANs). In this paper, we propose up and down residual blocks for convolutional GANs, dubbed upResBlock and downResBlock respectively. This structure is based on deconvolutions, strided convolutions, and residual blocks. With the upResBlock module for the generator of convolutional GANs, our method can further enhance the generative power of the feature extraction while synthesizing image details for the specified size. With the downResBlock module for discriminator combined with upResBlock for generator, the proposed method can speed up the back propagation of gradient and doesn't suffer from the vanishing or exploding gradients problems, generating more realistic images as well. Extensive experiments demonstrate that the proposed up and down residual blocks can help convolutional GANs in generating photo-realistic images. In addition, our method shows its universality for the improvement of existing methods.","2169-3536","","10.1109/ACCESS.2021.3056572","MOE (Ministry of Education in China) Project of Humanities and Social Sciences(grant numbers:18YJCZH103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9344669","Generative adversarial network;convolutional neural network;residual block;sampling","Generators;Generative adversarial networks;Task analysis;Gallium nitride;Image synthesis;Deconvolution;Training","backpropagation;convolutional neural nets;feature extraction;gradient methods;realistic images","residual blocks;convolutional generative adversarial networks;image generation methods;high-quality images;convolutional GANs;strided convolutions;generative power;photorealistic image generation;image detail synthesis;feature extraction;backpropagation;gradient problems;realistic images;deconvolutions;dubbed upResBlock module;downResBlock module;discriminator","","3","","39","CCBY","2 Feb 2021","","","IEEE","IEEE Journals"
"Attributes Aware Face Generation with Generative Adversarial Networks","Z. Yuan; J. Zhang; S. Shan; X. Chen","University of Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","1657","1664","Recent studies have shown remarkable success in face image generations. However, most of the existing methods only generate face images from random noise, and cannot generate face images according to the specific attributes. In this paper, we focus on the problem of face synthesis from attributes, which aims at generating faces with specific characteristics corresponding to the given attributes. To this end, we propose a novel attributes aware face image generator method with generative adversarial networks called AFGAN. Specifically, we firstly propose a two-path embedding layer and self-attention mechanism to convert binary attribute vector to rich attribute features. Then three stacked generators generate 64 × 64, 128 × 128 and 256 × 256 resolution face images respectively by taking the attribute features as input. In addition, an image-attribute matching loss is proposed to enhance the correlation between the generated images and input attributes. Extensive experiments on CelebA demonstrate the superiority of our AFGAN in terms of both qualitative and quantitative evaluations.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412022","","Image resolution;Correlation;Image synthesis;Face recognition;Generative adversarial networks;Generators;Faces","face recognition;image resolution;neural nets;vectors","generative adversarial networks;face image generations;face synthesis;binary attribute vector;attribute features;image-attribute matching loss;attribute aware face generation;CelebA;attribute aware face image generator method;AFGAN;two-path embedding layer;self-attention mechanism","","3","","29","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Exploration of the evolution-based footprint on the Generative Adversarial Networks","P. Kopciewicz; V. Morskyi","AGH University of Science and Technology, Krakow, Poland; Rzeszow University of Technology, Rzeszow, Poland","2021 IEEE Symposium Series on Computational Intelligence (SSCI)","24 Jan 2022","2021","","","1","7","In the last few years, the Generative Adversarial Networks (GAN) have proved to be an excellent tool for image generation and found a variety of practical applications. In this paper, the specific evolutional concept of mutation for the generative model training improvement is proposed and examined. The idea behind the research is that the generative model learning can be optionally tackled as a complex optimization task. Such an approach is associated with a two-fold problem raised in the paper, meaning that comparing different generators to each other is hard to define, so is the fitness function of the weight vectors. It can be assumed that the fitness function, if existed, might have an unknown but potentially high number of local minima, which could restrict the gradient descent to move in some directions. The vast number of degrees of freedom usually seen in the image processing layers exceeds the standard problems for intelligent optimization algorithms. Therefore the hybrid combination of the gradient descent with the smart usage of mutations is considered and examined, in particular with the proposed novel approach for the mutation masking strategy.","","978-1-7281-9048-8","10.1109/SSCI50451.2021.9659993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659993","","Training;Backpropagation;Image synthesis;Computational modeling;Graphics processing units;Stochastic processes;Generative adversarial networks","evolutionary computation;genetic algorithms;gradient methods;image processing;learning (artificial intelligence);optimisation;vectors","Generative Adversarial Networks;excellent tool;image generation;generative model training improvement;generative model learning;complex optimization task;different generators;fitness function;unknown but potentially high number;gradient descent;evolution-based footprint","","","","72","IEEE","24 Jan 2022","","","IEEE","IEEE Conferences"
"S2I-Bird: Sound-to-Image Generation of Bird Species using Generative Adversarial Networks","J. Y. Shim; J. Kim; J. -K. Kim","School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Electrical Engineering, Korea University, Seoul, Republic of Korea","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","2226","2232","Generating images from sound is a challenging task. This paper proposes a novel deep learning model that generates bird images from their corresponding sound information. Our proposed model includes a sound encoder in order to extract suitable feature representations from audio recordings, and then it generates bird images that corresponds to its calls using conditional generative adversarial networks (cGANs) with auxiliary classifiers. We demonstrate that our model produces better image generation results which outperforms other state-of-the-art methods in a similar context.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412721","National Research Foundation of Korea; Ministry of Education(grant numbers:NRF-2016R1D1A1B04933156); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412721","","Deep learning;Image synthesis;Birds;Generative adversarial networks;Feature extraction;Audio recording;Pattern recognition","audio recording;deep learning (artificial intelligence);feature extraction;image classification;image motion analysis;image representation","feature representation extraction;bird images;corresponding sound information;sound encoder;conditional generative adversarial networks;feature representation extraction;image generation results;sound-to-image generation;bird species;deep learning;S2I-Bird","","","","22","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Multi-OctConv: Reducing Memory Requirements in Image Generative Adversarial Networks","F. T. M; C. E. Torres","Departamento de Informática, Universidad Técnica Federico Santa María, Valparaíso, Chile; Departamento de Informática and Centro Científico Tecnológico de Valparaíso, Universidad Técnica Federico Santa María, Valparaíso, Chile","2020 39th International Conference of the Chilean Computer Science Society (SCCC)","9 Dec 2020","2020","","","1","8","Generative Adversarial Networks (GANs) for image generation of human faces have provided excellent results in recent years. However, we were able to identify a common problem among them: high memory usage in their training phase due to the convolutional encoder architecture used in these models. We address this issue by replacing the traditional convolutional layers in a model by what we call a Multi-Octave Convolution (M-OctConv) without modifying its architecture. An advantage of this method is that it can be easily combined with traditional memory reduction techniques, such as pruning. We evaluate our proposition on StarGAN model achieving up to 40% of memory usage reduction without affecting the quality of the generated images.","1522-4902","978-1-7281-8328-2","10.1109/SCCC51225.2020.9281213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9281213","GANs;image generation;memory usage","Convolutional codes;Training;Convolution;Image synthesis;Memory management;Generative adversarial networks;Neck","convolutional codes;convolutional neural nets;image coding;learning (artificial intelligence)","convolutional encoder architecture;convolutional layers;MultiOctave Convolution;M-OctConv;traditional memory reduction techniques;StarGAN model;memory usage reduction;MultiOctConv;image generative Adversarial Networks;image generation;human faces","","","","44","IEEE","9 Dec 2020","","","IEEE","IEEE Conferences"
"GAN-based models and applications","J. Li; T. Li; R. Lin; Q. Nie","Tianjin University of Technology, Tianjin, China; Beijing Normal University - Hong Kong Baptist University, United International College, Zhuhai, China; University of Rochester, Rochester, NY, United State; Chang'an University, Xi'an, China","2022 IEEE 5th International Conference on Information Systems and Computer Aided Education (ICISCAE)","4 Nov 2022","2022","","","848","852","GANs have achieved great success in image generation. Gan consists of two main parts, the generator and the discriminator, the generator tries to generate real samples that fool the discriminator, and the discriminator tries to distinguish between real samples and generated samples. Here The continuous improvement of simulation ability and generation ability under this kind of confrontation game can realize the generation of fake images. Since GAN appeared in 2014, articles of various types of GAN have been published in major journals and conferences, and the application of GAN in image generation (specifying image synthesis, text-to-image, image-to-image, video) and GAN in NLP and other areas of application are the most studied, and research in this area has demonstrated the great potential of using GANs in image synthesis. In this paper, the classical basic GAN model is introduced at first. Then, the paper analysis the differences and characteristics of the recent GAN-based models, and introduces the applications of GAN-based models in different tasks.","2770-663X","978-1-6654-8122-9","10.1109/ICISCAE55891.2022.9927647","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9927647","GAN models;deep learning;image synthesis","Training;Analytical models;Visualization;Image synthesis;Generative adversarial networks;Prediction algorithms;Generators","deep learning (artificial intelligence);image representation","image generation;fake images;image synthesis;text-to-image;image-to-image;GAN-based models;NLP;image-to-video","","","","22","IEEE","4 Nov 2022","","","IEEE","IEEE Conferences"
"Robustness Evaluation of Stacked Generative Adversarial Networks using Metamorphic Testing","H. Park; T. Waseem; W. Q. Teo; Y. Hwei Low; M. K. Lim; C. Yong Chong","School of Information Technology, Monash University Malaysia, Bandar Sunway, Selangor, Malaysia; School of Information Technology, Monash University Malaysia, Bandar Sunway, Selangor, Malaysia; School of Information Technology, Monash University Malaysia, Bandar Sunway, Selangor, Malaysia; School of Information Technology, Monash University Malaysia, Bandar Sunway, Selangor, Malaysia; School of Information Technology, Monash University Malaysia, Bandar Sunway, Selangor, Malaysia; School of Information Technology, Monash University Malaysia, Bandar Sunway, Selangor, Malaysia","2021 IEEE/ACM 6th International Workshop on Metamorphic Testing (MET)","12 Jul 2021","2021","","","1","8","Synthesising photo-realistic images from natural language is one of the challenging problems in computer vision. Over the past decade, a number of approaches have been proposed, of which the improved Stacked Generative Adversarial Network (StackGAN-v2) has proven capable of generating high resolution images that reflect the details specified in the input text descriptions. In this paper, we aim to assess the robustness and fault-tolerance capability of the StackGAN-v2 model by introducing variations in the training data. However, due to the working principle of Generative Adversarial Network (GAN), it is difficult to predict the output of the model when the training data are modified. Hence, in this work, we adopt Metamorphic Testing technique to evaluate the robustness of the model with a variety of unexpected training dataset. As such, we first implement StackGAN-v2 algorithm and test the pre-trained model provided by the original authors to establish a ground truth for our experiments. We then identify a metamorphic relation, from which test cases are generated. Further, metamorphic relations were derived successively based on the observations of prior test results. Finally, we synthesise the results from our experiment of all the metamorphic relations and found that StackGAN-v2 algorithm is susceptible to input images with obtrusive objects, even if it overlaps with the main object minimally, which was not reported by the authors and users of StackGAN-v2 model. The proposed metamorphic relations can be applied to other text-to-image synthesis models to not only verify the robustness but also to help researchers understand and interpret the results made by the machine learning models.","","978-1-6654-4464-4","10.1109/MET52542.2021.00008","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477682","Metamorphic testing;Stacked Generative Adversarial Network;metamorphic relations;robustness testing","Training;Shape;Natural languages;Training data;Predictive models;Generative adversarial networks;Prediction algorithms","computer vision;interactive systems;learning (artificial intelligence);program testing;realistic images","computer vision;improved Stacked Generative Adversarial Network;high resolution images;input text;fault-tolerance capability;StackGAN-v2 model;training data;Metamorphic Testing technique;unexpected training dataset;StackGAN-v2 algorithm;metamorphic relation;prior test results;input images;text-to-image synthesis models;machine learning models;robustness evaluation;Stacked Generative Adversarial networks;photo-realistic images;natural language","","2","","17","IEEE","12 Jul 2021","","","IEEE","IEEE Conferences"
"PTcomp: Post-Training Compression Technique for Generative Adversarial Networks","D. Tantawy; M. Zahran; A. G. Wassal","Computer Engineering Department, Faculty of Engineering, Cairo University, Giza, Egypt; Courant Institute, New York University, New York, NY, USA; Computer Engineering Department, Faculty of Engineering, Cairo University, Giza, Egypt","IEEE Access","2 Feb 2023","2023","11","","9763","9774","In a time of virtual spaces, the usage of generative adversarial networks is inevitable. Generative adversarial networks (GANs) are generative deep-learning models that can generate realistic data. GANs have been used in many applications like text-to-image, image-to-image, image synthesis, speech synthesis, etc. Its power lies in the diversity and novelty of the generated data. Despite their advantages, GANs are resource-hungry. GANs’ output resolution and high correlation make it more challenging to compress and fit on edge-devices storage and power budget. Hence, traditional compression techniques are not the best fit to use with GANs. Additionally, GANs training instability adds another dimension of difficulty. Therefore, compression techniques that require retraining are challenging for GANs. In this paper, we developed a weight clustering technique to compress GANs without the need for retraining, hence the name post-training compression technique (PTcomp). We also proposed a clustered-based pruning which adds more savings. Experiments on Cyclegan, Deep convolution gan (DCGAN), and Stargan using several datasets show the superiority of our technique against traditional post-training quantization. Our technique provides a 4x to 8x compression ratio with comparable quality to original models and 14% fewer mac operations due to pruning.","2169-3536","","10.1109/ACCESS.2023.3239786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025749","Compression;deep-learning;generative adversarial networks;post-training;clustering;pruning","Generative adversarial networks;Generators;Quantization (signal);Training;Mathematical models;Clustering algorithms;Tensors","","","","","","26","CCBY","25 Jan 2023","","","IEEE","IEEE Journals"
"Camouflage Generative Adversarial Network: Coverless Full-image-to-image Hiding","X. Liu; Z. Ma; X. Guo; J. Hou; G. Schaefer; L. Wang; V. Wang; H. Fang","School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Computer Science, Loughborough University, Loughborough, U.K.; School of Computer Science and Engineering, textitCentral South University, Changsha, China; Institute for Criminal Justice Studies, University of Portsmouth, Portsmouth, U.K.; Department of Computer Science, Loughborough University, Loughborough, U.K.","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","166","172","Image hiding, one of the most important data hiding techniques, is widely used to enhance cybersecurity when transmitting multimedia data. In recent years, deep learning-based image hiding algorithms have been designed to improve the embedding capacity whilst maintaining sufficient imperceptibility to malicious eavesdroppers. These methods can hide a full-size secret image into a cover image, thus allowing full-image-to-image hiding. However, these methods suffer from a trade-off challenge to balance the possibility of detection from the container image against the recovery quality of secret image. In this paper, we propose Camouflage Generative Adversarial Network (Cam-GAN), a novel two-stage coverless full-image-to-image hiding method named, to tackle this problem. Our method offers a hiding solution through image synthesis to avoid using a modified cover image as the image hiding container and thus enhancing both image hiding imperceptibility and recovery quality of secret images. Our experimental results demonstrate that Cam-GAN outperforms state-of-the-art full-image-to-image hiding algorithms on both aspects.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283054","National Natural Science Foundation of China; Natural Science Foundation of Hunan Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283054","image hiding;deep learning;generative adversarial network;image synthesis","Image quality;Image synthesis;Conferences;Containers;Generative adversarial networks;Decoding;Cybernetics","cryptography;data encapsulation;deep learning (artificial intelligence);image coding","multimedia data;deep learning;full-size secret image;image synthesis;image hiding container;image hiding imperceptibility;full-image-to-image hiding algorithms;camouflage generative adversarial network;coverless full-image-to-image hiding;embedding capacity;malicious eavesdroppers;Cam-GAN;cybersecurity enhancement","","1","","21","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"Automated Sewer Defects Detection Using Style-Based Generative Adversarial Networks and Fine-Tuned Well-Known CNN Classifier","Z. Situ; S. Teng; H. Liu; J. Luo; Q. Zhou","School of Civil and Transportation Engineering, Guangdong University of Technology, Guangzhou, China; School of Civil and Transportation Engineering, Guangdong University of Technology, Guangzhou, China; School of Civil and Transportation Engineering, Guangdong University of Technology, Guangzhou, China; School of Civil and Transportation Engineering, Guangdong University of Technology, Guangzhou, China; School of Civil and Transportation Engineering, Guangdong University of Technology, Guangzhou, China","IEEE Access","23 Apr 2021","2021","9","","59498","59507","Automated sewer defects detection has become an important trend for better management and maintenance of urban sewer systems. Deep learning technology has developed rapidly and offers an innovative solution for automated detection in engineering applications. However, insufficient data and unbalanced samples have proposed a big challenge to deep learning model training. This study adopts the state-of-the-art Style-based Generative Adversarial Networks (StyleGANs) model and compares the performances of its two variants in producing high-quality synthetic sewer defects images. Seven well-known CNN models are further fine-tuned and trained using the synthetic images for automated sewer defects detection to examine the effects of StyleGANs on augmenting the detection performance. Results show that both StyleGANs are efficient in producing high-quality images with various styles and high-level details for multiple types of sewer defects. Specifically, the StyleGAN2-Adaptive Discriminator Augmentation (StyleGAN2-ADA) with the aid of Freeze Discriminator (Freeze-D) yields the best model performance. Among the adopted CNN classifiers, Inception_v3 achieves the highest detection accuracy. The mean detection accuracy is 94% (with a specific accuracy of 99.7%, 97%, 95.3% and 84% for tree root, residential wall, disjoint and obstacle, respectively) and confirms the reliability of the StyleGANs' performance. The study shows that StyleGANs provide a promising method to alleviate the limited and uneven dataset problem and can improve the deep learning model performance.","2169-3536","","10.1109/ACCESS.2021.3073915","National Natural Science Foundation of China(grant numbers:51809049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406802","Automated detection;image synthesis;sewer defects;well-known CNN classifiers;StyleGANs","Training;Generators;Generative adversarial networks;Deep learning;Transfer learning;Image quality;Computational modeling","convolutional neural nets;deep learning (artificial intelligence);fault diagnosis;image classification;sanitary engineering;structural engineering computing","CNN models;style-based generative adversarial networks model;high-quality synthetic sewer defects images;deep learning model training;automated detection;urban sewer systems;automated sewer defects detection;deep learning model performance;mean detection accuracy;StyleGAN2-Adaptive Discriminator Augmentation","","9","","72","CCBY","19 Apr 2021","","","IEEE","IEEE Journals"
"DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis","M. Tao; H. Tang; F. Wu; X. Jing; B. -K. Bao; C. Xu","Nanjing University of Posts and Telecommunications; CVL, ETH Zürich; Nanjing University of Posts and Telecommunications; Wuhan University; Nanjing University of Posts and Telecommunications; NLPR, Institute of Automation, CAS","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","16494","16504","Synthesizing high-quality realistic images from text descriptions is a challenging task. Existing text-to-image Generative Adversarial Networks generally employ a stacked architecture as the backbone yet still remain three flaws. First, the stacked architecture introduces the entanglements between generators of different image scales. Second, existing studies prefer to apply and fix extra networks in adversarial learning for text-image semantic consistency, which limits the supervision capability of these networks. Third, the cross-modal attention-based text-image fusion that widely adopted by previous works is limited on several special image scales because of the computational cost. To these ends, we propose a simpler but more effective Deep Fusion Generative Adversarial Networks (DF-GAN). To be specific, we propose: (i) a novel one-stage text-to-image backbone that directly synthesizes high-resolution images without entanglements between different generators, (ii) a novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty and One-Way Output, which enhances the text-image semantic consistency without introducing extra networks, (iii) a novel deep text-image fusion block, which deepens the fusion process to make a full fusion between text and visual features. Compared with current state-of-the-art methods, our proposed DF-GAN is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance on widely used datasets. Code is available at https://github.com/tobran/DF-GAN.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01602","Natural Science Foundation of Jiangsu Province(grant numbers:bk20200037,BK20210595); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879122","Vision + language; Image and video synthesis and generation","Visualization;Computer vision;Codes;Semantics;Computer architecture;Generative adversarial networks;Generators","feature extraction;image fusion;image matching;image resolution;learning (artificial intelligence);sensor fusion;text analysis","DF-GAN;text-to-image synthesis;high-quality realistic images;text descriptions;Existing text-to-image Generative Adversarial Networks;stacked architecture;different image scales;extra networks;text-image semantic consistency;cross-modal attention-based text-image fusion;special image scales;simpler but more effective Deep Fusion Generative Adversarial Networks;one-stage text-to-image backbone;high-resolution images;different generators;novel deep text-image fusion block;text-matching images","","5","","60","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Fundamentals and Challenges of Generative Adversarial Networks for Image-based Applications","V. L. T. De Souza; B. A. D. Marques; J. P. Gois","Universidade Federal do ABC, Santo Andrée, Brazil; Universidade Federal do ABC, Santo Andrée, Brazil; Universidade Federal do ABC, Santo Andrée, Brazil","2022 35th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)","26 Dec 2022","2022","1","","308","313","Significant advances in image-based applications have been achieved in recent years, many of which are arguably due to recent developments in Generative Adversarial Networks (GANs). Although the continuous improvement in the architectures of GAN has significantly increased the quality of synthetic images, this is not without challenges such as training stability and convergence issues, to name a few. In this work, we present the fundamentals and notable architectures of GANs, especially for image-based applications. We also discuss relevant issues such as training problems, diversity generation, and quality assessment (metrics).","2377-5416","978-1-6654-5385-1","10.1109/SIBGRAPI55357.2022.9991776","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991776","Generative Adversarial Network;image manipulation;deep image synthesis;deep neural network","Training;Measurement;Graphics;Neural networks;Generative adversarial networks;Quality assessment;Convergence","image resolution;learning (artificial intelligence);neural nets","GANs;generative adversarial networks;image-based applications;synthetic images","","","","43","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"CorGAN: Context aware Recurrent Generative Adversarial Network for Medical Image Generation","Z. Qiao; Z. Qian; H. Tang; G. Gong; Y. Yin; C. Huang; W. Fan","IQVIA, Analytics Center of Excellence; Tencent America, Palo Alto, CA, USA; Tencent America, Palo Alto, CA, USA; Shandong Cancer Hospital and Institute, Jinan, China; Shandong Cancer Hospital and Institute, Jinan, China; Tencent America, Palo Alto, CA, USA; Tencent America, Palo Alto, CA, USA","2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","13 Jan 2021","2020","","","1100","1103","Multi-modal imaging plays a critical role in various clinical applications. However, due to the associated high cost and potential risk, the acquisition of multi-modal images could be limited. To address this issue, many cross-modality image synthesis methods have been proposed. The state-of-the-art methods are mainly based on traditional convolutional generative adversarial networks (GANs) for generating target images. In 3D medical image synthesis, an open problem is how to efficiently exploit the spatial correlations of the 3D image sequence to resolve the inter-slice discontinuity and unevenness artifacts. In this paper, we propose a novel Context aware Residual Recurrent Generative Adversarial Network (short for CorGAN) for sequential medical image generation, which jointly exploits the spatial dependencies of the sequences as well as the peer image generation with GANs. Experimental results show the robustness and accuracy of our method, which outperforms the-state-of-the-art methods in synthesizing target 3D images from the corresponding source images.","","978-1-7281-6215-7","10.1109/BIBM49941.2020.9313470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9313470","","Three-dimensional displays;Convolution;Computed tomography;Two dimensional displays;Medical diagnostic imaging;Generative adversarial networks;Magnetic resonance imaging","convolutional neural nets;image registration;image sequences;medical image processing;recurrent neural nets;ubiquitous computing","CorGAN;multimodal imaging;associated high cost;multimodal images;cross-modality image synthesis methods;traditional convolutional generative adversarial networks;target images;3D medical image synthesis;3D image sequence;inter-slice discontinuity;unevenness artifacts;sequential medical image generation;peer image generation;target 3D images;source images;context aware residual recurrent generative adversarial network","","1","","11","IEEE","13 Jan 2021","","","IEEE","IEEE Conferences"
"Aggregated Contextual Transformations for High-Resolution Image Inpainting","Y. Zeng; J. Fu; H. Chao; B. Guo","School of Computer Science and Engineering, Sun Yat-Sen University, 26469 Guangzhou, Guangdong, China; multimedia, Microsoft Research Asia, 216064 Beijing, Beijing, China, 100080; Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education of the People's Republic of China, 12543 Guangzhou, Guangdong, China; 5F Beijing Sigma Center, Microsoft Research Asia, Beijing, Beijing, China, 100080","IEEE Transactions on Visualization and Computer Graphics","","2022","PP","99","1","1","Image inpainting that completes large free-form missing regions in images is a promising yet challenging task. State-of-the-art approaches have achieved significant progress by taking advantage of generative adversarial networks (GAN). However, these approaches can suffer from generating distorted structures and blurry textures in high-resolution images (e.g.,512 512). The challenges mainly drive from (1) image content reasoning from distant contexts, and (2) fine-grained texture synthesis for a large missing region. To overcome these two challenges, we propose an enhanced GAN-based model, named Aggregated COntextual-Transformation GAN (AOT-GAN), for high-resolution image inpainting. Specifically, to enhance context reasoning, we construct the generator of AOT-GAN by stacking multiple layers of a proposed AOT block. The AOT blocks aggregate contextual transformations from various receptive fields, allowing to capture both informative distant image contexts and rich patterns of interest for context reasoning. For improving texture synthesis, we enhance the discriminator of AOT-GAN by training it with a tailored mask-prediction task. Such a training objective forces the discriminator to distinguish the detailed appearances of real and synthesized patches, and in turn facilitates the generator to synthesize clear textures. Extensive comparisons on Places2, the most challenging benchmark with 1.8 million high-resolution images of 365 complex scenes, show that our model outperforms the state-of-the-art. A user study including more than 30 subjects further validates the superiority of AOT-GAN. We further evaluate the proposed AOT-GAN in practical applications, e.g., logo removal, face editing, and object removal. Results show that our model achieves promising completions in the real world.","1941-0506","","10.1109/TVCG.2022.3156949","National Natural Science Foundation of China(grant numbers:61672548,U1611461); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729564","Image synthesis;image inpainting;object removal;generative adversarial networks (GAN)","Generators;Generative adversarial networks;Cognition;Training;Task analysis;Filling;Convolution","","","","6","","","IEEE","7 Mar 2022","","","IEEE","IEEE Early Access Articles"
"Transformation GAN for Unsupervised Image Synthesis and Representation Learning","J. Wang; W. Zhou; G. -J. Qi; Z. Fu; Q. Tian; H. Li","Department of EEIS, CAS Key Laboratory of GIPAS, University of Science and Technology of China; Department of EEIS, CAS Key Laboratory of GIPAS, University of Science and Technology of China; Futurewei Technologies; Department of EST, University of Science and Technology of China; Department of EEIS, CAS Key Laboratory of GIPAS, University of Science and Technology of China; Department of EEIS, CAS Key Laboratory of GIPAS, University of Science and Technology of China","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","469","478","Generative Adversarial Networks (GAN) have shown promising performance in image synthesis and unsupervised learning (USL). In most cases, however, the representations extracted from unsupervised GAN are usually unsatisfactory in other computer vision tasks. By using conditional GAN (CGAN), this problem could be solved to some extent, but the main drawback of such models is the necessity for labeled data. To improve both image synthesis quality and representation learning performance under the unsupervised setting, in this paper, we propose a simple yet effective Transformation Generative Adversarial Networks (TrGAN). In our approach, instead of capturing the joint distribution of image-label pairs p(x,y) as in conditional GAN, we try to estimate the joint distribution of transformed image t(x) and transformation t. Specifically, given a randomly sampled transformation t, we train the discriminator to give an estimate of input transformation, while following the adversarial training scheme of the original GAN. In addition, intermediate feature matching as well as feature-transform matching methods are introduced to strengthen the regularization on the generated features. To evaluate the quality of both generated samples and extracted representations, extensive experiments are conducted on four public datasets. The experimental results on the quality of both the synthesized images and the extracted representations demonstrate the effectiveness of our method.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156940","","Gallium nitride;Feature extraction;Generative adversarial networks;Generators;Training;Task analysis;Image generation","computer vision;feature extraction;image capture;image matching;image representation;image sampling;neural nets;transforms;unsupervised learning","Transformation Generative Adversarial Networks;adversarial training scheme;randomly sampled transformation;image-label pairs;representation learning performance;image synthesis;conditional GAN;computer vision tasks;unsupervised GAN;unsupervised learning;unsupervised image synthesis;Transformation GAN;extracted representations;feature-transform matching methods","","8","","42","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"SatGAN: Satellite Image Generation using Conditional Adversarial Networks","M. Shah; M. Gupta; P. Thakkar","Computer Science and Engineering, Nirma University, Ahmedabad, India; Computer Science and Engineering, Nirma University, Ahmedabad, India; Computer Science and Engineering, Nirma University, Ahmedabad, India","2021 International Conference on Communication information and Computing Technology (ICCICT)","12 Aug 2021","2021","","","1","6","Mapping accurate satellite images from street view is a challenging image-to-image translation task. Recent advances in Generative Adversarial Networks (GANs) have shown promising results in image-to-image translation. Pix2Pix is a generalized framework for image-to-image translation which uses Conditional Adversarial Networks. It has performed well on a large number of datasets. However, it uses a simple pixel-level reconstruction loss due to which the output suffers distortion in some cases. In this paper, we propose SatGAN, which is based on Pix2Pix but adds the perceptual reconstruction loss with the pixel-level reconstruction loss to produce colourful and blur free images. The perceptual reconstruction loss forces the Generator to generate samples having a similar feature representation with that of the ground truth. We experimentally prove that SatGAN produces both qualitatively and quantitatively better results than Pix2Pix for street-view to satellite dataset.","","978-1-6654-0430-3","10.1109/ICCICT50803.2021.9510104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9510104","Conditional Adversarial Network;Image-to-Image translation;Perceptual reconstruction loss;Pix2Pix","Computers;Satellites;Image synthesis;Generative adversarial networks;Distortion;Generators;Task analysis","cartography;image colour analysis;image reconstruction;image restoration;neural nets;satellite communication","Pix2Pix;simple pixel-level reconstruction loss;SatGAN;perceptual reconstruction loss;satellite image generation;street view;conditional adversarial networks;accurate satellite images mapping;image-to-image translation task;generative adversarial networks;pixel-level reconstruction loss;blur free images;colourful images;feature representation;ground truth","","","","37","IEEE","12 Aug 2021","","","IEEE","IEEE Conferences"
"Image Generation from Brainwaves using Dual Generative Adversarial Training","Y. -C. Wang; Y. -J. Chen; B. -H. Chen",Yuan Ze University; Yuan Ze University; Yuan Ze University,"2022 IEEE 11th Global Conference on Consumer Electronics (GCCE)","18 Jan 2023","2022","","","484","485","Representing content of brainwaves captured through a non-invasive EEG device is of practical importance to various diagnostic applications. Although existing generative adversarial networks can obtain decent results in certain fields of neuroscience, performance declines significantly when representing content of brainwaves due to the insufficiency of real brainwaves. This lets us introduce a dual generative adversarial training paradigm to learn a smooth transition between brainwave and target image distributions. Our experiments demonstrate that our training approach outperforms the state-of-the-art training strategy on three benchmark datasets.","2378-8143","978-1-6654-9232-4","10.1109/GCCE56475.2022.10014299","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10014299","Brainwaves;image generation;generative adversarial training","Training;Performance evaluation;Neuroscience;Image synthesis;Benchmark testing;Generative adversarial networks;Electroencephalography","electroencephalography;learning (artificial intelligence);medical signal processing;neurophysiology","brainwave;brainwaves;decent results;diagnostic applications;dual generative adversarial training paradigm;generative adversarial networks;image generation;noninvasive EEG device;representing content;state-of-the-art training strategy;target image distributions;training approach","","","","8","IEEE","18 Jan 2023","","","IEEE","IEEE Conferences"
"An Adaptive Control Algorithm for Stable Training of Generative Adversarial Networks","X. Ma; R. Jin; K. -A. Sohn; J. -Y. Paik; T. -S. Chung","Department of Computer Engineering, Ajou University, Suwon, South Korea; School of Computer Science and Technology, Tianjin Polytechnic University, Tianjin, China; Department of Computer Engineering, Ajou University, Suwon, South Korea; School of Computer Science and Technology, Tianjin Polytechnic University, Tianjin, China; Department of Computer Engineering, Ajou University, Suwon, South Korea","IEEE Access","27 Dec 2019","2019","7","","184103","184114","Generative adversarial networks (GANs) have shown significant progress in generating highquality visual samples, however they are still well known both for being unstable to train and for the problem of mode collapse, particularly when trained on data collections containing a diverse set of visual objects. In this paper, we propose an Adaptive k-step Generative Adversarial Network (Ak-GAN), which is designed to mitigate the impact of instability and saturation in the original by dynamically adjusting the ratio of the training steps of both the generator and discriminator. To accomplish this, we track and analyze stable training curves of relatively narrow datasets and use them as the target fitting lines when training more diverse data collections. Furthermore, we conduct experiments on the proposed procedure using several optimization techniques (e.g., supervised guiding from previous stable learning curves with and without momentum) and compare their performance with that of state-of-the-art models on the task of image synthesis from datasets consisting of diverse images. Empirical results demonstrate that Ak-GAN works well in practice and exhibits more stable behavior than regular GANs during training. A quantitative evaluation has been conducted on the Inception Score (IS) and the relative inverse Inception Score (RIS); compared with regular GANs, the former has been improved by 61% and 83%, and the latter by 21% and 60%, on the CelebA and the Anime datasets, respectively.","2169-3536","","10.1109/ACCESS.2019.2960461","National Research Foundation of Korea; Ministry of Education(grant numbers:2019R1F1A1058548); Natural Science Foundation of Tianjin City(grant numbers:18JCYBJC44000); Tianjin Science and Technology Program(grant numbers:19PTZWHZ00020); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936350","Generative adversarial networks;image generation;adaptive algorithm;mode collapse","Training;Gallium nitride;Generators;Generative adversarial networks;Adaptation models;Adaptive control;Convergence","adaptive control;image processing;learning (artificial intelligence);neural nets","relative inverse inception score;inception score;high-quality visual samples;adaptive k-step generative adversarial network;training curves;Ak-GAN;learning curves;data collections;training steps;visual objects;adaptive control algorithm","","4","","29","CCBY","18 Dec 2019","","","IEEE","IEEE Journals"
"Unsupervised Facial Image Synthesis Using Two-Discriminator Adversarial Autoencoder Network","X. Wu; J. Shao; D. Zhang; J. Chen","Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China; Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China; Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China; Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China","2019 IEEE International Conference on Multimedia and Expo (ICME)","5 Aug 2019","2019","","","1162","1167","Recent years have witnessed the unprecedented success in single image synthesis by the means of convolutional neural networks (CNNs). High-level synthesis of facial image such as expression translation and attribute swap is still a challenging task due to high non-linearity. Previous methods suffer from the limitations that being unable to transfer multiple face attributes simultaneously, or incapability of transferring an attribute to another by a continuously changing way. To address this problem, we propose a two-discriminator adversarial autoencoder network (TAAN). The latent-discriminator is trained to disentangle an input image from its original facial attribute, while the pixel-discriminator is trained to make the output image attach to the target facial attribute. By controlling the attribute values, we can choose which and how much a specific attribute can be perceivable in the generated image. Quantitative and qualitative evaluations are conducted on the celebA and KDEF datasets, and the comparison with the state-of-the-art methods shows the competency of our proposed TAAN.","1945-788X","978-1-5386-9552-4","10.1109/ICME.2019.00203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784741","Adversarial autoencoder network;facial image synthesis;unsupervised learning","Generative adversarial networks;Image synthesis;Decoding;Face;Facial features;Task analysis;Convolution","convolutional neural nets;face recognition;feature extraction;unsupervised learning","unsupervised facial image synthesis;two-discriminator adversarial autoencoder network;single image synthesis;convolutional neural networks;high-level synthesis;pixel-discriminator;target facial attribute;CNNs;multiple face attributes;TAAN;qualitative evaluations;quantitative evaluations;KDEF datasets","","1","","32","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"Text-Guided Sketch-to-Photo Image Synthesis","U. Osahor; N. M. Nasrabadi","Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV, USA","IEEE Access","23 Sep 2022","2022","10","","98278","98289","We propose a text-guided sketch-to-image synthesis model that semantically mixes style and content features from the latent space of an inverted Generative Adversarial Network (GAN). Our goal is to synthesize plausible images from human facial sketches and their respective text descriptions. In our approach, we adapted a generative model termed Contextual GAN (CT-GAN) that efficiently encodes visual-linguistic semantic features pre-trained on over 400 million text-image pairs at different resolutions along the model. Also, we introduced an intermediate mapping network called c-Map that combines textual and visual-based features to a disentangled latent space  $\mathcal {W{+}}$  for better feature matching. Furthermore to maximise the computational performance of our model, we implemented a linear-based attention scheme along the pipeline of our model to eliminate the drawbacks of inefficient attention modules that are quadratic in complexity. Finally, the hierarchical setting of our model ensures that textual, style and content features are synthesised based on their unique fine grained details, which result in visually appealing images.","2169-3536","","10.1109/ACCESS.2022.3206771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893101","Contextual GAN (CT-GAN);generative adversarial network (GAN);text-guided sketch-to-image synthesis","Adaptation models;Computational modeling;Semantics;Generative adversarial networks;Generators;Image synthesis;Context modeling;Text processing","face recognition;feature extraction;image classification;image matching;learning (artificial intelligence);neural nets;object detection;text analysis","visual-linguistic semantic features;text-image pairs;intermediate mapping network;disentangled latent space;feature matching;linear-based attention scheme;content features;visually appealing images;text-guided sketch-to-photo image synthesis;text-guided sketch-to-image synthesis model;plausible images;human facial sketches;respective text descriptions;generative model;contextual GAN;CT-GAN;inverted generative adversarial network;semantic style mixing","","","","52","CCBY","15 Sep 2022","","","IEEE","IEEE Journals"
"Generative Adversarial Networks Using Neural Architecture Search for Semantic Image Segmentation","V. V. V. Ganepola; T. Wirasingha","Department of Computing, Informatics Institute of Technology, Colombo, Sri Lanka; Department of Computing, Informatics Institute of Technology, Colombo, Sri Lanka","2021 IEEE 4th International Conference on Big Data and Artificial Intelligence (BDAI)","20 Aug 2021","2021","","","236","241","Semantic image segmentation is a crucial task in various fields that use computer-vision based applications. Generative Adversarial Networks (GANs) are attracting widespread interest in the data science community for their prowess in image feature recognition due to their adversarial nature of training. Neural Architecture Search (NAS) is known as the process of obtaining a neural architectural schema that performs the best for a particular task. NAS has been applied in GANs, and it achieved striking success compared to human-designed architectures in conditional and unconditional image generation and GAN-compression. Our research was inspired by the success of NAS applied in GANs. This paper proposes a novel approach for NAS in GANs for semantic image segmentation. After extensive research on related works, the architecture of the Pix2Pix GAN variant was selected for the proposed approach. The architecture of the Pix2Pix GAN consists of a U-Net as the Generator and a PatchGAN classifier as the Discriminator. The NAS component is searched for U-Net architectures using PASCAL VOC 2012 dataset. The NAS component is adapted from using the NAS-Unet research proposed by Weng et al. in 2019. The NAS searched architecture was used as the Generator of the proposed GAN by transferring the searched architecture from the PASCAL VOC 2012 dataset to the Cityscapes dataset. To determine the success of the proposed approach, quantitative analysis was performed with Mean Pixel Accuracy (MPA) and mean Intersection over Union (mIoU) metrics. Several experiments were done on the Cityscape validation set and achieved 81.73 MPA and 71.91 mIoU. The proposed approach outperformed several NAS in semantic segmentation approaches and GANs in semantic segmentation approaches. This study is a preliminary attempt to apply NAS for semantic segmentation using GANs. Further, this research has raised many possible areas in need of further investigation.","","978-1-6654-1270-4","10.1109/BDAI52447.2021.9515223","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9515223","semantic segmentation;generative adversarial networks;neural architecture search","Training;Measurement;Image segmentation;Statistical analysis;Image synthesis;Semantics;Computer architecture","computer vision;convolutional neural nets;feature extraction;image classification;image segmentation;neural net architecture","neural architecture search;semantic image segmentation;computer vision;generative adversarial networks;image feature recognition;Pix2Pix GAN;U-net architectures;PASCAL VOC 2012 dataset;NAS-Unet;data science community;PatchGAN classifier;discriminator;generator;Cityscapes dataset;quantitative analysis;mean pixel accuracy;MPA;mean intersection over union;mIoU","","","","36","IEEE","20 Aug 2021","","","IEEE","IEEE Conferences"
"Facial Age Progression using Conditional Generative Adversarial Network with Heritable Visual Features","P. Siritanawan; H. Ichikawa; K. Kotani","School of Information Science, Japan Advanced Institute of Science and Technology (JAIST), Nomi, Japan; School of Information Science, Japan Advanced Institute of Science and Technology (JAIST), Nomi, Japan; School of Information Science, Japan Advanced Institute of Science and Technology (JAIST), Nomi, Japan","2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","6 Jan 2022","2021","","","1449","1454","Age progression of face images has been an important tool to search for missing children. Many studies on age progression were recently conducted by conditional Generative Adversarial Networks (cGAN) based methods. However, these methods cannot estimate facial aging from a child’s face in the early childhood stage, which exhibits drastic facial shape changes over time. Thus, the problem of age progression from young children remains challenging in the field. In this study, we propose a cGAN based two-stage age progression model considering heritable facial features from parents and child to generate candidates for age-progressed face images from a young child’s face image.","2577-1655","978-1-6654-4207-7","10.1109/SMC52423.2021.9659067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659067","","Visualization;Image resolution;Correlation;Shape;Image synthesis;Generative adversarial networks;Skeleton","ageing;face recognition;paediatrics","two-stage age progression model;heritable facial features;age-progressed face images;young child;facial age progression;conditional Generative Adversarial network;heritable visual features;conditional Generative Adversarial Networks based methods;facial aging;drastic facial shape changes","","","","14","IEEE","6 Jan 2022","","","IEEE","IEEE Conferences"
"Image Outpainting using Wasserstein Generative Adversarial Network with Gradient Penalty","A. Nair; J. Deshmukh; A. Sonare; T. Mishra; R. Joseph","dept. of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India; dept. of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India; dept. of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India; dept. of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India; dept. of Computer Engineering, Vivekanand Education Society’s Institute of Technology, Mumbai, India","2022 6th International Conference on Computing Methodologies and Communication (ICCMC)","13 Apr 2022","2022","","","1248","1255","With advancements in AI technology, machines can perform or even mimic tasks that humans can do. One of its achievements can be seen in image generation, one of it being Image Inpainting (completion). In Image Inpainting, AI is used to complete missing data in an image. This is an extensive field of research, but its contemporary field, i.e. image outpainting, is not a well-researched one. In Image Outpainting (extrapolation) the image is extended beyond its borders. This is similar to our brain picturing the whole image of an object that is partially seen through a gap. This task can be achieved by using Generative Adversarial Networks (GANs). Compared to Inpainting, the biggest challenge is to achieve spatial correlation between the generated image and the ground truth image. Also, the process of overcoming this challenge is also sometimes affected because of the training instability of GAN. With the help of Wasserstein GAN (WGAN), the above issue can be solved. So, a model is proposed based on the Wasserstein GAN with Gradient Penalty (WGAN-GP) algorithm and deep convolutional neural networks for image outpainting using a dataset on natural images. From this proposed model it is found that the results of WGAN-GP algorithm was better than GAN algorithm in various aspects.","","978-1-6654-1028-1","10.1109/ICCMC53470.2022.9753713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9753713","Wasserstein Generative Adversarial Network with Gradient Penalty(WGAN-GP);Image outpainting;Image inpainting;Generative Adversarial Network (GAN);deep convolutional neural network;extrapolation","Training;Extrapolation;Correlation;Image synthesis;Neural networks;Generative adversarial networks;Generators","convolution;edge detection;image denoising;image restoration;image texture;learning (artificial intelligence);neural nets","image outpainting;Wasserstein Generative Adversarial network;image generation;Image Inpainting;Generative Adversarial Networks;ground truth image;Wasserstein GAN;Gradient Penalty algorithm;natural images","","","","13","IEEE","13 Apr 2022","","","IEEE","IEEE Conferences"
"The comparison between Conditional Generative Adversarial Nets and Deep Convolutional Generative Adversarial Network, and its GUI-related application","X. Li; Z. Zhang","Shanghai University, Shanghai, China; Shanghai University of Engineering Science, Shanghai, China","2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)","3 Feb 2022","2021","","","601","609","In recent years, Generative Adversarial Nets (GAN), Conditional Generative Adversarial Nets (CGAN), and Deep convolutional generative adversarial networks (DCGAN) have generally been well-received in Artificial Intelligence (AI) industry. This paper first briefly introduces the fundamentals of GAN, CGAN, and DCGAN. Next, we focus on comparing two improved GAN variants– CGAN and DCGAN. To be specific, we train them with certain architectural constraints on two datasets – MNIST and Animation images. We show convincing evidence that DCGAN outperforms CGAN in terms of processing image datasets to a large extent. Additionally, we make a Graphical User Interface (GUI), enabling users to choose face photos with different tags generated by DCGAN.","","978-1-6654-2709-8","10.1109/ICBASE53849.2021.00119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696015","CGAN;DCGAN;GUI design","Training;Industries;Fluctuations;Image synthesis;Generative adversarial networks;Market research;Artificial intelligence","convolutional neural nets;deep learning (artificial intelligence);graphical user interfaces","GUI-related application;deep convolutional generative adversarial networks;DCGAN;artificial intelligence;graphical user interface;GUI;face photos","","","","9","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Total Generate: Cycle in Cycle Generative Adversarial Networks for Generating Human Faces, Hands, Bodies, and Natural Scenes","H. Tang; N. Sebe","Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, Italy; Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, Italy","IEEE Transactions on Multimedia","9 Jun 2022","2022","24","","2963","2974","We propose a novel and unified Cycle in CycleGenerative Adversarial Network (C2GAN) for generating human faces, hands, bodies, and natural scenes. Our proposed C2GAN is a cross-modal model exploring the joint exploitation of the input image data and guidance data in an interactive manner. C2GAN contains two different generators, i.e., an image-generation generator and a guidance-generation generator. Both generators are mutually connected and trained in an end-to-end fashion and explicitly form three cycled subnets, i.e., one image generation cycle and two guidance generation cycles. Each cycle aims at reconstructing the input domain and simultaneously produces a useful output involved in the generation of another cycle. In this way, the cycles constrain each other implicitly providing complementary information from both image and guidance modalities and bringing an extra supervision gradient across the cycles, facilitating a more robust optimization of the whole model. Extensive results on four guided image-to-image translation subtasks demonstrate that the proposed C2GAN is effective in generating more realistic images compared with state-of-the-art models.","1941-0077","","10.1109/TMM.2021.3091847","EU H2020 AI4Media(grant numbers:951911); Italy-China collaboration(grant numbers:TALENT:2018YFE0118400); PRIN project PREVUE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464730","GANs;cycle in cycle;cycle consistency;guided image-to-image translation","Generators;Image synthesis;Task analysis;Generative adversarial networks;Optimization;Image reconstruction;Skeleton","gradient methods;image motion analysis;image reconstruction;neural nets;optimisation;realistic images;solid modelling","human faces;natural scenes;C2GAN;cross-modal model;input image data;guidance data;image-generation generator;guidance-generation generator;cycled subnets;image generation cycle;guidance generation cycles;guidance modalities;guided image-to-image translation subtasks;cycle in cycle generative adversarial networks","","2","","63","IEEE","24 Jun 2021","","","IEEE","IEEE Journals"
"Learning Layout and Style Reconfigurable GANs for Controllable Image Synthesis","W. Sun; T. Wu","Department of Electrical and Computer Engineering and the Visual Narrative Initiative, North Carolina State University, Raleigh, NC, USA; Department of Electrical and Computer Engineering and the Visual Narrative Initiative, North Carolina State University, Raleigh, NC, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","4 Aug 2022","2022","44","9","5070","5087","With the remarkable recent progress on learning deep generative models, it becomes increasingly interesting to develop models for controllable image synthesis from reconfigurable structured inputs. This paper focuses on a recently emerged task, layout-to-image, whose goal is to learn generative models for synthesizing photo-realistic images from a spatial layout (i.e., object bounding boxes configured in an image lattice) and its style codes (i.e., structural and appearance variations encoded by latent vectors). This paper first proposes an intuitive paradigm for the task, layout-to-mask-to-image, which learns to unfold object masks in a weakly-supervised way based on an input layout and object style codes. The layout-to-mask component deeply interacts with layers in the generator network to bridge the gap between an input layout and synthesized images. Then, this paper presents a method built on Generative Adversarial Networks (GANs) for the proposed layout-to-mask-to-image synthesis with layout and style control at both image and object levels. The controllability is realized by a proposed novel Instance-Sensitive and Layout-Aware Normalization (ISLA-Norm) scheme. A layout semi-supervised version of the proposed method is further developed without sacrificing performance. In experiments, the proposed method is tested in the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained.","1939-3539","","10.1109/TPAMI.2021.3078577","National Science Foundation(grant numbers:IIS-1909644); ARO(grant numbers:W911NF1810295); National Science Foundation(grant numbers:IIS-1822477,CMMI-2024688,IUSE-2013451); DHHS-ACL(grant numbers:90IFDV0017-01-00); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427066","Image synthesis;layout-to-image;layout-to-mask-to-image;deep generative learning;GAN;ISLA-norm","Layout;Image synthesis;Generators;Snow;Task analysis;Training;Fasteners","genomics;learning (artificial intelligence);realistic images","style reconfigurable GANs;controllable image synthesis;remarkable recent progress;deep generative models;reconfigurable structured inputs;recently emerged task;layout-to-image;photo-realistic images;spatial layout;object bounding boxes;image lattice;style codes;structural appearance variations;object masks;layout-to-mask component;generator network;Generative Adversarial Networks;layout-to-mask-to-image synthesis;style control;controllability;Layout-Aware Normalization scheme;layout semisupervised version","Algorithms;Deep Learning;Image Processing, Computer-Assisted","9","","64","IEEE","10 May 2021","","","IEEE","IEEE Journals"
"Wasserstein Based EmoGANs+","W. S. S. Khine; P. Siritanawan; K. Kotani","School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan; School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan; School of Information Science, Japan Advanced Institute of Science and Technology, Ishikawa, Japan","2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","18 Oct 2021","2021","","","1","8","Nowadays, numerous generative models are powerful and becoming popular for image synthesis because their generated images are more and more similar to the actual images, especially, Generative Adversarial Networks (GANs) and their variants. Such generative models are helpful to cover the shortage of datasets in various areas with their impressive realistic looking generated images. In this paper, we proposed the EmoGANs+ model to create the compound facial expressions images with stable adversarial training by using Wasserstein loss. The proposed methodology consists of three steps: preprocessing, image generation with proposed EmoGANs+, and lastly, evaluation. Our experiments are conducted on the Multimedia Understanding Group (MUG) facial expression dataset, Extended Cohn-Kanade dataset (CK+), and Japanese Female Facial Expressions (JAFFE) dataset. Our proposed model provides high feature similarity scores between the features of generated images and ground-truth compound facial expressions images.","","978-1-6654-4923-6","10.1109/ICIEVicIVPR52578.2021.9564216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564216","Emotion Generative Adversarial Networks;Compound Facial Expressions;Compound Emotions","Training;Image synthesis;Generative adversarial networks;Generators;Pattern recognition;Compounds;Facial features","emotion recognition;face recognition;feature extraction;neural nets","Japanese Female Facial Expressions dataset;ground-truth compound facial expressions images;image synthesis;generative adversarial networks;stable adversarial training;Wasserstein loss;image generation;Multimedia Understanding Group facial expression dataset;Extended Cohn-Kanade dataset;Wasserstein based EmoGANs+ model;generative models;impressive realistic looking generated images;MUG;JAFFE;feature similarity scores","","","","22","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Research on Cartoon Face Generation Based on CycleGAN Assisted with Facial Landmarks","K. Ma; X. Wang","Liaoning Petrochemical University, College of artificial intelligence and software engineering, Fushun, Liaoning, China; Liaoning Petrochemical University, College of information and control engineering, Fushun, Liaoning, China","2022 International Conference on Cyber-Physical Social Intelligence (ICCSI)","16 Dec 2022","2022","","","356","361","Turn real faces into cartoon faces is a topic of style transfer, and style transfer is a hot topic in the application of generative adversarial networks in image. CycleGAN is one of generative adversarial networks. It has obvious universal applicability, and has a good transformation effect on various types of style transfer. But to the facial style transfer, it only focuses on the transformation of the whole face, and it is not ideal for the transformation of the details of the facial features. How can this situation be improved? In this paper, we use facial landmarks to assist the transformation of facial features. In the beginning, we use stacked hourglass networks to detection and capture landmarks of real faces. And then, use them to assist cartoon faces generation. In view of the fact that the hourglass network has its own advantages in feature extraction, we use it to replace the generator structure of the original CycleGAN for transformation. And in order to avoid the Checkerboard Artifacts and ensure the quality of image generation, we use bilinear interpolation in the upsampling part of the generator to replace the deconvolution of the original generator and the nearest interpolation of the hourglass network. Experiments show that these practices have good results in optimizing conversion performance and improving image quality.","","978-1-6654-9835-7","10.1109/ICCSI55536.2022.9970645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9970645","style transfer;CycleGAN;stacked hourglass networks facial landmark","Image quality;Interpolation;Deconvolution;Image synthesis;Generative adversarial networks;Feature extraction;Generators","face recognition;feature extraction;image resolution;image sampling;interpolation","capture landmarks;cartoon face generation;cartoon faces generation;CycleGAN assisted;facial features;facial landmarks;facial style transfer;generative adversarial networks;generator structure;good transformation effect;hourglass network;image generation;obvious universal applicability;original CycleGAN;original generator","","","","25","IEEE","16 Dec 2022","","","IEEE","IEEE Conferences"
"Cross-View Panorama Image Synthesis","S. Wu; H. Tang; X. -Y. Jing; H. Zhao; J. Qian; N. Sebe; Y. Yan","Computer Science, Guangdong University of Petrochemical Technology, 117811 Maoming, China, 525000; Department of Information Engineering and Computer Science, University of Trento, Trento, TN, Italy, 38123; School of Computer Luo Jia Shan, Wuhan University, 12390 Wuhan, China, 430072; School of Software Engineering, Jinling Institute of Technology, 66499 Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, 12436 Nanjing, China, 210094; Information Engineering and Computer Science, University of Trento, Trento, Italy, 38123; Computer Science, Texas State University, 7174 San Marcos, Texas, United States","IEEE Transactions on Multimedia","","2022","PP","99","1","1","In this paper, we tackle the problem of ground-view panorama image generation conditioning on top-view aerial image, which is a challenging problem due to large gap between image domains associated with different view-points. Instead of learning the underlying cross-view mapping by a feedforward network as previous methods, we propose a novel adversarial feedback GAN framework named PanoGAN consisting of two key components: an adversarial feedback module and a dual branch discrimination strategy. First, aerial image is fed into the generator to produce panorama image and segmentation map, which facilitates the use of semantic layout information for model training. Second, the discriminator's feature responses of model outputs are encoded by the adversarial feedback module and then fed back to the generator for the next round of generation. Continual improvement of generated image quality is achieved through an iterative generation process. Third, to pursue high-fidelity and semantic consistency of the generated panorama image, we propose a pixel-segmentation alignment mechanism under the dual branch discrimiantion strategy that promotes the cooperation between generator and discriminator. Extensive experimental results on two challenging cross-view image datasets show that the proposed PanoGAN generates high-quality panorama images with more convincing details than state-of-the-art methods. The source code and trained models are available at https://github.com/sswuai/PanoGAN.","1941-0077","","10.1109/TMM.2022.3162474","National Natural Science Foundation of China(grant numbers:61933013, 62176069); Natural Science Foundation of Guangdong Province(grant numbers:2019A1515011076); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9743312","cross-view panorama generation;feedback adversarial learning;multi-scale feature alignment;GANs","Generators;Image synthesis;Task analysis;Image segmentation;Feature extraction;Semantics;Generative adversarial networks","","","","1","","","IEEE","25 Mar 2022","","","IEEE","IEEE Early Access Articles"
"Assessing the ability of generative adversarial networks to learn canonical medical image statistics","V. A. Kelkar; D. S. Gotsis; F. J. Brooks; K. Prabhat; K. J. Myers; R. Zeng; M. A. Anastasio","Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Bioengineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Center for Devices and Radiological Health, Food and Drug Administration, Silver Spring, MD, USA; Center for Devices and Radiological Health, Food and Drug Administration, Silver Spring, MD, USA; Center for Devices and Radiological Health, Food and Drug Administration, Silver Spring, MD, USA; Department of Bioengineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA","IEEE Transactions on Medical Imaging","","2023","PP","99","1","1","In recent years, generative adversarial networks (GANs) have gained tremendous popularity for potential applications in medical imaging, such as medical image synthesis, restoration, reconstruction, translation, as well as objective image quality assessment. Despite the impressive progress in generating high-resolution, perceptually realistic images, it is not clear if modern GANs reliably learn the statistics that are meaningful to a downstream medical imaging application. In this work, the ability of a state-of-the-art GAN to learn the statistics of canonical stochastic image models (SIMs) that are relevant to objective assessment of image quality is investigated. It is shown that although the employed GAN successfully learned several basic first- and second-order statistics of the specific medical SIMs under consideration and generated images with high perceptual quality, it failed to correctly learn several per-image statistics pertinent to the these SIMs, highlighting the urgent need to assess medical image GANs in terms of objective measures of image quality.","1558-254X","","10.1109/TMI.2023.3241454","Oak Ridge Institute for Science and Education; National Institutes of Health(grant numbers:EB020604,EB023045,EB028652,NS102213); U.S. Food and Drug Administration(grant numbers:Critical Path Funding); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10034780","Generative models;generative adversarial networks;stochastic image models;objective image quality assessment","Biomedical imaging;Generative adversarial networks;Stochastic processes;Data models;Training;Task analysis;Breast","","","","","","","CCBY","1 Feb 2023","","","IEEE","IEEE Early Access Articles"
"Neural Architecture Search With a Lightweight Transformer for Text-to-Image Synthesis","W. Li; S. Wen; K. Shi; Y. Yang; T. Huang","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Australian AI Institute, Faculty of Engineering and Information Technology, University of Technology Sydney, Sydney, NSW, Australia; School of Information Science and Engineering, Chengdu University, Chengdu, Sichuan, China; College of Science and Engineering, Hamad Bin Khalifa University, Doha, Qatar; Science Program, Texas A&M University at Qatar, Doha, Qatar","IEEE Transactions on Network Science and Engineering","24 May 2022","2022","9","3","1567","1576","Despite the cross-modal text-to-imagesynthesis task has achieved great success, most of the latest works in this field are based on the network architectures proposed by predecessors, such as StackGAN, AttnGAN, etc. Since the quality for text-to-image synthesis is more and more demanding, these old and tandem architectures with simple convolution operations are no longer suitable. Therefore, a novel text-to-image synthesis network combining with the latest technologies is in urgent need of exploration. To tackle with this challenge, we creatively propose a unique architecture for text-to-image synthesis, dubbed T2IGAN, which is automatically searched by neural architecture search (NAS). In addition, considering the amazing capabilities of the popular transformer in natural language processing and computer vision, a lightweight transformer is applied in our search space to efficiently integrate the text features and image features. Ultimately, the effectiveness of our searched T2IGAN is remarkable by experimentally evaluating it on the typical text-to-image synthesis datasets. Specifically, we achieve an excellent result of IS 5.12 and FID 10.48 on CUB-200 Birds, IS 4.89 and FID 13.55 on Oxford-102 Flowers, IS 31.93 and FID 26.45 on COCO. By contrast with the state-of-the-art works, ours gets better performance on CUB-200 Birds and Oxford-102 Flowers.","2327-4697","","10.1109/TNSE.2022.3147787","National Natural Science Foundation of China(grant numbers:61673187); Qatar National Research Fund(grant numbers:NPRP 9-466-1-103); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699403","Generative adversarial network;neural architecture search;text-to-image synthesis;transformer","Transformers;Task analysis;Computer architecture;Generative adversarial networks;Image synthesis;Search problems;Semantics","computer vision;deep learning (artificial intelligence);natural language processing;search problems;text analysis;text detection","lightweight transformer;network architectures;text-to-image synthesis network;neural architecture search;text feature;image feature;searched T2IGAN;dubbed T2IGAN;natural language processing;computer vision;CUB-200 Birds dataset;Oxford-102 Flowers dataset;COCO dataset","","11","","64","IEEE","1 Feb 2022","","","IEEE","IEEE Journals"
"Learning Fast Converging, Effective Conditional Generative Adversarial Networks with a Mirrored Auxiliary Classifier","Z. Wang","The University of Tennessee, Knoxville","2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 Jun 2021","2021","","","2565","2574","Training conditional generative adversarial networks (GANs) has been remaining as a challenging task, though standard GANs have developed substantially and gained huge successes in recent years. In this paper, we propose a novel conditional GAN architecture with a mirrored auxiliary classifier (MAC-GAN) in its discriminator for the purpose of label conditioning. Unlike existing works, our mirrored auxiliary classifier contains both a real and a fake node for each specific class to distinguish real samples from generated samples that are assigned into the same category by previous models. Comparing with previous auxiliary classifier-based conditional GANs, our MAC-GAN learns a fast converging model for high-quality image generation, taking benefits from its robust, newly designed auxiliary classifier. Experiments on multiple benchmark datasets illustrate that our proposed model improves the quality of image synthesis compared with state-of-the-art approaches. Moreover, much better classification performance can be achieved with the mirrored auxiliary classifier, which can in turn promote the use of MAC-GAN in various transfer learning tasks.","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423179","","Training;Computer vision;Image synthesis;Conferences;Computational modeling;Transfer learning;Computer architecture","learning (artificial intelligence);neural nets;pattern classification","effective conditional generative adversarial networks;mirrored auxiliary classifier;training conditional generative adversarial networks;standard GANs;conditional GAN architecture;MAC-GAN;label conditioning;fast converging model;high-quality image generation;auxiliary classifier;auxiliary classifier-based conditional GANs","","2","","49","IEEE","14 Jun 2021","","","IEEE","IEEE Conferences"
"Retinal Image Synthesis and Semi-Supervised Learning for Glaucoma Assessment","A. Diaz-Pinto; A. Colomer; V. Naranjo; S. Morales; Y. Xu; A. F. Frangi","Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Instituto de Investigación e Innovación en Bioingeniería, Universitat Politècnica de València, Valencia, Spain; Artificial Intelligence Innovation Business, Baidu, Inc., Beijing, China; CISTIB Center for Computational Imaging and Simulation Technologies in Biomedicine, University of Leeds, Leeds, U.K.","IEEE Transactions on Medical Imaging","30 Aug 2019","2019","38","9","2211","2218","Recent works show that generative adversarial networks (GANs) can be successfully applied to image synthesis and semi-supervised learning, where, given a small labeled database and a large unlabeled database, the goal is to train a powerful classifier. In this paper, we trained a retinal image synthesizer and a semi-supervised learning method for automatic glaucoma assessment using an adversarial model on a small glaucoma-labeled database and a large unlabeled database. Various studies have shown that glaucoma can be monitored by analyzing the optic disc and its surroundings, and for that reason, the images used in this paper were automatically cropped around the optic disc. The novelty of this paper is to propose a new retinal image synthesizer and a semi-supervised learning method for glaucoma assessment based on the deep convolutional GANs. In addition, and to the best of our knowledge, this system is trained on an unprecedented number of publicly available images (86926 images). This system, hence, is not only able to generate images synthetically but to provide labels automatically. Synthetic images were qualitatively evaluated using t-SNE plots of features associated with the images and their anatomical consistency was estimated by measuring the proportion of pixels corresponding to the anatomical structures around the optic disc. The resulting image synthesizer is able to generate realistic (cropped) retinal images, and subsequently, the glaucoma classifier is able to classify them into glaucomatous and normal with high accuracy (AUC = 0.9017). The obtained retinal image synthesizer and the glaucoma classifier could then be used to generate an unlimited number of cropped retinal images with glaucoma labels.","1558-254X","","10.1109/TMI.2019.2903434","Project GALAHAD(grant numbers:H2020-ICT-2016-2017,732613); Generalitat Valenciana(grant numbers:GRISOLIA/2015/027); Spanish Government through a FPI Grant(grant numbers:BES-2014-067889); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8662628","Glaucoma assessment;retinal image synthesis;fundus images;DCGAN;medical imaging","Optical imaging;Biomedical optical imaging;Retina;Databases;Semisupervised learning;Synthesizers;Generative adversarial networks","biomedical optical imaging;diseases;eye;image classification;learning (artificial intelligence);medical image processing;pattern classification","retinal image synthesis;generative adversarial networks;retinal image synthesizer;semisupervised learning method;automatic glaucoma assessment;optic disc;glaucoma classifier;glaucoma labels;retinal images;small glaucoma-labeled database;t-SNE plots","Algorithms;Databases, Factual;Diagnostic Techniques, Ophthalmological;Glaucoma;Humans;Image Interpretation, Computer-Assisted;Retina;Supervised Machine Learning","86","","39","IEEE","7 Mar 2019","","","IEEE","IEEE Journals"
"Verbal-Person Nets: Pose-Guided Multi-Granularity Language-to-Person Generation","D. Liu; L. Wu; F. Zheng; L. Liu; M. Wang","Anhui Provincial Key Laboratory of Multimodal Cognitive Computation, School of Artificial Intelligence, Anhui University, Hefei 230039, China.; Key Laboratory of Knowledge Engineering with Big Data, Ministry of Education, the Intelligent Interconnected Systems Laboratory of Anhui Province, and the School of Computer Science and Information Engineering, Hefei University of Technology, Hefei 230009, China.; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China.; School of Computer Science, The University of Adelaide, Adelaide, SA 5005, Australia.; Key Laboratory of Knowledge Engineering with Big Data, Ministry of Education, the Intelligent Interconnected Systems Laboratory of Anhui Province, and the School of Computer Science and Information Engineering, Hefei University of Technology, Hefei 230009, China.","IEEE Transactions on Neural Networks and Learning Systems","","2022","PP","99","1","13","Person image generation conditioned on natural language allows us to personalize image editing in a user-friendly manner. This fashion, however, involves different granularities of semantic relevance between texts and visual content. Given a sentence describing an unknown person, we propose a novel pose-guided multi-granularity attention architecture to synthesize the person image in an end-to-end manner. To determine what content to draw at a global outline, the sentence-level description and pose feature maps are incorporated into a U-Net architecture to generate a coarse person image. To further enhance the fine-grained details, we propose to draw the human body parts with highly correlated textual nouns and determine the spatial positions with respect to target pose points. Our model is premised on a conditional generative adversarial network (GAN) that translates language description into a realistic person image. The proposed model is coupled with two-stream discriminators: 1) text-relevant local discriminators to improve the fine-grained appearance by identifying the region-text correspondences at the finer manipulation and 2) a global full-body discriminator to regulate the generation via a pose-weighting feature selection. Extensive experiments conducted on benchmarks validate the superiority of our method for person image generation.","2162-2388","","10.1109/TNNLS.2022.3151631","NSFC(grant numbers:U19A2073,62002096); Co-Operative Innovation Project of Colleges in Anhui(grant numbers:GXXT-2019-025); National Key Research and Development Program of China(grant numbers:2018YFB0804205); NSFC(grant numbers:61725203,61732008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732175","Fine-grained generation;generative adversarial networks (GANs);human poses;person image generation;text-to-image translation.","Visualization;Image synthesis;Generators;Generative adversarial networks;Electronic mail;Semantics;Computer science","","","","1","","","IEEE","9 Mar 2022","","","IEEE","IEEE Early Access Articles"
"Image Generation Based on Texture Guided VAE-AGAN for Regions of Interest Detection in Remote Sensing Images","L. Zhang; Y. Liu","School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","2310","2314","Deep learning has shown great strength in regions of interest (ROIs) detection for remote sensing images (RSIs). However, for most of RSIs, the unbalanced distribution of positive and negative samples greatly limits the performance of the deep learning-based methods. To cope with this issue, we propose a novel method based on texture guided variational autoencoder-attention wise generative adversarial network (VAE-AGAN) to augment the training data for ROI detection. First, to generate realistic texture details of RSIs, we propose a texture guidance block to embed texture prior information into encoder and decoder networks. Second, we introduce the channel and spatial-wise attention layers in the discriminator construct to adaptively recalibrate the varying importance of different channels and spatial regions of input RSIs. Finally, we apply the RSI dataset balanced by our proposal to the weakly supervised ROI detection method. Experimental results demonstrate that the proposal can not only improve the performance of ROI detection, but also outperform other competing augmentation methods.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9413823","National Natural Science Foundation of China; Beijing Normal University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413823","Image generation;texture guidance;attention;generative adversarial networks","Learning systems;Deep learning;Image synthesis;Training data;Signal processing;Generative adversarial networks;Decoding","deep learning (artificial intelligence);image texture;remote sensing","remote sensing images;unbalanced distribution;positive samples;negative samples;deep learning;texture guidance block;spatial-wise attention layers;RSI;weakly supervised ROI detection method;image generation;texture guided VAE-AGAN;texture guided variational autoencoder;attention wise generative adversarial network","","1","","23","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"GAN based Hairstyle Generation Framework for Standardization of Lightweight-model","S. -J. Gee; Y. -I. Cho; Q. Man","Department of Computer Engineering, Gachon University, Seongnamdaero, Seongnam-si, Korea; Department of Computer Engineering, Gachon University, Seongnamdaero, Seongnam-si, Korea; Department of Computer Engineering, Gachon University, Seongnamdaero, Seongnam-si, Korea","2022 13th International Conference on Information and Communication Technology Convergence (ICTC)","25 Nov 2022","2022","","","754","756","Recently GAN-based image generation models are not properly generating local image accurately, especially for a hairstyle generation task. For the reason, we propose a new hairstyle generation framework based on GAN method. The framework consists of the Face segmentation module (F) with the Hairstyle Generator (HG) module, and network interface to control internal modules. The F module detects hairstyle and facial features, and then extracts global and facial feature masks. The HG module using a GAN method based on transformer to generate hairstyle image, while fixing the detailed fusion in the hairstyle generation process. We used FFHQ and CelebA-HQ datasets for training and evaluation of our framework. The evaluation process used FID, PSNR, SSIM methods and we compared our model with other hairstyle generation models. We get higher test score and robust generated image than other model, especially in lightness of model size.","2162-1241","978-1-6654-9939-2","10.1109/ICTC55196.2022.9952719","ITRC(Information Technology Research Center)(grant numbers:IITP-2022-2017-0-01630); Institute for Information & communications Technology Promotion; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952719","convolutional neural network;generative adversarial networks;lightweight-model;Standardization","Training;Image synthesis;Neural networks;Standardization;Generative adversarial networks;Transformers;Generators","face recognition;feature extraction;image segmentation;neural nets","CelebA-HQ datasets;F module;face segmentation module;facial feature masks;FFHQ datasets;GAN method;GAN-based image generation models;global feature masks;hairstyle generation framework;hairstyle generation models;hairstyle generation process;hairstyle generation task;hairstyle generator module;hairstyle image;HG module;internal modules;lightweight-model;local image;network interface;robust generated image","","","","9","IEEE","25 Nov 2022","","","IEEE","IEEE Conferences"
"Comparison of Deep Generative Models for the Generation of Handwritten Character Images","Ö. Kırbıyık; E. Simsar; A. T. Cemgil","Bogazici Universitesi, Istanbul, TR; Bogazici Universitesi, Istanbul, TR; Bogazici Universitesi, Istanbul, TR","2019 27th Signal Processing and Communications Applications Conference (SIU)","22 Aug 2019","2019","","","1","4","In this study, we compare deep learning methods for generating images of handwritten characters. This problem can be thought of as a restricted Turing test: A human draws a character from any desired alphabet and the system synthesizes images with similar appearances. The intention here is not to merely duplicate the input image but to add random perturbations to give the impression of being human-produced. For this purpose, the images produced by two different generative models (Generative Adversarial Network and Variational Autoencoder) and the related training method (Reptile) are examined with respect to their visual quality in a subjective manner. Also, the capability of transferring the knowledge that is obtained by the model is challenged by using different datasets for the training and test processes. Using the proposed model and meta-learning method, it is possible to produce not only images similar to the ones in the training set but also novel images that belong to a class which is seen for the first time.","2165-0608","978-1-7281-1904-5","10.1109/SIU.2019.8806416","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806416","Generative Adversarial Networks;Variational Autoencoders;Generative models;Meta-learning","Training;Generative adversarial networks;Gallium nitride;Information processing;Image synthesis;Stochastic processes;Deep learning","handwritten character recognition;learning (artificial intelligence)","generative models;related training method;Variational Autoencoder;Generative Adversarial Network;random perturbations;input image;restricted Turing test;handwritten characters;deep learning methods;handwritten character images;deep Generative models;meta-learning method;test processes","","","","","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"Anomaly Detection for Alzheimer’s Disease in Brain MRIs via Unsupervised Generative Adversarial Learning","J. N. Cabreza; G. A. Solano; S. A. Ojeda; V. Munar","Department of Physical Sciences and Mathematics, College of Arts and Sciences, University of the Philippines, Manila; Department of Physical Sciences and Mathematics, College of Arts and Sciences, University of the Philippines, Manila; Department of Physical Sciences and Mathematics, College of Arts and Sciences, University of the Philippines, Manila; Department of Physical Sciences and Mathematics, College of Arts and Sciences, University of the Philippines, Manila","2022 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","1 Mar 2022","2022","","","1","5","Alzheimer’s disease (AD) is a neurodegenerative disease that results in cognitive decline, and even dementia, in patients. To diagnose AD, a combination of tools is typically used, with structural magnetic resonance imaging (sMRI) being one of them. sMRI images have mostly been used in supervised deep learning approaches, which requires large amounts of labeled data. To alleviate the need for labels, unsupervised deep learning could be used as an alternative. This study proposes an unsupervised model based on the deep convolutional generative adversarial network that performs anomaly detection on brain MRIs to diagnose AD. The model is able to yield an AUROC of 0.7951, a precision of 0.8228, a recall of 0.7386, and an accuracy of 74.44%.","","978-1-6654-5818-4","10.1109/ICAIIC54071.2022.9722678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722678","Alzheimer’s disease;generative adversarial networks;unsupervised learning;anomaly detection","Deep learning;Costs;Image synthesis;Magnetic resonance imaging;Computer architecture;Brain modeling;Generative adversarial networks","biomedical MRI;brain;cognition;diseases;image classification;learning (artificial intelligence);medical image processing;neurophysiology;patient diagnosis;unsupervised learning","anomaly detection;alzheimer;brain MRIs;unsupervised generative adversarial learning;neurodegenerative disease;cognitive decline;structural magnetic resonance imaging;sMRI images;supervised deep learning approaches;unsupervised deep learning;unsupervised model;deep convolutional generative adversarial network","","","","25","IEEE","1 Mar 2022","","","IEEE","IEEE Conferences"
"A method for Style-Based Domain Conversion by Generative Adversarial Network","Y. Onishi; I. Shimizu","Graduate School of Engineering, Tokyo University of Agriculture and Technology; Graduate School of Engineering, Tokyo University of Agriculture and Technology","2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)","1 Dec 2021","2021","","","818","819","In recent, many methods for image generation using the Generative Adversarial Network(GAN) have been proposed. CycleGAN is one of the methods for image domain conversion which is an unsupervised learning method using two unpaired datasets. On the other hand, a method for image generation called StyleGAN is known as a method which improves accuracy by learning and incorporating the style of the image. In this paper, we proposed a method for domain conversion based on CycleGAN model, and added a new module that can learn the style and perform style transfer. The generated images change by inputting the noise parameters to different resolution during inverse convolution in the style transfer module. In experiments, we confirmed that the generated images were change depending on which resolution image the noise is input into.","2378-8143","978-1-6654-3676-2","10.1109/GCCE53005.2021.9622031","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622031","Generative adversarial networks;Image-to-image translation;Style transfer","Image resolution;Image synthesis;Shape;Image color analysis;Mouth;Transforms;Generative adversarial networks","convolution;image resolution;neural nets;unsupervised learning","image domain conversion;unsupervised learning;image generation;CycleGAN;image resolution;style-based domain conversion;generative adversarial network;noise parameters;inverse convolution;style transfer module","","","","5","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"Multi-head Mutual Self-attention Generative Adversarial Network for Texture Synthesis","S. Xie; W. Qian","School of Information Science and Engineering, Yunnan University, Kunming, China; School of Information Science and Engineering, Yunnan University, Kunming, China","2022 7th International Conference on Intelligent Computing and Signal Processing (ICSP)","24 May 2022","2022","","","1484","1487","Example-based texture synthesis requires synthesizing textures that are as similar as possible to the exemplar. However, for complex texture patterns, the existing methods lead to wrong synthesis results due to insufficient feature extraction capabilities. To address this problem, this paper proposed an optimized generative adversarial network model to address the quality issues such as low resolution and insufficient detail in texture synthesis. To this end, we propose a new multi-head mutual self-attention (MHMSA) mechanism. Different from the self-attention, MHMSA is to model the mutual relationship of each position in the feature space, and clues from all feature positions can be used to generate details. Therefore, embedding the MHMSA into the generator can help to improve its ability to extract detailed features and global features. Experimental results show that the proposed model significantly improves the visual quality of texture synthesis images, and demonstrates that MHMSA outperforms self-attention in the image generation task.","","978-1-6654-7857-1","10.1109/ICSP54964.2022.9778480","Yunnan University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9778480","Texture synthesis;Generative adversarial networks;Attention mechanism;Residual network","Visualization;Image synthesis;Signal processing;Feature extraction;Generative adversarial networks;Generators;Task analysis","feature extraction;image texture","feature positions;MHMSA;detailed features;global features;texture synthesis images;image generation task;multihead mutual self-attention generative adversarial network;complex texture patterns;feature extraction capabilities;optimized generative adversarial network model;mutual relationship;feature space;multihead mutual self attention mechanism;wrong synthesis;texture synthesis","","","","20","IEEE","24 May 2022","","","IEEE","IEEE Conferences"
"Cluster-guided Image Synthesis with Unconditional Models","M. Georgopoulos; J. Oldfield; G. G. Chrysos; Y. Panagakis",Imperial College London; Queen Mary University of London; EPFL; University of Athens,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","11533","11542","Generative Adversarial Networks (GANs) are the driving force behind the state-of-the-art in image generation. Despite their ability to synthesize high-resolution photo-realistic images, generating content with on-demand conditioning of different granularity remains a challenge. This challenge is usually tackled by annotating massive datasets with the attributes of interest, a laborious task that is not always a viable option. Therefore, it is vital to introduce control into the generation process of unsupervised generative models. In this work, we focus on controllable image generation by leveraging GANs that are well-trained in an unsupervised fashion. To this end, we discover that the representation space of intermediate layers of the generator forms a number of clusters that separate the data according to semantically meaningful attributes (e.g., hair color and pose). By conditioning on the cluster assignments, the proposed method is able to control the semantic class of the generated image. Our approach enables sampling from each cluster by Implicit Maximum Likelihood Estimation (IMLE). We showcase the efficacy of our approach on faces (CelebA-HQ and FFHQ), animals (Imagenet) and objects (LSUN) using different pre-trained generative models. The results highlight the ability of our approach to condition image generation on attributes like gender, pose and hair style on faces, as well as a variety of features on different object classes.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879474","Image and video synthesis and generation; Explainable computer vision","Hair;Maximum likelihood estimation;Image synthesis;Semantics;Process control;Generative adversarial networks;Generators","art;computer animation;image classification;image sampling;image texture;learning (artificial intelligence);maximum likelihood estimation;pattern clustering;realistic images;solid modelling;unsupervised learning","cluster-guided image synthesis;unconditional models;Generative Adversarial Networks;driving force;high-resolution photo-realistic images;on-demand conditioning;different granularity;massive datasets;laborious task;generation process;unsupervised generative models;controllable image generation;leveraging GANs;unsupervised fashion;semantically meaningful attributes;cluster assignments;different pre-trained generative models;condition image generation;different object classes","","","","46","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Dairy Goat Image Generation Based on Improved-Self-Attention Generative Adversarial Networks","H. Li; J. Tang","College of Information Engineering, Northwest A&F University, Xianyang, China; Shaanxi Key Laboratory of Agricultural Information Perception and Intelligent Service, Xianyang, China","IEEE Access","10 Apr 2020","2020","8","","62448","62457","The lack of long-range dependence in convolutional neural networks causes weaker performance in generative adversarial networks(GANs) with regard to generating image details. The self-attention generative adversarial network(SAGAN) use the self-attention mechanism to calculate the correlation coefficient between feature vectors, which improves the global coherence of the network. In this paper, we put forward an improved-self-attention GANs(Improved-SAGAN) to improve the method for calculating correlation in the SAGAN. We can better measure the correlation between features by normalizing the feature vectors to eliminate as many errors caused by noise as possible. As the network learns the global information by calculating the correlation coefficient between all features, it can make up for the defects of local receptive field in the convolution network. We replace the conventional one-hot label with multi-label to obtain more supervised information for generative adversarial networks. We generate dairy goat images based on auxiliary condition generative adversarial network(ACGAN) incorporating the normalized self-attention mechanism and prove that images generated under multi-label are of higher quality than images generated under one-hot label. The generative results of different networks on the public dataset are compared by the inception score and FID evaluation algorithms, and we propose a new evaluation algorithm called SSIM-Mean to measure the quality of generated dairy goat images to further verify the effectiveness of the improved-self-attention GANs.","2169-3536","","10.1109/ACCESS.2020.2981496","National Natural Science Foundation of China(grant numbers:31101075); Major Research Development Program of Shaanxi Province of China(grant numbers:2019NY-170); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039669","Generative adversarial network;deeping learning;image generation;self-attention mechanism","Convolution;Correlation;Gallium nitride;Training;Image synthesis;Generative adversarial networks;Coherence","agricultural engineering;convolutional neural nets;image enhancement;learning (artificial intelligence)","improved-SAGAN;improved-self-attention GAN;generated dairy goat images;normalized self-attention mechanism;multilabel;one-hot label;convolution network;feature vectors;correlation coefficient;self-attention generative adversarial network;image details;convolutional neural networks;dairy goat image generation","","8","","32","CCBY","17 Mar 2020","","","IEEE","IEEE Journals"
"GAN Using Capsule Network for Discriminator and Generator","K. Marusaki; H. Watanabe","NEC Corporation, Tokyo, Japan; Graduate School of Fundamental Science and Engineering, Waseda University, Tokyo, Japan","2021 IEEE 10th Global Conference on Consumer Electronics (GCCE)","1 Dec 2021","2021","","","647","650","In this paper, we propose Capsule GAN, which incorporates the capsule network into the structure of both discriminator and generator of Generative Adversarial Networks (GAN). Many CNN-based GANs have been studied. Among them, Deep Convolutional GAN (DCGAN) has been attracting particular attention. Other examples include convolutional GAN, auxiliary classifier GAN, Wasserstein GAN (WGAN) which uses Wasserstein distance to prevent mode collapse during the learning process, and Wasserstein GAN-gp (WGAN-gp). However, image generation by GAN is not stable and prone to mode collapse. As a result, the quality of the generated images is not satisfactory. It is expected to generate better quality images by incorporating a capsule network, which compensates for the shortcomings of CNN, into the structure of GAN. Therefore, in this paper, we propose two approaches to generate images with better quality by incorporating the capsule network into GAN. The experimental results show that the proposed method is superior to the conventional method.","2378-8143","978-1-6654-3676-2","10.1109/GCCE53005.2021.9622060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622060","capsule network;gan;deep learning;cnn","Cats;Image synthesis;Conferences;Generative adversarial networks;Generators;Consumer electronics","biomedical optical imaging;cellular neural nets;convolution;edge detection;endoscopes;Gaussian distribution;image classification;image denoising;learning (artificial intelligence);maximum likelihood estimation;neural nets;nonparametric statistics;wavelet transforms","capsule network;Capsule GAN;generator;Generative Adversarial Networks;Deep Convolutional GAN;auxiliary classifier GAN;Wasserstein GAN-gp;image generation","","1","","12","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"Conditional GAN for Small Datasets","K. Hiruta; R. Saito; T. Hatakeyama; A. Hashimoto; S. Kurihara","Graduate School of Science and Technology, Keio University, Kanagawa, Japan; Graduate School of Science and Technology, Keio University, Kanagawa, Japan; Graduate School of Science and Technology, Keio University, Kanagawa, Japan; Graduate School of Science and Technology, Keio University, Kanagawa, Japan; Faculty of Science and Technology, Keio University, Kanagawa, Japan","2022 IEEE International Symposium on Multimedia (ISM)","23 Jan 2023","2022","","","278","281","Generating high-quality images with Generative Adversarial Networks (GANs) generally requires 100k+ training data. The required data amount is too large when we consider using GANs to support professional art creators; they need to follow the specific art style while interactively controlling the results along with their theme. This research proposes Conditional FastGAN, which adds a condition vector to FastGAN to produce high-quality different domain images even on small datasets. In our experiments, the MUCT Face Database of images consisting of face photos in various orientations and manga face images extracted from Osamu Tezuka’s works were used as a small-scale dataset. Fine-tuning with manga face images to a model pre-trained with photo-only face images enabled control of the generated images according to explicit conditions, such as photos and manga, for the same latent variables. In addition, the proposed method improved the FID score by 2.55 from the original FastGAN in the case of manga face generation.","","978-1-6654-7172-5","10.1109/ISM55400.2022.00062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10019704","Conditional GANs;Deep Generative Model;Manga","Training;Art;Image synthesis;Databases;Training data;Generative adversarial networks;Generators","art;face recognition;feature extraction","condition vector;conditional FastGAN;face photos;GANs;generative adversarial networks;high-quality different domain images;high-quality images;manga face generation;manga face images;original FastGAN;photo-only face images;professional art creators;required data amount;small-scale dataset;specific art style;training data","","","","25","IEEE","23 Jan 2023","","","IEEE","IEEE Conferences"
"Textile Design Generation Using GANs","R. A. Fayyaz; M. Maqbool; M. Hanif","Department of Computer Science, FAST NUCES, Lahore, Pakistan; Department of Computer Science, FAST NUCES, Lahore, Pakistan; Department of Computer Science, FAST NUCES, Lahore, Pakistan","2020 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)","19 Nov 2020","2020","","","1","5","In this work, we propose a novel method for automated textile design patterns generation using generative models. We first improve the accuracy of state-of-the-art results in classification of textile design patterns by 2% through data cleaning and pseudo labeling. Then a new dataset which is an improvement of existing dataset is also proposed. On this new dataset we compare the performance of image generative models like Wasserstein Generative Adversarial Networks Gradient Penalty (WGANs GP), Deep Convolutional GANs (DCGANs) and Convolutional Variational Autoencoders (CVAEs) for all classes separately and have evaluated the models using the inception score. We further use a style transfer model to combine multiple designs generated by WGANs GP, due to its better results among all three approaches and form more complex and appealing textile designs. Moreover we present results of unsupervised clustering of different patterns in the latent space captured by a CVAE.","2576-7046","978-1-7281-5442-8","10.1109/CCECE47787.2020.9255674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9255674","generatve adversarial networks;textile pattern generation;variational autoencoders;latent code;clustering","Textiles;Labeling;Image synthesis;Generative adversarial networks;Conferences;Data models;Training","convolutional neural nets;design engineering;gradient methods;image classification;neural nets;pattern clustering;production engineering computing;textile industry;unsupervised learning","latent space;unsupervised clustering;inception score;CVAEs;DCGANs;pseudolabeling;textile design pattern classification;automated textile design pattern generation;complex textile designs;multiple designs;style transfer model;Convolutional Variational Autoencoders;Deep Convolutional GANs;WGANs GP;Wasserstein Generative Adversarial Networks Gradient Penalty;image generative models;data cleaning","","","","20","IEEE","19 Nov 2020","","","IEEE","IEEE Conferences"
"On Constructing Vessel Dataset Structure Using GAN-based Data Augmentation","A. R. Oh; J. Lee; S. -W. Moon; J. S. Lee; D. -W. Nam; W. Yoo","Conten Information Retrieval Research Section, ETRI, Daejeon, Korea; Conten Information Retrieval Research Section, ETRI, Daejeon, Korea; Conten Information Retrieval Research Section, ETRI, Daejeon, Korea; Conten Information Retrieval Research Section, ETRI, Daejeon, Korea; Conten Information Retrieval Research Section, ETRI, Daejeon, Korea; Conten Information Retrieval Research Section, ETRI, Daejeon, Korea","2021 International Conference on Information and Communication Technology Convergence (ICTC)","7 Dec 2021","2021","","","1700","1702","Conventional methods using classical image processing techniques is to limitedly augment basic data for extension of the dataset volume in deep learning network system. A new proposed approach using synthetic data by 3D virtual model and Generative Adversarial Networks (GAN) is able to resolve the lack of dataset adequately, and the performance of the dataset structure can be verified by a classification network model. The single and combined data groups with various types of images were constructed for the accuracy comparison of classification system, and it indicated that the proposal has an appropriate profit for improvement of the system. The composed dataset using data augmentation methods can be applied on both academic and industrial field which have little actual data for deep leaning network systems. Further work will aim to improve the quality of the data from GAN and find the relevant quantity of dataset according to data type.","2162-1233","978-1-6654-2383-0","10.1109/ICTC52510.2021.9620827","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9620827","Data augmentation;Data construction;Style-transferring;Generative adversarial network","Solid modeling;Three-dimensional displays;Image resolution;Image synthesis;Generative adversarial networks;Data models;Information and communication technology","computer vision;data handling;deep learning (artificial intelligence);image classification;marine engineering computing;ships;stereo image processing","GAN-based data augmentation;image processing;deep learning network system;synthetic data;3D virtual model;Generative Adversarial Networks;classification network model;vessel dataset structure;data quality","","","","7","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"Comparison of Controllable Image Generation Methods for Face Synthesis","S. Lee; D. Kim; B. C. Song","Department of Electrical and Computer Engineering, Inha University, Incheon, South Korea; Department of Electrical and Computer Engineering, Inha University, Incheon, South Korea; Department of Electrical and Computer Engineering, Inha University, Incheon, South Korea","2022 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)","28 Nov 2022","2022","","","1","3","Generative Adversarial Networks (GAN) is widely used in the field of image generation because they can synthesize images reflecting various properties such as color, edges, drawing style, or background. In particular, GAN excels at realistically synthesizing faces, and they have had great success in manually controlling face attributes. However, when features extracted from face images are entangled, failure cases still occur during image generation. In this paper, we select two representative methods that can successfully solve these problems. We then analyze their strengths and weaknesses by direct performance comparison on CelebA. In these experiments, we identified which parts of the model are key to controlling face attributes when generating images.","","978-1-6654-6434-5","10.1109/ICCE-Asia57006.2022.9954854","Korea Institute for Advancement of Technology(grant numbers:P0008475); NRF(grant numbers:2022R1A2C2010095,2022RIA4A1033549); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954854","GAN;Face synthesis;Disentanglement","Three-dimensional displays;Annealing;Image synthesis;Image color analysis;Image edge detection;Feature extraction;Generative adversarial networks","face recognition;feature extraction;solid modelling","controllable image generation methods;direct performance comparison;face images;face synthesis;GAN;Generative Adversarial Networks;manually controlling face attributes;representative methods","","","","11","IEEE","28 Nov 2022","","","IEEE","IEEE Conferences"
"PTR-MirrorGAN: Parallel Text Redescription module for advancing MirrorGAN","H. Deng; Z. Liu; H. He; Q. Shu; Y. Peng; M. Liao","Software Institute, Jiangxi Agricultural University, Nanchang, China; College of Computer and Information Engineering, Jiangxi Agricultural University, Nanchang, China; Software Institute, Jiangxi Agricultural University, Nanchang, China; Software Institute, Jiangxi Agricultural University, Nanchang, China; Software Institute, Jiangxi Agricultural University, Nanchang, China; School of Electronics and Information Engineering, Shenzhen University, Shenzhen, China","2020 7th International Conference on Information Science and Control Engineering (ICISCE)","20 Sep 2021","2020","","","1037","1042","During generating images from a given text description, two aspects of consistency are usually considered: visual consistency and semantic consistency. Although there has been a great improvement in the use of generative adversarial networks to generate images, it remains a challenging project to ensure that the semantics of the generated images are consistent with the semantics of the input text descriptions. This study proposed a Real-Fake Images Parallel Text Redescription Module (RFIPTRM) for advancing MirrorGAN based on the idea of mirroring and reconstructed the loss of the image generation model by adding the loss between the real text and the text from generated images to the genarators for balancing the textual semantic space. Intuitively, there may be inconsistency between the textual semantic space structure of our real text and the generated image, so we conducted adversarial learning between redescriptive text from the real images and the real text to improve the inconsistency of this text space structure. Comprehensive experiments conducted on the public benchmark datasets CUB and COCO have proved that PTR-MirrorGAN is superior to other methods and achieved a better generation effect.","","978-1-7281-6406-9","10.1109/ICISCE50968.2020.00212","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9532027","PTR-MirrorGAN;text-to-image;parallel generating","Visualization;Information science;Control engineering;Image synthesis;Semantics;Benchmark testing;Generative adversarial networks","data mining;image representation;learning (artificial intelligence);text analysis","visual consistency;semantic consistency;generative adversarial networks;image generation model;textual semantic space structure;redescriptive text;RFIPTRM;public benchmark dataset;Real Fake Images Parallel Text Redescription Module;PTR MirrorGAN","","","","32","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Bridge-GAN: Interpretable Representation Learning for Text-to-Image Synthesis","M. Yuan; Y. Peng","Wangxuan Institute of Computer Technology, Peking University, Beijing, China; Wangxuan Institute of Computer Technology, Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","28 Oct 2020","2020","30","11","4258","4268","Text-to-image synthesis is to generate images with the consistent content as the given text description, which is a highly challenging task with two main issues: visual reality and content consistency. Recently, it is available to generate images with high visual reality due to the significant progress of generative adversarial networks. However, translating text description to image with high content consistency is still ambitious. For addressing the above issues, it is reasonable to establish a transitional space with interpretable representation as a bridge to associate text and image. So we propose a text-to-image synthesis approach named Bridge-like Generative Adversarial Networks (Bridge-GAN). Its main contributions are: (1) A transitional space is established as a bridge for improving content consistency, where the interpretable representation can be learned by guaranteeing the key visual information from given text descriptions. (2) A ternary mutual information objective is designed for optimizing the transitional space and enhancing both the visual reality and content consistency. It is proposed under the goal to disentangle the latent factors conditioned on text description for further interpretable representation learning. Comprehensive experiments on two widely-used datasets verify the effectiveness of our Bridge-GAN with the best performance.","1558-2205","","10.1109/TCSVT.2019.2953753","National Natural Science Foundation of China(grant numbers:61925201,61771025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902154","Text-to-image synthesis;interpretable representation learning;Bridge-GAN","Visualization;Mutual information;Image synthesis;Task analysis;Training;Bridge circuits;Semantics","data visualisation;learning (artificial intelligence);text analysis","high visual reality;highly challenging task;consistent content;Bridge-GAN;interpretable representation learning;given text description;visual information;generative adversarial networks;text-to-image synthesis approach;transitional space;high content consistency","","18","","37","IEEE","15 Nov 2019","","","IEEE","IEEE Journals"
"Wavelet-Based Unsupervised Label-to-Image Translation","G. Eskandar; M. Abdelsamad; K. Armanious; S. Zhang; B. Yang","Institute of Signal Processing and System Theory, University of Stuttgart, Stuttgart, Germany; Institute of Signal Processing and System Theory, University of Stuttgart, Stuttgart, Germany; Institute of Signal Processing and System Theory, University of Stuttgart, Stuttgart, Germany; Institute of Signal Processing and System Theory, University of Stuttgart, Stuttgart, Germany; Institute of Signal Processing and System Theory, University of Stuttgart, Stuttgart, Germany","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","1760","1764","Semantic Image Synthesis (SIS) is a subclass of image-to-image translation where a semantic layout is used to generate a photorealistic image. State-of-the-art conditional Generative Adversarial Networks (GANs) need a huge amount of paired data to accomplish this task while generic un-paired image-to-image translation frameworks underperform in comparison, because they color-code semantic layouts and learn correspondences in appearance instead of semantic content. Starting from the assumption that a high quality generated image should be segmented back to its semantic layout, we propose a new Unsupervised paradigm for SIS (USIS) that makes use of a self-supervised segmentation loss and whole image wavelet based discrimination. Furthermore, in order to match the high-frequency distribution of real images, a novel generator architecture in the wavelet domain is proposed. We test our methodology on 3 challenging datasets and demonstrate its ability to bridge the performance gap between paired and unpaired models.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746759","Semantic Image Synthesis;Unsupervised Training;GANs;Wavelet Transform","Image segmentation;Wavelet domain;Image synthesis;Semantics;Layout;Transforms;Signal processing","image segmentation;image texture;learning (artificial intelligence);unsupervised learning;wavelet transforms","color-code semantic layouts;semantic content;high quality generated image;semantic layout;Unsupervised paradigm;SIS;novel generator architecture;wavelet-based;label-to-Image translation;Semantic Image Synthesis;photorealistic image;State-of-the-art conditional Generative Adversarial Networks;paired data;image-to-image translation frameworks","","","","45","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Multi-Modal MRI Image Synthesis via GAN With Multi-Scale Gate Mergence","B. Zhan; D. Li; X. Wu; J. Zhou; Y. Wang","School of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, Sichuan, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, Sichuan, China; School of Computer Science, Sichuan University, Chengdu, Sichuan, China","IEEE Journal of Biomedical and Health Informatics","17 Jan 2022","2022","26","1","17","26","Multi-modal magnetic resonance imaging (MRI) plays a critical role in clinical diagnosis and treatment nowadays. Each modality of MRI presents its own specific anatomical features which serve as complementary information to other modalities and can provide rich diagnostic information. However, due to the limitations of time consuming and expensive cost, some image sequences of patients may be lost or corrupted, posing an obstacle for accurate diagnosis. Although current multi-modal image synthesis approaches are able to alleviate the issues to some extent, they are still far short of fusing modalities effectively. In light of this, we propose a multi-scale gate mergence based generative adversarial network model, namely MGM-GAN, to synthesize one modality of MRI from others. Notably, we have multiple down-sampling branches corresponding to input modalities to specifically extract their unique features. In contrast to the generic multi-modal fusion approach of averaging or maximizing operations, we introduce a gate mergence (GM) mechanism to automatically learn the weights of different modalities across locations, enhancing the task-related information while suppressing the irrelative information. As such, the feature maps of all the input modalities at each down-sampling level, i.e., multi-scale levels, are integrated via GM module. In addition, both the adversarial loss and the pixel-wise loss, as well as gradient difference loss (GDL) are applied to train the network to produce the desired modality accurately. Extensive experiments demonstrate that the proposed method outperforms the state-of-the-art multi-modal image synthesis methods.","2168-2208","","10.1109/JBHI.2021.3088866","National Natural Science Foundation of China; National Stem Cell Foundation(grant numbers:62071314); Sichuan Science and Technology Program(grant numbers:2020YFG0079,2021YFG0326); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9454402","Multi-modal synthesis;magnetic resonance imaging (MRI);generative adversarial network (GAN);gate mergence;multi-scale fusion","Magnetic resonance imaging;Logic gates;Generative adversarial networks;Feature extraction;Generators;Image synthesis;Convolution","biomedical MRI;feature extraction;image fusion;image registration;image segmentation;image sequences;learning (artificial intelligence);medical image processing","multiscale gate mergence;multiscale levels;multimodal MRI image synthesis;multimodal magnetic resonance imaging;clinical diagnosis;multimodal image synthesis methods;multimodal image synthesis approaches","Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging","9","","37","IEEE","14 Jun 2021","","","IEEE","IEEE Journals"
"StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks","H. Zhang; T. Xu; H. Li; S. Zhang; X. Wang; X. Huang; D. Metaxas",Rutgers University; Lehigh University; The Chinese University of Hong Kong; Baidu Research; The Chinese University of Hong Kong; Lehigh University; Rutgers University,"2017 IEEE International Conference on Computer Vision (ICCV)","25 Dec 2017","2017","","","5908","5916","Synthesizing high-quality images from text descriptions is a challenging problem in computer vision and has many practical applications. Samples generated by existing textto- image approaches can roughly reflect the meaning of the given descriptions, but they fail to contain necessary details and vivid object parts. In this paper, we propose Stacked Generative Adversarial Networks (StackGAN) to generate 256.256 photo-realistic images conditioned on text descriptions. We decompose the hard problem into more manageable sub-problems through a sketch-refinement process. The Stage-I GAN sketches the primitive shape and colors of the object based on the given text description, yielding Stage-I low-resolution images. The Stage-II GAN takes Stage-I results and text descriptions as inputs, and generates high-resolution images with photo-realistic details. It is able to rectify defects in Stage-I results and add compelling details with the refinement process. To improve the diversity of the synthesized images and stabilize the training of the conditional-GAN, we introduce a novel Conditioning Augmentation technique that encourages smoothness in the latent conditioning manifold. Extensive experiments and comparisons with state-of-the-arts on benchmark datasets demonstrate that the proposed method achieves significant improvements on generating photo-realistic images conditioned on text descriptions.","2380-7504","978-1-5386-1032-9","10.1109/ICCV.2017.629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237891","","Gallium nitride;Training;Generators;Manifolds;Image resolution;Shape;Gaussian distribution","computer vision;feature extraction;image representation;image resolution;object recognition;realistic images;text analysis","sketch-refinement process;sketches the primitive shape;colors;given text description;low-resolution images;Stage-II GAN;Stage-I results;text descriptions;high-resolution images;photo-realistic details;synthesized images;StackGAN;photo-realistic image synthesis;stacked generative adversarial networks;existing textto- image approaches;vivid object parts","","1086","","39","IEEE","25 Dec 2017","","","IEEE","IEEE Conferences"
"Part-Preserving Pose Manipulation for Person Image Synthesis","H. Dong; X. Liang; C. Zhou; H. Lai; J. Zhu; J. Yin","School of Data and Computer Science, Sun Yat-sen University; School of Intelligent Systems Engineering, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University; School of Data and Computer Science, Sun Yat-sen University; School of Computer Science, South China Normal University; School of Data and Computer Science, Sun Yat-sen University","2019 IEEE International Conference on Multimedia and Expo (ICME)","5 Aug 2019","2019","","","1234","1239","Manipulating person images under diverse poses, which transfers a person from one pose to another desired pose, is an interesting yet challenging task due to large non-rigid spatial deformation. Most existing works fail to preserve the fine-grained appearance consistency along with the pose changes due to the lack of explicit constraints and spatial modeling, leading to unrealistic results with severe artifacts. In this paper, we propose a novel Part-Preserving Generative Adversarial Network (PP-GAN) to achieve good manipulation quality by explicitly enforcing rich structure constraints over generative modeling. PP-GAN is proposed to decompose the challenging spatial transformation of the whole body into fine-grained part-level transformations, which are then integrated via human joint structure constraint. Given arbitrary poses, PP-GAN integrates human joint structure and region-level part cues as inputs to perform explicit generative modeling. Besides, we introduce a parsing-consistent loss to enforce semantic consistency among images with diverse poses, which guides the image synthesis from a semantic perspective. Extensive qualitative and quantitative evaluations on two benchmarks show that our PP-GAN significantly outperforms the state-of-the-art baselines in generating more realistic and plausible image synthesis results. PP-GAN successfully preserves part-level characteristics even for most challenging pose changes while prior works are easy to fail.","1945-788X","978-1-5386-9552-4","10.1109/ICME.2019.00215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8785023","Person Image Synthesis;Generative Adversarial Network;Human Parsing","Image synthesis;Generators;Feature extraction;Generative adversarial networks;Task analysis;Semantics;Heating systems","pose estimation","semantic consistency;diverse poses;PP-GAN;part-level characteristics;Part-preserving pose manipulation;person image synthesis;nonrigid spatial deformation;fine-grained appearance consistency;explicit constraints;spatial modeling;novel Part-Preserving Generative Adversarial Network;rich structure constraints;challenging spatial transformation;fine-grained part-level transformations;human joint structure constraint;region-level part cues;explicit generative modeling;parsing-consistent loss;arbitrary poses;manipulation quality","","5","","27","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"Multi-Style Unsupervised Image Synthesis Using Generative Adversarial Nets","G. Lv; S. M. Israr; S. Qi","School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China; School of Electronics and Information, Northwestern Polytechnical University, Xi’an, China","IEEE Access","18 Jun 2021","2021","9","","86025","86036","Unsupervised cross-domain image-to-image translation is a very active topic in computer vision and graphics. This task has two challenges: 1) lack of paired training data and 2) numerous possible outputs from a single image. The existing methods rely on either paired data or perform one-to-one translation. A novel Multi-Style Unsupervised image synthesis model using Generative Adversarial Nets (MSU-GAN) is proposed in this paper to overcome these disadvantages. Firstly, the encoder-decoder structure is used to map the image to domain-shared content features space and domain-specific style features space. Secondly, to translate an image into another domain, the content code and the style code are combined to synthesize the resulting image. Finally, the bidirectional cycle-consistency loss is used for the unpaired training data; the inter-domain adversarial loss and the reconstruction loss are used to ensure the output image's realism. Simultaneously, MSU-GAN is able to synthesize multi-style images due to disentangled representation. A Multi-Style Unsupervised Feature-Wise image synthesis model using Generative Adversarial Nets (MSU-FW-GAN) based on the MSU-GAN is proposed for the shape variation tasks. There are two different testing strategies, which include random style transfer and style guide transfer. For objective comparison, the proposed model performs well on all evaluation metrics. The random style transfer experiment results show that compared with CycleGAN on the photo2portraits dataset, MSU-FW-GAN FID, IS scores dropped by 12.77% and 8.06%. For the summer2winter dataset, MSU-GAN FID and IS scores increased by 24.51% and 3.64%. Qualitative results show that without paired training data, MSU-GAN and MSU-FW-GAN can synthesize multi-style and better realistic images on various tasks.","2169-3536","","10.1109/ACCESS.2021.3087665","Seed Foundation of Innovation and Creation for Graduate Students in Northwestern Polytechnical University(grant numbers:CX2020169); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448164","Generative Adversarial Nets;convolutional neural network;image synthesis;ResNet","Generators;Task analysis;Image synthesis;Training data;Generative adversarial networks;Shape;Training","computer vision;feature extraction;image classification;image representation;image segmentation;unsupervised learning","realistic images;unsupervised cross-domain image-to-image translation;computer vision;generative adversarial nets;domain-shared content features space;domain-specific style features space;style code;bidirectional cycle-consistency loss;unpaired training data;inter-domain adversarial loss;multistyle images;random style transfer experiment results;MSU-FW-GAN FID;MSU-GAN FID;multistyle unsupervised feature-wise image synthesis model","","1","","51","CCBY","8 Jun 2021","","","IEEE","IEEE Journals"
"Class-Balanced Text to Image Synthesis With Attentive Generative Adversarial Network","M. Wang; C. Lang; L. Liang; G. Lyu; S. Feng; a. T. Wang","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China","IEEE MultiMedia","20 Sep 2021","2021","28","3","21","31","Although the text-to-image synthesis task has shown significant progress, it still remains a challenge in generating high-quality images. In this article, we first propose an attention-driven, cycle-refinement generative adversarial network, AGAN-v1, to bridge the domain gap between visual contents and semantic concepts by constructing spatial configurations of objects. The generation of image contours is the core component, in which an attention mechanism is developed to refine local details of images by focusing on the objects that complement one subregion. Second, an advanced class-balanced generative adversarial network, AGAN-v2, is proposed to address the problem of long-tailed data distribution. Importantly, it is the first method to solve this problem in the text-to-image synthesis task. Our AGAN-v2 introduces a reweighting scheme, which adopts the effective number of samples for each class to rebalance the generative loss. Extensive quantitative and qualitative experiments on CUB and MS-COCO datasets demonstrate that the proposed AGAN-v2 significantly outperforms the state-of-the-art methods.","1941-0166","","10.1109/MMUL.2020.3048939","Natural Science Foundation of Beijing Municipality(grant numbers:4202057,4202058,4202060); National Natural Science Foundation of China(grant numbers:62072027,61872032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314089","generative adversarial network;text-to-image synthesis;attention-driven;long-tailed data distribution;rebalance","Generative adversarial networks;Training data;Semantics;Text processing;Image synthesis","computer vision;learning (artificial intelligence);natural language processing;object recognition;query processing","class-balanced text;attentive generative adversarial network;text-to-image synthesis task;high-quality images;attention-driven;cycle-refinement generative adversarial network;AGAN-v;image contours;attention mechanism;advanced class-balanced;generative loss","","1","","18","IEEE","5 Jan 2021","","","IEEE","IEEE Magazines"
"Approaching Zero-shot Learning from a Text-to-Image GAN perspective","A. Talkani; A. Bhojan","School of Computing National University of Singapore, Singapore; School of Computing National University of Singapore, Singapore","2022 IEEE 24th International Workshop on Multimedia Signal Processing (MMSP)","22 Nov 2022","2022","","","1","6","With the recent research advancements in generative adversarial networks, synthesizing images from textual descriptions has become an active research area. Their applications in fields such as the use of localizing phrases to identify unseen objects in images is an important part of image understanding and can be useful in many applications that rely on mappings between textual and visual information. Although most zero shot learning algorithms consider the problem to be a visual semantic embedding one, we attempt to utilize the demonstrative capability of Generative Adversarial network [7] for ZSL tasks, similar to [20], where an image is generated based on an object description utilizing a GAN architecture inspired by the cascaded GAN structure used in state of the art text to image frameworks [10], [16], [18], [19] in order to help the network generate more accurate feature vectors that relate to the noisy input text from wikipedia. By using a cascaded structure, we essentially divide the problem into consequent sub-problems that are easier to solve, thus leading to a more positive result on the zero-shot learning task. Our architecture utilizes a cascaded GAN architecture inspired by text-to-image generators like [10], [16], [18], [19] to generate feature vectors that can be applied to the ZSL task at various stages, with each stage increasing the dimension size and complexity of the vector. While the initial stages are responsible for generating low dimension primitive feature vectors, the later stages focus on increasing the complexity of the vector to generate more accurate outputs. The output of each stage is also guided by it's respective discriminator, which tries to classify the vector as real/fake, along with it's corresponding class label. We will empirically compare our model to state of the art of the zero learning models [6], [9], [12], [20] on the CUB and NAB datasets with the SCS and SCE splits introduced in [6], which have been modified by [6] to also include noisy Wikipedia article texts as input for each class. We then compare our architecture with state-of-the-art methods in the Zero-shot-recognition and Generalized Zero-shot learning tasks and present quantitative results for the same.","2473-3628","978-1-6654-7189-3","10.1109/MMSP55362.2022.9948764","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9948764","Text-to Image synthesis;Generative Adversarial Networks (GANs);Semantic correlation;Generalized zero-shot learning;image classification;Zero-shot learning","Visualization;Semantics;Encyclopedias;Signal processing;Generative adversarial networks;Internet;Complexity theory","feature extraction;learning (artificial intelligence);neural nets;object recognition;text analysis;vectors","accurate feature vectors;art text;cascaded GAN architecture;cascaded GAN structure;cascaded structure;consequent sub-problems;generalized zero-shot learning tasks;Generative Adversarial network;generative adversarial networks;image frameworks;image understanding;low dimension primitive feature vectors;noisy input text;noisy Wikipedia article texts;object description;text-to-image GAN perspective;text-to-image generators;textual descriptions;textual information;unseen objects;visual information;visual semantic embedding;zero learning models;zero shot learning algorithms;zero-shot learning task;Zero-shot-recognition;ZSL task","","","","26","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Hierarchical Amortized GAN for 3D High Resolution Medical Image Synthesis","L. Sun; J. Chen; Y. Xu; M. Gong; K. Yu; K. Batmanghelich","School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA, USA; School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; School of Mathematics and Statistics, University of Melbourne, Parkville, VIC, Australia; School of Computing and Information, University of Pittsburgh, Pittsburgh, PA, USA; Department of Biomedical Informatics, University of Pittsburgh, Pittsburgh, PA, USA","IEEE Journal of Biomedical and Health Informatics","11 Aug 2022","2022","26","8","3966","3975","Generative Adversarial Networks (GAN) have many potential medical imaging applications, including data augmentation, domain adaptation, and model explanation. Due to the limited memory of Graphical Processing Units (GPUs), most current 3D GAN models are trained on low-resolution medical images, these models either cannot scale to high-resolution or are prone to patchy artifacts. In this work, we propose a novel end-to-end GAN architecture that can generate high-resolution 3D images. We achieve this goal by using different configurations between training and inference. During training, we adopt a hierarchical structure that simultaneously generates a low-resolution version of the image and a randomly selected sub-volume of the high-resolution image. The hierarchical design has two advantages: First, the memory demand for training on high-resolution images is amortized among sub-volumes. Furthermore, anchoring the high-resolution sub-volumes to a single low-resolution image ensures anatomical consistency between sub-volumes. During inference, our model can directly generate full high-resolution images. We also incorporate an encoder with a similar hierarchical structure into the model to extract features from the images. Experiments on 3D thorax CT and brain MRI demonstrate that our approach outperforms state of the art in image generation. We also demonstrate clinical applications of the proposed model in data augmentation and clinical-relevant feature extraction.","2168-2208","","10.1109/JBHI.2022.3172976","National Institutes of Health(grant numbers:1R01HL141813-01); National Science Foundation(grant numbers:1839332 Tripod+X); Pennsylvania Department of Health; Pittsburgh SuperComputing(grant numbers:TG-ASC170024); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770375","3D image synthesis;generative adver- sarial networks;high resolution","Three-dimensional displays;Generative adversarial networks;Feature extraction;Training;Solid modeling;Image resolution;Image reconstruction","biomedical MRI;brain;computerised tomography;feature extraction;image resolution;medical image processing;neural nets","clinical-relevant feature extraction;brain MRI;3D thorax CT;high-resolution sub-volumes;hierarchical structure;3D GAN models;GPU;generative adversarial networks;graphical processing units;domain adaptation;data augmentation;computed tomography;end-to-end GAN architecture;low-resolution medical images;medical imaging applications;3D high resolution medical image synthesis;hierarchical amortized GAN;image generation;single low-resolution image","Artifacts;Humans;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Tomography, X-Ray Computed","5","","42","CCBYNCND","6 May 2022","","","IEEE","IEEE Journals"
"Retro-Remote Sensing With Doc2Vec Encoding","M. B. Bejiga; G. Hoxha; F. Melgani","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy","2020 Mediterranean and Middle-East Geoscience and Remote Sensing Symposium (M2GARSS)","2 Jun 2020","2020","","","89","92","In this work, we attempt to address the issue of developing a sophisticated text encoder for retro-remote sensing application. The encoder converts ancient landscape descriptions into a fixed-size vector that, adequately, represents the available information. This vector is then used as a conditioning data to a Generative adversarial network (GAN) that synthesizes the equivalent image. We propose using a pre-trained Doc2Vec encoder for text encoding and train a Wasserstein GAN (a variant of GAN) to convert landscape descriptions written by travelers and geographers into the equivalent image. Qualitative and quantitative analysis of the generated images signify usefulness of the proposed method.","","978-1-7281-2190-1","10.1109/M2GARSS47143.2020.9105139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105139","Deep learning;Generative adversarial networks;Retro-remote sensing;Text embedding;Text-to-image synthesis","Knowledge engineering;Image coding;Statistical analysis;Semantics;Geoscience and remote sensing;Machine learning;Generative adversarial networks","geophysical image processing;image coding;neural nets;remote sensing;text analysis;vectors","ancient landscape descriptions;fixed-size vector;text encoding;Wasserstein GAN;text encoder;retro-remote sensing;Doc2Vec encoder;generative adversarial network;Doc2Vec encoding","","2","","11","IEEE","2 Jun 2020","","","IEEE","IEEE Conferences"
"Synthesis and Segmentation Method of Cross-Staining Style Nuclei Pathology Image Based on Adversarial Learning","Z. Zhou; H. Guo; Y. Guo; H. Sheng","College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China; College of Computer Science and Technology, Wuhan University of Science and Technology, Wuhan, China","2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","22 Dec 2021","2021","","","522","532","In modern clinical diagnosis, nuclei is an important basis for obtaining disease information, and rapid and accurate nuclei segmentation technology can effectively help doctors diagnose diseases. However, the difference in staining equipment or organs will cause staining differences between histopathological images, which will affect the effect of nuclei segmentation. Aiming at the problem of staining differences, we propose a cross-staining style nuclei pathology image synthesis and segmentation method based on adversarial learning. The algorithm is composed of a synthetic network and a segmentation network. The synthetic network adds a content discriminator to the cycle generative adversarial networks. Used to generate an image with the same staining style as the target domain, but the same content. The segmentation network segmented the generated image to verify the necessity of staining normalization and the performance of synthesis. In addition, the synthetic network also uses content loss function and style loss function to optimize network training. Experiments show that we propose a cross-staining style nuclei pathology image synthesis and segmentation method based on adversarial learning can effectively solve the problem of staining differences and optimize the effect of segmentation network.","","978-1-6654-3574-1","10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9644920","Image;nuclei segmentation;image synthesis;generating adversarial network;staining normalization Histopathology","Training;Image segmentation;Image synthesis;Histopathology;Generative adversarial networks;Adversarial machine learning;Clinical diagnosis","biomedical optical imaging;diseases;image segmentation;learning (artificial intelligence);medical image processing;patient diagnosis","staining equipment;cross-staining style nuclei pathology image synthesis;segmentation method;adversarial learning;synthetic network;segmentation network;cycle generative adversarial networks;rapid segmentation technology;accurate nuclei segmentation technology","","","","49","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"Toward Intelligent Design: An AI-based Fashion Designer Using Generative Adversarial Networks Aided by Sketch and Rendering Generators","H. Yan; H. Zhang; L. Liu; D. Zhou; X. Xu; Z. Zhang; S. Yan","School of Computer Science and Technology, Harbin Institute of Technology Shenzhen, 529484 Shenzhen, Guangdong, China, (e-mail: 20b351014@stu.hit.edu.cn); Computer Science, Harbin Institute of Technology, 47822 shenzhen, China, 518055 (e-mail: hjzhang@hit.edu.cn); Computer Science, Harbin Institute of Technology Shenzhen, 529484 Shenzhen, Guangdong, China, (e-mail: liulinlin@stu.hit.edu.cn); Department of Computer Science, Harbin Institute of Technology Shenzhen, 529484 Shenzhen, Guangdong, China, 518055 (e-mail: zhou_dongliang@stu.hit.edu.cn); Computer Science, Harbin Institute of Technology Shenzhen, 529484 Shenzhen, Guangdong, China, (e-mail: xiaofei@hit.edu.cn); Department of Electronic Engineering, City University of Hong Kong, Hong Kong, Hong Kong, 0000000 (e-mail: cszzhang@gmail.com); Electrical and Computer Engineering, National University of Singapore, Singapore, Singapore, 1 (e-mail: eleyans@nus.edu.sg)","IEEE Transactions on Multimedia","","2022","PP","99","1","1","The traditional fashion industry is heavily dependent on designers whose talent and vision have a significant impact on their innovative designs. Through taking advantage of recent advances in image-to-image translation by generative adversarial networks (GANs), marked improvement in designers efficiency is now possible. Considering both randomness and controllability in the design process, this article presents a novel artificial intelligence (AI)-based framework for fashion design. Under this framework, a sketch-generation module which is based on latent space is firstly introduced for designing various sketches. Secondly, a rendering-generation module is proposed to learn mapping between textures and sketches to complete the task of fashion design. In order to achieve effectiveness in synthesizing semantic-aware textures on sketches, a multi-conditional feature interaction module is developed in the rendering-generation model. Moreover, two different training schemes are introduced to optimize both the sketch-generation module and the rendering-generation module. In order to evaluate the performance of our proposed models, we built a large-scale dataset which consists of 115,584 pairs of fashion item images. Experimental results demonstrate the effectiveness of our proposed method, and indicate that our model can facilitate designers design process by taking full advantage of the controllability of different conditions (e.g., sketch and texture) and the randomness of latent space.","1941-0077","","10.1109/TMM.2022.3146010","National Natural Science Foundation of China(grant numbers:61832004,61972112); HITSZ-JA Joint Laboratory of Digital Design and Intelligent Fabrication(grant numbers:HITSZ-J&A-2021A01); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021B1515020088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693190","Fashion design;generative adversarial network;image translation;fashion data","Rendering (computer graphics);Training;Clothing;Solid modeling;Image synthesis;Aerospace electronics;Process control","","","","1","","","IEEE","25 Jan 2022","","","IEEE","IEEE Early Access Articles"
"Improving human perception of GAN generated facial image synthesis by filtering the training set considering facial attributes","D. Da Silva Costa; P. N. Moura; A. C. B. Garcia","Daniel Costa - Federal University of the State of Rio de Janeiro, Brazil; Pedro Moura - Federal University of the State of Rio de Janeiro, Brazil; Cristina Garcia - Federal University of the State of Rio de Janeiro, Brazil","2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","6 Jan 2022","2021","","","100","106","Data anonymization methods have been investigated as strategies to provide data privacy and to minimize prejudices against minorities. Recent works on machine learning have shown impressive results in generating new examples of images using Generative Adversarial Networks (GAN). Creating synthetic facial images that resembles a group, but no one in particular seem an interesting strategy to anonymize a face. Nevertheless, the image synthesis process is computationally expensive and requires a very large training dataset to produce a human-acceptable outcome. This paper presents an investigation on the trade-off between the required number of instances (size) and the images’ inter-variability of a training set for using Deep Convolutional Generative Adversarial Network (DCGAN) to generate human-acceptable synthetic images. We report an experiment using the Appen crowdsourcing platform to evaluate the human acceptance of the synthetic images generated by our DCGAN trained using different samples of the CelebA public dataset, taken according to facial attributes, such as cheekbones protuberance, face shape and eyebrow thickness. The results indicate that (1) Facial attributes have a significant effect on diminishing the images’ inter-variability, specially face shape; (2) smaller images’ inter-variability leads to smaller required training dataset and (3) filtering the DCGAN’s training datasets using oval face shape led to a required dataset almost 50% lesser than without filtering for generating images humanly acceptable. We believe this result may help individuals to anonymize their visual data maintaining certain characteristics but keeping their privacy at a lower computational cost.","2577-1655","978-1-6654-4207-7","10.1109/SMC52423.2021.9659033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659033","","Training;Crowdsourcing;Data privacy;Visualization;Filtering;Shape;Image synthesis","data privacy;face recognition;learning (artificial intelligence)","facial image synthesis;data anonymization methods;data privacy;machine learning;Generative Adversarial Networks;GAN;synthetic facial images;interesting strategy;image synthesis process;human-acceptable outcome;Deep Convolutional Generative Adversarial Network;human-acceptable synthetic images;human acceptance;CelebA public dataset;smaller images;smaller required training dataset;DCGAN's training datasets;oval face shape;required dataset;visual data;human perception","","","","31","IEEE","6 Jan 2022","","","IEEE","IEEE Conferences"
"DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-To-Image Synthesis","M. Zhu; P. Pan; W. Chen; Y. Yang","Centre for Artificial Intelligence, University of Technology Sydney; Centre for Artificial Intelligence, University of Technology Sydney; State Key Lab of CAD&CG, Zhejiang University; Centre for Artificial Intelligence, University of Technology Sydney","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","5795","5803","In this paper, we focus on generating realistic images from text descriptions. Current methods first generate an initial image with rough shape and color, and then refine the initial image to a high-resolution one. Most existing text-to-image synthesis methods have two main problems. (1) These methods depend heavily on the quality of the initial images. If the initial image is not well initialized, the following processes can hardly refine the image to a satisfactory quality. (2) Each word contributes a different level of importance when depicting different image contents, however, unchanged text representation is used in existing image refinement processes. In this paper, we propose the Dynamic Memory Generative Adversarial Network (DM-GAN) to generate high-quality images. The proposed method introduces a dynamic memory module to refine fuzzy image contents, when the initial images are not well generated. A memory writing gate is designed to select the important text information based on the initial image content, which enables our method to accurately generate images from the text description. We also utilize a response gate to adaptively fuse the information read from the memories and the image features. We evaluate the DM-GAN model on the Caltech-UCSD Birds 200 dataset and the Microsoft Common Objects in Context dataset. Experimental results demonstrate that our DM-GAN model performs favorably against the state-of-the-art approaches.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954283","Image and Video Synthesis;Deep Learning;Vision + Language","Adaptation models;Shape;Image color analysis;Fuses;Memory modules;Logic gates;Writing","feature extraction;image representation;learning (artificial intelligence);text analysis","realistic images;text description;high-quality images;fuzzy image contents;initial image content;image features;dynamic memory generative adversarial networks;image contents;image refinement processes;text-to-image synthesis methods","","140","","33","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Adversarial nets with perceptual losses for text-to-image synthesis","M. Cha; Y. Gwon; H. T. Kung",Harvard University; Harvard University; Harvard University,"2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP)","7 Dec 2017","2017","","","1","6","Recent approaches in generative adversarial networks (GANs) can automatically synthesize realistic images from descriptive text. Despite the overall fair quality, the generated images often expose visible flaws that lack structural definition for an object of interest. In this paper, we aim to extend state of the art for GAN-based text-to-image synthesis by improving perceptual quality of generated images. Differentiated from previous work, our synthetic image generator optimizes on perceptual loss functions that measure pixel, feature activation, and texture differences against a natural image. We present visually more compelling synthetic images of birds and flowers generated from text descriptions in comparison to some of the most prominent existing work.","","978-1-5090-6341-3","10.1109/MLSP.2017.8168140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8168140","Generative adversarial nets;conditional generative adversarial nets;text-to-image synthesis","Gallium nitride;Generators;Birds;Training;Image reconstruction;Image resolution","computer animation;image texture;neural nets;optimisation;realistic images;text analysis","GAN;perceptual quality;synthetic image generator;natural image;text descriptions;adversarial nets;generative adversarial networks;realistic images;descriptive text;fair quality;perceptual loss function optimization","","21","","18","IEEE","7 Dec 2017","","","IEEE","IEEE Conferences"
"Transforming Intensity Distribution of Brain Lesions Via Conditional Gans for Segmentation","M. Hamghalam; T. Wang; J. Qin; B. Lei","Faculty of Electrical, Biomedical and Mechatronics Engineering, Qazvin Branch, Islamic Azad University, Qazvin, Iran; National- Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China; Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong; National- Regional Key Technology Engineering Laboratory for Medical Ultrasound, Guangdong Key Laboratory for Biomedical Measurements and Ultrasound Imaging, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China","2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)","22 May 2020","2020","","","1","4","Brain lesion segmentation is crucial for diagnosis, surgical planning, and analysis. Owing to the fact that pixel values of brain lesions in magnetic resonance (MR) scans are distributed over the wide intensity range, there is always a considerable overlap between the class-conditional densities of lesions. Hence, an accurate automatic brain lesion segmentation is still a challenging task. We present a novel architecture based on conditional generative adversarial networks (cGANs) to improve the lesion contrast for segmentation. To this end, we propose a novel generator adaptively calibrating the input pixel values, and a Markovian discriminator to estimate the distribution of tumors. We further propose the Enhancement and Segmentation GAN (Enh-Seg-GAN) which effectively incorporates the classifier loss into the adversarial one during training to predict the central labels of the sliding input patches. Particularly, the generated synthetic MR images are a substitute for the real ones to maximize lesion contrast while suppressing the background. The potential of proposed frameworks is confirmed by quantitative evaluation compared to the state-of-the-art methods on BraTS'13 dataset.","1945-8452","978-1-5386-9330-8","10.1109/ISBI45749.2020.9098347","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098347","Brain lesion segmentation;conditional GANs;distribution transformation;image synthesis;MRI","Image segmentation;Lesions;Gallium nitride;Generators;Generative adversarial networks;Biomedical imaging","biomedical MRI;brain;image classification;image segmentation;medical image processing;tumours","conditional generative adversarial networks;lesion contrast;input pixel values;Enh-Seg-GAN;generated synthetic MR images;intensity distribution;brain lesions;conditional gans;surgical planning;magnetic resonance scans;wide intensity range;accurate automatic brain lesion segmentation","","6","","10","IEEE","22 May 2020","","","IEEE","IEEE Conferences"
"Generalizing Factorization of Gans by Characterizing Convolutional Layers","Y. Wang; Q. Wang; D. Zhang",Sun Yat-Sen University; Sun Yat-Sen University; Sun Yat-Sen University,"2022 IEEE International Conference on Multimedia and Expo (ICME)","26 Aug 2022","2022","","","01","06","Existing unsupervised disentanglement methods in latent space of the Generative Adversarial Networks (GANs) rely on the analysis and decomposition of pre-trained weight matrix. However, they only consider the weight matrix of the fully connected layers, ignoring the convolutional layers which are indispensable for image processing in modern generative models. This results in the learned latent semantics lack inter-pretability, which is unacceptable for image editing tasks. In this paper, we propose a more generalized closed-form factor-ization of latent semantics in GANs, which takes the convolutionallayers into consideration when searching for the under-lying variation factors. Our method can be applied to a wide range of deep generators with just a few lines of code. Exten-sive experiments on multiple GAN models trained on various datasets show that our approach is capable of not only finding semantically meaningful dimensions, but also maintaining the consistency and interpretability of image content.","1945-788X","978-1-6654-8563-0","10.1109/ICME52920.2022.9859692","National Natural Science Foundation of China(grant numbers:61876224); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859692","Latent Semantic Interpretation;Generative Adversarial Network;Image Synthesis;Deep Learning","Convolutional codes;Convolution;Image processing;Semantics;Generative adversarial networks;Generators;Eigenvalues and eigenfunctions","convolutional neural nets;deep learning (artificial intelligence);image classification;matrix algebra","image content;generalizing factorization;unsupervised disentanglement methods;generative adversarial networks;decomposition;pre-trained weight matrix;fully connected layers;convolutional layers;image processing;learned latent semantics;interpretability;image editing tasks;variation factors;deep generators;multiple GAN models;generalized closed-form factorization","","","","20","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Hierarchical Knee Image Synthesis Framework for Generative Adversarial Network: Data From the Osteoarthritis Initiative","H. -S. Gan; M. H. Ramlee; B. A. S. Al-Rimy; Y. -S. Lee; P. Akkaraekthalin","Department of Data Science, Universiti Malaysia Kelantan, UMK City Campus, Pengkalan Chepa, Kelantan, Malaysia; Bioinspired Devices and Tissue Engineering (BIOINSPIRA) Group, Faculty of Engineering, School of Biomedical Engineering and Health Sciences, Universiti Teknologi Malaysia, Johor Bahru, Johor, Malaysia; Faculty of Engineering, School of Computing, Universiti Teknologi Malaysia, Johor Bahru, Johor, Malaysia; Department of Electronic Engineering Technology, Faculty of Engineering Technology, Universiti Malaysia Perlis, Arau, Perlis, Malaysia; Department of Electrical and Computer Engineering, Faculty of Engineering, King Mongkut’s University of Technology North Bangkok, Bangkok, Thailand","IEEE Access","30 May 2022","2022","10","","55051","55061","Medical images synthesis is useful to address persistent issues such as the lack of training data diversity and inflexibility of traditional data augmentation faced by medical image analysis researchers when developing their deep learning models. Generative adversarial network (GAN) can generate realistic image to overcome the abovementioned problems. We proposed a GAN model with hierarchical framework (HieGAN) to generate high-quality synthetic knee images as a prerequisite to enable effective training data augmentation for deep learning applications. During the training, the proposed framework embraced attention mechanism before the 256  $\times256$  scale in generator and discriminator to capture salient information of knee images. Then, a novel pixelwise-spectral normalization configuration was implemented to stabilize the training performance of HieGAN. We evaluated the proposed HieGAN on large scale knee image dataset by using Am Score and Mode Score. The results showed that HieGAN outperformed all relevant state-of-art. Hence, HieGAN can potentially serve as an important milestone to promote future development of more robust deep learning models for knee image segmentation. Future works should extend the image synthesis evaluation to clinical-related Visual Turing Test and synthetic data augmentation for deep learning segmentation task.","2169-3536","","10.1109/ACCESS.2022.3175506","Geran Penyelidikan UMK Fundamental (UMK Fund) (Project Title: Attention-based Generative Adversarial Network for Realistic Knee Image Synthesis) through Universiti Malaysia Kelantan(grant numbers:R/FUND/A1500/01934A/2022/01043); National Science, Research and Innovation Fund (NSRF); King Mongkut’s University of Technology North Bangkok(grant numbers:KMUTNB-FF-66-10); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775719","Generative adversarial network;knee;image synthesis;biomedical image processing","Training;Biomedical imaging;Image synthesis;Generators;Generative adversarial networks;Training data;Deep learning","","","","","","36","CCBY","16 May 2022","","","IEEE","IEEE Journals"
"Customizable GAN: A Method for Image Synthesis of Human Controllable","Z. Zhang; W. Yu; J. Zhou; X. Zhang; N. Jiang; G. He; Z. Yang","School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, China; School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, China; School of Science and Engineering, Hosei University, Tokyo, Japan; School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, China; School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, China; School of Communication Engineering, Xidian University, Xi’an, China; School of Computing, Guangdong University of Technology, Guangzhou, China","IEEE Access","16 Jun 2020","2020","8","","108004","108017","In the research of computer vision, artificial controllability of image synthesis is a significant and challenging task. At present, there are two available methods. One is to utilize a simple contour to determine the shape of the synthetic object. This method has a promising effect, but it can only control the shape information of the synthetic object, but not the specific content. The other is to employ the text description to synthesize the corresponding image, which effectively controls the specific content of the synthesis, but it cannot do anything for the synthesized shape. In this paper, we propose a highly flexible and human customizable image synthesis model based on simple contour and natural language description, in which the specific content of contour and text description can be determined artificially. The contour determines basic synthetic object shape, and the natural language describes specific object content. Based on these, highly authentic and customizable images can be synthesized. The experiments are executed in the Caltech-UCSD Birds (CUB) and Oxford-102 flower datasets, and the experimental results demonstrate the effectiveness and superiority of our method. The results not only maintain the contour but also conform to the natural language description. Simultaneously, the high-quality image synthesis results, based on artificial hand-drawing contour and text description, are displayed to illustrate the high flexibility and customizability of our model.","2169-3536","","10.1109/ACCESS.2020.3001070","Sichuan Science and Technology Program(grant numbers:2020YFS0307,2019YFS0146,2019YFS0155); National Natural Science Foundation of China(grant numbers:61907009); Science and Technology Planning Project of Guangdong Province(grant numbers:2019B010150002); Natural Science Foundation of Guangdong Province(grant numbers:2018A030313802); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9112217","Artificial neural networks;computer vision;image generation;image processing;text analysis;text processing","Image synthesis;Shape;Generative adversarial networks;Birds;Training;Image color analysis;Computer vision","computer vision;natural language processing;neural nets;text analysis","customizable GAN;human controllable;computer vision;artificial controllability;simple contour;shape information;text description;highly flexible image synthesis model;human customizable image synthesis model;natural language description;basic synthetic object shape;artificial hand-drawing contour;CUB;Caltech-UCSD Birds dataset;Oxford-102 flower dataset","","4","","60","CCBY","9 Jun 2020","","","IEEE","IEEE Journals"
"A Conditional Deep Framework for Automatic Layout Generation","Y. Shi; M. Shang; Z. Qi","College of Information Science and Technology, University of Nebraska, Omaha, NE, USA; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; Key Laboratory of Big Data Mining and Knowledge Management, Chinese Academy of Sciences, Beijing, China","IEEE Access","22 Aug 2022","2022","10","","86092","86100","Automatic layout generation, which means making computers enjoy creativity, is difficult yet exciting work. Up to now, how to generate reasonable and visually appealing layouts remains a complex challenge. In this paper, we propose a novel layout generation model based on Conditional Generative Adversarial Networks (L-CGAN), which can generate layouts simply and efficiently by positioning, scaling, and flipping the given primitives. To break the bottleneck of limitation of the fixed input size of Generative Adversarial Networks, we develop a pre-processing algorithm to enable the model to generate layouts with an unrestricted number of input elements. Moreover, a graph-constraint module is proposed to guide layout optimization. We demonstrate the competitive performance of our designs in diverse data domains such as handwriting digit layout generation (MNIST Layouts), scene layout generation (AbstractScene-Layouts), and document layout generation (PubLayNet).","2169-3536","","10.1109/ACCESS.2022.3198686","Key Project of National Natural Science Foundation of China(grant numbers:71932008); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856669","Deep learning;layout generation;conditional generative adversarial networks;abstract scene layout","Layout;Image analysis;Generative adversarial networks;Deep learning;Data models;Image synthesis;Transformers","data visualisation;deep learning (artificial intelligence);graph theory;inference mechanisms;optimisation","Conditional Generative Adversarial Networks;layout optimization;MNIST Layouts;scene layout generation;document layout generation;Conditional deep framework;automatic layout generation;graph-constraint module;handwriting digit layout generation;deep learning","","","","38","CCBY","16 Aug 2022","","","IEEE","IEEE Journals"
"CKD: Cross-Task Knowledge Distillation for Text-to-Image Synthesis","M. Yuan; Y. Peng","Wangxuan Institute of Computer Technology, Peking University, Beijing, China; Wangxuan Institute of Computer Technology, Peking University, Beijing, China","IEEE Transactions on Multimedia","24 Jul 2020","2020","22","8","1955","1968","Text-to-image synthesis (T2IS) has drawn increasing interest recently, which can automatically generate images conditioned on text descriptions. It is a highly challenging task that learns a mapping from a semantic space of text description to a complex RGB pixel space of image. The main issues of T2IS lie in two aspects: semantic consistency and visual quality. The distributions between text descriptions and image contents are inconsistent since they belong to different modalities. So it is ambitious to generate images containing consistent semantic contents with the text descriptions, which is the semantic consistency issue. Moreover, due to the discrepancy of data distributions between real and synthetic images in huge pixel space, it is hard to approximate the real data distribution for synthesizing photo-realistic images, which is the visual quality issue. For addressing the above issues, we propose a cross-task knowledge distillation (CKD) approach to transfer knowledge from multiple image semantic understanding tasks into T2IS task. There is amount of knowledge in image semantic understanding tasks to translate image contents into semantic representation, which is advantageous to address the issues of semantic consistency and visual quality for T2IS. Moreover, we design a multi-stage knowledge distillation paradigm to decompose the distillation process into multiple stages. By this paradigm, it is effective to approximate the distributions of real image and understand textual information for T2IS, which can improve the visual quality and semantic consistency of synthetic images. Comprehensive experiments on widely-used datasets show the effectiveness of our proposed CKD approach.","1941-0077","","10.1109/TMM.2019.2951463","National Natural Science Foundation of China(grant numbers:61771025); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8890866","Text-to-image synthesis;knowledge distillation;transfer learning;image semantic understanding","Semantics;Visualization;Task analysis;Image synthesis;Generative adversarial networks;Neural networks;Image color analysis","image colour analysis;learning (artificial intelligence);pattern recognition;realistic images;text analysis","huge pixel space;approximate the real data distribution;photo-realistic images;visual quality issue;cross-task knowledge distillation approach;multiple image semantic understanding tasks;image contents;semantic representation;T2IS;multistage knowledge distillation paradigm;synthetic images;text-to-image synthesis;text description;highly challenging task;semantic space;complex RGB pixel space;consistent semantic contents;semantic consistency issue;CKD","","28","","64","IEEE","4 Nov 2019","","","IEEE","IEEE Journals"
"Learning a Prototype Discriminator With RBF for Multimodal Image Synthesis","Z. Bi; B. Cao; W. Zuo; Q. Hu","Engineering Research Center of City Intelligence and Digital Governance, Ministry of Education, Tianjin, China; Engineering Research Center of City Intelligence and Digital Governance, Ministry of Education, Tianjin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Engineering Research Center of City Intelligence and Digital Governance, Ministry of Education, Tianjin, China","IEEE Transactions on Image Processing","26 Oct 2022","2022","31","","6664","6678","Multimodal image synthesis has emerged as a viable solution to the modality missing challenge. Most existing approaches employ softmax-based classifiers to provide modal constraints for the generated models. These methods, however, focus on learning to distinguish inter-domain differences while failing to build intra-domain compactness, resulting in inferior synthetic results. To provide sufficient domain-specific constraint, we hereby introduce a novel prototype discriminator for generative adversarial network (PT-GAN) to effectively estimate the missing or noisy modalities. Different from most previous works, we introduce the Radial Basis Function (RBF) network, endowing the discriminator with domain-specific prototypes, to improve the optimization of generative model. Since the prototype learning extracts more discriminative representation of each domain, and emphasizes intra-domain compactness, it reduces the sensitivity of discriminator to pixel changes in generated images. To address this dilemma, we further propose a reconstructive regularization term which connects the discriminator with the generator, thus enhancing its pixel detectability. To this end, the proposed PT-GAN provides not only consistent domain-specific constraints, but also reasonable uncertainty estimation of generated images with the RBF distance. Experimental results show that our method outperforms the state-of-the-art techniques. The source code will be available at: https://github.com/zhiweibi/PT-GAN.","1941-0042","","10.1109/TIP.2022.3214336","National Key Research and Development Program of China(grant numbers:2019YFB2101901); National Natural Science Foundation of China(grant numbers:62106171,61925602,61732011); Tianjin Natural Science Foundation(grant numbers:21JCYBJC00580); China Postdoctoral Science Foundation(grant numbers:2020M680034); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925140","Prototype discriminator;multimodal image synthesis;radial basis function;reconstructive regularization","Uncertainty;Prototypes;Generators;Image synthesis;Generative adversarial networks;Estimation;Task analysis","image classification;image reconstruction;image representation;inference mechanisms;learning (artificial intelligence);pattern classification;radial basis function networks;text analysis","modal constraints;generated models;inter-domain differences;intra-domain compactness;inferior synthetic results;sufficient domain-specific constraint;novel prototype discriminator;generative adversarial network;PT-GAN;missing modalities;noisy modalities;Radial Basis Function network;domain-specific prototypes;generative model;prototype learning;discriminative representation;consistent domain-specific constraints;RBF distance;multimodal image synthesis;modality missing challenge;softmax-based classifiers","","1","","71","IEEE","19 Oct 2022","","","IEEE","IEEE Journals"
"Multi-Modality MR Image Synthesis via Confidence-Guided Aggregation and Cross-Modality Refinement","B. Peng; B. Liu; Y. Bin; L. Shen; J. Lei","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; Center for Future Media, University of Electronic Science and Technology of China, Chengdu, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","IEEE Journal of Biomedical and Health Informatics","17 Jan 2022","2022","26","1","27","35","Magnetic resonance imaging (MRI) can provide multi-modality MR images by setting task-specific scan parameters, and has been widely used in various disease diagnosis and planned treatments. However, in practical clinical applications, it is often difficult to obtain multi-modality MR images simultaneously due to patient discomfort, and scanning costs, etc. Therefore, how to effectively utilize the existing modality images to synthesize missing modality image has become a hot research topic. In this paper, we propose a novel confidence-guided aggregation and cross-modality refinement network (CACR-Net) for multi-modality MR image synthesis, which effectively utilizes complementary and correlative information of multiple modalities to synthesize high-quality target-modality images. Specifically, to effectively utilize the complementary modality-specific characteristics, a confidence-guided aggregation module is proposed to adaptively aggregate the multiple target-modality images generated from multiple source-modality images by using the corresponding confidence maps. Based on the aggregated target-modality image, a cross-modality refinement module is presented to further refine the target-modality image by mining correlative information among the multiple source-modality images and aggregated target-modality image. By training the proposed CACR-Net in an end-to-end manner, high-quality and sharp target-modality MR images are effectively synthesized. Experimental results on the widely used benchmark demonstrate that the proposed method outperforms state-of-the-art methods.","2168-2208","","10.1109/JBHI.2021.3082541","National Natural Science Foundation of China(grant numbers:61722112); Natural Science Foundation of Tianjin City(grant numbers:18ZXZNGX00110,18JCJQJC45800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439166","Magnetic resonance imaging;medical image synthesis;confidence-guided aggregation;cross-modality refinement","Image synthesis;Medical diagnostic imaging;Magnetic resonance imaging;Generative adversarial networks;Task analysis;Fuses;Training","biomedical MRI;data mining;diseases;image registration;learning (artificial intelligence);medical image processing","multimodality MR image synthesis;magnetic resonance imaging;multimodality MR images;task-specific scan parameters;existing modality images;missing modality image;novel confidence-guided aggregation;cross-modality refinement network;multiple modalities;high-quality target-modality images;complementary modality-specific characteristics;confidence-guided aggregation module;multiple target-modality images;multiple source-modality images;aggregated target-modality image;cross-modality refinement module;sharp target-modality","Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging","1","","42","IEEE","21 May 2021","","","IEEE","IEEE Journals"
"Person Image Synthesis in Arbitrary 3D Poses Based on Part Affinity Fields","J. Wang; S. Huang; D. Tao","University of Technology Sydney, Ultimo, NSW, Australia; UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney, Darlington, NSW, Australia; UBTECH Sydney AI Centre, School of Computer Science, FEIT, University of Sydney, Darlington, NSW, Australia","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","7","We consider the person image synthesis problem, where an output image is generated from an arbitrary source image and an arbitrary target 3D pose. Prior person image synthesizing methods usually use 2D keypoint heatmaps to represent the target pose. However, this 2D representation can be ambiguous in self-occlusion scenarios due to the lack of depth information, resulting in generating inappropriate images. To solve this problem, we propose to synthesize person image from 3D poses. We introduce an improved part affinity field representation to describe the 3D configuration of the target pose and the 2D location of the person in the pixel space. Compared to using 2D poses, our synthesized images have visually better details, such as correct self-occlusion, brightness, face direction et al. Moreover, in contrast with prior person image generators, our method predicts the difference between the source image and target image instead of the output images. This strategy allows the network to generate better foreground and significantly reduces the noise in the image background. We evaluate our method on images of fifteen different actions on Human3.6M dataset. Extensive experiments demonstrate that our method can synthesize much better person images than 2D pose-based ones. If given a sequence of desired poses, our method can produce a sequence of temporally smooth and coherent images, even on another subject and actions, which means that our method has great potential to generate high-quality person videos.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533749","China Scholarship Council (CSC)(grant numbers:201603170329); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533749","Person image synthesis;3D pose;part affinity field;motion retarget","Heating systems;Three-dimensional displays;Image synthesis;Neural networks;Brightness;Generative adversarial networks;Generators","computer vision;Gaussian processes;image colour analysis;image motion analysis;object tracking;pose estimation","arbitrary 3D poses;part affinity fields;person image synthesis problem;output image;arbitrary source image;arbitrary target 3D pose;target pose;inappropriate images;improved part affinity field representation;synthesized images;prior person image generators;target image;image background;Human3.6M dataset;2D pose-based ones;desired poses;temporally smooth images;coherent images;high-quality person videos","","","","23","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Attention-Guided Generative Adversarial Networks for Unsupervised Image-to-Image Translation","H. Tang; D. Xu; N. Sebe; Y. Yan","Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Department of Computer Science, Texas State University, San Marcos, USA","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","8","The state-of-the-art approaches in Generative Adversarial Networks (GANs) are able to learn a mapping function from one image domain to another with unpaired image data. However, these methods often produce artifacts and can only be able to convert low-level information, but fail to transfer high-level semantic part of images. The reason is mainly that generators do not have the ability to detect the most discriminative semantic part of images, which thus makes the generated images with low-quality. To handle the limitation, in this paper we propose a novel Attention-Guided Generative Adversarial Network (AGGAN), which can detect the most discriminative semantic object and minimize changes of unwanted part for semantic manipulation problems without using extra data and models. The attention-guided generators in AGGAN are able to produce attention masks via a built-in attention mechanism, and then fuse the input image with the attention mask to obtain a target image with high-quality. Moreover, we propose a novel attention-guided discriminator which only considers attended regions. The proposed AGGAN is trained by an end-to-end fashion with an adversarial loss, cycle-consistency loss, pixel loss and attention loss. Both qualitative and quantitative results demonstrate that our approach is effective to generate sharper and more accurate images than existing models.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8851881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851881","GANs;Image-to-Image Translation;Attention","Generators;Generative adversarial networks;Semantics;Task analysis;Computational modeling;Image synthesis;Training","computer vision;image segmentation;learning (artificial intelligence);neural nets","unsupervised image-to-image translation;mapping function;image domain;unpaired image data;AGGAN;attention-guided generators;attention-guided discriminator;adversarial loss;pixel loss;attention loss;discriminative semantic object detection;attention-guided generative adversarial network;cycle-consistency loss","","44","1","48","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"Multi-Metric Evaluation of Thermal-to-Visual Face Recognition","K. Lai; S. N. Yanushkevich","Dept. Electrical & Computer Engineering, University of Calgary, Canada; Dept. Electrical & Computer Engineering, University of Calgary, Canada","2019 Eighth International Conference on Emerging Security Technologies (EST)","22 Aug 2019","2019","","","1","6","In this paper, we aim to address the problem of heterogeneous or cross-spectral face recognition using machine learning to synthesize visual spectrum face from infrared images. The synthesis of visual-band face images allows for more optimal extraction of facial features to be used for face identification and/or verification. We explore the ability to use Generative Adversarial Networks (GANs) for face image synthesis, and examine the performance of these images using pre-trained Convolutional Neural Networks (CNNs). The features extracted using CNNs are applied in face identification and verification. We explore the performance in terms of acceptance rate when using various similarity measures for face verification.","2472-7601","978-1-7281-5546-3","10.1109/EST.2019.8806202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806202","","Visualization;Face;Face recognition;Feature extraction;Measurement;Generators;Generative adversarial networks","convolutional neural nets;face recognition;feature extraction;infrared imaging;learning (artificial intelligence)","multimetric evaluation;thermal-to-visual face recognition;cross-spectral face recognition;machine learning;visual spectrum face;infrared images;visual-band face images;facial features;face identification;face image synthesis;face verification;generative adversarial networks;pre-trained convolutional neural networks;GANs;CNNs;feature extraction","","4","","21","IEEE","22 Aug 2019","","","IEEE","IEEE Conferences"
"MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks","A. Karnewar; O. Wang",TomTom; Adobe Research,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","7796","7805","While Generative Adversarial Networks (GANs) have seen huge successes in image synthesis tasks, they are notoriously difficult to adapt to different datasets, in part due to instability during training and sensitivity to hyperparameters. One commonly accepted reason for this instability is that gradients passing from the discriminator to the generator become uninformative when there isn't enough overlap in the supports of the real and fake distributions. In this work, we propose the Multi-Scale Gradient Generative Adversarial Network (MSG-GAN), a simple but effective technique for addressing this by allowing the flow of gradients from the discriminator to the generator at multiple scales. This technique provides a stable approach for high resolution image synthesis, and serves as an alternative to the commonly used progressive growing technique. We show that MSG-GAN converges stably on a variety of image datasets of different sizes, resolutions and domains, as well as different types of loss functions and architectures, all with the same set of fixed hyperparameters. When compared to state-of-the-art GANs, our approach matches or exceeds the performance in most of the cases we tried.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00782","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156324","","Generators;Image resolution;Gallium nitride;Training;Image generation;Task analysis;Generative adversarial networks","gradient methods;image resolution;neural nets","MSG-GAN;multiscale gradient generative adversarial network;high resolution image synthesis;image datasets","","75","","41","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Conditional Introspective Variational Autoencoder for Image Synthesis","K. Zheng; Y. Cheng; X. Kang; H. Yao; T. Tian","School of Geography and Information Engineering, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China","IEEE Access","28 Aug 2020","2020","8","","153905","153913","We present a variational autoencoder (VAE) learning framework with introspective training for conditional image synthesis, and explore conditional capsule encoder by class-wise mask label insertion for this framework. Our model only consists of encoder (E), generator (G) and classifier (C), where E and G can be adversarially optimized, and C helps to boost conditional generation, improve authenticity and provide generation measures for E and G. Discriminator is not necessary in our framework and its absence makes our model more concise with fewer artifacts and pattern collapse problems. To compensate for the blurry weakness of VAE-like models, feature matching is introduced into loss functions by means of C to offer more reasonable measures between real and synthesized images. Moreover, in consideration of the key role of E in autoencoders as well as the interesting characteristics of capsule structure, conditional capsule encoder is preliminary explored in the image synthesis model. Class labels participate conditional encoding by masking high-level capsules of other categories, and capsule loss for the encoder is added to facilitate conditional synthesis. Experiments on MNIST and Fashion-MNIST data sets show that our model achieves real conditional synthesis performances with better diversity and fewer artifacts. And conditional capsule encoder also reveals interesting synthesis effects.","2169-3536","","10.1109/ACCESS.2020.3018228","National Basic Research Program of China (973 Program)(grant numbers:2018YFB1004600); National Science and Technology Major Project of China(grant numbers:2017ZX05036-001-010); Science and Technology Planning Project of Guangdong Province, China(grant numbers:2018B020207012); National Natural Science Foundation of China(grant numbers:41701417,61972365,61672474); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9172064","Image generation;artificial neural networks;image processing","Generators;Image synthesis;Training;Generative adversarial networks;Gallium nitride;Image reconstruction;Task analysis","feature extraction;image classification;image coding;image matching;image representation;learning (artificial intelligence)","Fashion-MNIST data sets;feature matching;pattern collapse problems;synthesis effects;capsule loss;conditional encoding;image synthesis model;VAE-like models;class-wise mask label insertion;conditional capsule encoder;conditional image synthesis;introspective training;variational autoencoder learning framework;conditional introspective variational autoencoder","","5","","31","CCBY","20 Aug 2020","","","IEEE","IEEE Journals"
"ScoreMix: A Scalable Augmentation Strategy for Training GANs With Limited Data","J. Cao; M. Luo; J. Yu; M. -H. Yang; R. He","CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, China; CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, China; CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, China; Google; CRIPAC & NLPR, Institute of Automation, Chinese Academy of Sciences, University of Chinese Academy of Sciences, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2022","PP","99","1","16","Generative Adversarial Networks (GANs) typically suffer from overfitting when limited training data is available. To facilitate GAN training, current methods propose to use data-specific augmentation techniques. Despite the effectiveness, it is difficult for these methods to scale to practical applications. In this work, we present ScoreMix, a novel and scalable data augmentation approach for various image synthesis tasks. We first produce augmented samples using the convex combinations of the real samples. Then, we optimize the augmented samples by minimizing the norms of the data scores, i.e., the gradients of the log-density functions. This procedure enforces the augmented samples close to the data manifold. To estimate the scores, we train a deep estimation network with multi-scale score matching. For different image synthesis tasks, we train the score estimation network using different data. We do not require the tuning of the hyperparameters or modifications to the network architecture. The ScoreMix method effectively increases the diversity of data and reduces the overfitting problem. Moreover, it can be easily incorporated into existing GAN models with minor modifications. Experimental results on numerous tasks demonstrate that GAN models equipped with the ScoreMix method achieve significant improvements.","1939-3539","","10.1109/TPAMI.2022.3231649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9998118","Data augmentation;few-shot image-to-image translation;generative adversarial networks;image synthesis","Training;Task analysis;Training data;Image synthesis;Generative adversarial networks;Data models;Optimization","","","","","","","IEEE","23 Dec 2022","","","IEEE","IEEE Early Access Articles"
"Physics-Informed Hyperspectral Remote Sensing Image Synthesis With Deep Conditional Generative Adversarial Networks","L. Liu; W. Li; Z. Shi; Z. Zou","State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Astronautics, Beihang University, Beijing, China; Department of Guidance, Navigation and Control, School of Astronautics, Beihang University, Beijing, China","IEEE Transactions on Geoscience and Remote Sensing","18 May 2022","2022","60","","1","15","High-resolution hyperspectral remote sensing images are of great significance to agricultural, urban, and military applications. However, collecting and labeling hyperspectral images are time-consuming, expensive, and usually heavily rely on domain knowledge. In this article, we propose a new method for generating high-resolution hyperspectral images and subpixel ground-truth annotations from RGB images. Given a single high-resolution RGB image as its conditional input, unlike previous methods that directly predict spectral reflectance and ignores the physics behind it, we consider both imaging mechanism and spectral mixing, introduce a deep generative network that first recovers the spectral abundance for each pixel, and then generate the final spectral data cube with the standard USGS spectral library. In this way, our method not only synthesizes high-quality spectral data existing in the real world but also generates subpixel-level spectral abundance with well-defined spectral reflectance characteristics. We also introduce a spatial discriminative network and a spectral discriminative network to improve the fidelity of the synthetic output from both spatial and spectral perspectives. The whole framework can be trained end-to-end in an adversarial training paradigm. We refer to our method as “Physics-informed Deep Adversarial Spectral Synthesis (PDASS).” On the IEEE grss_dfc_2018 dataset, our method achieves an MPSNR of 47.56 on spectral reconstruction accuracy and outperforms other state-of-the-art methods. As latent variables, the generated spectral abundance and the atmospheric absorption coefficients of sunlight also suggest the effectiveness of our method.","1558-0644","","10.1109/TGRS.2022.3173532","National Natural Science Foundation of China(grant numbers:62125102); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770778","Generation adversarial networks (GANs);hyperspectral image;imaging model;remote sensing;spectral super-resolution (SSR)","Hyperspectral imaging;Superresolution;Atmospheric modeling;Image reconstruction;Absorption;Spatial resolution;Libraries","geophysical image processing;geophysical signal processing;hyperspectral imaging;image classification;image colour analysis;image reconstruction;image resolution;remote sensing;spectral analysis","Physics-informed hyperspectral remote sensing image synthesis;deep conditional generative adversarial networks;high-resolution hyperspectral remote sensing images;agricultural applications;urban, applications;military applications;high-resolution hyperspectral images;subpixel ground-truth annotations;RGB images;single high-resolution RGB image;conditional input;imaging mechanism;spectral mixing;deep generative network;final spectral data cube;standard USGS spectral library;high-quality spectral data;subpixel-level spectral abundance;well-defined spectral reflectance characteristics;spatial discriminative network;spectral discriminative network;spatial perspectives;spectral perspectives;adversarial training paradigm;Physics-informed Deep Adversarial Spectral Synthesis;spectral reconstruction accuracy;generated spectral abundance","","1","","79","IEEE","9 May 2022","","","IEEE","IEEE Journals"
"Text to Image Synthesis using Residual GAN","P. Mishra; T. Singh Rathore; S. Shivani; S. Tendulkar","Computer Science & Engineering, Indian Institute of Information Technology, Kota, India; Computer Science & Engineering, Indian Institute of Information Technology, Kota, India; Computer Science & Engineering, Indian Institute of Information Technology, Kota, India; Computer Science & Engineering, Indian Institute of Information Technology, Kota, India","2020 3rd International Conference on Emerging Technologies in Computer Engineering: Machine Learning and Internet of Things (ICETCE)","14 May 2020","2020","","","139","144","In the world of computer vision, a very intriguing problem is synthesizing or generating images (from the noise) of the reasonable quality from text descriptions. The applications of this problem are immense such as photo-editing, computer- aided design, etc. But the current AI systems are not up to the mark to reach the desired outcome. However, in recent years the progress in the field of text classification and image classification fields have paved the way for more advanced AI systems that can be used to achieve the desired goal by utilizing the discriminative power and strong generalization properties of attribute representations of recurrent neural networks and convolutional neural networks. Meanwhile, GANs have proved to produce reasonable images of birds, flowers, etc. In this work, we present GAN architecture to effectively aid the translation visual concepts from the text to image.","","978-1-7281-1683-9","10.1109/ICETCE48199.2020.9091779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091779","GAN;Generator;Discriminator;Text to Image;Residual GAN","Generators;Gallium nitride;Training;Generative adversarial networks;Visualization;Computer science;Information technology","artificial intelligence;computer vision;convolutional neural nets;image classification;recurrent neural nets;text analysis","image synthesis;residual GAN;computer vision;text descriptions;text classification;AI systems;discriminative power;generalization properties;attribute representations;recurrent neural networks;convolutional neural networks;GAN architecture;translation visual concepts;image classification","","3","","13","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Normalizing Flow for Synthetic Medical Images Generation","M. Hajij; G. Zamzmi; R. Paul; L. Thukar","Faculty of Mathematics and Computer Science, Santa Clara University; Department of Computer Science and Engineering, University of South Florida; Harvard Medical School, Massachusetts General Hospital; Harvard Medical School","2022 IEEE Healthcare Innovations and Point of Care Technologies (HI-POCT)","1 Apr 2022","2022","","","46","49","Deep generative models, such as generative adversarial network (GAN) and variational autoencoder (VAE), have been utilized extensively for medical image generation. While these models made remarkable progress in medical image synthesis, they can not explicitly learn the probability density function of the input data and are highly sensitive to the hyperparameter selections. To mitigate these issues, a new type of deep generative model, called Normalizing Flows (NFs), have emerged in recent years. In this paper, we investigate NFs as an alternative for synthesizing medical images. In particular, we utilize realNVP, a popular NF model for the purpose of synthesizing medical images. To evaluate our synthesized images, we propose to utilize Wasserstien distance along with the permutation test to quantify the quality of the generated images. Within our quantifying metric, our results indicate that the two sample distributions, the first being the samples obtained from our NF model and second being the original dataset, are similar providing a promising indication of normalizing flow’s capability in medical images generation.","","978-1-6654-9615-5","10.1109/HI-POCT54491.2022.9744072","National Institutes of Health; University of South Florida; Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9744072","The performance of deep learning models relies heavily on the size and diversity of data. As it is well-known that medical imaging data is scarce and expensive to generate, image augmentation (a.k.a., image synthesis) becomes an important problem in medical images. This work presents a normalizing flow-based method for synthesizing medical images. Our results show superior performance of the proposed method as compared to the state-of-the-arts (e.g., generative models), and demonstrate its ability to generate diverse syntactic medical images, ultimately improving the performance of deep learning models and result in reliable decision-making","Deep learning;Representation learning;Technological innovation;Image synthesis;Syntactics;Generative adversarial networks;Data models","medical image processing;neural nets","synthetic medical images generation;deep generative model;variational autoencoder;NF model;normalizing flows;Wasserstien distance;sample distributions","","","","46","IEEE","1 Apr 2022","","","IEEE","IEEE Conferences"
"End-To-End Retina Image Synthesis Based on CGAN Using Class Feature Loss and Improved Retinal Detail Loss","N. Liang; L. Yuan; X. Wen; H. Xu; J. Wang","Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin, China","IEEE Access","16 Aug 2022","2022","10","","83125","83137","Retinal images are the most direct and effective basis for Diabetic Retinopathy (DR) diagnosis. With the rapid development of deep learning, the technology of retinal image-assisted diagnosis based on deep learning is widely used in the field of DR intelligent diagnosis. However, the training of deep neural network usually requires a large number of annotated samples, but retinal images annotated by professional doctors are cost-expensive and difficult to obtain, which limits the application of deep learning technology in DR intelligent diagnosis. In order to alleviate the scarcity of labelled retinal images, we propose an end-to-end conditional generative adversarial network with class feature loss and improved retinal detail loss. The network combines the above two losses with the adversarial loss, and jointly constrains the generator to generate high-quality retinal images. The proposed retinal detail loss is summed over physiological detail loss which is meant to preserve high-level semantic features of the physiological details contained in the fundus images and pixel loss which ensures the low-level features in synthesized image will not deviate from the real image. In addition, the class feature loss constrains the synthesized images to be consistent with the real images in class features representation, which further makes the synthesized images have pathological features of the corresponding grade. The generated images by the proposed network are evaluated from three objective metrics including the subjective effect and the FID, SWD, which are used to evaluate the quality and diversity of generated images, and the effect of retinal vessel segmentation, respectively. Experimental results demonstrate that our synthesized images have superior performance on both the quality and quantity.","2169-3536","","10.1109/ACCESS.2022.3196377","New-Generation Artificial Intelligence Major Scientific and Technological Special Project of Tianjin(grant numbers:18ZXZNGX00150); Special Foundation for Technology Innovation of Tianjin(grant numbers:21YDTPJC00250); Graduate Scientific Research Innovation Project of Tianjin(grant numbers:2021YJSS088); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9849644","Diabetic retinopathy;retinal image synthesis;conditional generative adversarial network;deep learning;DR~grading","Retina;Generative adversarial networks;Feature extraction;Deep learning;Image synthesis;Physiology;Diabetes","biomedical optical imaging;deep learning (artificial intelligence);diseases;eye;feature extraction;image enhancement;image segmentation;medical image processing;neural nets","diabetic retinopathy diagnosis;retinal image-assisted diagnosis;DR intelligent diagnosis;deep neural network;deep learning technology;labelled retinal images;end-to-end conditional generative adversarial network;high-quality retinal images;physiological detail loss;high-level semantic features;fundus images;low-level features;retinal vessel segmentation;end-to-end retina image synthesis","","","","36","CCBY","4 Aug 2022","","","IEEE","IEEE Journals"
"Dual Projection Generative Adversarial Networks for Conditional Image Generation","L. Han; M. R. Min; A. Stathopoulos; Y. Tian; R. Gao; A. Kadav; D. Metaxas","Department of Computer Science, Rutgers University; NEC Labs America; Department of Computer Science, Rutgers University; Department of Computer Science, Rutgers University; McCombs School of Business, The University of Texas at Austin; NEC Labs America; Department of Computer Science, Rutgers University","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","14418","14427","Conditional Generative Adversarial Networks (cGANs) extend the standard unconditional GAN framework to learning joint data-label distributions from samples, and have been established as powerful generative models capable of generating high-fidelity imagery. A challenge of training such a model lies in properly infusing class information into its generator and discriminator. For the discriminator, class conditioning can be achieved by either (1) directly incorporating labels as input or (2) involving labels in an auxiliary classification loss. In this paper, we show that the former directly aligns the class-conditioned fake-and-real data distributions P (image|class) (data matching), while the latter aligns data-conditioned class distributions P (class|image) (label matching). Although class separability does not directly translate to sample quality and becomes a burden if classification itself is intrinsically difficult, the discriminator cannot provide useful guidance for the generator if features of distinct classes are mapped to the same point and thus become inseparable. Motivated by this intuition, we propose a Dual Projection GAN (P2GAN) model that learns to balance between data matching and label matching. We then propose an improved cGAN model with Auxiliary Classification that directly aligns the fake and real conditionals P (class|image) by minimizing their f-divergence. Experiments on a synthetic Mixture of Gaussian (MoG) dataset and a variety of real-world datasets including CIFAR100, ImageNet, and VGGFace2 demonstrate the efficacy of our proposed models.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711101","Image and video synthesis;Adversarial learning","Training;Computer vision;Image synthesis;Computational modeling;Generative adversarial networks;Generators;Data models","","","","4","","40","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Training Generative Adversarial Networks in One Stage","C. Shen; Y. Yin; X. Wang; X. Li; J. Song; M. Song",Zhejiang University; Zhejiang University; Stevens Institute of Technology; Alibaba Group; Zhejiang Lab; Zhejiang University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","3349","3359","Generative Adversarial Networks (GANs) have demonstrated unprecedented success in various image generation tasks. The encouraging results, however, come at the price of a cumbersome training process, during which the generator and discriminator are alternately updated in two stages. In this paper, we investigate a general training scheme that enables training GANs efficiently in only one stage. Based on the adversarial losses of the generator and discriminator, we categorize GANs into two classes, Symmetric GANs and Asymmetric GANs, and introduce a novel gradient decomposition method to unify the two, allowing us to train both classes in one stage and hence alleviate the training effort. We also computationally analyze the efficiency of the proposed method, and empirically demonstrate that, the proposed method yields a solid 1.5× acceleration across various datasets and network architectures. Furthermore, we show that the proposed method is readily applicable to other adversarial-training scenarios, such as data-free knowledge distillation. The code is available at https://github.com/zju-vipa/OSGAN.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00336","National Natural Science Foundation of China; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578859","","Training;Computer vision;Image synthesis;Network architecture;Generative adversarial networks;Solids;Generators","estimation theory;gradient methods;image processing;learning (artificial intelligence);neural nets","GAN training;adversarial losses;symmetric GAN;asymmetric GAN;gradient decomposition method;training effort;network architectures;adversarial-training scenarios;generative adversarial network training;image generation;training process;general training scheme;data-free knowledge distillation","","4","","77","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Unsupervised Image Generation with Infinite Generative Adversarial Networks","H. Ying; H. Wang; T. Shao; Y. Yang; K. Zhou",Zhejiang University; University of Leeds; Zhejiang University; Clemson University; Zhejiang University,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","14264","14273","Image generation has been heavily investigated in computer vision, where one core research challenge is to generate images from arbitrarily complex distributions with little supervision. Generative Adversarial Networks (GANs) as an implicit approach have achieved great successes in this direction and therefore been employed widely. However, GANs are known to suffer from issues such as mode collapse, non-structured latent space, being unable to compute likelihoods, etc. In this paper, we propose a new unsupervised non-parametric method named mixture of infinite conditional GANs or MIC-GANs, to tackle several GAN issues together, aiming for image generation with parsimonious prior knowledge. Through comprehensive evaluations across different datasets, we show that MIC-GANs are effective in structuring the latent space and avoiding mode collapse, and outperform state-of-the-art methods. MICGANs are adaptive, versatile, and robust. They offer a promising solution to several well-known GAN issues. Code available:github.com/yinghdb/MICGANs.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01402","Zhejiang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711227","Image and video synthesis;Machine learning architectures and formulations","Computer vision;Codes;Image synthesis;Generative adversarial networks","","","","","","59","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"MemGANs: Memory Management for Energy-Efficient Acceleration of Complex Computations in Hardware Architectures for Generative Adversarial Networks","M. A. Hanif; M. Zuhaib Akbar; R. Ahmed; S. Rehman; A. Jantsch; M. Shafique","Technische Universität Wien (TU Wien), Vienna, Austria; National University of Sciences and Technology (NUST), Islamabad, Pakistan; National University of Sciences and Technology (NUST), Islamabad, Pakistan; Technische Universität Wien (TU Wien), Vienna, Austria; Technische Universität Wien (TU Wien), Vienna, Austria; Technische Universität Wien (TU Wien), Vienna, Austria","2019 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)","5 Sep 2019","2019","","","1","6","Generative Adversarial Networks (GANs) have gained importance because of their tremendous unsupervised learning capability and enormous applications in data generation, for example, text to image synthesis, synthetic medical data generation, video generation, and artwork generation. Hardware acceleration for GANs become challenging due to the intrinsic complex computational phases, which require efficient data management during the training and inference. In this work, we propose a distributed on-chip memory architecture, which aims at efficiently handling the data for complex computations involved in GANs, such as strided convolution or transposed convolution. We also propose a controller that improves the computational efficiency by pre-arranging the data from either the off-chip memory or the computational units before storing it in the on-chip memory. Our architectural enhancement supports to achieve 3.65x performance improvement in state-of-the-art, and reduces the number of read accesses and write accesses by 85% and 75%, respectively.","","978-1-7281-2954-9","10.1109/ISLPED.2019.8824833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8824833","Generative Adversarial Networks;Hardware Accelerator;DNN;GAN;DCGAN;Memory Architecture","Registers;Convolution;Memory management;System-on-chip;Arrays;Microarchitecture;Clocks","distributed memory systems;memory architecture;neural nets;storage management","on-chip memory architecture;complex computations;computational efficiency;off-chip memory;computational units;memory management;energy-efficient acceleration;hardware architectures;hardware acceleration;efficient data management;generative adversarial networks","","2","","11","IEEE","5 Sep 2019","","","IEEE","IEEE Conferences"
"Dualattn-GAN: Text to Image Synthesis With Dual Attentional Generative Adversarial Network","Y. Cai; X. Wang; Z. Yu; F. Li; P. Xu; Y. Li; L. Li","Beijing Key Laboratory of Network System and Network Culture, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Network System and Network Culture, Beijing University of Posts and Telecommunications, Beijing, China; Intel China Research Center, Beijing, China; Department of Electrical and Computer Engineering, Portland States University, Portland, USA; Beijing Key Laboratory of Network System and Network Culture, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Network System and Network Culture, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Key Laboratory of Network System and Network Culture, Beijing University of Posts and Telecommunications, Beijing, China","IEEE Access","25 Dec 2019","2019","7","","183706","183716","Recent generative adversarial network based methods have shown promising results for the charming but challenging task of synthesizing images from text descriptions. These approaches can generate images with general shape and color but often produce distorted global structures with unnatural local semantic details. It is due to ineffectiveness of convolutional neural networks in capturing the high-level semantic information for pixel-level image synthesis. In this paper, we propose a Dual Attentional Generative Adversarial Network (DualAttn-GAN) in which the dual attention modules are introduced to enhance local details and global structures by attending to related features from relevant words and different visual regions. As one of the dual modules, the textual attention module is designed to explore the fine-grained interaction between vision and language. On the other hand, visual attention module models internal representations of vision from channel and spatial axes, which can better capture the global structures. Meanwhile, we apply an attention embedding module to merge multi-path features. Furthermore, we present an inverted residual structure to boost representation power of CNNs and apply spectral normalization to stabilize GAN training. With extensive experimental validation on two benchmark datasets, our method significantly improves state-of-the-art models over the evaluation metrics of inception score and Fréchet inception distance.","2169-3536","","10.1109/ACCESS.2019.2958864","National Natural Science Foundation of China(grant numbers:61672108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930532","Generative adversarial network;textual attention;visual attention;inverted residual structure;spectral normalization","Visualization;Generative adversarial networks;Gallium nitride;Semantics;Training;Image synthesis;Task analysis","computer vision;convolutional neural nets;feature extraction;image colour analysis;image enhancement;image representation;learning (artificial intelligence);video signal processing","generative adversarial network based methods;convolutional neural networks;high-level semantic information;pixel-level image synthesis;DualAttn-GAN;dual attention modules;textual attention module;attention embedding module;dual attentional generative adversarial network;Dualattn-GAN;inception score metric;Fréchet inception distance metric;text to image synthesis","","20","","47","CCBY","10 Dec 2019","","","IEEE","IEEE Journals"
"StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesis","M. Meshry; Y. Ren; L. S. Davis; A. Shrivastava","University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park; University of Maryland, College Park","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","3711","3720","We propose a novel approach for multi-modal Image-to-image (I2I) translation. To tackle the one-to-many relationship between input and output domains, previous works use complex training objectives to learn a latent embedding, jointly with the generator, that models the variability of the output domain. In contrast, we directly model the style variability of images, independent of the image synthesis task. Specifically, we pre-train a generic style encoder using a novel proxy task to learn an embedding of images, from arbitrary domains, into a low-dimensional style latent space. The learned latent space introduces several advantages over previous traditional approaches to multi-modal I2I translation. First, it is not dependent on the target dataset, and generalizes well across multiple domains. Second, it learns a more powerful and expressive latent space, which improves the fidelity of style capture and transfer. The proposed style pre-training also simplifies the training objective and speeds up the training significantly. Furthermore, we provide a detailed study of the contribution of different loss terms to the task of multi-modal I2I translation, and propose a simple alternative to VAEs to enable sampling from unconstrained latent spaces. Finally, we achieve state-of-the-art results on six challenging benchmarks with a simple training objective that includes only a GAN loss and a reconstruction loss.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578574","","Training;Computer vision;Codes;Image synthesis;Benchmark testing;Generative adversarial networks;Generators","approximation theory;image sampling;learning (artificial intelligence)","complex training objectives;latent embedding;output domain;style variability;image synthesis task;generic style;arbitrary domains;low-dimensional style latent space;learned latent space;traditional approaches;powerful space;expressive latent space;style capture;unconstrained latent spaces;simple training objective;encoder pre-training;multimodal Image synthesis;output domains;multimodal Image-to-image translation;style pretraining;proxy task","","2","","57","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Robust LWIR-based Eye Center Detection through Thermal to Visible Image Synthesis","S. R. Mokalla; T. Bourlai","College of Electrical and Computer Engineering, University of Georgia, Athens, USA; College of Electrical and Computer Engineering, University of Georgia, Athens, USA","2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)","12 Jan 2022","2021","","","1","8","This paper proposes a novel approach to automatically detect the eye centers in challenging thermal (Long-Wave Infrared; 8 – 14 µm) face images so they can be geometrically normalized, which is an important part of face recognition systems. Developing a new LWIR based eye center detection model would require many thermal images, which is not possible due to the unavailability of publicly available large scale LWIR face datasets. An alternative solution would be to use a pre-trained, visible band based facial landmark detection model and test it on LWIR face images. In the latter case, the challenges are the significant differences between the visible and thermal face images. In this paper, we focus on addressing this research gap by proposing a solution that is based on the following approach. First, we synthesize visible from thermal face images. Then, we exploit the existing robust visible band facial landmark detection models to detect eye centers in the synthesized visible band face images. While we empirically test different image synthesis models, we determine that StarGAN2 (an image-to-image translation generative adversarial network model that learns a mapping between the different visual domains) yields the highest eye center detection accuracy when compared to the other state-of-the-art models. Thus, we train a StarGAN2 model to be able to synthesize good quality visible band images from their thermal band counterparts. Next, we use an efficient visible band based facial landmark detection model to detect the eye centers in the synthesized visible band face images. Finally, we map these coordinates to the original LWIR face images, which are used for geometric normalization and, finally run a set of face recognition experiments. Compared to the baseline model, our approach increases the eye detection accuracy by 14%, 50%, and 30% when the normalized error (e) is set to be ≤ 0.05, ≤ 0.10, and ≤ 0.25 respectively compared to the baseline model. Our approach yields up to a 15 % and 7% increase in face recognition accuracy when using advanced deep-learning based matchers, namely Facenet and VGGFace respectively, and by 30% when using other conventional face matching techniques such as LBP-LTP matchers.","","978-1-6654-3176-7","10.1109/FG52635.2021.9667069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667069","","Visualization;Image synthesis;Face recognition;Conferences;Gesture recognition;Generative adversarial networks","deep learning (artificial intelligence);face recognition;infrared imaging;object detection","robust LWIR-based eye center detection;visible image synthesis;face recognition systems;LWIR based eye center detection model;face datasets;visible face images;thermal face images;image-to-image translation generative adversarial network model;StarGAN2 model;thermal band counterparts;face recognition experiments;baseline model;face matching techniques;robust visible band facial landmark detection models;visible band based facial landmark detection model;visible band face image synthesis;long-wave infrared","","","","36","IEEE","12 Jan 2022","","","IEEE","IEEE Conferences"
"PoT-GAN: Pose Transform GAN for Person Image Synthesis","T. Li; W. Zhang; R. Song; Z. Li; J. Liu; X. Li; S. Lu","School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; School of Control Science and Engineering, Shandong University, Jinan, China; Information Systems Technology and Design Pillar, Singapore University of Technology and Design, Singapore; School of Control Science and Engineering, Shandong University, Jinan, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore","IEEE Transactions on Image Processing","8 Sep 2021","2021","30","","7677","7688","Pose-based person image synthesis aims to generate a new image containing a person with a target pose conditioned on a source image containing a person with a specified pose. It is challenging as the target pose is arbitrary and often significantly differs from the specified source pose, which leads to large appearance discrepancy between the source and the target images. This paper presents the Pose Transform Generative Adversarial Network (PoT-GAN) for person image synthesis where the generator explicitly learns the transform between the two poses by manipulating the corresponding multi-scale feature maps. By incorporating the learned pose transform information into the multi-scale feature maps of the source image in a GAN architecture, our method reliably transfers the appearance of the person in the source image to the target pose with no need for any hard-coded spatial information depicting the change of pose. According to both qualitative and quantitative results, the proposed PoT-GAN demonstrates a state-of-the-art performance on three publicly available datasets for person image synthesis.","1941-0042","","10.1109/TIP.2021.3104183","National Natural Science Foundation of China(grant numbers:U1913204,61991411,62076148); Natural Science Foundation of Shandong Province for Distinguished Young Scholars(grant numbers:ZR2020JQ29); Shandong Major Scientific and Technological Innovation(grant numbers:2019JZZY010428); Young Taishan Scholars Program of Shandong Province(grant numbers:tsqn201909029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524559","Image synthesis;pose transform;generative adversarial network","Transforms;Image synthesis;Generative adversarial networks;Generators;Training data;Feature extraction;Computer architecture","","","Algorithms;Humans;Image Processing, Computer-Assisted","3","","48","IEEE","27 Aug 2021","","","IEEE","IEEE Journals"
"RiFeGAN: Rich Feature Generation for Text-to-Image Synthesis From Prior Knowledge","J. Cheng; F. Wu; Y. Tian; L. Wang; D. Tao","The Chinese University of Hong Kong, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China; Graduate School of Information, Production and Systems, Waseda University, Japan; The Chinese University of Hong Kong, Hong Kong, China; School of Information Science and Engineering, Yunnan University, China","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","10908","10917","Text-to-image synthesis is a challenging task that generates realistic images from a textual sequence, which usually contains limited information compared with the corresponding image and so is ambiguous and abstractive. The limited textual information only describes a scene partly, which will complicate the generation with complementing the other details implicitly and lead to low-quality images. To address this problem, we propose a novel rich feature generating text-to-image synthesis, called RiFeGAN, to enrich the given description. In order to provide additional visual details and avoid conflicting, RiFeGAN exploits an attention-based caption matching model to select and refine the compatible candidate captions from prior knowledge. Given enriched captions, RiFeGAN uses self-attentional embedding mixtures to extract features across them effectually and handle the diverging features further. Then it exploits multi-captions attentional generative adversarial networks to synthesize images from those features. The experiments conducted on widely-used datasets show that the models can generate images from enriched captions effectually and improve the results significantly.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.01092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156682","","Birds;Generators;Training;Gallium nitride;Feature extraction;Task analysis;Generative adversarial networks","feature extraction;neural nets;realistic images;text analysis;video signal processing","text-to-image synthesis;RiFeGAN;realistic images;feature generation;generative adversarial networks;candidate captions;attention-based caption matching model;visual details;image quality;textual information;textual sequence","","20","","38","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"ArtGAN: Artwork synthesis with conditional categorical GANs","W. R. Tan; C. S. Chan; H. E. Aguirre; K. Tanaka","Faculty of Engineering, Shinshu University, Nagano, Japan; Centre of Image & Signal Processing, University of Malaya, Malaysia; Faculty of Engineering, Shinshu University, Nagano, Japan; Faculty of Engineering, Shinshu University, Nagano, Japan","2017 IEEE International Conference on Image Processing (ICIP)","22 Feb 2018","2017","","","3760","3764","This paper proposes an extension to the Generative Adversarial Networks (GANs), namely as ArtGAN to synthetically generate more challenging and complex images such as artwork that have abstract characteristics. This is in contrast to most of the current solutions that focused on generating natural images such as room interiors, birds, flowers and faces. The key innovation of our work is to allow back-propagation of the loss function w.r.t. the labels (randomly assigned to each generated images) to the generator from the discriminator. With the feedback from the label information, the generator is able to learn faster and achieve better generated image quality. Empirically, we show that the proposed ArtGAN is capable to create realistic artwork, as well as generate compelling real world images that globally look natural with clear shape on CIFAR-10.","2381-8549","978-1-5090-2175-8","10.1109/ICIP.2017.8296985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296985","image synthesis;generative adversarial networks;deep learning","Gallium nitride;Painting;Image reconstruction;Generators;Training;Shape;Art","art;image processing","ArtGAN;artwork synthesis;Generative Adversarial Networks;complex images;natural images;label information;conditional categorical GAN;image quality","","54","","19","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Exploring Explicit Domain Supervision for Latent Space Disentanglement in Unpaired Image-to-Image Translation","J. Lin; Z. Chen; Y. Xia; S. Liu; T. Qin; J. Luo","University of Science and Technology of China, Hefei, Anhui, China; University of Science and Technology of China, Hefei, Anhui, China; Microsoft Research Asia, Beijing, China; University of Science and Technology of China, Hefei, Anhui, China; Microsoft Research Asia, Beijing, China; Department of Computer Science, University of Rochester, Rochester, NY, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","5 Mar 2021","2021","43","4","1254","1266","Image-to-image translation tasks have been widely investigated with Generative Adversarial Networks (GANs). However, existing approaches are mostly designed in an unsupervised manner, while little attention has been paid to domain information within unpaired data. In this article, we treat domain information as explicit supervision and design an unpaired image-to-image translation framework, Domain-supervised GAN (DosGAN), which takes the first step towards the exploration of explicit domain supervision. In contrast to representing domain characteristics using different generators or domain codes, we pre-train a classification network to explicitly classify the domain of an image. After pre-training, this network is used to extract the domain-specific features of each image. Such features, together with the domain-independent features extracted by another encoder (shared across different domains), are used to generate image in target domain. Extensive experiments on multiple facial attribute translation, multiple identity translation, multiple season translation and conditional edges-to-shoes/handbags demonstrate the effectiveness of our method. In addition, we can transfer the domain-specific feature extractor obtained on the Facescrub dataset with domain supervision information to unseen domains, such as faces in the CelebA dataset. We also succeed in achieving conditional translation with any two images in CelebA, while previous models like StarGAN cannot handle this task.","1939-3539","","10.1109/TPAMI.2019.2950198","National Natural Science Foundation of China(grant numbers:61571413,61632001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886528","Image-to-image translation;explicit domain supervision;generative adversarial networks","Feature extraction;Task analysis;Gallium nitride;Generative adversarial networks;Generators;Data mining;Image synthesis","face recognition;feature extraction;image classification;neural nets;supervised learning;visual databases","explicit domain supervision;latent space disentanglement;generative adversarial networks;unpaired data;unpaired image-to-image translation framework;domain characteristics;domain-independent features;multiple facial attribute translation;multiple identity translation;multiple season translation;domain-specific feature extractor;domain supervision information;unseen domains;conditional translation;domain-supervised GAN;DosGAN;domain codes;classification network;domain-independent feature extraction;Facescrub dataset;CelebA dataset","","23","","42","IEEE","30 Oct 2019","","","IEEE","IEEE Journals"
"Image Generation Framework for Unbalanced License Plate Data Set","M. Sun; F. Zhou; C. Yang; X. Yin","University of Science and Technology Beijing, Beijing, China; University of Science and Technology Beijing, Beijing, China; University of Science and Technology Beijing, Beijing, China; University of Science and Technology Beijing, Beijing, China","2019 International Conference on Data Mining Workshops (ICDMW)","13 Jan 2020","2019","","","883","889","Deep learning based methods have achieved promising performance in the image fields, but they are significantly sensitive to the distribution of data. To obtain balanced data, the acquisition of annotation data is very time-consuming and laborious. Generative Adversarial Networks have made a lot of progress in data generation but may degrade greatly in the unbalanced data set. In this paper, we propose an image generation framework to generate photo-realistic, various, and balanced images of text. Specifically, we add the module for training unpaired images(U-module) and target selector to our framework, and the target selector uses text string to select images in the extended real images, which contain real images and the generated images by the U-module, in addition, global generator and local enhancer network are applied to improve the quality of the generated images. We demonstrate our method on the Chinese license plates, and the unbalance of the license plate data set is shown in the provincial category and the special license plates. The Inception Score and the FID Score are used as metrics to validate our method. The experimental results show that the Inception Score of the generated images is close to that of the real images, and our method achieves the lower FID Score than the state-of-the-art. In the SYSU-ITS dataset, the accuracy of license plate recognition has been largely improved, especially for the provinces with no or few real samples.","2375-9259","978-1-7281-4896-0","10.1109/ICDMW.2019.00129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955649","Unbalanced data, Image generation, Generative adversarial networks, License plate recognition","License plate recognition;Training;Generators;Image synthesis;Generative adversarial networks;Feature extraction;Image segmentation","character recognition;image recognition;neural nets;text analysis;traffic engineering computing","U-module;deep learning;text string;unbalanced license plate data set;license plate recognition;special license plates;Chinese license plates;local enhancer network;global generator;target selector;balanced images;photo-realistic images;image generation framework;unbalanced data;data generation;generative adversarial networks;annotation data","","1","","23","IEEE","13 Jan 2020","","","IEEE","IEEE Conferences"
"A Shared Representation for Photorealistic Driving Simulators","S. Saadatnejad; S. Li; T. Mordan; A. Alahi","VITA Laboratory, EPFL, Lausanne, Switzerland; VITA Laboratory, EPFL, Lausanne, Switzerland; VITA Laboratory, EPFL, Lausanne, Switzerland; VITA Laboratory, EPFL, Lausanne, Switzerland","IEEE Transactions on Intelligent Transportation Systems","10 Aug 2022","2022","23","8","13835","13845","A powerful simulator highly decreases the need for real-world tests when training and evaluating autonomous vehicles. Data-driven simulators flourished with the recent advancement of conditional Generative Adversarial Networks (cGANs), providing high-fidelity images. The main challenge is synthesizing photorealistic images while following given constraints. In this work, we propose to improve the quality of generated images by rethinking the discriminator architecture. The focus is on the class of problems where images are generated given semantic inputs, such as scene segmentation maps or human body poses. We build on successful cGAN models to propose a new semantically-aware discriminator that better guides the generator. We aim to learn a shared latent representation that encodes enough information to jointly do semantic segmentation, content reconstruction, along with a coarse-to-fine grained adversarial reasoning. The achieved improvements are generic and simple enough to be applied to any architecture of conditional image synthesis. We demonstrate the strength of our method on the scene, building, and human synthesis tasks across three different datasets. The code is available https://github.com/vita-epfl/SemDisc.","1558-0016","","10.1109/TITS.2021.3131303","European Union’s Horizon 2020 Research and Innovation Program under the Marie Skodowska-Curie(grant numbers:754354); Valeo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635715","Image synthesis;generative adversarial networks;autonomous vehicles;shared representation","Semantics;Image synthesis;Generators;Training;Task analysis;Image segmentation;Image reconstruction","image representation;image segmentation;inference mechanisms;interactive systems;learning (artificial intelligence);pose estimation;rendering (computer graphics)","shared representation;photorealistic driving;powerful simulator;real-world tests;evaluating autonomous vehicles;data-driven simulators;conditional Generative Adversarial Networks;cGANs;high-fidelity images;photorealistic images;discriminator architecture;semantic inputs;scene segmentation maps;human body poses;successful cGAN models;semantically-aware discriminator;shared latent representation;semantic segmentation;coarse-to-fine grained adversarial reasoning;achieved improvements;conditional image synthesis;building;human synthesis tasks","","","","53","IEEE","3 Dec 2021","","","IEEE","IEEE Journals"
"SoloGAN: Multi-domain Multimodal Unpaired Image-to-Image Translation via a Single Generative Adversarial Network","S. Huang; C. He; R. Cheng","Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; Guangdong Provincial Key Laboratory of Brain-inspired Intelligent Computation, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China","IEEE Transactions on Artificial Intelligence","21 Oct 2022","2022","3","5","722","737","Despite significant advances in image-to-image (I2I) translation with generative adversarial networks (GANs), it remains challenging to effectively translate an image to a set of diverse images in multiple target domains using a pair of generator and discriminator. Existing multimodal I2I translation methods adopt multiple domain-specific content encoders for different domains, where each domain-specific content encoder is trained with images from the same domain only. Nevertheless, we argue that the content (domain-invariance) features should be learned from images among all of the domains. Consequently, each domain-specific content encoder of existing schemes fails to extract the domain-invariant features efficiently. To address this issue, we present a flexible and general SoloGAN model for efficient multimodal I2I translation among multiple domains with unpaired data. In contrast to existing methods, the SoloGAN algorithm uses a single projection discriminator with an additional auxiliary classifier and shares the encoder and generator for all domains. As such, the SoloGAN model can be trained effectively with images from all domains so that the domain-invariance content representation can be efficiently extracted. Qualitative and quantitative results over a wide range of datasets against several counterparts and variants of the SoloGAN model demonstrate the merits of the method, especially for challenging I2I translation tasks, i.e., tasks that involve extreme shape variations or need to keep the complex backgrounds unchanged after translations. Furthermore, we demonstrate the contribution of each component using ablation studies.","2691-4581","","10.1109/TAI.2022.3187384","National Natural Science Foundation of China(grant numbers:61906081,U20A20306); Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X386); Shenzhen Peacock Plan(grant numbers:KQTD2016112514355531); Guangdong Provincial Key Laboratory(grant numbers:2020B121201001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9811405","Generative adversarial network (GAN);image synthesis;image-to-image (I2I) translation","Generative adversarial networks;Image synthesis","feature extraction;learning (artificial intelligence);object detection","content features;diverse images;domain-invariance content representation;domain-invariant features;domain-specific content encoder;efficient multimodal;flexible SoloGAN model;general SoloGAN model;generative adversarial networks;generator;multidomain multimodal unpaired image-to-image;multiple domain-specific;multiple target domains;single generative adversarial network;translation methods;translation tasks","","","","55","IEEE","30 Jun 2022","","","IEEE","IEEE Journals"
"Historical Document Synthesis with Generative Adversarial Networks","V. Pondenkandath; M. Alberti; M. Diatta; R. Ingold; M. Liwicki","Document Image and Voice Analysis Group (DIVA), University of Fribourg, Switzerland; Document Image and Voice Analysis Group (DIVA), University of Fribourg, Switzerland; Document Image and Voice Analysis Group, University of Fribourg, Fribourg, Switzerland; Document Image and Voice Analysis Group (DIVA), University of Fribourg, Switzerland; Machine Learning Group, Luleå University of Technology, Sweden","2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)","7 Nov 2019","2019","5","","146","151","The following topics are dealt with: learning (artificial intelligence); document image processing; convolutional neural nets; text analysis; handwritten character recognition; feature extraction; neural nets; optical character recognition; image segmentation; image recognition.","","978-1-7281-5054-3","10.1109/ICDARW.2019.40096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8893115","historical document;deep learning;document synthesis","Task analysis;Training;Software;Gallium nitride;Electronic publishing;Image synthesis;Generators","convolutional neural nets;document image processing;handwritten character recognition;image segmentation;learning (artificial intelligence);text analysis","learning (artificial intelligence);document image processing;convolutional neural nets;text analysis;handwritten character recognition;feature extraction;optical character recognition;image segmentation;image recognition;International Conference on Document Analysis and Recognition Workshops","","6","","35","IEEE","7 Nov 2019","","","IEEE","IEEE Conferences"
"Active Image Synthesis for Efficient Labeling","J. Chen; Y. Xie; K. Wang; C. Zhang; M. A. Vannan; B. Wang; Z. Qian","H. Milton Stewart School of Industrial and Systems Engineering, Georgia Tech Manufacturing Institute, Georgia Institute of Technology, Atlanta, GA, USA; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA; Gerogia Tech Manufacturing Institute, Georgia Institute of Technology, Atlanta, GA, USA; H. Milton Stewart School of Industrial and Systems Engineering, Georgia Tech Manufacturing Institute, Georgia Institute of Technology, Atlanta, GA, USA; Marcus Heart Valve Center, Piedmont Heart Institute, Atlanta, GA, USA; H. Milton Stewart School of Industrial and Systems Engineering, School of Materials Science and Engineering, Georgia Institute of Technology, Gerogia Tech Manufacturing Institute, Atlanta, GA, USA; Hippocrates Research Lab, Tencent America, Palo Alto, CA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","1 Oct 2021","2021","43","11","3770","3781","The great success achieved by deep neural networks attracts increasing attention from the manufacturing and healthcare communities. However, the limited availability of data and high costs of data collection are the major challenges for the applications in those fields. We propose in this work AISEL, an active image synthesis method for efficient labeling, to improve the performance of the small-data learning tasks. Specifically, a complementary AISEL dataset is generated, with labels actively acquired via a physics-based method to incorporate underlining physical knowledge at hand. An important component of our AISEL method is the bidirectional generative invertible network (GIN), which can extract interpretable features from the training images and generate physically meaningful virtual images. Our AISEL method then efficiently samples virtual images not only further exploits the uncertain regions but also explores the entire image space. We then discuss the interpretability of GIN both theoretically and experimentally, demonstrating clear visual improvements over the benchmarks. Finally, we demonstrate the effectiveness of our AISEL framework on aortic stenosis application, in which our method lowers the labeling cost by 90 percent while achieving a 15 percent improvement in prediction accuracy.","1939-3539","","10.1109/TPAMI.2020.2993221","National Science Foundation(grant numbers:CMMI-1921646); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091327","Active learning;computer-aided diagnosis;data augmentation;generative adversarial networks;small-data learning","Task analysis;Gallium nitride;Labeling;Manufacturing;Feature extraction;Generative adversarial networks;Medical services","cardiology;data acquisition;deep learning (artificial intelligence);feature extraction;health care;medical image processing","deep neural networks;health care communities;data collection;physics-based method;bidirectional generative invertible network;GIN;training images;virtual images;image space;clear visual improvements;aortic stenosis;labeling cost;small-data learning;active image synthesis;AISEL dataset;physical knowledge;interpretable feature extraction","","6","","54","IEEE","11 May 2020","","","IEEE","IEEE Journals"
"3D Segmentation Guided Style-Based Generative Adversarial Networks for PET Synthesis","Y. Zhou; Z. Yang; H. Zhang; E. I. -C. Chang; Y. Fan; Y. Xu","State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, School of Biological Science and Medical Engineering, Research Institute of Beihang University in Shenzhen, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, School of Biological Science and Medical Engineering, Research Institute of Beihang University in Shenzhen, Beihang University, Beijing, China; Department of Biomedical Engineering, Tsinghua University, Beijing, China; Microsoft Research, Beijing, China; State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, School of Biological Science and Medical Engineering, Research Institute of Beihang University in Shenzhen, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, School of Biological Science and Medical Engineering, Research Institute of Beihang University in Shenzhen, Beihang University, Beijing, China","IEEE Transactions on Medical Imaging","1 Aug 2022","2022","41","8","2092","2104","Potential radioactive hazards in full-dose positron emission tomography (PET) imaging remain a concern, whereas the quality of low-dose images is never desirable for clinical use. So it is of great interest to translate low-dose PET images into full-dose. Previous studies based on deep learning methods usually directly extract hierarchical features for reconstruction. We notice that the importance of each feature is different and they should be weighted dissimilarly so that tiny information can be captured by the neural network. Furthermore, the synthesis on some regions of interest is important in some applications. Here we propose a novel segmentation guided style-based generative adversarial network (SGSGAN) for PET synthesis. (1) We put forward a style-based generator employing style modulation, which specifically controls the hierarchical features in the translation process, to generate images with more realistic textures. (2) We adopt a task-driven strategy that couples a segmentation task with a generative adversarial network (GAN) framework to improve the translation performance. Extensive experiments show the superiority of our overall framework in PET synthesis, especially on those regions of interest.","1558-254X","","10.1109/TMI.2022.3156614","National Science and Technology Major Project of the Ministry of Science and Technology in China(grant numbers:2017YFC0110903); National Natural Science Foundation of China(grant numbers:62022010,81771910); SinoUnion Healthcare Inc., under the eHealth Program; Fundamental Research Funds for the Central Universities of China from the State Key Laboratory of Software Development Environment in Beihang University in China; 111 Project in China(grant numbers:B13003); High Performance Computing (HPC) Resources at Beihang University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727148","PET;GAN;style modulation;task-driven;segmentation","Image segmentation;Task analysis;Positron emission tomography;Modulation;Three-dimensional displays;Image synthesis;Generators","image reconstruction;image representation;image segmentation;image texture;learning (artificial intelligence);medical image processing;positron emission tomography","3d segmentation guided style-based generative adversarial networks;PET synthesis;potential radioactive hazards;full-dose positron emission tomography;low-dose images;low-dose PET images;deep learning methods;hierarchical features;neural network;style-based generative adversarial network;style-based generator;style modulation;segmentation task;generative adversarial network framework","Image Processing, Computer-Assisted;Neural Networks, Computer;Positron-Emission Tomography","","","66","IEEE","3 Mar 2022","","","IEEE","IEEE Journals"
"Chest Radiography Few-Shot Image Synthesis for Automated Pathology Screening Applications","M. Q. E Sousa; J. Pedrosa; J. Rocha; S. C. Pereira; A. M. Mendonça; A. Campilho","Faculty of Engineering, University of Porto (FEUP), Porto, Portugal; Faculty of Engineering, University of Porto (FEUP), Porto, Portugal; Faculty of Engineering, University of Porto (FEUP), Porto, Portugal; Faculty of Engineering, University of Porto (FEUP), Porto, Portugal; Faculty of Engineering, University of Porto (FEUP), Porto, Portugal; Faculty of Engineering, University of Porto (FEUP), Porto, Portugal","2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","14 Jan 2022","2021","","","1791","1798","Chest radiography is one of the most ubiquitous imaging modalities, playing an essential role in screening, diagnosis and disease management. However, chest radiography interpretation is a time-consuming and complex task, requiring the availability of experienced radiologists. As such, automated diagnosis systems for pathology detection have been proposed aiming to reduce the burden on radiologists and reduce variability in image interpretation. While promising results have been obtained, particularly since the advent of deep learning, there are significant limitations in the developed solutions, namely the lack of representative data for less frequent pathologies and the learning of biases from the training data, such as patient position, medical devices and other markers as proxies for certain pathologies. The lack of explainability is also a challenge for the adoption of these solutions in clinical practice.Generative adversarial networks could play a significant role as a solution for these challenges as they allow to artificially create new realistic images. This way, new synthetic chest radiography images could be used to increase the prevalence of less represented pathology classes and decrease model biases as well as improving the explainability of automatic decisions by generating samples that serve as examples or counter-examples to the image being analysed, ensuring patient privacy.In this study, a few-shot generative adversarial network is used to generate synthetic chest radiography images. A minimum Fréchet Inception Distance score of 17.83 was obtained, allowing to generate convincing synthetic images. Perceptual validation was then performed by asking multiple readers to classify a mixed set of synthetic and real images. An average accuracy of 83.5% was obtained but a strong dependency on reader experience level was observed. While synthetic images showed structural irregularities, the overall image sharpness was a major factor in the decision of readers. The synthetic images were then validated using a MobileNet abnormality classifier and it was shown that over 99% of images were classified correctly, indicating that the generated images were correctly interpreted by the classifier. Finally, the use of the synthetic images during training of a YOLOv5 pathology detector showed that the addition of the synthetic images led to an improvement of mean average precision of 0.05 across 14 pathologies.In conclusion, the usage of few-shot generative adversarial networks for chest radiography image generation was shown and tested in multiple scenarios, establishing a baseline for future experiments to increase the applicability of generative models in clinical scenarios of automatic CXR screening and diagnosis tools.","","978-1-6654-0126-5","10.1109/BIBM52615.2021.9669859","European Regional Development Fund; Foundation for Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669859","","Training;Pathology;Privacy;Medical devices;Image synthesis;Training data;Detectors","computerised tomography;diagnostic radiography;diseases;feature extraction;learning (artificial intelligence);medical image processing;patient diagnosis","frequent pathologies;generative adversarial networks;realistic images;synthetic chest radiography images;represented pathology classes;few-shot generative adversarial network;convincing synthetic images;image sharpness;YOLOv5 pathology detector;chest radiography image generation;diagnosis tools;few-shot image synthesis;automated pathology screening applications;ubiquitous imaging modalities;disease management;chest radiography interpretation;automated diagnosis systems;pathology detection;image interpretation","","","","24","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"Mitigating Label Noise in GANs via Enhanced Spectral Normalization","Y. Chen; C. Jin; G. Li; T. H. Li; W. Gao","Peng Cheng Laboratory, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China; School of Electronic and Computer Engineering, Peking University, Shenzhen, China; Advanced Institute of Information Technology, Peking University, Hangzhou, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Transactions on Circuits and Systems for Video Technology","","2023","PP","99","1","1","Label noise is a ubiquitous issue in GANs, which degrades the generalization ability of the discriminator and usually leads to instability when training GANs. This issue stems from both real data and generated data. Previous works either only consider one of these two sources, or are not robust enough to noisy labels. In this paper, we revisit spectral normalization in robust learning with noisy labels. Based on its pros and cons, we propose to combine spectral normalization and weight decay to regularize the discriminator, which enjoys a more robust training process. To extend to conditional GANs, we propose to balance the relative importance of marginal matching and conditional matching in the projection discriminator. The proposed Enhanced Spectral Normalization for Generative Adversarial Networks (ESNGAN) can be easily integrated into various existing GANs frameworks without excessive additional cost. The effectiveness of the proposed method is validated on the CIFAR10, LSUN Church, CelebA, and ImageNet datasets, including the unconditional image generation task and the class-conditional image generation task. We also show that the proposed method can further improve the performance of the high-resolution image generation task.","1558-2205","","10.1109/TCSVT.2023.3235410","Shenzhen Fundamental Research Program(grant numbers:GXWD20201231165807007-20200806163656003); National Natural Science Foundation of China(grant numbers:62172021); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012343","Generative adversarial networks;robust learning;image synthesis;learning with noisy labels","Noise measurement;Training;Task analysis;Image synthesis;Generators;Generative adversarial networks;Neural networks","","","","","","","IEEE","9 Jan 2023","","","IEEE","IEEE Early Access Articles"
"JGAN: A Joint Formulation of GAN for Synthesizing Images and Labels","M. Park","Intel, Seoul, South Korea","IEEE Access","23 Oct 2020","2020","8","","188883","188888","Image generation with explicit condition or label generally works better than unconditional methods. In modern GAN frameworks, both generator and discriminator are formulated to model the conditional distribution of images given with labels. In this article, we provide an alternative formulation of GAN which models the joint distribution of images and labels. There are two advantages in this joint formulation over conditional approaches. The first advantage is that the joint formulation is more robust to label noises if it's properly modeled. This alleviates the burden of making noise-free labels and allows the use of weakly-supervised labels in image generation. The second is that we can use any kinds of weak labels or image features that have correlations with the original image data to enhance unconditional image generation. We will show the effectiveness of our joint formulation on CIFAR10, CIFAR100, and STL dataset with the state-of-the-art GAN architecture.","2169-3536","","10.1109/ACCESS.2020.3031292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224732","Deep learning;image synthesis;generative adversarial network","Generators;Image synthesis;Gallium nitride;Generative adversarial networks;Probability distribution;Training;Network architecture","image classification;supervised learning","STL dataset;CIFAR100 dataset;CIFAR10 dataset;JGAN;unconditional image generation;image data;image features;weakly-supervised labels;noise-free labels;conditional approaches;joint distribution;explicit condition;joint formulation","","2","","20","CCBY","15 Oct 2020","","","IEEE","IEEE Journals"
"Effects of Demographics and Photometric Normalization on Image Translation GANs for Cross-Spectral Face Recognition","S. R. Mokalla; T. Bourlai","College of Electrical and Computer Engineering, University of Georgia, Athens, GA, USA; College of Electrical and Computer Engineering, University of Georgia, Athens, GA, USA","2021 IEEE International Conference on Big Data (Big Data)","13 Jan 2022","2021","","","2109","2118","This paper focuses on thermal-to-visible face matching through image synthesis. Most of the legacy face image datasets are composed of visible band data. Thermal band as well as dual band, i.e. visible and thermal face datasets, are limited. Operating in the thermal band and therefore working on visible thermal face recognition (FR) systems can be beneficial in various scenarios. The challenge is cross-spectral matching, i.e. matching gallery, visible band, face images against thermal ones. To address this problem, we train and test two of the most popular image-to-image translation Generative Adversarial Networks (GANs). These are Pix2pix and StarGAN2. In this work, the two aforementioned GAN trained models are tested, and the visible face images generated are matched against the ground truth visible faces using one of the most powerful visible-to-visible face matching algorithms, namely Facenet. We also perform an ablation study where the original thermal and visible images are photometrically normalized before training the image synthesis-specific models. The main outcomes of our study are that the FR accuracy from the pix2pix model did not vary significantly; when using StarGAN2, the original face images yield much higher accuracy compared to the photometrically normalized ones; finally, we observe that, when using the pix2pix model for image synthesis, bearded and non-Caucasian generated face images suffer the most from different noise factors. Specifically, the FR accuracy when using pix2pix after image synthesis yields a face verification area under curve (AUC) of 58.3%, while the same models when tested on data excluding bearded and non-Caucasian faces yields an accuracy of 68.6%.","","978-1-6654-3902-2","10.1109/BigData52589.2021.9671292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671292","Image synthesis;Thermal-to-visible;Pix2pix;StarGAN2;Photometric normalization;Thermal spectrum;Visible spectrum;Multi-spectral;Cross-spectral;Face recognition","Training;Image recognition;Image synthesis;Face recognition;Conferences;Dual band;Big Data","demography;face recognition;feature extraction;image matching;infrared imaging;neural nets;photometry","photometric normalization;cross-spectral face recognition;thermal-to-visible face;legacy face image datasets;visible band data;thermal band;dual band;visible face datasets;thermal face datasets;visible thermal face recognition systems;cross-spectral matching;matching gallery;image-to-image translation;StarGAN2;aforementioned GAN trained models;visible face images;ground truth visible faces;visible-to-visible face matching algorithms;original thermal images;visible images;image synthesis-specific models;FR accuracy;pix2pix model;original face images;photometrically normalized ones;image synthesis yields;face verification area;nonCaucasian faces;generative adversarial networks;image translation GAN;demographics normalization;thermal-to-visible face matching;image synthesis;visible band;face images;face verification;area under curve","","","","46","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Facial Age and Expression Synthesis Using Ordinal Ranking Adversarial Networks","Y. Sun; J. Tang; Z. Sun; M. Tistarelli","School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Center for Excellence in Brain Science and Intelligence Technology, Chinese Academy of Sciences, Beijing, China; Department of Science and Information Technology, University of Sassari, Sassari, Italy","IEEE Transactions on Information Forensics and Security","31 Mar 2020","2020","15","","2960","2972","Facial image synthesis has been extensively studied, for a long time, in both computer graphics and computer vision. Particularly, the synthesis of face images with varying ages, expressions and poses has received an increasing attention owing to several real-world applications. In this paper, facial age and expression synthesis are addressed. While previous and current research papers on facial age synthesis mostly adopt an age span of 10 years, this paper investigates face aging with a shorter time span. For expression synthesis, given a neutral face, we work on synthesizing faces with varying expression intensities (e.g., from zero to high). Note that both human ages and expression intensities are inherently ordinal. To fully exploit this ordinal nature, we devise ordinal ranking generative adversarial networks (ranking GAN). For each face, a one-hot label is assigned to define its age range/expression intensity. By exploiting the relative order information among age ranges/expression intensities, a binary ranking vector is further computed for each face. In ranking GAN, one-hot labels are used as the condition of the generator for synthesizing faces with target age groups/expression intensities. Moreover, we add a sequence of cost-sensitive ordinal rankers on top of several multi-scale discriminators, with the aim of minimizing age/intensity rank estimation loss when optimizing both the generator and discriminators. In order to evaluate the proposed ranking GAN, extensive experiments are carried out on several public face databases. As demonstrated by the experimental testing, this ranking scheme performs well even when the amount of available labeled training data is limited. The reported experimental results well demonstrate the effectiveness of ranking GAN on synthesizing face aging sequences and faces with varying expression intensities.","1556-6021","","10.1109/TIFS.2020.2980792","National Basic Research Program of China (973 Program)(grant numbers:2016YFB1001001); National Natural Science Foundation of China(grant numbers:61603391,61925204,61427811); Italian Ministry of Research(grant numbers:PRIN 2015,SPADA); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9036898","Face image aging;facial expression synthesis;generative adversarial networks;ordinal ranking","Face;Aging;Gallium nitride;Sun;Generative adversarial networks;Generators;Estimation","computer vision;emotion recognition;face recognition;neural nets","expression synthesis;facial image synthesis;facial age synthesis;ordinal ranking generative adversarial networks;GAN;binary ranking vector;cost-sensitive ordinal rankers;public face databases;face aging sequences;computer graphics;computer vision;multiscale discriminators;age-intensity rank estimation","","9","","60","IEEE","16 Mar 2020","","","IEEE","IEEE Journals"
"EM-GAN: Data-Driven Fast Stress Analysis for Multi-Segment Interconnects","W. Jin; S. Sadiqbatcha; Z. Sun; H. Zhou; S. X. . -D. Tan","Department of Electrical and Computer Engineering, University of California, Riverside, CA; Department of Electrical and Computer Engineering, University of California, Riverside, CA; Department of Electrical and Computer Engineering, University of California, Riverside, CA; Department of Electrical and Computer Engineering, University of California, Riverside, CA; Department of Electrical and Computer Engineering, University of California, Riverside, CA","2020 IEEE 38th International Conference on Computer Design (ICCD)","21 Dec 2020","2020","","","296","303","Electromigration (EM) analysis for complicated interconnects requires the solving of partial differential equations, which is expensive. In this paper, we propose a fast transient hydrostatic stress analysis for EM failure assessment for multisegment interconnects using generative adversarial networks (GANs). Our work is inspired by the image synthesis and feature of generative deep neural networks. The stress evaluation of multi-segment interconnects, modeled by partial differential equations, can be viewed as time-varying 2D-images-to-image problem where the input is the multi-segment interconnects topology with current densities and the output is the EM stress distribution in those wire segments at the given aging time. We show that the conditional GAN can be exploited to attend the temporal dynamics for modeling the time-varying dynamic systems like stress evolution over time. The resulting algorithm, called EM-GAN, can quickly give accurate stress distribution of a general multi-segment wire tree for a given aging time, which is important for full-chip fast EM failure assessment. Our experimental results show that the EM-GAN shows 6.6% averaged error compared to COMSOL simulation results with orders of magnitude speedup. It also delivers 8.3× speedup over state-of-the-art analytic based EM analysis solver.","2576-6996","978-1-7281-9710-4","10.1109/ICCD50377.2020.00057","NSF(grant numbers:CCF-1816361); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283508","electromigration;hydrostatic stress analysis;generative adversarial networks","Training;Wires;Aging;Generative adversarial networks;Topology;Transient analysis;Stress","ageing;current density;electromigration;failure analysis;integrated circuit interconnections;integrated circuit reliability;internal stresses;neural nets;partial differential equations;stress analysis","data-driven fast stress analysis;electromigration analysis;complicated interconnects;partial differential equations;fast transient hydrostatic stress analysis;generative adversarial networks;GANs;image synthesis;generative deep neural networks;stress evaluation;time-varying 2D-images-to-image problem;multisegment interconnects topology;EM stress distribution;wire segments;given aging time;conditional GAN;time-varying dynamic systems;stress evolution;called EM-GAN;accurate stress distribution;multisegment wire tree;full-chip fast EM failure assessment;EM analysis solver","","6","","32","IEEE","21 Dec 2020","","","IEEE","IEEE Conferences"
"Adaptive Weighted Discriminator for Training Generative Adversarial Networks","V. Zadorozhnyy; Q. Cheng; Q. Ye","Department of Mathematics, University of Kentucky, Lexington, Kentucky; Departments of Computer Science and Internal Medicine, Institute for Biomedical Informatics, University of Kentucky, Lexington, Kentucky; Department of Mathematics, University of Kentucky, Lexington, Kentucky","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","4779","4788","Generative adversarial network (GAN) has become one of the most important neural network models for classical unsupervised machine learning. A variety of discriminator loss functions have been developed to train GAN’s discriminators and they all have a common structure: a sum of real and fake losses that only depends on the actual and generated data respectively. One challenge associated with an equally weighted sum of two losses is that the training may benefit one loss but harm the other, which we show causes instability and mode collapse. In this paper, we introduce a new family of discriminator loss functions that adopts a weighted sum of real and fake parts, which we call adaptive weighted loss functions or aw-loss functions. Using the gradients of the real and fake parts of the loss, we can adaptively choose weights to train a discriminator in the direction that benefits the GAN’s stability. Our method can be potentially applied to any discriminator model with a loss that is a sum of the real and fake parts. Experiments validated the effectiveness of our loss functions on unconditional and conditional image generation tasks, improving the baseline results by a significant margin on CIFAR-10, STL-10, and CIFAR-100 datasets in Inception Scores (IS) and Fréchet Inception Distance (FID) metrics.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577290","","Training;Measurement;Computer vision;Image synthesis;Neural networks;Machine learning;Generative adversarial networks","image classification;neural nets;unsupervised learning","unconditional image generation tasks;adaptive weighted discriminator;generative adversarial network;neural network models;unsupervised machine learning;discriminator loss functions;GAN discriminators;fake losses;adaptive weighted loss functions;aw-loss functions;discriminator model;GAN stability;CIFAR-100 datasets;STL-10;inception scores;Frechet inception distance metrics;FID metrics","","2","","46","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Image Creation Based on Transformer and Generative Adversarial Networks","H. Liu; Q. Liu","School of Computer and Control Engineering, Yantai University, Yantai, China; School of Computer and Control Engineering, Yantai University, Yantai, China","IEEE Access","17 Oct 2022","2022","10","","108296","108306","To address the problem of low authenticity of generated images in existing generative models, the transformer super-resolution generative adversarial network(TransSRGAN) model based on the generative adversarial network is proposed. The generator of the model uses the transformer encoder sub-module as the basic module. The features of the input vector are extracted. low-definition images are generated through the transformer encoder submodule, and the low-definition image is up-sampled by the convolutional neural network to complete the image generation. The discriminator of this model uses the convolutional neural network as the basic module. To discriminate the real samples from the generated fake samples, the discriminator extracts the image features by the convolutional neural network. The experimental results show that the TransSRGAN model brings the distribution of the generated samples closer to the training samples, effectively raises the quality of the generated samples, improves the authenticity of the generated samples, and enriches the diversity of the generated samples. During the training process, there was no mode collapse or instability.","2169-3536","","10.1109/ACCESS.2022.3213079","National Natural Science Foundation of China(grant numbers:62172351); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913966","Image generation;generative adversarial network;transformer;self-attention","Generative adversarial networks;Transformers;Feature extraction;Training data;Convolutional neural networks;Image generation;Image synthesis;Superresolution;Encoding","convolutional neural nets;image resolution;image sampling","TransSRGAN model;image generation;convolutional neural network;transformer encoder submodule;low-definition image;basic module;transformer encoder sub-module;generative models;image creation","","","","28","CCBY","10 Oct 2022","","","IEEE","IEEE Journals"
"Cellcyclegan: Spatiotemporal Microscopy Image Synthesis Of Cell Populations Using Statistical Shape Models And Conditional Gans","D. Bähr; D. Eschweiler; A. Bhattacharyya; D. Moreno-Andrés; W. Antonin; J. Stegmaier","Institute of Imaging and Computer Vision, RWTH Aachen University, Aachen, Germany; Institute of Imaging and Computer Vision, RWTH Aachen University, Aachen, Germany; Institute of Imaging and Computer Vision, RWTH Aachen University, Aachen, Germany; Institute of Biochemistry and Molecular Cell Biology, Medical School, RWTH Aachen University, Aachen, Germany; Institute of Biochemistry and Molecular Cell Biology, Medical School, RWTH Aachen University, Aachen, Germany; Institute of Imaging and Computer Vision, RWTH Aachen University, Aachen, Germany","2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)","25 May 2021","2021","","","15","19","Automatic analysis of spatio-temporal microscopy images is inevitable for state-of-the-art research in the life sciences. Recent developments in deep learning provide powerful tools for automatic analyses of such image data, but heavily depend on the amount and quality of provided training data to perform well. To this end, we developed a new method for realistic generation of synthetic 2D+t microscopy image data of fluorescently labeled cellular nuclei. The method combines spatiotemporal statistical shape models of different cell cycle stages with a conditional GAN to generate time series of cell populations and provides instance-level control of cell cycle stage and the fluorescence intensity of generated cells. We show the effect of the GAN conditioning and create a set of synthetic images that can be readily used for training and benchmarking of cell segmentation and tracking approaches.","1945-8452","978-1-6654-1246-9","10.1109/ISBI48211.2021.9433896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433896","Data Synthesis;Statistical Shape Models;Generative Adversarial Networks;Microscopy;Cell Biology","Training;Shape;Microprocessors;Microscopy;Sociology;Training data;Computer architecture","biological techniques;biomedical optical imaging;cellular biophysics;deep learning (artificial intelligence);fluorescence;image segmentation;medical image processing;optical microscopy;spatiotemporal phenomena;statistical analysis;time series","cellcyclegan;spatiotemporal microscopy image synthesis;cell populations;conditional gans;life sciences;deep learning;automatic analyses;realistic generation;microscopy image data;fluorescently labeled cellular nuclei;spatiotemporal statistical shape models;conditional GAN;instance-level control;cell cycle stage;fluorescence intensity;GAN conditioning;synthetic images;cell segmentation;synthetic 2D+t microscopy image data;time series","","2","","20","IEEE","25 May 2021","","","IEEE","IEEE Conferences"
"HRInversion: High-Resolution GAN Inversion for Cross-Domain Image Synthesis","P. Zhou; L. Xie; B. Ni; L. Liu; Q. Tian","Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; Huawei Cloud BU, Shenzhen, Guangdong, China; Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai, China; University of Science and Technology of China, Hefei, Anhui, China; Huawei Cloud BU, Shenzhen, Guangdong, China","IEEE Transactions on Circuits and Systems for Video Technology","","2022","PP","99","1","1","We investigate GAN inversion problems of using pre-trained GANs to reconstruct real images. Recent methods for such problems typically employ a VGG perceptual loss to measure the difference between images. While the perceptual loss has achieved remarkable success in various computer vision tasks, it may cause unpleasant artifacts and is sensitive to changes in input scale. This paper delivers an important message that algorithm details are crucial for achieving satisfying performance. In particular, we propose two important but undervalued design principles: (i) not down-sampling the input of the perceptual loss to avoid high-frequency artifacts; and (ii) calculating the perceptual loss using convolutional features which are robust to scale. Integrating these designs derives the proposed framework, HRInversion, that achieves superior performance in reconstructing image details. We validate the effectiveness of HRInversion on a cross-domain image synthesis task and propose a post-processing approach named local style optimization (LSO) to synthesize clean and controllable stylized images. For the evaluation of the cross-domain images, we introduce a metric named ID retrieval which captures the similarity of face identities of stylized images to content images. We also test HRInversion on non-square images. Equipped with implicit neural representation, HRInversion applies to ultra-high resolution images with more than 10 million pixels. Furthermore, we show applications of style transfer and 3D-aware GAN inversion, paving the way for extending the application range of HRInversion.","1558-2205","","10.1109/TCSVT.2022.3222456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9953153","GAN inversion;perceptual loss;image synthesis","Image reconstruction;Image resolution;Generative adversarial networks;Task analysis;Semantics;Generators;Image synthesis","","","","","","","IEEE","16 Nov 2022","","","IEEE","IEEE Early Access Articles"
"Transformer-Based T2-weighted MRI Synthesis from T1-weighted Images","K. Pan; P. Cheng; Z. Huang; L. Lin; X. Tang","Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China; Department of Electronic and Electrical Engineering, The University of Hong Kong, Hong Kong, China; Department of Electronic and Electrical Engineering, Southern University of Science and Technology, Shenzhen, China","2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)","8 Sep 2022","2022","","","5062","5065","Multi-modality magnetic resonance (MR) images provide complementary information for disease diagnoses. However, modality missing is quite usual in real-life clinical practice. Current methods usually employ convolution-based generative adversarial network (GAN) or its variants to synthesize the missing modality. With the development of vision transformer, we explore its application in the MRI modality synthesis task in this work. We propose a novel supervised deep learning method for synthesizing a missing modality, making use of a transformer-based encoder. Specifically, a model is trained for translating 2D MR images from T1-weighted to T2-weighted based on conditional GAN (cGAN). We replace the encoder with transformer and input adjacent slices to enrich spatial prior knowledge. Experimental results on a private dataset and a public dataset demonstrate that our proposed model outperforms state-of-the-art supervised methods for MR image synthesis, both quantitatively and qualitatively. Clinical relevance— This work proposes a method to synthesize T2-weighted images from T1-weighted ones to address the modality missing issue in MRI.","2694-0604","978-1-7281-2782-8","10.1109/EMBC48229.2022.9871183","National Natural Science Foundation of China(grant numbers:62071210); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9871183","","Deep learning;Image synthesis;Magnetic resonance imaging;Biological system modeling;Magnetic resonance;Transformers;Generative adversarial networks","biomedical MRI;diseases;image segmentation;learning (artificial intelligence);medical image processing","Transformer-Based T2-weighted MRI Synthesis;T1-weighted Images;multimodality magnetic resonance;complementary information;disease diagnoses;real-life clinical practice;convolution-based generative adversarial network;missing modality;vision transformer;MRI modality synthesis task;supervised deep learning method;transformer-based encoder;translating 2D;conditional GAN;input adjacent slices;state-of-the-art supervised methods;MR image synthesis;T2-weighted images;T1-weighted ones;modality missing issue","Magnetic Resonance Imaging","","","18","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"Multi-Constraint Adversarial Networks for Unsupervised Image-to-Image Translation","D. Saxena; T. Kulshrestha; J. Cao; S. -C. Cheung","Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Image Processing","1 Feb 2022","2022","31","","1601","1612","Unsupervised image-to-image translation aims to learn the mapping from an input image in a source domain to an output image in a target domain without paired training dataset. Recently, remarkable progress has been made in translation due to the development of generative adversarial networks (GANs). However, existing methods suffer from the training instability as gradients passing from discriminator to generator become less informative when the source and target domains exhibit sufficiently large discrepancies in appearance or shape. To handle this challenging problem, in this paper, we propose a novel multi-constraint adversarial model (MCGAN) for image translation in which multiple adversarial constraints are applied at generator’s multi-scale outputs by a single discriminator to pass gradients to all the scales simultaneously and assist generator training for capturing large discrepancies in appearance between two domains. We further notice that the solution to regularize generator is helpful in stabilizing adversarial training, but results may have unreasonable structure or blurriness due to less context information flow from discriminator to generator. Therefore, we adopt dense combinations of the dilated convolutions at discriminator for supporting more information flow to generator. With extensive experiments on three public datasets, cat-to-dog, horse-to-zebra, and apple-to-orange, our method significantly improves state-of-the-arts on all datasets.","1941-0042","","10.1109/TIP.2022.3144886","Research Grants Council (RGC) Research Impact Fund(grant numbers:R5034-18); RGC Collaborative Research Fund(grant numbers:C5026-18G); Hong Kong Polytechnic University (PolyU) Internal Start-Up Fund(grant numbers:P0038876); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9694500","Generative adversarial networks;unsupervised image-to-image translation;generative modeling;GANs;image synthesis","Training;Generators;Image resolution;Image synthesis;Shape;Convolutional codes;Task analysis","feature extraction;gradient methods;image segmentation;neural nets;unsupervised learning","adversarial training;multiconstraint adversarial networks;unsupervised image-to-image translation;input image;source domain;output image;target domain;paired training dataset;generative adversarial networks;target domains;novel multiconstraint adversarial model;multiple adversarial constraints;multiscale outputs;generator training","","1","","44","IEEE","26 Jan 2022","","","IEEE","IEEE Journals"
"Mutually Improved Endoscopic Image Synthesis and Landmark Detection in Unpaired Image-to-Image Translation","L. Sharan; G. Romano; S. Koehler; H. Kelm; M. Karck; R. De Simone; S. Engelhardt","Department of Innere Medizin III, Group Artificial Intelligence in Cardiovascular Medicine, University Hospital Heidelberg, Heidelberg, Germany; Department of Cardiac Surgery, Heidelberg University Hospital, Heidelberg, Germany; Department of Innere Medizin III, Group Artificial Intelligence in Cardiovascular Medicine, University Hospital Heidelberg, Heidelberg, Germany; Department of Innere Medizin III, Group Artificial Intelligence in Cardiovascular Medicine, University Hospital Heidelberg, Heidelberg, Germany; Department of Cardiac Surgery, Heidelberg University Hospital, Heidelberg, Germany; Department of Cardiac Surgery, Heidelberg University Hospital, Heidelberg, Germany; Department of Innere Medizin III, Group Artificial Intelligence in Cardiovascular Medicine, University Hospital Heidelberg, Heidelberg, Germany","IEEE Journal of Biomedical and Health Informatics","17 Jan 2022","2022","26","1","127","138","The CycleGAN framework allows for unsupervised image-to-image translation of unpaired data. In a scenario of surgical training on a physical surgical simulator, this method can be used to transform endoscopic images of phantoms into images which more closely resemble the intra-operative appearance of the same surgical target structure. This can be viewed as a novel augmented reality approach, which we coined <italic>Hyperrealism</italic> in previous work. In this use case, it is of paramount importance to display objects like needles, sutures or instruments consistent in both domains while altering the style to a more tissue-like appearance. Segmentation of these objects would allow for a direct transfer, however, contouring of these, partly tiny and thin foreground objects is cumbersome and perhaps inaccurate. Instead, we propose to use landmark detection on the points when sutures pass into the tissue. This objective is directly incorporated into a CycleGAN framework by treating the performance of pre-trained detector models as an additional optimization goal. We show that a task defined on these sparse landmark labels improves consistency of synthesis by the generator network in both domains. Comparing a baseline CycleGAN architecture to our proposed extension (<italic>DetCycleGAN</italic>), mean precision (PPV) improved by <inline-formula><tex-math notation=""LaTeX"">$+61.32$</tex-math></inline-formula>, mean sensitivity (TPR) by <inline-formula><tex-math notation=""LaTeX"">$+37.91$</tex-math></inline-formula>, and mean <inline-formula><tex-math notation=""LaTeX"">$F_1$</tex-math></inline-formula> score by <inline-formula><tex-math notation=""LaTeX"">$+0.4743$</tex-math></inline-formula>. Furthermore, it could be shown that by dataset fusion, generated intra-operative images can be leveraged as additional training data for the detection network itself.","2168-2208","","10.1109/JBHI.2021.3099858","Klaus Tschira Foundation and the German Research Foundation DFG(grant numbers:398787259,DE 2131/2-1,EN 1197/2-1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9496194","Generative adversarial networks;surgical simulation;surgical training;CycleGAN;landmark localization;landmark detection;mitral valve repair","Task analysis;Surgery;Valves;Maintenance engineering;Training;Semantics;Generative adversarial networks","augmented reality;biological tissues;biomedical equipment;endoscopes;image segmentation;medical image processing;object detection;phantoms;surgery","landmark detection;CycleGAN framework;unsupervised image-to-image translation;surgical training;endoscopic images;intra-operative appearance;surgical target structure;tissue-like appearance;thin foreground objects;pre-trained detector models;sparse landmark labels;CycleGAN architecture;generated intra-operative images;endoscopic image synthesis;Hyperrealism","Endoscopy;Humans;Image Processing, Computer-Assisted;Phantoms, Imaging","5","","66","IEEE","26 Jul 2021","","","IEEE","IEEE Journals"
"Image Synthesis Using Conditional GANs for Selective Laser Melting Additive Manufacturing","A. Ramlatchan; Y. Li","Department of Computer Science, Old Dominion University, Norfolk, VA; Department of Computer Science, Old Dominion University, Norfolk, VA","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","In-situ process monitoring for metals additive manufacturing is paramount to the successful build of an object for application in extreme or high stress environments. Yet in selective laser melting additive manufacturing, it is extremely difficult to evaluate the build process. The difficulty is that obtaining enough variety of data to quantify the internal microstructures for the evaluation of its physical properties is problematic, as the laser passes at high speeds over powder grains at a micrometer scale. Using generative models, a type of machine learning, has been shown here to provide new artificially generated data with the same properties as the experimental images. The Generative Adversarial Network (GAN) synthesized new computationally derived data through a process that learns the underlying features of images that correspond to the different laser process parameters in a generator network. While this technique was effective at delivering high-quality images that closely matched the training data when tested against holdout samples, modifications to the general form of the network through a conditional generative adversarial network (CGAN) showed improved capabilities at creating these new images. Using multiple evaluation metrics, it has been shown that generative models can be used to create new data for various laser process parameter combinations, thereby allowing a more comprehensive evaluation of ideal laser conditions for any particular build. The new data can supplement the experimental data, thereby growing the overall knowledge framework for build characteristics.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892033","generative adversarial networks;deep learning;artificial neural networks;additive manufacturing","Laser theory;Power lasers;Neural networks;Training data;Laser modes;Generative adversarial networks;Three-dimensional printing","gallium compounds;laser materials processing;laser sintering;learning (artificial intelligence);melting;process monitoring;rapid prototyping (industrial);three-dimensional printing;wide band gap semiconductors","image synthesis;conditional GANs;selective laser melting additive manufacturing;In-situ process;metals additive manufacturing;successful build;extreme stress environments;high stress environments;build process;internal microstructures;physical properties;powder grains;micrometer scale;generative models;machine learning;artificially generated data;experimental images;Generative Adversarial Network synthesized new computationally derived data;GAN;different laser process parameters;generator network;high-quality images;training data;conditional generative adversarial network;multiple evaluation metrics;laser process parameter combinations;comprehensive evaluation;ideal laser conditions;particular build;build characteristics","","1","","27","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Towards Audio to Scene Image Synthesis Using Generative Adversarial Network","C. -H. Wan; S. -P. Chuang; H. -Y. Lee","Graduate Institute of Electrical Engineering, National Taiwan University; Graduate Institute of Communication Engineering, National Taiwan University; Graduate Institute of Communication Engineering, National Taiwan University","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","496","500","Humans can imagine a scene from a sound. We want machines to do so by using conditional generative adversarial networks (GANs). By applying the techniques including spectral norm, projection discriminator and auxiliary classifier, compared with naive conditional GAN, the model can generate images with better quality in terms of both subjective and objective evaluations. Almost three-fourth of people agree that our model have the ability to generate images related to sounds. By inputting different volumes of the same sound, our model output different scales of changes based on the volumes, showing that our model truly knows the relationship between sounds and images to some extent.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8682383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682383","conditional GANs;audio-visual;cross-modal generation","","audio signal processing;image processing;neural nets;spectral analysis","conditional generative adversarial networks;auxiliary classifier;image synthesis;conditional GAN","","20","","23","IEEE","17 Apr 2019","","","IEEE","IEEE Conferences"
"Lightweight Privacy-Preserving GAN Framework for Model Training and Image Synthesis","Y. Yang; K. Mu; R. H. Deng","School of Computing and Information Systems, Singapore Management University, Singapore; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; School of Computing and Information Systems, Singapore Management University, Singapore","IEEE Transactions on Information Forensics and Security","23 Mar 2022","2022","17","","1083","1098","Generative adversarial network (GAN) has excellent performance for data generation and is widely used in image synthesis. Outsourcing GAN to cloud platform is a popular way to save local computation resources and improve the efficiency, but it still faces the privacy leakage concerns: (1) the sensitive information of the training dataset may be disclosed in the cloud; (2) the trained model may reveal the privacy of training samples since it extracts the characteristics from the data. In this paper, we propose a lightweight privacy-preserving GAN framework (LP-GAN) for model training and image synthesis based on secret sharing scheme. Specifically, we design a series of efficient secure interactive protocols for different layers (convolution, batch normalization, ReLU, Sigmoid) of neural network (NN) used in GAN. Our protocols are scalable to build secure training or inference tasks for NN-based applications. We utilize edge computing to reduce the latency and all the protocols are executed on two edge servers collaboratively. Compared with the existing schemes, the proposed solution greatly improves efficiency, reduces communication overhead, and guarantees the privacy. We prove the correctness and security of LP-GAN by theoretical analysis. Extensive experiments on different real-world datasets demonstrate the effectiveness, accuracy, and efficiency of our scheme.","1556-6021","","10.1109/TIFS.2022.3156818","National Natural Science Foundation of China(grant numbers:61872091); Singapore National Research Foundation(grant numbers:NRF2018NCR-NSOE004-0001); AXA Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729111","Privacy-preserving;generative adversarial network;secret sharing;secure computation;deep learning","Protocols;Generative adversarial networks;Training;Cryptography;Computational modeling;Image synthesis;Privacy","cloud computing;convolutional neural nets;cryptographic protocols;data privacy;image coding;learning (artificial intelligence)","lightweight privacy-preserving GAN framework;model training;image synthesis;generative adversarial network;data generation;local computation resources;privacy leakage;training dataset;secure interactive protocols;LP-GAN;cloud platform;neural network;edge computing;secret sharing scheme","","3","","50","IEEE","4 Mar 2022","","","IEEE","IEEE Journals"
"Density-Aware Haze Image Synthesis by Self-Supervised Content-Style Disentanglement","C. Zhang; Z. Lin; L. Xu; Z. Li; W. Tang; Y. Liu; G. Meng; L. Wang; L. Li","Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; Institute of Artificial Intelligence and Robotics, Xi’an Jiaotong University, Xi’an, China; Department of Automation, BNRist, Tsinghua University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","1 Jul 2022","2022","32","7","4552","4572","The key procedure of haze image synthesis with adversarial training lies in the disentanglement of the feature involved only in haze synthesis, i.e., the style feature, from the feature representing the invariant semantic content, i.e., the content feature. Previous methods introduced a binary classifier to constrain the domain membership from being distinguished through the learned content feature during the training stage, thereby the style information is separated from the content feature. However, we find that these methods cannot achieve complete content-style disentanglement. The entanglement of the flawed style feature with content information inevitably leads to the inferior rendering of haze images. To address this issue, we propose a self-supervised style regression model with stochastic linear interpolation that can suppress the content information in the style feature. Ablative experiments demonstrate the disentangling completeness and its superiority in density-aware haze image synthesis. Moreover, the synthesized haze data are applied to test the generalization ability of vehicle detectors. Further study on the relation between haze density and detection performance shows that haze has an obvious impact on the generalization ability of vehicle detectors and that the degree of performance degradation is linearly correlated to the haze density, which in turn validates the effectiveness of the proposed method.","1558-2205","","10.1109/TCSVT.2021.3130158","National Key Research and Development Project of New Generation Artificial Intelligence of China(grant numbers:2018AAA0102504); National Natural Science Foundation of China(grant numbers:61973245); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9624947","Haze synthesis;unsupervised image-to-image translation;self-supervised disentanglement","Feature extraction;Image synthesis;Scattering;Generative adversarial networks;Atmospheric modeling;Training;Testing","feature extraction;image classification;interpolation;learning (artificial intelligence);regression analysis","density-aware haze image synthesis;self-supervised content-style disentanglement;invariant semantic content;learned content feature;style information;complete content-style disentanglement;flawed style feature;content information;self-supervised style regression model;disentangling completeness;synthesized haze data;haze density;generalization ability","","","","47","IEEE","23 Nov 2021","","","IEEE","IEEE Journals"
"When Two are Better Than One: Synthesizing Heavily Unbalanced Data","F. Ferreira; N. Lourenço; B. Cabral; J. P. Fernandes","CISUC, DEI, University of Coimbra, Coimbra, Portugal; CISUC, DEI, University of Coimbra, Coimbra, Portugal; CISUC, DEI, University of Coimbra, Coimbra, Portugal; LIACC, DEI, Faculdade de Engenharia da Universidade do Porto, Porto, Portugal","IEEE Access","12 Nov 2021","2021","9","","150459","150469","Nowadays, data is king and if treated and used properly it promises to give organizations a competitive edge over rivals by enabling them to develop and design Intelligent Systems to improve their services. However, they need to fully comply with not only ethical but also regulatory obligations, where, e.g., privacy (strictly) needs to be respected when using or sharing data, thus protecting both the interests of users and organizations. Fraud Detection systems are examples of such systems where Machine Learning algorithms leverage information to classify financial transactions as legitimate or illicit. The data used to create these solutions is usually highly structured and contains categorical and continuous features characterised by complex distributions. One of the main challenges of fraud detection is concerned with the scarcity of fraudulent instances which results in highly unbalanced datasets. Additionally, privacy is crucial, and it is usually forbidden, or not possible, to share the data of organizations and individuals for creating or improving models.In this paper we propose a framework for private data sharing based on synthetic data generation using Generative Adversarial Networks (GAN) that learns the specificities of financial transactions data and generates fictitious data that keeps the utility of the original datasets. Our proposal, called Duo-GAN, uses two GAN generators to handle the data imbalance problem, one generator for fraudulent instances and the other for legitimate instances. With this approach, we observed, at most, a 5% disparity in F1 scores between classifiers trained and tested with actual data and the ones trained with synthetic data and tested with actual data.","2169-3536","","10.1109/ACCESS.2021.3126656","National Funds through the FCT—Foundation for Science and Technology, I.P., within the scope of the project Centre for Informatics and Systems of the University of Coimbra (CISUC)(grant numbers:UID/CEC/00326/2020); European Social Fund, through the Regional Operational Program Centro 2020; Carnegie Mellon University (CMU)|Portugal Project autonomiC plAtform for MachinE Learning using anOnymized daTa (CAMELOT)(grant numbers:POCI-01-0247-FEDER-045915); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606863","Fraud detection;generative adversarial networks;privacy;machine learning;synthetic data generation;tabular data","Data models;Generative adversarial networks;Data privacy;Generators;Image synthesis;Feeds;Training","data mining;data privacy;financial data processing;fraud;learning (artificial intelligence);pattern classification;security of data","financial transactions data;fictitious data;called Duo-GAN;GAN generators;data imbalance problem;fraudulent instances;legitimate instances;synthesizing heavily unbalanced data;organizations;competitive edge;Intelligent Systems;ethical but also regulatory obligations;privacy;Fraud Detection systems;Machine Learning algorithms leverage information;categorical features;continuous features;complex distributions;highly unbalanced datasets;creating improving models;private data;synthetic data generation;Generative Adversarial Networks","","","","38","CCBY","8 Nov 2021","","","IEEE","IEEE Journals"
"End-to-End Adversarial Retinal Image Synthesis","P. Costa; A. Galdran; M. I. Meyer; M. Niemeijer; M. Abràmoff; A. M. Mendonça; A. Campilho","Institute for Systems and Computer Engineering, Technology and Science, Porto, Portugal; Institute for Systems and Computer Engineering, Technology and Science, Porto, Portugal; Institute for Systems and Computer Engineering, Technology and Science, Porto, Portugal; IDx LLC, Iowa City, IA, USA; Stephen A. Wynn Institute for Vision Research, University of Iowa, Iowa City, IA, USA; Faculdade de Engenharia, Universidade do Porto, Porto, Portugal; Faculdade de Engenharia, Universidade do Porto, Porto, Portugal","IEEE Transactions on Medical Imaging","1 Mar 2018","2018","37","3","781","791","In medical image analysis applications, the availability of the large amounts of annotated data is becoming increasingly critical. However, annotated medical data is often scarce and costly to obtain. In this paper, we address the problem of synthesizing retinal color images by applying recent techniques based on adversarial learning. In this setting, a generative model is trained to maximize a loss function provided by a second model attempting to classify its output into real or synthetic. In particular, we propose to implement an adversarial autoencoder for the task of retinal vessel network synthesis. We use the generated vessel trees as an intermediate stage for the generation of color retinal images, which is accomplished with a generative adversarial network. Both models require the optimization of almost everywhere differentiable loss functions, which allows us to train them jointly. The resulting model offers an end-to-end retinal image synthesis system capable of generating as many retinal images as the user requires, with their corresponding vessel networks, by sampling from a simple probability distribution that we impose to the associated latent space. We show that the learned latent space contains a well-defined semantic structure, implying that we can perform calculations in the space of retinal images, e.g., smoothly interpolating new data points between two retinal images. Visual and quantitative results demonstrate that the synthesized images are substantially different from those in the training set, while being also anatomically consistent and displaying a reasonable visual quality.","1558-254X","","10.1109/TMI.2017.2759102","ERDF European Regional Development Fund through the Operational Programme for Competitiveness and Internationalisation - COMPETE 2020 Programme; National Funds through the FCT Fundação para a Ciência e a Tecnologia Portuguese Foundation for Science and Technology(grant numbers:CMUP-ERI/TIC/0028/2014); North Portugal Regional Operational Programme (NORTE 2020), under the PORTUGAL 2020 Partnership Agreement within the project NanoSTIMA: Macro-to-Nano Human Sensing: Towards Integrated Multimodal Health Monitoring and Analytics(grant numbers:NORTE-01-0145-FEDER-000016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8055572","Retinal image synthesis;retinal image analysis;generative adversarial networks;adversarial autoencoders","Training;Biomedical imaging;Image generation;Retinal vessels;Mathematical model","blood vessels;eye;image colour analysis;image segmentation;learning (artificial intelligence);medical image processing;probability;retinal recognition;statistical distributions","medical image analysis applications;annotated medical data;retinal color images;adversarial learning;generative model;adversarial autoencoder;retinal vessel network synthesis;generated vessel trees;color retinal images;generative adversarial network;differentiable loss functions;end-to-end retinal image synthesis system;data points;synthesized images;vessel networks;latent space learning;end-to-end adversarial retinal image synthesis;probability distribution","Algorithms;Diagnostic Techniques, Ophthalmological;Humans;Image Processing, Computer-Assisted;Neural Networks (Computer);Retina;Retinal Vessels","203","","35","IEEE","2 Oct 2017","","","IEEE","IEEE Journals"
"SEAN: Image Synthesis With Semantic Region-Adaptive Normalization","P. Zhu; R. Abdal; Y. Qin; P. Wonka",KAUST; KAUST; Cardiff University; KAUST,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","5103","5112","We propose semantic region-adaptive normalization (SEAN), a simple but effective building block for Generative Adversarial Networks conditioned on segmentation masks that describe the semantic regions in the desired output image. Using SEAN normalization, we can build a network architecture that can control the style of each semantic region individually, e.g., we can specify one style reference image per region. SEAN is better suited to encode, transfer, and synthesize style than the best previous method in terms of reconstruction quality, variability, and visual quality. We evaluate SEAN on multiple datasets and report better quantitative metrics (e.g. FID, PSNR) than the current state of the art. SEAN also pushes the frontier of interactive image editing. We can interactively edit images by changing segmentation masks or the style for any given region. We can also interpolate styles from two reference images per region.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156510","","Semantics;Image segmentation;Image generation;Gallium nitride;Generative adversarial networks;Generators;Training","image segmentation;interactive systems","quantitative metric;image synthesis;reference images;interactive image editing;style reference image;SEAN normalization;output image;segmentation masks;generative adversarial networks","","130","1","54","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"3D Aided Duet GANs for Multi-View Face Image Synthesis","J. Cao; Y. Hu; B. Yu; R. He; Z. Sun","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Noahs Ark Laboratory, Huawei Technologies Co., Ltd., Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","8 May 2019","2019","14","8","2028","2042","Multi-view face synthesis from a single image is an ill-posed computer vision problem. It often suffers from appearance distortions if it is not well-defined. Producing photo-realistic and identity preserving multi-view results is still a not well-defined synthesis problem. This paper proposes 3D aided duet generative adversarial networks (AD-GAN) to precisely rotate the yaw angle of an input face image to any specified angle. AD-GAN decomposes the challenging synthesis problem into two well-constrained subtasks that correspond to a face normalizer and a face editor. The normalizer first frontalizes an input image, and then the editor rotates the frontalized image to a desired pose guided by a remote code. In the meantime, the face normalizer is designed to estimate a novel dense UV correspondence field, making our model aware of 3D face geometry information. In order to generate photo-realistic local details and accelerate convergence process, the normalizer and the editor are trained in a two-stage manner and regulated by a conditional self-cycle loss and a perceptual loss. Exhaustive experiments on both controlled and uncontrolled environments demonstrate that the proposed method not only improves the visual realism of multi-view synthetic images but also preserves identity information well.","1556-6021","","10.1109/TIFS.2019.2891116","National Natural Science Foundation of China(grant numbers:U1836217,61427811,61573360,61721004); National Key Research and Development Program of China(grant numbers:2017YFC0821602); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603840","Face rotation and frontalization;multi-view face synthesis;pose-invariant face recognition;face reconstruction","Face;Gallium nitride;Generative adversarial networks;Three-dimensional displays;Face recognition;Generators;Training","computational geometry;computer vision;face recognition;learning (artificial intelligence);solid modelling","frontalized image;face normalizer;dense UV correspondence field;3D face geometry information;photo-realistic local details;multiview synthetic images;multiview face image synthesis;single image;appearance distortions;generative adversarial networks;AD-GAN;yaw angle;input face image;specified angle;face editor;input image;3D aided duet GAN;ill-posed computer vision problem","","25","","56","IEEE","6 Jan 2019","","","IEEE","IEEE Journals"
"Cross-modal Image Synthesis within Dual-Energy X-ray Security Imagery","B. K. S. Isaac-Medina; N. Bhowmik; C. G. Willcocks; T. P. Breckon","Department of Computer Science, Durham University, Durham, UK; Department of Computer Science, Durham University, Durham, UK; Department of Computer Science, Durham University, Durham, UK; Department of Engineering, Durham University, Durham, UK","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","23 Aug 2022","2022","","","332","340","Dual-energy X-ray scanners are used for aviation security screening given their capability to discriminate materials inside passenger baggage. To facilitate manual operator inspection, a pseudo-colouring is assigned to the effective composition of the material. Recently, paired image to image translation models based on conditional Generative Adversarial Networks (cGAN) have shown to be effective for image colourisation. In this work, we investigate the use of such a model to translate from the raw X-ray energy responses (high, low, effective-Z) to the pseudo-coloured images and vice versa. Specifically, given N X-ray modalities, we train a cGAN conditioned in N − m domains to generate the remaining m representation. Our method achieves a mean squared error (MSE) of 16.5 and a structural similarity index (SSIM) of 0.9815 when using the raw modalities to generate the pseudo-colour representation. Additionally, raw X-ray high energy, low energy and effective-Z projections were generated given the pseudo-colour image with minimum MSE of 2.57, 5.63 and 1.43, and maximum SSIM of 0.9953, 0.9901 and 0.9921. Furthermore, we assess the quality of our synthesised pseudo-colour reconstructions by measuring the performance of two object detection models originally trained on real X-ray pseudo-colour images over our generated pseudo-colour images. Interestingly, our generated pseudo-colour images obtain marginally improved detection performance than the corresponding real X-ray pseudo-colour images, showing that meaningful representations are synthesized and that these reconstructions are applicable for differing aviation security tasks.","2160-7516","978-1-6654-8739-9","10.1109/CVPRW56347.2022.00048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857314","","Computer architecture;Predictive models;Generative adversarial networks;Generators;Data models;Pattern recognition;Security","computerised tomography;image colour analysis;image reconstruction;image segmentation;inspection;mean square error methods;object detection;security;X-ray imaging","dual-energy;aviation security screening;pseudocolouring;effective composition;paired image;image translation models;conditional generative adversarial networks;image colourisation;raw X-ray energy responses;pseudocoloured images;X-ray modalities;raw modalities;pseudocolour representation;raw X-ray high energy;synthesised pseudocolour reconstructions;X-ray pseudocolour images;generated pseudocolour images;cross-modal image synthesis;dual-energy X-ray scanners;dual-energy X-ray security imagery;cGAN;mean squared error;MSE","","","","32","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"Text2FaceGAN: Face Generation from Fine Grained Textual Descriptions","O. R. Nasir; S. K. Jha; M. S. Grover; Y. Yu; A. Kumar; R. R. Shah","MIDAS Lab, IIIT-Delhi, Delhi, India; MIDAS Lab, IIIT-Delhi, Delhi, India; MIDAS Lab, IIIT-Delhi, Delhi, India; NII, Tokyo, Japan; Adobe Systems; MIDAS Lab, IIIT-Delhi, Delhi, India","2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)","5 Dec 2019","2019","","","58","67","In recent years, powerful generative adversarial networks (GAN) have been developed to automatically synthesize realistic images from text. However, most existing tasks are limited to generating simple images such as flowers from captions. In this paper, we extend this problem to the less addressed domain of face generation from fine-grained textual descriptions of face, e.g., ""A person has curly hair, oval face, and mustache"". We are motivated by the potential of automated face generation to impact and assist critical tasks such as criminal face reconstruction. Since current datasets for the task are either very small or do not contain captions, we generate captions for images in the CelebA dataset by creating an algorithm to automatically convert a list of attributes to a set of captions. The generated captions are meaningful, versatile and consistent with the general semantics of a face. We then model the highly multi-modal problem of text to face generation as learning the conditional distribution of faces (conditioned on text) in same latent space. We utilize the current state-of-the-art GAN (DC-GAN with GAN-CLS loss) for learning conditional multi-modality. The presence of more fine-grained details and variable length of the captions makes the problem easier for a user but more difficult to handle compared to the other text-to-image tasks. We flipped the labels for real and fake images and added noise in discriminator. Generated images for diverse textual descriptions show promising results. In the end, we show how the widely used inceptions score is not a good metric to evaluate the performance of generative models used for synthesizing faces from text.","","978-1-7281-5527-2","10.1109/BigMM.2019.00-42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8919389","Datasets;Generative Adversarial Networks;Text to Image;Facial Attributes;Face Generation","Face;Gallium nitride;Generative adversarial networks;Task analysis;Hair;Image generation;Facial features","face recognition;image reconstruction;learning (artificial intelligence);realistic images;text analysis","criminal face reconstruction;caption generation;fine-grained details;text-to-image tasks;textual descriptions;generative models;face synthesis;text2FaceGAN;fine grained textual descriptions;generative adversarial networks;realistic image synthesis;automated face generation","","17","","34","IEEE","5 Dec 2019","","","IEEE","IEEE Conferences"
"Differentially Private Functional Mechanism for Generative Adversarial Networks","X. Zhang; J. Ding; S. M. Errapotu; X. Huang; P. Li; M. Pan","Department of Electrical and Computer Engineering, University of Houston, Houston, TX; Department of Electrical and Computer Engineering, University of Houston, Houston, TX; Department of Electrical and Computer Engineering, University of Texas at El Paso, El Paso, TX; School of Electronics and Communication Engineering, Sun Yat-sen University, Guangzhou, China; Department of Electrical Engineering and Computer Science, Case Western Reserve University, Cleveland, OH; Department of Electrical and Computer Engineering, University of Houston, Houston, TX","2019 IEEE Global Communications Conference (GLOBECOM)","27 Feb 2020","2019","","","1","6","In recent years, generative adversarial network (GAN) has attracted great attention due to its impressive performance and potential numerous applications, such as data augmentation, real-like image synthesis, image compression improvement, etc. The generator in GAN learns the density of the distribution from real data in order to generate high fidelity fake samples from latent space and deceive the discriminator. Despite its advantages, GAN can easily memorize training samples because of the high model complexity of deep neural networks. Thus, training a GAN with sensitive or private data samples may compromise the privacy of training data. To address this privacy issue, we propose a novel Privacy Preserving Generative Adversarial Network (PPGAN) that perturbs the objective function of discriminator by injecting Laplace noises based on functional mechanism to guarantee the differential privacy of training data. Since generator training is considered as a post-processing step while guaranteeing differential privacy of discriminator, the trained generator should be differentially private to effectively protect data samples. Through detailed privacy analysis, we theoretically prove that PPGAN can provide such strict differential privacy guarantee. With extensive simulation study on the benchmark dataset MNIST, we show the efficacy of the proposed PPGAN under practical privacy budgets.","2576-6813","978-1-7281-0962-6","10.1109/GLOBECOM38437.2019.9014134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9014134","","Generators;Generative adversarial networks;Gallium nitride;Training;Linear programming","data privacy;deep learning (artificial intelligence);neural nets","differentially private functional mechanism;data augmentation;image synthesis;image compression improvement;high fidelity fake samples;training samples;high model complexity;deep neural networks;sensitive data samples;private data samples;training data;objective function;generator training;trained generator;strict differential privacy guarantee;practical privacy budgets;Privacy Preserving Generative Adversarial Network;Laplace noises","","5","","13","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Anatomic and Molecular MR Image Synthesis Using Confidence Guided CNNs","P. Guo; P. Wang; R. Yasarla; J. Zhou; V. M. Patel; S. Jiang","Whiting School of Engineering, Johns Hopkins University, Baltimore, MD, USA; Whiting School of Engineering, Johns Hopkins University, Baltimore, MD, USA; Whiting School of Engineering, Johns Hopkins University, Baltimore, MD, USA; School of Medicine, Johns Hopkins University, Baltimore, MD, USA; Whiting School of Engineering, Johns Hopkins University, Baltimore, MD, USA; School of Medicine, Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Medical Imaging","30 Sep 2021","2021","40","10","2832","2844","Data-driven automatic approaches have demonstrated their great potential in resolving various clinical diagnostic dilemmas in neuro-oncology, especially with the help of standard anatomic and advanced molecular MR images. However, data quantity and quality remain a key determinant, and a significant limit of the potential applications. In our previous work, we explored the synthesis of anatomic and molecular MR image networks (SAMR) in patients with post-treatment malignant gliomas. In this work, we extend this through a confidence-guided SAMR (CG-SAMR) that synthesizes data from lesion contour information to multi-modal MR images, including T1-weighted ( ${T}_{1}\text{w}$ ), gadolinium enhanced  ${T}_{1}\text{w}$  (Gd- ${T}_{1}\text{w}$ ), T2-weighted ( ${T}_{2}\text{w}$ ), and fluid-attenuated inversion recovery ( $\textit {FLAIR}$ ), as well as the molecular amide proton transfer-weighted ( $\textit {APT}\text{w}$ ) sequence. We introduce a module that guides the synthesis based on a confidence measure of the intermediate results. Furthermore, we extend the proposed architecture to allow training using unpaired data. Extensive experiments on real clinical data demonstrate that the proposed model can perform better than current the state-of-the-art synthesis methods. Our code is available at https://github.com/guopengf/CG-SAMR.","1558-254X","","10.1109/TMI.2020.3046460","National Institutes of Health(grant numbers:R01CA228188); National Science Foundation(grant numbers:1910141); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302607","Generative adversarial network;confidence guidance;multi-modal MR image synthesis;glioma;segmentation","Lesions;Image synthesis;Training;Magnetic resonance imaging;Image segmentation;Generative adversarial networks;Decoding","biomedical MRI;brain;cancer;medical image processing;patient treatment;tumours","confidence guided;data-driven automatic approaches;clinical diagnostic dilemmas;neuro-oncology;standard anatomic MR images;advanced molecular MR images;data quantity;key determinant;significant limit;anatomic MR image networks;molecular MR image networks;post-treatment malignant gliomas;confidence-guided SAMR;synthesizes data;lesion contour information;multimodal MR images;Gd- T 1 w;fluid-attenuated inversion recovery;proton transfer-weighted;confidence measure;unpaired data;clinical data;state-of-the-art synthesis methods","Glioma;Humans;Magnetic Resonance Imaging","5","","77","IEEE","22 Dec 2020","","","IEEE","IEEE Journals"
"Efficient Neural Architecture for Text-to-Image Synthesis","D. M. Souza; J. Wehrmann; D. D. Ruiz","School of Technology, Pontifícia Universidade Católica do Rio Grande do Sul, Porto Alegre, Brazil; School of Technology, Pontifícia Universidade Católica do Rio Grande do Sul, Porto Alegre, Brazil; School of Technology, Pontifícia Universidade Católica do Rio Grande do Sul, Porto Alegre, Brazil","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","8","Text-to-image synthesis is the task of generating images from text descriptions. Image generation, by itself, is a challenging task. When we combine image generation and text, we bring complexity to a new level: we need to combine data from two different modalities. Most of recent works in text-to-image synthesis follow a similar approach when it comes to neural architectures. Due to aforementioned difficulties, plus the inherent difficulty of training GANs at high resolutions, most methods have adopted a multi-stage training strategy. In this paper we shift the architectural paradigm currently used in text-to-image methods and show that an effective neural architecture can achieve state-of-the-art performance using a single stage training with a single generator and a single discriminator. We do so by applying deep residual networks along with a novel sentence interpolation strategy that enables learning a smooth conditional space. Finally, our work points a new direction for text-to-image research, which has not experimented with novel neural architectures recently.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207584","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207584","text-to-image synthesis;generative models;multimodal learning","Training;Gallium nitride;Generators;Computer architecture;Interpolation;Generative adversarial networks;Task analysis","interpolation;learning (artificial intelligence);neural net architecture;text analysis;text detection;unsupervised learning","neural architecture;text-to-image synthesis;text descriptions;image generation;GAN;deep residual networks;sentence interpolation strategy","","5","","47","IEEE","28 Sep 2020","","","IEEE","IEEE Conferences"
"DR-GAN: Conditional Generative Adversarial Network for Fine-Grained Lesion Synthesis on Diabetic Retinopathy Images","Y. Zhou; B. Wang; X. He; S. Cui; L. Shao","School of Computer Science, and Engineering, Southeast University, Nanjing, China; Computer Vision, Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; Bodhi Laboratory, Beijing BeYes Technology Co. Ltd., Beijing, China; Computer Vision, Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; Computer Vision, Inception Institute of Artificial Intelligence, Abu Dhabi, UAE","IEEE Journal of Biomedical and Health Informatics","17 Jan 2022","2022","26","1","56","66","Diabetic retinopathy (DR) is a complication of diabetes that severely affects eyes. It can be graded into five levels of severity according to international protocol. However, optimizing a grading model to have strong generalizability requires a large amount of balanced training data, which is difficult to collect, particularly for the high severity levels. Typical data augmentation methods, including random flipping and rotation, cannot generate data with high diversity. In this paper, we propose a diabetic retinopathy generative adversarial network (DR-GAN) to synthesize high-resolution fundus images which can be manipulated with arbitrary grading and lesion information. Thus, large-scale generated data can be used for more meaningful augmentation to train a DR grading and lesion segmentation model. The proposed retina generator is conditioned on the structural and lesion masks, as well as adaptive grading vectors sampled from the latent grading space, which can be adopted to control the synthesized grading severity. Moreover, a multi-scale spatial and channel attention module is devised to improve the generation ability to synthesize small details. Multi-scale discriminators are designed to operate from large to small receptive fields, and joint adversarial losses are adopted to optimize the whole network in an end-to-end manner. With extensive experiments evaluated on the EyePACS dataset connected to Kaggle, as well as the FGADR dataset, we validate the effectiveness of our method, which can both synthesize highly realistic ($1280 \times 1280$) controllable fundus images and contribute to the DR grading task.","2168-2208","","10.1109/JBHI.2020.3045475","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9296950","Diabetic retinopathy;image synthesis;generative adversarial networks","Lesions;Generators;Retina;Gallium nitride;Generative adversarial networks;Image synthesis;Image segmentation","biomedical optical imaging;diseases;eye;image segmentation;medical disorders;medical image processing","lesion information;large-scale generated data;meaningful augmentation;lesion segmentation model;retina generator;structural lesion masks;adaptive grading vectors;latent grading space;synthesized grading severity;channel attention module;generation ability;multiscale discriminators;joint adversarial losses;highly realistic controllable fundus images;DR grading task;DR-GAN;conditional generative adversarial network;fine-grained lesion synthesis;international protocol;grading model;strong generalizability;balanced training data;high severity levels;typical data;random flipping rotation;diabetic retinopathy generative adversarial network;high-resolution fundus images;arbitrary grading","Diabetes Mellitus;Diabetic Retinopathy;Fundus Oculi;Humans;Image Processing, Computer-Assisted;Retina","21","","50","IEEE","17 Dec 2020","","","IEEE","IEEE Journals"
"PollenGAN: Synthetic Pollen Grain Image Generation for Data Augmentation","P. Viertel; M. König; J. Rexilius","Campus Minden, Bielefeld University of Applied Sciences, Minden, Germany; Campus Minden, Bielefeld University of Applied Sciences, Minden, Germany; Campus Minden, Bielefeld University of Applied Sciences, Minden, Germany","2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)","25 Jan 2022","2021","","","44","49","Palynology, the study of pollen, is becoming the focus of attention in computer vision in recent years. Various proposed automated classification and segmentation methods have been evaluated on a number of data sets. However, as of 2021 most data sets are sparse; they either contain only a small number of pollen classes, images in total or are imbalanced overall. In this work, we explore the possibility of creating synthetic pollen grain images from less than 2,000 images per pollen class via a Generative Adversarial Network (GAN). For that purpose, we selected two distinct pollen classes from a state of the art pollen data set and evaluated the data set with and without synthetic data on a Convolutional Neural Network (CNN). The enriched data set performed better overall (+1.4%) and specifically for the two pollen classes (+2%). We also drastically reduced the no. of real images and were still able to achieve a score of 60% to 80%. The experiments show, that our synthesized pollen images are visually close to real-life pollen grains and can be used to enrich imbalanced data sets as an addition to traditional data augmentation methods.","","978-1-6654-4337-1","10.1109/ICMLA52953.2021.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680079","Machine Learning;image synthesis;data augmentation;deep learning;palynology;computer vision;generative adversarial networks","Image segmentation;Computer vision;Image synthesis;Conferences;Machine learning;Generative adversarial networks;Convolutional neural networks","biology computing;computer vision;feature extraction;image classification;image segmentation;learning (artificial intelligence);neural nets;pattern classification","synthetic pollen grain image generation;classification;segmentation methods;pollen class;synthetic pollen grain images;Generative Adversarial Network;distinct pollen classes;art pollen data;synthetic data;enriched data;synthesized pollen images;real-life pollen grains;imbalanced data sets;traditional data augmentation methods","","3","","17","IEEE","25 Jan 2022","","","IEEE","IEEE Conferences"
"Video Description with GAN","M. Wang","North China University of Technology, Beijing, China","2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology (CCET)","6 Oct 2020","2020","","","10","13","Video description is to convert rich information of video data into text information, Which has been attracting broad research attention in the Artificial Intelligence Community. Deep learning has given computers a strong understanding of one-dimensional picture data and two-dimensional video data. However, In real application scenarios, it still faces the problem of insufficient robustness. For example, the generated text information is unreasonable, the scene information and semantic information rich in video data cannot be extracted effectively. GAN (Generative Adversarial Nets) is a model that generates data using countermeasures, in recent years, which is widely used in text generation, dialogue system, image synthesis, etc. However, there has not been much effort on exploring GAN for Video description. In this paper, we design a new discriminant network on the basis of the traditional text description. In addition, we add the long-short Time memory networks into the model to minimize the loss of information in the encoding or decoding process, so as to generate more reasonable sentences. Experimental results demonstrate that our proposed model exceeds most video description methods in public datasets.","","978-1-7281-8811-9","10.1109/CCET50901.2020.9213129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9213129","long-short time memory networks;generative adversarial networks;video description","Decoding;Visualization;Generators;Encoding;Convolutional neural networks;Feature extraction;Generative adversarial networks","data mining;feature extraction;learning (artificial intelligence);neural nets;text analysis;video signal processing","Generative Adversarial Nets;semantic information;scene information;generated text information;two-dimensional video data;one-dimensional picture data;Artificial Intelligence Community;broad research attention;video description methods;traditional text description;GAN;text generation","","","","19","IEEE","6 Oct 2020","","","IEEE","IEEE Conferences"
"Protecting Intellectual Property of Generative Adversarial Networks from Ambiguity Attacks","D. S. Ong; C. Seng Chan; K. W. Ng; L. Fan; Q. Yang",University of Malaya; University of Malaya; WeBank AI Lab; WeBank AI Lab; Hong Kong University of Science and Technology,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","3629","3638","Ever since Machine Learning as a Service emerges as a viable business that utilizes deep learning models to generate lucrative revenue, Intellectual Property Right (IPR) has become a major concern because these deep learning models can easily be replicated, shared, and re-distributed by any unauthorized third parties. To the best of our knowledge, one of the prominent deep learning models - Generative Adversarial Networks (GANs) which has been widely used to create photorealistic image are totally unprotected despite the existence of pioneering IPR protection methodology for Convolutional Neural Networks (CNNs). This paper therefore presents a complete protection framework in both black-box and white-box settings to enforce IPR protection on GANs. Empirically, we show that the proposed method does not compromise the original GANs performance (i.e. image generation, image super-resolution, style transfer), and at the same time, it is able to withstand both removal and ambiguity attacks against embedded watermarks. Codes are available at https://github.com/dingsheng-ong/ipr-gan.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00363","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577609","","Deep learning;Knowledge engineering;Computer vision;Image synthesis;Superresolution;Intellectual property;Watermarking","authorisation;convolutional neural nets;image resolution;image watermarking;industrial property;learning (artificial intelligence);security of data","lucrative revenue;Intellectual Property Right;prominent deep learning models;generative adversarial networks;IPR protection methodology;Convolutional Neural Networks;complete protection framework;GAN performance;image generation;ambiguity attacks;intellectual property protection;Machine Learning;convolutional neural networks","","3","","33","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Data InStance Prior (DISP) in Generative Adversarial Networks","P. Mangla; N. Kumari; M. Singh; B. Krishnamurthy; V. N. Balasubramanian","IIT Hyderabad, India; CMU; CMU; Media and Data Science Research lab, Adobe; IIT Hyderabad, India","2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","15 Feb 2022","2022","","","3471","3481","Recent advances in generative adversarial networks (GANs) have shown remarkable progress in generating high-quality images. However, this gain in performance depends on the availability of a large amount of training data. In limited data regimes, training typically diverges, and therefore the generated samples are of low quality and lack diversity. Previous works have addressed training in low data setting by leveraging transfer learning and data augmentation techniques. We propose a novel transfer learning method for GANs in the limited data domain by leveraging informative data prior derived from self-supervised/supervised pre-trained networks trained on a diverse source domain. We perform experiments on several standard vision datasets using various GAN architectures (BigGAN, SNGAN, StyleGAN2) to demonstrate that the proposed method effectively transfers knowledge to domains with few target images, outperforming existing state-of-the-art techniques in terms of image quality and diversity. We also show the utility of data instance prior in large-scale unconditional image generation.","2642-9381","978-1-6654-0915-5","10.1109/WACV51458.2022.00353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706672","Deep Learning Datasets;Evaluation and Comparison of Vision Algorithms;Deep Learning -> Neural Generative Models;Autoencoders;GANs;Transfer;Few-shot;Semi- and Un- supervised Learning","Training;Image quality;Computer vision;Image synthesis;Transfer learning;Training data;Computer architecture","data handling;learning (artificial intelligence);neural nets;search problems","low data setting;data augmentation;transfer learning;data domain;diverse source domain;GAN architectures;image quality;data instance prior;large-scale unconditional image generation;generative adversarial networks;high-quality images;training data;data regimes;Data InStance Prior;DISP;BigGAN;SNGAN;StyleGAN2;informative data prior;standard vision datasets","","1","","70","IEEE","15 Feb 2022","","","IEEE","IEEE Conferences"
"A Unified Framework for Biphasic Facial Age Translation With Noisy-Semantic Guided Generative Adversarial Networks","M. Sun; J. Wang; J. Liu; J. Li; T. Chen; Z. Sun","National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Ant Group, Beijing, China; Ant Group, Beijing, China; Ant Group, Beijing, China; National Laboratory of Pattern Recognition, Center for Research on Intelligent Perception and Computing, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Information Forensics and Security","14 Apr 2022","2022","17","","1513","1527","Biphasic facial age translation aims at predicting the appearance of the input face at any age. Facial age translation has received considerable research attention in the last decade due to its practical value in cross-age face recognition and various entertainment applications. However, most existing methods model age changes between holistic images, regardless of the human face structure and the age-changing patterns of individual facial components. Consequently, the lack of semantic supervision will cause infidelity of generated faces in detail. To this end, we propose a unified framework for biphasic facial age translation with noisy-semantic guided generative adversarial networks. Structurally, we project the class-aware noisy semantic layouts to “soft” latent maps for the following injection operation on the individual facial parts. In particular, we introduce two sub-networks, ProjectionNet and ConstraintNet. ProjectionNet introduces the low-level structural semantic information with noise map and produces “soft” latent maps. ConstraintNet disentangles the high-level spatial features to constrain the “soft” latent maps, which endows more age-related context into the “soft” latent maps. Specifically, attention mechanism is employed in ConstraintNet for feature disentanglement. Meanwhile, in order to mine the strongest mapping ability of the network, we embed two types of learning strategies in the training procedure, supervised self-driven generation and unsupervised condition-driven cycle-consistent generation. As a result, extensive experiments conducted on MORPH and CACD datasets demonstrate the prominent ability of our proposed method which achieves state-of-the-art performance.","1556-6021","","10.1109/TIFS.2022.3164187","National Natural Science Foundation of China(grant numbers:U1836217); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745962","Biphasic facial age translation;noisy semantic injection;generative adversarial network;attention mechanism;feature disentanglement","Semantics;Faces;Face recognition;Training;Noise measurement;Layout;Image synthesis","face recognition;feature extraction;neural nets;supervised learning;unsupervised learning","biphasic facial age translation;noisy-semantic guided generative adversarial networks;cross-age face recognition;human face structure;age-changing patterns;facial components;class-aware noisy semantic layouts;soft latent maps;individual facial parts;low-level structural semantic information;age-related context;ProjectionNet;ConstraintNet;noise map;high-level spatial features;attention mechanism;feature disentanglement;learning strategies;supervised self-driven generation;unsupervised condition-driven cycle-consistent generation;CACD datasets;MORPH datasets","","","","55","IEEE","1 Apr 2022","","","IEEE","IEEE Journals"
"3D-aware Image Synthesis via Learning Structural and Textural Representations","Y. Xu; S. Peng; C. Yang; Y. Shen; B. Zhou",The Chinese University of Hong Kong; Zhejiang University; The Chinese University of Hong Kong; Bytedance Inc.; The Chinese University of Hong Kong,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","18409","18418","Making generative models 3D-aware bridges the 2D image space and the 3D physical world yet remains challenging. Recent attempts equip a Generative Adversarial Network (GAN) with a Neural Radiance Field (NeRF), which maps 3D coordinates to pixel values, as a 3D prior. However, the implicit function in NeRF has a very local receptive field, making the generator hard to become aware of the global structure. Meanwhile, NeRF is built on volume rendering which can be too costly to produce high-resolution results, increasing the optimization difficulty. To alleviate these two problems, we propose a novel framework, termed as VolumeGAN, for high-fidelity 3D-aware image synthesis, through explicitly learning a structural representation and a textural representation. We first learn a feature volume to represent the underlying structure, which is then converted to a feature field using a NeRF-like model. The feature field is further accumulated into a 2D feature map as the textural representation, followed by a neural renderer for appearance synthesis. Such a design enables independent control of the shape and the appearance. Project page is at https://genforce.github.io/volumegan.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880006","Image and video synthesis and generation; 3D from single images","Solid modeling;Computer vision;Three-dimensional displays;Image synthesis;Shape;Generative adversarial networks;Rendering (computer graphics)","feature extraction;image recognition;image reconstruction;image representation;image texture;learning (artificial intelligence);rendering (computer graphics);solid modelling","high-fidelity 3D-aware image synthesis;structural representation;textural representation;feature volume;feature field;NeRF-like model;2D feature map;neural renderer;appearance synthesis;generative models 3D-aware bridges;2D image space;3D physical world;Generative Adversarial Network;Neural Radiance Field;implicit function;local receptive field;global structure;volume rendering;high-resolution results","","8","","52","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"TilGAN: GAN for Facilitating Tumor-Infiltrating Lymphocyte Pathology Image Synthesis With Improved Image Classification","M. Saha; X. Guo; A. Sharma","Department of Biomedical Informatics, School of Medicine, Emory University, Atlanta, GA, USA; Department of Computer Science, Emory University, Atlanta, GA, USA; Department of Biomedical Informatics, School of Medicine, Emory University, Atlanta, GA, USA","IEEE Access","7 Jun 2021","2021","9","","79829","79840","Tumor-infiltrating lymphocytes (TILs) act as immune cells against cancer tissues. The manual assessment of TILs is usually erroneous, tedious, costly and subject to inter- and intraobserver variability. Machine learning approaches can solve these issues, but they require a large amount of labeled data for model training, which is expensive and not readily available. In this study, we present an efficient generative adversarial network, TilGAN, to generate high-quality synthetic pathology images followed by classification of TIL and non-TIL regions. Our proposed architecture is constructed with a generator network and a discriminator network. The novelty exists in the TilGAN architecture, loss functions, and evaluation techniques. Our TilGAN-generated images achieved a higher Inception score than the real images (2.90 vs. 2.32, respectively). They also achieved a lower kernel Inception distance (1.44) and a lower Fréchet Inception distance (0.312). It also passed the Turing test performed by experienced pathologists and clinicians. We further extended our evaluation studies and used almost one million synthetic data, generated by TilGAN, to train a classification model. Our proposed classification model achieved a 97.83% accuracy, a 97.37% F1-score, and a 97% area under the curve. Our extensive experiments and superior outcomes show the efficiency and effectiveness of our proposed TilGAN architecture. This architecture can also be used for other types of images for image synthesis.","2169-3536","","10.1109/ACCESS.2021.3084597","National Cancer Institute, National Institutes of Health(grant numbers:U24CA215109); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9443091","Digital pathology;deep learning;generative adversarial network;lung cancer;artificial intelligence","Generative adversarial networks;Computer architecture;Pathology;Image synthesis;Convolution;Cancer;Generators","cancer;cellular biophysics;image classification;learning (artificial intelligence);medical image processing;neural net architecture;tumours","high-quality synthetic pathology images;nonTIL regions;generator network;discriminator network;TilGAN architecture;TilGAN-generated images;inception score;Fréchet inception distance;tumor-infiltrating lymphocyte pathology image synthesis;image classification;immune cells;cancer tissues;intraobserver variability;machine learning;model training;generative adversarial network;kernel inception distance;loss functions;Turing test;F1-score;area under the curve","","2","","67","CCBY","28 May 2021","","","IEEE","IEEE Journals"
"Arbitrary-Scale Image Synthesis","E. Ntavelis; M. Shahbazi; I. Kastanis; R. Timofte; M. Danelljan; L. Van Gool","Robotics & ML, CSEM, CH; ETH Zurich, CH, Computer Vision Lab; Robotics & ML, CSEM, CH; ETH Zurich, CH, Computer Vision Lab; ETH Zurich, CH, Computer Vision Lab; U Leuven, BE","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","11523","11532","Positional encodings have enabled recent works to train a single adversarial network that can generate images of different scales. However, these approaches are either limited to a set of discrete scales or struggle to maintain good perceptual quality at the scales for which the model is not trained explicitly. We propose the design of scale-consistent positional encodings invariant to our generator's layers transformations. This enables the generation of arbitrary-scale images even at scales unseen during training. Moreover, we incorporate novel inter-scale augmentations into our pipeline and partial generation training to facilitate the synthesis of consistent images at arbitrary scales. Lastly, we show competitive results for a continuum of scales on various commonly used datasets for image synthesis.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880400","Image and video synthesis and generation","Training;Computer vision;Image coding;Image synthesis;Pipelines;Generative adversarial networks;Encoding","image coding;image denoising;image representation;image resolution;prediction theory;wavelet transforms","arbitrary-scale images;novel inter-scale augmentations;pipeline;partial generation training;consistent images;arbitrary scales;arbitrary-scale image synthesis;single adversarial network;discrete scales;good perceptual quality;scale-consistent positional encodings invariant;generator","","1","","42","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Mask-Embedded Discriminator with Region-based Semantic Regularization for Semi-Supervised Class-Conditional Image Synthesis","Y. Liu; X. Huo; T. Chen; X. Zeng; S. Wu; Z. Yu; H. -S. Wong","School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; Department of Computer Science, City University of Hong Kong; School of Computer Science and Engineering, South China University of Technology; Department of Computer Science, City University of Hong Kong","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","5502","5511","Semi-supervised generative learning (SSGL) makes use of unlabeled data to achieve a trade-off between the data collection/annotation effort and generation performance, when adequate labeled data are not available. Learning precise class semantics is crucial for class-conditional image synthesis with limited supervision. Toward this end, we propose a semi-supervised Generative Adversarial Network with a Mask-Embedded Discriminator, which is referred to as MED-GAN. By incorporating a mask embedding module, the discriminator features are associated with spatial information, such that the focus of the discriminator can be limited in the specified regions when distinguishing between real and synthesized images. A generator is enforced to synthesize the instances holding more precise class semantics in order to deceive the enhanced discriminator. Also benefiting from mask embedding, region-based semantic regularization is imposed on the discriminator feature space, and the degree of separation between real and fake classes and among object categories can thus be increased. This eventually improves class-conditional distribution matching between real and synthesized data. In the experiments, the superior performance of MED-GAN demonstrates the effectiveness of mask embedding and associated regularizers in facilitating SSGL.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00546","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577643","","Computer vision;Image synthesis;Semantics;Generative adversarial networks;Generators;Pattern recognition","image classification;natural language processing;supervised learning","region-based semantic regularization;semisupervised class-conditional image synthesis;semisupervised generative learning;SSGL;unlabeled data;generation performance;class semantics;semisupervised Generative Adversarial Network;MED-GAN;mask embedding module;discriminator features;specified regions;synthesized images;enhanced discriminator;discriminator feature space;fake classes;class-conditional distribution;synthesized data;associated regularizers;mask-embedded discriminator","","1","","47","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"GAN You Hear Me? Reclaiming Unconditional Speech Synthesis from Diffusion Models","M. Baas; H. Kamper","MediaLab, Electrical & Electronic Engineering, Stellenbosch University, South Africa; MediaLab, Electrical & Electronic Engineering, Stellenbosch University, South Africa","2022 IEEE Spoken Language Technology Workshop (SLT)","27 Jan 2023","2023","","","906","911","We propose AudioStyleGAN (ASGAN), a new generative adversarial network (GAN) for unconditional speech synthesis. As in the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation to probabilistically skip discriminator updates. ASGAN achieves state-of-the-art results in unconditional speech synthesis on the Google Speech Commands dataset. It is also substantially faster than the top-performing diffusion models. Through a design that encourages disentanglement, ASGAN is able to perform voice conversion and speech editing without being explicitly trained to do so. ASGAN demonstrates that GANs are still highly competitive with diffusion models. Code, models, samples: https://github.com/RF5/simple-asgan/.","","979-8-3503-9690-4","10.1109/SLT54892.2023.10023153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023153","Unconditional speech synthesis;generative adversarial networks;speech disentanglement;voice conversion","Training;Measurement;Adaptation models;Image synthesis;Conferences;Generative adversarial networks;Probabilistic logic","learning (artificial intelligence);neural nets;speech synthesis;vectors","adaptive discriminator augmentation;ASGAN maps;audio features;diffusion models;disentangled latent vector;generative adversarial network;Google Speech Commands dataset;image synthesis models;signal aliasing;speech editing;StyleGAN family;unconditional speech synthesis;voice conversion","","","","35","IEEE","27 Jan 2023","","","IEEE","IEEE Conferences"
"Synthesis and Edition of Ultrasound Images via Sketch Guided Progressive Growing GANS","J. Liang; X. Yang; H. Li; Y. Wang; M. T. Van; H. Dou; C. Chen; J. Fang; X. Liang; Z. Mai; G. Zhu; Z. Chen; D. Ni","Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China; Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China; Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China; Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China; Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China; Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China; Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China; Experimental Center of Liwan Hospital, The Third Affiliated Hospital of Guangzhou Medical University, Guangzhou, China; Experimental Center of Liwan Hospital, The Third Affiliated Hospital of Guangzhou Medical University, Guangzhou, China; Experimental Center of Liwan Hospital, The Third Affiliated Hospital of Guangzhou Medical University, Guangzhou, China; Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China; Experimental Center of Liwan Hospital, The Third Affiliated Hospital of Guangzhou Medical University, Guangzhou, China; Medical UltraSound Image Computing (MUSIC) Lab, Shenzhen University, Shenzhen, China","2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)","22 May 2020","2020","","","1793","1797","Ultrasound (US) is widely accepted in clinic for anatomical structure inspection. However, lacking in resources to practice US scan, novices often struggle to learn the operation skills. Also, in the deep learning era, automated US image analysis is limited by the lack of annotated samples. Efficiently synthesizing realistic, editable and high resolution US images can solve the problems. The task is challenging and previous methods can only partially complete it. In this paper, we devise a new framework for US image synthesis. Particularly, we firstly adopt a sketch generative adversarial networks (Sgan) to introduce background sketch upon object mask in a conditioned generative adversarial network. With enriched sketch cues, Sgan can generate realistic US images with editable and fine-grained structure details. Although effective, Sgan is hard to generate high resolution US images. To achieve this, we further implant the Sgan into a progressive growing scheme (PGSgan). By smoothly growing both generator and discriminator, PGSgan can gradually synthesize US images from low to high resolution. By synthesizing ovary and follicle US images, our extensive perceptual evaluation, user study and segmentation results prove the promising efficacy and efficiency of the proposed PGSgan.","1945-8452","978-1-5386-9330-8","10.1109/ISBI45749.2020.9098384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098384","Ultrasound;Image synthesis;Conditional GAN;High resolution;Progressive growing","Training;Generators;Image resolution;Image generation;Ultrasonic imaging;Gallium nitride;Image segmentation","biomedical ultrasonics;image resolution;learning (artificial intelligence);medical image processing;neural nets","automated US image analysis;annotated samples;US image synthesis;sketch generative adversarial networks;background sketch;conditioned generative adversarial network;enriched sketch cues;realistic US images;fine-grained structure details;high resolution US images;PGSgan;generator;ovary US images;follicle US images;ultrasound images;anatomical structure inspection;deep learning era;sketch guided progressive growing GANS scheme;object mask;discriminator;extensive perceptual evaluation","","8","","17","IEEE","22 May 2020","","","IEEE","IEEE Conferences"
"Spatial Fusion GAN for Image Synthesis","F. Zhan; H. Zhu; S. Lu","Nanyang Technological University, Singapore; Institute for Infocomm, A*STAR, Singapore; Nanyang Technological University, Singapore","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","3648","3657","Recent advances in generative adversarial networks (GANs) have shown great potentials in realistic image synthesis whereas most existing works address synthesis realism in either appearance space or geometry space but few in both. This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a geometry synthesizer and an appearance synthesizer to achieve synthesis realism in both geometry and appearance spaces. The geometry synthesizer learns contextual geometries of background images and transforms and places foreground objects into the background images unanimously. The appearance synthesizer adjust the color, brightness and styles of the foreground objects and embeds them into background images harmoniously, where a guided filter is incorporated for detail preserving. The two synthesizers are inter-connected as mutual references which can be trained end-to-end with little supervision. The SF-GAN has been evaluated in two tasks: (1) realistic scene text image synthesis for training better recognition models; (2) glass and hat wearing for realistic matching glasses and hats with real portraits. Qualitative and quantitative comparisons with the state-of-the-art demonstrate the superiority of the proposed SF-GAN.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953967","Image and Video Synthesis;Deep Learning;Document Analysis","Geometry;Training;Image recognition;Image synthesis;Text recognition;Synthesizers;Computational modeling","brightness;computational geometry;image colour analysis;image filtering;image fusion;image matching;image representation;learning (artificial intelligence);realistic images;text detection","foreground objects;background images;appearance synthesizer;SF-GAN;hats;generative adversarial networks;realistic scene text image synthesis;geometry synthesizer;appearance spaces;contextual geometries;spatial fusion GAN;realistic matching;glasses;color adjustment;brightness adjustment;foreground object style adjustment;guided filter;detail preserving","","55","2","55","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"HumanGAN: A Generative Model of Human Images","K. Sarkar; L. Liu; V. Golyanik; C. Theobalt","Max Planck Institute for Informatics, SIC; Max Planck Institute for Informatics, SIC; Max Planck Institute for Informatics, SIC; Max Planck Institute for Informatics, SIC","2021 International Conference on 3D Vision (3DV)","6 Jan 2022","2021","","","258","267","Generative adversarial networks achieve great performance in photorealistic image synthesis in various domains, including human images. However, they usually employ latent vectors that encode the sampled outputs globally. This does not allow convenient control of semantically relevant individual parts of the image, and cannot draw samples that only differ in partial aspects, such as clothing style. We address these limitations and present a generative model for images of dressed humans offering control over pose, local body part appearance and garment style. This is the first method to solve various aspects of human image generation, such as global appearance sampling, pose transfer, parts and garment transfer, and part sampling jointly in a unified framework. As our model encodes part-based latent appearance vectors in a normalized pose-independent space and warps them to different poses, it preserves body and clothing appearance under varying posture. Experiments show that our flexible and general generative method outperforms task-specific baselines for pose-conditioned image generation, pose transfer and part sampling in terms of realism and output resolution.","2475-7888","978-1-6654-2688-6","10.1109/3DV53792.2021.00036","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665953","","Solid modeling;Three-dimensional displays;Image resolution;Image synthesis;Clothing;Aerospace electronics;Generative adversarial networks","clothing;pose estimation;realistic images;rendering (computer graphics);solid modelling","generative model;human images;generative adversarial networks;photorealistic image synthesis;latent vectors;sampled outputs;convenient control;semantically relevant individual parts;partial aspects;clothing style;dressed humans;local body part appearance;garment style;human image generation;global appearance sampling;garment transfer;part-based latent appearance vectors;pose-independent space;different poses;clothing appearance;flexible method;general generative method;pose-conditioned image generation","","9","","54","IEEE","6 Jan 2022","","","IEEE","IEEE Conferences"
"Generative Hierarchical Features from Synthesizing Images","Y. Xu; Y. Shen; J. Zhu; C. Yang; B. Zhou",The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","4430","4430","Generative Adversarial Networks (GANs) have recently advanced image synthesis by learning the underlying distribution of the observed data. However, how the features learned from solving the task of image generation are applicable to other vision tasks remains seldom explored. In this work, we show that learning to synthesize images can bring remarkable hierarchical visual features that are generalizable across a wide range of applications. Specifically, we consider the pre-trained StyleGAN generator as a learned loss function and utilize its layer-wise representation to train a novel hierarchical encoder. The visual feature produced by our encoder, termed as Generative Hierarchical Feature (GH-Feat), has strong transferability to both generative and discriminative tasks, including image editing, image harmonization, image classification, face verification, landmark detection, and layout prediction. Extensive qualitative and quantitative experimental results demonstrate the appealing performance of GH-Feat.1","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578038","","Visualization;Computer vision;Image synthesis;Face recognition;Layout;Generative adversarial networks;Feature extraction","computer vision;face recognition;feature extraction;image classification;image representation;learning (artificial intelligence);object detection","Generative hierarchical features;Generative Adversarial Networks;image synthesis;image generation;vision tasks;remarkable hierarchical visual features;pre-trained StyleGAN generator;learned loss function;novel hierarchical encoder;visual feature;Generative Hierarchical Feature;generative tasks;discriminative tasks;image editing;image harmonization;image classification","","7","","69","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"A Primer on Conditional Text based Image Generation through Generative Models","A. Singh Parihar; A. Kaushik; A. V. Choudhary; A. K. Singh","Dept. of Computer Engineering, Delhi Technological University, New Delhi, India; Dept. of Computer Engineering, Delhi Technological University, New Delhi, India; Dept. of Computer Engineering, Delhi Technological University, New Delhi, India; Dept. of Computer Engineering, Delhi Technological University, New Delhi, India","2020 5th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE)","26 Feb 2021","2020","","","1","6","Synthesis of Images from text descriptions has emerged as an interesting albeit a challenging task in the domain of Image Synthesis. Many promising advances have been made in the direction of text-based image generation in the recent years, with the emergence of Multi-modal Generative Adversarial Networks. In this paper, we discuss the various approaches which utilise Conditional-GANs to accomplish the task of generating photo-realistic images based on their text descriptions and compare their architectures and performance on various benchmark datasets. The performance of these approaches are evaluated using various well-known metrics.","","978-1-7281-8867-6","10.1109/ICRAIE51050.2020.9358343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9358343","","Measurement;Technological innovation;Image synthesis;Conferences;Benchmark testing;Generative adversarial networks;Task analysis","gallium compounds;image retrieval;realistic images;rendering (computer graphics);text analysis","photo-realistic images;text descriptions;Conditional text;Generative models;Image Synthesis;promising advances;text-based image generation;Multimodal Generative Adversarial Networks;utilise Conditional-GANs","","","","21","IEEE","26 Feb 2021","","","IEEE","IEEE Conferences"
"SynLibras: A Disentangled Deep Generative Model for Brazilian Sign Language Synthesis","W. Silveira; A. Alaniz; M. Hurtado; B. C. Da Silva; R. De Bem","Center of Computational Sciences (C3), Federal University of Rio Grande (FURG), Brazil; Center of Computational Sciences (C3), Federal University of Rio Grande (FURG), Brazil; Center of Computational Sciences (C3), Federal University of Rio Grande (FURG), Brazil; Center of Computational Sciences (C3), Federal University of Rio Grande (FURG), Brazil; Center of Computational Sciences (C3), Federal University of Rio Grande (FURG), Brazil","2022 35th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)","26 Dec 2022","2022","1","","210","215","Recent advances regarding deep generative models have strengthened a realm of approaches in which discriminative and generative tasks are tackled jointly in an analysis-by-synthesis manner. In this category, variational autoencoders (VAEs) and generative adversarial networks (GANs) aim for learning latent data representations from which sampling of synthetic images may be performed. However, sampling in such models normally does not allow for independent control of diverse factors of variation. Despite general efforts to overcome this issue, deep generative models tailored for sign language with disentangled factors of variation are yet not vastly explored in the literature. In this work, we introduce the SynLibras, a novel model that allows for disentangling appearance and gestural communication (i.e. body, hands and face poses) on image synthesis. Our model is capable of performing cross-language pose-transfer while maintaining the appearance of the source signer. We perform experiments on the RWTH-PHOENIX-Weather dataset and evaluation using the PSNR and the SSIM metrics. To our knowledge, the SynLibras is the first method for Brazilian sign language (Libras) synthesis in images. We compare our model with the EDN, a well-known general pose-transfer method, achieving better results on Libras synthesis. Finally, we also introduce the SynLibras-Pose, a dataset with annotated poses of Libras signers performing single words.","2377-5416","978-1-6654-5385-1","10.1109/SIBGRAPI55357.2022.9991748","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9991748","","Measurement;Graphics;Analytical models;Image synthesis;Gesture recognition;Assistive technologies;Generative adversarial networks","data structures;feature extraction;gesture recognition;handicapped aids;image classification;learning (artificial intelligence);natural language processing;natural languages","analysis-by-synthesis manner;Brazilian sign language synthesis;cross-language pose-transfer;deep generative models;discriminative tasks;disentangled deep generative model;generative adversarial networks aim;generative tasks;image synthesis;latent data representations;Libras synthesis;SynLibras-Pose;variational autoencoders","","","","59","IEEE","26 Dec 2022","","","IEEE","IEEE Conferences"
"Detection of Synthesized Satellite Images Using Deep Neural Networks","W. -H. Liao; Y. -S. Chang; Y. -C. Wu","Dept. of Computer Science, National Chengchi University, Taipei, Taiwan; Dept. of Computer Science, National Chengchi University, Taipei, Taiwan; Artificial Intelligence and E-learning Center, National Chengchi University, Taipei, Taiwan","2023 17th International Conference on Ubiquitous Information Management and Communication (IMCOM)","8 Feb 2023","2023","","","1","5","The technology of generative adversarial networks (GAN) is constantly evolving, and synthesized images can no longer be accurately distinguished by the human eyes alone. GAN has been applied to the analysis of satellite images, mostly for the purpose of data augmentation. Recently, however, we have seen a twist in its usage. In information warfare, GAN has been used to create fake satellite images or modify the image content by putting fake bridges, buildings and clouds to mislead or conceal important intelligence. To address the increasing counterfeit cases in satellite images, the goal of this research is to develop algorithms that can classify fake remote sensing images robustly and efficiently. There exist many techniques to synthesize or manipulate the content of satellite images. In this paper, we focus on the case when the entire image is forged. Three satellite image synthesis methods, including ProGAN, cGAN and CycleGAN will be investigated. The effect of image pre-processing such as histogram equalization and bilateral filter will also be evaluated. Experiments show that satellite images generated by different GANs can be easily identified by individually trained models. The performance degraded when model trained with one type of GAN samples is employed to determine the originality of images synthesized with other types of GANs. Additionally, when histogram equalization is applied to the images, the detection model fails to distinguish its authenticity. A four-class universal classification model is proposed to address this issue. An overall accuracy of over 99% has been achieved even when pre-processing has been applied.","","978-1-6654-5348-6","10.1109/IMCOM56909.2023.10035570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035570","Satellite Imagery;Generative Adversarial Network;Image Forgery Detection","Histograms;Satellites;Image synthesis;Neural networks;Interference;Generative adversarial networks;Forgery","","","","","","12","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"High-Resolution Driving Scene Synthesis Using Stacked Conditional Gans and Spectral Normalization","S. Lin; L. Chen; Q. Zou; W. Tian","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science, Wuhan University, Wuhan, China; Karlsruhe Institute of Technology, Karlsruhe, Germany","2019 IEEE International Conference on Multimedia and Expo (ICME)","5 Aug 2019","2019","","","1330","1335","Large-scale dataset plays a key role in the driving scene understanding for deep learning based-autonomous driving tasks. Due to the fact that the annotation for a large number of images is extremely labor-intensive and time-consuming, many researchers turn to using image-synthesis techniques for automatic construction of training data. However, traditional methods often have difficulties in producing high-definition driving scene images. To tackle this problem, in this paper, we propose a novel deep model - hdCGAN - for high-definition image-to-image translation. The hdCGAN is built on a conditional GAN in combination with a spectral normalization. Moreover, we improve the hdCGAN by using a stacked network architecture and the enhanced model is called stack-hdCGAN. With the guidance of multi-scale discriminators and the constraint of spectral normalization in the training procedure, the learned models can generate high-resolution and high-quality driving scene images from corresponding semantic segmentation maps. Quantitative and qualitative evaluations on the Cityscapes dataset demonstrate the effectiveness of the proposed models.","1945-788X","978-1-5386-9552-4","10.1109/ICME.2019.00231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784886","Conditional generative adversarial networks, Image to image translation, Stack structure, Spectral normalization","Training;Semantics;Generators;Image segmentation;Gallium nitride;Feature extraction;Image synthesis","image classification;image representation;image resolution;image segmentation;learning (artificial intelligence);mobile robots;robot vision","stack-hdCGAN;multiscale discriminators;spectral normalization;high-resolution driving scene synthesis;large-scale dataset;driving scene understanding;deep learning based-autonomous driving tasks;image-synthesis techniques;high-definition driving scene images;high-definition image-to-image translation;conditional GAN;stacked network architecture;stacked conditional GAN;automatic training data construction;semantic segmentation maps","","2","","19","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"A Privacy Preservation Pipeline for Personally Identifiable Data in Images Using Convolutional and Transformer Architectures","K. Brkić; T. Hrkać; Z. Kalafatić","University of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb, Croatia; University of Zagreb, Faculty of Electrical Engineering and Computing, Zagreb, Croatia","2022 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO)","27 Jun 2022","2022","","","924","929","Image and video data of people, shared voluntarily and involuntarily, is ubiquitous. There is an increased need for techniques that enable privacy protection via removal of personally identifiable information in such data, spurred by regulatory interest and increased social awareness of privacy implications. In this paper, we introduce a privacy preservation pipeline that enables de-identifying personal data in images and videos via replacement image synthesis while retaining data utility. We utilize the recently proposed convolutional VQGANs with autoregressive transformers to synthesize realistic and fully de-identified images of people that are then blended with the original scene. Experimental results show that the method provides strong de-identification while retaining the realism of the scene.","2623-8764","978-953-233-103-5","10.23919/MIPRO55190.2022.9803731","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9803731","sensitive data;de-identification;privacy protection;generative adversarial networks;transformers;machine learning;deep learning","Training;Data privacy;Image segmentation;Graphical models;Image synthesis;Pipelines;Semantics","autoregressive processes;convolutional neural nets;data privacy;image processing;video signal processing","data utility;autoregressive transformers;privacy preservation pipeline;personally identifiable data;video data;privacy protection;personally identifiable information;social awareness;personal data identification;replacement image synthesis;convolutional VQGAN;convolutional architecture;transformer architecture;image data;person deidentification","","","","22","","27 Jun 2022","","","IEEE","IEEE Conferences"
"Migrating Face Swap to Mobile Devices: A Lightweight Framework and a Supervised Training Solution","H. Yu; H. Zhu; X. Lu; J. Liu","iQIYI Inc, Beijing, China; Nanjing University, Nanjing, China; iQIYI Inc, Beijing, China; iQIYI Inc, Beijing, China","2022 IEEE International Conference on Multimedia and Expo (ICME)","26 Aug 2022","2022","","","1","6","Existing face swap methods rely heavily on large-scale networks for adequate capacity to generate visually plausible results, which inhibits its applications on resource-constraint platforms. In this work, we propose MobileFSGAN, a novel lightweight GAN for face swap that can run on mobile devices with much fewer parameters while achieving competitive performance. A lightweight encoder-decoder structure is designed especially for image synthesis tasks, which is only 10.2MB and can run on mobile devices at a real-time speed. To tackle the unstability of training such a small network, we construct the FSTriplets dataset utilizing facial attribute editing techniques. FSTriplets provides source-target-result training triplets, yielding pixel-level labels thus for the first time making the training process supervised. We also designed multi-scale gradient losses for efficient back-propagation, resulting in faster and better convergence. Experimental results show that our model reaches comparable performance towards state-of-the-art methods, while significantly reducing the number of network parameters. Codes and the dataset have been released11https://githuh.com/HoiM/MobileFSGAN.","1945-788X","978-1-6654-8563-0","10.1109/ICME52920.2022.9859806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859806","Face swap;lightweight neural network;generative adversarial networks;deep learning","Training;Performance evaluation;Image synthesis;Wearable computers;Neural networks;Mobile handsets;Real-time systems","face recognition;gradient methods;image representation;learning (artificial intelligence);pose estimation","mobile devices;competitive performance;lightweight encoder-decoder structure;image synthesis tasks;10.2MB;real-time speed;training such a small network;FSTriplets dataset;facial attribute editing techniques;source-target-result training triplets;training process;multiscale gradient losses;network parameters;lightweight framework;supervised training solution;face swap methods;large-scale networks;adequate capacity;visually plausible results;resource-constraint platforms;novel lightweight GAN;memory size 10.2 MByte","","","","26","IEEE","26 Aug 2022","","","IEEE","IEEE Conferences"
"Towards Unsupervised Learning of Generative Models for 3D Controllable Image Synthesis","Y. Liao; K. Schwarz; L. Mescheder; A. Geiger","University of Tübingen; University of Tübingen; Amazon, Tübingen; University of Tübingen","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","5870","5879","In recent years, Generative Adversarial Networks have achieved impressive results in photorealistic image synthesis. This progress nurtures hopes that one day the classical rendering pipeline can be replaced by efficient models that are learned directly from images. However, current image synthesis models operate in the 2D domain where disentangling 3D properties such as camera viewpoint or object pose is challenging. Furthermore, they lack an interpretable and controllable representation. Our key hypothesis is that the image generation process should be modeled in 3D space as the physical world surrounding us is intrinsically three-dimensional. We define the new task of 3D controllable image synthesis and propose an approach for solving it by reasoning both in 3D space and in the 2D image domain. We demonstrate that our model is able to disentangle latent 3D factors of simple multi-object scenes in an unsupervised fashion from raw images. Compared to pure 2D baselines, it allows for synthesizing scenes that are consistent wrt. changes in viewpoint or object pose. We further evaluate various 3D representations in terms of their usefulness for this challenging task.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00591","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156436","","Three-dimensional displays;Two dimensional displays;Solid modeling;Image generation;Rendering (computer graphics);Generators;Task analysis","image representation;pose estimation;realistic images;rendering (computer graphics);solid modelling;unsupervised learning","unsupervised learning;3D controllable image synthesis;Generative Adversarial Networks;photorealistic image synthesis;image synthesis models;camera viewpoint;image generation process;2D image domain;classical rendering pipeline","","25","","52","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"LoGANv2: Conditional Style-Based Logo Generation with Generative Adversarial Networks","C. Oeldorf; G. Spanakis","Department of Data Science and Knowledge Engineering, Maastricht University, Maastricht, The Netherlands; Department of Data Science and Knowledge Engineering, Maastricht University, Maastricht, The Netherlands","2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)","17 Feb 2020","2019","","","462","468","Domains such as logo synthesis, in which the data has a high degree of multi-modality, still pose a challenge for generative adversarial networks (GANs). Recent research shows that progressive training (ProGAN) and mapping network extensions (StyleGAN) enable both increased training stability for higher dimensional problems and better feature separation within the embedded latent space. However, these architectures leave limited control over shaping the output of the network. This paper explores a conditional extension to the StyleGAN architecture with the aim of firstly, improving on the low resolution results of previous research and, secondly, increasing the controllability of the output through the use of synthetic class-conditions. Furthermore, methods of extracting such class conditions are explored, where the challenge lies in the fact that, visual logo characteristics are hard to define. The introduced conditional style-based generator architecture is trained on the extracted class-conditions in two experiments and studied relative to the performance of an unconditional model. Results show that, whilst the unconditional model more closely matches the training distribution, high quality conditions enabled the embedding of finer details onto the latent space, leading to more diverse output.","","978-1-7281-4550-1","10.1109/ICMLA.2019.00086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999346","Generative Adversarial Neural Networks;Image Synthesis;Deep Learning","Training;Gallium nitride;Generators;Image resolution;Aerospace electronics;Visualization;Mathematical model","image resolution;neural nets","LoGANv2;conditional style-based logo generation;generative adversarial networks;logo synthesis;mapping network extensions;embedded latent space;conditional extension;StyleGAN architecture;synthetic class-conditions;visual logo characteristics;conditional style-based generator architecture;training distribution;progressive training","","11","","17","IEEE","17 Feb 2020","","","IEEE","IEEE Conferences"
"An Adversarial Learning Approach to Medical Image Synthesis for Lesion Detection","L. Sun; J. Wang; Y. Huang; X. Ding; H. Greenspan; J. Paisley","School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; School of Informatics, Xiamen University, Xiamen, China; Department of Biomedical Engineering, Tel Aviv University, Tel Aviv, Israel; Department of Electrical Engineering, Columbia University, New York, USA","IEEE Journal of Biomedical and Health Informatics","5 Aug 2020","2020","24","8","2303","2314","The identification of lesion within medical image data is necessary for diagnosis, treatment and prognosis. Segmentation and classification approaches are mainly based on supervised learning with well-paired image-level or voxel-level labels. However, labeling the lesion in medical images is laborious requiring highly specialized knowledge. We propose a medical image synthesis model named abnormal-to-normal translation generative adversarial network (ANT-GAN) to generate a normal-looking medical image based on its abnormal-looking counterpart without the need for paired training data. Unlike typical GANs, whose aim is to generate realistic samples with variations, our more restrictive model aims at producing a normal-looking image corresponding to one containing lesions, and thus requires a special design. Being able to provide a “normal” counterpart to a medical image can provide useful side information for medical imaging tasks like lesion segmentation or classification validated by our experiments. In the other aspect, the ANT-GAN model is also capable of producing highly realistic lesion-containing image corresponding to the healthy one, which shows the potential in data augmentation verified in our experiments.","2168-2208","","10.1109/JBHI.2020.2964016","National Natural Science Foundation of China(grant numbers:61571382,81671766,61571005,81671674,61671309,U1605252); Fundamental Research Funds for the Central Universities; 2019 Principal Fund Innovation Team Cultivation Project(grant numbers:20720190116); Natural Science Foundation of Fujian Province(grant numbers:2017J01126); Columbia University(grant numbers:201806310090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950113","Medical image synthesis;generative adversarial network;unsupervised learning","Lesions;Medical diagnostic imaging;Gallium nitride;Magnetic resonance imaging;Generators;Generative adversarial networks","image classification;image segmentation;learning (artificial intelligence);medical image processing","adversarial learning approach;lesion detection;medical image data;well-paired image-level;voxel-level labels;medical image synthesis model;abnormal-to-normal translation generative adversarial network;paired training data;medical imaging tasks;lesion segmentation;ANT-GAN model;lesion classification","Brain;Humans;Image Interpretation, Computer-Assisted;Unsupervised Machine Learning","48","","47","IEEE","6 Jan 2020","","","IEEE","IEEE Journals"
"Improving Human Image Synthesis with Residual Fast Fourier Transformation and Wasserstein Distance","J. Wu; S. Si; J. Wang; J. Xiao","University of Science and Technology of China, Hefei, China; Ping An Technology (Shenzhen) Co., Ltd., Shenzhen, China; Ping An Technology (Shenzhen) Co., Ltd., Shenzhen, China; Ping An Technology (Shenzhen) Co., Ltd., Shenzhen, China","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","8","With the rapid development of the Metaverse, virtual humans have emerged, and human image synthesis and editing techniques, such as pose transfer, have recently become popular. Most of the existing techniques rely on GANs, which can generate good human images even with large variants and occlusions. But from our best knowledge, the existing state-of-the-art method still has the following problems: the first is that the rendering effect of the synthetic image is not realistic, such as poor rendering of some regions. And the second is that the training of GAN is unstable and slow to converge, such as model collapse. Based on the above two problems, we propose several methods to solve them. To improve the rendering effect, we use the Residual Fast Fourier Transform Block to replace the traditional Residual Block. Then, spectral normalization and Wasserstein distance are used to improve the speed and stability of GAN training. Experiments demonstrate that the methods we offer are effective at solving the problems listed above, and we get state-of-the-art scores in LPIPS and PSNR.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892625","human image synthesis;GAN;residual fourier transform;Metaverse","Training;Metaverse;Image color analysis;Fast Fourier transforms;Neural networks;Rendering (computer graphics);Generative adversarial networks","fast Fourier transforms;image classification;image coding;image segmentation;rendering (computer graphics);statistical analysis","occlusions;existing state-of-the-art method;rendering effect;synthetic image;poor rendering;Residual Fast Fourier Transform Block;traditional Residual Block;Wasserstein distance;GAN training;state-of-the-art scores;improving human image synthesis;Residual Fast Fourier transformation;virtual humans;editing techniques;GANs;good human images","","","","39","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Improved ArtGAN for Conditional Synthesis of Natural Image and Artwork","W. R. Tan; C. S. Chan; H. E. Aguirre; K. Tanaka","NA; Universiti Malaya, Kuala Lumpur, Wilayah Persekutuan, MY; NA; NA","IEEE Transactions on Image Processing","23 Sep 2018","2019","28","1","394","409","This paper proposes a series of new approaches to improve generative adversarial network (GAN) for conditional image synthesis and we name the proposed model as “ArtGAN. ” One of the key innovation of ArtGAN is that, the gradient of the loss function w.r.t. the label (randomly assigned to each generated image) is back-propagated from the categorical discriminator to the generator. With the feedback from the label information, the generator is able to learn more efficiently and generate image with better quality. Inspired by recent works, an autoencoder is incorporated into the categorical discriminator for additional complementary information. Last but not least, we introduce a novel strategy to improve the image quality. In the experiments, we evaluate ArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results showed that our proposed model outperforms the state-of-the-art results on CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN is able to generate plausible-looking images on Oxford-102 and CUB-200, as well as able to draw realistic artworks based on style, artist, and genre. The source code and models are available at: https://github.com/cs-chan/ArtGAN.","1941-0042","","10.1109/TIP.2018.2866698","Fundamental Research Grant Scheme (FRGS) MoHE from the Ministry of Education Malaysia(grant numbers:FP004-2016); UM Frontier Research from University of Malaya(grant numbers:FG002-17AFR); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444471","Generative adversarial networks;deep learning;image synthesis;artwork synthesis;ArtGAN","Gallium nitride;Generators;Image generation;Image quality;Generative adversarial networks;Training;Image resolution","art;image processing;learning (artificial intelligence)","ArtGAN;CIFAR-10;conditional synthesis;natural image;generative adversarial network;image quality;image synthesis;STL-10","","19","","65","IEEE","22 Aug 2018","","","IEEE","IEEE Journals"
"Feature axes orthogonalization in semantic face editing","L. Antal; Z. Bodó","Faculty of Mathematics and Computer Science, Babeş –Bolyai University, Cluj-Napoca, Romania; Faculty of Mathematics and Computer Science, Babeş –Bolyai University, Cluj-Napoca, Romania","2021 IEEE 17th International Conference on Intelligent Computer Communication and Processing (ICCP)","16 Mar 2022","2021","","","163","169","Human image synthesis is the technology that allows a computer program to create realistic photos of non-existing people. Though it is a relatively novel research topic that is mostly used to synthesize human faces, generating moving human figures is also possible using this method.At first, composing the believable and realistic images was a complex process. To achieve decent results, photo-realistic modelling, animating and mapping of the soft dynamics of the human body was required. Nowadays these methods are replaced by approaches based on machine learning and neural networks.Our system is able to create realistic images, consisting of three main components. The first component is a Generative Adversarial Network (GAN) that can generate a random face from a noise vector. Secondly, a convolutional neural network is responsible to recognize facial features on the input photos. Lastly, a regression model computes the correspondence between the input noise vector and output features of the generated face.Using a well-known face dataset, we report results applying the newly proposed model and we also analyze the accuracy and the plausibility of these results.","","978-1-6654-0976-6","10.1109/ICCP53602.2021.9733549","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733549","human image synthesis;semantic face editing;composite sketches;Generative Adversarial Networks;least squares;custom loss function;orthogonality","Analytical models;Computational modeling;Face recognition;Semantics;Machine learning;Generative adversarial networks;Convolutional neural networks","computer animation;convolutional neural nets;face recognition;feature extraction;realistic images;regression analysis","semantic face editing;human image synthesis;computer program;realistic photos;human faces;human figures;believable images;realistic images;complex process;decent results;soft dynamics;human body;machine learning;generative adversarial network;random face;convolutional neural network;facial features;input photos;regression model;input noise vector;output features;generated face;well-known face dataset;photo-realistic modelling;photo-realistic animating;photo-realistic mapping","","","","23","IEEE","16 Mar 2022","","","IEEE","IEEE Conferences"
"Knowledge-Driven Generative Adversarial Network for Text-to-Image Synthesis","J. Peng; Y. Zhou; X. Sun; L. Cao; Y. Wu; F. Huang; R. Ji","National Institute for Data Science in Health and Medicine, Xiamen University, Xiamen, China; Media Analytics and Computing Laboratory, Department of Artificial Intelligence, Xiamen University, Xiamen, China; Media Analytics and Computing Laboratory, Department of Artificial Intelligence, Xiamen University, Xiamen, China; Media Analytics and Computing Laboratory, Department of Artificial Intelligence, Xiamen University, Xiamen, China; Tencent Technology (Shanghai) Company, Ltd, Shanghai, China; Tencent Technology (Shanghai) Company, Ltd, Shanghai, China; Institute of Artificial Intelligence, Xiamen University, Xiamen, China","IEEE Transactions on Multimedia","5 Oct 2022","2022","24","","4356","4366","Text-to-Image (T2I) synthesis is a challenging task that aims to convert natural language descriptions to real images. It remains an open problem mainly due to the diversity of text descriptions, which poses a huge obstacle in generating vivid and relevant images. Moreover, the existing evaluation metrics in T2I synthesis are mainly used to evaluate the visual quality of the generated images, while the semantic consistency between the two modalities is often ignored. To address these issues, we present a novel Knowledge-Driven Generative Adversarial Network, termed KD-GAN, and a new evaluation system, named Pseudo Turing Test (PTT for short). Concretely, KD-GAN takes a further step in imitating the behavior of human painting, i.e., drawing an image according to reference knowledge. The introduction of reference knowledge in KD-GAN not only improves the quality of the generated images but also enhances the semantic consistency between them and the input texts. In addition, KD-GAN can also greatly avoid some flaws against common sense during image generation, e.g., skiing in the blue sky. The proposed PTT is an important supplement to the existing evaluation system of T2I synthesis. It includes a set of pseudo-experts of different multimedia tasks to evaluate the semantic consistency between the given texts and the generated images. To validate the proposed KD-GAN, we conducted extensive experiments on two benchmark datasets, i.e., Caltech-UCSD Birds (CUB), and MS-COCO (COCO). The experimental results demonstrate that KD-GAN outperforms state-of-the-art methods on IS, FID, and the proposed PTT metrics.11The codes of KD-GAN are at [Online]. Available: https://github.com/pengjunn/KD-GAN and the codes and models of PTT are at [Online]. Available: https://github.com/pengjunn/PTT.","1941-0077","","10.1109/TMM.2021.3116416","National Science Fund for Distinguished Young Scholars(grant numbers:62025603); Fundamental Research Funds for the central universities(grant numbers:20720200077,20720200090,20720200091); National Natural Science Foundation of China(grant numbers:U1705262,62072386,62072387,62072389,62002305,61772443,61802324,61702136); China Postdoctoral Science Foundation(grant numbers:2021T40397); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B1515120049); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9552559","Text-to-image;knowledge-driven;generative adversarial network;pseudo turing test","Visualization;Generative adversarial networks;Task analysis;Semantics;Measurement;Image synthesis;Feature extraction","computer vision;image classification;learning (artificial intelligence);natural language processing;natural languages;object recognition;text analysis","reference knowledge;semantic consistency;image generation;given texts;text-to-Image synthesis;natural language descriptions;vivid images;existing evaluation metrics;novel Knowledge-Driven Generative Adversarial Network;termed KD-GAN;MS-COCO;T2I synthesis;Pseudo Turing Test;Caltech-UCSD Birds","","2","","42","IEEE","29 Sep 2021","","","IEEE","IEEE Journals"
"Deep Learning Based Approach to Generate Realistic Data for ADAS Applications","R. K. Soni; B. B. Nair","Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India; Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore Amrita Vishwa Vidyapeetham, India","2021 5th International Conference on Computer, Communication and Signal Processing (ICCCSP)","29 Jun 2021","2021","","","1","5","Quantity, quality and diversity of datasets are prerequisites for training the deep-learning autonomous driving models. One major issue identified from the literature is the lack of realistic training data which eventually leads to a less robust model. Simulators can help in dealing with reducing reality gaps, however, commonly available simulators generate data that are far removed from the real world scenarios. The model proposed in this study is based on video-to-video synthesis and image synthesis methods using Generative Adversarial Networks (GANs). The results indicate improved realism. Kanade-Lucas-Tomasi (KLT) and Fr'echet Inception Distance (FID) based temporal coherence evaluation metrics have also been proposed as a possible alternative to human perception driven evaluations.","","978-1-6654-3277-1","10.1109/ICCCSP52374.2021.9465529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465529","Advanced driver assistance systems (ADAS);Generative adversarial network (GAN);Kanade-Lucas-Tomasi (KLT);Fr’echet Inception Distance (FID)","Measurement;Training;Deep learning;Image synthesis;Computational modeling;Training data;Graphics processing units","deep learning (artificial intelligence);image sequences;road traffic;traffic engineering computing;video signal processing","ADAS applications;deep-learning autonomous driving models;video-to-video synthesis;image synthesis;generative adversarial networks;Frechet Inception Distance;Kanade-Lucas-Tomasi;temporal coherence evaluation metrics","","2","","28","IEEE","29 Jun 2021","","","IEEE","IEEE Conferences"
"Generative Adversarial Networks: An Overview","A. Creswell; T. White; V. Dumoulin; K. Arulkumaran; B. Sengupta; A. A. Bharath","Imperial College London, London, London, GB; School of Design, University of Wellington, Victoria, New Zealand; Universite de Montreal, Montreal, QC, CA; Imperial College London, London, London, GB; Imperial College London, London, London, GB; Imperial College London, London, London, GB","IEEE Signal Processing Magazine","10 Jan 2018","2018","35","1","53","65","Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this by deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image superresolution, and classification. The aim of this review article is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.","1558-0792","","10.1109/MSP.2017.2765202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253599","","Machine learning;Generators;Training data;Data models;Convolutional codes;Image resolution;Signal resolution;Semantics","backpropagation;image classification;image resolution","image classification;image superresolution;style transfer;semantic image editing;image synthesis;signal processing community;backpropagation signals;GAN;generative adversarial networks","","1000","","","IEEE","10 Jan 2018","","","IEEE","IEEE Magazines"
"Red Blood Cell Image Generation for Data Augmentation Using Conditional Generative Adversarial Networks","O. Bailo; D. Ham; Y. M. Shin",Noul Inc.; Noul Inc.; Noul Inc.,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","9 Apr 2020","2019","","","1039","1048","In this paper, we describe how to apply image-to-image translation techniques to medical blood smear data to generate new data samples and meaningfully increase small datasets. Specifically, given the segmentation mask of the microscopy image, we are able to generate photorealistic images of blood cells which are further used alongside real data during the network training for segmentation and object detection tasks. This image data generation approach is based on conditional generative adversarial networks which have proven capabilities to high-quality image synthesis. In addition to synthesizing blood images, we synthesize segmentation mask as well which leads to a diverse variety of generated samples. The effectiveness of the technique is thoroughly analyzed and quantified through a number of experiments on a manually collected and annotated dataset of blood smear taken under a microscope.","2160-7516","978-1-7281-2506-0","10.1109/CVPRW.2019.00136","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025635","","Blood;Image segmentation;Cells (biology);Shape;Biomedical imaging;Gallium nitride;Databases","blood;cellular biophysics;image segmentation;image texture;medical image processing;object detection;rendering (computer graphics);statistical analysis","photorealistic images;blood cells;network training;object detection tasks;image data generation approach;conditional generative adversarial networks;high-quality image synthesis;blood images;segmentation mask;red blood cell image generation;data augmentation;image-to-image translation techniques;medical blood smear data;data samples;microscopy image","","19","","39","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Methods and Datasets of Text to Image Synthesis Based on Generative Adversarial Network","J. Li; T. Sun; Z. Yang; Z. Yuan","Xi'an University of Architecture and Technology, Xi'an, China; Beijing University of Technology, Beijing, China; The Ohio State University, Ohio, United States; Beijing University of Technology, Beijing, China","2022 IEEE 5th International Conference on Information Systems and Computer Aided Education (ICISCAE)","4 Nov 2022","2022","","","843","847","Text-to-Image (T2I) synthesis refers to the computational method of translating normal human text description into images which have alike semantic meaning as text with the use of keywords or sentences. T2I has achieved great success in a few areas, especially in generating vivid, realistic visual and photographic images. Recent years, the development of deep learning has brought some new methods for unsupervised deep learning area, which provide some models to generate visually natural images with suitably trained neural network models. However, T2I now still faces some challenges. For example, current T2I models are unable to generate high resolution images with multiple objects. And it can be difficult to reproduce the results of many approaches. Since present methods are mostly based on GAN (Generative Adversarial Network) models, this paper will focus on the methods depending on it and systematically clear up the developments of T2I based on GAN. Also, datasets always play a supporting role in task development, several current T2I-related datasets will be discussed in the paper to explain more about T2I's development. A brief discussion of the future work and challenges will be showed at the end of the paper.","2770-663X","978-1-6654-8122-9","10.1109/ICISCAE55891.2022.9927634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9927634","Text-to-Image;GAN;synthesis;Neural Network","Deep learning;Visualization;Image resolution;Computational modeling;Neural networks;Semantics;Generative adversarial networks","learning (artificial intelligence);natural language processing;neural nets;text analysis;unsupervised learning","Generative Adversarial Network;text-to-Image;computational method;normal human text description;alike semantic meaning;vivid images;realistic visual images;photographic images;unsupervised deep learning area;visually natural images;suitably trained neural network models;high resolution images;GAN models;task development;T2I-related datasets","","","","10","IEEE","4 Nov 2022","","","IEEE","IEEE Conferences"
"Joint Expression Synthesis and Representation Learning for Facial Expression Recognition","X. Zhang; F. Zhang; C. Xu","School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Peng Cheng Laboratory, Shenzhen, China","IEEE Transactions on Circuits and Systems for Video Technology","8 Mar 2022","2022","32","3","1681","1695","Facial expression recognition (FER) is a challenging task due to the large appearance variations and the lack of sufficient training data. Conventional deep approaches either learn a good representation through deep models or synthesize images automatically to enlarge the training set. In this paper, we perform both tasks jointly and propose an end-to-end deep model for simultaneous facial expression recognition and facial image synthesis. The proposed model is based on Generative Adversarial Network (GAN) and enjoys several merits. First, the facial image synthesis and facial expression recognition tasks can boost their performance for each other via the unified model. Second, paired images are not required in our facial image synthesis network, which makes the proposed model much more general and flexible. Meanwhile, the generated facial images largely expand the training set and ease the overfitting problem in our FER task. Third, different expressions are encoded in a disentangled manner in a latent space, which enables us to synthesize facial images with arbitrary expressions by exchanging certain parts of their latent identity features. Quantitative and qualitative evaluations on both controlled and in-the-wild FER benchmarks (Multi-PIE, MMI, and RAF-DB) demonstrate the effectiveness of our proposed method on both facial image synthesis and facial expression recognition task.","1558-2205","","10.1109/TCSVT.2021.3056098","National Key Research and Development Program of China(grant numbers:2017YFB1002804); National Natural Science Foundation of China(grant numbers:61720106006,61721004,61832002,61532009,62002355,U1705262,U1836220,61702511,61672267,61751211); Key Research Program of Frontier Sciences, CAS(grant numbers:QYZDJ-SSW-JSC039); National Postdoctoral Program for Innovative Talents(grant numbers:BX20190367); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343871","Facial expression recognition;facial image synthesis;generative adversarial network;representation learning","Face recognition;Task analysis;Generative adversarial networks;Image synthesis;Image recognition;Faces;Training","emotion recognition;face recognition;learning (artificial intelligence);neural nets","arbitrary expressions;different expressions;FER task;generated facial images;facial image synthesis network;paired images;facial expression recognition task;simultaneous facial expression recognition;end-to-end deep model;training set;synthesize images;deep models;conventional deep approaches;representation learning;joint expression synthesis","","7","","69","IEEE","1 Feb 2021","","","IEEE","IEEE Journals"
"EEG data augmentation for Personal Identification Using SF-GAN","S. Zhang; X. Mao; L. Sun; Y. Yang","Information Engineering University, Zhengzhou, China; Information Engineering University, Zhengzhou, China; Information Engineering University, Zhengzhou, China; Information Engineering University, Zhengzhou, China","2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)","18 Jul 2022","2022","","","1","6","Because EEG-based identity requires a large amount of training data when training a classification model, and the collection of EEG signals requires a lot of time and effort. Therefore, we hope to perform data augmentation on the EEG data used for identity. Generative adversarial networks have achieved great success in image generation, but the raw EEG signals are not in the form of images. Therefore, we process the EEG signal into an EEG topomap with stronger spatial feature representation, and use a spatial feature-based generative adversarial network image augmentation method (SF-GAN). To verify the generality of our proposed method, we use real EEG topomap samples processed from two different EEG datasets, BCI Competition IV 1 and BCI Competition IV 2a, to train SF-GAN to generate augmented samples for training identity classification model. The proposed method can use smaller real samples to expand the training set of identity, reduce the data dependence on real samples, and reduce the time of data collection to a certain extent. And it is proved by experiments that the data generated by this method can further improve the training effect of the classification model.","","978-1-6654-5911-2","10.1109/CVIDLICCEA56201.2022.9824276","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9824276","Identification;EEG topomap;SF-GAN","Training;Image synthesis;Training data;Data collection;Feature extraction;Brain modeling;Generative adversarial networks","brain-computer interfaces;electroencephalography;feature extraction;image classification;medical image processing","training identity classification model;training set;data dependence;data collection;training effect;EEG data augmentation;personal identification;SF-GAN;EEG-based identity;training data;EEG signal;generative adversarial networks;image generation;raw EEG signals;stronger spatial feature representation;spatial feature-based generative adversarial network image augmentation method;topomap samples;augmented samples","","1","","13","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"Text to Video GANs:TFGAN, IRC-GAN, BoGAN","R. Mehmood; R. Bashir; K. J. Giri","Department of Computer Science, Islamic University of Science &Technology, J&K, India; Department of Computer Science, Islamic University of Science &Technology, J&K, India; Department of Computer Science, Islamic University of Science &Technology, J&K, India","2022 8th International Conference on Advanced Computing and Communication Systems (ICACCS)","7 Jun 2022","2022","1","","1234","1239","Generative adversarial networks (GANs) have demonstrated high accuracy on image generation tasks. A large number of studies have applied image generating models to video generation as well. However, because of the complexities of video generation, it's not that trivial to use GANs in the video domain. In the video generation, the resulting content has to be spatially and temporally coherent. Moreover, generating videos from text is even more challenging since besides maintaining the temporal and spatial coherence, semantic consistency also needs to be maintained. In this paper, we have compared three recently proposed text-to-video GAN architectures. Text-Filter Conditioning Generative Adversarial Network (TFGAN) is the first architecture, which employs a superior feature fusion method in which firstly the discriminative convolutional filters are produced from text features and then convolved with image features in the discriminator. The Introspective Recurrent Convolutional GAN (IRC-GAN) is the second architecture, which leverages mutual-information introspection to maintain semantic consistency between the generated videos and the input text. The third model is Bottom-up GAN (BoGAN) which introduces three levels of losses viz, region level loss, frame-level loss, and video level loss.","2575-7288","978-1-6654-0816-5","10.1109/ICACCS54159.2022.9785103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785103","GAN;TFGAN;IRC-GAN;BoGAN;computation power;resolution","Three-dimensional displays;Image synthesis;Semantics;Spatial coherence;Deep architecture;Generative adversarial networks;Information filters","convolution;feature extraction;image classification;image fusion;image texture;text analysis;video signal processing","generated videos;Generative adversarial networks;image generation tasks;Introspective Recurrent Convolutional GAN;IRC-GAN;spatial coherence;temporal coherence;text features;Text-Filter Conditioning Generative Adversarial Network;text-to-video GAN architectures;video domain;Video GANs:TFGAN;video generation;video level loss","","","","19","IEEE","7 Jun 2022","","","IEEE","IEEE Conferences"
"CMOS-GAN: Semi-Supervised Generative Adversarial Model for Cross-Modality Face Image Synthesis","S. Yu; H. Han; S. Shan; X. Chen","University of Chinese Academy of Sciences, Beijing, China; Peng Cheng Laboratory, Shenzhen, China; Peng Cheng Laboratory, Shenzhen, China; University of Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","19 Dec 2022","2023","32","","144","158","Cross-modality face image synthesis such as sketch-to-photo, NIR-to-RGB, and RGB-to-depth has wide applications in face recognition, face animation, and digital entertainment. Conventional cross-modality synthesis methods usually require paired training data, i.e., each subject has images of both modalities. However, paired data can be difficult to acquire, while unpaired data commonly exist. In this paper, we propose a novel semi-supervised cross-modality synthesis method (namely CMOS-GAN), which can leverage both paired and unpaired face images to learn a robust cross-modality synthesis model. Specifically, CMOS-GAN uses a generator of encoder-decoder architecture for new modality synthesis. We leverage pixel-wise loss, adversarial loss, classification loss, and face feature loss to exploit the information from both paired multi-modality face images and unpaired face images for model learning. In addition, since we expect the synthetic new modality can also be helpful for improving face recognition accuracy, we further use a modified triplet loss to retain the discriminative features of the subject in the synthetic modality. Experiments on three cross-modality face synthesis tasks (NIR-to-VIS, RGB-to-depth, and sketch-to-photo) show the effectiveness of the proposed approach compared with the state-of-the-art. In addition, we also collect a large-scale RGB-D dataset (VIPL-MumoFace-3K) for the RGB-to-depth synthesis task. We plan to open-source our code and VIPL-MumoFace-3K dataset to the community (https://github.com/skgyu/CMOS-GAN).","1941-0042","","10.1109/TIP.2022.3226413","National Key Research and Development Program of China(grant numbers:2017YFA0700804); National Natural Science Foundation of China(grant numbers:61732004,62176249); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9975261","Cross-modality synthesis;semi-supervised synthesis;cross-modality face recognition;generative adversarial networks","Face recognition;Task analysis;Probes;Image synthesis;Information processing;Computers;Training","face recognition;learning (artificial intelligence);neural nets","CMOS-GAN;conventional cross-modality synthesis methods;cross-modality face image synthesis;cross-modality face synthesis tasks;face animation;face feature loss;face recognition accuracy;novel semisupervised cross-modality synthesis method;paired multimodality face images;paired training data;RGB-to-depth synthesis task;robust cross-modality synthesis model;semisupervised generative adversarial model;synthetic modality;synthetic new modality;unpaired face images","","","","73","IEEE","7 Dec 2022","","","IEEE","IEEE Journals"
"GAN Compression: Efficient Architectures for Interactive Conditional GANs","M. Li; J. Lin; Y. Ding; Z. Liu; J. -Y. Zhu; S. Han",Shanghai Jiao Tong University; Massachusetts Institute of Technology; Shanghai Jiao Tong University; Massachusetts Institute of Technology; Adobe Research; Massachusetts Institute of Technology,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","5283","5293","Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many computer vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more computationally-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. Directly applying existing CNNs compression methods yields poor performance due to the difficulty of GAN training and the differences in generator architectures. We address these challenges in two ways. First, to stabilize the GAN training, we transfer knowledge of multiple intermediate representations of the original model to its compressed model, and unify unpaired and paired learning. Second, instead of reusing existing CNN designs, our method automatically finds efficient architectures via neural architecture search (NAS). To accelerate the search process, we decouple the model training and architecture search via weight sharing. Experiments demonstrate the effectiveness of our method across different supervision settings (paired and unpaired), model architectures, and learning methods (e.g., pix2pix, GauGAN, CycleGAN). Without losing image quality, we reduce the computation of CycleGAN by more than 20x and GauGAN by 9x, paving the way for interactive image synthesis. The code and demo are publicly available.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157494","","Gallium nitride;Generators;Training;Computational modeling;Computer architecture;Generative adversarial networks;Image coding","computer vision;data compression;inference mechanisms;learning (artificial intelligence);neural nets;search problems","GAN compression;conditional generative adversarial networks;controllable image synthesis;computer vision;graphics applications;GauGAN;general-purpose compression framework;generator architectures;transfer knowledge;multiple intermediate representations;unpaired learning;neural architecture search;search process;model training;model architectures;image quality;interactive image synthesis;interactive conditional GANs;MobileNet-v3;CNNs compression methods;paired learning;CycleGAN","","64","","71","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"GAN Compression: Efficient Architectures for Interactive Conditional GANs","M. Li; J. Lin; Y. Ding; Z. Liu; J. -Y. Zhu; S. Han","Carnegie Mellon University, Pittsburgh, PA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; University of Toronto, Toronto, ON, Canada; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","7 Nov 2022","2022","44","12","9331","9346","Conditional Generative Adversarial Networks (cGANs) have enabled controllable image synthesis for many vision and graphics applications. However, recent cGANs are 1-2 orders of magnitude more compute-intensive than modern recognition CNNs. For example, GauGAN consumes 281G MACs per image, compared to 0.44G MACs for MobileNet-v3, making it difficult for interactive deployment. In this work, we propose a general-purpose compression framework for reducing the inference time and model size of the generator in cGANs. Directly applying existing compression methods yields poor performance due to the difficulty of GAN training and the differences in generator architectures. We address these challenges in two ways. First, to stabilize GAN training, we transfer knowledge of multiple intermediate representations of the original model to its compressed model and unify unpaired and paired learning. Second, instead of reusing existing CNN designs, our method finds efficient architectures via neural architecture search. To accelerate the search process, we decouple the model training and search via weight sharing. Experiments demonstrate the effectiveness of our method across different supervision settings, network architectures, and learning methods. Without losing image quality, we reduce the computation of CycleGAN by 21×, Pix2pix by 12×, MUNIT by 29×, and GauGAN by 9×, paving the way for interactive image synthesis.","1939-3539","","10.1109/TPAMI.2021.3126742","National Science Foundation; MIT-IBM Watson AI Lab; Adobe; Samsung, for supporting this research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609572","GAN compression;GAN;compression;image-to-image translation;distillation;neural architecture search","Training;Computational modeling;Generative adversarial networks;Computer architecture;Generators;Image coding;Image edge detection","convolutional neural nets;data compression;image coding;image representation;learning (artificial intelligence);search problems","CNN designs;compressed model;conditional generative adversarial networks;controllable image synthesis;different supervision settings;existing compression methods yields poor performance;GAN compression;GAN training;GauGAN;general-purpose compression framework;generator architectures;graphics applications;image quality;inference time;interactive conditional;interactive deployment;interactive image synthesis;learning methods;model training;multiple intermediate representations;network architectures;neural architecture search;Pix2pix;recent cGAN;recent cGAN;search process;transfer knowledge;unify unpaired learning","","2","","85","IEEE","9 Nov 2021","","","IEEE","IEEE Journals"
"Efficient Geometry-aware 3D Generative Adversarial Networks","E. R. Chan; C. Z. Lin; M. A. Chan; K. Nagano; B. Pan; S. de Mello; O. Gallo; L. Guibas; J. Tremblay; S. Khamis; T. Karras; G. Wetzstein",NVIDIA; Stanford University; Stanford University; NVIDIA; Stanford University; NVIDIA; NVIDIA; Stanford University; NVIDIA; NVIDIA; NVIDIA; Stanford University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","16102","16112","Unsupervised generation of high-quality multi-view-consistent images and 3D shapes using only collections of single-view 2D photographs has been a long-standing challenge. Existing 3D GANs are either compute intensive or make approximations that are not 3D-consistent; the former limits quality and resolution of the generated images and the latter adversely affects multi-view consistency and shape quality. In this work, we improve the computational efficiency and image quality of 3D GANs without overly relying on these approximations. We introduce an expressive hybrid explicit implicit network architecture that, together with other design choices, synthesizes not only high-resolution multi-view-consistent images in real time but also produces high-quality 3D geometry. By decoupling feature generation and neural rendering, our framework is able to leverage state-of-the-art 2D CNN generators, such as StyleGAN2, and inherit their efficiency and expressiveness. We demonstrate state-of-the-art 3D-aware synthesis with FFHQ and AFHQ Cats, among other experiments.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880428","Representation learning; Face and gestures; Vision + graphics","Solid modeling;Three-dimensional displays;Shape;Image synthesis;Network architecture;Rendering (computer graphics);Rapid prototyping","convolutional neural nets;image representation;image resolution;rendering (computer graphics)","computational efficiency;image quality;3D GANs;high-resolution multiview-consistent images;high-quality 3D geometry;feature generation;state-of-the-art 3D-aware synthesis;unsupervised generation;high-quality multiview-consistent images;single-view 2D photographs;shape quality;geometry-aware 3D generative adversarial networks;hybrid explicit implicit network architecture;2D CNN generators;neural rendering;StyleGAN2;AFHQ Cats;FFHQ Cats","","33","","74","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Styleformer: Transformer based Generative Adversarial Networks with Style Vector","J. Park; Y. Kim",mAy-I Inc.; MINDsLab Inc.,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","8973","8982","We propose Styleformer, a generator that synthesizes image using style vectors based on the Transformer structure. In this paper, we effectively apply the modified Transformer structure (e.g., Increased multi-head attention and Prelayer normalization) and introduce novel Attention Style Injection module which is style modulation and demodulation method for self-attention operation. The new generator components have strengths in CNN's shortcomings, handling long-range dependency and understanding global structure of objects. We present two methods to generate high-resolution images using Styleformer. First, we apply Linformer in the field of visual synthesis (Styleformer-L), enabling Styleformer to generate higher resolution images and result in improvements in terms of computation cost and performance. This is the first case using Linformer to image generation. Second, we combine Styleformer and Style-GAN2 (Styleformer-C) to generate high-resolution compositional scene efficiently, which Styleformer captures long-range dependencies between components. With these adaptations, Styleformer achieves comparable performances to state-of-the-art in both single and multi-object datasets. Furthermore, groundbreaking results from style mixing and attention map visualization demonstrate the advantages and efficiency of our model.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879491","Computer vision theory; Deep learning architectures and techniques; Machine learning; Vision + X","Computer vision;Visualization;Costs;Image resolution;Image synthesis;Computational modeling;Media","data visualisation;demodulation;feature extraction;image classification;image reconstruction;image resolution;image sampling","synthesized image;style vectors;modified transformer structure;style modulation;demodulation method;generator components;long-range dependency;high-resolution images;Styleformer-L;image generation;Style-GAN2;style mixing;attention map visualization;attention style injection module;transformer based generative adversarial networks;single-object dataset;multiobject dataset","","4","","60","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"GenLoc: A New Paradigm for Signal Fingerprinting with Generative Adversarial Networks","R. Guan; Y. Zhang; M. Li","Riemann Laboratory, 2012 Laboratories, Huawei; Riemann Laboratory, 2012 Laboratories, Huawei; Riemann Laboratory, 2012 Laboratories, Huawei","2022 IEEE 12th International Conference on Indoor Positioning and Indoor Navigation (IPIN)","26 Oct 2022","2022","","","1","8","Predicting signals propagated indoors is the key to radio map building for indoor positioning. Conventional models allow limited learning from well-surveyed fingerprints and can not transfer to new areas. Inspired by recent advances in computer vision, this work frames the radio map building problem as an image generation problem and explores how to learn signal propagation from a massive amount of fingerprints collected from multiple floors of multiple buildings. We demonstrate a generative framework we call GenLoc to model signal propagation implicitly in a data-driven way. Ultimately, GenLoc provides a generalised model that can generate radio maps for new buildings readily. Meanwhile, GenLoc can optionally incorporate floor plan information to condition the generation of a radio map. Evaluation across multiple buildings shows that, compared to conventional methods, GenLoc achieves a decisively improved (15%) positioning accuracy at a lower generation cost.","2471-917X","978-1-7281-6218-8","10.1109/IPIN54987.2022.9918126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918126","Generative Adversarial Network (GAN);Signal Fingerprinting;Indoor Positioning","Wireless communication;Image synthesis;Fuses;Computational modeling;Indoor navigation;Buildings;Fingerprint recognition","fingerprint identification;geophysics computing;indoor radio;mobile robots;radiowave propagation","signal propagation;GenLoc;generalised model;floor plan information;multiple buildings;decisively improved positioning accuracy;lower generation cost;signal fingerprinting;generative adversarial networks;predicting signals;indoor positioning;well-surveyed fingerprints;computer vision;work frames;radio map building problem;image generation problem;multiple floors;generative framework","","","","30","IEEE","26 Oct 2022","","","IEEE","IEEE Conferences"
"Face Synthesis With a Focus on Facial Attributes Translation Using Attention Mechanisms","R. Li; T. Fontanini; A. Prati; B. Bhanu","Department of Computer Science, University of California at Riverside, Riverside, CA, USA; Department of Engineering and Architecture, University of Parma, Parma, Italy; Department of Engineering and Architecture, University of Parma, Parma, Italy; Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA, USA","IEEE Transactions on Biometrics, Behavior, and Identity Science","23 Dec 2022","2023","5","1","76","90","Synthesis of face images by translating facial attributes is an important problem in computer vision and biometrics and has a wide range of applications in forensics, entertainment, etc. Recent advances in deep generative networks have made progress in synthesizing face images with certain target facial attributes. However, visualizing and interpreting generative adversarial networks (GANs) is a relatively unexplored area and generative models are still being employed as black-box tools. This paper takes the first step to visually interpret conditional GANs for facial attribute translation by using a gradient-based attention mechanism. Next, a key innovation is to include new learning objectives for knowledge distillation using attention in generative adversarial training, which result in improved synthesized face results, reduced visual confusions and boosted training for GANs in a positive way. Firstly, visual attentions are calculated to provide interpretations for GANs. Secondly, gradient-based visual attentions are used as knowledge to be distilled in a teacher-student paradigm for face synthesis with focus on facial attributes translation tasks in order to improve the performance of the model. Finally, it is shown how “pseudo”-attentions knowledge distillation can be employed during the training of face synthesis networks when teacher and student networks are trained to generate different facial attributes. The approach is validated on facial attribute translation and human expression synthesis with both qualitative and quantitative results being presented.","2637-6407","","10.1109/TBIOM.2022.3199707","Bourns Endowment Funds of University of California at Riverside; Programme “FIL-Quota Incentivante” of University of Parma; Fondazione Cariparma; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9866115","Facial attributes translation;face image synthesis;visual attention maps;explainable AI;generative adversarial network;deep learning","Facial features;Face recognition;Visualization;Faces;Task analysis;Training;Generative adversarial networks","computer vision;face recognition;learning (artificial intelligence)","attention mechanisms;attentions knowledge distillation;deep generative networks;different facial attributes;face images;face synthesis networks;facial attribute translation;facial attributes translation tasks;GANs;generative adversarial networks;generative adversarial training;generative models;gradient-based attention mechanism;gradient-based visual attentions;improved synthesized face results;reduced visual confusions;target facial","","","","54","IEEE","24 Aug 2022","","","IEEE","IEEE Journals"
"DETGAN: GAN for Arbitrary-oriented Object Detection in Remote Sensing Images","S. Cheng; P. Yao; K. Deng; L. Fu","Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology Chinese Academy of Sciences, Beijing, China","2022 Asia Conference on Algorithms, Computing and Machine Learning (CACML)","19 Aug 2022","2022","","","337","341","Object detection in remote sensing images has be-come a research focus in recent years with the development of deep learning. However, due to objective reasons such as weather, cost, etc., we can hardly obtain abundant high-quality remote sensing images, especially for specific targets, which severely limits the training of the object detector, leading to poor detection performance. Thus for the first time, this paper introduces the Generative Adversarial Networks(GANs) for arbitrary-oriented object detection in remote sensing images, by augmenting the dataset to improve the performance of detectors. We construct DETGAN with two-layer self-attention modules to capture long-distance dependence for high-quality image generation. To solve the mismatch between generated slices and the samples for detectors, we propose the GAN-to-Detection transfer strategy, in which the slices are inserted into a background with the same size as the samples for detectors and then added to the training set. Experiments show that the performance of ship detectors is successfully improved with the transfer strategy, and demonstrate that GAN is an effective way to alleviate the problem of data insufficiency in remote sensing image object detection.","","978-1-6654-8290-5","10.1109/CACML55074.2022.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9852535","Generative Adversarial Network;remote sensing;arbitrary-oriented object detection;transfer strategy","Training;Machine learning algorithms;Image synthesis;Detectors;Object detection;Generative adversarial networks;Sensors","geophysical image processing;learning (artificial intelligence);object detection;remote sensing;ships","GAN-to-Detection transfer strategy;remote sensing image object detection;arbitrary-oriented object detection;objective reasons;high-quality remote sensing images;object detector;poor detection performance;high-quality image generation","","","","25","IEEE","19 Aug 2022","","","IEEE","IEEE Conferences"
"BachGAN: High-Resolution Image Synthesis From Salient Object Layout","Y. Li; Y. Cheng; Z. Gan; L. Yu; L. Wang; J. Liu",University of Central Florida; Microsoft Dynamics 365 AI Research; Microsoft Dynamics 365 AI Research; Microsoft Dynamics 365 AI Research; University of Central Florida; Microsoft Dynamics 365 AI Research,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","8362","8371","We propose a new task towards more practical applications for image generation - high-quality image synthesis from salient object layout. This new setting requires users to provide only the layout of salient objects (i.e., foreground bounding boxes and categories) and lets the model complete the drawing with an invented background and a matching foreground. Two main challenges spring from this new task: (i) how to generate fine-grained details and realistic textures without segmentation map input; and (ii) how to create and weave a background into standalone objects in a seamless way. To tackle this, we propose Background Hallucination Generative Adversarial Network (BachGAN), which leverages a background retrieval module to first select a set of segmentation maps from a large candidate pool, then encodes these candidate layouts via a background fusion module to hallucinate a suitable background for the given objects. By generating the hallucinated background representation dynamically, our model can synthesize high-resolution images with both photo-realistic foreground and integral background. Experiments on Cityscapes and ADE20K datasets demonstrate the advantage of BachGAN over existing approaches, measured on both visual fidelity of generated images and visual alignment between output images and input layouts.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157573","","Layout;Image segmentation;Semantics;Image generation;Task analysis;Gallium nitride;Generative adversarial networks","image motion analysis;image resolution;image segmentation;image texture;object detection;video signal processing","BachGAN;high-resolution image synthesis;salient object layout;image generation;invented background;matching foreground;fine-grained details;segmentation map input;standalone objects;background retrieval module;segmentation maps;background fusion module;hallucinated background representation;high-resolution images;photo-realistic foreground background;integral background;input layouts;high-quality image synthesis;background hallucination generative adversarial network;Cityscapes dataset;ADE20K dataset","","17","","42","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Inspirational Adversarial Image Generation","B. Rozière; M. Riviere; O. Teytaud; J. Rapin; Y. LeCun; C. Couprie","Facebook AI Research, Paris, France; Facebook AI Research, Paris, France; Facebook AI Research, Paris, France; Facebook AI Research, Paris, France; Facebook Research, New York, NY, USA; Facebook AI Research, Paris, France","IEEE Transactions on Image Processing","7 Apr 2021","2021","30","","4036","4045","The task of image generation started receiving some attention from artists and designers, providing inspiration for new creations. However, exploiting the results of deep generative models such as Generative Adversarial Networks can be long and tedious given the lack of existing tools. In this work, we propose a simple strategy to inspire creators with new generations learned from a dataset of their choice, while providing some control over the output. We design a simple optimization method to find the optimal latent parameters corresponding to the closest generation to any input inspirational image. Specifically, we allow the generation given an inspirational image of the user’s choosing by performing several optimization steps to recover optimal parameters from the model’s latent space. We tested several exploration methods from classical gradient descents to gradient-free optimizers. Many gradient-free optimizers just need comparisons (better/worse than another image), so they can even be used without numerical criterion nor inspirational image, only with human preferences. Thus, by iterating on one’s preferences we can make robust facial composite or fashion generation algorithms. Our results on four datasets of faces, fashion images, and textures show that satisfactory images are effectively retrieved in most cases.","1941-0042","","10.1109/TIP.2021.3065845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381602","Optimization;generative adversarial networks;similarity search","Training data;Generative adversarial networks;Image synthesis;Face recognition;Computational modeling","","","","5","","61","IEEE","18 Mar 2021","","","IEEE","IEEE Journals"
"Infrared Image Generation By Pix2pix Based on Multi-receptive Field Feature Fusion","Y. Ma; Y. Hua; Z. Zuo","School of Artificial Intelligence and Automation, Huazhong University of Science & Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science & Technology, Wuhan, China; School of Artificial Intelligence and Automation, Huazhong University of Science & Technology, Wuhan, China","2021 International Conference on Control, Automation and Information Sciences (ICCAIS)","9 Dec 2021","2021","","","1029","1036","Infrared imaging has the advantages of strong anti-interference capability, long-range imaging, and night imaging, and has important applications in both civilian and military fields. In the development of infrared-related equipment, a large number of infrared images under a variety of conditions are required as verification test data. The field test of infrared images requires huge manpower and material resources, and it is difficult to obtain full-time infrared images. To address the problem of insufficient infrared image samples, the paper introduces generative adversarial networks into the infrared image generation task and investigates the infrared image generation method based on visible images by applying Pix2pix networks to paired visible infrared image datasets. To address the problem of missing detailed information of infrared images generated by the Pix2pix network, the paper proposes a Pix2pix network based on multi-receptive field feature fusion and constructs a multi-receptive field feature extractor based on Unet++ structure; the multi-receptive field feature fusion mechanism of nested pixel level by level is proposed. Experiments show that the Pix2pix network based on multi-receptive field feature fusion achieves finer infrared texture generation.","2475-7896","978-1-6654-4029-5","10.1109/ICCAIS52680.2021.9624500","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9624500","Generative Adversarial Networks;infrared image generation;multi-receptive-field feature fusion","Automation;Image synthesis;Infrared imaging;Feature extraction;Generative adversarial networks;Data mining;Task analysis","feature extraction;image fusion;image sensors;image texture;infrared imaging;object detection","multireceptive field feature fusion;Pix2pix network;infrared imaging;long-range imaging;night imaging;infrared-related equipment;full-time infrared images;insufficient infrared image samples;infrared image generation task;infrared image generation method;visible images;paired visible infrared image datasets","","","","12","IEEE","9 Dec 2021","","","IEEE","IEEE Conferences"
"Retro-Remote Sensing: Generating Images From Ancient Texts","M. B. Bejiga; F. Melgani; A. Vascotto","Department of Computer Science and Information Engineering, University of Trento, Trento, Italy; Department of Computer Science and Information Engineering, University of Trento, Trento, Italy; Department of Computer Science and Information Engineering, University of Trento, Trento, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","27 Mar 2019","2019","12","3","950","960","The data available in the world come in various modalities, such as audio, text, image, and video. Each data modality has different statistical properties. Understanding each modality, individually, and the relationship between the modalities is vital for a better understanding of the environment surrounding us. Multimodal learning models allow us to process and extract useful information from multimodal sources. For instance, image captioning and text-to-image synthesis are examples of multimodal learning, which require mapping between texts and images. In this paper, we introduce a research area that has never been explored by the remote sensing community, namely the synthesis of remote sensing images from text descriptions. More specifically, in this paper, we focus on exploiting ancient text descriptions of geographical areas, inherited from previous civilizations, to generate equivalent remote sensing images. From a methodological perspective, we propose to rely on generative adversarial networks (GANs) to convert the text descriptions into equivalent pixel values. GANs are a recently proposed class of generative models that formulate learning the distribution of a given dataset as an adversarial competition between two networks. The learned distribution is represented using the weights of a deep neural network and can be used to generate more samples. To fulfill the purpose of this paper, we collected satellite images and ancient texts to train the network. We present the interesting results obtained and propose various future research paths that we believe are important to further develop this new research area.","2151-1535","","10.1109/JSTARS.2019.2895693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8660422","Convolutional neural networks (CNN);deep learning;generative adversarial networks (GAN);multimodal learning;remote sensing;text-to-image synthesis","Remote sensing;Gallium nitride;Earth;Sensors;Generators;Satellites;Technological innovation","convolutional neural nets;geophysical image processing;image retrieval;learning (artificial intelligence);remote sensing;statistical analysis;text analysis","retro-remote sensing;ancient texts;data modality;multimodal learning models;text-to-image synthesis;ancient text descriptions;generative adversarial networks;GANs;deep neural network;satellite images;image generation;statistical properties;image captioning;information extraction","","11","","34","IEEE","5 Mar 2019","","","IEEE","IEEE Journals"
"Arterial Spin Labeling Image Synthesis From Structural MRI Using Improved Capsule-Based Networks","W. Huang; M. Luo; X. Liu; P. Zhang; H. Ding","Informatization Office, Nanchang University, Nanchang, China; Lab of Medical UltraSound Image Computing, MUSIC, School of Biomedical Engineering, Shenzhen University, Shenzhen, China; Department of Computer Science, School of Information Engineering, Nanchang University, Nanchang, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Lab of Medical UltraSound Image Computing, MUSIC, School of Biomedical Engineering, Shenzhen University, Shenzhen, China","IEEE Access","12 Oct 2020","2020","8","","181137","181153","Medical image synthesis receives much popularity in recent years, and ample medical images can be synthesized by diverse deep learning models to alleviate the problem of lack of data in many medical imaging utilizations. However, most medical image synthesis methods still incorporate the well-known pooling operation in their convolutional neural networks-based / generative adversarial networks-based models, from which image details will be inevitably lost due to the pooling operation. In order to tackle the above problem, improved capsule-based networks, in which no pooling operation is executed and spatial details of images can be effectively preserved thanks to the equivariance characteristics of capsule models, are proposed in this paper to synthesize arterial spin labeling images, for the first time. Technically, three important issues in constructing improved capsule-based networks, including the depth of basic convolutions, the layer of capsules, and the capacity of capsules, are thoroughly investigated. Comprehensive experiments made up of region-based / voxel-based partial volume corrections and dementia diseases diagnosis based on two different datasets are conducted. The superiority of improved capsule-based networks introduced in this paper is substantiated from the statistical point of view.","2169-3536","","10.1109/ACCESS.2020.3028113","National Natural Science Foundation of China(grant numbers:61862043,61971352); Natural Science Foundation of Jiangxi Province(grant numbers:S2020RCDT2K0033); Natural Science Foundation of Shaanxi Province(grant numbers:2018JM6015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210597","Image analysis;image generation;computer aided diagnosis;capsule;arterial spin labeling;deep learning","Image generation;Magnetic resonance imaging;Medical diagnostic imaging;Machine learning;Dementia;Medical diagnosis","biomedical MRI;blood vessels;convolutional neural nets;diseases;learning (artificial intelligence);medical image processing","arterial spin labeling image synthesis;ample medical images;medical imaging utilizations;medical image synthesis methods;pooling operation;capsule-based networks;structural MRI;convolutional neural networks-based-generative adversarial networks-based models;region-based-voxel-based partial volume;dementia disease diagnosis","","","","48","CCBYNCND","1 Oct 2020","","","IEEE","IEEE Journals"
"Consistent Embedded GAN for Image-to-Image Translation","F. Xiong; Q. Wang; Q. Gao","State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China","IEEE Access","16 Sep 2019","2019","7","","126651","126661","Generative Adversarial Networks (GANs) have achieved remarkable progress in image-to-image translation tasks. However, these methods have the common problem that lacking the ability to generate both perceptually realistic and diverse images in the target domain. To tackle the problem, in this paper, we propose a novel model named Consistent Embedded Generative Adversarial Networks (CEGAN) for the image-to-image translation task. It aims to learn conditional generation models for generating perceptually realistic outputs and capture the full distribution of potential multiple modes of results by enforcing tight connections in both the real image space and latent space. To achieve realism, unlike existing GANs models that their discriminators attempt to differentiate between real images from the dataset and fake samples produced by the generator, the discriminator in our model distinguishes the real images and fake images in the latent space to alleviate the impact of the redundancy and noise in generated images. On the other hand, we learn a low-dimensional latent code that is distilled from the possible multiple distribution in the latent space to achieve diversity. By this way, our model avoids the problem of mode collapse and produces more diverse and realistic results. Extensive experimental results demonstrate the superiority of the proposed method.","2169-3536","","10.1109/ACCESS.2019.2939654","National Natural Science Foundation of China(grant numbers:61773302,61906142); Xidian University(grant numbers:10221150004); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825805","Image-to-image translation;GAN;latent space","Generative adversarial networks;Generators;Gallium nitride;Task analysis;Training;Redundancy;Image synthesis","image processing;neural nets;realistic images","image-to-image translation task;diverse images;conditional generation models;image space;latent space;fake images;realistic images;consistent embedded generative adversarial networks;consistent embedded GAN models;CEGAN","","14","","53","CCBY","5 Sep 2019","","","IEEE","IEEE Journals"
"Disease-Image-Specific Learning for Diagnosis-Oriented Neuroimage Synthesis With Incomplete Multi-Modality Data","Y. Pan; M. Liu; Y. Xia; D. Shen","Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an, China; Department ofArtificial Intelligence, Korea University, Seoul, South Korea","IEEE Transactions on Pattern Analysis and Machine Intelligence","15 Sep 2022","2022","44","10","6839","6853","Incomplete data problem is commonly existing in classification tasks with multi-source data, particularly the disease diagnosis with multi-modality neuroimages, to track which, some methods have been proposed to utilize all available subjects by imputing missing neuroimages. However, these methods usually treat image synthesis and disease diagnosis as two standalone tasks, thus ignoring the specificity conveyed in different modalities, i.e., different modalities may highlight different disease-relevant regions in the brain. To this end, we propose a disease-image-specific deep learning (DSDL) framework for joint neuroimage synthesis and disease diagnosis using incomplete multi-modality neuroimages. Specifically, with each whole-brain scan as input, we first design a Disease-image-Specific Network (DSNet) with a spatial cosine module to implicitly model the disease-image specificity. We then develop a Feature-consistency Generative Adversarial Network (FGAN) to impute missing neuroimages, where feature maps (generated by DSNet) of a synthetic image and its respective real image are encouraged to be consistent while preserving the disease-image-specific information. Since our FGAN is correlated with DSNet, missing neuroimages can be synthesized in a diagnosis-oriented manner. Experimental results on three datasets suggest that our method can not only generate reasonable neuroimages, but also achieve state-of-the-art performance in both tasks of Alzheimer’s disease identification and mild cognitive impairment conversion prediction.","1939-3539","","10.1109/TPAMI.2021.3091214","National Natural Science Foundation of China(grant numbers:61771397); Science and Technology Innovation Committee of Shenzhen Municipality(grant numbers:JCYJ20180306171334997); NIH(grant numbers:AG041721); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462380","Multi-modality neuroimaging;generative adversarial network;missing image synthesis;brain disease diagnosis","Diseases;Magnetic resonance imaging;Medical diagnosis;Image synthesis;Generative adversarial networks;Task analysis;Neuroimaging","biomedical MRI;brain;cognition;diseases;feature extraction;image classification;learning (artificial intelligence);medical image processing;neurophysiology","disease-image-specific information;respective real image;synthetic image;Feature-consistency Generative Adversarial Network;disease-image specificity;Disease-image-Specific Network;incomplete multimodality neuroimages;joint neuroimage synthesis;disease-image-specific deep learning framework;different disease-relevant regions;image synthesis;imputing missing neuroimages;disease diagnosis;multisource data;incomplete data problem;incomplete multimodality data;diagnosis-oriented neuroimage synthesis;Disease-image-Specific learning;Alzheimer's disease identification;reasonable neuroimages;diagnosis-oriented manner","Algorithms;Alzheimer Disease;Brain;Humans;Image Interpretation, Computer-Assisted;Magnetic Resonance Imaging;Neuroimaging","6","","56","IEEE","22 Jun 2021","","","IEEE","IEEE Journals"
"Identity-Aware Facial Expression Recognition Via Deep Metric Learning Based on Synthesized Images","W. Huang; S. Zhang; P. Zhang; Y. Zha; Y. Fang; Y. Zhang","Department of Computer Science, School of Information Engineering, Nanchang University, Nanchang, China; Department of Computer Science, School of Information Engineering, Nanchang University, Nanchang, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; School of Information Technology, Jiangxi University of Finance and Economics, Nanchang, China; School of Computer Science, Northwestern Polytechnical University, Xi’an, China","IEEE Transactions on Multimedia","12 Jul 2022","2022","24","","3327","3339","Person-dependent facial expression recognition has received considerable research attention in recent years. Unfortunately, different identities can adversely influence recognition accuracy, and the recognition task becomes challenging. Other adverse factors, including limited training data and improper measures of facial expressions, can further contribute to the above dilemma. To solve these problems, a novel identity-aware method is proposed in this study. Furthermore, this study also represents the first attempt to fulfill the challenging person-dependent facial expression recognition task based on deep metric learning and facial image synthesis techniques. Technically, a StarGAN is incorporated to synthesize facial images depicting different but complete basic emotions for each identity to augment the training data. Then, a deep-convolutional-neural-network-based network is employed to automatically extract latent features from both real facial images and all synthesized facial images. Next, a Mahalanobis metric network trained based on extracted latent features outputs a learned metric that measures facial expression differences between images, and the recognition task can thus be realized. Extensive experiments based on several well-known publicly available datasets are carried out in this study for performance evaluations. Person-dependent datasets, including CK+, Oulu (all 6 subdatasets), MMI, ISAFE, ISED, etc., are all incorporated. After comparing the new method with several popular or state-of-the-art facial expression recognition methods, its superiority in person-dependent facial expression recognition can be proposed from a statistical point of view.","1941-0077","","10.1109/TMM.2021.3096068","National Natural Science Foundation of China(grant numbers:61862043,61971352); Natural Science Foundation of Jiangxi Province(grant numbers:20204BCJ22011); Natural Science Foundation of Shaanxi Province(grant numbers:2018JM6015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9479695","Deep learning;facial expression recognition;image synthesis;person-dependent;metric learning","Task analysis;Measurement;Generative adversarial networks;Face recognition;Feature extraction;Image synthesis;Image recognition","convolutional neural nets;deep learning (artificial intelligence);emotion recognition;face recognition;feature extraction","identity-aware facial expression recognition;deep metric learning;synthesized images;person-dependent facial expression recognition;recognition accuracy;recognition task;training data;facial expressions;challenging person-dependent;facial image synthesis techniques;deep-convolutional-neural-network-based network;synthesized facial images;Mahalanobis metric network;facial expression differences;person-dependent datasets;facial expression recognition methods;identity-aware method","","2","","64","IEEE","9 Jul 2021","","","IEEE","IEEE Journals"
"TextFace: Text-to-Style Mapping based Face Generation and Manipulation","X. Hou; X. Zhang; Y. Li; L. Shen","School of Computer Science and Software Engineering, Shenzhen University, 47890 Shenzhen, China; School of Computer Science and Software Engineering, Shenzhen University, 47890 Shenzhen, Guangdong, China; School of Computer Science and Software Engineering, Shenzhen University, 47890 Shenzhen, Guangdong, China; School of Computer Science and Software Engineering, Shenzhen University, P.R. China, Shenzhen, China, 518060","IEEE Transactions on Multimedia","","2022","PP","99","1","1","As a sub-topic of Text-to-Image synthesis, Text-to-Face generation has a great potential in face related applications. In this paper, we propose a generic Text-to-Face framework, namely TextFace, to achieve diverse and high-quality face image generation from text description. We introduce a novel method called Text-to-Style mapping, where the text description can be directly encoded into the latent space of a pretrained StyleGAN. Guided by our text-image similarity matching and face captioning based text alignment, the textual latent code can be fed into a well-trained StyleGAN's generator, to produce diverse face images with high resolution (1024 1024). Furthermore, our model inherently supports the semantic face editing using text descriptions. Finally, experimental results quantitatively and qualitatively demonstrate the superior performance of our model.","1941-0077","","10.1109/TMM.2022.3160360","Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515111199); National Natural Science Foundation of China(grant numbers:91959108); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9737433","GANs;text-to-image generation;cross modal;text-to-face generation;text-guided semantic face manipulation","Faces;Semantics;Codes;Generators;Generative adversarial networks;Image synthesis;Training","","","","","","","IEEE","17 Mar 2022","","","IEEE","IEEE Early Access Articles"
"GAC-GAN: A General Method for Appearance-Controllable Human Video Motion Transfer","D. Wei; X. Xu; H. Shen; K. Huang","Department of Information Science, and Electronic Engineering, Zhejiang University, Hangzhou, China; Guangdong Provincial People's Hospital, Guangdong Academy of Medical Sciences, Guangzhou, China; Department of Information Science, and Electronic Engineering, Zhejiang University, Hangzhou, China; Department of Information Science, and Electronic Engineering, Zhejiang University, Hangzhou, China","IEEE Transactions on Multimedia","30 Jul 2021","2021","23","","2457","2470","Human video motion transfer has a wide range of applications in multimedia, computer vision, and graphics. Recently, due to the rapid development of Generative Adversarial Networks (GANs), there has been significant progress in the field. However, almost all existing GAN-based works are prone to address the mapping from human motions to video scenes, with scene appearances encoded individually in the trained models. Therefore, each trained model can only generate videos with a specific scene appearance, and new models are required to be trained to generate new appearances. Besides, existing works lack the capability of appearance control. For example, users have to provide video records of wearing new clothes or performing in new backgrounds to enable clothes or background changing in their synthetic videos, which greatly limits the application flexibility. In this paper, we propose General Appearance-Controllable GAN (GAC-GAN), a general method for appearance-controllable human video motion transfer. To enable general-purpose appearance synthesis, we propose to include appearance information in the conditioning inputs. Thus, once trained, our model can generate new appearances by altering the input appearance information. To achieve appearance control, we first obtain the appearance-controllable conditioning inputs, and then utilize a two-stage GAC-GAN to generate the corresponding appearance-controllable outputs, where we utilize an Appearance-Consistency GAN (ACGAN) loss, and a shadow extraction module for output foreground, and background appearance control respectively. We further build a solo dance dataset containing a large number of dance videos for training, and evaluation. Experimental results on our solo dance dataset, and iPER dataset show that our proposed GAC-GAN can not only support appearance-controllable human video motion transfer but also achieve higher video quality than state-of-art methods.","1941-0077","","10.1109/TMM.2020.3011290","National Natural Science Foundation of China(grant numbers:U19B2043); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9147027","Motion transfer;video generation;image synthesis;generative adversarial networks (GANs)","Gallium nitride;Layout;Generative adversarial networks;Training;Three-dimensional displays;Computational modeling;Solid modeling","computer vision;feature extraction;gallium compounds;image motion analysis;image sequences;learning (artificial intelligence);object detection;video recording;video signal processing","GAC-GAN;appearance-controllable human video motion transfer;existing GAN-based works;scene appearances;trained model;specific scene appearance;General Appearance-Controllable GAN;general-purpose appearance synthesis;input appearance information;appearance-controllable conditioning inputs;corresponding appearance-controllable outputs;Appearance-Consistency GAN loss;background appearance control","","6","","51","IEEE","24 Jul 2020","","","IEEE","IEEE Journals"
"Semantic Object Accuracy for Generative Text-to-Image Synthesis","T. Hinz; S. Heinrich; S. Wermter","Knowledge Technology Group, University of Hamburg, Hamburg, Germany; Knowledge Technology Group, University of Hamburg, Hamburg, Germany; Knowledge Technology Group, University of Hamburg, Hamburg, Germany","IEEE Transactions on Pattern Analysis and Machine Intelligence","3 Feb 2022","2022","44","3","1552","1565","Generative adversarial networks conditioned on textual image descriptions are capable of generating realistic-looking images. However, current methods still struggle to generate images based on complex image captions from a heterogeneous domain. Furthermore, quantitatively evaluating these text-to-image models is challenging, as most evaluation metrics only judge image quality but not the conformity between the image and its caption. To address these challenges we introduce a new model that explicitly models individual objects within an image and a new evaluation metric called Semantic Object Accuracy (SOA) that specifically evaluates images given an image caption. The SOA uses a pre-trained object detector to evaluate if a generated image contains objects that are mentioned in the image caption, e.g., whether an image generated from “a car driving down the street” contains a car. We perform a user study comparing several text-to-image models and show that our SOA metric ranks the models the same way as humans, whereas other metrics such as the Inception Score do not. Our evaluation also shows that models which explicitly model objects outperform models which only model global image characteristics.","1939-3539","","10.1109/TPAMI.2020.3021209","German Research Foundation DFG(grant numbers:TRR 169); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9184960","Text-to-image synthesis;generative adversarial network (GAN);evaluation of generative models;generative models","Layout;Semantics;Gallium nitride;Measurement;Generators;Image resolution;Image quality","feature extraction;image classification;image segmentation;image texture;object detection;visual databases","model global image characteristics;Semantic Object Accuracy;generative text-to-image synthesis;generative adversarial networks;textual image descriptions;complex image captions;text-to-image models;evaluation metric;image quality;image caption;pre-trained object detector;SOA metric ranks","Algorithms;Humans;Image Processing, Computer-Assisted;Neural Networks, Computer;Semantics","15","","65","CCBY","2 Sep 2020","","","IEEE","IEEE Journals"
"The Synthesis of Unpaired Underwater Images Using a Multistyle Generative Adversarial Network","N. Li; Z. Zheng; S. Zhang; Z. Yu; H. Zheng; B. Zheng","Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China","IEEE Access","15 Oct 2018","2018","6","","54241","54257","Underwater image datasets are crucial in underwater vision research. Because of the strong absorption and scattering effects that occur underwater, some ground truth such as the depth map, which can be easily collected in-air, becomes a great challenge in underwater environments. To solve the issues associated with the lack of underwater ground truth, we propose a trainable end-to-end system of an underwater multistyle generative adversarial network (UMGAN) that takes advantage of a cycle-consistent adversarial network (CycleGAN) and conditional generative adversarial networks. This system can generate multiple realistic underwater images from in-air images using a hybrid adversarial system and an unpaired method. Moreover, our model can translate in-air images to underwater images that retain the main content and structural information of the in-air images under specified turbidities or water styles through a style classifier and a conditional vector. Furthermore, we define the color loss and include the structural similarity index measure loss for the system to preserve the content and structure of original in-air images while transferring the backgrounds of the images from air to water. Using UMGAN, we can take advantage of the in-air ground truth and convert the corresponding in-air images into an underwater dataset with multiple water color styles. Our experiments demonstrate that our synthesized underwater images have a high score on image assessment against CycleGAN, WaterGAN, StarGAN, AdaIN, and other state-of-the-art methods. We also show that our synthesized underwater images with in-air depths can be applied to real underwater image depth map estimation.","2169-3536","","10.1109/ACCESS.2018.2870854","National Natural Science Foundation of China(grant numbers:61701463); Natural Science Foundation of Shandong Province(grant numbers:ZR2017BF011); Fundamental Research Funds for the Central Universities(grant numbers:201713019); National Postdoctoral Foundation of China(grant numbers:2017M622277); Qingdao Postdoctoral Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8467323","Underwater technology;artificial neural networks;image processing","Estimation;Image color analysis;Gallium nitride;Loss measurement;Task analysis;Generative adversarial networks;Neural networks","geophysical image processing;image classification;image colour analysis;learning (artificial intelligence);oceanographic techniques","underwater image depth map estimation;underwater vision research;underwater ground truth;trainable end-to-end system;underwater multistyle generative adversarial network;conditional generative adversarial networks;multiple realistic underwater images;image classification;AdaIN;StarGAN;WaterGAN;structural similarity index measure loss;underwater image generation;CycleGAN;UMGAN;absorption effects;scattering effects;underwater image datasets;hybrid cycle-consistent adversarial network;unpaired underwater image synthesis","","19","1","78","CCBY","18 Sep 2018","","","IEEE","IEEE Journals"
"Cross-View Image Synthesis Using Conditional GANs","K. Regmi; A. Borji","Center for Research in Computer Vision, University of Central Florida; Center for Research in Computer Vision, University of Central Florida","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","3501","3510","Learning to generate natural scenes has always been a challenging task in computer vision. It is even more painstaking when the generation is conditioned on images with drastically different views. This is mainly because understanding, corresponding, and transforming appearance and semantic information across the views is not trivial. In this paper, we attempt to solve the novel problem of cross-view image synthesis, aerial to street-view and vice versa, using conditional generative adversarial networks (cGAN). Two new architectures called Crossview Fork (X-Fork) and Crossview Sequential (X-Seq) are proposed to generate scenes with resolutions of 64 × 64 and 256 × 256 pixels. X-Fork architecture has a single discriminator and a single generator. The generator hallucinates both the image and its semantic segmentation in the target view. X-Seq architecture utilizes two cGANs. The first one generates the target image which is subsequently fed to the second cGAN for generating its corresponding semantic segmentation map. The feedback from the second cGAN helps the first cGAN generate sharper images. Both of our proposed architectures learn to generate natural images as well as their semantic segmentation maps. The proposed methods show that they are able to capture and maintain the true semantics of objects in source and target views better than the traditional image-to-image translation method which considers only the visual appearance of the scene. Extensive qualitative and quantitative evaluations support the effectiveness of our frameworks, compared to two state of the art methods, for natural scene generation across drastically different views.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578467","","Gallium nitride;Image segmentation;Task analysis;Generators;Generative adversarial networks;Image generation;Semantics","computer vision;image colour analysis;image representation;image resolution;image sampling;image segmentation;learning (artificial intelligence);natural scenes","cross-view image synthesis;natural scenes;drastically different views;semantic information;cGAN;X-Fork architecture;single generator;target view;X-Seq architecture;target image;corresponding semantic segmentation map;sharper images;natural images;semantic segmentation maps;target views;natural scene generation;conditional GAN;conditional generative adversarial network;image-to-image translation method","","74","1","39","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Sliced Wasserstein Generative Models","J. Wu; Z. Huang; D. Acharya; W. Li; J. Thoma; D. P. Paudel; L. Van Gool","Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland; Computer Vision Lab, ETH Zurich, Switzerland; VISICS, KU Leuven, Belgium","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","3708","3717","In generative modeling, the Wasserstein distance (WD) has emerged as a useful metric to measure the discrepancy between generated and real data distributions. Unfortunately, it is challenging to approximate the WD of high-dimensional distributions. In contrast, the sliced Wasserstein distance (SWD) factorizes high-dimensional distributions into their multiple one-dimensional marginal distributions and is thus easier to approximate. In this paper, we introduce novel approximations of the primal and dual SWD. Instead of using a large number of random projections, as it is done by conventional SWD approximation methods, we propose to approximate SWDs with a small number of parameterized orthogonal projections in an end-to-end deep learning fashion. As concrete applications of our SWD approximations, we design two types of differentiable SWD blocks to equip modern generative frameworks---Auto-Encoders (AE) and Generative Adversarial Networks (GAN). In the experiments, we not only show the superiority of the proposed generative models on standard image synthesis benchmarks, but also demonstrate the state-of-the-art performance on challenging high resolution image and video generation in an unsupervised manner.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00383","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953921","Image and Video Synthesis;Deep Learning;Optimization Methods","Measurement;Deep learning;Computer vision;Image resolution;Image synthesis;Computational modeling;Generative adversarial networks","learning (artificial intelligence);neural nets;statistical distributions","data distributions;sliced Wasserstein distance;one-dimensional marginal distributions;parameterized orthogonal projections;end-to-end deep learning;SWD approximations;Wasserstein generative adversarial networks;autoencoders;sliced Wasserstein generative models","","27","","38","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"HSIGAN: A Conditional Hyperspectral Image Synthesis Method With Auxiliary Classifier","W. Liu; J. You; J. Lee","Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea; Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea; Artificial Intelligence Laboratory, Jeonbuk National University, Jeonju, South Korea","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","1 Apr 2021","2021","14","","3330","3344","In this article, we explore a conditional hyperspectral image (HSI) synthesis method with generative adversarial networks (GAN). A new multistage and multipole generative adversarial network, which is suitable for conditional HSI generation and classification (HSIGAN), is proposed. For HSIs synthesis, it is crucial to learn a great deal of spatial-spectral distribution features from source data. The multistage progressive training makes the generator effectively imitate the real data by fully exploiting the high-dimension learning capability of GAN models. The coarse-to-fine information extraction method helps the discriminator to understand the semantic feature better while the multiscale classification prediction presents a positive impact on results. A spectral classifier joins the adversarial network, which offers a helping hand to stabilize and optimize the model. Moreover, we apply the 3-D DropBlock layer in the generator to remove semantic information in a contiguous spatial-spectral region and avoid model collapse. Experimental results of the quantitative and qualitative evaluation show that HSIGAN could generate high-fidelity, diverse hyperspectral cubes while achieving top-ranking accuracy for supervised classification. This result is encouraging for using GANs as a data augmentation strategy in the HSI vision task.","2151-1535","","10.1109/JSTARS.2021.3063911","Natural Science Foundation of Hebei Province(grant numbers:F2020403030); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369834","Classification;generative adversarial network (GAN);hyperspectral image (HSI);synthesis","Gallium nitride;Generative adversarial networks;Generators;Hyperspectral imaging;Deep learning;Training;Task analysis","feature extraction;hyperspectral imaging;image classification;image colour analysis;neural nets;supervised learning","3-D DropBlock layer;conditional HSI generation and classification;HSI vision task;data augmentation strategy;supervised classification;hyperspectral cubes;semantic information;spectral classifier;multiscale classification prediction;semantic feature;coarse-to-fine information extraction method;GAN models;high-dimension learning capability;multistage progressive training;spatial-spectral distribution features;multipole generative adversarial network;auxiliary classifier;conditional hyperspectral image synthesis method;HSIGAN","","2","","58","CCBYNCND","4 Mar 2021","","","IEEE","IEEE Journals"
"A Patch-Based Algorithm for Diverse and High Fidelity Single Image Generation","N. Cherel; A. Almansa; Y. Gousseau; A. Newson","LTCI, Télécom Paris, Institut Polytechnique de Paris; MAP5, CNRS & Université de Paris; LTCI, Télécom Paris, Institut Polytechnique de Paris; LTCI, Télécom Paris, Institut Polytechnique de Paris","2022 IEEE International Conference on Image Processing (ICIP)","18 Oct 2022","2022","","","3221","3225","Image generation is the task of producing new samples from one or several example images. Until recently, this has been done using large image databases, in particular using Generative Adversarial Networks (GANs). However, Shaham et al. [1] recently proposed the SinGAN method, which achieves this generation using a single image example. At the same time, researchers are realizing that classical patch- based methods can replace certain neural networks, with no costly training. In this paper, we present a purely patch-based method, named Patches for Single image generation (PSin), which requires no training and generates samples in seconds. Our algorithm is based on the minimization of a global, patch- based energy functional, which ensures the visual fidelity of the result to the original image. We also ensure diversity of the results by carefully choosing the initialization of the algorithm. We propose two initialization variants. We compare our results to both the original SinGAN and another recent patch-based image generation approach, both qualitatively and quantitatively using multiple metrics.","2381-8549","978-1-6654-9620-9","10.1109/ICIP46576.2022.9897913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9897913","patch;single image generation;generative adversarial networks","Training;Measurement;Visualization;Image synthesis;Image databases;Neural networks;Minimization","","","","","","17","IEEE","18 Oct 2022","","","IEEE","IEEE Conferences"
"Panoramic Image Generation: From 2-D Sketch to Spherical Image","Y. Duan; C. Han; X. Tao; B. Geng; Y. Du; J. Lu","Beijing Innovation Center for Future Chip, Beijing, China; Beijing Innovation Center for Future Chip, Beijing, China; Beijing Innovation Center for Future Chip, Beijing, China; Beijing Innovation Center for Future Chip, Beijing, China; State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China; Beijing Innovation Center for Future Chip, Beijing, China","IEEE Journal of Selected Topics in Signal Processing","6 Feb 2020","2020","14","1","194","208","The 360-degree video/image, also called an omnidirectional video/image or panoramic video/image, is very important in some emerging areas such as virtual reality (VR). Therefore, corresponding image generation algorithms are urgently needed. However, existing image generation models mainly focus on 2-D images and do not consider the spherical structures of panoramic images. In this article, we propose a panoramic image generation method based on spherical convolution and generative adversarial networks, called spherical generative adversarial networks (SGANs). We adopt the sketch map as the input, which is a concise geometric structure representation of the panoramic image, e.g., comprising approximately 7% of the pixels for a 583 × 1163 image. Through adversarial learning, a realistic-looking, plausible and high-fidelity spherical image can be obtained from the sparse sketch map. In particular, we build a dataset of the sketch maps using a visual computation-based sketching model. Then, by optimizing SGANs with GAN loss, feature matching loss and perceptual loss, realistic textures and details are recovered gradually. On one hand, it is an improvement using the sparse sketch map as input rather than the denser input, e.g., the features of the textures and colors. On the other hand, spherical convolution helps to remedy space-varying distortions of the planar projection. We conduct extensive experiments on some public panoramic image datasets and compare them with state-of-the-art techniques to validate the superior performance of the proposed approach.","1941-0484","","10.1109/JSTSP.2020.2968772","National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFF0301205); National Natural Science Foundation of China(grant numbers:61801260,61925105); China Postdoctoral Science Foundation(grant numbers:2018T110098); iQIYI; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966243","Panoramic image generation;generative adversarial networks;spherical convolution;sparse sketch map","Image reconstruction;Image synthesis;Compressed sensing;Matching pursuit algorithms;Image coding;Transforms;Distortion","feature extraction;image matching;image motion analysis;image representation;image sensors;image texture;learning (artificial intelligence);realistic images;solid modelling;video cameras;video signal processing;virtual reality","2-D sketch;spherical structures;panoramic image generation method;spherical convolution;high-fidelity spherical image;sparse sketch map;public panoramic image datasets;spherical generative adversarial networks;360-degree video;360-degree image;omnidirectional image;omnidirectional video;geometric structure representation;SGANs;feature matching loss;perceptual loss","","8","","66","IEEE","22 Jan 2020","","","IEEE","IEEE Journals"
"TiVGAN: Text to Image to Video Generation With Step-by-Step Evolutionary Generator","D. Kim; D. Joo; J. Kim","School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea","IEEE Access","27 Aug 2020","2020","8","","153113","153122","Advances in technology have led to the development of methods that can create desired visual multimedia. In particular, image generation using deep learning has been extensively studied across diverse fields. In comparison, video generation, especially on conditional inputs, remains a challenging and less explored area. To narrow this gap, we aim to train our model to produce a video corresponding to a given text description. We propose a novel training framework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN), which evolves frame-by-frame and finally produces a full-length video. In the first phase, we focus on creating a high-quality single video frame while learning the relationship between the text and an image. As the steps proceed, our model is trained gradually on more number of consecutive frames. This step-by-step learning process helps stabilize the training and enables the creation of high-resolution video based on conditional text descriptions. Qualitative and quantitative experimental results on various datasets demonstrate the effectiveness of the proposed method.","2169-3536","","10.1109/ACCESS.2020.3017881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9171240","Computer vision;deep learning;generative adversarial networks;video generation;text-to-video generation","Training;Generators;Gallium nitride;Generative adversarial networks;Image synthesis;Task analysis;Streaming media","evolutionary computation;image resolution;learning (artificial intelligence);neural nets;video signal processing","video generation;text description;TiVGAN;full-length video;high-quality single video frame;consecutive frames;step-by-step learning process;high-resolution video;conditional text descriptions;step-by-step evolutionary generator;visual multimedia;image generation;deep learning;frame-by-frame video;text-to-image-to-video generative adversarial network","","15","","29","CCBY","19 Aug 2020","","","IEEE","IEEE Journals"
"CGAN-IRB: A Novel Data Augmentation Method for Apple Leaf Diseases","X. Yuan; C. Yu; B. Liu; H. Sun; X. Zhu","College of Information Engineering, Northwest A&F University, Yangling, China; College of Information Engineering, Northwest A&F University, Yangling, China; College of Information Engineering, Northwest A&F University, Yangling, China; College of Information Engineering, Northwest A&F University, Yangling, China; College of Information Engineering, Northwest A&F University, Yangling, China","2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)","9 Sep 2021","2021","","","192","200","At present, the identification of apple leaf diseases plays an important role in controlling apple leaf diseases and improving apple yield. CNNs(Convolutional Neural Networks) have been widely used in apple leaf diseases identification, but the training of the CNNs requires a large number of images. The lack of images would make the CNNs hard to generalize. Thus the CNNs are unable to recognize new disease images. Focusing on this problem, this paper proposes a new model named CGAN-IRB(Conditional Generative Adversarial Network with the Improved Residual Block) for data augmentation. Firstly, various improvements have been made based on CGAN to generate high-quality, robust, and specific-category images of apple leaf diseases. Among which the embedding of the residual block has been found to significantly improve the model performance. Then the interpolation algorithm is used instead of deconvolution to increase the image size. Finally, the TTUR(Two-Timescale Update Rule) training strategy is employed and all the convolutional layers of the network are spectrally normalized to stabilize the training of the network. The performance of CGAN-IRB was tested both on image generation and classification tasks. Experiment results show that the images generated by the network possess high quality and robust features, pro-viding a novel solution for the data augmentation of apple leaf diseases. The new GAN-based data augmentation method leads to significant improvements in the classification accuracy of CNNs. In the case of all tested CNNs, the classification accuracy improvements are 11.75% and 2.17% on average over non-augmented and traditional-augmented, respectively. Among them, the classification accuracy of GoogLeNet V2 and ShuffleNet V2 is 99.34% and 99.67%, respectively. The data augmentation approach proposed in this paper can be used more widely in the field of disease identification, solving the problem of insufficient data sets, and can be extended to related fields where data sets are difficult to obtain.","0730-3157","978-1-6654-2463-9","10.1109/COMPSAC51774.2021.00037","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529763","data augmentation;generative adversarial networks;apple leaf disease identification;convolutional neural networks","Training;Interpolation;Image recognition;Image synthesis;Neural networks;Generative adversarial networks;Data models","biology computing;botany;convolutional neural nets;feature extraction;image classification;learning (artificial intelligence);plant diseases","image generation;image classification;data augmentation;CGAN-IRB;apple leaf disease identification;convolutional neural networks;CNN training;disease image recognition;conditional generative adversarial network;improved residual block;TTUR training;two-timescale update rule training;image quality;image features;GoogLeNet V2;ShuffleNet V2","","1","","32","IEEE","9 Sep 2021","","","IEEE","IEEE Conferences"
"Generative Difference Image for Blind Image Quality Assessment","Y. Han; Y. Wang; Y. Ma","Xinjiang Laboratory of Minority Speech and Language Information Processing, Chinese Academy of Sciences, Urumqi, China; Xinjiang Laboratory of Minority Speech and Language Information Processing, Chinese Academy of Sciences, Urumqi, China; Xinjiang Laboratory of Minority Speech and Language Information Processing, Chinese Academy of Sciences, Urumqi, China","2021 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)","4 Oct 2021","2021","","","108","115","Image quality usually refers to the degree of error of the distorted image relative to the reference image in the human visual perception system. Image quality assessment is to score the image quality objectively. No-reference image quality assessment is limited to distorted image information, which is more challenging in the field of computer vision. In this paper, we proposed an approach based on difference image generation to address this problem. First, by removing the up-sampling layer and batch normalization layer in the Super-Resolution Generative Adversarial Network (SRGAN) to build a difference image generation model, and applying the content loss function to optimize the model. Then, the regression network is constructed based on the convolutional neural network (CNN). The regression network contains 4 convolutional layers and 2 fully connected layers and learns the correlation between the generated difference image and the image quality score to predict the distorted image quality. Finally, comparative experiments were evaluated on three public datasets. Compared with the previous state-of-the-art methods, our method obtains similar results on the LIVE dataset and achieves significant improvement on the TID2013 and CSIQ datasets. The results demonstrate that our proposed approach achieves state-of-the-art image quality prediction.","","978-1-6654-3960-2","10.1109/ICCEAI52939.2021.00021","West Light Foundation of The Chinese Academy of Sciences(grant numbers:2019-XBQNXZ-B-009); Natural Science Foundation of Xinjiang, China(grant numbers:2020D01B55); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9544120","image quality assessment;generative adversarial networks;difference image generation","Image quality;Correlation;Image synthesis;Superresolution;Generative adversarial networks;Distortion;Generators","computer vision;convolutional neural nets;image resolution;image sampling;learning (artificial intelligence);regression analysis;visual perception","blind image quality assessment;human visual perception system;no-reference image quality assessment;distorted image information;batch normalization layer;super-resolution generative adversarial network;difference image generation model;regression network;generated difference image;image quality score;distorted image quality;image quality prediction;generative difference image;content loss function;TID2013 dataset;CSIQ dataset;convolutional neural network;up-sampling layer","","","","39","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"Combining Noise-to-Image and Image-to-Image GANs: Brain MR Image Augmentation for Tumor Detection","C. Han; L. Rundo; R. Araki; Y. Nagano; Y. Furukawa; G. Mauri; H. Nakayama; H. Hayashi","Department of Radiology, University of Cambridge, Cambridge, U.K.; Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milan, Italy; Machine Perception and Robotics Group, Graduate School of Engineering, Chubu University, Kasugai, Japan; Machine Perception Group, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Department of Psychiatry, Jikei University School of Medicine, Tokyo, Japan; Department of Informatics, Systems and Communication, University of Milano-Bicocca, Milan, Italy; International Research Center for Neurointelligence (WPI-IRCN), Institutes for Advanced Study, The University of Tokyo, Tokyo, Japan; Department of Advanced Information Technology, Human Interface Laboratory, Kyushu University, Fukuoka, Japan","IEEE Access","4 Nov 2019","2019","7","","156966","156977","Convolutional Neural Networks (CNNs) achieve excellent computer-assisted diagnosis with sufficient annotated training data. However, most medical imaging datasets are small and fragmented. In this context, Generative Adversarial Networks (GANs) can synthesize realistic/diverse additional training images to fill the data lack in the real image distribution; researchers have improved classification by augmenting data with noise-to-image (e.g., random noise samples to diverse pathological images) or image-to-image GANs (e.g., a benign image to a malignant one). Yet, no research has reported results combining noise-to-image and image-to-image GANs for further performance boost. Therefore, to maximize the DA effect with the GAN combinations, we propose a two-step GAN-based DA that generates and refines brain Magnetic Resonance (MR) images with/without tumors separately: (i) Progressive Growing of GANs (PGGANs), multi-stage noise-to-image GAN for high-resolution MR image generation, first generates realistic/diverse 256×256 images; (ii) Multimodal UNsupervised Image-to-image Translation (MUNIT) that combines GANs/Variational AutoEncoders or SimGAN that uses a DA-focused GAN loss, further refines the texture/shape of the PGGAN-generated images similarly to the real ones. We thoroughly investigate CNN-based tumor classification results, also considering the influence of pre-training on ImageNet and discarding weird-looking GAN-generated images. The results show that, when combined with classic DA, our two-step GAN-based DA can significantly outperform the classic DA alone, in tumor detection (i.e., boosting sensitivity 93.67% to 97.48%) and also in other medical imaging tasks.","2169-3536","","10.1109/ACCESS.2019.2947606","Qdai-jump Research Program; Japan Society for the Promotion of Science(grant numbers:JP17K12752); Japan Agency for Medical Research and Development(grant numbers:JP18lk1010028); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869751","Data augmentation;synthetic image generation;GANs;brain MRI;tumor detection","Gallium nitride;Training;Tumors;Generative adversarial networks;Medical diagnostic imaging;Image synthesis","biomedical MRI;brain;convolutional neural nets;image classification;image coding;image resolution;image segmentation;image texture;learning (artificial intelligence);medical image processing;tumours","medical imaging datasets;image distribution;diverse pathological images;benign image;GAN combinations;two-step GAN-based DA;high-resolution MR image generation;multimodal unsupervised image-to-image translation;GAN-generated images;medical imaging tasks;brain magnetic resonance images;image-to-image GAN;brain MR image augmentation;tumor detection;convolutional neural networks;computer-assisted diagnosis;generative adversarial networks;multistage noise-to-image GAN;GAN-variational autoencoders;SimGAN;PGGAN-generated image texture;CNN-based tumor classification;ImageNet","","93","","53","CCBY","16 Oct 2019","","","IEEE","IEEE Journals"
"HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms","M. Afifi; M. A. Brubaker; M. S. Brown",York University; York University; York University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","7937","7946","While generative adversarial networks (GANs) can successfully produce high-quality images, they can be challenging to control. Simplifying GAN-based image generation is critical for their adoption in graphic design and artistic work. This goal has led to significant interest in methods that can intuitively control the appearance of images generated by GANs. In this paper, we present HistoGAN, a color histogram-based method for controlling GAN-generated images’ colors. We focus on color histograms as they provide an intuitive way to describe image color while remaining decoupled from domain-specific semantics. Specifically, we introduce an effective modification of the recent StyleGAN architecture [31] to control the colors of GAN-generated images specified by a target color histogram feature. We then describe how to expand HistoGAN to recolor real images. For image recoloring, we jointly train an encoder network along with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised approach trained to encourage the network to keep the original image’s content while changing the colors based on the given target histogram. We show that this histogram-based approach offers a better way to control GAN-generated and real images’ colors while producing more compelling results compared to existing alternative strategies.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00785","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577403","","Graphics;Histograms;Computer vision;Image color analysis;Image synthesis;Semantics;Generative adversarial networks","image colour analysis;neural nets","image recoloring;HistoGAN;original image;target histogram;histogram-based approach;color histograms;generative adversarial networks;high-quality images;GAN-based image generation;graphic design;artistic work;color histogram-based method;GAN-generated images;image color;target color histogram feature","","21","","59","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"OSTeC: One-Shot Texture Completion","B. Gecer; J. Deng; S. Zafeiriou","Imperial College, London, Huawei CBG; Imperial College, London, Huawei CBG; Imperial College, London, Huawei CBG","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","7624","7634","The last few years have witnessed the great success of non-linear generative models in synthesizing high-quality photorealistic face images. Many recent 3D facial texture reconstruction and pose manipulation from a single image approaches still rely on large and clean face datasets to train image-to-image Generative Adversarial Networks (GANs). Yet the collection of such a large scale high-resolution 3D texture dataset is still very costly and difficult to maintain age/ethnicity balance. Moreover, regression-based approaches suffer from generalization to the in-the-wild conditions and are unable to fine-tune to a target-image. In this work, we propose an unsupervised approach for one-shot 3D facial texture completion that does not re-quire large-scale texture datasets, but rather harnesses the knowledge stored in 2D face generators. The proposed approach rotates an input image in 3D and fill-in the unseen regions by reconstructing the rotated image in a 2D face generator, based on the visible parts. Finally, we stitch the most visible textures at different angles in the UV image-plane. Further, we frontalize the target image by projecting the completed texture into the generator. The qualitative and quantitative experiments demonstrate that the completed UV textures and frontalized images are of high quality, resembles the original identity, can be used to train a texture GAN model for 3DMM fitting and improve pose-invariant face recognition. 1","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577745","","Solid modeling;Computer vision;Three-dimensional displays;Image synthesis;Face recognition;Fitting;Generative adversarial networks","face recognition;image reconstruction;image texture;neural nets;pose estimation;regression analysis;unsupervised learning","clean face datasets;image-to-image Generative Adversarial Networks;scale high-resolution 3D texture dataset;regression-based approaches;target-image;unsupervised approach;one-shot 3D facial texture completion;large-scale texture datasets;2D face generator;input image;rotated image;visible textures;UV image-plane;target image;completed texture;completed UV textures;frontalized images;texture GAN model;pose-invariant face recognition;one-shot texture completion;nonlinear generative models;high-quality photorealistic face images;recent 3D facial texture reconstruction;single image","","13","","52","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Efficient Conditional GAN Transfer with Knowledge Propagation across Classes","M. Shahbazi; Z. Huang; D. P. Paudel; A. Chhatkuli; L. Van Gool","Computer Vision Lab, ETH Zürich, Switzerland; Computer Vision Lab, ETH Zürich, Switzerland; Computer Vision Lab, ETH Zürich, Switzerland; Computer Vision Lab, ETH Zürich, Switzerland; PSI, KU Leuven, Belgium","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","12162","12171","Generative adversarial networks (GANs) have shown impressive results in both unconditional and conditional image generation. In recent literature, it is shown that pre-trained GANs, on a different dataset, can be transferred to improve the image generation from a small target data. The same, however, has not been well-studied in the case of conditional GANs (cGANs), which provides new opportunities for knowledge transfer compared to unconditional setup. In particular, the new classes may borrow knowledge from the related old classes, or share knowledge among themselves to improve the training. This motivates us to study the problem of efficient conditional GAN transfer with knowledge propagation across classes. To address this problem, we introduce a new GAN transfer method to explicitly propagate the knowledge from the old classes to the new classes. The key idea is to enforce the popularly used conditional batch normalization (BN) to learn the class-specific information of the new classes from that of the old classes, with implicit knowledge sharing among the new ones. This allows for an efficient knowledge propagation from the old classes to the new ones, with the BN parameters increasing linearly with the number of new classes. The extensive evaluation demonstrates the clear superiority of the proposed method over state-of-the-art competitors for efficient conditional GAN transfer tasks. The code is available at: https://github.com/mshahbazi72/cGANTransfer","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578839","","Training;Computer vision;Codes;Image synthesis;Generative adversarial networks;Pattern recognition;Task analysis","image processing;learning (artificial intelligence);neural nets","efficient knowledge propagation;efficient conditional GAN transfer tasks;generative adversarial networks;unconditional image generation;pre-trained GANs;conditional GANs;knowledge transfer;popularly used conditional batch normalization;class-specific information;implicit knowledge sharing;conditional image generation;cGANs;conditional batch normalization","","11","","47","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"LT-GAN: Self-Supervised GAN with Latent Transformation Detection","P. Patel; N. Kumari; M. Singh; B. Krishnamurthy","Birla Institute of Technology & Science, Pilani, India; Media and Data Science Research Lab, Adobe; Media and Data Science Research Lab, Adobe; Media and Data Science Research Lab, Adobe","2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 Jun 2021","2021","","","3188","3197","Generative Adversarial Networks (GANs) coupled with self-supervised tasks have shown promising results in unconditional and semi-supervised image generation. We propose a self-supervised approach (LT-GAN) to improve the generation quality and diversity of images by estimating the GAN-induced transformation (i.e. transformation induced in the generated images by perturbing the latent space of generator). Specifically, given two pairs of images where each pair comprises of a generated image and its transformed version, the self-supervision task aims to identify whether the latent transformation applied in the given pair is same as that of the other pair. Hence, this auxiliary loss encourages the generator to produce images that are distinguishable by the auxiliary network, which in turn promotes the synthesis of semantically consistent images with respect to latent transformations. We show the efficacy of this pretext task by improving the image generation quality in terms of FID on state-of-the-art models in conditional and unconditional settings on CIFAR-10, CelebA-HQ and ImageNet datasets. Moreover, we empirically show that LT-GAN helps in improving controlled image editing for CelebA-HQ, and ImageNet over baseline models. We experimentally demonstrate that our proposed LT self-supervision task can be effectively combined with other state-of-the-art training techniques for added benefits. Consequently, we show that our approach achieves the new state-of-the-art FID score of 9.8 on conditional CIFAR-10 image generation.","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423067","","Training;Measurement;Computer vision;Image synthesis;Conferences;Computational modeling;Generative adversarial networks","image processing;neural nets;supervised learning","LT-GAN;latent transformation detection;generative adversarial networks;semisupervised image generation;GAN-induced transformation;latent space;auxiliary network;semantically consistent images;pretext task;image generation quality;image editing;LT self-supervision task;conditional CIFAR-10 image generation;self-supervised GAN;auxiliary loss;CelebA-HQ;FID score;ImageNet datasets","","6","","53","IEEE","14 Jun 2021","","","IEEE","IEEE Conferences"
"Fast Adaptive Meta-Learning for Few-Shot Image Generation","A. Phaphuangwittayakul; Y. Guo; F. Ying","Department of Computer Science, and Engineering, East China University of Science, and Technology, Shanghai, China; Shanghai Engineering Research Center of Big Data, and Internet Audience, Shanghai, China; State Key Laboratory of Bioreactor Engineering, East China University of Science and Technology, Shanghai, China","IEEE Transactions on Multimedia","6 Apr 2022","2022","24","","2205","2217","Generative Adversarial Networks (GANs) are capable of effectively synthesising new realistic images and estimating the potential distribution of samples utilising adversarial learning. Nevertheless, conventional GANs require a large amount of training data samples to produce plausible results. Inspired by the capacity for humans to quickly learn new concepts from a small number of examples, several meta-learning approaches for the few-shot datasets are presented. However, most of meta-learning algorithms are designed to tackle few-shot classification and reinforcement learning tasks. Moreover, the existing meta-learning models for image generation are complex, thereby affecting the length of training time required. Fast Adaptive Meta-Learning (FAML) based on GAN and the encoder network is proposed in this study for few-shot image generation. This model demonstrates the capability to generate new realistic images from previously unseen target classes with only a small number of examples required. With 10 times faster convergence, FAML requires only one-fourth of the trainable parameters in comparison baseline models by training a simpler network with conditional feature vectors from the encoder, while increasing the number of generator iterations. The visualisation results are demonstrated in the paper. This model is able to improve few-shot image generation with the lowest FID score, highest IS, and comparable LPIPS to MNIST, Omniglot, VGG-Faces, and miniImageNet datasets. The source code is available on https://github.com/phaphuang/FAML.","1941-0077","","10.1109/TMM.2021.3077729","National Key Research and Development Program of China(grant numbers:2018YFC0807105); Science and Technology Committee of Shanghai Municipality(grant numbers:17DZ1101003,18511106602,18DZ2252300); Open Funding Project of the State Key Laboratory of Bioreactor Engineering; East China University of Science and Technology, Shanghai, China; International College of Digital Innovation; Chiang Mai University, Thailand; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9424414","Meta-learning;few-shot image generation;generative adversarial network;unsupervised learning","Task analysis;Training;Image synthesis;Adaptation models;Generative adversarial networks;Generators;Data models","feature extraction;image classification;neural nets;realistic images;reinforcement learning","few-shot image generation;realistic images;generator iterations;generative adversarial networks;GAN;adversarial learning;few-shot datasets;few-shot classification;reinforcement learning;fast adaptive meta-learning;FAML;encoder network;conditional feature vectors;FID score","","6","","63","IEEE","5 May 2021","","","IEEE","IEEE Journals"
"Parallel Optimal Transport GAN","G. Avraham; Y. Zuo; T. Drummond","ARC Centre of Excellence for Robotic Vision, Monash University, Melbourne, VIC, Australia; ARC Centre of Excellence for Robotic Vision, Monash University, Australia; ARC Centre of Excellence for Robotic Vision, Monash University, Australia","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","4406","4415","Although Generative Adversarial Networks (GANs) are known for their sharp realism in image generation, they often fail to estimate areas of the data density. This leads to low modal diversity and at times distorted generated samples. These problems essentially arise from poor estimation of the distance metric responsible for training these networks. To address these issues, we introduce an additional regularisation term which performs optimal transport in parallel within a low dimensional representation space. We demonstrate that operating in a low dimension representation of the data distribution benefits from convergence rate gains in estimating the Wasserstein distance, resulting in more stable GAN training. We empirically show that our regulariser achieves a stabilising effect which leads to higher quality of generated samples and increased mode coverage of the given data distribution. Our method achieves significant improvements on the CIFAR-10, Oxford Flowers and CUB Birds datasets over several GAN baselines both qualitatively and quantitatively.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953994","Deep Learning;Optimization Methods;Representation Learning","Training;Measurement;Computer vision;Image synthesis;Estimation;Generative adversarial networks;Generators","convergence;image representation;learning (artificial intelligence);mathematical analysis;neural nets;statistical distributions","CUB Birds dataset;Oxford Flowers dataset;CIFAR-10 dataset;network training;data density;image generation;Generative Adversarial Networks;parallel optimal transport GAN;GAN training;Wasserstein distance;data distribution;low dimensional representation space","","2","","40","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Disentangle, Assemble, and Synthesize: Unsupervised Learning to Disentangle Appearance and Location","H. Aizawa; H. Kataoka; Y. Satoh; K. Kato","Gifu University, Gifu, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Ibaraki, Japan; Gifu University, Gifu, Japan","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","2065","2072","The next step for the generative adversarial networks (GAN) is to learn representations that allow us to control only a certain factor in the image explicitly. Since such a representation of the factor is independent of other factors, the controllability obtained from these representations leads to interpretability by identifying the variation of the synthesized image and the transferability for downstream tasks by inference. However, since it is difficult to identify and strictly define latent factors, the annotation is laborious. Moreover, learning such representations by a GAN is challenging due to the complex generation process. Therefore, we resolve this limitation using a novel generative model that can disentangle latent space into the appearance, the x-axis, and the y-axis of the object, and reassemble these components in an unsupervised manner. Specifically, based on the concept of packing the appearance and location in each position of the feature map, we introduce a novel structural constraint technique that prevents these representations from interacting with each other. The proposed structural constraint promotes the disentanglement of these factors. In experiments, we found that the proposed method is simple but effective for controllability and allows us to control the appearance and location via latent space without supervision, as compared with the conditional GAN.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9413032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413032","","Computer vision;Image resolution;Image synthesis;Computational modeling;Generative adversarial networks;Controllability;Pattern recognition","control engineering computing;controllability;image representation;neural nets;unsupervised learning","disentangle appearance;generative adversarial networks;controllability;synthesized image;downstream tasks;latent factors;complex generation process;generative model;disentangle latent space;unsupervised manner;disentanglement;conditional GAN;feature map;image representation","","","","36","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"PROVES: Establishing Image Provenance using Semantic Signatures","M. Xie; M. Kulshrestha; S. Wang; J. Yang; A. Chakrabarti; N. Zhang; Y. Vorobeychik","Computer Science & Engineering, Washington University in St. Louis; Computer Science, University of Massachusetts at Amherst; Computer Science & Engineering, Washington University in St. Louis; Computer Science & Engineering, Washington University in St. Louis; Computer Science & Engineering, Washington University in St. Louis; Computer Science & Engineering, Washington University in St. Louis; Computer Science & Engineering, Washington University in St. Louis","2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","15 Feb 2022","2022","","","3017","3026","Modern AI tools, such as generative adversarial networks, have transformed our ability to create and modify visual data with photorealistic results. However, one of the deleterious side-effects of these advances is the emergence of nefarious uses in manipulating information in visual data, such as through the use of deep fakes. We propose a novel architecture for preserving the provenance of semantic information in images to make them less susceptible to deep fake attacks. Our architecture includes semantic signing and verification steps. We apply this architecture to verifying two types of semantic information: individual identities (faces) and whether the photo was taken indoors or outdoors. Verification accounts for a collection of common image transformation, such as translation, scaling, cropping, and small rotations, and rejects adversarial transformations, such as adversarially perturbed or, in the case of face verification, swapped faces. Experiments demonstrate that in the case of provenance of faces in an image, our approach is robust to black-box adversarial transformations (which are rejected) as well as benign transformations (which are accepted), with few false negatives and false positives. Background verification, on the other hand, is susceptible to black-box adversarial examples, but be-comes significantly more robust after adversarial training.","2642-9381","978-1-6654-0915-5","10.1109/WACV51458.2022.00307","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706700","Security/Surveillance Deep Learning -> Adversarial Learning;Adversarial Attack and Defense Methods","Training;Visualization;Computer vision;Image synthesis;Weapons;Semantics;Generative adversarial networks","face recognition;neural nets;security of data","PROVES;image provenance;semantic signatures;modern AI tools;generative adversarial networks;photorealistic results;deleterious side-effects;nefarious uses;semantic information;deep fake attacks;semantic signing;verification steps;individual identities;common image transformation;face verification;swapped faces;black-box adversarial transformations;benign transformations;background verification;black-box adversarial examples;adversarial training","","","","43","IEEE","15 Feb 2022","","","IEEE","IEEE Conferences"
"Attribute Group Editing for Reliable Few-shot Image Generation","G. Ding; X. Han; S. Wang; S. Wu; X. Jin; D. Tu; Q. Huang","Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China; Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China; Peng Cheng Laboratory, Shenzhen, China; Huawei Cloud EI Innovation Lab, China; Huawei Cloud EI Innovation Lab, China; Huawei Cloud EI Innovation Lab, China; Key Lab of Intell. Info. Process., Inst. of Comput. Tech., CAS, Beijing, China","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","11184","11193","Few-shot image generation is a challenging task even using the state-of-the-art Generative Adversarial Networks (GANs). Due to the unstable GAN training process and the limited training data, the generated images are often of low quality and low diversity. In this work, we propose a new “editing-based” method, i.e., Attribute Group Editing (AGE), for few-shot image generation. The basic assumption is that any image is a collection of attributes and the editing direction for a specific attribute is shared across all categories. AGE examines the internal representation learned in GANs and identifies semantically meaningful directions. Specifically, the class embedding, i.e., the mean vector of the latent codes from a specific category, is used to represent the category-relevant attributes, and the category-irrelevant attributes are learned globally by Sparse Dictionary Learning on the difference between the sample embedding and the class embedding. Given a GAN well trained on seen categories, diverse images of unseen categories can be synthesized through editing category-irrelevant attributes while keeping category-relevant attributes unchanged. Without re-training the GAN, AGE is capable of not only producing more realistic and diverse images for downstream visual applications with limited data but achieving controllable image editing with interpretable category-irrelevant directions. Code is available at https://github.com/UniBester/AGE.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01091","National Key R&D Program of China(grant numbers:2018AAAOI02000); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879434","Image and video synthesis and generation; Transfer/low-shot/long-tail learning","Training;Visualization;Codes;Image synthesis;Semantics;Training data;Generative adversarial networks","image classification;image representation;learning (artificial intelligence)","interpretable category-irrelevant directions;Attribute group editing;few-shot image generation;state-of-the-art Generative Adversarial Networks;unstable GAN training process;editing-based method;Attribute Group Editing;editing direction;specific attribute;class embedding;specific category;category-relevant attributes;diverse images;editing category-irrelevant attributes;realistic images;controllable image editing","","","","42","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"A New Contrastive GAN With Data Augmentation for Surface Defect Recognition Under Limited Data","Z. Du; L. Gao; X. Li","State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, School of Mechanical Science and Engineering, Huazhong University of Science and Technology, Wuhan, China","IEEE Transactions on Instrumentation and Measurement","13 Jan 2023","2023","72","","1","13","Surface defect recognition (SDC) is essential in intelligent manufacturing. Deep learning (DL) is a research hotspot in SDC. Limited defective samples are available in most real-world cases, which poses challenges for DL methods. Given such circumstances, generating defective samples by generative adversarial networks (GANs) is applied. However, insufficient samples and high-frequency texture details in defects make GANs very hard to train, yield mode collapse, and poor image quality, which can further impact SDC. To solve these problems, this article proposes a new GAN called contrastive GAN, which can be trained to generate diverse defects with only extremely limited samples. Specifically, a shared data augmentation (SDA) module is proposed for avoiding overfitting. Then, a feature attention matching (FAM) module is proposed to align features for improving the quality of generated images. Finally, a contrastive loss based on hypersphere is employed to constrain GANs to generate images that differ from the traditional transform. Experiments show that the proposed GAN generates defective images with higher quality and lower variance between real defects compared to other GANs. Synthetic images contribute to pretrained DL networks with accuracies of up to 95.00%–99.56% for Northeastern University (NEU) datasets of different sizes and 91.84% for printed circuit board (PCB) cases, which proves the effectiveness of the proposed method.","1557-9662","","10.1109/TIM.2022.3232649","National Natural Science Foundation of China(grant numbers:52188102,U21B2029); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999697","Data augmentation;defect image generation;generative adversarial network (GAN);limited data;surface defect recognition","Generative adversarial networks;Image recognition;Training;Feature extraction;Image synthesis;Image segmentation;Generators","automatic optical inspection;feature extraction;gallium compounds;image classification;image enhancement;image matching;image recognition;image texture;learning (artificial intelligence);mobile handsets;neural nets;object detection;object recognition","defective images;defective samples;diverse defects;extremely limited samples;generative adversarial networks;high-frequency texture details;insufficient samples;new contrastive GAN;SDC;shared data augmentation module;surface defect recognition","","","","42","IEEE","27 Dec 2022","","","IEEE","IEEE Journals"
"Dynamic Dual-Output Diffusion Models","Y. Benny; L. Wolf",Tel Aviv University; Tel Aviv University,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","11472","11481","Iterative denoising-based generation, also known as denoising diffusion models, has recently been shown to be comparable in quality to other classes of generative models, and even surpass them. Including, in particular, Generative Adversarial Networks, which are currently the state of the art in many subtasks of image generation. However, a major drawback of this method is that it requires hundreds of iterations to produce a competitive result. Recent works have proposed solutions that allow for faster generation with fewer iterations, but the image quality gradually deteriorates with increasingly fewer iterations being applied during generation. In this paper, we reveal some of the causes that affect the generation quality of diffusion models, especially when sampling with few iterations, and come up with a simple, yet effective, solution to mitigate them. We consider two opposite equations for the iterative denoising, the first predicts the applied noise, and the second predicts the image directly. Our solution takes the two options and learns to dynamically alternate between them through the denoising process. Our proposed solution is general and can be applied to any existing diffusion model. As we show, when applied to various SOTA architectures, our solution immediately improves their generation quality, with negligible added complexity and parameters. We experiment on multiple datasets and configurations and run an extensive ablation study to support these findings.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01119","European Research Council (ERC)(grant numbers:ERC CoG 725974); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880083","Image and video synthesis and generation","Image quality;Computer vision;Image synthesis;Noise reduction;Computer architecture;Generative adversarial networks;Mathematical models","image denoising;image sampling;iterative methods;learning (artificial intelligence);neural nets","dynamic dual-output diffusion models;iterative denoising-based generation;generative adversarial networks;image generation;image quality;denoising process;SOTA architectures","","","","38","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"A Novel GAN based on Progressive Growing Transformer with Capsule Embedding","X. Jiang; T. Zhang; Y. Xiao; T. Qiu; Q. Yang","College of Computer Science, Chongqing University; College of Computer Science, Chongqing University; College of Computer Science, Chongqing University; College of Computer Science, Chongqing University; College of Computer Science, Chongqing University","2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","18 Nov 2022","2022","","","1401","1406","Generative Adversarial Networks (GANs) have achieved great improvement after using Convolutional Neural Networks (CNNs) instead of Multi-Layer Perceptrons (MLPs) to build network architecture. Recently, since Transformer architecture has performed well in compute vision, building a Transformer-based image generation network helps solve some of problems caused by CNNs e.g. CNNs-based GANs are difficult to train. On the other hand, the learning of positional encoding in the Transformer structure is often ignored in Transformer-based GANs. Capsule networks are usually considered to be able to learn position information in image features. Therefore, this paper constructs a Progressive Growing Transformer network with Capsule Embedding GAN (PGTCEGAN). The results from the proposed approach are promising with 3.59 FID and 3.92 FID on CelebA and LSUN-Church datasets respectively in image generation task.","2577-1655","978-1-6654-5258-8","10.1109/SMC53654.2022.9945436","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9945436","GAN;transformer;capsule embedding;image generation","Image synthesis;Computer architecture;Network architecture;Transformers;Generative adversarial networks;Robustness;Encoding","computer vision;convolutional neural nets;image coding;learning (artificial intelligence);neural net architecture","capsule networks;CelebA dataset;CNN-based GAN;compute vision;convolutional neural networks;generative adversarial networks;image features;LSUN-Church dataset;network architecture;PGTCEGAN;positional encoding;progressive growing transformer network with capsule embedding GAN;transformer architecture;transformer structure;transformer-based GAN;transformer-based image generation network","","","","33","IEEE","18 Nov 2022","","","IEEE","IEEE Conferences"
"Generate and Purify: Efficient Person Data Generation for Re-Identification","J. Lu; W. Zhang; H. Yin","Advanced Institute of Information Technology, Peking University, Beijing, China; Advanced Institute of Information Technology, Peking University, Beijing, China; School of Information Engineering, Hangzhou Dianzi University, Hangzhou, Zhejiang, China","IEEE Transactions on Multimedia","8 Feb 2022","2022","24","","558","566","Generating person images has been a promising approach to enhance the input richness for re-identification (reID) tasks in recent works. A key challenge is that the generated data often contains noise, which is caused by identity inconsistency between the generated person and the original input and failure cases in generative adversarial networks (GAN). Directly training using generated images may greatly affect learning good feature embeddings, resulting in unsatisfactory reID performance. This work presents a two-stage framework that can generate high-quality person images and purify failure cases for reID training. Experimental results demonstrate that our proposed generative model can produce person images with superior appearance consistency comparing with other state-of-the-art methods. Furthermore, we show that our method yields a significant improvement in re-identification (reID) task on public datasets with insufficient training data.","1941-0077","","10.1109/TMM.2021.3054973","National Natural Science Foundation of China(grant numbers:61972123,61931008,61901150); Key R&D Projects(grant numbers:2018YFC0830106); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337217","Graph clustering;person generation;re-identification","Training;Convolutional codes;Data models;Image synthesis;Heating systems;Generative adversarial networks;Training data","convolutional neural nets;face recognition;feature extraction;graph theory;image enhancement;learning (artificial intelligence);object recognition","person reidentification;identity inconsistency;generative adversarial networks;high-quality person images;generative model;person data generation;input richness enhancement;GAN;learning;appearance consistency;convolution networks;graph convolutional network;GCN","","","","44","IEEE","27 Jan 2021","","","IEEE","IEEE Journals"
"Exocentric to Egocentric Image Generation Via Parallel Generative Adversarial Network","G. Liu; H. Tang; H. Latapie; Y. Yan","Department of Computer Science, Texas State University, USA; Department of Computer Science, Texas State University, USA; Chief Technology & Architecture Office, Cisco, USA; Department of Computer Science, Texas State University, USA","ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","9 Apr 2020","2020","","","1843","1847","Cross-view image generation has been recently proposed to generate images of one view from another dramatically different view. In this paper, we investigate exocentric (third-person) view to egocentric (first-person) view image generation. This is a challenging task since egocentric view sometimes is remarkably different from exocentric view. Thus, transforming the appearances across the two views is a nontrivial task. To this end, we propose a novel Parallel Generative Adversarial Network (P-GAN) with a novel cross-cycle loss to learn the shared information for generating egocentric images from exocentric view. We also incorporate a novel contextual feature loss in the learning procedure to capture the contextual information in images. Extensive experiments on the Exo-Ego datasets [1] show that our model outperforms the state-of-the-art approaches.","2379-190X","978-1-5090-6631-5","10.1109/ICASSP40776.2020.9053957","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9053957","Egocentric;Exocentric;Cross-View Image Generation;Parallel GANs","Image synthesis;Conferences;Signal processing;Generative adversarial networks;Task analysis;Speech processing;Gallium nitride","feature extraction;image processing;learning (artificial intelligence);neural nets","cross-view image generation;egocentric view;exocentric view;parallel generative adversarial network;exocentric to egocentric image generation;P-GAN;cross-cycle loss;contextual feature loss;learning procedure;Exo-Ego datasets","","11","","18","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Image-Based Virtual Try-on Network with Structural Coherence","F. Sun; J. Guo; Z. Su; C. Gao","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","519","523","Virtual try-on system could demonstrate the visual effect of wearing certain clothes, which is in a great demand for the online clothing customers. Due to the neglect of the original person structure, the previous try-on schemes usually encounter with the problems like the loss of body parts, the missing of body details and the deviation of clothing style. In this paper, we propose a novel image-based virtual try-on network, which could maintain the structural consistency between the generated image and the original image by human parsing. Our network consists of three components. Given the original person and the clothing images, the target human parsing maps are generated. Then, the parsing maps are matched with the target clothes to generate the warped clothes. Finally, according to the parsing results, the parts to be replaced the original images are intercepted, and more original information is retained as the input of the network to generate the final results. Experiments on an existing benchmark demonstrate our method maintains the consistency of structure and achieves the state-of-the-art performance.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803811","Virtual Try-On;Human Parsing;Structural Coherence;Detail Preservation","Clothing;Gallium nitride;Generators;Generative adversarial networks;Benchmark testing;Image synthesis;Training","clothing;electronic commerce;image matching","structural coherence;structural consistency;target human parsing maps;image-based virtual try-on network;original information","","8","","20","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Analysis of Adversarial based Augmentation for Diabetic Retinopathy Disease Grading","R. Balasubramanian; V. Sowmya; E. A. Gopalakrishnan; V. K. Menon; V. V. Sajith Variyar; K. P. Soman","Center for Computational Engineering and Networking (CEN), Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Center for Computational Engineering and Networking (CEN), Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Center for Computational Engineering and Networking (CEN), Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Center for Computational Engineering and Networking (CEN), Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Center for Computational Engineering and Networking (CEN), Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India; Center for Computational Engineering and Networking (CEN), Amrita School of Engineering, Amrita Vishwa Vidyapeetham, Coimbatore, India","2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)","15 Oct 2020","2020","","","1","5","Diabetic Retinopathy(DR) is one of the diseases, which is caused by damage to blood vessels of the light sensitive tissue of the eyes. DR is graded into five different levels; normal, mild, moderate, severe and proliferative. DR diagnosis demands more time on detection from fundus images. An accurate automatic model requires sufficient data for training which is unavailable. The open-source DR datasets are highly imbalanced between the levels of DR and it is also not easy to collect more data for proliferative cases. The synthetic generation of data for such highly imbalanced classes in a dataset provides better results on classification. In this paper, an analysis of classification results for the same is carried out with augmented proliferative class (highly imbalanced class) in the EYEPACS dataset using a generative adversarial network (GAN). We generated highly diverse images for proliferative cases without any constraints. The generated proliferative images do not influence other class images and have also improved the classification results obtained by the model, over that which was trained without synthetic generation. The results obtained before and after augmentation by the proposed generative based model is compared over various model attributes.","","978-1-7281-6851-7","10.1109/ICCCNT49239.2020.9225684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9225684","Diabetic Retinopathy;GAN;imbalance data;Synthetic images;Augmentation;CNN;Classification","Data models;Generative adversarial networks;Feature extraction;Gallium nitride;Retina;Image synthesis;Computational modeling","biomedical optical imaging;biomedical telemetry;blood vessels;diseases;eye;medical disorders;medical image processing","augmented proliferative images;image classification;open-source DR datasets;fundus images;DR diagnosis;light sensitive tissue;blood vessels;diseases;diabetic retinopathy disease;adversarial based augmentation;generative based model;synthetic generation;generative adversarial network;EYEPACS dataset","","5","","22","IEEE","15 Oct 2020","","","IEEE","IEEE Conferences"
"Semi-Supervised SAR ATR via Conditional Generative Adversarial Network with Multi-Discriminator","X. Liu; Y. Huang; C. Wang; J. Pei; W. Huo; Y. Zhang; J. Yang","School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China; School of Information and Communication Engineering University of Electronic Science and Technology of China, Chengdu, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","2361","2364","Convolutional neural networks (CNN) show superior potential in synthetic aperture radar automatic target recognition (SAR ATR). However, due to the difficulty of obtaining SAR images and the scarcity of labeled SAR images, supervised learning has poor performance in this area and is not widely applicable. To address this problem, a semi-supervised conditional generative adversarial network with a multi-discriminator (SCGAN-MD) is proposed in this paper. In our method, a conditional generative adversarial network (CGAN) is adopted with two discriminators for training the generated images and predicting the labels for unlabeled samples. Compared with other semi-supervised learning-based methods, our proposed method has more accurate image generation capability and can achieve improved recognition accuracy of SAR ATR. Experiments on the Moving and Stationary Target Acquisition and Recognition (MSTAR) database indicate that the proposed method can effectively improve the recognition accuracy and robustness of the network with a small number of labeled samples.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554365","National Natural Science Foundation of China(grant numbers:61901091,61901090,61671117); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554365","SAR ATR;semi-supervised;conditional generative adversarial network","Training;Image recognition;Target recognition;Image synthesis;Supervised learning;Generative adversarial networks;Feature extraction","image recognition;neural nets;radar computing;radar imaging;radar target recognition;supervised learning;synthetic aperture radar","recognition accuracy;semisupervised SAR ATR;conditional generative adversarial network;multidiscriminator;convolutional neural networks;synthetic aperture radar automatic target recognition;labeled SAR images;supervised learning;semisupervised learning-based methods;image generation capability;CNN;SCGAN-MD;CGAN;moving and stationary target acquisition and recognition database;MSTAR database","","3","","9","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Scene Retrieval for Video Summarization Based on Text-to-Image gan","R. Yanagi; R. Togo; T. Ogawa; M. Haseyama","School of Engineering, Hokkaido University, Sapporo, Japan; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Japan; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Japan; Graduate School of Information Science and Technology, Hokkaido University, Sapporo, Japan","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","1825","1829","We present a new scene retrieval method based on text-to-image Generative Adversarial Network (GAN) and its application to query-based video summarization. Text-to-image GAN is a deep learning method that can generate images from their corresponding sentences. In this paper, we reveal a characteristic that deep learning-based visual features extracted from images generated by text-to-image GAN include semantic information sufficiently. By utilizing the generated images as queries, the proposed method achieves higher scene retrieval performance than those of the state-of-the-art methods. In addition, we introduce a novel architecture that can consider order relationship of the input sentences to our method for realizing a target video summarization. Specifically, the proposed method generates multiple images thorough text-to-image GAN from multiple sentences summarizing target videos. Their summarized video can be obtained by performing the retrieval of corresponding scenes from the target videos according to the generated images with considering the order relationship. Experimental results show the effectiveness of the proposed method in the retrieval and summarization performance.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803177","Scene retrieval;video summarization;text-to-image GAN;deep learning","Feature extraction;Generative adversarial networks;Gallium nitride;Visualization;Task analysis;Image resolution;Image synthesis","feature extraction;learning (artificial intelligence);neural net architecture;video retrieval;video signal processing","summarized video;target videos;text-to-image GAN;scene retrieval method;text-to-image generative adversarial network;deep learning method;deep learning-based visual features;higher scene retrieval performance;target video summarization;multiple images","","3","","30","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Illumination-Robust Object Coordinate Detection by Adopting Pix2Pix GAN for Training Image Generation","Y. -H. Huang; C. -H. G. Li; Y. -M. Chang","Graduate Institute of Manufacturing Technology National Taipei University of Technology, Taipei, Taiwan, R.O.C.; Graduate Institute of Manufacturing Technology National Taipei University of Technology, Taipei, Taiwan, R.O.C.; Graduate Institute of Manufacturing Technology National Taipei University of Technology, Taipei, Taiwan, R.O.C.","2019 International Conference on Technologies and Applications of Artiﬁcial Intelligence (TAAI)","16 Jan 2020","2019","","","1","6","Illumination effects often result in error or failure in visual object localization. Whereas ConvNet-based object localization frameworks have shown tremendous robustness to the illumination effect, under some strong illumination effects, the system may still fail. In this paper, the authors propose a data augmentation method utilizing pix2pix GAN for automatic generation of object images under various illumination effects. Upon training for the object localization ConvNet, the generated images are included to enrich the training set for better performance under strong illumination effects. Experimental evidence shows that the accuracy of object coordinate detection can be improved significantly. The proposed framework maintains our concept of “one-shot” where the user only needs to take a basis photo of the target object; the rest of the process including image processing and data annotation are all automated.","2376-6824","978-1-7281-4666-9","10.1109/TAAI48200.2019.8959837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8959837","computer vision;convolutional neural network;generative adversarial network;pix2pix;localization;data augmentation;one-shot","Lighting;Training;Gallium nitride;Generative adversarial networks;Annotations;Detectors;Image synthesis","feature extraction;learning (artificial intelligence);neural nets;object detection;object recognition","one-shot concept;object images;ConvNet-based object localization frameworks;visual object localization;training image generation;pix2pix GAN;illumination-robust object coordinate detection;data annotation;image processing;strong illumination effects;object localization ConvNet;illumination effect","","3","","8","IEEE","16 Jan 2020","","","IEEE","IEEE Conferences"
"An Interactive Evolution Strategy based Deep Convolutional Generative Adversarial Network for 2D Video Game Level Procedural Content Generation","M. Jiang; L. Zhang","School of Computer Science, University of Sunderland, Sunderland, United Kingdom; National Subsea Centre, Robert Gordon University, Aberdeen, Unitied Kindom","2021 International Joint Conference on Neural Networks (IJCNN)","20 Sep 2021","2021","","","1","6","The generation of desirable video game contents has been a challenge of games level design and production. In this research, we propose a game player flow experience driven interactive latent variable evolution strategy incorporated with a Deep Convolutional Generative Adversarial Network (DCGAN) for undertaking game content generation with respect to a 2D Super Mario video game. Since the Generative Adversarial Network (GAN) models tend to capture the high-level style of the input images by learning the latent vectors, they are used to generate game scenarios and context images in this research. However, as GANs employ arbitrary inputs for game image generation without taking specific features into account, they generate game level images in an incoherent manner without the specific playable game level properties, such as a broken pipe in the Mario game level image. In order to overcome such drawbacks, we propose a game player flow experience driven optimised mechanism with human intervention, to guide the game level content generation process so that only plausible and even enjoyable images will be generated as the candidates for the final game design and production.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533847","Interactive Evolution Strategy;Deep Convolutional Generative Adversarial Network;Procedural Content Generation;Video Game","Learning systems;Image synthesis;Neural networks;Games;Production;Reinforcement learning;Generative adversarial networks","computer games;convolutional neural nets;deep learning (artificial intelligence);image processing;interactive systems","interactive evolution strategy;deep convolutional generative adversarial network;2D video game level procedural content generation;video game contents;game player flow experience;interactive latent variable evolution strategy;2D Super Mario video game;game image generation;game level images","","2","","18","IEEE","20 Sep 2021","","","IEEE","IEEE Conferences"
"Image Style Transfer Based on DPN-CycleGAN","M. Yang; J. He","Computer Science and Cyber Security(OXFORD BROOKS COLLEGE), ChengDu University Of Technology, ChengDu, China; Computer Science and Cyber Security(OXFORD BROOKS COLLEGE), ChengDu University Of Technology, ChengDu, China","2021 4th International Conference on Pattern Recognition and Artificial Intelligence (PRAI)","4 Oct 2021","2021","","","141","145","Image style transfer has always been a hot topic in the field of image generation. Generative Adversarial Network (GAN) is used to realize image style transfer can greatly reduce the workload, and get more fruitful results. The traditional image style transfer algorithm needs to convert between two paired images, but the paired data set used for training is difficult to obtain. In order to avoid being limited by data sets and improve the efficiency of image style transfer, this paper proposes an improved Cycle-Consistent Adversarial Network (DPN-CycleGAN), which uses Dual Path Network (DPN) replaces the Deep Residual Network (ResNet) of the original network generator, and adds Identity Loss on the basis of the original loss. The changes make the network structure more simplified, reduce the computational complexity, improve the network performance to a certain extent, and improve the quality of the image generated by style transfer. Finally, the experimental results show that the SSIM value and PSNR value of the generated images are increased by 3.6% and 9.7% on average, which proves the effectiveness of the DPN-CycleGAN image style transfer algorithm proposed by this paper.","","978-1-6654-1322-0","10.1109/PRAI53619.2021.9550797","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9550797","GAN;CycleGAN;ResNet;DPN;image style transfer","Training;Image synthesis;Computational modeling;Generative adversarial networks;Generators;Pattern recognition;Computational complexity","computational complexity;digital signatures;image colour analysis","paired images;image style transfer;improved Cycle-Consistent Adversarial Network;original network generator;DPN-CycleGAN image style;image generation;Generative Adversarial Network;traditional image style","","1","","16","IEEE","4 Oct 2021","","","IEEE","IEEE Conferences"
"A Pseudo-Siamese Feature Fusion Generative Adversarial Network for Synthesizing High-quality Fetal Four-chamber Views","S. Qiao; S. Pan; G. Luo; S. Pang; T. Chen; A. K. Singh; Z. Lv","Qingdao, Shandong, China, 266580 (e-mail: siboqiao@126.com); Qingdao, China, (e-mail: silinpan@126.com); Qingdao, China, (e-mail: lg1989jay@163.com); School of computer science and technology, China University of Petroleum East China - Qingdao Campus, 74591 Qingdao, China, China, 266580 (e-mail: pangsc@upc.edu.cn); Qingdao, China, (e-mail: 13608967759@163.com); CSE, NIT Patna, 230635 Patna, Bihar, India, 800005 (e-mail: amit.singh@nitp.ac.in); Uppsala University, 8097 Uppsala, Sweden, (e-mail: lvzhihan@gmail.com)","IEEE Journal of Biomedical and Health Informatics","","2022","PP","99","1","1","Four-chamber (FC) views are the primary ultrasound (US) images that cardiologists diagnose whether the fetus has congenital heart disease (CHD) in prenatal diagnosis and screening. FC views intuitively depict the developmental morphology of the fetal heart. Early diagnosis of fetal CHD has always been the focus and difficulty of prenatal screening. Furthermore, deep learning technology has achieved great success in medical image analysis. Hence, applying deep learning technology in the early screening of fetal CHD helps improve diagnostic accuracy. However, the lack of large-scale and high-quality fetal FC views brings incredible difficulties to deep learning models or cardiologists. Hence, we propose a Pseudo-Siamese Feature Fusion Generative Adversarial Network (PSFFGAN), synthesizing high-quality fetal FC views using FC sketch images. In addition, we propose a novel Triplet Generative Adversarial Loss Function (TGALF), which optimizes PSFFGAN to fully extract the cardiac anatomical structure information provided by FC sketch images to synthesize the corresponding fetal FC views with speckle noises, artifacts, and other ultrasonic characteristics. The experimental results show that the fetal FC views synthesized by our proposed PSFFGAN have the best objective evaluation values: SSIM of 0.4627, MS-SSIM of 0.6224, and FID of 83.92, respectively. More importantly, two professional cardiologists evaluate healthy FC views and CHD FC views synthesized by our PSFFGAN, giving a subjective score that the average qualified rate is 82% and 79%, respectively, which further proves the effectiveness of the PSFFGAN.","2168-2208","","10.1109/JBHI.2022.3143319","National Natural Science Foundation of China(grant numbers:61873281); the Major Science and Technology Innovation Project of Shandong Province(grant numbers:2019TSLH0214); the Tai Shan Industry Leading Talent Project(grant numbers:tscy20180416); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9682549","Fetal four-chamber views;Fetal fourchamber sketch images;Congenital heart disease;Deep) learning;GAN;Images synthesis","Generative adversarial networks;Generators;Deep learning;Image synthesis;Training;Speckle;Feature extraction","","","","1","","","IEEE","14 Jan 2022","","","IEEE","IEEE Early Access Articles"
"Laser Engraver Control System based on Reinforcement Adversarial Learning","E. Nikolaev","Institute of Information Technologies and Telecommunications, North-Caucasus Fedral University, Stavropol, Russia","2019 International Russian Automation Conference (RusAutoCon)","14 Oct 2019","2019","","","1","5","In recent years, deep neural networks have demonstrated high efficiency in solving the problems associated with image processing and analysis. The latest studies in generative adversarial neural networks open up broad prospects for solving problems in image style transfer, cross-domain adaptation, and the generation of new data for a target probability distribution. The ability of generative networks to implement complex and weakly formalized transformations allows the information systems based on generative adversarial technologies to solve problems in machine learning in a manner similar to human creative thinking. This paper presents the results of an experimental study on the design, training and deployment of an intelligent control system for a CNC laser engraver based on the generative adversarial network. In order to train deep models, large amounts of data are required. Therefore, one of the main challenges in this experimental study is a lack of labelled training dataset. The proposed control system for the laser engraver based on the deep generative model allows decreasing the time of the manufacturing process.","","978-1-7281-0265-8","10.1109/RUSAUTOCON.2019.8867762","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867762","generative adversarial network;reinforcement learning;intelligent control system;deep learning","Training;Generators;Laser modes;Neural networks;Computer numerical control;Generative adversarial networks;Image synthesis","image processing;intelligent control;learning (artificial intelligence);neural nets;probability;statistical distributions","reinforcement adversarial learning;laser engraver control system;deep generative model;labelled training dataset;deep models;generative adversarial network;CNC laser engraver;intelligent control system;human creative thinking;machine learning;generative adversarial technologies;information systems;weakly formalized transformations;complex transformations;generative networks;target probability distribution;cross-domain adaptation;image style transfer;broad prospects;generative adversarial neural networks;image processing;deep neural networks","","1","","28","IEEE","14 Oct 2019","","","IEEE","IEEE Conferences"
"Image Classification with Additional Non-decision Labels using Self-supervised learning and GAN","T. Hatano; T. Tsuneda; Y. Suzuki; K. Shintani; S. Yamane","Kanazawa University, Kanazawa, Japan; Kanazawa University, Kanazawa, Japan; Kanazawa University, Kanazawa, Japan; Kanazawa University, Kanazawa, Japan; Kanazawa University, Kanazawa, Japan","2020 Eighth International Symposium on Computing and Networking Workshops (CANDARW)","22 Feb 2021","2020","","","125","129","In recent years, image recognition has been improved by using machine learning to extract high accuracy. However, in most studies, it is assumed that the image input to a model has an answer. But then, when an unanswered image or an image of extremely low quality is entered into real space, the model is forced to answer and often makes a mistake. In this paper, we propose a method to discriminate between unscrutinized images using self-supervised learning and an image generation model using a subtask.","","978-1-7281-9919-1","10.1109/CANDARW51189.2020.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355844","Self-Supervised-Learning;Generative adversarial network;Deep Neural Network;Image classification","Learning systems;Image recognition;Image synthesis;Neural networks;Machine learning;Generative adversarial networks;Gallium nitride","image classification;learning (artificial intelligence);neural nets","machine learning;image input;unanswered image;extremely low quality;unscrutinized images;self-supervised learning;image generation model;image classification;nondecision labels;GAN;image recognition","","1","","8","IEEE","22 Feb 2021","","","IEEE","IEEE Conferences"
"Generative Adversarial Network ""Steerability"" for Brain PET Image Generation","J. Penning; R. John; H. Chandler; P. Fielding; C. Marshall; R. Smith","Wales Research and Diagnostic PET Imaging Centre (PETIC), Cardiff University, Cardiff, UK; Wales Research and Diagnostic PET Imaging Centre (PETIC), Cardiff University, Cardiff, UK; Cardiff Universities Brain Research Imaging Centre (CUBRIC); Wales Research and Diagnostic PET Imaging Centre (PETIC), Cardiff University, Cardiff, UK; Wales Research and Diagnostic PET Imaging Centre (PETIC), Cardiff University, Cardiff, UK; Wales Research and Diagnostic PET Imaging Centre (PETIC), Cardiff University, Cardiff, UK","2021 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)","9 Sep 2022","2021","","","1","4","An investigation was carried out in which the latent space distribution of a Wasserstein generative adversarial network was varied to observe whether outputs of a particular image classification could be specified. The WGAN was trained using 18F FDG Brain PET image scans of 96 patients with varying degrees / classes of neurodegeneration before being initialized with a chosen probability distribution. At first, the mean of a normal Gaussian distribution with a standard deviation of 1 was varied between -1 and +1 before a beta distribution was utilized with varying parameters. When α = β = 0.5, it was found that the generator was more likely to produce brain images with less neurodegeneracy compared to the normal Gaussian. Initiating the generator with a beta distribution of parameters α = 1, β = 5 gave a higher average EMCI SSIM score over five generative iterations.","2577-0829","978-1-6654-2113-3","10.1109/NSS/MIC44867.2021.9875600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9875600","","Brain;Image synthesis;Gaussian distribution;Generative adversarial networks;Generators;Probability distribution;Standards","brain;Gaussian distribution;image classification;medical image processing;neural nets;neurophysiology;positron emission tomography","latent space distribution;Wasserstein generative adversarial network;image classification;18F FDG Brain PET image scans;probability distribution;normal Gaussian distribution;beta distribution;brain PET image generation;EMCI SSIM;neurodegeneration;temperature 18.0 F","","1","","10","IEEE","9 Sep 2022","","","IEEE","IEEE Conferences"
"Clothes-Changing Image Generation Based on Attention for Person Re-identification","C. Tang; J. Guo","School of electronic information and electrical engineering, Shanghai Jiao Tong University, Shanghai, China; School of electronic information and electrical engineering, Shanghai Jiao Tong University, Shanghai, China","2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)","13 May 2021","2020","","","2009","2013","Person re-identification is an important issue in the field of video surveillance security. However, the differences of pedestrians under different cameras bring challenges. The existing person re-identification methods mainly focus on the person's posture and feature extraction, but do not pay much attention to the high-frequency clothes changing for one person. Therefore, the study of person clothes-changing images is of great help to person re-identification. In this paper, we introduce a network which fuses feature separation and self-attention jointly in a generative adversarial network for image generation of person clothes. We evaluate this model based on self-attention on a benchmark dataset: Market-1501. The experiment results validate the advances of attention mechanism by comparing with state-of-the-art baselines. Based on the generated datasets, we introduce a network containing channel attention and spatial attention which focus on local regions of clothes changes in a convolutional neural network for image recognition. We evaluate this network on our generative dataset. The experiment results show that joint attention network outperforms the baseline.","","978-1-6654-2314-4","10.1109/ICMCCE51767.2020.00439","National Key Research and Development Program of China(grant numbers:2017YFB1002401); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9421539","person re-identification;GAN;attention mechanism;clothes changing of person","Image recognition;Image synthesis;Fuses;Generative adversarial networks;Video surveillance;Feature extraction;Cameras","cameras;convolutional neural nets;feature extraction;image motion analysis;image recognition;video surveillance","clothes-changing image generation;video surveillance security;person re-identification methods;high-frequency clothes;person clothes-changing images;generative adversarial network;channel attention;spatial attention;generative dataset;joint attention network;image recognition;convolutional neural network","","1","","18","IEEE","13 May 2021","","","IEEE","IEEE Conferences"
"An Feature Image Generation Based on Adversarial Generation Network","M. Gong; Y. Wang","The School of computer science and engineering, Central South University, ChangSha, China; The School of computer science and engineering, Central South University, ChangSha, China","2020 12th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)","30 Mar 2020","2020","","","479","482","Generative antagonistic network (GAN) was proposed in 2014 to assist in generating realistic visual images, which has become one of the most popular research objects in deep learning in recent years. In the field of image generation, GAN is more effective than the traditional method, but it is difficult to train, unstable network and difficult to convergence. In this paper, GAN is applied in the field of feature image generation, and a new framework is proposed based on the C-SEGAN. By adding additional condition features to generator and discriminator, the similarity of distributed error is learned, and the discriminator is self-encoder, the mean square error loss is added to discriminator, and the generated model generates the specified sample. The model can generate the specified clear image according to the feature conditions. The experimental results show that the method can achieve faster convergence rate and generate better quality and diversity images with a simpler network than other supervised class generation models.","2157-1481","978-1-7281-7081-7","10.1109/ICMTMA50254.2020.00109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050299","GAN;Feature image generation;Supervised learning","Representation learning;Image quality;Visualization;Mechatronics;Image synthesis;Mean square error methods;Generative adversarial networks","feature extraction;image processing;mean square error methods;supervised learning","feature image generation;adversarial generation network;generative antagonistic network;GAN;realistic visual images;generated model;feature conditions;diversity images;supervised class generation models;deep learning;condition features;mean square error loss","","1","","10","IEEE","30 Mar 2020","","","IEEE","IEEE Conferences"
"Sparsity Improves Unsupervised Attribute Discovery in Stylegan","S. Liu; R. Anirudh; J. J. Thiagarajan; P. -T. Bremer","Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory; Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory; Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory; Center for Applied Scientific Computing (CASC), Lawrence Livermore National Laboratory","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","3388","3392","Rich semantics exist in latent spaces inferred using deep generative models. The ability to extract and interpret them is not only essential for understanding the underlying factors of variation in the data distribution, but also crucial for con-trolled image generation. Several methods have been proposed to identify semantically meaningful linear directions, either through existing annotations, or relying on identifying directions of large variation that arise from the data representation of the network. In this paper, we identify a new criterion, representation sparsity, that allows us to produce extremely efficient yet diverse semantic directions in GAN (generative adversarial network) latent spaces. The observation also reveals a potential deeper connection between representation sparsity and semantics in deep neural networks that worth further exploration.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746150","Lawrence Livermore National Laboratory; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746150","StyleGAN;semantic discovery;XAI;sparsity;deep learning","Deep learning;Image synthesis;Conferences;Semantics;Neural networks;Signal processing;Generative adversarial networks","data mining;deep learning (artificial intelligence);image representation;unsupervised learning","representation sparsity;GAN latent spaces;deep neural networks;styleGAN;deep generative models;semantic discovery;data distribution;controlled image generation;data representation;unsupervised attribute discovery;generative adversarial network latent spaces","","","","20","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Parallel Generative Adversarial Network for Third-person to First-person Image Generation","G. Liu; H. Latapie; O. Kilic; A. Lawrence","Emerging Technologies and Incubation, Cisco Research, USA; Emerging Technologies and Incubation, Cisco Research, USA; Emerging Technologies and Incubation, Cisco Research, USA; Emerging Technologies and Incubation, Cisco Research, USA","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","23 Aug 2022","2022","","","1916","1922","Cross-view image generation has been recently proposed to generate images of one view from another dramatically different view. In this paper, we investigate third-person (exocentric) view to first-person (egocentric) view image generation. This is a challenging task since egocentric view sometimes is remarkably different from exocentric view. Thus, transforming the appearances across the two views is a non-trivial task. To this end, we propose a novel Parallel Generative Adversarial Network (P-GAN) with a novel cross-cycle loss to learn the shared information for generating egocentric images from exocentric view. We also incorporate a novel contextual feature loss in the learning procedure to capture the contextual information in images. Extensive experiments on the Exo-Ego datasets [5] show that our model outperforms the state-of-the-art approaches.","2160-7516","978-1-6654-8739-9","10.1109/CVPRW56347.2022.00208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857267","","Computer vision;Image synthesis;Conferences;Computational modeling;Generative adversarial networks;Pattern recognition;Task analysis","feature extraction;image capture;learning (artificial intelligence);neural nets;object detection;user interfaces","cross-cycle loss;egocentric images;exocentric view;cross-view image generation;first-person view image generation;egocentric view;parallel generative adversarial network;P-GAN;contextual feature loss;Exo-Ego datasets;third-person view image generation","","","","45","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"On the Tradeoff between Computation-Time and Learning-Accuracy in GAN-based Super-Resolution Deep Learning","J. Shim; J. Kim; J. Kim","School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Electrical Engineering, Korea University, Seoul, Republic of Korea; School of Electrical Engineering, Korea University, Seoul, Republic of Korea","2021 International Conference on Information Networking (ICOIN)","2 Feb 2021","2021","","","422","424","The trade-off between accuracy and computation should be considered when applying generative adversarial network (GAN)-based image generation to real-world applications. This paper presents a simple yet efficient method based on Progressive Growing of GANs (PGGAN) to exploit the trade-off for image generation. The scheme is evaluated using the LSUN dataset.","1976-7684","978-1-7281-9101-0","10.1109/ICOIN50884.2021.9333991","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333991","","Deep learning;Image synthesis;Superresolution;Generative adversarial networks;Task analysis;Gallium nitride","image resolution;learning (artificial intelligence);neural nets","GAN-based super-resolution deep learning;GAN-based image generatioN;generative adversarial network-based image generation;computation time","","","","14","IEEE","2 Feb 2021","","","IEEE","IEEE Conferences"
"Frontal Face Landmark Generation using GAN","A. Poddar; S. Gawade; P. Varpe; S. Bhagwat","Department of Information Technology, D.Y.Patil Deemed to be University, Navi Mumbai, MH, India; Department of Information Technology, D.Y.Patil Deemed to be University, Navi Mumbai, MH, India; Department of Information Technology, D.Y.Patil Deemed to be University, Navi Mumbai, MH, India; Department of Information Technology, D.Y.Patil Deemed to be University, Navi Mumbai, MH, India","2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)","16 Jun 2022","2022","","","1172","1177","In the area of video surveillance, there are many challenges identified due to illumination variation, pose variation, angular variation, occlusion etc for face identification. It is very crucial to identify face images from CCTV footages, uncontrolled environment because of low resolution, poor quality of camera and distance between camera and face. This leads to poor quality image generation and difficult to identify face. This paper proposes a novel system which handles pose variation of face. In this paper, GAN (Generative Adversarial Network) is used to generate landmark of frontal face from side view of the face image. 2D face can be generated based on the frontal face landmark we have generated. Experimental results show that frontal face landmark can be generated easily which is further helpful to generate front face.","","978-1-6654-9710-7","10.1109/ICAAIC53929.2022.9793189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9793189","Frontalization;Frontal face;Face detection;Landmark;Synthetize face","Image synthesis;Face recognition;Terrorism;Sociology;Lighting;Generative adversarial networks;Cameras","closed circuit television;face recognition;pose estimation;video surveillance","face identification;face image;poor quality image generation;GAN;Generative Adversarial Network;frontal face landmark generation;video surveillance;illumination variation;angular variation","","","","16","IEEE","16 Jun 2022","","","IEEE","IEEE Conferences"
"Network Malicious Traffic Identification Method Based On WGAN Category Balancing","A. Wang; Y. Ding","Academy for Cyberspace Security, Gansu University of Political Science and Law, Lanzhou, China; Academy for Cyberspace Security, Gansu University of Political Science and Law, Lanzhou, China","2021 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","25 Oct 2021","2021","","","1","6","Aiming at the problem of data imbalance when in using deep learning model for traffic recognition tasks, a method of using Wasserstein Generative Adversarial Network (WGAN) to generate minority samples based on the image of the original traffic data packets is proposed to achieve a small number of data categories expansion to solve the problem of data imbalance. Firstly, through data preprocessing, the original traffic PCAP data in the dataset is segmented, filled, and mapped into grayscale pictures according to the flow unit. Then, the balance of dataset is achieved by using traditional random over sampling, WGAN confrontation network generation technology, ordinary GAN generation technology and synthetic minority oversampling technology. Finally, public datasets USTC- TFC2016 and CICIDS2017 are adopted to classify the unbalanced dataset and the balanced dataset on classic deep model CNN, and three evaluation indicators of precision, recall, and f1 are applied to evaluate classification effect. Experimental results show that the dataset balanced by the WGAN model is better than the ordinary GAN generation method, traditional oversampling method and the synthesis of the minority class sampling technique method under the same classification model.","","978-1-6654-2918-4","10.1109/ICSPCC52875.2021.9564824","Graduate Research Innovation Project of Gansu University of Political Science and Law(grant numbers:2019040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564824","traffic expansion;Wasserstein Generative Adversarial Network;data balance;traffic identification","Training;Image segmentation;Image recognition;Image synthesis;Signal processing;Gray-scale;Generative adversarial networks","","","","","","12","IEEE","25 Oct 2021","","","IEEE","IEEE Conferences"
"Infrared Image Generation Algorithm Based on GAN and contrastive learning","H. Liu; L. Ma","Hubei Key Laboratory of Optical Information and Pattern Recognition, Wuhan Institute of Technology, Wuhan, China; Hubei Key Laboratory of Optical Information and Pattern Recognition, Wuhan Institute of Technology, Wuhan, China","2022 International Conference on Artificial Intelligence and Computer Information Technology (AICIT)","1 Nov 2022","2022","","","1","4","For the task of converting dimly lit, low luminance nighttime visible to infrared images, we propose a Contrastive Visible-Infrared Image Translation Network (CVIIT). To better distinguish and translate objects such as pedestrians and vehicles, we introduce an attention module based on class activation map in the generator and discriminator of the Generative Adversarial Network (GAN), which captures richer context information in the images. In addition, we introduce contrastive learning to align the generated images with the visible images in terms of content. Qualitative and quantitative experiments on a publicly available visible-Infrared image pairing dataset (LLVIP) show that the proposed method generates infrared images of significantly higher quality than other state-of-the-art image-to-image translation (I2IT) methods.","","978-1-6654-5087-4","10.1109/AICIT55386.2022.9930233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930233","Visible-Infrared Image Translation;contrastive learning","Image synthesis;Learning (artificial intelligence);Generative adversarial networks;Generators;Task analysis;Information technology","infrared imaging;learning (artificial intelligence);object detection;visual perception","GAN;contrastive learning;class activation map;generative adversarial network;visible images;state-of-the-art image-to-image translation;image generation algorithm;contrastive visible infrared image translation network;visible infrared image pairing dataset;infrared image generation algorithm;CVIIT;context information","","","","11","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Adaptive Deep Convolutional GAN for Fingerprint Sample Synthesis","O. Striuk; Y. Kondratenko","Intelligent Information Systems Department, Petro Mohyla Black Sea National University, Mykolaiv, Ukraine; Intelligent Information Systems Department, Petro Mohyla Black Sea National University, Mykolaiv, Ukraine","2021 IEEE 4th International Conference on Advanced Information and Communication Technologies (AICT)","6 Dec 2021","2021","","","193","196","Real biometric fingerprint samples belong to the category of personal data, and therefore their usage for deep learning model training may have certain limitations. Artificially generated fingerprint images do not relate to a real person and can be used freely (“privacy-friendly”). Synthesized fingerprint samples are of interest for applied research: biological (papillary lines structure and alteration), forensic (computer fingerprint identification, reconstruction, and restoration of damaged samples), technological (various methods of biometric security). Generation of artificial fingerprints that accurately reproduce the textural features of real fingerprints could be a difficult task. In this paper, we present a deep learning framework — Adaptive Deep Convolutional Generative Adversarial Network (ADCGAN) — that we have developed and researched, and which has demonstrated the ability to generate realistic fingerprint samples that are similar to real ones in terms of their feature spectrum. ADCGAN makes it possible to conduct fingerprint research, without restrictions related to the confidential nature of biometric data.","","978-1-6654-0618-5","10.1109/AICT52120.2021.9628978","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9628978","neural network;artificial intelligence;generative adversarial network;fingerprints;sample","Deep learning;Training;Image synthesis;Image matching;Biological system modeling;Fingerprint recognition;Generative adversarial networks","biometrics (access control);convolutional neural nets;data privacy;deep learning (artificial intelligence);fingerprint identification;image forensics;image reconstruction;image restoration","privacy-friendly;papillary lines structure;computer fingerprint identification;biometric security;fingerprint sample synthesis;personal data;deep learning model training;fingerprint images;adaptive deep convolutional GAN;adaptive deep convolutional generative adversarial network;artificial fingerprint generation;damaged sample reconstruction;damaged sample restoration;papillary lines alteration;biological research;forensic research;technological research;real biometric fingerprint samples","","","","18","IEEE","6 Dec 2021","","","IEEE","IEEE Conferences"
"VR-FAM: Variance-Reduced Encoder with Nonlinear Transformation for Facial Attribute Manipulation","Y. Yuan; S. Ma; J. Zhang","Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China","ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","27 Apr 2022","2022","","","1755","1759","Facial attribute manipulation (FAM) aims to infer desired facial images by modifying specific attributes while keeping others unchanged. Existing works suffer from the entanglement of facial attributes, leading to unexpected artifacts and the loss of facial identity information after editing. To alleviate these issues, we propose a novel FAM framework based on StyleGAN, termed VR-FAM, which can meet the requirements of FAM—editing ability, distortion, and fidelity. First, we propose a variance-reduced encoder to make the latent space close to the one of StyleGAN. Second, we present a nonlinear latent transformation network, which can convert the source latent code to target latent code in line with the nonlinear latent space of StyleGAN. Experimentally, we evaluate the proposed FAM framework on the benchmark FFHQ dataset and demonstrate the improvement gain over the recently published models in terms of edit accuracy and fidelity.","2379-190X","978-1-6654-0540-9","10.1109/ICASSP43922.2022.9746046","National Natural Science Foundation of China; National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9746046","Facial attribute manipulation;disentangle learning;StyleGAN;GANs;style transfer","Measurement;Codes;Image synthesis;Nonlinear distortion;Focusing;Generative adversarial networks;Generators","face recognition;image coding;neural nets","variance-reduced encoder;nonlinear transformation;facial attribute manipulation;facial images;facial identity information;StyleGAN;nonlinear latent transformation network;source latent code;nonlinear latent space;VR-FAM;FFHQ dataset","","","","21","IEEE","27 Apr 2022","","","IEEE","IEEE Conferences"
"Generating face images from fine-grained sketches based on GAN with global-local joint discriminator","H. Gao; W. Mao; Y. Lin","School of Optoelectronic and Communication Engineering Xiamen University of Technology, Xiamen, China; School of Optoelectronic and Communication Engineering Xiamen University of Technology, Xiamen, China; School of Optoelectronic and Communication Engineering Xiamen University of Technology, Xiamen, China","2022 IEEE 2nd International Conference on Software Engineering and Artificial Intelligence (SEAI)","25 Jul 2022","2022","","","50","54","This paper explores the face image generating with clear details from fine-grained sketches. Edge maps are usually used as sketches in face image generation tasks. However, there are some problems such as discontinuous lines and a lack of detailed information in the edge map, so the generated face image is not clear enough and lacks details. To address this problem, a face sketch dataset with rich details is made. The discriminator combines a global discriminator and a local discriminator, which ensures that the generated face image has a complete face structure and generates clearer face details. A self-attention mechanism is employed in the generative model to establish long-term dependencies on partial feature information. The model is quantitatively evaluated by using the valuation criteria IS, FID, and KID. Among them, the evaluation scores of the model on IS, FID, and KID are 1.956 ± 0.053, 17.941 ± 0.970, and 0.009 ± 0.001, respectively. The evaluation results show that the model achieves good performance. Furthermore, the visual effects of the model are analyzed by comparing with the pix2pixHD and CycleGAN models on the generated images. The final results show that our model outperforms the other two models and can generate high-quality face images with sharp details.","","978-1-6654-8223-3","10.1109/SEAI55746.2022.9832103","Natural Science Foundation of Fujian Province(grant numbers:2021J011216); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832103","image-to-image translation;fined-grained sketches;self-attention;global-local joint discriminator","Analytical models;Image synthesis;Image edge detection;Conferences;Generative adversarial networks;Visual effects;Task analysis","face recognition;feature extraction;neural nets;realistic images","complete face structure;clearer face details;generative model;high-quality face images;fine-grained sketches;global-local joint discriminator;face sketch dataset;global discriminator;local discriminator;face images generation;GAN;self-attention mechanism;partial feature information;long-term dependencies;visual effects;CycleGAN models;pix2pixHD models","","","","27","IEEE","25 Jul 2022","","","IEEE","IEEE Conferences"
"Pose-Driven Realistic 2-D Motion Synthesis","G. Xia; F. Ma; Q. Liu; D. Zhang","Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing 210044, China, and also with the Faculty of Information Technology, Macau University of Science and Technology, Macau, China.; Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing 210044, China.; Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing 210044, China (e-mail: qsliu@nuist.edu.cn); Faculty of Information Technology, Macau University of Science and Technology, Macau, China.","IEEE Transactions on Cybernetics","","2021","PP","99","1","14","A realistic 2-D motion can be treated as a deforming process of an individual appearance texture driven by a sequence of human poses. In this article, we thereby propose to transform the 2-D motion synthesis into a pose conditioned realistic motion image generation task considering the promising performance of pose estimation technology and generative adversarial nets (GANs). However, the problem is that GAN is only suitable to do the region-aligned image translation task while motion synthesis involves a large number of spatial deformations. To avoid this drawback, we design a two-step and multistream network architecture. First, we train a special GAN to generate the body segment images with given poses in step-I. Then in step-II, we input the body segment images as well as the poses into the multistream network so that it only needs to generate the textures in each aligned body region. Besides, we provide a real face as another input of the network to improve the face details of the generated motion image. The synthesized results with realism and sharp details on four training sets demonstrate the effectiveness of the proposed model.","2168-2275","","10.1109/TCYB.2021.3120010","Science and Technology Development Fund Macau SAR(grant numbers:0002/2019/APD); National Natural Science Foundation of China(grant numbers:61802198,61825601,U2001211,61773219); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180788); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583675","Body segment;conditional generative adversarial net (GAN);motion synthesis;person image generation;pose.","Generative adversarial networks;Task analysis;Image segmentation;Faces;Training;Image synthesis;Motion segmentation","","","","","","","IEEE","21 Oct 2021","","","IEEE","IEEE Early Access Articles"
"Image Classification and Generation Based on GAN Model","H. Meng; F. Guo","School of Electrical and Electronic Engineering, Huazhong University of Science and Technology, Wuhan, China; Faculty of Science, University of Calgary, Calgary, Canada","2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)","17 Mar 2022","2021","","","180","183","The topic of image processing is becoming more and more popular in the field of artificial intelligence, and it can be applied to fields of biology, medicine, video games, art, and etc. In order to have a deeper understanding of how to optimize the image processing, this paper mainly proposed the Generative Adversarial Network (GAN), which is an emerging deep learning model with the ability to continuously improve modeling under the game, and there are already many applications related to image processing, such as video prediction, 3-dimensional object generation, image super-resolution and etc. In this paper, we mainly implement image generation and image classification based on GAN model. In order to indicate the performance of GAN model in image generation in detail, GAN models with linear layers and with convolution layers are trained and compared based on MNIST datasets. Furthermore, we train GAN model with linear layers, and GAN model with convolution layers, Multi-Layer Perceptron (MLP), Convolutional Neural Network (CNN), and Residual Network (ResNet) models in image classification based on the Mixed National Institute of Standards and Technology database (MNIST), and receive the training loss and testing accuracy of these models for different epochs in image classification. The experimental results demonstrated that GAN model with convolution layers performs best in both image generation and image classification.","","978-1-6654-1790-7","10.1109/MLBDBI54094.2021.00042","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9731058","Image generation;GAN model;Image classification;Experimental analysis","Training;Convolution;Image synthesis;Games;Predictive models;Generative adversarial networks;Data models","feature extraction;image classification;image resolution;learning (artificial intelligence);multilayer perceptrons;neural nets","image classification;GAN model;image processing;emerging deep learning model;image super-resolution;image generation;convolution layers;Residual Network models","","","","13","IEEE","17 Mar 2022","","","IEEE","IEEE Conferences"
"Text-to-Clipart using AttnGAN","Y. Kondo; T. Sakura; T. Yamasaki","Dept. Information and Communication Engineering, The University of Tokyo, Tokyo, Japan; Dept. Information and Communication Engineering, The University of Tokyo, Tokyo, Japan; Dept. Information and Communication Engineering, The University of Tokyo, Tokyo, Japan","2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM)","20 Oct 2020","2020","","","282","286","The Attentional Generative Adversarial Network (AttnGAN) [1] is a state-of-the-art text-to-image generation model. One of the key factors of AttnGAN's success is the ability to evaluate the similarity between an input sentence and the generated image in the same feature space (Deep Attentional Multimodal Similarity Model, DAMSM). However, the network architecture of AttnGAN is complicated and vast, which necessitates considerable computational costs in the training process. When AttnGAN is applied to a text-to-image generation task in different image domains such as clipart, the output images are simpler than the high-resolution natural images that AttnGAN originally assumes. Therefore, we propose a lightweight AttnGAN aiming at reducing the training computational cost without compromising the quality of the generated images. In particular, we focus on the image encoder; replacing it from Inception-v3 to VGG-16 reduces the DAMSM training time by approximately half of the original implementation.","","978-1-7281-9325-0","10.1109/BigMM50055.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9232489","image generation;generative adversarial network;deep learning;clipart;illustration;attention","Training;Computational modeling;Gallium nitride;Generative adversarial networks;Task analysis;Feature extraction;Image synthesis","feature extraction;image coding;image resolution;learning (artificial intelligence);object detection;text analysis","training computational cost;image encoder;text-to-clipart;state-of-the-art text-to-image generation model;deep attentional multimodal similarity model;text-to-image generation task;different image domains;high-resolution natural images;lightweight AttnGAN;attentional generative adversarial network","","","","12","IEEE","20 Oct 2020","","","IEEE","IEEE Conferences"
"Semi-automatic image generation for neural networks datasets","G. Valentin; F. Osamu; O. Hiroshi; Y. Nobuhiko","Faculty of Science and Engineering, Saga University, Saga, Japan; Faculty of Science and Engineering, Saga University, Saga, Japan; Faculty of Science and Engineering, Saga University, Saga, Japan; Faculty of Science and Engineering, Saga University, Saga, Japan","2021 7th International Conference on Electrical, Electronics and Information Engineering (ICEEIE)","30 Nov 2021","2021","","","411","415","Machine Learning and Deep Learning algorithms require to gather a substantial amount of data to train on. Scarcity of relevant data can make it difficult to prepare networks efficiently, so the creation of useful data appears as an interesting path to explore. In our case, the sparse data that had to be generated were plastic parts with defects on them. In order to create synthetic irregularities, several individual defects were randomly picked, warped and placed on the pictures of flawless plastic pieces. A brief succession of experiments was carried out to create a script that would allow the generation of such data semi-automatically. The process was “semi-automatic” as flawless pieces had to be cut from their backgrounds, and defects from their surrounding textures, manually beforehand. Thanks to the mixture of data the scripts generate, datasets composed of artificial pictures can be used to train/test algorithms on data that would have been laborious to get otherwise. Experimentations proved that these synthetic data were viable to train CNNs and that these networks could process new data effectively..","","978-1-6654-3232-0","10.1109/ICEEIE52663.2021.9616771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616771","Neural network;Dataset;Data generation;Data augmentation","Deep learning;Machine learning algorithms;Image synthesis;Neural networks;Predictive models;Generative adversarial networks;Prediction algorithms","computer vision;edge detection;feature extraction;gradient methods;learning (artificial intelligence);medical computing;neural nets","semiautomatic image generation;neural networks datasets;interesting path;sparse data;plastic parts;synthetic irregularities;individual defects;flawless plastic pieces;script;data semiautomatically;flawless pieces;artificial pictures;synthetic data","","","","7","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Semantic GAN: Application for Cross-Domain Image Style Transfer","P. Li; M. Yang","School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China","2019 IEEE International Conference on Multimedia and Expo (ICME)","5 Aug 2019","2019","","","910","915","Image style transfer has attracted much attention from many fields and received promising performance. However, style transfer in the cross-domain field, e.g., the transfer between near-infrared and visible light images, is rarely studied. In the cross-domain image style transfer, one key issue is mismatching problem existing in the generated semantic regions. In this paper, we propose a novel model of Semantic GAN, which integrates the semantic guidance and the recent CycleGAN. In particular, we present a semantic style loss with Gram matrix to well preserve the semantic information in the generated images. The proposed Semantic GAN can control the transfer in the right way with semantic masks and solve the mismatching problem. We apply our approach to two outdoor scene datasets to evaluate the performance of all competing methods. The experimental results show that our approach outperforms previous methods in addressing the mismatching problem and providing a good quality result.","1945-788X","978-1-5386-9552-4","10.1109/ICME.2019.00161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784957","SemanticGAN;Style Transfer;Cross Domain;mismatching problem","Semantics;Gallium nitride;Image segmentation;Generative adversarial networks;Generators;Training;Image synthesis","feature extraction;image classification;image fusion;image motion analysis;image representation;image segmentation;image texture;neural nets;object detection;video signal processing","Semantic GAN;cross-domain image style transfer;cross-domain field;near-infrared images;visible light images;semantic guidance;semantic style loss;semantic information;semantic masks;CycleGAN","","","","16","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"Gradient-guided GAN for dynamic scene deblurring","Z. Huang; Y. Zhou; Y. Shi; J. Chen; T. Lai; C. Shao","school of eclectrical and information engineering, Wuhan Institute of Technology, Wuhan, China; school of eclectrical and information engineering, Wuhan Institute of Technology, Wuhan, China; school of eclectrical and information engineering, Wuhan Institute of Technology, Wuhan, China; school of eclectrical and information engineering, Wuhan Institute of Technology, Wuhan, China; Wuhan Research Institute of Posts and Telecommunications Co. LTD, Wuhan, China; school of eclectrical and information engineering, Wuhan Institute of Technology, Wuhan, China","2022 International Conference on Artificial Intelligence and Computer Information Technology (AICIT)","1 Nov 2022","2022","","","1","4","Dynamic scene blur, mainly caused by camera shake and motions, is one of the most common causes of image degradation. Recent GAN-based strategies have performance on deblurring tasks. To further improve the performance of GAN-based approaches on deblurring tasks, we propose Gradient-guided GAN for dynamic scene deblurring, it includes image restoration branch and gradient branch, which uses the gradient as a guide to supervise the restoration process. In particular, perform an attention fusion of feature image generated by restoration branch and gradient feature image generated by gradient branch, which using gradient information to guide the network to fully learn the deep feature information. Extensive experiments on GOPRO dataset show that our method achieve state-of-the-art performance in dynamic scene deblurring.","","978-1-6654-5087-4","10.1109/AICIT55386.2022.9930290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930290","Dynamic scene deblurring;generative adversarial network;gradient guide;feature fusion","Measurement;Degradation;Image synthesis;Dynamics;Generative adversarial networks;Cameras;Image restoration","convex programming;gallium compounds;gradient methods;image resolution;image restoration;image segmentation;neural nets","dynamic scene deblurring;camera shake;image degradation;deblurring tasks;GAN-based approaches;image restoration branch;gradient branch;feature image;gradient information;GAN-based strategies;gradient-guided GAN;GOPRO dataset","","","","11","IEEE","1 Nov 2022","","","IEEE","IEEE Conferences"
"Deep Learning based Facility Drain Inspection using Class Balancing with Synthesized Image Generation","A. Lee; M. Abdullah; J. SeoHyun; D. Han","Department of Computer Engineering, Sejong University; Department of Computer Engineering, Sejong University; Department of Computer Engineering, Sejong University; Department of Computer Engineering, Sejong University","2022 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)","28 Nov 2022","2022","","","1","3","While aging facilities are rapidly increasing, there is a problem that safety management is disrupted due to insufficient manpower and time required for facility inspection. Currently, research on the development of a system that inspects facilities based on deep learning algorithms is increasing. In this paper, we propose an object-detection based inspection algorithm for gravity and siphonic drains on the roof of industrial complexes. It is difficult to find appropriate abnormal class samples for training. Therefore, a problem of class imbalance arises between normal and abnormal samples. To solve this problem, we generate abnormal class synthetically using image in-painting technique. The results show the effectiveness of approach used in this project.","","978-1-6654-6434-5","10.1109/ICCE-Asia57006.2022.9954766","Ministry of Science & ICT(grant numbers:2021-0-01072); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954766","Facility Inspection;Object Detection;Class Balancing","Training;Deep learning;Image synthesis;Safety management;Inspection;Aging;Generative adversarial networks","inspection;learning (artificial intelligence);object detection","abnormal samples;appropriate abnormal class samples;class balancing;class imbalance;deep learning algorithms;facility drain inspection;facility inspection;image in-painting technique;industrial complexes;insufficient manpower;normal samples;object-detection based inspection algorithm;safety management;siphonic drains;synthesized image generation","","","","8","IEEE","28 Nov 2022","","","IEEE","IEEE Conferences"
"Image Colored-Pencil-Style Transformation Based on Generative Adversarial Network","H. Chen; U. Kin Tak","Faculty of Information Technology, Macau University of Science and Technology, Macau, China; Faculty of Information Technology, Macau University of Science and Technology, Macau, China","2020 International Conference on Wavelet Analysis and Pattern Recognition (ICWAPR)","28 Jul 2021","2020","","","90","95","Generative Adversarial Network (GAN) has an excellent performance in “ Image-to-Image Translation” field, this paper proposes an effective algorithm to solve the problem of image colored-pencil-style transformation based on Pix2Pix of GAN. The improved generator and discriminator are designed to be suitable for high resolution image generation. In the loss function, TV Loss is using for improving the smoothness of images and removing noise and overlapping shadows. In order to adapt to the practical application requirement of colored-pencil-style transformation, our data set comes from Flickr, the world's largest photo-sharing site, and includes high-resolution images of different types. The experimental results show that our proposed algorithm not only improves the resolution of images, but also makes the details clearer. Moreover, in all terms of objective indicators, our proposed algorithm is superior to Pix2Pix in different categories of data set. Therefore, through the comparison in visual subjectively and in indicators objectively, it is proved that the proposed algorithm has better performance than Pix2Pix.","2158-5709","978-1-7281-9985-6","10.1109/ICWAPR51924.2020.9494617","Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9494617","Colored pencil style;Style transfer;Generative Adversarial Network","Visualization;TV;Image resolution;Image synthesis;Multimedia Web sites;Generative adversarial networks;Wavelet analysis","edge detection;feature extraction;image colour analysis;image resolution;image texture;realistic images;rendering (computer graphics)","image colored-pencil-style transformation;generative Adversarial Network;Generative Adversarial Network;GAN;Image-to-Image Translation field;Pix2Pix;improved generator;high resolution image generation;removing noise;high-resolution images","","","","21","IEEE","28 Jul 2021","","","IEEE","IEEE Conferences"
"Face Generation using DCGAN for Low Computing Resources","W. Liu; Y. Gu; K. Zhang","School of Automation, Guangdong University of Technology, Guangzhou, China; School of Computing and Information Systems, The University of Melbourne, Parkville, Victoria, Australia; School of Software Engineering, Central University for Nationalities, Beijing, China","2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)","3 Feb 2022","2021","","","377","382","Image generation is the task of generating brand new images from existing datasets. With the increasing development of the Generative Adversarial Network (GAN), it is now widely used in the field of image generation. Research based on its time cost and the convergence of its training process has been widely concerned. However, existing techniques for network convergence during the training process usually require high computing resources. This paper has used Deep Convolutional Generative Adversarial Network (DCGAN) on the CelebA dataset for experimental evaluation: The number of training epochs and the algorithm’s parameters is adjusted to achieve a balance between the quality of generation results and computing resources used. This is assumed to be convenient for future studies under the condition of limited computing resources. Besides, by adjusting the parameters of the optimization algorithm, the convergence of GAN under the conditions of a specific optimization algorithm is studied.","","978-1-6654-2709-8","10.1109/ICBASE53849.2021.00076","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696138","component;Image generation;DCGAN;Adam;Deep learning","Training;Costs;Image synthesis;Focusing;Generative adversarial networks;Generators;Task analysis","convolutional neural nets;deep learning (artificial intelligence);face recognition;optimisation;parallel processing","face generation;DCGAN;low computing resources;image generation;brand new images;GAN;deep convolutional generative adversarial network;network convergence;high computing resources;CelebA dataset;training epochs;optimization algorithm","","","","11","IEEE","3 Feb 2022","","","IEEE","IEEE Conferences"
"Does Generative Adversarial Network (GAN) help in SRAF image generation?","J. Huang; Y. Huang; Y. -t. Lin; Z. -y. Liu; Y. Lin; W. Wu","ASML-Brion, Shenzhen, China; ASML-Brion, Shenzhen, China; ASML-Brion, Shenzhen, China; ASML-Brion, Shenzhen, China; ASML-Brion, Shenzhen, China; The College of Electronics and Information Engineering, Shenzhen University, Shenzhen, China","2021 International Workshop on Advanced Patterning Solutions (IWAPS)","14 Jan 2022","2021","","","1","4","We propose a (Deep Neural Network) DNN based method that applying Generative Adversarial Network (GAN) to the SRAF image generation. We found that though the model with GAN loss may increase distortion, it can generate high fidelity images efficiently and mitigate the overfitting issues that widely exists in all kinds of deep learning methods. We also discuss the necessity of tuning the model structure when GAN loss is adopted.","","978-1-6654-2079-2","10.1109/IWAPS54037.2021.9671262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9671262","SRAF;Deep Learning;GAN","Deep learning;Image synthesis;Conferences;Neural networks;Generative adversarial networks;Distortion;Tuning","deep learning (artificial intelligence);image resolution;neural nets","generative adversarial network;SRAF image generation;GAN loss;high fidelity images;deep learning methods;deep neural network;DNN based method;model structure","","","","11","IEEE","14 Jan 2022","","","IEEE","IEEE Conferences"
"Fine-Grained Image Generation from Bangla Text Description using Attentional Generative Adversarial Network","M. A. Haque Palash; M. A. Al Nasim; A. Dhali; F. Afrin","Department of Research and Development, Pioneer Alpha, Dhaka, Bangladesh; Department of Research and Development, Pioneer Alpha, Dhaka, Bangladesh; Department of Research and Development, Pioneer Alpha, Dhaka, Bangladesh; Department of Research and Development, Pioneer Alpha, Dhaka, Bangladesh","2021 IEEE International Conference on Robotics, Automation, Artificial-Intelligence and Internet-of-Things (RAAICON)","2 Nov 2022","2021","","","79","84","Generating fine-grained, realistic images from text has many applications in the visual and semantic realm. Considering that, we propose Bangla Attentional Generative Adversarial Network (AttnGAN) that allows intensified, multi-stage processing for high-resolution Bangla text-to-image generation. Our model can integrate the most specific details at different sub-regions of the image. We distinctively concentrate on the relevant words in the natural language description. This framework has achieved a better inception score of $3.58\pm .06$ on the CUB dataset. For the first time, a fine-grained image is generated from Bangla text using attentional GAN. Bangla has achieved 7th position among 100 most spoken languages. This inspires us to explicitly focus on this language, which will ensure the inevitable need of many people. Moreover, Bangla has a more complex syntactic structure and less natural language processing resource that validates our work more.","","978-1-6654-7869-4","10.1109/RAAICON54709.2021.9929536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9929536","GAN;Bangla;text-to-image;Fine-grained image","Visualization;Automation;Image synthesis;Semantics;Syntactics;Generative adversarial networks;Natural language processing","image classification;natural language processing;natural languages;text analysis","natural language description;attentional GAN;complex syntactic structure;less natural language processing resource;fine-grained image generation;bangla text description;fine-grained images;realistic images;visual realm;semantic realm;Bangla Attentional Generative Adversarial Network;multistage processing;high-resolution Bangla text-to-image generation","","","","29","IEEE","2 Nov 2022","","","IEEE","IEEE Conferences"
"High Resolution Image Generation Using Learning Super-Resolution for Low Resolution Images","K. Nakayama; T. Goto","Dept. of Computer Science, Nagoya Institute of Technology, Nagoya, Japan; Dept. of Computer Science, Nagoya Institute of Technology, Nagoya, Japan","2022 4th International Conference on Computer Communication and the Internet (ICCCI)","9 Aug 2022","2022","","","83","86","Cameras have been installed in a variety of locations. This is to record evidence in case of trouble. However, many of these images have low resolution, which reduces the effectiveness of the evidence. In particular, license plate images can identify the parties involved, so it is important to read the characters. In this paper, we aim to improve the resolution of license plate images by using super-resolution with adversarial learning method.","","978-1-6654-6992-0","10.1109/ICCCI55554.2022.9850252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9850252","super-resolution;generative adversarial network","Image synthesis;Superresolution;License plate recognition;Generative adversarial networks;Cameras;Adversarial machine learning;Internet","cameras;image recognition;image resolution;learning (artificial intelligence)","adversarial learning method;high resolution image generation;super-resolution;low resolution images;license plate images","","","","5","IEEE","9 Aug 2022","","","IEEE","IEEE Conferences"
"Development of an Image Generation System based on a Depth Map","A. P. Losev; D. A. Tatarenkov; E. I. Tumanova","Master Student of Informational Systems and Technologies Faculty, The Bonch-Bruevich Saint-Petersburg State University of Telecommunications, Saint Petersburg, Russian Federation; Senior Lecturer of TV and Metrology Department, The Bonch-Bruevich Saint-Petersburg State University of Telecommunications, Saint Petersburg, Russian Federation; Senior Lecturer of TV and Metrology Department, The Bonch-Bruevich Saint-Petersburg State University of Telecommunications, Saint Petersburg, Russian Federation","2022 Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus)","20 Apr 2022","2022","","","53","56","Image processing and generation has always been an important part of broadcasting. There is color correction, creating test images, keying and other objectives in the process of video production. The task of generating new images is time consuming. Technology can provide wide opportunities for data operating due to progress in neural network development, which provide ample opportunities for processing and generating images. This research is focused on working with generative adversarial network (GAN) trained on a specific dataset based on the depth maps. The end result is an AI-generated map based on a heightmap by using lidar or ToF camera datasets.","2376-6565","978-1-6654-0993-3","10.1109/ElConRus54750.2022.9755716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9755716","image processing;GAN;depth map","Laser radar;Image synthesis;Image color analysis;Neural networks;Production;Broadcasting;Generative adversarial networks","artificial intelligence;cameras;image colour analysis;neural nets;video signal processing","image generation system;depth map;image processing;color correction;test images;video production;neural network development;generative adversarial network;AI-generated map;GAN;heightmap;LIDAR datasets;ToF camera datasets","","","","10","IEEE","20 Apr 2022","","","IEEE","IEEE Conferences"
"Generation of SAR Images with Features for Target Recognition","G. Peng; M. Liu; S. Chen; Y. Li; F. Lu","School of Computer Science, Shaanxi Normal University, Xi’an, China; School of Computer Science, Shaanxi Normal University, Xi’an, China; Xi’an Modern Control Technology Research Institute, China, Xi’an, China; North Automatic Control Technology Institute, Taiyuan, China; Xi’an Modern Control Technology Research Institute, China, Xi’an, China","2022 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","23 Dec 2022","2022","","","1","4","Since it is difficult to obtain a large number of the real samples of SAR images, the accuracy of synthetic aperture radar automatic target recognition (SAR-ATR) based on deep learning is often affected by the lack of real samples. Generative adversarial network (GAN) is a method that can effectively generate samples to expand dataset. This paper proposes a GAN that adds a condition to guide image generation and modifies the true and false discriminator to a discriminator with classification (DwC). In addition to correctly recognize the real SAR images, DwC recognizes the generated images as the class N + 1. In order to make the generated images recognized as the real images by DwC, the conditional generator gradually learns to generate the images with features of a specific category. Applying the SAR images generated by our model to target recognition based on deep learning can effectively improve the accuracy.","","978-1-6654-6972-2","10.1109/ICSPCC55723.2022.9984374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9984374","synthetic aperture radar automatic target recognition (SAR-ATR);generation adversarial network (GAN);discriminator with classification (DwC)","Deep learning;Image recognition;Target recognition;Image synthesis;Signal processing;Generative adversarial networks;Radar polarimetry","image recognition;learning (artificial intelligence);radar imaging;radar target recognition;synthetic aperture radar","conditional generator;deep learning;DwC;generative adversarial network;image generation;SAR images;SAR-ATR;synthetic aperture radar automatic target recognition","","","","9","IEEE","23 Dec 2022","","","IEEE","IEEE Conferences"
"Object Detection with Dataset Augmentation for Fire Images Based on GAN","H. Lee; S. Kang; K. Chung",School of Computer Information and Engineering; Department of Electronics and Communications Engineering; Department of Electronics and Communications Engineering,"2022 13th International Conference on Information and Communication Technology Convergence (ICTC)","25 Nov 2022","2022","","","2118","2123","Objection detection is the task to find and classify objects in images. Many object detection models based on a deep learning algorithm have been proposed. Deep learning algorithms require the models to be trained with affluent images with accurate annotations. However, in the case of fire detection, neither enough datasets to train the detection model nor correct and sufficient annotations exist. In this paper, we propose a GAN-based model to generate fire images with bounding boxes to enhance the performance of the fire detection model. Through the experiments, we demonstrated that the model can inject flame images into the clean images within specified areas and the generated images are enough to augment the fire detection dataset so that the model's object detection performance can be improved.","2162-1241","978-1-6654-9939-2","10.1109/ICTC55196.2022.9952972","Institute for Information & Communications Technology Planning & Evaluation (IITP); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952972","Generative Adversarial Network;Image-to-Image Translation;Dataset Augmentation;Object Detection","Deep learning;Annotations;Image synthesis;Fires;Object detection;Generative adversarial networks;Classification algorithms","deep learning (artificial intelligence);image classification;object detection","affluent images;clean images;dataset augmentation;deep learning algorithm;fire detection dataset;fire detection model;fire images;flame images;GAN-based model;object detection models;object detection performance;objection detection","","","","19","IEEE","25 Nov 2022","","","IEEE","IEEE Conferences"
"Expressive And Compressive Gan Inversion Network","Y. Xu; J. Zhang","School of Electronic and Computer Engineering, Peking University, China; School of Electronic and Computer Engineering, Peking University, China","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","3742","3746","This paper presents a novel encoding framework for the generative adversarial network (GAN), named compressive GAN inverter (Comp-GI), which can effectively compress, restore, and edit the attributes of real-world images. In particular, Comp-GI first transforms the input image into the latent variable space of GAN through an encoder network and then manipulates the latent code vector to enable various semantic image editing operations. StyleGAN [1] and its improved version StyleGANv2 [2] are adopted as the default GANs for image generation. Experiments on benchmark datasets FFHQ demonstrate that the proposed Comp-GI achieves better restoration performance with finer details and fewer artifacts than existing state-of-the-art GAN-inversion methods.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506059","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506059","GAN Inversion;Image-to-Image Translation;Image Compression;StyleGAN;GAN Dissection","Image coding;Image synthesis;Conferences;Semantics;Transforms;Benchmark testing;Generative adversarial networks","data compression;encoding;greedy algorithms;image coding;image representation;neural nets","novel encoding framework;generative adversarial network;named compressive GAN inverter;real-world images;input image;latent variable space;encoder network;latent code vector;semantic image editing operations;default GANs;image generation;StyleGANv2;Comp-GI","","","","22","IEEE","23 Aug 2021","","","IEEE","IEEE Conferences"
"Multi-Category SAR Images Generation Based on Improved Generative Adversarial Network","S. Du; J. Hong; Y. Wang; K. Xing; T. Qiu","National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China; National Key Laboratory of Science and Technology on Microwave Imaging, Aerospace Information Research Institute, Chinese Academy of Sciences, China","2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS","12 Oct 2021","2021","","","4260","4263","The generative adversarial network (GAN) provides a different way for SAR data augmentation. The traditional GAN model is mainly based on the Jensen-Shannon (JS) divergence or Wasserstein distance. The former faces mode collapse, while the latter is not suitable for multi-category image generation. In this paper, an improved model based on WGAN-GP is proposed. An encoder is used to learn the features of real samples as the input of the generator to control training to a certain extent and make the generated image quality better. In addition, a pre-trained classifier is introduced as the constraint of the generator to ensure the generated images have the correct category information. MSTAR dataset is used to verify the generation capability of the proposed model. The results show that the proposed model has the stable generation capability to provide high-quality SAR images as a supplementary training dataset, which could assist in achieving good classification accuracy.","2153-7003","978-1-6654-0369-6","10.1109/IGARSS47720.2021.9554120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9554120","P-band;calibration;BIOMASS;parabolic antenna;target of opportunity","Training;Image quality;Image synthesis;Geoscience and remote sensing;Generative adversarial networks;Generators;Radar polarimetry","image classification;learning (artificial intelligence);maximum likelihood estimation;pattern classification;radar imaging;synthetic aperture radar","SAR data augmentation;traditional GAN model;faces mode collapse;multicategory image generation;generated image quality;pre-trained classifier;correct category information;stable generation capability;high-quality SAR images;supplementary training dataset;multicategory SAR images generation;improved generative adversarial network","","","","7","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"Unpaired Image-to-Image Translation Using Negative Learning for Noisy Patches","Y. -H. Hung; J. Tan; T. -M. Huang; S. -C. Hsu; Y. -L. Chen; K. -L. Hua","National Taiwan University of Science and Technology, Taipei, Taiwan; National Taiwan University of Science and Technology, Taipei, Taiwan; National Taiwan University of Science and Technology, Taipei, Taiwan; National Taiwan University of Science and Technology, Taipei, Taiwan; National Taiwan University of Science and Technology, Taipei, Taiwan; National Taiwan University of Science and Technology, Taipei, Taiwan","IEEE MultiMedia","5 Jan 2023","2022","29","4","59","68","Unpaired image-to-image translation finds a mapping between two domains that do not have paired data. One approach is patchwise contrastive learning, a one-sided translation that maximizes mutual information between corresponding input and output patches. Noncorresponding patches are treated as negatives. Previous approaches randomly select noncorresponding patches, resulting in semantically similar patches incorrectly labeled as negatives. Inspired by negative learning, we propose the novel patchwise negative learning loss to address this issue. We do not naively minimize mutual information between all noncorresponding ones, unlike prior methods. Instead, we choose one noncorresponding patch and maximize dissimilarity with the query patch. The selected noncorresponding patch reduces the chance of choosing false negatives that contain high mutual information. By further maximizing dissimilarity with that single negative, we discourage our model from fitting on noisy negative patches. We demonstrate the capabilities of our model against other prominent image translation methods on the selfie2anime, horse2zebra, and cat2dog datasets.","1941-0166","","10.1109/MMUL.2022.3177452","Ministry of Science and Technology Taiwan(grant numbers:109-2221-E-011-125-MY3,109-2923-E-011-010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780547","negative learning;generative adversarial networks;image synthesis;image-to-image translation","Mutual information;Noise measurement;Training data;Image segmentation;Generators;Visualization;Generative adversarial networks","image classification;image denoising;learning (artificial intelligence);medical image processing","corresponding input;false negatives;high mutual information;maximizes mutual information;noisy negative patches;noisy patches;noncorresponding ones;novel patchwise negative learning loss;output patches;patchwise contrastive learning;prominent image translation methods;query patch;selected noncorresponding patch;semantically similar patches;unpaired image-to-image translation","","","","20","IEEE","24 May 2022","","","IEEE","IEEE Magazines"
"Mask2LFP: Mask-constrained Adversarial Latent Fingerprint Synthesis","H. Walhazi; A. Maalej; N. E. B. Amara","Institut supé rieur des sciences appliqué es et de technologie de Sousse, Université de Sousse, Sousse, Tunisia; Institut Supé rieur de Mathé matiques Appliquées et d’informatique de Kairouan, Université de Kairouan, Kairouan, Tunisie; LATIS- Laboratory of AdvancedTechnology and Intelligent Systems, Université de Sousse,Ecole Nationale d’Ingénieurs de Sousse, Sousse, Tunisie","2020 International Conference on Cyberworlds (CW)","30 Oct 2020","2020","","","265","271","Latent fingerprints are one of the most valuable and unique biometric attributes that are extensively used in forensic and law enforcement applications. Compared to rolled/plain fingerprint, latent fingerprint is of poor quality in term of friction ridge patterns, hence a more challenging for automatic fingerprint recognition systems. Considering the difficulties of dusting, lifting, and recovery of latent fingerprint, this type of fingerprints remain expensive to develop and collect. In this paper, we present a novel approach for synthetic latent fingerprint generation using Generative Adversarial Network (GAN). Our proposed framework, named mask to latent fingerprint (Mask2LFP), uses binary mask of distorted fingerprint-like shapes as input, and outputs a realistic latent fingerprint. This work focuses on the generation of synthetic latent fingerprints. The aim is to alleviate the scarcity issue of latent fingerprint data and serve the increasing need for developing, evaluating, and enhancing fingerprint-based identification systems, especially in forensic applications.","2642-3596","978-1-7281-6497-7","10.1109/CW49994.2020.00049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9240519","latent fingerprints;Generative Adversarial Networks;Image synthesis;Mask embedding.","Shape;Law enforcement;Forensics;Friction;Fingerprint recognition;Generative adversarial networks;Gallium nitride","biometrics (access control);fingerprint identification;image enhancement;image matching","distorted fingerprint;synthetic latent fingerprint generation;automatic fingerprint recognition systems;Mask-constrained Adversarial latent fingerprint synthesis;Mask2LFP;fingerprint-based identification systems;latent fingerprint data;synthetic latent fingerprints;realistic latent fingerprint","","","","16","IEEE","30 Oct 2020","","","IEEE","IEEE Conferences"
"Paint to Better Describe: Learning Image Caption by Using Text-to-Image Synthesis","R. Wang; L. Liu","School of Information Engineering, Ningxia University, Yinchuan, China; School of Information Engineering, Ningxia University, Yinchuan, China","2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)","15 Mar 2022","2021","","","958","964","Image caption aims to generate accurate and informative description according to given image. Although numerous existing methods have been proposed to generate high-quality and semantically real text description by using Encoder-Decoder architecture, how to guarantee whether the generated description is informative enough remains very challenging. In this paper, we propose a symmetrical structure named “Paint to better describe” (PTBD) by learning text-to-image synthesis to address this problem. Our PTBD approach consists of three modules, an image caption module (ICM), an image regeneration module (IREM), and a visual semantic alignment module (VSAM). ICM uses transformer-based method to generate image caption, leveraging both visual contents and object tags from the image. IREM uses generative adversarial network to regenerate the image according to the generated description, which semantically aligns with it. VSAM keeps network to generate accurate and informative description by measuring whether the regenerated image has enough semantics and visual information. Through experiments on MSCOCO dataset demonstrate the effectiveness of PTBD over other representative methods.","","978-1-6654-2174-4","10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730330","Image Caption;Symmetrical Structure;Generative Adversarial Network","Visualization;Computational modeling;Semantics;Computer architecture;Big Data;Transformers;Generative adversarial networks","decoding;image retrieval;image segmentation;learning (artificial intelligence);natural language processing;recurrent neural nets;text analysis","generated description;accurate description;informative description;regenerated image;visual information;Paint;text-to-image synthesis;numerous existing methods;semantically real text description;image caption module;image regeneration module;visual semantic alignment module;generative adversarial network","","1","","30","IEEE","15 Mar 2022","","","IEEE","IEEE Conferences"
"BSD-GAN: Branched Generative Adversarial Network for Scale-Disentangled Representation Learning and Image Synthesis","Z. Yi; Z. Chen; H. Cai; W. Mao; M. Gong; H. Zhang","Department of Computer Science, Memorial University of Newfoundland, St. John’s, Canada; School of Computing Science, Simon Fraser University, Burnaby, Canada; Department of Computer Science, Memorial University of Newfoundland, St. John’s, Canada; Department of Computer Science, Memorial University of Newfoundland, St. John’s, Canada; School of Computer Science, University of Guelph, Guelph, Canada; School of Computing Science, Simon Fraser University, Burnaby, Canada","IEEE Transactions on Image Processing","25 Sep 2020","2020","29","","9073","9083","We introduce BSD-GAN, a novel multi-branch and scale-disentangled training method which enables unconditional Generative Adversarial Networks (GANs) to learn image representations at multiple scales, benefiting a wide range of generation and editing tasks. The key feature of BSD-GAN is that it is trained in multiple branches, progressively covering both the breadth and depth of the network, as resolutions of the training images increase to reveal finer-scale features. Specifically, each noise vector, as input to the generator network of BSD-GAN, is deliberately split into several sub-vectors, each corresponding to, and is trained to learn, image representations at a particular scale. During training, we progressively “de-freeze” the sub-vectors, one at a time, as a new set of higher-resolution images is employed for training and more network layers are added. A consequence of such an explicit sub-vector designation is that we can directly manipulate and even combine latent (sub-vector) codes which model different feature scales. Extensive experiments demonstrate the effectiveness of our training method in scale-disentangled learning of image representations and synthesis of novel image contents, without any extra labels and without compromising quality of the synthesized high-resolution images. We further demonstrate several image generation and manipulation applications enabled or improved by BSD-GAN.","1941-0042","","10.1109/TIP.2020.3014608","NSERC Canada(grant numbers:2017-06086); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165961","Image Processing;Image Generation;Visual Effects","Training;Gallium nitride;Image generation;Generators;Image representation;Manifolds;Image coding","image representation;image resolution;neural nets;unsupervised learning;vectors","image representations;BSD-GAN;higher-resolution images;image contents;high-resolution images;image generation;manipulation applications;branched Generative Adversarial network;scale-disentangled representation learning;image synthesis;unconditional Generative Adversarial Networks;scale-disentangled training;multibranch training","","5","","35","IEEE","12 Aug 2020","","","IEEE","IEEE Journals"
"Beyond View Transformation: Cycle-Consistent Global and Partial Perception Gan for View-Invariant Gait Recognition","S. Li; W. Liu; H. Ma; S. Zhu",Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia; Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia,"2018 IEEE International Conference on Multimedia and Expo (ICME)","11 Oct 2018","2018","","","1","6","Cross-view gait recognition is a challenging problem when view-interval and pose variation are relatively large. In this paper, we propose Cycle-consistent Attentive Generative Adversarial Networks (CA-GAN) to map different views' gait images to view-consistent and photorealistic gait images for cross-view gait recognition. In CA-GAN, the generative network is composed of two branches, which simultaneously perceives human's global contexts and local body parts information respectively. Moreover, we design a novel Attentive Adversarial Network (AAN) to adaptively learn different weights for the discriminator's receptive fields with attention mechanism. Furthermore, as it is hard to collect the pose-aligned gait image pairs from different views for training CA-GAN’ we combine forward cycle-consistency loss and adver-sarial loss to learn the transformation relationship from source views to target view. The combined loss function can also preserve the discriminative gait structures of different identities at the training stage. Finally, we directly exploit the synthesized view-consistent gait images for cross-view gait recognition task. Experimental results on CASIA-B demonstrate that our method not only outperforms the state-of-the-art methods in cross-view gait recognition, but also presents compelling perceptual results even across the large view-interval.","1945-788X","978-1-5386-1737-3","10.1109/ICME.2018.8486484","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486484","Gait recognition;generative adversarial network;attention mechanism;image synthesis;cycle-consistency loss","Gait recognition;Gallium nitride;Image generation;Generative adversarial networks;Training;Task analysis;Feature extraction","gait analysis;image motion analysis;image recognition;pose estimation;visual perception","view-invariant gait recognition;Cycle-consistent Attentive Generative Adversarial Networks;CA-GAN;photorealistic gait images;pose-aligned gait image pairs;forward cycle-consistency loss;discriminative gait structures;synthesized view-consistent gait images;cross-view gait recognition task;Cycle-consistent global GAN;Partial Perception GAN;CASIA-B","","10","","21","IEEE","11 Oct 2018","","","IEEE","IEEE Conferences"
"Optimized latent-code selection for explainable conditional text-to-image GANs","Z. Zhang; L. Schomaker","Bernoulli Institute, University of Groningen, Groningen, The Netherlands; Bernoulli Institute, University of Groningen, Groningen, The Netherlands","2022 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2022","2022","","","1","9","The task of text-to-image generation has achieved remarkable progress due to the advances in conditional generative adversarial networks (GANs). However, existing conditional text-to-image GANs approaches mostly concentrate on improving both image quality and semantic relevance but ignore the explainability of the model which plays a vital role in real-world applications. In this paper, we present a variety of techniques to take a deep look into the latent space and semantic space of a conditional text-to-image GANs model. We introduce pairwise linear interpolation of latent codes and ‘linguistic’ linear interpolation to study what the model has learned within the latent space and ‘linguistic’ embeddings. Subsequently, we extend linear interpolation to triangular interpolation conditioned on three corners to further analyze the model. After that, we build a Good/Bad data set containing unsuccessfully and successfully synthesized samples and corresponding latent codes for the image-quality research. Based on this data set, we propose a framework for finding good latent codes by utilizing a linear SVM. Experimental results on the recent DiverGAN generator trained on two benchmark data sets qualitatively prove the effectiveness of our presented techniques, with a better than 94% accuracy in predicting Good/Bad classes for latent vectors. The Good/Bad data set is publicly available at https://zenodo.org/record/5850224#.YeGMwP7MKUk.","2161-4407","978-1-7281-8671-9","10.1109/IJCNN55064.2022.9892738","Center for Information Technology; University of Groningen; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892738","text-to-image synthesis;linear interpolation;triangular interpolation;a Good/Bad data set","Image quality;Support vector machines;Interpolation;Codes;Semantics;Neural networks;Generative adversarial networks","data handling;interpolation;learning (artificial intelligence);pattern classification;support vector machines","latent-code selection;explainable conditional text-to-image GANs;text-to-image generation;conditional generative adversarial networks;image quality;semantic relevance;explainability;latent space;semantic space;conditional text-to-image GANs model;pairwise linear interpolation;linguistic linear interpolation;corresponding latent codes;image-quality research;good latent codes;linear SVM;recent DiverGAN generator;latent vectors","","","","27","IEEE","30 Sep 2022","","","IEEE","IEEE Conferences"
"Book Cover Synthesis from the Summary","E. Haque; M. F. K. Khan; M. I. Jubair; J. Anjum; A. Z. Niloy","Ahsanullah University of Science and Technology, Bangladesh; Ahsanullah University of Science and Technology, Bangladesh; University of Colorado Boulder, United States; Ahsanullah University of Science and Technology, Bangladesh; Ahsanullah University of Science and Technology, Bangladesh","2022 IEEE/ACS 19th International Conference on Computer Systems and Applications (AICCSA)","20 Jan 2023","2022","","","1","6","The cover is the face of a book and is a point of attraction for the readers. Designing book covers is an essential task in the publishing industry. One of the main challenges in creating a book cover is representing the theme of the book's content in a single image. In this research, we explore ways to produce a book cover using artificial intelligence based on the fact that there exists a relationship between the summary of the book and its cover. Our key motivation is the application of text-to-image synthesis methods to generate images from given text or captions. We explore several existing text-to-image conversion techniques for this purpose and propose an approach to exploit these frameworks for producing book covers from provided summaries. We construct a dataset of English books that contains a large number of samples of summaries of existing books and their cover images. In this paper, we describe our approach to collecting, organizing, and pre-processing the dataset to use it for training models. We apply different text-to-image synthesis techniques to generate book covers from the summary and exhibit the results in this paper.","2161-5330","979-8-3503-1008-5","10.1109/AICCSA56895.2022.10017541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10017541","Book cover synthesis;Book summary;Generative Adversarial Networks;Text-to-image synthesis","Training;Industries;Visualization;Systematics;Publishing;Computer architecture;Task analysis","artificial intelligence;electronic publishing;information retrieval systems;text analysis","book cover synthesis;cover images;English books;text-to-image conversion techniques;text-to-image synthesis methods;text-to-image synthesis techniques","","","","19","IEEE","20 Jan 2023","","","IEEE","IEEE Conferences"
"Interpreting the Latent Space of GANs for Semantic Face Editing","Y. Shen; J. Gu; X. Tang; B. Zhou","The Chinese University of Hong Kong; The Chinese University of Hong Kong, Shenzhen; The Chinese University of Hong Kong; The Chinese University of Hong Kong","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","9240","9249","Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00926","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157070","","Semantics;Gallium nitride;Face;Generative adversarial networks;Aerospace electronics;Generators;Image generation","face recognition;image representation;learning (artificial intelligence);neural nets;realistic images","latent space;semantic face editing;generative adversarial networks;high-fidelity image synthesis;latent code;photo-realistic image;latent semantics;face synthesis;entangled semantics;GAN inversion methods;disentangled attribute representation;controllable facial attribute representation","","287","","39","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Learning Compositional Visual Concepts with Mutual Consistency","Y. Gong; S. Karanam; Z. Wu; K. -C. Peng; J. Ernst; P. C. Doerschuk","School of Electrical and Computer Engineering, Cornell University, Ithaca, NY; Siemens Corporate Technology, Princeton, NJ; Siemens Corporate Technology, Princeton, NJ; Siemens Corporate Technology, Princeton, NJ; Siemens Corporate Technology, Princeton, NJ; Nancy E. and Peter C. Meinig School of Biomedical Engineering, Cornell University, Ithaca, NY","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","8659","8668","Compositionality of semantic concepts in image synthesis and analysis is appealing as it can help in decomposing known and generatively recomposing unknown data. For instance, we may learn concepts of changing illumination, geometry or albedo of a scene, and try to recombine them to generate physically meaningful, but unseen data for training and testing. In practice however we often do not have samples from the joint concept space available: We may have data on illumination change in one data set and on geometric change in another one without complete overlap. We pose the following question: How can we learn two or more concepts jointly from different data sets with mutual consistency where we do not have samples from the full joint space? We present a novel answer in this paper based on cyclic consistency over multiple concepts, represented individually by generative adversarial networks (GANs). Our method, ConceptGAN, can be understood as a drop in for data augmentation to improve resilience for real world applications. Qualitative and quantitative evaluations demonstrate its efficacy in generating semantically meaningful images, as well as one shot face verification as an example application.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579001","","Training data;Training;Image generation;Generative adversarial networks;Face;Semantics;Gallium nitride","face recognition;learning (artificial intelligence)","compositional visual concepts;semantic concepts;image synthesis;generative adversarial networks;data augmentation;semantically meaningful images","","4","1","49","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Towards High-Resolution Face Pose Synthesis","D. M. Souza; D. D. Ruiz","Pontifícia Universidade Católica do Rio Grande do Sul Av. Ipiranga, Porto Alegre, RS, Brazil; Pontifícia Universidade Católica do Rio Grande do Sul Av. Ipiranga, Porto Alegre, RS, Brazil","2018 International Joint Conference on Neural Networks (IJCNN)","14 Oct 2018","2018","","","1","8","Synthesizing different views of a face image is a challenging task that can potentially help in several computer graphics and computer vision applications. In this work, we present a novel approach to address this task. We leverage the power of Generative Adversarial Networks (GANs) to synthesize face poses in a high-resolution and realistic fashion. We control the rotation of synthesized faces along the three axes of space (roll, pitch, yaw). We start by estimating the pose of each face in the training set and storing a vector containing the rotation angles. Then, we use the images along with the angles to train a conditioned version of a state-of-the-art GAN. Our experiments show image synthesis with a high-realistic finish, plus the absolute control of the pose of synthesized face images.","2161-4407","978-1-5090-6014-6","10.1109/IJCNN.2018.8488993","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8488993","","Gallium nitride;Face;Training;Generators;Generative adversarial networks;Cameras;Task analysis","computer graphics;computer vision;face recognition;image resolution;neural nets;pose estimation","image synthesis;face image;computer graphics;computer vision applications;generative adversarial networks;GAN;high-resolution face pose synthesis;pose estimation","","2","1","30","IEEE","14 Oct 2018","","","IEEE","IEEE Conferences"
"PPIG: Productive and Pathogenic Image Generation for Plant Disease Diagnosis","S. Kanno; S. Nagasawa; Q. H. Cap; S. Shibuya; H. Uga; S. Kagiwada; H. Iyatomi","Applied Informatics, Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; Applied Informatics, Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; Applied Informatics, Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; Applied Informatics, Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; Saitama Agricultural Technology Research Center, Saitama, Japan; Clinical Plant Science, Faculty of Bioscience and Applied Chemistry, Hosei University, Tokyo, Japan; Applied Informatics, Graduate School of Science and Engineering, Hosei University, Tokyo, Japan","2020 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES)","14 Apr 2021","2021","","","554","559","Image-based autonomous diagnosis for plants is a difficult task since plant symptoms are visually subtle. This subtlety leads to the system overfitting as it sometimes responds to non-essential parts in images such as background or sunlight conditions. Thus, this causes a significant drop in performance when diagnosing diseases in different test fields. Several data augmentation methods utilizing generative adversarial networks (GANs) have been proposed to address this overfitting problem. However, performance improvement is limited due to the limited variety of generated images. This study proposes a productive and pathogenic image generation (PPIG) technique, a framework for generating varied and quality plant images to train the diagnostic systems. PPIG is comprised of two phases: the bulk production phase and the pathogenic phase. In the first phase, a number of healthy leaf images are generated to form the basis for the generation of disease images. Then, in the second phase, the symptomatic characteristics are added to the leaf part of the generated healthy images. In this study, we conducted experiments to evaluate PPIG using test images taken in different fields from the training images, assuming six disease classes of cucumber leaves. The proposed PPIG can generate natural-looking, healthy and disease images, and data augmentation using these images effectively improved the robustness of the diagnostic system. Experiments on 8,834 test images taken in different fields from 53,045 training images show that our proposal improved the disease diagnostic performance from the baseline by 9.4% for the macro-average F1-score. Moreover, it also outperformed the previous cutting-edge data augmentation methodology by 4.5%.","","978-1-7281-4245-6","10.1109/IECBES48179.2021.9398772","Ministry of Agriculture, Forestry and Fisheries (MAFF) Japan(grant numbers:JP17935051); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9398772","data augmentation;plant disease diagnosis;generative adversarial networks;image processing;deep learning","Training;Image synthesis;Robustness;Medical diagnosis;Proposals;Task analysis;Diseases","botany;diseases;feature extraction;image classification;medical image processing;patient diagnosis;plant diseases","PPIG;plant disease diagnosis;autonomous diagnosis;plant symptoms;nonessential parts;diagnosing diseases;different test fields;generative adversarial networks;overfitting problem;performance improvement;quality plant images;diagnostic system;bulk production phase;pathogenic phase;healthy leaf images;disease images;leaf part;generated healthy images;disease classes;8 test images;834 test images;53 training images;045 training images;disease diagnostic performance;previous cutting-edge data augmentation methodology","","2","","33","IEEE","14 Apr 2021","","","IEEE","IEEE Conferences"
"AffectGAN: Affect-Based Generative Art Driven by Semantics","T. Galanos; A. Liapis; G. N. Yannakakis","Institute of Digital Games, University of Malta, Msida, Malta; Institute of Digital Games, University of Malta, Msida, Malta; Institute of Digital Games, University of Malta, Msida, Malta","2021 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)","10 Jan 2022","2021","","","01","07","This paper introduces a novel method for generating artistic images that express particular affective states. Leveraging state-of-the-art deep learning methods for visual generation (through generative adversarial networks), semantic models from OpenAI, and the annotated dataset of the visual art encyclopedia WikiArt, our AffectGAN model is able to generate images based on specific or broad semantic prompts and intended affective outcomes. A small dataset of 32 images generated by AffectGAN is annotated by 50 participants in terms of the particular emotion they elicit, as well as their quality and novelty. Results show that for most instances the intended emotion used as a prompt for image generation matches the participants' responses. This small-scale study brings forth a new vision towards blending affective computing with computational creativity, enabling generative systems with intentionality in terms of the emotions they wish their output to elicit.","","978-1-6654-0021-3","10.1109/ACIIW52867.2021.9666317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9666317","Generative Adversarial Networks;semantic models;generative art;WikiArt;emotions","Visualization;Affective computing;Art;Image resolution;Image synthesis;Semantics;Training data","art;deep learning (artificial intelligence);emotion recognition","visual generation;generative adversarial networks;semantic models;annotated dataset;visual art encyclopedia WikiArt;AffectGAN model;specific prompts;broad semantic prompts;affective outcomes;intended emotion;image generation;affective computing;generative systems;generative art;artistic images;particular affective states;deep learning methods","","1","","31","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"MedViTGAN: End-to-End Conditional GAN for Histopathology Image Augmentation with Vision Transformers","M. Li; C. Li; P. Hobson; T. Jennings; B. C. Lovell","School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, QLD, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, QLD, Australia; Sullivan Nicolaides Pathology, Bowen Hills, QLD, Australia; Sullivan Nicolaides Pathology, Bowen Hills, QLD, Australia; School of Information Technology and Electrical Engineering, The University of Queensland, St Lucia, QLD, Australia","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","4406","4413","Deep learning networks have demonstrated competitive performance for various tasks on medical images. However, obtaining promising results requires a large amount of annotated data for supervised training, which is labor-intensive. Recently, the increasing interest in transformers has suggested their robust performance on computer vision tasks, including generative adversarial networks (GANs). In this paper, we propose a conditional GAN built on pure transformer-based architectures, named MedViTGAN, to assist in generating synthetic histopathology images for data augmentation in an end-to-end manner. The presented model adopts a conditioned training strategy by incorporating a transformer-based auxiliary classifier to facilitate the discriminative image generation process. We further introduce an adaptive hybrid loss weighting mechanism to balance multiple losses over sources and classes to stabilize the training. Extensive experiments on the histopathology datasets show that leveraging MedViTGAN generated images results in a significant and consistent improvement in classification performance.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956431","Australian Research Council; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956431","Generative adversarial networks;Deep learning;Histopathology medical image analysis;Classification;Vision transformer","Training;Adaptation models;Histopathology;Image synthesis;Semantic segmentation;Computer architecture;Transformers","computer vision;deep learning (artificial intelligence);feature extraction;image classification;medical image processing;neural net architecture;supervised learning","adaptive hybrid loss weighting mechanism;annotated data;classification performance;computer vision tasks;conditioned training strategy;data augmentation;deep learning networks;discriminative image generation process;end-to-end conditional GAN;generative adversarial networks;histopathology datasets;histopathology image augmentation;medical images;MedViTGAN;pure transformer-based architectures;robust performance;supervised training;synthetic histopathology images;transformer-based auxiliary classifier;vision transformers","","","","46","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"Assessment of Data Augmentation Strategies Toward Performance Improvement of Abnormality Classification in Chest Radiographs","P. Ganesan; S. Rajaraman; R. Long; B. Ghoraani; S. Antani","Department of Electrical Engineering, Florida Atlantic University, Boca Raton, FL; National Library of Medicine, National Institutes of Health, Bethesda, MD; National Library of Medicine, National Institutes of Health, Bethesda, MD; Department of Electrical Engineering, Florida Atlantic University, Boca Raton, FL; National Library of Medicine, National Institutes of Health, Bethesda, MD","2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","7 Oct 2019","2019","","","841","844","Image augmentation is a commonly performed technique to prevent class imbalance in datasets to compensate for insufficient training samples, or to prevent model overfitting. Traditional augmentation (TA) techniques include various image transformations, such as rotation, translation, channel splitting, etc. Alternatively, Generative Adversarial Network (GAN), due to its proven ability to synthesize convincingly-realistic images, has been used to perform image augmentation as well. However, it is unclear whether GAN augmentation (GA) strategy provides an advantage over TA for medical image classification tasks. In this paper, we study the usefulness of TA and GA for classifying abnormal chest X-ray (CXR) images. We first trained a progressive-growing GAN (PG-GAN) to synthesize high-resolution CXRs for performing GA. Then, we trained an abnormality classifier using three training sets individually - training set with TA, with GA and with no augmentation (NA). Finally, we analyzed the abnormality classifier's performance for the three training cases, which led to the following conclusions: (1) GAN strategy is not always superior to TA for improving the classifier's performance; (2) in comparison to NA, however, both TA and GA leads to a significant performance improvement; and, (3) increasing the quantity of images in TA and GA strategies also improves the classifier's performance.","1558-4615","978-1-5386-1311-5","10.1109/EMBC.2019.8857516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8857516","Deep learning;Generative adversarial network;Medical image synthesis;Chest X-ray;Abnormality classification;Progressive-growing GAN","Training;Generative adversarial networks;Image resolution;Gallium nitride;Biomedical imaging;Measurement;X-ray imaging","diagnostic radiography;image classification;image resolution;learning (artificial intelligence);medical image processing;neural nets","data augmentation;abnormality classification;chest radiographs;image augmentation;image transformations;medical image classification;abnormal chest X-ray images;generative adversarial network;GAN augmentation;high-resolution CXRs;transfer learning","Radiography;Thorax","11","","13","IEEE","7 Oct 2019","","","IEEE","IEEE Conferences"
"Generative Adversarial Network with Local Discriminator for Synthesizing Breast Contrast-Enhanced MRI","E. Kim; H. -h. Cho; E. Ko; H. Park","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Republic of Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Republic of Korea; Department of Radiology and Center for Imaging Science, Samsung Medical Center, Sungkyunkwan University College of Medicine, Seoul, Republic of Korea; School of Electronic and Electrical Engineering, Sungkyunkwan University and Center for Neuroscience Imaging Research, Institute for Basic Science, Suwon, Republic of Korea","2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI)","10 Aug 2021","2021","","","1","4","Breast magnetic resonance imaging (MRI) has been widely used as a sensitive imaging technique capable of differentiating benign from malignant tumors or predicting response to treatment. Dynamic contrast-enhanced (DCE) MRI, obtained with contrast agent (CA) injection, provides hemodynamic information and tumor characteristics. However, gadolinium-based CA could deposit in the brain and other organs and have been reported to cause nephrogenic systemic fibrosis. Thus, synthetic images having similar quality compared with contrast-enhanced (CE) MRI has important clinical implications. In this study, we proposed a generative adversarial network with a local discriminator to synthesize T1-weighted CE MRI from non-contrast MRI. Residual learning and spectral normalized weights were adopted for stable convergent properties. The attention layer and local discriminator learned nonlinear mappings and enhanced the representation of the tumors better than other methods. Our approach showed better performance using four evaluation metrics both in whole breast and tumor regions. Our method would be helpful in patients who cannot undergo CE MRI and in screening for high-risk patients by substituting CE MRI.","2641-3604","978-1-6654-0358-0","10.1109/BHI50953.2021.9508579","National Research Foundation; Institute for Basic Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9508579","breast cancer;contrast-enhanced T1-weighted image;synthesis;generative adversarial network;local discriminator","Measurement;Magnetic resonance imaging;Malignant tumors;Conferences;Biological systems;Breast;Generative adversarial networks","biological organs;biomedical MRI;brain;cancer;gynaecology;haemodynamics;image enhancement;medical image processing;tumours","contrast agent injection;hemodynamic information;tumor characteristics;gadolinium-based CA;nephrogenic systemic fibrosis;synthetic images;important clinical implications;generative adversarial network;local discriminator;T1-weighted CE MRI;noncontrast MRI;spectral normalized weights;tumor regions;synthesizing breast contrast-enhanced MRI;breast magnetic resonance imaging;sensitive imaging technique;malignant tumors;predicting response;dynamic contrast-enhanced MRI","","3","","15","IEEE","10 Aug 2021","","","IEEE","IEEE Conferences"
"Improved CyeleGAN for MR to CT synthesis","G. Cao; S. Liu; H. Mao; S. Zhang","School of Computer Science & Information Engineering, Shanghai Institute of Technology, Shanghai, China; School of Computer Science & Information Engineering, Shanghai Institute of Technology, Shanghai, China; School of Computer Science & Information Engineering, Shanghai Institute of Technology, Shanghai, China; School of Computer Science & Information Engineering, Shanghai Institute of Technology, Shanghai, China","2021 6th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)","29 Dec 2021","2021","6","","205","208","Radiotherapy treatment planning requires CT images to accurately calculate the dose distribution, but sometimes only MR images can be obtained, therefore it is necessary to generate CT images from its corresponding MR images. In order to synthesize quality pseudo CT images, an improved cycle generative adversarial network (CycleGAN) with residual network (ResNet) and U-Net was proposed. The improved CycleGAN was trained by unpaired data to get better result, and approved by the cycle inconsistency of CycleGAN, the slow-down gradient disappearance of ResNet and the multi-scale fusion of U-Nets. Avoiding the disappearance of input information and the vanishing of gradient information, the improved network synthesized more quality CT images. Compared with the original CycleGAN method, the MAE of the proposed method was reduced by 5.12%, the SSIM increased by 0.7% and the PSNR increased by 1.29%, which was trained and tested on a dataset of 18 patients. Additional, compared with the DCNN method, the atlas-based method, and the pix2pix method, the relative error of MAE was reduced by 1.73%, 2.215% and 0.228% respectively. The proposed method synthesizes more vivid CT images owing to the advantages of deep learning model, which better meets the requirements of clinical analysis.","2189-8723","978-1-7281-6714-5","10.1109/ICIIBMS52876.2021.9651571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9651571","Image synthesis;CycleGAN;U-Net;ResNet","Deep learning;Computed tomography;Medical treatment;Generative adversarial networks;Generators;Planning;Clinical diagnosis","biomedical MRI;computerised tomography;dosimetry;image segmentation;learning (artificial intelligence);medical image processing;radiation therapy","CT synthesis;radiotherapy treatment planning;dose distribution;pseudoCT images;cycle generative adversarial network;residual network;ResNet;cycle inconsistency;gradient disappearance;gradient information;CT images;CycleGAN method;pix2pix method","","","","11","IEEE","29 Dec 2021","","","IEEE","IEEE Conferences"
"TDM-Stargan: Stargan Using Time Difference Map to Generate Dynamic Contrast-Enhanced Mri from Ultrafast Dynamic Contrast-Enhanced Mri","Y. -T. Oh; E. Ko; H. Park","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea; Department of Radiology and Center for Imaging Science, Samsung Medical Center, Sungkyunkwan University School of Medicine, Seoul, Korea; Center for Neuroscience Imaging Research, Institute for Basic Science, Suwon, Korea","2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)","26 Apr 2022","2022","","","1","5","Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a sensitive imaging technique to manage many types of cancer including breast cancer. The conventional DCE-MRI takes a long time (7-12 minutes) to acquire and there is a clinical need to reduce scan time. Ultrafast DCE-MRI takes less than a minute to acquire and has sufficient information relative to conventional DCE-MRI. We propose a generative adversarial network (GAN) to generate the delay phase of synthetic conventional DCE-MRI from ultrafast DCE-MRI. We allow our model to better generate the area expected to be a lesion through the difference map of different phases to incorporate time-varying enhancement patterns. The difference map also allows us to generate pseudo tumor labels for segmentation. Our approach was trained and tested on 300 cases using three evaluation metrics. Our method showed better performance (structural similarity index map increase of 11.69%) compared to Pix2Pix baseline method.","1945-8452","978-1-6654-2923-8","10.1109/ISBI52829.2022.9761463","National Research Foundation; Institute for Basic Science; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761463","breast cancer;dynamic contrast-enhanced MRI;ultrafast dynamic contrast-enhanced MRI;GAN;image synthesis;difference map","Measurement;Magnetic resonance imaging;Semisupervised learning;Generative adversarial networks;Breast cancer;Delays;Lesions","biomedical MRI;cancer;image enhancement;image segmentation;medical image processing;neural nets;tumours","dynamic contrast-enhanced magnetic resonance imaging;sensitive imaging technique;breast cancer;scan time;ultrafast DCE-MRI;generative adversarial network;synthetic conventional DCE-MRI;time-varying enhancement patterns;TDM-stargan;time difference map;dynamic contrast-enhanced MRI;ultrafast dynamic contrast-enhanced MRI;delay phase;lesion;pseudotumor labels;evaluation metrics;structural similarity index map;time 7.0 min to 12.0 min","","","","14","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"Generating and Modifying High Resolution Fashion Model Image using StyleGAN","I. Choi; S. Park; J. Park","Content Research Division, ETRI, Daejeon, Korea; Content Research Division, ETRI, Daejeon, Korea; Content Research Division, ETRI, Daejeon, Korea","2022 13th International Conference on Information and Communication Technology Convergence (ICTC)","25 Nov 2022","2022","","","1536","1538","In this paper, a research of synthesizing fashion model images by utilizing a state-of-the-art generative adversarial network (i.e., GAN) is introduced. After training GAN with fashion model images, the network was able to generate realistic fashion model images having various characteristics such as pose and clothes. Moreover, two image modifications named Fashion Model Morphing and Fashion Transfer are also proposed by merging attributes of two generated fashion model images. The research investigates the effectiveness of using GAN for fashion to create a large number of images for exploring new design and styles. The generated images are even more beneficial for fashion industries because the generated images have no legal issues such as portrait right and copyright.","2162-1241","978-1-6654-9939-2","10.1109/ICTC55196.2022.9952574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952574","Generative Neural Network;Image synthesis;Fashion","Training;Industries;Image resolution;Law;Image color analysis;Merging;Generative adversarial networks","clothing;clothing industry;image morphing;solid modelling","fashion industries;Fashion Model Morphing;Fashion Transfer;GAN;image modifications;realistic fashion model images;state-of-the-art generative adversarial network","","","","4","IEEE","25 Nov 2022","","","IEEE","IEEE Conferences"
"Deep Group-Wise Angular Translation of Cardiac Diffusion MRI in q-space via Manifold Regularized GAN","Y. He; L. Wang; F. Yang; P. Clarysse; Y. Zhu","Univ Lyon, INSA-Lyon, Université Claude Bernard Lyon 1, UJM-Saint Etienne, CNRS, In-serm, CREATIS UMR 5220, U1206, LYON, France; Key Laboratory of Intelligent Medical Image Analysis and Precise Diagnosis of Guizhou Province, School of Computer Science and Technology, Guizhou University, Guiyang, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Univ Lyon, INSA-Lyon, Université Claude Bernard Lyon 1, UJM-Saint Etienne, CNRS, In-serm, CREATIS UMR 5220, U1206, LYON, France; Univ Lyon, INSA-Lyon, Université Claude Bernard Lyon 1, UJM-Saint Etienne, CNRS, In-serm, CREATIS UMR 5220, U1206, LYON, France","2020 15th IEEE International Conference on Signal Processing (ICSP)","18 Jan 2021","2020","1","","511","515","Diffusion magnetic resonance imaging (dMRI) has become an indispensable tool for non-invasive characterization of fiber structures of tissues. Clinical applicability of dMRI is often shackled by trade-off between image quality and long acquisition time. We propose a novel group-wise image translation method to improve the angular resolution of cardiac dMRI data. It consists in using a generative adversarial network (GAN) model to estimate a sequence of images from given DW images acquired in a limited number of diffusion gradient directions. We embed a supervised manifold regularized term in the GAN loss function to exploit the correlation between multiple DW images acquired in different gradient directions. Experimental results on cardiac dMRI data demonstrated that our method can significantly improve the quality of diffusion tensor imaging (DTI) reconstruction.","2164-5221","978-1-7281-4480-1","10.1109/ICSP48669.2020.9320925","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320925","diffusion MRI;super-resolution;cardiac DTI;image synthesis;deep learning;spatial-angular information","Gallium nitride;Generators;Generative adversarial networks;Correlation;Manifolds;Diffusion tensor imaging;Three-dimensional displays","biodiffusion;biological tissues;biomedical MRI;cardiology;image reconstruction;image resolution;image sequences;medical image processing;neural nets;supervised learning","angular resolution;diffusion gradient directions;supervised manifold regularized term;GAN loss function;multiple DW images;diffusion tensor imaging reconstruction;deep group-wise angular translation;cardiac diffusion MRI;manifold regularized GAN;diffusion magnetic resonance imaging;noninvasive characterization;image quality;group-wise image translation;cardiac dMRI;generative adversarial network;q-space;image sequence;diffusion-weighted images;fiber structures","","","","25","IEEE","18 Jan 2021","","","IEEE","IEEE Conferences"
"Everyone is A Forensic Artist: Sketch-to-Photo Transformation for Human Face","D. J. Salim; B. -S. Lin","Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan","2021 IEEE 4th International Conference on Knowledge Innovation and Invention (ICKII)","27 Oct 2021","2021","","","118","122","There are a lot of crimes happening all over the world every day. Among the criminal acts, homicide is the type of crime with a large number of victims. Occassionally there are witnesses who see the incident and remember the face of the criminal. Thus the police ask them to sketch to find out the suspect. Since the human face is the most significant and informative part of the human body, the sketch of the face is used to identify the suspect with high certainty. However, the suspects may change their facial features by makeup, such as putting on glasses or dyeing hair. If a sketch is converted into photographic images with modified facial features flexibly, the investigation of crime might accelerate effectively. Recent research has shown the techniques that transform a sketch of a human face into a photographic image or change the style of a human face according to the designated facial features. However there has not yet been an integrated architecture to transform pencil sketches to photographic images directly with desired facial features. In addition there is no fast and automatic evaluation approach that consider multiple metrics of images jointly. It is hence difficult to optimize and select the setting among a few alternatives efficiently. With the limitation we propose the Forensic GAN a network of performing the sketch-to-photo transformation and image manipulation according to the designated attributes. A voting mechanism with multiple metrics including PSNR SSIM SCC and ERGAS for fast evaluating the image quality jointly was proposed. Accordingly the training settings such as the loss function and the number of epochs can be optimized and selected based on the quality of the synthesized images. The performance of the Forensic GAN was tested and its potential for the real forensic task has been verified with synthesized pencil images and real hand-drawing sketches","","978-1-6654-2307-6","10.1109/ICKII51822.2021.9574719","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9574719","Forensic GAN;Cycle GAN;Star GAN;Data Augmentation;Automatic Evaluation of Image Synthesis","Measurement;Training;Technological innovation;Law enforcement;Forensics;Transforms;Generative adversarial networks","face recognition;image forensics;neural nets;visual databases","photographic image;human face;designated facial features;pencil sketches;desired facial features;Forensic GAN;sketch-to-photo transformation;hand-drawing sketches;human body;modified facial features","","","","9","IEEE","27 Oct 2021","","","IEEE","IEEE Conferences"
"Label-Guided Generative Adversarial Network for Realistic Image Synthesis","J. Zhu; L. Gao; J. Song; Y. -F. Li; F. Zheng; X. Li; H. T. Shen","Future Media Center and School of Computer Science and Engineering, The University of Electronic Science and Technology of China, Chengdu, China; Future Media Center and School of Computer Science and Engineering, The University of Electronic Science and Technology of China, Chengdu, China; Future Media Center and School of Computer Science and Engineering, The University of Electronic Science and Technology of China, Chengdu, China; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; Key Laboratory of Intelligent Interaction and Applications (Northwestern Polytechnical University), Ministry of Industry and Information Technology, Xi'an, China; Future Media Center and School of Computer Science and Engineering, The University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Pattern Analysis and Machine Intelligence","3 Feb 2023","2023","45","3","3311","3328","Generating photo-realistic images from labels (e.g., semantic labels or sketch labels) is much more challenging than the general image-to-image translation task, mainly due to the large differences between extremely sparse labels and detail rich images. We propose a general framework Lab2Pix to tackle this issue from two aspects: 1) how to extract useful information from the input; and 2) how to efficiently bridge the gap between the labels and images. Specifically, we propose a Double-Guided Normalization (DG-Norm) to use the input label for semantically guiding activations in normalization layers, and use global features with large receptive fields for differentiating the activations within the same semantic region. To efficiently generate the images, we further propose Label Guided Spatial Co-Attention (LSCA) to encourage the learning of incremental visual information using limited model parameters while storing the well-synthesized part in lower-level features. Accordingly, Hierarchical Perceptual Discriminators with Foreground Enhancement Masks are proposed to toughly work against the generator thus encouraging realistic image generation and a sharp enhancement loss is further introduced for high-quality sharp image generation. We instantiate our Lab2Pix for the task of label-to-image in both unpaired (Lab2Pix-V1) and paired settings (Lab2Pix-V2). Extensive experiments conducted on various datasets demonstrate that our method significantly outperforms state-of-the-art methods quantitatively and qualitatively in both settings.","1939-3539","","10.1109/TPAMI.2022.3186752","National Natural Science Foundation of China(grant numbers:62020106008,62122018,61772116,61872064,61871470); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810175","Generative Adversarial Networks (GANs);label-to-image synthesis;photo-realistic image generation","Task analysis;Visualization;Semantics;Generators;Feature extraction;Adaptation models;Training","","","","","","62","IEEE","28 Jun 2022","","","IEEE","IEEE Journals"
"LDGAN: A Synthetic Aperture Radar Image Generation Method for Automatic Target Recognition","C. Cao; Z. Cao; Z. Cui","School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China; Center for Information Geoscience, UESTC, Chengdu, China; School of Information and Communication Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Geoscience and Remote Sensing","22 Apr 2020","2020","58","5","3495","3508","Under the framework of a supervised learning-based automatic target recognition (ATR) approach, recognition performance is primarily dependent on the amount of training samples. However, shortage in training samples is a consistent issue for ATR. In this article, we propose a new image to image generation method, called label-directed generative adversarial networks (LDGANs), which will provide labeled samples to be used for recognition model training. We define an entirely new loss function for the LDGAN, which utilizes the Wasserstein distance to replace the original distance measurement of the conventional generative adversarial networks (GANs), thus efficiently avoiding the collapse mode problem. The label information is also added to the loss function of the LDGAN to avoid generating a large number of unlabeled target images. More importantly, the proposed method also makes corresponding changes to the network architecture regarding the new GANs. At the same time, the detailed algorithm about the LDGAN is also introduced in this article to deal with the issue that characteristically GANs are not easy to train. Based on comparisons with other directed generation methods, the experimental results show comparative results of several types of generated images in statistical features, gradient features, classic features of synthetic aperture radar (SAR) targets and the independence from the real image. While demonstrating that the images generated by the LDGAN produced better results using the assumptions of independent and identical distribution, the experiment also explores the performance of the generated image in the ATR. A comparison of these experimental results demonstrates a better way to use the generated image for ATR. The experimental results also prove that the proposed method does have the ability to supplement information for ATR when the training sample information is insufficient.","1558-0644","","10.1109/TGRS.2019.2957453","National Natural Science Foundation of China(grant numbers:61801098); Fundamental Research Funds for the Central Universities(grant numbers:2672018ZYGX2018J013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8938701","Automatic target recognition (ATR);information supplement;label-directed generative adversarial network (LDGAN);synthetic aperture radar (SAR) image","Gallium nitride;Radar polarimetry;Image recognition;Synthetic aperture radar;Target recognition;Training;Image synthesis","feature extraction;learning (artificial intelligence);neural nets;radar computing;radar imaging;radar target recognition;synthetic aperture radar","label-directed generative adversarial networks;supervised learning-based automatic target recognition approach;synthetic aperture radar image generation method;training sample information;synthetic aperture radar targets;directed generation methods;unlabeled target images;label information;GAN;conventional generative adversarial networks;loss function;recognition model training;labeled samples;LDGAN;training samples;recognition performance;ATR","","22","","32","IEEE","23 Dec 2019","","","IEEE","IEEE Journals"
"SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis","W. Chen; J. Hays","Georgia Institute of Technology; Georgia Institute of Technology, Argo AI","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","9416","9425","Synthesizing realistic images from human drawn sketches is a challenging problem in computer graphics and vision. Existing approaches either need exact edge maps, or rely on retrieval of existing photographs. In this work, we propose a novel Generative Adversarial Network (GAN) approach that synthesizes plausible images from 50 categories including motorcycles, horses and couches. We demonstrate a data augmentation technique for sketches which is fully automatic, and we show that the augmented data is helpful to our task. We introduce a new network building block suitable for both the generator and discriminator which improves the information flow by injecting the input image at multiple scales. Compared to state-of-the-art image translation methods, our approach generates more realistic images and achieves significantly higher Inception Scores.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579079","","Image edge detection;Image generation;Gallium nitride;Training;Databases;Task analysis;Generative adversarial networks","edge detection;feature extraction;image reconstruction;image representation;neural nets;realistic images","computer graphics;horses;couches;feature representations;motorcycles;computer vision;generative adversarial network;sketch to image synthesis;image translation;data augmentation;human drawn sketches;realistic images;SketchyGAN","","148","","60","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Semantics-Enhanced Adversarial Nets for Text-to-Image Synthesis","H. Tan; X. Liu; X. Li; Y. Zhang; B. Yin",Dalian University of Technology; Dalian University of Technology; Louisiana State University; Dalian University of Technology; Peng Cheng Laboratory,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","10500","10509","This paper presents a new model, Semantics-enhanced Generative Adversarial Network (SEGAN), for fine-grained text-to-image generation. We introduce two modules, a Semantic Consistency Module (SCM) and an Attention Competition Module (ACM), to our SEGAN. The SCM incorporates image-level semantic consistency into the training of the Generative Adversarial Network (GAN), and can diversify the generated images and improve their structural coherence. A Siamese network and two types of semantic similarities are designed to map the synthesized image and the groundtruth image to nearby points in the latent semantic feature space. The ACM constructs adaptive attention weights to differentiate keywords from unimportant words, and improves the stability and accuracy of SEGAN. Extensive experiments demonstrate that our SEGAN significantly outperforms existing state-of-the-art methods in generating photo-realistic images. All source codes and models will be released for comparative study.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.01060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010053","","Semantics;Training;Generators;Synthesizers;Generative adversarial networks;Gallium nitride;Stability analysis","image representation;image segmentation;neural nets;realistic images;text analysis","SEGAN;fine-grained text-to-image generation;SCM;image-level semantic consistency;siamese network;semantic similarities;synthesized image;groundtruth image;latent semantic feature space;ACM constructs adaptive attention weights;photo-realistic images;text-to-image synthesis;semantics-enhanced adversarial nets;semantics-enhanced generative adversarial network;semantic consistency module;attention competition module","","34","","46","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"KT-GAN: Knowledge-Transfer Generative Adversarial Network for Text-to-Image Synthesis","H. Tan; X. Liu; M. Liu; B. Yin; X. Li","School of Mathematical Sciences, Dalian University of Technology, Dalian, China; School of Mathematical Sciences, Dalian University of Technology, Dalian, China; School of Computer Science and Technology, Shandong Jianzhu University, Jinan, China; Peng Cheng Laboratory, Shenzhen, China; Center for Computation and Technology, Louisiana State University, Baton Rouge, LA, USA","IEEE Transactions on Image Processing","23 Dec 2020","2021","30","","1275","1290","This paper presents a new framework, Knowledge-Transfer Generative Adversarial Network (KT-GAN), for fine-grained text-to-image generation. We introduce two novel mechanisms: an Alternate Attention-Transfer Mechanism (AATM) and a Semantic Distillation Mechanism (SDM), to help generator better bridge the cross-domain gap between text and image. The AATM updates word attention weights and attention weights of image sub-regions alternately, to progressively highlight important word information and enrich details of synthesized images. The SDM uses the image encoder trained in the Image-to-Image task to guide training of the text encoder in the Text-to-Image task, for generating better text features and higher-quality images. With extensive experimental validation on two public datasets, our KT-GAN outperforms the baseline method significantly, and also achieves the competive results over different evaluation metrics.","1941-0042","","10.1109/TIP.2020.3026728","National Natural Science Foundation of China(grant numbers:61976040); Ministry of Science and Technology of the People’s Republic of China(grant numbers:2018AAA0102003); National Science Foundation of USA(grant numbers:OIA-1946231); Science and Technology Foundation of Dalian(grant numbers:2018J11CY010); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210842","Generative adversarial network;knowledge distillation;Text-to-Image Generation;alternate attention-transfer mechanism","Task analysis;Semantics;Generators;Generative adversarial networks;Knowledge engineering;Feature extraction;Image generation","image coding;neural nets;text analysis","KT-GAN;text-to-image synthesis;text-to-image generation;AATM;image encoder;image-to-image task;text encoder;text features;higher-quality images;word attention weights;knowledge-transfer generative adversarial network;semantic distillation mechanism;image subregions;alternate attention-transfer mechanism;cross-domain gap","","11","","43","IEEE","1 Oct 2020","","","IEEE","IEEE Journals"
"VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers","S. A. Kamran; K. F. Hossain; A. Tavakkoli; S. L. Zuckerbrod; S. A. Baker","University of Nevada, Reno; University of Nevada, Reno; University of Nevada, Reno; Houston Eye Associates; University of Nevada, Reno","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","3228","3238","In Fluorescein Angiography (FA), an exogenous dye is injected in the bloodstream to image the vascular structure of the retina. The injected dye can cause adverse reactions such as nausea, vomiting, anaphylactic shock, and even death. In contrast, color fundus imaging is a non-invasive technique used for photographing the retina but does not have sufficient fidelity for capturing its vascular structure. The only non-invasive method for capturing retinal vasculature is optical coherence tomography-angiography (OCTA). However, OCTA equipment is quite expensive, and stable imaging is limited to small areas on the retina. In this paper, we propose a novel conditional generative adversarial network (GAN) capable of simultaneously synthesizing FA images from fundus photographs while predicting retinal degeneration. The proposed system has the benefit of addressing the problem of imaging retinal vasculature in a non-invasive manner as well as predicting the existence of retinal abnormalities. We use a semi-supervised approach to train our GAN using multiple weighted losses on different modalities of data. Our experiments validate that the proposed architecture exceeds recent state-of-the-art generative networks for fundus-to-angiography synthesis. Moreover, our vision transformer-based discriminators generalize quite well on out-of-distribution data sets for retinal disease prediction.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00362","National Aeronautics and Space Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607858","","Integrated optics;In vivo;Computer architecture;Retina;Generative adversarial networks;Transformers;Optical imaging","biomedical optical imaging;blood vessels;diseases;eye;medical image processing;optical tomography","semisupervised retinal image synthesis;vision transformers;Fluorescein Angiography;exogenous dye;vascular structure;retina;injected dye;adverse reactions;anaphylactic shock;color fundus imaging;noninvasive technique;sufficient fidelity;noninvasive method;retinal vasculature;optical coherence tomography-angiography;OCTA equipment;stable imaging;novel conditional generative adversarial network capable;GAN;FA images;fundus photographs;retinal degeneration;noninvasive manner;retinal abnormalities;semisupervised approach;recent state-of-the-art generative networks;fundus-to-angiography synthesis;vision transformer-based discriminators;retinal disease prediction","","10","","58","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Recent Progress of Face Image Synthesis","Z. Lu; Z. Li; J. Cao; R. He; Z. Sun","University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China","2017 4th IAPR Asian Conference on Pattern Recognition (ACPR)","16 Dec 2018","2017","","","7","12","Face synthesis has been a fascinating yet challenging problem in computer vision and machine learning. Its main research effort is to design algorithms to generate photo-realistic face images via given semantic domain. It has been a crucial prepossessing step of main-stream face recognition approaches and an excellent test of AI ability to use complicated probability distributions. In this paper, we provide a comprehensive review of typical face synthesis works that involve traditional methods as well as advanced deep learning approaches. Particularly, Generative Adversarial Net (GAN) is highlighted to generate photo-realistic and identity preserving results. Furthermore, the public available databases and evaluation metrics are introduced in details. We end the review with discussing unsolved difficulties and promising directions for future research.","2327-0985","978-1-5386-3354-0","10.1109/ACPR.2017.2","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575791","","Face;Gallium nitride;Three-dimensional displays;Hidden Markov models;Generative adversarial networks;Computational modeling;Shape","computer vision;face recognition;learning (artificial intelligence);probability","photo-realistic face images;crucial prepossessing step;recognition approaches;AI ability;complicated probability distributions;typical face synthesis works;advanced deep learning approaches;photo-realistic identity preserving results;public available databases;evaluation metrics;face image synthesis;computer vision;machine learning;semantic domain;generative adversarial net;GAN","","10","","76","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"C4Synth: Cross-Caption Cycle-Consistent Text-to-Image Synthesis","K. J. Joseph; A. Pal; S. Rajanala; V. N. Balasubramanian","Indian Institute of Technology Hyderabad, India; Indian Institute of Technology, Hyderabad, India; Indian Institute of Technology, Hyderabad, India; Indian Institute of Technology, Hyderabad, India","2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","7 Mar 2019","2019","","","358","366","Generating an image from its description is a challenging task worth solving because of its numerous practical applications ranging from image editing to virtual reality. All existing methods use one single caption to generate a plausible image. A single caption by itself, can be limited and may not be able to capture the variety of concepts and behavior that would be present in the image. We propose two deep generative models that generate an image by making use of multiple captions describing it. This is achieved by ensuring 'Cross-Caption Cycle Consistency' between the multiple captions and the generated image(s). We report quantitative and qualitative results on the standard Caltech-UCSD Birds (CUB) and Oxford-102 Flowers datasets to validate the efficacy of the proposed approach.","1550-5790","978-1-7281-1975-5","10.1109/WACV.2019.00044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658689","","Gallium nitride;Generators;Generative adversarial networks;Image generation;Computational modeling;Transforms;Data models","image capture;image processing;virtual reality;visual databases","image editing;virtual reality;plausible image;deep generative models;multiple captions;generated image;C4Synth;cross-caption cycle-consistent text-to-image synthesis;cross-caption cycle consistency;standard Caltech-UCSD Birds;Oxford-102 Flowers datasets","","8","","28","IEEE","7 Mar 2019","","","IEEE","IEEE Conferences"
"Instance Mask Embedding and Attribute-Adaptive Generative Adversarial Network for Text-to-Image Synthesis","J. Ni; S. Zhang; Z. Zhou; J. Hou; F. Gao","School of Software, Qufu Normal University, Qufu, China; School of Software, Qufu Normal University, Qufu, China; School of Software, Qufu Normal University, Qufu, China; School of Software, Qufu Normal University, Qufu, China; School of Software, Qufu Normal University, Qufu, China","IEEE Access","2 Mar 2020","2020","8","","37697","37711","Existing image generation models have achieved the synthesis of reasonable individuals and complex but low-resolution images. Directly from complicated text to high-resolution image generation still remains a challenge. To this end, we propose the instance mask embedding and attribute-adaptive generative adversarial network (IMEAA-GAN). Firstly, we use the box regression network to compute a global layout containing the class labels and locations for each instance. Then the global generator encodes the layout, combines the whole text embedding and noise to preliminarily generate a low-resolution image; the instance embedding mechanism is used firstly to guide local refinement generators obtain fine-grained local features and generate a more realistic image. Finally, in order to synthesize the exact visual attributes, we introduce the multi-scale attribute-adaptive discriminator, which provides local refinement generators with the specific training signals to explicitly generate instance-level features. Extensive experiments based on the MS-COCO dataset and the Caltech-UCSD Birds-200-2011 dataset show that our model can obtain globally consistent attributes and generate complex images with local texture details.","2169-3536","","10.1109/ACCESS.2020.2975841","National Natural Science Foundation of China(grant numbers:61601261); Plan Project of Graduate Education Quality Improvement of Shandong Province(grant numbers:SDYY17136); Interdisciplinary Research Project of Qufu Normal University(grant numbers:QFNUSKC291809120); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007390","Generative adversarial network;global generator;local refinement generator;instance mask embedding;attribute-adaptive discriminator","Generators;Layout;Image generation;Gallium nitride;Generative adversarial networks;Visualization;Training","computer vision;feature extraction;image classification;image resolution;image texture;learning (artificial intelligence);neural nets","low-resolution image;instance embedding mechanism;local refinement generators;visual attributes;multiscale attribute-adaptive discriminator;instance-level features;instance mask embedding;attribute-adaptive generative adversarial network;text-to-image synthesis;high-resolution image generation;box regression network;global generator;IMEAA-GAN;MS-COCO dataset;Caltech-UCSD Birds-200-2011 dataset","","6","","51","CCBY","24 Feb 2020","","","IEEE","IEEE Journals"
"Fashion Attributes-to-Image Synthesis Using Attention-Based Generative Adversarial Network","H. Lee; S. -G. Lee",Seoul National University; Seoul National University,"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","7 Mar 2019","2019","","","462","470","In this paper, we present a method to generate fashion product images those are consistent with a given set of fashion attributes. Since distinct fashion attributes are related to different local sub-regions of a product image, we propose to use generative adversarial network with attentional discriminator. The attribute-attended loss signal from discriminator leads generator to generate more consistent images with given attributes. In addition, we present a generator based on Product-of-Gaussian to encode the composition of fashion attributes in effective way. To verify the proposed model whether it generates consistent image, an oracle attribute classifier is trained and judge the consistency of given attributes and the generated images. Our model significantly outperforms the baseline model in terms of correctness measured by the pre-trained oracle classifier. We show not only qualitative performance but also synthesized images with various combinations of attributes, so we can compare them with baseline model.","1550-5790","978-1-7281-1975-5","10.1109/WACV.2019.00055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658244","","Generators;Generative adversarial networks;Gallium nitride;Image generation;Neck;Image color analysis;Training","Gaussian processes;image classification;image coding","oracle attribute classifier;Product-of-Gaussian;attribute-attended loss signal;attentional discriminator;fashion product images;attention-based generative adversarial network;fashion attributes-to-image synthesis","","5","2","27","IEEE","7 Mar 2019","","","IEEE","IEEE Conferences"
"ReStGAN: A step towards visually guided shopper experience via text-to-image synthesis","S. Surya; A. Setlur; A. Biswas; S. Negi",Amazon; Amazon; Amazon; Amazon,"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","1189","1197","E-commerce companies like Amazon, Alibaba and Flip-kart have an extensive catalogue comprising of billions of products. Matching customer search queries to plausible products is challenging due to the size and diversity of the catalogue. These challenges are compounded in apparel due to the semantic complexity and a large variation of fashion styles, product attributes and colours. Providing aids that can help the customer visualise the styles and colours matching their ""search queries"" will provide customers with necessary intuition about what can be done next. This helps the customer buy a product with the styles, embellishments and colours of their liking. In this work, we propose a Generative Adversarial Network (GAN) for generating images from text streams like customer search queries. Our GAN learns to incrementally generate possible images complementing the fine-grained style, colour of the apparel in the query. We incorporate a novel colour modelling approach enabling the GAN to render a wide spectrum of colours accurately. We compile a dataset from an e-commerce website to train our model. The proposed approach outperforms the baselines on qualitative and quantitative evaluations.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093459","","Gallium nitride;Image color analysis;Generators;Training;Generative adversarial networks;Image resolution;Data models","clothing;data visualisation;electronic commerce;image colour analysis;image retrieval;learning (artificial intelligence);neural nets;retail data processing;text analysis;user experience;Web services;Web sites","ReStGAN;visually guided shopper experience;text-to-image synthesis;e-commerce companies;apparel;Generative Adversarial Network;GAN;text streams;customer search queries;image generation;colour modelling;e-commerce Website;customer visualisation","","2","","26","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Small animal PET to CT image synthesis based on conditional generation network","J. Li; Y. Wang; Y. Yang; X. Zhang; Z. Qu; S. Hu","School of Information Science and Engineering, Linyi University, Linyi, China; Library, Linyi University, Linyi, China; School of Information Science and Engineering, Linyi University, Linyi, China; School of Environmental Science and Engineering, Qingdao University, Qingdao, China; School of Chemistry and Chemical Engineering, Linyi University, Linyi, China; School of Information Science and Engineering, Linyi University, Linyi, China","2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)","7 Dec 2021","2021","","","1","6","During the image reconstruction process of Positron Emission Computed Tomography (PET), the tissue of the object being photographed will undergo Compton scattering with photons generated by the annihilation of the positive and negative electrons, which will cause the reconstructed image to be blurred. Traditional PET equipment will add a Computed Tomography (CT) equipment module. During PET shooting process, CT images are taken at the same time, and the CT images are used for attenuation correction to reduce scattering effect of PET. Especially in PET equipment that requires high accuracy, attenuation correction has a greater impact on the quality of the final PET reconstruction. The CT image is generally used for attenuation correction, not for final diagnosis. The additional CT shooting will bring more radiation to the shooting target and increase manufacturing cost of the equipment. This paper proposes a method of generating Synthetic CT image from PET based on Generative Adversarial Network (GAN). The generator of our network uses an improved generator based on encoder-decoder structure, and the discriminator has a local field of view. This article compares the effects of several loss functions respectively. These loss functions successfully improve the model's capabilities and results. The quantitative and qualitative results show that our proposed method outperforms common methods.","","978-1-6654-0004-6","10.1109/CISP-BMEI53629.2021.9624232","National Natural Science Foundation of China(grant numbers:61771230); Shandong Provincial Natural Science Foundation(grant numbers:ZR2020QF011); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9624232","Deep learning;Generative Adversarial Network;Medical image translation;PET attenuation correction","Training;Image quality;Three-dimensional displays;Computed tomography;Scattering;Attenuation;Generative adversarial networks","computerised tomography;image reconstruction;image resolution;medical image processing;positron emission tomography","CT image synthesis;conditional generation network;image reconstruction process;Positron Emission Computed Tomography;reconstructed image;traditional PET equipment;Computed Tomography equipment module;PET shooting process;attenuation correction;final PET reconstruction;additional CT shooting;shooting target;increase manufacturing cost;Synthetic CT image;Generative Adversarial Network;improved generator;animal PET","","","","19","IEEE","7 Dec 2021","","","IEEE","IEEE Conferences"
"GAN and IEC Approach for Image Generation","A. Shukla; H. F. Mahdi; I. Tandon; R. Singh; T. Choudhury","School of Computer Science, University of Petroleum and Energy Studies, Dehradun, India; Department of Computer and Software Engineering, University of Diyala, Baquba, Iraq; School of Computer Science, University of Petroleum and Energy Studies, Dehradun, India; School of Computer Science, University of Petroleum and Energy Studies, Dehradun, India; School of Computer Science, University of Petroleum and Energy Studies, Dehradun, India","2022 International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)","14 Nov 2022","2022","","","907","911","Generative adversarial networks (GANs) are capable of learning deep representations efficiently without requiring a significant amount of annotation. Through Backpropagation, signals are derived from one network by another through a competitive process. This article presents a machine learning framework that incorporates generative adversarial networks (GANs) and interactive evolutionary computation (IEC) to generate images and to resample those images for the detection of fake images. In our study, we found that GANs trained on a particular domain could produce reliable and compact phenotypic maps. Through the use of a user research method, participants able to produce images closely matched target images, demonstrating the advantages of this unique approach.","2770-7962","978-1-6654-7013-1","10.1109/ISMSIT56059.2022.9932848","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9932848","Generative Adversarial Network;Interactive Evolutionary Computation (IEC);Image Generation","Training;Backpropagation;Tensors;Image synthesis;Convolution;IEC;Evolutionary computation","backpropagation;evolutionary computation;image matching;image representation;learning (artificial intelligence)","competitive process;deep representations;fake images;GAN;generative adversarial networks;IEC;image generation;interactive evolutionary computation;machine learning framework;target images","","","","19","IEEE","14 Nov 2022","","","IEEE","IEEE Conferences"
"Adversarial Network for edge detection","Z. Zeng; Y. K. Yu; K. Hong Wong","Department of Computer science and Engineering, The Chinese University of Hong Kong; Department of Computer science and Engineering, The Chinese University of Hong Kong; Department of Computer science and Engineering, The Chinese University of Hong Kong","2018 Joint 7th International Conference on Informatics, Electronics & Vision (ICIEV) and 2018 2nd International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","14 Feb 2019","2018","","","19","23","Edge detection is a fundamental problem in computer vision and has been explored for many decades. Due to the rapid development of machine learning techniques and their applications to image processing, there is a proliferation of neural network-based approaches to solve the edge detection problem. These methods, such as richer convolutional features for edge detection(RCF), have good performance and even outperform human beings. Most of the existing neural network-based systems use the convolutional network or its variant. They usually produce thick edges and the application of non-maximum-suppression to suppress the edge is necessary. In this paper, we explore another type of neural network called the conditional generative adversarial network (cGAN) to address the edge detection problem. cGAN is an innovative framework to do the image synthesis task. It can generate an image close to the real one. After training, our network can produce an edge map that contains more detailed information and thinner edges compared to the state-of-the-art methods that require the well-known non-maximum-suppression for post-processing. The proposed approach is able to produce a high quality edge map directly without further processing. Our solution is computation efficient. It can achieve a speed of 59 and 26 frames per second (fps) for an image resolution of [256×256×3] and [512×512×3], respectively.","","978-1-5386-5163-6","10.1109/ICIEV.2018.8641005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641005","","Image edge detection;Generators;Gallium nitride;Training;Neural networks;Generative adversarial networks;Feature extraction","convolutional neural nets;edge detection;image resolution","edge detection;image resolution;image synthesis task;conditional generative adversarial network;convolutional network;neural network-based approaches;image processing;machine learning techniques;high quality edge map","","4","1","33","IEEE","14 Feb 2019","","","IEEE","IEEE Conferences"
"A Generative Adversarial Network for AI-Aided Chair Design","Z. Liu; F. Gao; Y. Wang","National Engineering Lab for Video Technology, Peking University, Beijing, China; The Future Lab, Tsinghua University, Beijing, China; National Engineering Lab for Video Technology, Peking University, Beijing, China","2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)","25 Apr 2019","2019","","","486","490","We present a method for improving human design of chairs. The goal of the method is generating enormous chair candidates in order to facilitate human designer by creating sketches and 3d models accordingly based on the generated chair design. It consists of an image synthesis module, which learns the underlying distribution of training dataset, a super-resolution module, which improve quality of generated image and human involvements. Finally, we manually pick one of the generated candidates to create a real life chair for illustration.","","978-1-7281-1198-8","10.1109/MIPR.2019.00098","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695313","deep learning;GAN;AI-aided Design","Image resolution;Image generation;Prototypes;Training;Generative adversarial networks;Shape;Generators","furniture;image resolution;learning (artificial intelligence);solid modelling;stereo image processing","generated chair design;image synthesis module;super-resolution module;generated image;generated candidates;life chair;generative adversarial network;AI-aided chair design;human designer;chair candidates","","3","","26","IEEE","25 Apr 2019","","","IEEE","IEEE Conferences"
"Weakly Supervised Attention Inference Generative Adversarial Network for Text-to-Image","L. Mei; X. Ran; J. Hu","School of Civil Engineering, Chongqing Jiaotong University, Chongqing, China; School of Mathematics and Statistics, Chongqing Jiaotong University, Chongqing, China; School of Mathematics and Statistics, Chongqing Jiaotong University, Chongqing, China","2019 IEEE Symposium Series on Computational Intelligence (SSCI)","20 Feb 2020","2019","","","1574","1578","Text-to-Image is a significant problem in computer vision. Recently, there are some problems in the quality and semantic consistency of the generated image. In this paper we propose an approach for Text-to-Image synthesis by focusing on the perception. We use text embeddings to generate semantic feature maps before target images synthesis instead of generating target images directly. The ground truth semantic layouts are calculated by interpretable classification network, and we will learn to generate semantic layouts before inferring target images from them. We have trained our approach on the CUB2011 dataset and verified the quality of its generation and the interpretability of the network in simple background and small scale feature generation.","","978-1-7281-2485-8","10.1109/SSCI44817.2019.9002666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9002666","attention inference;generative adversarial network;Text-to-Image.","Feature extraction;Semantics;Task analysis;Image generation;Gallium nitride;Birds;Generative adversarial networks","computer vision;feature extraction;image classification;neural nets;supervised learning;text analysis","text embeddings;semantic feature maps;supervised attention inference generative adversarial network;computer vision;text-to-image synthesis;classification network","","1","","36","IEEE","20 Feb 2020","","","IEEE","IEEE Conferences"
"Intensity-aware GAN for Single Image Reflection Removal","N. -H. Chou; L. -C. Chuang; M. -S. Lee","Department of Computer Science and Information Engineering, Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan","2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","5 Mar 2020","2019","","","590","594","Single image reflection removal is a challenging task in computer vision. Most existing approaches rely on carefully handcrafted priors to solve the problem. Contrast to the optimization-based methods, an intensity-aware GAN with dual generators is proposed to directly estimate the function which transforms the mixture image into the reflection image itself. From the observation that the reflection layer has more discriminating power in the region with low intensity than that in the region with high intensity, the proposed architecture better describes the characteristic of the model. Moreover, a reflection image synthesis method based on the screen blending model is also presented. Experimental results demonstrate that the results of reflection removal are satisfactory in real cases while comparing with state-of-the-art methods.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023087","","Reflection;Generators;Gallium nitride;Generative adversarial networks;Additives;Glass;Adaptation models","computer vision;image restoration;neural nets;optimisation;supervised learning;transforms","reflection image synthesis method;intensity-aware GAN;single image reflection removal;optimization-based methods;dual generators;screen blending;computer vision","","1","","15","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Image Alpha Matting via Residual Convolutional Grid Network","H. Zhang; Y. Zhou; L. Chen; J. Zhao","School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, ON, Canada","2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","28 Jan 2020","2019","","","1","5","Alpha matting is an important topic in areas of computer vision. It has various applications, such as virtual reality, digital image and video editing, and image synthesis. Conventional approaches for alpha matting do not perform well when they encounter complicated background or when foreground and background color distributions overlap. It is also difficult to extract alpha matte accurately when the foreground objects are semi-transparent or hairy. In this paper, we propose a residual convolutional grid network for alpha matting, which deals with those matting problems well and has a performance comparable to the best image matting method in the literature. Meanwhile, the number of parameters in our method is less than one-third of the number of parameters in the best method.","","978-1-7281-2723-1","10.1109/GlobalSIP45357.2019.8969197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8969197","Alpha matting;deep learning;convolutional neural networks (CNNs);residual block;grid network","Image color analysis;Deep learning;Training;Semantics;Network architecture;Gallium nitride;Generative adversarial networks","computer vision;convolutional neural nets;image colour analysis","background color distributions;foreground color distributions;computer vision;image matting method;matting problems;alpha matte;image synthesis;video editing;digital image;residual convolutional grid network;image alpha matting","","","","23","IEEE","28 Jan 2020","","","IEEE","IEEE Conferences"
"Blind Image Quality Assessment for a Single Image From Text-to-Image Synthesis","W. Yu; X. Zhang; Y. Zhang; Z. Zhang; J. Zhou","School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, Sichuan, China; School of Computer Science and Technology, Southwest University of Science and Technology, Mianyang, Sichuan, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; Graduate School of Science and Engineering, Hosei University, Tokyo, Japan","IEEE Access","8 Jul 2021","2021","9","","94656","94667","A fundamental bottleneck in text-to-image synthesis is that there are rarely subjective quality evaluation metrics for a single generated image. To address this issue, this paper proposed a procedure to evaluate the single generated image, which includes a specific dataset named multiple metrics quality assessment for birds(MMQA Birds) and a learning model named blind generated image evaluator(BGIE). The motivation of our proposal is twofold. On the one hand, subjective image quality evaluation is a human perceptual task; Therefore, it tends to be a process of supervised learning. To the best of our knowledge, there are not any datasets for this study. Thus, we handle this problem via designing a specific dataset. On the other hand, we observed that the spatial content of generated image attracts more attention when humans judge its quality; According to this finding, an efficient machine-learning model that combines both pixel-level features and spatial features is proposed. Extensive experiments manifest our method can solve this problem to some extent. In the generated image dataset, BGIE surpasses the state-of-art NSS-based method by 6.3% in PLCC and SRCC. In practice, we further discuss the rationality of the MMQA Birds dataset and the application of BGIE. It proves that both in subjective and objective aspects, our method achieves convincing results.","2169-3536","","10.1109/ACCESS.2021.3094048","Sichuan Science and Technology Program(grant numbers:2020YFS0307,2020YFG0430,2019YFS0146); Mianyang Science and Technology Program(grant numbers:2020YFZJ016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9469895","Generated image quality assessment;generative adversarial networks;image quality evaluation dataset","Image quality;Distortion;Task analysis;Measurement;Birds;Training;Semantics","feature extraction;image processing;supervised learning","blind image quality assessment;single image;text-to-image synthesis;single generated image;subjective image quality evaluation;machine learning;subjective quality evaluation metrics;multiple metrics quality assessment;MMQA birds dataset;blind generated image evaluator;BGIE;supervised learning","","1","","34","CCBY","1 Jul 2021","","","IEEE","IEEE Journals"
"Synthesis of Mammogram From Digital Breast Tomosynthesis Using Deep Convolutional Neural Network With Gradient Guided cGANs","G. Jiang; J. Wei; Y. Xu; Z. He; H. Zeng; J. Wu; G. Qin; W. Chen; Y. Lu","School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Radiology, University of Michigan, Ann Arbor, MI, USA; Department of Mathematics and Statistics, Old Dominion University, Norfolk, VA, USA; Department of Radiology, Nanfang Hospital, Southern Medical University, Guangzhou, China; Department of Radiology, Nanfang Hospital, Southern Medical University, Guangzhou, China; Department of Radiology, Nanfang Hospital, Southern Medical University, Guangzhou, China; Department of Radiology, Nanfang Hospital, Southern Medical University, Guangzhou, China; Department of Radiology, Nanfang Hospital, Southern Medical University, Guangzhou, China; Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, Guangzhou, China","IEEE Transactions on Medical Imaging","30 Jul 2021","2021","40","8","2080","2091","Synthetic digital mammography (SDM), a 2D image generated from digital breast tomosynthesis (DBT), is used as a potential substitute for full-field digital mammography (FFDM) in clinic to reduce the radiation dose for breast cancer screening. Previous studies exploited projection geometry and fused projection data and DBT volume, with different post-processing techniques applied on re-projection data which may generate different image appearance compared to FFDM. To alleviate this issue, one possible solution to generate an SDM image is using a learning-based method to model the transformation from the DBT volume to the FFDM image using current DBT/FFDM combo images. In this study, we proposed to use a deep convolutional neural network (DCNN) to learn the transformation to generate SDM using current DBT/FFDM combo images. Gradient guided conditional generative adversarial networks (GGGAN) objective function was designed to preserve subtle MCs and the perceptual loss was exploited to improve the performance of the proposed DCNN on perceptual quality. We used various image quality criteria for evaluation, including preserving masses and MCs which are important in mammogram. Experiment results demonstrated progressive performance improvement of network using different objective functions in terms of those image quality criteria. The methodology we exploited in the SDM generation task to analyze and progressively improve image quality by designing objective functions may be helpful to other image generation tasks.","1558-254X","","10.1109/TMI.2021.3071544","China Department of Science and Technology(grant numbers:2018YFC1704206,2016YFB0200602); NSFC(grant numbers:81971691,81801809,81830052,81827802,U1811461,11401601); Science and Technology Program of Guangzhou(grant numbers:201804020053); Science and Technology Innovative Project of Guangdong Province(grant numbers:2016B030307003,2015B010110003,2015B020233008); Science and Technology Program of Guangzhou(grant numbers:201906010014); Department of Science and Technology of Jilin Province(grant numbers:20190302108GX); Guangdong Provincial Science and Technology(grant numbers:2017B020210001); Guangzhou Science and Technology Creative(grant numbers:201604020003); Guangdong Province Key Laboratory of Computational Science Open(grant numbers:2018009); Construction Project of Shanghai Key Laboratory of Molecular Imaging(grant numbers:18DZ2260400); US National Science Foundation(grant numbers:DMS-1912958); Guangdong Province Key Laboratory of Computational Science at Sun Yat-sen University(grant numbers:2020B1212060032); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9398655","Breast cancer;digital breast tomosynthesis;image synthesis;deep learning;generative adversarial networks","Two dimensional displays;Image edge detection;Biomedical imaging;Linear programming;Mammography;Image reconstruction;Three-dimensional displays","cancer;diagnostic radiography;image enhancement;image reconstruction;learning (artificial intelligence);mammography;medical image processing;neural nets","FFDM image;deep convolutional neural network;conditional generative adversarial networks objective function;image quality criteria;SDM generation task;image generation tasks;digital breast tomosynthesis;gradient guided;synthetic digital mammography;full-field digital mammography;breast cancer screening;projection geometry;DBT volume;different post-processing techniques;re-projection data;different image appearance;SDM image","Early Detection of Cancer;Mammography;Neural Networks, Computer;Radiographic Image Enhancement","11","","45","IEEE","7 Apr 2021","","","IEEE","IEEE Journals"
"Lifelong learning of interpretable image representations","F. Ye; A. G. Bors","Department of Computer Science, University of York, York, UK; Department of Computer Science, University of York, York, UK","2020 Tenth International Conference on Image Processing Theory, Tools and Applications (IPTA)","14 Dec 2020","2020","","","1","6","Existing machine learning systems are trained to adapt to a single database and their ability to acquire additional information is limited. Catastrophic forgetting occurs in all deep learning systems when attempting to train them with additional databases. The information learnt previously is forgotten and no longer recognized when such a learning systems is trained using a new database. In this paper, we develop a new image generation approach defined under the lifelong learning framework which prevents forgetting. We employ the mutual information maximization between the latent variable space and the outputs of the generator network in order to learn interpretable representations, when learning using the data from a series of databases sequentially. We also provide the theoretical framework for the generative replay mechanism, under the lifelong learning setting. We perform a series of experiments showing that the proposed approach is able to learn a set of disjoint data distributions in a sequential manner while also capturing meaningful data representations across domains.","2154-512X","978-1-7281-8750-1","10.1109/IPTA50016.2020.9286663","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286663","Lifelong learning;Representation learning;Generative Adversarial Networks;Mutual information","Learning systems;Interpolation;Databases;Image synthesis;Tools;Generators;Mutual information","deep learning (artificial intelligence);image representation","interpretable image representations;machine learning systems;single database;catastrophic forgetting;deep learning systems;image generation;lifelong learning framework;information maximization;generative replay mechanism;data representations","","5","","28","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"Watermark Faker: Towards Forgery of Digital Image Watermarking","R. Wang; C. Lin; Q. Zhao; F. Zhu","College of Computer Science, Sichuan University, China; College of Computer Science, Sichuan University, China; College of Computer Science, Sichuan University, China; College of Computer Science, Sichuan University, China","2021 IEEE International Conference on Multimedia and Expo (ICME)","9 Jun 2021","2021","","","1","6","Digital watermarking has been widely used to protect the copyright and integrity of multimedia data. Previous studies mainly focus on designing watermarking techniques that are robust to attacks of destroying the embedded watermarks. However, the emerging deep learning based image generation technology raises new open issues that whether it is possible to generate fake watermarked images for circumvention. In this paper, we make the first attempt to develop digital image watermark fakers by using generative adversarial learning. Suppose that a set of paired images of original and watermarked images generated by the targeted watermarker are available, we use them to train a watermark faker with U-Net as the backbone, whose input is an original image, and after a domain-specific preprocessing, it outputs a fake watermarked image. Our experiments show that the proposed watermark faker can effectively crack digital image watermarkers in both spatial and frequency domains, suggesting the risk of such forgery attacks.","1945-788X","978-1-6654-3864-3","10.1109/ICME51207.2021.9428410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428410","Digital watermarking;generative adversarial networks (GANs);image-to-image translation;forgery","Deep learning;Image synthesis;Digital images;Frequency-domain analysis;Conferences;Data preprocessing;Watermarking","convolutional neural nets;copy protection;copyright;data integrity;deep learning (artificial intelligence);frequency-domain analysis;image coding;image forensics;image watermarking","generative adversarial learning;fake watermarked image;digital image watermarkers;digital image watermarking forgery;watermarking techniques;deep learning based image generation technology;digital image watermark fakers;copyright;multimedia data integrity;U-Net;domain-specific preprocessing;spatial domain;frequency domain","","1","","24","IEEE","9 Jun 2021","","","IEEE","IEEE Conferences"
"3D Mesh Generation by Introducing Extended Attentive Normalization","Y. Fukatsu; M. Aono","Department of Computer Science & Engineering, Toyohashi University of Technology, Toyohashi, Aichi, Japan; Department of Computer Science & Engineering, Toyohashi University of Technology, Toyohashi, Aichi, Japan","2021 8th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)","16 Dec 2021","2021","","","1","6","In recent years, research on conditional image generation using GANs of the type where conditions are given by class labels or texts has been successful. On the other hand, the generation of conditional 3D models consisting of 3D meshes is still in its infancy. In this research, we add global information based on Attentive Normalization to local information using CNN to improve 3D mesh generation. Specifically, we propose Conditional Attentive Normalization, which is an extension of Attentive Normalization and can add conditional information. Comparative experiments conditioned by class labels and texts have been carried out by using Caltech-UCSD Birds-200-201. It turns out that our proposed method outperforms the conventional methods.","","978-1-6654-1743-3","10.1109/ICAICTA53211.2021.9640290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9640290","3D Mesh Generation;Generative Adversarial Networks;Attentive Normalization","Solid modeling;Three-dimensional displays;Image synthesis;Data visualization;Mesh generation","image processing;mesh generation;neural nets","local information;3D mesh generation;conditional information;class labels;conditional image generation;conditional 3D models;global information;extended attentive normalization;GANs;CNN;conditional attentive normalization;Caltech-UCSD Birds-200-201","","","","20","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"SSTRN: Semantic Style Transfer Reference Network for Face Super-Resolution","S. Farhangfar; A. Baradarani; M. A. Balafar; M. Asadpour","Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran; Faculty of Electrical and Computer Engineering, University of Tabriz, Tabriz, Iran","2022 29th International Conference on Systems, Signals and Image Processing (IWSSIP)","17 Aug 2022","2022","CFP2255E-ART","","1","4","Reference super-resolution (RefSR) has achieved promising results in the single image super-resolution (SISR) field by providing additional details from the reference images. Existing RefSR methods usually tend to extract similar or aligned features from reference images to further enhance the resolution of the final result. Therefore, the efficiency of RefSR models highly depends on the conformity between extracted features from the low-resolution (LR) and reference images. In this paper, we propose a new reference image generation scheme via semantic style transfer to unleash our model from relevant feature extraction computations. The generated reference images have the most content similarity and identical alignment with the LR input that compensates for the lost details of the LR images. Despite previous RefSR methods that rely on extracting and transferring texture information from the reference image to LR input, provided reference images are enriched with the style information of high-resolution (HR) images. Extensive experiments indicate the effectiveness of the proposed reference images.","2157-8702","978-1-6654-9578-3","10.1109/IWSSIP55020.2022.9854432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9854432","Semantic style transfer;Reference-based super-resolution;Generative adversarial networks;Face images","Image synthesis;Computational modeling;Semantics;Superresolution;Feature extraction;Data mining;Signal resolution","face recognition;feature extraction;image enhancement;image resolution;image texture","image enhancement;texture information;LR images;semantic style transfer;RefSR model;SISR model;feature extraction;SSTRN;high-resolution images;reference image generation scheme;low-resolution images;single image super-resolution field;reference super-resolution;face super-resolution;semantic style transfer reference network","","","","16","IEEE","17 Aug 2022","","","IEEE","IEEE Conferences"
"LWComicGAN: A Lightweight Method for Realizing Scene Animation","Q. Yang; W. Chen; Y. Cai; X. Liu; T. Liu; G. Wang","College of Intelligent Equipment, Shandong University of Science and Technology, Tai'an, China; College of Intelligent Equipment, Shandong University of Science and Technology, Tai'an, China; College of Intelligent Equipment, Shandong University of Science and Technology, Tai'an, China; College of Intelligent Equipment, Shandong University of Science and Technology, Tai'an, China; College of Intelligent Equipment, Shandong University of Science and Technology, Tai'an, China; College of Intelligent Equipment, Shandong University of Science and Technology, Tai'an, China","2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Aug 2022","2022","10","","2285","2289","The style transfer algorithm was originally proposed to solve the generation problem of art paintings. In recent years, the generation of animation style images has gradually become a hot research direction. The content presented in many animation film and television works is fascinating. In order to satisfy people's desire to turn real scenes into animation scenes and reduce the workload of animation producers, a Light Weight Animation Generated Adversarial Network (LWComicGAN) is proposed, which can reduce the amount of parameters and enable low-memory devices to complete network training. An optional instance layer normalization function is designed to adapt the input of each layer, and an optional instance layer residual block is proposed. The LWComicGAN algorithm uses the objective function of WGAN-GP and other various loss functions as the total loss, and also considers the gradient penalty mechanism in discriminator. The former guarantees the generation quality of all aspects of the image, and the latter guarantees the stability of the training process. The effectiveness of the proposed algorithm is verified after animation transfer experiments of realistic landscapes and characters, and we have produced an ink painting dataset and completed ink animation style transfer.","2693-2865","978-1-6654-2207-9","10.1109/ITAIC54216.2022.9836461","Shandong University of Science and Technology(grant numbers:2013KYTD04,2014KYPT30); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836461","deep learning;image style transfer;generative adversarial networks;photo animation;LWComicGAN","Training;TV;Image synthesis;Conferences;Ink;Animation;Linear programming","art;computer animation;learning (artificial intelligence)","loss functions;generation quality;animation transfer experiments;ink animation style transfer;lightweight method;realizing scene;style transfer algorithm;generation problem;art paintings;animation style images;hot research direction;animation film;television works;people;animation scenes;animation producers;Light Weight Animation Generated Adversarial Network;low-memory devices;network training;optional instance layer normalization function;optional instance layer residual block;LWComicGAN algorithm;objective function","","","","22","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"Analysis of Image Generation Techniques","A. Chaudhary; A. P. Singh; A. Krishna","Information Technology, Delhi Technological University, New Delhi, India; Information Technology, Delhi Technological University, New Delhi, India; Information Technology, Delhi Technological University, New Delhi, India","2021 3rd International Conference on Advances in Computing, Communication Control and Networking (ICAC3N)","9 Mar 2022","2021","","","913","915","In today’s world machines are getting smarter by the day and generation of images is one such task that can be delegated to the machines and they are performing to near perfection and due to this distinguishing between original images and machine generated images is becoming more and more difficult. In this we are reviewing such methods used for generating images and results or problems related to such methods.","","978-1-6654-3811-7","10.1109/ICAC3N53548.2021.9725629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9725629","Variational Autoencoders;Generative Adversarial Networks","Image synthesis;Focusing;Computer architecture;Stability analysis;Task analysis","image processing;neural nets","image generation techniques","","","","7","IEEE","9 Mar 2022","","","IEEE","IEEE Conferences"
"DeformableGAN: Generating Medical Images With Improved Integrity for Healthcare Cyber Physical Systems","Z. Shen; F. Ding; A. Jolfaei; K. Yadav; S. Vashisht; K. Yu","Hangzhou Dianzi University, Hangzhou, China; Nanchang University, Nanchang, China; College of Science and Engineering, Flinders University, Adelaide, Australia; College of Computer Science and Engineering, University of Ha&#x0027;il, Ha&#x0027;il, Saudi Arabia; Department of Computer Science and Engineering, Shree Guru Gobind Singh Tricentenary University, Gurugram, India; RIKEN Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan","IEEE Transactions on Network Science and Engineering","","2022","PP","99","1","13","The development of deep learning enables the production of new images via generative adversarial networks (GANs). The GANs have now been widely applied in the industry as well as academic research that brought tremendous progress to our community. Many researchers in medical imaging also introduced this novel technology for medical image reconstruction, segmentation, synthesis, etc. On the other hand, the GAN-generated images may also suffer from exhibiting unique textures, namely the checkerboard artifacts. For medical diagnosis and healthcare, such artifacts could bring negative impacts as they may distort information collected in medical images. Improper treatment and rehabilitation plans based on the disinformation of checkerboard artifacts could be harmful for patients and healthcare cyber physical systems. Thus, we investigate the checkerboard artifact synthesized during adversarial training in this paper. Based on the theoretical analysis, we propose a method for GANs to generate images without producing checkerboard artifacts. It could protect medical images preserving high integrity for healthcare cyber physical systems. Our experiments justify the efficiency of proposed method when associating with a variety of GANs for image synthesis. Also, we prove that it is feasible to detect GAN-generated images by tracing the checkerboard artifacts.","2327-4697","","10.1109/TNSE.2022.3190765","Fundamental Research Funds for the Provincial Universities of Zhejiang(grant numbers:GK229909299001-019); Jiangxi Double Thousand Plan(grant numbers:JXSQ201901075); Japan Society for the Promotion of Science (JSPS) Grants-in-Aid for Scientific Research (KAKENHI)(grant numbers:JP18K18044); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9831051","medical imaging;healthcare;checkerboard artifacts;generative adversarial networks;AI-driven cyber security","Generative adversarial networks;Convolution;Training;Medical diagnostic imaging;Deep learning;Electronic mail;Gray-scale","","","","","","","IEEE","15 Jul 2022","","","IEEE","IEEE Early Access Articles"
"High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs","T. -C. Wang; M. -Y. Liu; J. -Y. Zhu; A. Tao; J. Kautz; B. Catanzaro",NVIDIA Corporation; NVIDIA Corporation; UC Berkeley; NVIDIA Corporation; NVIDIA Corporation; NVIDIA Corporation,"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","8798","8807","We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048 × 1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8579015","","Generators;Image resolution;Semantics;Gallium nitride;Training;Image generation;Task analysis","data visualisation;image resolution;image segmentation;unsupervised learning","deep image synthesis;high-resolution photo-realistic images;semantic label maps;conditional generative adversarial networks;interactive visual manipulation;object instance segmentation information;object manipulations","","1547","7","64","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Image Synthesis From Reconfigurable Layout and Style","W. Sun; T. Wu","Department of ECE and the Visual Narrative Initiative, North Carolina State University; Department of ECE and the Visual Narrative Initiative, North Carolina State University","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","10530","10539","Despite remarkable recent progress on both unconditional and conditional image synthesis, it remains a long- standing problem to learn generative models that are capable of synthesizing realistic and sharp images from re- configurable spatial layout (i.e., bounding boxes + class labels in an image lattice) and style (i.e., structural and appearance variations encoded by latent vectors), especially at high resolution. By reconfigurable, it means that a model can preserve the intrinsic one-to-many mapping from a given layout to multiple plausible images with different styles, and is adaptive with respect to perturbations of a layout and style latent code. In this paper, we present a layout- and style-based architecture for generative adversarial networks (termed LostGANs) that can be trained end-to-end to generate images from reconfigurable layout and style. Inspired by the vanilla StyleGAN, the proposed LostGAN consists of two new components: (i) learning fine-grained mask maps in a weakly-supervised manner to bridge the gap between layouts and images, and (ii) learning object instance-specific layout-aware feature normalization (ISLA-Norm) in the generator to realize multi-object style generation. In experiments, the proposed method is tested on the COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art performance obtained. The code and pretrained models are available at https://github.com/iVMCL/LostGANs.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.01063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008768","","Layout;Image generation;Generators;Gallium nitride;Task analysis;Semantics;Image resolution","genomics;learning (artificial intelligence)","LostGAN;reconfigurable layout;fine-grained mask maps;object instance-specific layout-aware feature normalization;multiobject style generation;pretrained models;unconditional image synthesis;generative models;realistic images;sharp images;configurable spatial layout;class labels;image lattice;structural appearance variations;latent vectors;multiple plausible images;style latent code;style-based architecture;generative adversarial networks","","38","1","39","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"Extending HoloGAN by Embedding Image Content into Latent Vectors for Novel View Synthesis","J. Wang; L. E. Hafi; A. Taniguchi; Y. Hagiwara; T. Taniguchi","Ritsumeikan University, Kusatsu, Shiga, Japan; Ritsumeikan University, Kusatsu, Shiga, Japan; Ritsumeikan University, Kusatsu, Shiga, Japan; Ritsumeikan University, Kusatsu, Shiga, Japan; Ritsumeikan University, Kusatsu, Shiga, Japan","2022 IEEE/SICE International Symposium on System Integration (SII)","16 Feb 2022","2022","","","383","389","This study aims to further develop the task of novel view synthesis by generative adversarial networks (GAN). The goal of novel view synthesis is to, given one or more input images, synthesize images of the same target content but from different viewpoints. Previous research showed that the unsupervised learning model HoloGAN achieved high performance in generating images from different viewpoints. However, HoloGAN is less capable of specifying the target content to generate and is difficult to train due to high data requirements. Therefore, this study proposes two approaches to improve the current limitations of HoloGAN and make it suitable for the task of novel view synthesis. The first approach reuses the encoder network of HoloGAN to get the corresponding latent vectors of the image contents to specify the target content of the generated images. The second approach introduces an auto-encoder architecture to HoloGAN so that more viewpoints can be generated correctly. The experiment results indicate that the first approach is efficient in specifying a target content. Meanwhile, the second approach method helps HoloGAN to learn a richer range of viewpoints but is not compatible with the first approach. The combination of these two approaches and their application to service robotics are discussed in conclusion.","2474-2325","978-1-6654-4540-5","10.1109/SII52469.2022.9708823","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9708823","","Three-dimensional displays;Image resolution;System integration;Grasping;Generative adversarial networks;Planning;Task analysis","neural nets;service robots;stereo image processing;unsupervised learning;vectors","novel view synthesis;generative adversarial networks;image synthesis;unsupervised learning model;latent vectors;image contents;HoloGAN;image content embedding;autoencoder architecture;service robotics","","","","20","IEEE","16 Feb 2022","","","IEEE","IEEE Conferences"
"DEEPFAKE Image Synthesis for Data Augmentation","N. Waqas; S. I. Safie; K. A. Kadir; S. Khan; M. H. Kaka Khel","Department of Instrumentation and Control Engineering, Universiti Kuala Lumpur Malaysian Institute of Industrial Technology, Kuala Lumpur, Malaysia; Department of Instrumentation and Control Engineering, Universiti Kuala Lumpur Malaysian Institute of Industrial Technology, Kuala Lumpur, Malaysia; Electronic Section, British Malaysian Institute, Universiti Kuala Lumpur, Jalan Sungai Pusu, Kuala Lumpur, Selangor, Malaysia; Department of Electrical Engineering, Onaizah College of Engineering and Information Technology, Qassim, Saudi Arabia; Department of Instrumentation and Control Engineering, Universiti Kuala Lumpur Malaysian Institute of Industrial Technology, Kuala Lumpur, Malaysia","IEEE Access","8 Aug 2022","2022","10","","80847","80857","Field of medical imaging is scarce in terms of a dataset that is reliable and extensive enough to train distinct supervised deep learning models. One way to tackle this problem is to use a Generative Adversarial Network to synthesize DEEPFAKE images to augment the data. DEEPFAKE refers to the transfer of important features from the source image (or video) to the target image (or video), such that the target modality appears to animate the source almost close to reality. In the past decade, medical image processing has made significant advances using the latest state-of-art-methods of deep learning techniques. Supervised deep learning models produce super-human results with the help of huge amount of dataset in a variety of medical image processing and deep learning applications. DEEPFAKE images can be a useful in various applications like translating to different useful and sometimes malicious modalities, unbalanced datasets or increasing the amount of datasets. In this paper the data scarcity has been addressed by using Progressive Growing Generative Adversarial Networks (PGGAN). However, PGGAN consists of convolution layer that suffers from the training-related issues. PGGAN requires a large number of convolution layers in order to obtain high-resolution image training, which makes training a difficult task. In this work, a subjective self-attention layer has been added before  $256 \times 256$  convolution layer for efficient feature learning and the use of spectral normalization in the discriminator and pixel normalization in the generator for training stabilization - the two tasks resulting into what is referred to as Enhanced-GAN. The performance of Enhanced-GAN is compared to PGGAN performance using the parameters of AM Score and Mode Score. In addition, the strength of Enhanced-GAN and PGGAN synthesized data is evaluated using the U-net supervised deep learning model for segmentation tasks. Dice Coefficient metrics show that U-net trained on Enhanced-GAN DEEPFAKE data optimized with real data performs better than PGGAN DEEPFAKE data with real data.","2169-3536","","10.1109/ACCESS.2022.3193668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9839427","DEEPFAKE;PGGAN;self-attention layer;spectral normalization;unbalanced dataset","Deepfakes;Image resolution;Training data;Data models;Convolutional neural networks;Biomedical imaging","deep learning (artificial intelligence);image forensics;image resolution;image segmentation;medical image processing","DEEPFAKE image synthesis;data augmentation;medical imaging;distinct supervised deep learning models;DEEPFAKE images;source image;target image;target modality;medical image processing;state-of-art-methods;deep learning techniques;super-human results;deep learning applications;unbalanced datasets;data scarcity;progressive growing generative adversarial networks;training-related issues;high-resolution image training;subjective self-attention layer;efficient feature learning;training stabilization;PGGAN synthesized data;U-net supervised deep learning model;Enhanced-GAN DEEPFAKE data;data performs;PGGAN DEEPFAKE data;convolution layer;useful modalities;AM score;mode score;malicious modalities","","2","","45","CCBY","25 Jul 2022","","","IEEE","IEEE Journals"
"Unlimited Resolution Image Generation with R2D2-GANs","M. Jegorova; A. I. Karjalainen; J. Vazquez; T. M. Hospedales","University of Edinburgh, UK; Seebyte, UK; Seebyte, UK; University of Edinburgh, UK","Global Oceans 2020: Singapore – U.S. Gulf Coast","9 Apr 2021","2020","","","1","5","In this paper we present a novel simulation technique for generating high quality images of any predefined resolution. This method can be used to synthesize sonar scans of size equivalent to those collected during a full-length mission, with across track resolutions of any chosen magnitude. In essence, our model extends Generative Adversarial Networks (GANs) based architecture into a conditional recursive setting, that facilitates the continuity of the generated images. The data produced is continuous, realistically -looking, and can also be generated at least two times faster than the real speed of acquisition for the sonars with higher resolutions, such as EdgeTech. The seabed topography can be fully controlled by the user. The visual assessment tests demonstrate that humans cannot distinguish the simulated images from real. Moreover, experimental results suggest that in the absence of real data the autonomous recognition systems can benefit greatly from training with the synthetic data, produced by the R2D2-GANs.","0197-7385","978-1-7281-5446-6","10.1109/IEEECONF38699.2020.9389260","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9389260","","Training;Visualization;Image resolution;Image synthesis;Sonar applications;Surfaces;Synthetic aperture sonar","image resolution;recursive estimation;sonar imaging","unlimited resolution image generation;R2D2-GANs;simulation technique;high quality images;predefined resolution;full-length mission;track resolutions;Generative Adversarial Networks based architecture;conditional recursive setting;sonars;simulated images;seabed topography;visual assessment tests;autonomous recognition systems;synthetic data","","3","","14","IEEE","9 Apr 2021","","","IEEE","IEEE Conferences"
"Loss functions for Style Transfer with CycleGAN","X. Wang","School of Economics and Management, Nanjing Tech University, Nanjing, China","2022 3rd International Conference on Electronic Communication and Artificial Intelligence (IWECAI)","11 Apr 2022","2022","","","63","66","Generative Adversarial Networks has been used in many fields now, and it is particularly essential in the field of computer vision. In respect of image-to-image translation, CycleGAN is an important part. In this paper, CycleGAN is used to translate portrait photographs to sketches, and ℓ1 loss, ℓ2 loss, perceptual loss and their combination losses are compared to find a cycle consistency loss function with better performance. After evaluating the generated image quality through two metrics: the peak signal to noise ratio (PSNR) and structural similarity (SSIM), the conclusion that combining the perceptual loss with ℓ1 and ℓ2 makes the generated images more structurally similar with their reference image is drawn.","","978-1-6654-7997-4","10.1109/IWECAI55315.2022.00020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9750677","CycleGAN;style transfer;perceptual loss","Image quality;Measurement;Computer vision;PSNR;Image synthesis;Semantics;Layout","computer vision;data compression;image coding;image resolution;least squares approximations;minimisation;regression analysis","loss functions;style transfer;CycleGAN;Generative Adversarial Networks;computer vision;image-to-image translation;portrait photographs;perceptual loss;combination losses;cycle consistency loss function;generated image quality;structural similarity;reference image","","","","13","IEEE","11 Apr 2022","","","IEEE","IEEE Conferences"
"MontageGAN: Generation and Assembly of Multiple Components by GANs","C. F. Shee; S. Uchida","Kyushu University, Fukuoka, Japan; Kyushu University, Fukuoka, Japan","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","1478","1484","A multi-layer image is more valuable than a single-layer image from a graphic designer’s perspective. However, most of the proposed image generation methods so far focus on single-layer images. In this paper, we propose MontageGAN, which is a Generative Adversarial Networks (GAN) framework for generating multi-layer images. Our method utilized a two-step approach consisting of local GANs and global GAN. Each local GAN learns to generate a specific image layer, and the global GAN learns the placement of each generated image layer . Through our experiments, we show the ability of our method to generate multi-layer images and estimate the placement of the generated image layers.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956028","","Training;Image quality;Graphics;Image synthesis;Image color analysis;Convolution;Face recognition","computer graphics;image processing;neural nets","generated image layer;generative adversarial networks framework;global GAN;image generation methods;local GAN;MontageGAN;multilayer image;single layer image;specific image layer","","","","36","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"A Deep Learning Approach to Terracotta Warriors’ Facial Recovery and Quadratic","Y. Xu; L. Ma","School of Computer Science and Engineering, Xi’an Technological University, Xi’an, China; School of Computer Science and Engineering, Xi’an Technological University, Xi’an, China","2022 International Conference on Computer Network, Electronic and Automation (ICCNEA)","2 Nov 2022","2022","","","117","120","Facial restoration and animation of degraded images of Terracotta Warriors are important for enhancing the vitality and promotion of cultural relics. In this paper, based on a deep learning theoretical framework, blind facial restoration is performed on degraded images of terracotta warriors using rich and diverse facial priors in pre-trained generative adversarial networks. The facial prior is involved in the image generation process through the spatial feature transformation layer to achieve a good balance of realism and fidelity in the terracotta warrior restoration process. The anime style migration algorithm is used to realize the animalization of the terracotta warrior restoration image by learning the style features of the anime reference image and fitting the style migration process. The experimental results show that the method in this paper achieves facial recovery of the terracotta warriors with high fidelity; the generated secondary terracotta heads have obvious anime style and the content information remains highly consistent with the original images.","2770-7695","978-1-6654-9109-9","10.1109/ICCNEA57056.2022.00035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933198","Terracotta Warriors;Facial Restoration;Secondary Applications;Style Migration","Deep learning;Training;Measurement;Pain;Image synthesis;Image color analysis;Animation","computer animation;face recognition;feature extraction;image matching;image restoration;learning (artificial intelligence)","blind facial restoration;degraded images;rich priors;diverse facial priors;pre-trained generative adversarial networks;image generation process;spatial feature transformation layer;terracotta warrior restoration process;anime style migration algorithm;terracotta warrior restoration image;anime reference image;style migration process;generated secondary terracotta heads;obvious anime style;original images;deep learning approach;Terracotta Warriors' facial recovery;animation;deep learning theoretical framework","","","","12","IEEE","2 Nov 2022","","","IEEE","IEEE Conferences"
"Ketchup As You Like: Drawing Editor for Foods","S. Naritomi; G. Benitez-Garcia; K. Yanai","The University of Electro-Communications, Tokyo, Chofu-shi, Tokyo, Japan; The University of Electro-Communications, Tokyo, Chofu-shi, Tokyo, Japan; The University of Electro-Communications, Tokyo, Chofu-shi, Tokyo, Japan","2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR)","22 Dec 2021","2021","","","207","209","Omelet rice is a popular Japanese dish made from an egg omelet filled with rice. There is a culture of freely drawing patterns or messages on it with tomato ketchup. The advantage of this practice is that it is easy to create originality and express creativity. We believe that utilizing this advantage would be possible to generate original food models in VR space easily. Therefore, we introduce a web-based real-time application called “Ketchup As You Like”, which allows users to freely draw with ketchup on an omelet image. This application is able to generate omelet images with realistic ketchup patterns. The image generation used in “Ketchup As You Like” is created by combining CNN-based segmentation and generative adversarial networks (GAN). Demo video is available at https://youtu.be/m6AjwEY6Jp8","","978-1-6654-3225-2","10.1109/AIVR52153.2021.00047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9644304","food image;GAN;image translation;photo editing","Solid modeling;Image segmentation;Three-dimensional displays;Image synthesis;Conferences;Virtual reality;Writing","convolutional neural nets;food products;humanities;image segmentation;Internet;virtual reality","omelet image;realistic ketchup patterns;image generation;CNN-based segmentation;generative adversarial networks;omelet rice;egg omelet;tomato ketchup;food models;VR space;web-based real-time application;Japanese dish;ketchup as you like application","","","","8","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"Self-Supervised GANs via Auxiliary Rotation Loss","T. Chen; X. Zhai; M. Ritter; M. Lucic; N. Houlsby","University of California, Los Angeles; Google Brain; Google Brain; Google Brain; Google Brain","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","12146","12155","Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954184","Image and Video Synthesis;Deep Learning;Representation Learning","Training;Representation learning;Image synthesis;Games;Image representation;Generative adversarial networks;Data models","image representation;neural nets;unsupervised learning","auxiliary rotation loss;natural image synthesis;unsupervised learning techniques;adversarial training;self-supervision;unconditional GAN;representation learning;classic GAN game;feature representations;image representations;synthesized images;self-supervised GAN;unconditional ImageNet generation","","118","1","46","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"Cross-Modal Contrastive Learning for Text-to-Image Generation","H. Zhang; J. Y. Koh; J. Baldridge; H. Lee; Y. Yang",Google Research; Google Research; Google Research; University of Michigan; Google Research,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","833","842","The output of text-to-image synthesis systems should be coherent, clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. Our Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN) addresses this challenge by maximizing the mutual information between image and text. It does this via multiple contrastive losses which capture inter-modality and intra-modality correspondences. XMC-GAN uses an attentional self-modulation generator, which enforces strong text-image correspondence, and a contrastive discriminator, which acts as a critic as well as a feature encoder for contrastive learning. The quality of XMC-GAN’s output is a major step up from previous models, as we show on three challenging datasets. On MS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33, but– more importantly–people prefer XMC-GAN by 77.3% for image quality and 74.1% for image-text alignment, compared to three other recent models. XMC-GAN also generalizes to the challenging Localized Narratives dataset (which has longer, more detailed descriptions), improving state-of-the-art FID from 48.70 to 14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images data, establishing a strong benchmark FID score of 26.91.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577676","","Image quality;Image synthesis;Computational modeling;Impedance matching;Semantics;Natural languages;Generative adversarial networks","feature extraction;image classification;learning (artificial intelligence);text analysis;video signal processing","Cross-Modal Contrastive Generative Adversarial Network;multiple contrastive losses;capture inter-modality;intra-modality correspondences;attentional self-modulation generator;strong text-image correspondence;contrastive discriminator;XMC-GAN's output;XMC-GAN improve state-of-the-art FID;image quality;image-text alignment;challenging Open Images data;Cross-Modal Contrastive learning;text-to-image generation;text-to-image synthesis systems;conditioned text descriptions","","41","","66","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"InfoMax-GAN: Improved Adversarial Image Generation via Information Maximization and Contrastive Learning","K. S. Lee; N. -T. Tran; N. -M. Cheung",Snap Inc.; Singapore University of Technology and Design; Singapore University of Technology and Design,"2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 Jun 2021","2021","","","3941","3951","While Geerative Adversarial Networks (GANs) are fundamental to many generative modelling applications, they suffer from numerous issues. In this work, we propose a principled framework to simultaneously mitigate two fundamental issues in GANs: catastrophic forgetting of the discriminator and mode collapse of the generator. We achieve this by employing for GANs a contrastive learning and mutual information maximization approach, and perform extensive analyses to understand sources of improvements. Our approach significantly stabilizes GAN training and improves GAN performance for image synthesis across five datasets under the same training and evaluation conditions against state-of-the-art works. In particular, compared to the state-of-the-art SSGAN, our approach does not suffer from poorer performance on image domains such as faces, and instead improves performance significantly. Our approach is simple to implement and practical: it involves only one auxiliary objective, has low computational cost, and performs robustly across a wide range of training settings and datasets without any hyperparameter tuning. For reproducibility, our code is available in the open-source GAN library, Mimicry [34].","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00399","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423208","","Training;Image synthesis;Generative adversarial networks;Reproducibility of results;Libraries;Generators;Tuning","image processing;learning (artificial intelligence);neural nets;optimisation","SSGAN;image domains;open-source GAN library;InfoMax-GAN;adversarial image generation;contrastive learning;GANs;generative modelling applications;principled framework;catastrophic forgetting;mutual information maximization;GAN training;GAN performance;image synthesis;evaluation conditions;geerative adversarial networks","","10","","64","IEEE","14 Jun 2021","","","IEEE","IEEE Conferences"
"Dual Contradistinctive Generative Autoencoder","G. Parmar; D. Li; K. Lee; Z. Tu","Carnegie Mellon University; Carnegie Mellon University; UC, San Diego; UC, San Diego","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","823","832","We present a new generative autoencoder model with dual contradistinctive losses to improve generative autoencoder that performs simultaneous inference (reconstruction) and synthesis (sampling). Our model, named dual contradistinctive generative autoencoder (DC-VAE), integrates an instance-level discriminative loss (maintaining the instance-level fidelity for the reconstruction/synthesis) with a set-level adversarial loss (encouraging the set-level fidelity for the reconstruction/synthesis), both being contradistinctive. Extensive experimental results by DC-VAE across different resolutions including 32×32, 64×64, 128×128, and 512×512 are reported. The two contradistinctive losses in VAE work harmoniously in DC-VAE leading to a significant qualitative and quantitative performance enhancement over the baseline VAEs without architectural changes. State-of-the-art or competitive results among generative autoencoders for image reconstruction, image synthesis, image interpolation, and representation learning are observed. DC-VAE is a general-purpose VAE model, applicable to a wide variety of downstream tasks in computer vision and machine learning.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577829","","Interpolation;Computer vision;Image resolution;Image synthesis;Computational modeling;Generative adversarial networks;Encoding","computer vision;image reconstruction;image representation;interpolation;learning (artificial intelligence);neural nets","DC-VAE leading;quantitative performance enhancement;image reconstruction;image synthesis;general-purpose VAE model;generative autoencoder model;dual contradistinctive losses;instance-level discriminative loss;instance-level fidelity;set-level adversarial loss;set-level fidelity;dual contradistinctive generative autoencoder;image interpolation;computer vision;machine learning","","9","","69","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"StyleSwin: Transformer-based GAN for High-resolution Image Generation","B. Zhang; S. Gu; B. Zhang; J. Bao; D. Chen; F. Wen; Y. Wang; B. Guo",University of Science and Technology of China; University of Science and Technology of China; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; Microsoft Research Asia; University of Science and Technology of China; Microsoft Research Asia,"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","11294","11304","Despite the tantalizing success in a broad of vision tasks, transformers have not yet demonstrated on-par ability as ConvNets in high-resolution image generative modeling. In this paper, we seek to explore using pure transformers to build a generative adversarial network for high-resolution image synthesis. To this end, we believe that local attention is crucial to strike the balance between computational efficiency and modeling capacity. Hence, the proposed generator adopts Swin transformer in a style-based architecture. To achieve a larger receptive field, we propose double attention which simultaneously leverages the context of the local and the shifted windows, leading to improved generation quality. Moreover, we show that offering the knowledge of the absolute position that has been lost in window-based transformers greatly benefits the generation quality. The proposed StyleSwin is scalable to high resolutions, with both the coarse geometry and fine structures benefit from the strong expressivity of transformers. However, blocking artifacts occur during high-resolution synthesis because performing the local attention in a block-wise manner may break the spatial coherency. To solve this, we empirically investigate various solutions, among which we find that employing a wavelet discriminator to examine the spectral discrepancy effectively suppresses the artifacts. Extensive experiments show the superiority over prior transformer-based GANs, especially on high resolutions, e.g., $1024 \times$ 1024. The StyleSwin, without complex training strategies, excels over StyleGAN on CelebA-HQ 1024, and achieves on-par performance on FFHQ-1024, proving the promise of using transformers for high-resolution image generation. The code and pretrained models are available at https://github.com/microsoft/StyleSwin.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9880033","Image and video synthesis and generation; Deep learning architectures and techniques","Wavelet transforms;Training;Image synthesis;Computational modeling;Spatial coherence;Transformers;Generative adversarial networks","computer vision;data compression;feature extraction;image classification;image coding;image representation;image resolution;learning (artificial intelligence);neural nets;wavelet transforms","StyleSwin;transformer-based GAN;high-resolution image generation;high-resolution image generative modeling;pure transformers;generative adversarial network;high-resolution image synthesis;local attention;Swin transformer;style-based architecture;improved generation quality;window-based transformers;high-resolution synthesis","","7","","68","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Contrastive Feature Loss for Image Prediction","A. Andonian; T. Park; B. Russell; P. Isola; J. -Y. Zhu; R. Zhang",Adobe Research; Adobe Research; Adobe Research; MIT; CMU; Adobe Research,"2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","1934","1943","Training supervised image synthesis models requires a critic to compare two images: the ground truth to the result. Yet, this basic functionality remains an open problem. A popular line of approaches uses the L1 (mean absolute error) loss, either in the pixel or the feature space of pretrained deep networks. However, we observe that these losses tend to produce overly blurry and grey images, and other techniques such as GANs need to be employed to fight these artifacts. In this work, we introduce an information theory based approach to measuring similarity between two images. We argue that a good reconstruction should have high mutual information with the ground truth. This view enables learning a lightweight critic to ""calibrate"" a feature space in a contrastive manner, such that reconstructions of corresponding spatial patches are brought together, while other patches are repulsed. We show that our formulation immediately boosts the perceptual realism of output images when used as a drop-in replacement for the L1 loss, with or without an additional GAN loss.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607505","","Training;Computer vision;Image synthesis;Conferences;Generative adversarial networks;Extraterrestrial measurements;Standards","image motion analysis;information theory;learning (artificial intelligence);neural nets","contrastive feature loss;image prediction;supervised image synthesis models;ground truth;basic functionality;open problem;L1 loss;mean absolute error;feature space;pretrained deep networks;overly blurry images;grey images;information theory;high mutual information;lightweight critic;contrastive manner;GAN loss;spatial patches","","5","","50","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Using Photorealistic Face Synthesis and Domain Adaptation to Improve Facial Expression Analysis","B. Bozorgtabar; M. S. Rad; H. Kemal Ekenel; J. -P. Thiran","École Polytechnique Fédérale de Lausanne, Switzerland; École Polytechnique Fédérale de Lausanne, Switzerland; Istanbul Technical University, Istanbul, Turkey; École Polytechnique Fédérale de Lausanne, Switzerland","2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)","11 Jul 2019","2019","","","1","8","Cross-domain synthesizing realistic faces to learn deep models has attracted increasing attention for facial expression analysis as it helps to improve the performance of expression recognition accuracy despite having small number of real training images. However, learning from synthetic face images can be problematic due to the distribution discrepancy between low-quality synthetic images and real face images and may not achieve the desired performance when the learned model applies to real world scenarios. To this end, we propose a new attribute guided face image synthesis to perform a translation between multiple image domains using a single model. In addition, we adopt the proposed model to learn from synthetic faces by matching the feature distributions between different domains while preserving each domain's characteristics. We evaluate the effectiveness of the proposed approach on several face datasets on generating realistic face images. We demonstrate that the expression recognition performance can be enhanced by benefiting from our face synthesis model. Moreover, we also conduct experiments on a near-infrared dataset containing facial expression videos of drivers to assess the performance using in-the-wild data for driver emotion recognition.","","978-1-7281-0089-0","10.1109/FG.2019.8756632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756632","","Face;Gallium nitride;Training;Image synthesis;Face recognition;Generative adversarial networks;Generators","emotion recognition;face recognition;feature extraction;image matching;learning (artificial intelligence)","photorealistic face synthesis;domain adaptation;facial expression analysis;expression recognition accuracy;low-quality synthetic images;feature distributions;realistic face images;driver emotion recognition;multiple image domains;attribute guided face image synthesis;real face images;cross-domain synthesizing realistic faces","","3","1","41","IEEE","11 Jul 2019","","","IEEE","IEEE Conferences"
"Learning Depth Information in Layout for Sketch Generation from Scene Graph","Y. Sun; H. Lin; C. Liu; Y. Fu",Fudan University; Fudan University; HKUST; Fudan University,"2021 7th International Conference on Big Data and Information Analytics (BigDIA)","6 Dec 2021","2021","","","253","260","The research of generating images from scene graphs has become a hot topic, benefiting from the success of Generative Adversarial Network (GAN). Previous works in this field are still challenged by the complexity of texture pattern and object structure in images. In fact, it is more desirable to generate sketches directly, rather than image synthesis from scene graphs. The sketch is an abstract and iconic image representation, which describes the object structure and scene layout well but ignoring complex texture patterns, leading to a more reasonable generation task. Furthermore, there are two real-world tasks can illustrate the importance of this problem: Courtroom sketch and Crime Scene sketch, which are less studied before. To this end, we, for the first time, study the task of generating sketches from scene graphs. Essentially, the main novelties are two folds. First, a new sketch generation framework is developed which is trained with both newly designed perceptual loss and adversarial loss. Second, we propose a new layout encoding block to learn the depth information for each object. Extensive experiments on several widely used datasets validate the good performance of the proposed approach.","","978-1-6654-2466-0","10.1109/BigDIA53151.2021.9619707","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619707","Sketch Generation;Scene Graph;Generative Model","Analytical models;Visualization;Image synthesis;Layout;Genomics;Image representation;Generative adversarial networks","feature extraction;graph theory;image representation;image retrieval;image texture;learning (artificial intelligence)","depth information;Scene graph;scene graphs;Generative Adversarial Network;image synthesis;abstract image representation;iconic image representation;object structure;scene layout;complex texture patterns;reasonable generation task;Courtroom sketch;Crime Scene sketch;sketch generation framework","","","","33","IEEE","6 Dec 2021","","","IEEE","IEEE Conferences"
"PSG-GAN: Progressive Person Image Generation with Self-Guided Local Focuses","Z. Xiaomao; W. Wei; D. Bing","Purple Mountain Laboratories, Center of Future Network Research, Nanjing, China; Purple Mountain Laboratories, Center of Future Network Research, Nanjing, China; Purple Mountain Laboratories, Center of Future Network Research, Nanjing, China","2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI)","21 Dec 2021","2021","","","763","769","This paper proposes PSG-GAN, a novel Generative Adversarial Network for pose-guided person image synthesis, which can progressively generate realistic person images of desired poses together with corresponding semantic segmentation masks. Specifically, PSG-GAN consists of a sequence of Region-Focal Transfer Blocks (RFBs) where each contains two generation pathways: the appearance generation pathway and the semantic generation pathway. The former pathway is responsible for generating the target image by explicitly preserving appearance-related features within certain regions, where local region transformations are considered. The latter pathway is used to generate semantic masks which define the areas for the local transformations to attend to. These two learning pathways work together and reinforce each other to simultaneously generate the target image and semantic masks progressively. Qualitative and quantitative experimental results on two benchmark datasets demonstrate PSG-GAN’s superiority over other approaches in generating realistic person images in pose transfer tasks.","2375-0197","978-1-6654-0898-1","10.1109/ICTAI52525.2021.00121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643402","GAN;pose-guided image generation;segmentation mask","Adaptation models;Image segmentation;Image synthesis;Conferences;Semantics;Benchmark testing;Generative adversarial networks","feature extraction;image classification;image segmentation;image sequences;learning (artificial intelligence);neural nets;pose estimation","semantic generation pathway;appearance-related features;local region transformations;semantic masks;local transformations;learning pathways;PSG-GAN's superiority;realistic person images;progressive person image generation;self-guided local focuses;generative adversarial network;pose-guided person image synthesis;semantic segmentation masks;appearance generation pathway;region-focal transfer blocks","","","","21","IEEE","21 Dec 2021","","","IEEE","IEEE Conferences"
"Multi-Attributed and Structured Text-to-Face Synthesis","R. Wadhawan; T. Drall; S. Singh; S. Chakraverty","Department of Computer Science and Engineering, Netaji Subhas Institute of Technology, New Delhi, India; Department of Computer Science and Engineering, Netaji Subhas Institute of Technology, New Delhi, India; Department of Computer Science and Engineering, Netaji Subhas Institute of Technology, New Delhi, India; Department of Computer Science and Engineering, Netaji Subhas University of Technology, New Delhi, India","2020 IEEE International Conference on Technology, Engineering, Management for Societal impact using Marketing, Entrepreneurship and Talent (TEMSMET)","6 Oct 2021","2020","","","1","7","Generative Adversarial Networks (GANs) have revolutionized image synthesis through many applications like face generation, photograph editing, and image super-resolution. Image synthesis using GANs has predominantly been uni-modal, with few approaches that can synthesize images from text or other data modes. Text-to-image synthesis, especially text-to-face synthesis, has promising use cases of robust face-generation from eye witness accounts and augmentation of the reading experience with visual cues. However, only a couple of datasets provide consolidated face data and textual descriptions for text-to-face synthesis. Moreover, these textual annotations are less extensive and descriptive, which reduces the diversity of faces generated from it. This paper empirically proves that increasing the number of facial attributes in each textual description helps GANs generate more diverse and real-looking faces. To prove this, we propose a new methodology that focuses on using structured textual descriptions. We also consolidate a Multi-Attributed and Structured Text-to-face (MAST) dataset consisting of high-quality images with structured textual annotations and make it available to researchers to experiment and build upon. Lastly, we report benchmark Fréchet’s Inception Distance (FID), Facial Semantic Similarity (FSS), and Facial Semantic Distance (FSD) scores for the MAST dataset.","","978-1-6654-0482-2","10.1109/TEMSMET51618.2020.9557583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557583","Machine Learning;Text-to-face synthesis;Generative Adversarial Network;Face Generation;MAST dataset;Crowdsourcing;Frechet’s Inception Distance","Frequency selective surfaces;Visualization;Image synthesis;Annotations;Semantics;Superresolution;Machine learning","face recognition;feature extraction;image resolution;text analysis","face generation;image super-resolution;GANs;Text-to-image synthesis;text-to-face synthesis;robust face-generation;consolidated face data;textual description;structured textual descriptions;Text-to-face dataset;high-quality images;structured textual annotations;Generative Adversarial Networks","","","","20","IEEE","6 Oct 2021","","","IEEE","IEEE Conferences"
"GANs for Biological Image Synthesis","A. Osokin; A. Chessel; R. E. C. Salas; F. Vaggi","INRIA/ENS, PSL Research University, Paris, France; LOB, Université Paris-Saclay, France; University of Bristol, UK; ENS, PSL Research University, Paris, France","2017 IEEE International Conference on Computer Vision (ICCV)","25 Dec 2017","2017","","","2252","2261","In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells imaged by fluorescence microscopy. Compared to natural images, cells tend to have a simpler and more geometric global structure that facilitates image generation. However, the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and synthesized images have to capture these relationships to be relevant for biological applications. We adapt GANs to the task at hand and propose new models with casual dependencies between image channels that can generate multichannel images, which would be impossible to obtain experimentally. We evaluate our approach using two independent techniques and compare it against sensible baselines. Finally, we demonstrate that by interpolating across the latent space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing us to predict temporal evolution from static images.","2380-7504","978-1-5386-1032-9","10.1109/ICCV.2017.245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237507","","Proteins;Gallium nitride;Training;Biological system modeling;Generators;Image generation","biomedical optical imaging;cellular biophysics;medical image processing;optical microscopy;proteins","GAN;image channels;multichannel images;cell cycle;static images;biological image synthesis;Generative Adversarial Networks;fluorescence microscopy;natural images;image generation;spatial pattern;synthesized images;biological applications;fluorescent proteins;biological functions;geometric global structure","","43","","55","IEEE","25 Dec 2017","","","IEEE","IEEE Conferences"
"Non-Adversarial Image Synthesis With Generative Latent Nearest Neighbors","Y. Hoshen; K. Li; J. Malik",Facebook AI Research; UC Berkeley; UC Berkeley,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","5804","5812","Unconditional image generation has recently been dominated by generative adversarial networks (GANs). GAN methods train a generator which regresses images from random noise vectors, as well as a discriminator that attempts to differentiate between the generated images and a training set of real images. GANs have shown amazing results at generating realistic looking images. Despite their success, GANs suffer from critical drawbacks including: unstable training and mode-dropping. The weaknesses in GANs have motivated research into alternatives including: variational auto-encoders (VAEs), latent embedding learning methods (e.g. GLO) and nearest-neighbor based implicit maximum likelihood estimation (IMLE). Unfortunately at the moment, GANs still significantly outperform the alternative methods for image generation. In this work, we present a novel method - Generative Latent Nearest Neighbors (GLANN) - for training generative models without adversarial training. GLANN combines the strengths of IMLE and GLO in a way that overcomes the main drawbacks of each method. Consequently, GLANN generates images that are far better than GLO and IMLE. Our method does not suffer from mode collapse which plagues GAN training and is much more stable. Qualitative results show that GLANN outperforms a baseline consisting of 800 GANs and VAEs on commonly used datasets. Our models are also shown to be effective for training truly non-adversarial unsupervised image translation.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953233","Image and Video Synthesis","","image texture;learning (artificial intelligence);maximum likelihood estimation","IMLE;generative latent nearest neighbors;GLANN;training generative models;adversarial training;GLO;GAN training;nonadversarial unsupervised image translation;nonadversarial image synthesis;unconditional image generation;generative adversarial networks;GAN methods;training set;realistic looking images;unstable training;latent embedding learning methods;nearest-neighbor method","","19","1","39","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"CookGAN: Meal Image Synthesis from Ingredients","F. Han; R. Guerrero; V. Pavlovic","Rutgers University, Piscataway, NJ, USA; Samsung AI Center, Cambridge, UK; Rutgers University, Piscataway, NJ, USA","2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","1439","1447","In this work we propose a new computational framework, based on generative deep models, for synthesis of photo-realistic food meal images from textual list of its ingredients. Previous works on synthesis of images from text typically rely on pre-trained text models to extract text features, followed by generative neural networks (GAN) aimed to generate realistic images conditioned on the text features. These works mainly focus on generating spatially compact and well-defined categories of objects, such as birds or flowers, but meal images are significantly more complex, consisting of multiple ingredients whose appearance and spatial qualities are further modified by cooking methods. To generate real-like meal images from ingredients, we propose Cook Generative Adversarial Networks (CookGAN), CookGAN first builds an attention-based ingredients-image association model, which is then used to condition a generative neural network tasked with synthesizing meal images. Furthermore, a cycle-consistent constraint is added to further improve image quality and control appearance. Experiments show our model is able to generate meal images corresponding to the ingredients.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093463","","Feature extraction;Gallium nitride;Training;Neural networks;Generators;Image resolution;Computational modeling","feature extraction;image segmentation;learning (artificial intelligence);neural nets;realistic images;text analysis","text features;multiple ingredients;CookGAN;attention-based ingredients-image association model;generative neural network;synthesizing meal images;improve image quality;control appearance;meal image synthesis;generative deep models;photo-realistic food meal images;pre-trained text models;realistic images;cook generative adversarial networks","","10","","29","IEEE","14 May 2020","","","IEEE","IEEE Conferences"
"Seamless Virtual Whole Slide Image Synthesis and Validation Using Perceptual Embedding Consistency","A. Lahiani; I. Klaman; N. Navab; S. Albarqouni; E. Klaiman","Computer Aided Medical Procedures, Technical University of Munich, Garching bei München, Germany; Early Biomarker Development Oncology, Pharma Research and Early Development, Roche Innovation Center Munich, Penzberg, Germany; Computer Aided Medical Procedures, Technical University of Munich, Garching bei München, Germany; Computer Vision Laboratory, ETH Zurich, Zürich, Switzerland; Early Biomarker Development Oncology, Pharma Research and Early Development, Roche Innovation Center Munich, Penzberg, Germany","IEEE Journal of Biomedical and Health Informatics","5 Feb 2021","2021","25","2","403","411","Stain virtualization is an application with growing interest in digital pathology allowing simulation of stained tissue images thus saving lab and tissue resources. Thanks to the success of Generative Adversarial Networks (GANs) and the progress of unsupervised learning, unsupervised style transfer GANs have been successfully used to generate realistic, clinically meaningful and interpretable images. The large size of high resolution Whole Slide Images (WSIs) presents an additional computational challenge. This makes tilewise processing necessary during training and inference of deep learning networks. Instance normalization has a substantial positive effect in style transfer GAN applications but with tilewise inference, it has the tendency to cause a tiling artifact in reconstructed WSIs. In this paper we propose a novel perceptual embedding consistency (PEC) loss forcing the network to learn color, contrast and brightness invariant features in the latent space and hence substantially reducing the aforementioned tiling artifact. Our approach results in more seamless reconstruction of the virtual WSIs. We validate our method quantitatively by comparing the virtually generated images to their corresponding consecutive real stained images. We compare our results to state-of-the-art unsupervised style transfer methods and to the measures obtained from consecutive real stained tissue slide images. We demonstrate our hypothesis about the effect of the PEC loss by comparing model robustness to color, contrast and brightness perturbations and visualizing bottleneck embeddings. We validate the robustness of the bottleneck feature maps by measuring their sensitivity to the different perturbations and using them in a tumor segmentation task. Additionally, we propose a preliminary validation of the virtual staining application by comparing interpretation of 2 pathologists on real and virtual tiles and inter-pathologist agreement.","2168-2208","","10.1109/JBHI.2020.2975151","Deutsche Forschungsgemeinschaft(grant numbers:DFG, SFB 824); Bayerische Forschungsstiftung(grant numbers:BFS, IPN2); German Academic Exchange Service (DAAD); German Federal Ministry of Education and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003176","Digital pathology;GANs;inter-pathologist agreement;style transfer;virtual staining;WSI","Generators;Training;Image reconstruction;Tumors;Image segmentation;Biomedical imaging;Virtualization","biomedical optical imaging;feature extraction;image classification;image colour analysis;image segmentation;image texture;learning (artificial intelligence);medical image processing;tumours;unsupervised learning","stain virtualization;digital pathology allowing simulation;stained tissue images;tissue resources;generative adversarial networks;unsupervised learning;unsupervised style transfer GANs;interpretable images;high resolution Whole Slide Images;additional computational challenge;deep learning networks;instance normalization;substantial positive effect;style transfer GAN applications;tilewise inference;reconstructed WSIs;brightness invariant features;aforementioned tiling artifact;seamless reconstruction;virtual WSIs;virtually generated images;state-of-the-art unsupervised style transfer methods;consecutive real stained tissue slide images;PEC loss;brightness perturbations;visualizing bottleneck embeddings;preliminary validation;seamless virtual whole slide image synthesis;perceptual embedding consistency loss;consecutive real stained images;virtual tiles;virtual staining application","Humans;Image Processing, Computer-Assisted","7","","41","IEEE","19 Feb 2020","","","IEEE","IEEE Journals"
"Retinal image synthesis through the least action principle","D. L. Castro; C. Valenti; D. Tegolo","Dipartimento di Matematica e Informatica, Università degli Studi di Palermo, Italy; Dipartimento di Matematica e Informatica, Università degli Studi di Palermo, Italy; Institute of Biophysics, National Research Council, Palermo, Italy","2020 5th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)","2 Feb 2021","2020","","","111","116","Eye fundus image analysis is a fundamental approach in medical diagnosis and follow-up ophthalmic diagnostics. Manual annotation by experts needs hard work, thus only a small set of annotated vessel structures is available. Examples such as DRIVE and STARE include small sets for training images of fundus image benchmarks. Moreover, there is no vessel structure annotation for a number of fundus image datasets. Synthetic images have been generated by using appropriate parameters for the modeling of vascular networks or by methods developing deep learning techniques and supported by performance hardware. Our methodology aims to produce high-resolution synthetic fundus images alternative to the increasing use of generative adversarial networks, to overcome the problems that arise in producing slightly modified versions of the same real images, to simulate pathologies and for the prediction of eye-related diseases. Our approach is based on the principle of the least action to place vessels on the simulated eye fundus.","2189-8723","978-1-7281-8022-9","10.1109/ICIIBMS50712.2020.9336421","University of Dundee; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9336421","fundus image analysis;synthetic retinal image;data augmentation;statistical features;predictive evaluation diseases","Training;Pathology;Annotations;Manuals;Retina;Medical diagnosis;Diseases","biomedical optical imaging;blood vessels;diseases;eye;learning (artificial intelligence);medical image processing;neural nets","simulated eye fundus;retinal image synthesis;action principle;eye fundus image analysis;medical diagnosis;ophthalmic diagnostics;DRIVE STARE;fundus image benchmarks;vessel structure annotation;fundus image datasets;vascular networks;deep learning techniques;generative adversarial networks;eye-related diseases;place vessels;high-resolution synthetic fundus images","","","","35","IEEE","2 Feb 2021","","","IEEE","IEEE Conferences"
"Cast-Gan: Learning To Remove Colour Cast From Underwater Images","C. Y. Li; A. Cavallaro","Centre for Intelligent Sensing, Queen Mary University of London; Centre for Intelligent Sensing, Queen Mary University of London","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","1083","1087","Underwater images are degraded by blur and colour cast caused by the attenuation of light in water. To remove the colour cast with neural networks, images of the scene taken under white illumination are needed as reference for training, but are generally unavailable. As an alternative, one can use surrogate reference images taken close to the water surface or degraded images synthesised from reference datasets. However, the former still suffer from colour cast and the latter generally have limited colour diversity. To address these problems, we exploit open data and typical colour distributions of objects to create a synthetic image dataset that reflects degradations naturally occurring in underwater photography. We use this dataset to train Cast-GAN, a Generative Adversarial Network whose loss function includes terms that eliminate artefacts that are typical of underwater images enhanced with neural networks. We compare the enhancement results of Cast-GAN with four state-of-the-art methods and validate the cast removal with a subjective evaluation.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9191157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191157","Image enhancement;Underwater images;Generative Adversarial Networks;Image synthesis","Image color analysis;Attenuation;Training;Image enhancement;Scattering;Neural networks;Cameras","image colour analysis;image enhancement;image restoration;lighting;neural nets;photography;unsupervised learning","generative adversarial network;underwater photography;cast removal;Cast-GAN;synthetic image dataset;colour diversity;surrogate reference images;neural networks;underwater image enhancement;colour cast removal","","6","","36","IEEE","30 Sep 2020","","","IEEE","IEEE Conferences"
"Stable and improved generative adversarial nets (GANS): A constructive survey","G. Zhang; E. Tu; D. Cui","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; Rolls-Royce@NTUCorporate Lab, Nanyang Technological University, Singapore; Energy Research Institute@NTU, Interdisciplinary Graduate School, Singapore","2017 IEEE International Conference on Image Processing (ICIP)","22 Feb 2018","2017","","","1871","1875","In this paper, we present a general and applicable adversarial training framework based on a comprehensive survey, not limited to straightforward GANs related works, also including shallow neural networks and reinforcement learning. Concentrating on challenging face synthesis task, we summarize a stable training pipeline: 1) booting training procedure with noise injection; 2) fixing weights of fully connected layer in generator to improve performance further; 3) involving Markov decision module to dynamically choose learning rates of discriminator and generator respectively. Finally in experiments, we highlight a mutual evaluation criterion over entropy score based on a pre-trained classifier and manual voting.","2381-8549","978-1-5090-2175-8","10.1109/ICIP.2017.8296606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296606","image synthesis;generative adversarial networks;GAN;stable GAN","Gallium nitride;Training;Generators;Pipelines;Task analysis;Convolution;Face","entropy;learning (artificial intelligence);Markov processes;neural nets","constructive survey;general training framework;shallow neural networks;reinforcement learning;face synthesis task;stable training pipeline;noise injection;fully connected layer;stable adversarial nets;generative adversarial nets;adversarial training framework;discriminator learning;generator learning;pretrained classifier;manual voting","","5","","22","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"A Unified Conditional Disentanglement Framework For Multimodal Brain Mr Image Translation","X. Liu; F. Xing; G. E. Fakhri; J. Woo","Dept. of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Dept. of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Dept. of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Dept. of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA","2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)","25 May 2021","2021","","","10","14","Multimodal MRI provides complementary and clinically relevant information to probe tissue condition and to characterize various diseases. However, it is often difficult to acquire sufficiently many modalities from the same subject due to limitations in study plans, while quantitative analysis is still demanded. In this work, we propose a unified conditional disentanglement framework to synthesize any arbitrary modality from an input modality. Our framework hinges on a cycleconstrained conditional adversarial training approach, where it can extract a modality-invariant anatomical feature with a modality-agnostic encoder and generate a target modality with a conditioned decoder. We validate our framework on four MRI modalities, including T1-weighted, T1 contrast enhanced, T2-weighted, and FLAIR MRI, from the BraTS’18 database, showing superior performance on synthesis quality over the comparison methods. In addition, we report results from experiments on a tumor segmentation task carried out with synthesized data.","1945-8452","978-1-6654-1246-9","10.1109/ISBI48211.2021.9433897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433897","Image synthesis;Generative Adversarial Networks;Deep learning;Brain tumor","Training;Image segmentation;Statistical analysis;Magnetic resonance imaging;Training data;Fasteners;Feature extraction","biological tissues;biomedical MRI;brain;diseases;image segmentation;medical image processing;tumours","unified conditional disentanglement framework;multimodal brain MR image translation;multimodal MRI;tissue condition;modality-invariant anatomical feature;modality-agnostic encoder;conditioned decoder;MRI modalities;conditional adversarial training approach;FLAIR MRI","","3","","23","IEEE","25 May 2021","","","IEEE","IEEE Conferences"
"Deep Non-Linear Embedding Deformation Network for Cross-Modal Brain MRI Synthesis","Y. Lin; H. Han; S. Kevin Zhou","University of Chinese Academy of Sciences, Beijing, China; Peng Cheng Laboratory, Shenzhen, China; Medical Imaging, Robotics, and Analytic Computing Laboratory and Engineering (MIRACLE), School of Biomedical Engineering & Suzhou Institute for Advance Research, University of Science and Technology of China, Suzhou, China","2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)","26 Apr 2022","2022","","","1","5","Multimodal MRI (e.g. T1, T2, and Flair) can provide rich anatomical and functional information, thereby facilitating clinical diagnosis and treatment. However, multimodal MRI takes a long scan time, easily leading to artifacts or corruption in certain modalities. Therefore, it is of great value to synthesize a new MRI modality from a complete MRI modality to obtain complementary information for clinical diagnosis. Existing GAN-based approaches treat cross-modal MRI synthesis as an end-to-end learning process without explicit consideration of the inherent correlations between different modalities, leading to inaccurate anatomical and lesion structure in the synthesized modality. In this paper, we propose a deep non-linear embedding deformation network (NEDNet) for cross-modal brain MRI synthesis. NEDNet represents each modality as a non-linear embedding based w.r.t. its own atlas, and learns a deformation feature that is assumed to be the same across modalities. The modality-specific atlas and multi-modal shared deformation are jointly used for generating the new MRI modality. Experiments show that our approach can obtain better cross-modality synthesis results than several baseline methods.","1945-8452","978-1-6654-2923-8","10.1109/ISBI52829.2022.9761711","Youth Innovation Promotion Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9761711","Image Synthesis;Generative Adversarial Networks;Deep Learning;Magnetic Resonance Imaging","Correlation;Magnetic resonance imaging;Clinical diagnosis;Lesions;Strain;Biomedical imaging","biomedical MRI;brain;medical image processing","deep nonlinear embedding deformation network;cross-modal brain MRI synthesis;multimodal MRI;clinical diagnosis;complete MRI modality;cross-modal MRI synthesis;end-to-end learning process;synthesized modality;modality-specific atlas;cross-modality synthesis results","","","","15","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"OCSVM-based Evaluation Method for Generative Neural Networks","A. I. Károly; M. Takács; P. Galambos","Doctoral School of Applied Informatics and Applied Mathematics, Óbuda University, Budapest, Hungary; John von Neumann Faculty of Informatics, Óbuda University, Budapest, Hungary; Doctoral School of Applied Informatics and Applied Mathematics, Óbuda University, Budapest, Hungary","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","6","Humanity has desired to create machines that can comprehend fine arts for a long while. Recently, several articles report neural network models that managed to create something which can be considered as art, such as paintings or music. Naturally, these proposals are also leveraged in the field of robotics and autonomous vehicles for tasks such as training environment generation and efficient exploration in reinforcement learning. The common feature of all these approaches is the utilization of generative neural networks. These generative models are a trendy and actively researched area of machine learning methods. However, it is hard to find a general and objective metric for the evaluation of such models. In this paper, a One-Class Support Vector Machine-based solution is proposed. The basic concept is demonstrated via experiments on a set of GANs. The presented method can be extended and refined in further, more exhaustive studies to serve as an alternative for the current state-of-the-art procedures.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852095","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852095","Generative Adversarial Networks;One-Class Support Vector Machine;Image Synthesis","Training;Measurement;Gallium nitride;Support vector machines;Generators;Computational modeling;Neural networks","learning (artificial intelligence);neural nets;support vector machines","fine arts;articles report neural network models;paintings;music;reinforcement learning;generative neural networks;generative models;machine learning methods;OCSVM-based evaluation method;one-class support vector machine-based solution;environment generation","","","","24","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"3D Auto-Context-Based Locality Adaptive Multi-Modality GANs for PET Synthesis","Y. Wang; L. Zhou; B. Yu; L. Wang; C. Zu; D. S. Lalush; W. Lin; X. Wu; J. Zhou; D. Shen","School of Computer Science, Sichuan University, Chengdu, China; School of Electrical and Information Engineering, The University of Sydney, Sydney, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; North Carolina State University, Raleigh, NC, USA; Department of Radiology and BRIC, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; School of Computer Science, Chengdu University of Information Technology, Chengdu, China; IDEA Lab, University of North Carolina at Chapel Hill, Chapel Hill, NC, USA","IEEE Transactions on Medical Imaging","31 May 2019","2019","38","6","1328","1339","Positron emission tomography (PET) has been substantially used recently. To minimize the potential health risk caused by the tracer radiation inherent to PET scans, it is of great interest to synthesize the high-quality PET image from the low-dose one to reduce the radiation exposure. In this paper, we propose a 3D auto-context-based locality adaptive multi-modality generative adversarial networks model (LA-GANs) to synthesize the high-quality FDG PET image from the low-dose one with the accompanying MRI images that provide anatomical information. Our work has four contributions. First, different from the traditional methods that treat each image modality as an input channel and apply the same kernel to convolve the whole image, we argue that the contributions of different modalities could vary at different image locations, and therefore a unified kernel for a whole image is not optimal. To address this issue, we propose a locality adaptive strategy for multimodality fusion. Second, we utilize 1 × 1 × 1 kernel to learn this locality adaptive fusion so that the number of additional parameters incurred by our method is kept minimum. Third, the proposed locality adaptive fusion mechanism is learned jointly with the PET image synthesis in a 3D conditional GANs model, which generates high-quality PET images by employing large-sized image patches and hierarchical features. Fourth, we apply the auto-context strategy to our scheme and propose an auto-context LA-GANs model to further refine the quality of synthesized images. Experimental results show that our method outperforms the traditional multi-modality fusion methods used in deep networks, as well as the state-of-the-art PET estimation approaches.","1558-254X","","10.1109/TMI.2018.2884053","National Natural Science Foundation of China(grant numbers:61701324); Australian Research Council(grant numbers:DE160100241); NIH(grant numbers:EB006733); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8552676","Image synthesis;positron emission topography (PET);generative adversarial networks (GANs);locality adaptive fusion;multi-modality","Positron emission tomography;Generators;Three-dimensional displays;Kernel;Gallium nitride;Image generation;Magnetic resonance imaging","image fusion;image segmentation;learning (artificial intelligence);medical image processing;positron emission tomography","LA-GANs model;synthesized images;traditional multimodality fusion methods;state-of-the-art PET estimation approaches;auto-context-based;PET synthesis;positron emission tomography;PET scans;high-quality PET image;high-quality FDG PET image;accompanying MRI images;image modality;locality adaptive strategy;locality adaptive fusion mechanism;PET image synthesis;3D conditional GANs model;large-sized image patches;auto-context strategy;image locations","Brain;Databases, Factual;Deep Learning;Humans;Imaging, Three-Dimensional;Magnetic Resonance Imaging;Phantoms, Imaging;Positron-Emission Tomography;Radiation Dosage","85","","48","IEEE","29 Nov 2018","","","IEEE","IEEE Journals"
"Synthesizing Scene Text Images for Recognition with Style Transfer","H. Liu; A. Zhu","School of Computer, Wuhan University of Technology, Wuhan, China; School of Computer, Wuhan University of Technology, Wuhan, China","2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)","7 Nov 2019","2019","5","","8","13","Majority of the existing datasets for tasks like scene text recognition only include a few thousand images with a very limited words and characters. Therefore, it cannot meet the need of the typical deep learning based text recognition approaches. In the same time, although the standard synthetic datasets usually comprise millions of scene text images, which means that the data distribution of the small target datasets cannot be learned very well. We propose a word image generating method called Synth-Text Transfer Network to solve these problems. It has the capability of estimating and simulating the distribution of target datasets. Synth-Text Transfer Network utilizes a style transfer approach to synthesis images with arbitrary text content with preserving the texture of the referenced style image in the target dataset. The large amount of synthesized images can help to alleviate the overfitting problem and improve the accuracy in latter scene text image recognition tasks. In addition, our proposed method is flexible and fast, which has a relatively fast speed among regular style transfer approaches.","","978-1-7281-5054-3","10.1109/ICDARW.2019.40073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8893118","data synthesis;style transfer;deep learning based;scene text recognition","Text recognition;Task analysis;Standards;Generative adversarial networks;Decoding;Deep learning;Training","image recognition;learning (artificial intelligence);object recognition","target dataset;scene text image recognition tasks;regular style transfer approaches;scene text image synthesis;scene text recognition;standard synthetic datasets;word image generating method;style transfer approach;arbitrary text content;referenced style imaging;synth-text transfer network;deep learning based text recognition approaches","","1","","27","IEEE","7 Nov 2019","","","IEEE","IEEE Conferences"
"Synthesis of Facial Image using Conditional Generative Adversarial Network","S. Roy; M. A. H. Akhand; N. Siddique","Dept. of Computer Science and Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; Dept. of Computer Science and Engineering, Khulna University of Engineering & Technology, Khulna, Bangladesh; School of Computing, Engineering, and Intelligent Systems, Ulster University, United Kingdom","2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2)","16 Mar 2020","2019","","","1","6","Face sketch is done by sketch artist for a suspected or missing person from the description of an eyewitness. These methods have been widely used by forensic investigators. It is difficult for the sketch artist to draw perfectly from such verbal descriptions given by eyewitness of scenes and hard for the informer to confirm whether the sketch looks like the real person. In this work, we proposed a conditional generative adversarial network (cGAN) for synthesizing real human face taking a sketch as an input image. The focus of our model is to generate realistic images that preserve the identity the target person verified by face recognition algorithms. The proposed cGAN has been verified on a variety of facial sketches, which confirms the effectiveness and improved facial recognition score.","","978-1-7281-3060-6","10.1109/IC4ME247184.2019.9036488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9036488","Convolution Neural Networks;Generative model;Image Translation;Generative Adversarial Network;Face Synthesis.","Generative adversarial networks;Generators;Gallium nitride;Task analysis;Face recognition;Training;Decoding","face recognition;image forensics;neural nets","conditional generative adversarial network;cGAN;human face synthesis;input image;realistic images;face recognition algorithms;facial sketches;facial recognition score;sketch artist;eyewitness description;forensic investigators;verbal descriptions;facial image synthesis","","1","","28","IEEE","16 Mar 2020","","","IEEE","IEEE Conferences"
"GAN based Data Augmentation for Enhanced Tumor Classification","D. S; M. S; K. S; S. S; M. R","Department of IT, SSN College of Engineering, Chennai, India; Department of IT, SSN College of Engineering, Chennai, India; Department of IT, SSN College of Engineering, Chennai, India; Department of IT, SSN College of Engineering, Chennai, India; Department of IT, SSN College of Engineering, Chennai, India","2020 4th International Conference on Computer, Communication and Signal Processing (ICCCSP)","15 Jan 2021","2020","","","1","5","With the incredible breakthrough of medical imaging intertwined with the computer aided diagnosis and artificial intelligence paved a way for the early detection of tumor. Though Deep Neural Networks is the new paradigm in the field of computer vision yet, they are highly reliable on large dataset to avoid overfitting. To overcome this problem, our work focuses on data augmentation, a quintessential solution to handle the inadequate medical data. This paper involves conventional data augmentation using affine transformations. The conventionally augmented data are further synthesized using General Adversarial Networks (GANs). These methods are employed on the benchmark breast tumor datasets namely MIAS, DDSM and INBreast. The classification of benchmark dataset resulted in an accuracy of 69.85%, which are increased by the conventional data augmentation techniques to 88%. The synthesized image when merged with the original benchmark dataset and the conventional augmented images outperformed the others with an increased accuracy of 94%.","","978-1-7281-6509-7","10.1109/ICCCSP49186.2020.9315189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9315189","General Adversarial Networks;CNN;Breast Tumor;data augmentation;classification","Generative adversarial networks;Breast;Biomedical imaging;Training;Medical diagnostic imaging;Mammography;Feature extraction","cancer;computer vision;data handling;deep learning (artificial intelligence);feature extraction;image classification;mammography;medical computing;medical image processing;tumours","GAN;tumor classification;medical imaging;computer aided diagnosis;artificial intelligence;deep neural networks;computer vision;general adversarial networks;breast tumor datasets;data augmentation;image synthesis;image augmentation;tumor detection;medical data handling;MIAS dataset;DDSM dataset;INBreast dataset;mammogram images","","1","","17","IEEE","15 Jan 2021","","","IEEE","IEEE Conferences"
"Example-Guided Style-Consistent Image Synthesis From Semantic Labeling","M. Wang; G. -Y. Yang; R. Li; R. -Z. Liang; S. -H. Zhang; P. M. Hall; S. -M. Hu","State Key Laboratory of Virtual Reality Technology and Systems, Beihang University; Department of Computer Science and Technology, Tsinghua University, Beijing; Department of Computer Science and Technology, Tsinghua University, Beijing; Department of Computer Science and Technology, Tsinghua University, Beijing; Department of Computer Science and Technology, Tsinghua University, Beijing; University of Bath; State Key Laboratory of Virtual Reality Technology and Systems, Beihang University","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","1495","1504","Example-guided image synthesis aims to synthesize an image from a semantic label map and an exemplary image indicating style. We use the term ""style"" in this problem to refer to implicit characteristics of images, for example: in portraits ""style"" includes gender, racial identity, age, hairstyle; in full body pictures it includes clothing; in street scenes it refers to weather and time of day and such like. A semantic label map in these cases indicates facial expression, full body pose, or scene segmentation. We propose a solution to the example-guided image synthesis problem using conditional generative adversarial networks with style consistency. Our key contributions are (i) a novel style consistency discriminator to determine whether a pair of images are consistent in style; (ii) an adaptive semantic consistency loss; and (iii) a training data sampling strategy, for synthesizing style-consistent results to the exemplar. We demonstrate the efficiency of our method on face, dance and street view synthesis tasks.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953682","Image and Video Synthesis;Deep Learning","","face recognition;feature extraction;image sampling;image segmentation;image texture;learning (artificial intelligence);pose estimation","semantic labeling;semantic label map;exemplary image indicating style;term style;example-guided image synthesis problem;novel style consistency discriminator;adaptive semantic consistency loss;synthesizing style-consistent results;dance;street view synthesis tasks;example-guided style-consistent image synthesis","","45","","52","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"CFC-GAN: Forecasting Road Surface Crack Using Forecasted Crack Generative Adversarial Network","A. Sekar; V. Perumal","Department of Computer Technology, Madras Institute of Technology, Anna University, Chennai, India; Department of Computer Technology, Madras Institute of Technology, Anna University, Chennai, India","IEEE Transactions on Intelligent Transportation Systems","8 Nov 2022","2022","23","11","21378","21391","Forecasting the road surface crack images with given present crack images is an important task to assist the road survivors in planning for their next lay down of road with the required financial assistance. We develop a Crack ForeCast (CFC) dataset comprised of road crack images captured with certain time intervals (months), the number of vehicles traveled (collected from toll plaza), and climatic information like temperature and precipitation. We have proposed a Conditional Forecasted Crack-Generative Adversarial Network (CFC-GAN) model to forecast the road surface crack images with various conditional factors. The proposed CFC-GAN model is trained in a paired end-to-end manner to avoid accumulative blurriness and to generate the appropriate forecasted crack images. Moreover, this paper introduces a new estimation loss function to improve the accuracy of the forecasted crack images. Wide experimental results were demonstrated to showcase the better performance of our model. The quantitative and qualitative analyses were made with various evaluation metrics like structural similarity, peak signal to noise ratio, inception score, Frechet inception distance, and intersection over union for the CFC-GAN model with the proposed dataset and other existing benchmark datasets.","1558-0016","","10.1109/TITS.2022.3171433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770464","Conditional forecasted crack-generative adversarial network (CFC-GAN);crack image synthesis;image-to-image translation;paired image synthesis;road crack forecasting","Roads;Generative adversarial networks;Predictive models;Faces;Surface cracks;Forecasting;Aging","crack detection;cracks;forecasting theory;roads;surface cracks;traffic engineering computing","appropriate forecasted crack images;CFC-GAN model;Conditional Forecasted Crack-Generative Adversarial Network;Crack ForeCast;Forecasted Crack Generative Adversarial Network;forecasting road surface Crack;given present crack images;paired end-to-end manner;required financial assistance;road crack images;road survivors","","1","","56","IEEE","6 May 2022","","","IEEE","IEEE Journals"
"Synthetic data augmentation using GAN for improved liver lesion classification","M. Frid-Adar; E. Klang; M. Amitai; J. Goldberger; H. Greenspan","Department of Biomedical Engineering, Tel Aviv University, Tel Aviv, Israel; Department of Diagnostic Imaging, The Chaim Sheba Medical Center, Tel-Hashomer, Israel; Department of Diagnostic Imaging, The Chaim Sheba Medical Center, Tel-Hashomer, Israel; Faculty of Engineering, Bar-Ilan University, Ramat-Gan, Israel; Department of Biomedical Engineering, Tel Aviv University, Tel Aviv, Israel","2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)","24 May 2018","2018","","","289","293","In this paper, we present a data augmentation method that generates synthetic medical images using Generative Adversarial Networks (GANs). We propose a training scheme that first uses classical data augmentation to enlarge the training set and then further enlarges the data size and its diversity by applying GAN techniques for synthetic data augmentation. Our method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). The classification performance using only classic data augmentation yielded 78.6% sensitivity and 88.4% specificity. By adding the synthetic data augmentation the results significantly increased to 85.7% sensitivity and 92.4% specificity.","1945-8452","978-1-5386-3636-7","10.1109/ISBI.2018.8363576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8363576","Image synthesis;data augmentation;generative adversarial network;liver lesions;lesion classification","Lesions;Liver;Gallium nitride;Training;Medical diagnostic imaging;Task analysis","cancer;computerised tomography;image classification;liver;medical image processing;tumours","synthetic data augmentation;data augmentation method;synthetic medical images;classic data augmentation;generative adversarial networks;liver lesion classification;computed tomography images","","239","","12","IEEE","24 May 2018","","","IEEE","IEEE Conferences"
"3D Conditional Adversarial Learning for Synthesizing Microscopic Neuron Image Using Skeleton-to-Neuron Translation","Z. Tang; D. Zhang; Y. Song; H. Wang; D. Liu; C. Zhang; S. Liu; H. Peng; W. Cai","School of Computer Science, University of Sydney, Australia; School of Computer Science, University of Sydney, Australia; School of Computer Science and Engineering, University of New South Wales, Australia; School of Computer Science, University of Sydney, Australia; School of Computer Science, University of Sydney, Australia; School of Computer Science, University of Sydney, Australia; Digital Services, Digital Technology & Innovation, Siemens Healthineers, Princeton, NJ, USA; Allen Institute for Brain Science, Seattle, WA, USA; School of Computer Science, University of Sydney, Australia","2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)","22 May 2020","2020","","","1775","1779","The automatic reconstruction of single neuron cells from microscopic images is essential to establishing the research on neuron morphology. However, the performance of single neuron reconstruction algorithms is constrained by both the quantity and the quality of the annotated 3D microscopic images since annotating large-scale single neuron models is highly labour intensive. We propose a framework for synthesizing microscopy-realistic 3D neuron images from simulated single neuron skeletons using conditional Generative Adversarial Networks (cGAN). We build the generator network with multi-resolution sub-modules to improve the output fidelity. We evaluate our framework on Janelia-Fly dataset from the BigNeuron project. With both qualitative and quantitative analysis, we show that the proposed framework outperforms the other state-of-the-art methods regarding the quality of the synthetic neuron images. We also show that combining the real neuron images and the synthetic images generated from our framework can improve the performance of neuron segmentation.","1945-8452","978-1-5386-9330-8","10.1109/ISBI45749.2020.9098345","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098345","Image Synthesis;Data Augmentation;Neuron Image","Neurons;Three-dimensional displays;Generators;Skeleton;Microscopy;Training;Morphology","biomedical optical imaging;brain models;image reconstruction;image resolution;image segmentation;medical image processing;neural nets;neurophysiology;optical microscopy","neuron segmentation;microscopic neuron image;skeleton-to-neuron translation;single neuron cells;neuron morphology;single neuron reconstruction algorithms;large-scale single neuron models;simulated single neuron skeletons;conditional generative adversarial networks;synthetic neuron images;3D conditional adversarial learning;microscopy-realistic 3D neuron images;Janelia-Fly dataset;BigNeuron project","","5","","24","IEEE","22 May 2020","","","IEEE","IEEE Conferences"
"T1-Weighted Contrast-Enhanced Synthesis for Multi-Contrast MRI Segmentation","M. Özbey; T. Çukur","Ulusal Manyetik Rezonans Araştırma Merkezi, Bilkent Üniversitesi, Ankara, Türkiye; Sinirbilim Programı, Mühendislik ve Fen Bilimleri Enstitüsü, Bilkent Üniversitesi, Ankara, Türkiye","2020 28th Signal Processing and Communications Applications Conference (SIU)","7 Jan 2021","2020","","","1","4","In recent years, deep learning techniques have been used in computer science as well as in many other disciplines. Successful detection of complex relationships and connections within big data enables effective use of deep learning in many areas. Deep learning with the detection of patterns and abnormalities in images is also a promising method for the field of Radiology. Detection of abnormalities in MR images enables detection of brain tumor and can automate this process. However, the deep learning models developed for brain tumor detection are sensitive to missing MR images in the input data and therefore the model is not robust enough. One of the models used for brain tumor detection requires a combined MR image in 4 different contrast and sequence; T1, T2, Flair and T1c images. In this study, it is proposed to synthesize the missing contrast image in the input data with another deep learning technique. Incomplete T1c MR image was synthesized by the generative adversarial networks (GAN) method and brain tumor detection performance was examined.","2165-0608","978-1-7281-7206-4","10.1109/SIU49456.2020.9302109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9302109","Tumor segmentation;Deep learning;Generative adversarial network;MR image synthesis","Gallium nitride;Image segmentation;Brain modeling;Deep learning;Tumors;Three-dimensional displays;Data models","biomedical MRI;brain;image segmentation;learning (artificial intelligence);medical image processing;tumours","deep learning models;brain tumor detection;combined MR image;deep learning technique;multicontrast MRI segmentation;big data;T1-weighted contrast-enhanced synthesis;generative adversarial networks","","2","","","IEEE","7 Jan 2021","","","IEEE","IEEE Conferences"
"Diverse Audio-to-Image Generation via Semantics and Feature Consistency","P. -T. Yang; F. -G. Su; Y. -C. F. Wang","Department of Elecmcal Engineering, National Taiwan University, Taiwan; Department of Elecmcal Engineering, National Taiwan University, Taiwan; Department of Elecmcal Engineering, National Taiwan University, Taiwan","2020 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","31 Dec 2020","2020","","","1188","1192","Humans are capable of imagining scene images when hearing ambient sounds. Therefore, audio-to-image synthesis becomes a challenging yet practical topic for both natural language comprehension and image content understanding. In this paper, we propose an audio-to-image generation network by applying the conditional generative adversarial networks. Specifically, we utilize such generative models with the proposed feature consistency and conditional adversarial losses, so that diverse image outputs with satisfactory visual quality can be synthesized from a single audio input. Experimental results on sports audio/visual data verify that the effectiveness and practicality of the proposed method over the state-of-the-art approaches on audio-to-image synthesis.","2640-0103","978-988-14768-8-3","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306323","audio-to-image generation;conditional generative adversarial network;cross-modal generation","Image segmentation;Sports;Feature extraction;Generators;Visualization;Testing;Acoustics","audio signal processing;data visualisation;feature extraction;natural languages;speech processing;sport","challenging yet practical topic;natural language comprehension;image content understanding;audio-to-image generation network;conditional generative adversarial networks;generative models;feature consistency;conditional adversarial losses;diverse image;single audio input;audio-to-image synthesis;scene images;ambient sounds","","","","20","IEEE","31 Dec 2020","","","IEEE","IEEE Conferences"
"Synthesis of Optical Nerve Head Region of Fundus Image","A. Deshmukh; J. Sivaswamy","Center for Visual Information Technology, IIIT Hyderabad, India; Center for Visual Information Technology, IIIT Hyderabad, India","2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)","11 Jul 2019","2019","","","583","586","The Optic Disc (OD) and Optic Cup (OC) boundaries play a critical role in the detection of glaucoma. However, very few annotated datasets are available for both OD and OC that are required for segmentation. Recently, Convolutional Neural Networks have shown significant improvements in segmentation performance. However, the full potential of CNNs is hindered by the lack of a large amount of annotated training images. To address this issue, we explore a method to generate synthetic images which can be used to augment the training data. Given the segmentation masks of OD, OC and vessels from arbitrarily different fundus images, the proposed method employs a combination of B-spline registration and GAN to generate high quality images that ensure that the vessels bend at the edge of the OC in a realistic manner. In contrast, the existing GAN based methods for fundus image synthesis fail to capture the local details and vasculature in the Optic Nerve Head (ONH) region. The utility of the proposed method in training deep networks for the challenging problem of OC segmentation is explored and an improvement in the dice score from 0.85 to 0.902 is seen with the inclusion of the synthetic images in the training set.","1945-8452","978-1-5386-3641-1","10.1109/ISBI.2019.8759414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759414","Generative Adversarial Networks;Synthetic Images;Cup Segmentation","Image segmentation;Training;Image edge detection;Optical imaging;Biomedical optical imaging;Gallium nitride;Convolution","biomedical optical imaging;convolutional neural nets;diseases;image classification;image registration;image segmentation;medical image processing;splines (mathematics)","annotated training images;synthetic images;segmentation masks;vessels;B-spline registration;high quality images;fundus image synthesis;deep networks;OC segmentation;OD;annotated datasets;convolutional neural networks;GAN based methods;optic nerve head region;optic disc;optic cup","","5","","13","IEEE","11 Jul 2019","","","IEEE","IEEE Conferences"
"LiWGAN: A Light Method to Improve the Performance of Generative Adversarial Network","N. A. Mashudi; N. Ahmad; N. Mohd Noor","Faculty of Information Technology, City University Malaysia, Petaling Jaya, Selangor, Malaysia; Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur Campus, Kuala Lumpur, Malaysia; Razak Faculty of Technology and Informatics, Universiti Teknologi Malaysia, Kuala Lumpur Campus, Kuala Lumpur, Malaysia","IEEE Access","13 Sep 2022","2022","10","","93155","93167","Generative adversarial networks (GANs) gained tremendous growth due to the potency and efficiency in producing realistic samples. This study proposes a light-weight GAN (LiWGAN) to learn non-image synthesis with minimum computational time for less power computing. Hence, the LiWGAN method enhanced a new skip-layer channel-wise excitation module (SLE) and a self-supervised discriminator design for non-synthesis performance using the facemask dataset. Facemask is one of the preventative strategies pioneered by the current COVID-19 pandemic. LiWGAN manipulates a non-image synthesis of facemasks that could be beneficial for some researchers to identify an individual using lower power devices, occlusion challenges for face recognition, and alleviate the accuracy challenges due to limited datasets. The study evaluates the performance of the processing time in terms of batch sizes and image resolutions using the facemask dataset. The Fréchet inception distance (FID) was also measured on the facemask images to evaluate the quality of the augmented image using LiWGAN. The findings for 3000 generated images showed a nearly similar FID score at 220.43 with significantly less processing time per iteration at 1.03s than StyleGAN at 219.97 FID score. One experiment was conducted using the CelebA dataset to compare with GL-GAN and DRAGAN, proving LiWGAN is appropriate for other datasets. The outcomes found LiWGAN performed better than GL-GAN and DRAGAN at 91.31 FID score with 3.50s processing time per iteration. Therefore, LiWGAN could aim to enhance the FID score to be near zero in the future with less processing time by using different datasets.","2169-3536","","10.1109/ACCESS.2022.3203065","Universiti Teknologi Malaysia(grant numbers:R.K130000.2656.18J05); Ministry of Higher Education Malaysia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870777","Non-image synthesis;self-supervised discriminator;data augmentation;deep learning;generative adversarial network","Generative adversarial networks;Training data;Generators;Face recognition;Data models;Optimization;Neural networks;Self-supervised learning;Data augmentation;Deep learning","face recognition;feature extraction;image resolution;image sensors;learning (artificial intelligence);neural nets;object detection;statistical analysis","facemask dataset;facemask images;augmented image;processing time;219.97 FID score;GL-GAN;light method;generative adversarial network;light-weight GAN;nonimage synthesis;minimum computational time;power computing;LiWGAN method;skip-layer channel-wise excitation module;nonsynthesis performance;image resolutions;time 1.03 s;time 3.5 s","","","","64","CCBY","31 Aug 2022","","","IEEE","IEEE Journals"
"Adversarial synthesis learning enables segmentation without target modality ground truth","Y. Huo; Z. Xu; S. Bao; A. Assad; R. G. Abramson; B. A. Landman","Electrical Engineering, Vanderbilt University, Nashville, TN, USA; Electrical Engineering, Vanderbilt University, Nashville, TN, USA; Computer Science, Vanderbilt University, Nashville, TN, USA; Incyte Corporation, Wilmington, DE, USA; Radiology and Radiological Science, Vanderbilt University, Nashville, TN, USA; Radiology and Radiological Science, Vanderbilt University, Nashville, TN, USA","2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)","24 May 2018","2018","","","1217","1220","A lack of generalizability is one key limitation of deep learning based segmentation. Typically, one manually labels new training images when segmenting organs in different imaging modalities or segmenting abnormal organs from distinct disease cohorts. The manual efforts can be alleviated if one is able to reuse manual labels from one modality (e.g., MRI) to train a segmentation network for a new modality (e.g., CT). Previously, two stage methods have been proposed to use cycle generative adversarial networks (CycleGAN) to synthesize training images for a target modality. Then, these efforts trained a segmentation network independently using synthetic images. However, these two independent stages did not use the complementary information between synthesis and segmentation. Herein, we proposed a novel end-to-end synthesis and segmentation network (EssNet) to achieve the unpaired MRI to CT image synthesis and CT splenomegaly segmentation simultaneously without using manual labels on CT. The end-to-end EssNet achieved significantly higher median Dice similarity coefficient (0.9188) than the two stages strategy (0.8801), and even higher than canonical multi-atlas segmentation (0.9125) and ResNet method (0.9107), which used the CT manual labels.","1945-8452","978-1-5386-3636-7","10.1109/ISBI.2018.8363790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8363790","","Computed tomography;Image segmentation;Magnetic resonance imaging;Manuals;Training;Biomedical imaging;Biological systems","biomedical MRI;computerised tomography;diseases;image classification;image segmentation;image sensors;learning (artificial intelligence);medical image processing","CT splenomegaly segmentation;end-to-end EssNet;canonical multiatlas segmentation;CT manual labels;adversarial synthesis learning;target modality ground truth;deep learning based segmentation;manually labels;training images;segmenting organs;segmenting abnormal organs;distinct disease cohorts;manual efforts;segmentation network;cycle generative adversarial networks;synthetic images;novel end-to-end synthesis;CT image synthesis;imaging modalities;CycleGAN;unpaired MRI","","63","","17","IEEE","24 May 2018","","","IEEE","IEEE Conferences"
"Spectral Normalization and Relativistic Adversarial Training for Conditional Pose Generation with Self-Attention","Y. Horiuchi; S. Iizuka; E. Simo-Serra; H. Ishikawa","Waseda University, Tokyo, Japan; University of Tsukuba, Tsukuba, Ibaraki; Waseda University, Tokyo, Japan; Waseda University, Tokyo, Japan","2019 16th International Conference on Machine Vision Applications (MVA)","11 Jul 2019","2019","","","1","5","We address the problem of conditional image generation of synthesizing a new image of an individual given a reference image and target pose. We base our approach on generative adversarial networks and leverage deformable skip connections to deal with pixel-to-pixel misalignments, self-attention to leverage complementary features in separate portions of the image, e.g., arms or legs, and spectral normalization to improve the quality of the synthesized images. We train the synthesis model with a nearest-neighbour loss in combination with a relativistic average hinge adversarial loss. We evaluate on the Market-1501 dataset and show how our proposed approach can surpass existing approaches in conditional image synthesis performance.","","978-4-901122-18-4","10.23919/MVA.2019.8758013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758013","","Antenna arrays;MIMO communication;Correlation;Channel estimation;Downlink;Fading channels","image resolution;learning (artificial intelligence);pose estimation","spectral normalization;relativistic adversarial training;conditional pose generation;conditional image generation;reference image;generative adversarial networks;leverage deformable skip connections;pixel-to-pixel misalignments;leverage complementary features;separate portions;synthesized images;nearest-neighbour loss;relativistic average hinge adversarial loss;conditional image synthesis performance;self-attention;Market-1501 dataset","","3","","16","","11 Jul 2019","","","IEEE","IEEE Conferences"
"Controllable Skin Lesion Synthesis Using Texture Patches, Bézier Curves and Conditional GANs","D. A. B. Oliveira","IBM Research, Rua Tutoia 1157, Vila Mariana, Sao Paulo, Brazil","2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)","22 May 2020","2020","","","1798","1802","Data synthesis is an important tool for improving data availability in cases where data is hard to capture or annotate. In the context of skin lesions data, data synthesis has been used for data augmentation in automated classification methods or for supporting training of dermoscopic images visual inspection. In this paper, we propose a simple yet effective approach for diverse skin lesion image synthesis using conditional generative adversarial networks. Our pipeline takes as input a random Bézier curve representing the lesion mask, and two texture patches: one for skin, and one for lesion; and synthesizes a new dermoscopic image. Our method generates images where lesions and skin reproduce the corresponding provided texture patches, and the lesion conforms to the provided Bézier mask. Our results report realistic controllable synthesis and improved performance for skin lesion segmentation task considering different semantic segmentation networks in a public challenge in comparison to classic data augmentation.","1945-8452","978-1-5386-9330-8","10.1109/ISBI45749.2020.9098676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098676","","Lesions;Skin;Training;Image segmentation;Gallium nitride;Generators;Shape","biomedical optical imaging;cancer;image classification;image segmentation;image texture;medical image processing;neural nets;skin","controllable skin lesion synthesis;data synthesis;data availability;skin lesion data;automated classification methods;diverse skin lesion image synthesis;conditional generative adversarial networks;lesion mask;dermoscopic image;texture patches;realistic controllable synthesis;skin lesion segmentation task;classic data augmentation;random Bezier curve;Bezier mask;conditional GAN;dermoscopic image visual inspection","","2","","13","IEEE","22 May 2020","","","IEEE","IEEE Conferences"
"Quality Assessment of Synthetic Fluorescence Microscopy Images for Image Segmentation","Y. Feng; X. Chai; Q. Ba; G. Yang","Department of Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA, USA","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","814","818","Synthetic images are widely used in image segmentation for algorithm training and performance assessment. Recently, advances in image synthesis techniques, especially generative adversarial networks (GANs), have made it possible to generate fluorescence microscopy images with remarkably realistic appearance. However, intuitive and specific metrics to assess the quality of these images remain lacking. Here, we propose three quality metrics that quantify the fidelity of the foreground signal, the background noise, and blurring, respectively, of synthesized fluorescence microscopy images. Using these metrics, we examine images of mitochondria synthesized by two representative GANs: pix2pix, which requires paired training data, and CycleGAN, which does not require paired training data. We find that both networks generate realistic images and achieve similar fidelity in reproducing background noise and blurring of real images. However, CycleGAN achieves significantly higher fidelity than pix2pix in reproducing intensity patterns of real mitochondria. When used to train the U-Net for segmentation, images synthesized by both networks achieve performance on par with real images. Overall, we have developed a method to assess quality of synthetic fluorescence microscopy images and to evaluate their training performance in image segmentation. The quality metrics proposed are general and can be used to assess fluorescence microscopy images synthesized by different methods.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8802971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8802971","Quality assessment;synthetic image;fluorescence microscopy;generative adversarial network;image segmentation","Fluorescence;Image segmentation;Microscopy;Training;Noise measurement;Gallium nitride","biomedical optical imaging;fluorescence;image segmentation;medical image processing;optical microscopy;realistic images","U-Net;mitochondria;quality assessment;image segmentation;synthetic fluorescence microscopy images;paired training data;quality metrics;generative adversarial networks;image synthesis techniques","","2","","20","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Structured GANs","I. Peleg; L. Wolf",Tel Aviv University; Facebook AI Research,"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)","7 May 2018","2018","","","719","728","We present Generative Adversarial Networks (GANs), in which the symmetric property of the generated images is controlled. This is obtained through the generator network's architecture, while the training procedure and the loss remain the same. The symmetric GANs are applied to face image synthesis in order to generate novel faces with a varying amount of symmetry. We also present an unsupervised face rotation capability, which is based on the novel notion of one-shot fine tuning.","","978-1-5386-4886-5","10.1109/WACV.2018.00084","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8354188","","Gallium nitride;Generators;Kernel;Training;Face;Computer architecture;Measurement","image processing;learning (artificial intelligence);neural nets","symmetric property;training procedure;unsupervised face rotation capability;generative adversarial networks;image generation;structured GAN;symmetric GAN;face image synthesis;one-shot fine tuning","","2","","13","IEEE","7 May 2018","","","IEEE","IEEE Conferences"
"Dcgan for the Synthesis of Multivariate Multifractal Textures: How do We Know it Works?","V. Mauduit; P. Abry; R. Leonarduzzi; S. G. Roux; E. Quemener","Univ Lyon, ENS de Lyon, Univ Claude Bernard, CNRS, Laboratoire de Physique, Lyon, France; Univ Lyon, ENS de Lyon, Univ Claude Bernard, CNRS, Laboratoire de Physique, Lyon, France; ENS, PSL, Paris, France; Univ Lyon, ENS de Lyon, Univ Claude Bernard, CNRS, Laboratoire de Physique, Lyon, France; Univ Lyon, ENS de Lyon, Univ Claude Bernard, CNRS, Laboratoire de Physique, Lyon, France","2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)","20 Oct 2020","2020","","","1","6","Deep Learning is nowadays widely used for several tasks in image processing. Notably, it has been massively used for image synthesis, mostly however with strong geometrical contents. Focused on a race for better performance via more complex architectures, research on Deep Learning, however, left behind the critical issue of assessing quantitatively and in a reproducible manner the quality of the synthesized images, notably for the case of pure textures. The present work aims to study the ability of Deep Convolutional Generative Adversarial Networks to synthesize multivariate textures characterized by rich multiscale multivariate statistics (multifractals). The focus is thus on quantifying the quality of the synthesized textures, on assessing the reproducibility of the learning procedure and on studying the impact of loss functions and of training dataset sizes, rather than on proposing yet another architecture.","1551-2541","978-1-7281-6662-9","10.1109/MLSP49062.2020.9231828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9231828","Generative Adversarial Network;Multivariate Texture Synthesis;Quality Assessment;Multifractals","Fractals;Training;Wavelet analysis;Gallium nitride;Generators;Optimization;Deep learning","convolutional neural nets;image classification;image texture;learning (artificial intelligence);statistical analysis","dcgan;multivariate multifractal textures;deep learning;image processing;image synthesis;strong geometrical contents;deep convolutional generative adversarial networks;multiscale multivariate statistics;synthesized textures","","1","","30","IEEE","20 Oct 2020","","","IEEE","IEEE Conferences"
"Synthesizing Optical and SAR Imagery From Land Cover Maps and Auxiliary Raster Data","G. Baier; A. Deschemps; M. Schmitt; N. Yokoya","RIKEN Center for Advanced Intelligence Project, Tokyo, Japan; SERPICO Team of Inria, Bretagne-Atlantique, Rennes, France; Department of Geoinformatics, Munich University of Applied Sciences, Munich, Germany; RIKEN Center for Advanced Intelligence Project, Tokyo, Japan","IEEE Transactions on Geoscience and Remote Sensing","20 Dec 2021","2022","60","","1","12","We synthesize both optical RGB and synthetic aperture radar (SAR) remote sensing images from land cover maps and auxiliary raster data using generative adversarial networks (GANs). In remote sensing, many types of data, such as digital elevation models (DEMs) or precipitation maps, are often not reflected in land cover maps but still influence image content or structure. Including such data in the synthesis process increases the quality of the generated images and exerts more control on their characteristics. Spatially adaptive normalization layers fuse both inputs and are applied to a full-blown generator architecture consisting of encoder and decoder to take full advantage of the information content in the auxiliary raster data. Our method successfully synthesizes medium (10 m) and high (1 m) resolution images when trained with the corresponding data set. We show the advantage of data fusion of land cover maps and auxiliary information using mean intersection over unions (mIoUs), pixel accuracy, and Fréchet inception distances (FIDs) using pretrained U-Net segmentation models. Handpicked images exemplify how fusing information avoids ambiguities in the synthesized images. By slightly editing the input, our method can be used to synthesize realistic changes, i.e., raising the water levels. The source code is available at https://github.com/gbaier/rs_img_synth, and we published the newly created high-resolution data set at https://ieee-dataport.org/open-access/geonrw.","1558-0644","","10.1109/TGRS.2021.3068532","Japan Society for the Promotion of Science through KAKENHI(grant numbers:18K18067,20K19834); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9406194","Deep learning;generative adversarial network (GAN);image synthesis;synthetic aperture radar (SAR)","Generators;Semantics;Remote sensing;Image synthesis;Radar polarimetry;Image segmentation;Training","geophysical image processing;image segmentation;land cover;synthetic aperture radar;terrain mapping","land cover maps;auxiliary raster data;synthetic aperture radar remote sensing images;precipitation maps;image content;high resolution images;data fusion;synthesized images;high-resolution data;optical Imagery;SAR Imagery;optical RGB;generative adversarial networks;size 1.0 m;size 10.0 m","","7","","49","IEEE","16 Apr 2021","","","IEEE","IEEE Journals"
"ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement","Y. Alaluf; O. Patashnik; D. Cohen-Or","Blavatnik School of Computer Science, Tel Aviv University; Blavatnik School of Computer Science, Tel Aviv University; Blavatnik School of Computer Science, Tel Aviv University","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","6691","6700","Recently, the power of unconditional image synthesis has significantly advanced through the use of Generative Adversarial Networks (GANs). The task of inverting an image into its corresponding latent code of the trained GAN is of utmost importance as it allows for the manipulation of real images, leveraging the rich semantics learned by the network. Recognizing the limitations of current inversion approaches, in this work we present a novel inversion scheme that extends current encoder-based inversion methods by introducing an iterative refinement mechanism. Instead of directly predicting the latent code of a given real image using a single pass, the encoder is tasked with predicting a residual with respect to the current estimate of the inverted latent code in a self-correcting manner. Our residual-based encoder, named ReStyle, attains improved accuracy compared to current state-of-the-art encoder-based methods with a negligible increase in inference time. We analyze the behavior of ReStyle to gain valuable insights into its iterative nature. We then evaluate the performance of our residual encoder and analyze its robustness compared to optimization-based inversion and state-of-the-art encoders. Code is available via our project page: https: //yuval-alaluf.github.io/restyle-encoder/","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711424","Neural generative models;Image and video synthesis;Representation learning;Vision applications and systems","Training;Codes;Image synthesis;Semantics;Generative adversarial networks;Robustness;Iterative methods","","","","62","","52","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"One-Shot Synthesis of Images and Segmentation Masks","V. Sushko; D. Zhang; J. Gall; A. Khoreva",Bosch Center for Artificial Intelligence; University of Tübingen; University of Bonn; University of Tübingen,"2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","6 Feb 2023","2023","","","6274","6283","Joint synthesis of images and segmentation masks with generative adversarial networks (GANs) is promising to reduce the effort needed for collecting image data with pixel-wise annotations. However, to learn high-fidelity image-mask synthesis, existing GAN approaches first need a pre-training phase requiring large amounts of image data, which limits their utilization in restricted image domains. In this work, we take a step to reduce this limitation, introducing the task of one-shot image-mask synthesis. We aim to generate diverse images and their segmentation masks given only a single labelled example, and assuming, contrary to previous models, no access to any pre-training data. To this end, inspired by the recent architectural developments of single-image GANs, we introduce our OSMIS model which enables the synthesis of segmentation masks that are precisely aligned to the generated images in the one-shot regime. Besides achieving the high fidelity of generated masks, OSMIS outperforms state-of-the-art single-image GAN models in image synthesis quality and diversity. In addition, despite not using any additional data, OSMIS demonstrates an impressive ability to serve as a source of useful data augmentation for one-shot segmentation applications, providing performance gains that are complementary to standard data augmentation techniques. Code is available at https://github.com/boschresearch/one-shot-synthesis.","2642-9381","978-1-6654-9346-8","10.1109/WACV56688.2023.00622","Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030467","Algorithms: Computational photography;image and video synthesis;Machine learning architectures;formulations;and algorithms (including transfer;low-shot;semi-;self-;and un-supervised learning)","Training;Image segmentation;Computer vision;Codes;Image synthesis;Performance gain;Generative adversarial networks","","","","","","35","IEEE","6 Feb 2023","","","IEEE","IEEE Conferences"
"GEN: Generative Equivariant Networks for Diverse Image-to-Image Translation","P. Shamsolmoali; M. Zareapoor; S. Das; S. García; E. Granger; J. Yang","School of Communication and Electronic Engineering, East China Normal University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Electronics and Communication Sciences Unit, Indian Statistical Institute, Kolkata, India; Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain; Department of Systems Engineering, École de technologie supérieure, Université du Québec, Quebec City, QC, Canada; Department of Automation, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Cybernetics","13 Jan 2023","2023","53","2","874","886","Image-to-image (I2I) translation has become a key asset for generative adversarial networks. Convolutional neural networks (CNNs), despite having a significant performance, are not able to capture the spatial relationships among different parts of an object and, thus, do not qualify as the ideal representative model for image translation tasks. As a remedy to this problem, capsule networks have been proposed to represent patterns for a visual object in such a way that preserves hierarchical spatial relationships. The training of capsules is constrained by learning all pairwise relationships between capsules of consecutive layers. This design would be prohibitively expensive both in time and memory. In this article, we present a new framework for capsule networks to provide a full description of the input components at various levels of semantics, which can successfully be applied to the generator-discriminator architectures without incurring computational overhead compared to the CNNs. To successfully apply the proposed capsules in the generative adversarial network, we put forth a novel Gromov–Wasserstein (GW) distance as a differentiable loss function that compares the dissimilarity between two distributions and then guides the learned distribution toward target properties, using optimal transport (OT) discrepancy. The proposed method—which is called generative equivariant network (GEN)—is an alternative architecture for GANs with equivariance capsule layers. The proposed model is evaluated through a comprehensive set of experiments on I2I translation and image generation tasks and compared with several state-of-the-art models. Results indicate that there is a principled connection between generative and capsule models that allows extracting discriminant and invariant information from image data","2168-2275","","10.1109/TCYB.2022.3166761","NSFC(grant numbers:61876107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770477","Capsule networks;disentangle representation;generative model;image-to-image (I2I) translation","Semantics;Generators;Task analysis;Faces;Shape;Image synthesis;Generative adversarial networks","convolutional neural nets;feature extraction;image representation","capsule models;capsule networks;CNN;convolutional neural networks;diverse image-to-image translation;equivariance capsule layers;GEN;generative adversarial network;generative equivariant network;generative models;generator-discriminator architectures;hierarchical spatial relationships;ideal representative model;image data;image generation tasks;pairwise relationships;visual object","","1","","61","IEEE","6 May 2022","","","IEEE","IEEE Journals"
"Semantic Palette: Guiding Scene Generation with Class Proportions","G. Le Moing; T. -H. Vu; H. Jain; P. Pérez; M. Cord",Inria; Valeo.ai; Valeo.ai; Valeo.ai; Valeo.ai,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","9338","9346","Despite the recent progress of generative adversarial networks (GANs) at synthesizing photo-realistic images, producing complex urban scenes remains a challenging problem. Previous works break down scene generation into two consecutive phases: unconditional semantic layout synthesis and image synthesis conditioned on layouts. In this work, we propose to condition layout generation as well for higher semantic control: given a vector of class proportions, we generate layouts with matching composition. To this end, we introduce a conditional framework with novel architecture designs and learning objectives, which effectively accommodates class proportions to guide the scene generation process. The proposed architecture also allows partial layout editing with interesting applications. Thanks to the semantic control, we can produce layouts close to the real distribution, helping enhance the whole scene generation process. On different metrics and urban scene benchmarks, our models outperform existing baselines. Moreover, we demonstrate the merit of our approach for data augmentation: semantic segmenters trained on real layout-image pairs along with additional ones generated by our approach outperform models only trained on real pairs.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00922","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577811","","Training;Measurement;Image segmentation;Image synthesis;Semantics;Layout;Process control","feature extraction;image classification;image matching;image segmentation;integrated circuit layout;learning (artificial intelligence);natural scenes;realistic images;video signal processing","guiding scene generation;class proportions;generative adversarial networks;photo-realistic images;complex urban scenes;unconditional semantic layout synthesis;image synthesis;condition layout generation;higher semantic control;conditional framework;architecture designs;learning objectives;scene generation process;partial layout editing;urban scene benchmarks;semantic segmenters;layout-image pairs;semantic palette","","","","34","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Edge-guided Adversarial Network Based on Contrastive Learning for Image-to-Image Translation","C. Zhu; R. Lai; L. Bi; X. Wang; J. Du","School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Mechnical Engineering, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China; School of Automation, Beijing Institute of Technology, Beijing, China","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","7949","7954","In recent years, generative adversarial networks have made great progress in image synthesis and image translation tasks in the field of image processing and computer vision. However, the quality of the generated image and the scalability over multiple datasets is still not satisfying. We briefly review some prior works and propose a method for image-to-image translation, which is learning a mapping between different visual domains. The network extracts edge feature from both domains of output and target, and minimizes the difference using a framework based on patchwise contrastive learning. We apply edge feature guidance in our method and select Sobel operator among several classical edge detection operators. We demonstrate that our method outperforms existing approaches in the task of unpaired image-to-image translation across datasets.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549847","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549847","Generative adversarial network;Image-to-Image translation;Contrastive learning;Edge detection","Training;Visualization;Image synthesis;Statistical analysis;Image edge detection;Scalability;Feature extraction","computer vision;edge detection;feature extraction;image processing","edge-guided adversarial network;generative adversarial networks;image synthesis;image translation tasks;image processing;computer vision;patchwise contrastive learning;unpaired image-to-image translation","","","","27","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Text to Image Generation with Semantic-Spatial Aware GAN","W. Liao; K. Hu; M. Y. Yang; B. Rosenhahn","TNT, Leibniz University, Hannover, Germany; TNT, Leibniz University, Hannover, Germany; SUG, University of Twente, The Netherlands; TNT, Leibniz University, Hannover, Germany","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","18166","18175","Text-to-image synthesis (T2I) aims to generate photorealistic images which are semantically consistent with the text descriptions. Existing methods are usually built upon conditional generative adversarial networks (GANs) and initialize an image from noise with sentence embedding, and then refine the features with fine-grained word embedding iteratively. A close inspection of their generated images reveals a major limitation: even though the generated image holistically matches the description, individual image regions or parts of somethings are often not recognizable or consistent with words in the sentence, e.g. “a white crown”. To address this problem, we propose a novel framework Semantic-Spatial Aware GAN for synthesizing images from input text. Concretely, we introduce a simple and effective Semantic-Spatial Aware block, which (1) learns semantic-adaptive transformation conditioned on text to effectively fuse text features and image features, and (2) learns a semantic mask in a weakly-supervised way that depends on the current text-image fusion process in order to guide the transformation spatially. Experiments on the challenging COCO and CUB bird datasets demonstrate the advantage of our method over the recent state-of-the-art approaches, regarding both visual fidelity and alignment with input text description. Code available at https://github.com/wtliao/text2image.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01765","Federal Ministry of Education and Research(grant numbers:01DD20003); Deutsche Forschungsgemeinschaft; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879138","Image and video synthesis and generation; Vision + language","Visualization;Computer vision;Image recognition;Image synthesis;Fuses;Computational modeling;Semantics","feature extraction;image classification;image colour analysis;image fusion;image processing;image representation;image segmentation;learning (artificial intelligence);text analysis","input text description;image generation;text-to-image synthesis;photorealistic images;text descriptions;conditional generative adversarial networks;GANs;sentence embedding;fine-grained word;individual image regions;novel framework Semantic-Spatial Aware GAN;Semantic-Spatial Aware block;semantic-adaptive transformation;text features;image features;semantic mask;current text-image fusion process","","","","39","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"Text to Image Synthesis for Improved Image Captioning","M. Z. Hossain; F. Sohel; M. F. Shiratuddin; H. Laga; M. Bennamoun","Discipline of Information Technology, Murdoch University, Perth, WA, Australia; Discipline of Information Technology, Murdoch University, Perth, WA, Australia; Discipline of Information Technology, Murdoch University, Perth, WA, Australia; Discipline of Information Technology, Murdoch University, Perth, WA, Australia; Department of Computer Science and Software Engineering, The University of Western Australia, Perth, WA, Australia","IEEE Access","3 May 2021","2021","9","","64918","64928","Generating textual descriptions of images has been an important topic in computer vision and natural language processing. A number of techniques based on deep learning have been proposed on this topic. These techniques use human-annotated images for training and testing the models. These models require a large number of training data to perform at their full potential. Collecting human generated images with associative captions is expensive and time-consuming. In this paper, we propose an image captioning method that uses both real and synthetic data for training and testing the model. We use a Generative Adversarial Network (GAN) based text to image generator to generate synthetic images. We use an attention-based image captioning method trained on both real and synthetic images to generate the captions. We demonstrate the results of our models using both qualitative and quantitative analysis on popularly used evaluation metrics. We show that our experimental results achieve two fold benefits of our proposed work: i) it demonstrates the effectiveness of image captioning for synthetic images, and ii) it further improves the quality of the generated captions for real images, understandably because we use additional images for training.","2169-3536","","10.1109/ACCESS.2021.3075579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9416431","Image captioning;synthetic images;attention;generative adversarial network","Training;Visualization;Generative adversarial networks;Decoding;Feature extraction;Computer architecture;Generators","computer vision;feature extraction;image processing;image retrieval;learning (artificial intelligence);natural language processing;object detection;text analysis;video signal processing;visual databases","synthetic data;Generative Adversarial Network based text;image generator;synthetic images;attention-based image captioning method;generated captions;additional images;improved image captioning;natural language processing;human-annotated images;training data;collecting human generated images;associative captions","","8","","51","CCBY","26 Apr 2021","","","IEEE","IEEE Journals"
"Collaging Class-specific GANs for Semantic Image Synthesis","Y. Li; Y. Li; J. Lu; E. Shechtman; Y. J. Lee; K. K. Singh",University of Wisconsin-Madison; Adobe Research; Adobe Research; Adobe Research; University of Wisconsin-Madison; Adobe Research,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","14398","14407","We propose a new approach for high resolution semantic image synthesis. It consists of one base image generator and multiple class-specific generators. The base generator generates high quality images based on a segmentation map. To further improve the quality of different objects, we create a bank of Generative Adversarial Networks (GANs) by separately training class-specific models. This has several benefits including – dedicated weights for each class; centrally aligned data for each model; additional training data from other sources, potential of higher resolution and quality; and easy manipulation of a specific object in the scene. Experiments show that our approach can generate high quality images in high resolution while having flexibility of object-level control by using class-specific generators.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710212","Image and video synthesis","Training;Image segmentation;Computer vision;Image resolution;Image synthesis;Semantics;Training data","","","","6","","39","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Content-Aware GAN Compression","Y. Liu; Z. Shu; Y. Li; Z. Lin; F. Perazzi; S. Y. Kung",Princeton University; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Princeton University,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","12151","12161","Generative adversarial networks (GANs), e.g., StyleGAN2, play a vital role in various image generation and synthesis tasks, yet their notoriously high computational cost hinders their efficient deployment on edge devices. Directly applying generic compression approaches yields poor results on GANs, which motivates a number of recent GAN compression works. While prior works mainly accelerate conditional GANs, e.g., pix2pix and Cycle-GAN, compressing state-of-the-art unconditional GANs has rarely been explored and is more challenging. In this paper, we propose novel approaches for unconditional GAN compression. We first introduce effective channel pruning and knowledge distillation schemes specialized for unconditional GANs. We then propose a novel content-aware method to guide the processes of both pruning and distillation. With content-awareness, we can effectively prune channels that are unimportant to the contents of interest, e.g., human faces, and focus our distillation on these regions, which significantly enhances the distillation quality. On StyleGAN2 and SN-GAN, we achieve a substantial improvement over the state-of-the-art compression method. Notably, we reduce the FLOPs of StyleGAN2 by 11× with visually negligible image quality loss compared to the full-size model. More interestingly, when applied to various image manipulation tasks, our compressed model forms a smoother and better disentangled latent manifold, making it more effective for image editing.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578495","","Manifolds;Image quality;Visualization;Image coding;Image synthesis;Pipelines;Generative adversarial networks","data compression;image coding;neural nets","content-aware GAN compression;StyleGAN2;image generation;edge devices;pix2pix;Cycle-GAN;unconditional GAN compression;effective channel pruning;knowledge distillation schemes;content-awareness;distillation quality;SN-GAN;image quality loss;image manipulation tasks","","9","","59","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Dual Contrastive Loss and Attention for GANs","N. Yu; G. Liu; A. Dundar; A. Tao; B. Catanzaro; L. Davis; M. Fritz",Max Planck Institute for Informatics; Nvidia; Bilkent University; Nvidia; Nvidia; University of Maryland; Cispa Helmholtz Center for Information Security,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","6711","6722","Generative Adversarial Networks (GANs) produce impressive results on unconditional image generation when powered with large-scale image datasets. Yet generated images are still easy to spot especially on datasets with high variance (e.g. bedroom, church). In this paper, we propose various improvements to further push the boundaries in image generation. Specifically, we propose a novel dual contrastive loss and show that, with this loss, discriminator learns more generalized and distinguishable representations to incentivize generation. In addition, we revisit attention and extensively experiment with different attention blocks in the generator. We find attention to be still an important module for successful image generation even though it was not used in the recent state-of-the-art models. Lastly, we study different attention architectures in the discriminator, and propose a reference attention mechanism. By combining the strengths of these remedies, we improve the compelling state-of-the-art Fréchet Inception Distance (FID) by at least 17.5% on several benchmark datasets. We obtain even more significant improvements on compositional synthetic scenes (up to 47.5% in FID).","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710622","Neural generative models;Image and video synthesis","Computer vision;Image synthesis;Computer architecture;Benchmark testing;Generative adversarial networks;Generators","","","","9","","109","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Controlled Multi-modal Image Generation for Plant Growth Modeling","M. Miranda; L. Drees; R. Roscher","German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany; Data Science in Earth Observation, Technical University of Munich, Ottobrunn, Germany","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","5118","5124","Predicting plant development is an important task in precision farming and an essential metric for decision-making by researchers and farmers. In this work, we propose a novel generative modeling technique for plant growth prediction based on conditional generative adversarial networks. We formulate plant growth as an image-to-image translation task and predict the appearance of a plant growth stage as a function of its previous stage. We take into account that plant growth is inherently multi-modal, depending on numerous and highly variable environmental factors, and thus a single input belongs to a distribution of potential outputs. We encode the ambiguity in an interpretable and low-dimensional latent vector space representing the various factors of variation that are influencing plant growth. We use a novel encoder-based data fusion technique and combine information contained in remote sensing imagery of different cropping systems with data containing the factors of variation to adequately model plant growth. This offers several advantages over existing methods: (1) we show that we can model a distribution of potential appearances and simultaneously outperform existing methods in providing more realistic predictions, (2) the complexity of plant growth is more adequately captured, as various factors influencing plant growth can be included, (3) predictions are controllable by being conditioned by an interpretable latent vector representing the factors of variation along with an input image of a previous growth stage.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956115","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956115","","Measurement;Image synthesis;Ecosystems;Predictive models;Generative adversarial networks;Pattern recognition;Task analysis","agriculture;crops;encoding;environmental factors;geophysical image processing;image fusion;regression analysis;remote sensing;sensor fusion;vectors","controlled multimodal image generation;image-to-image translation task;plant growth modeling;plant growth prediction;plant growth stage;predicting plant development;previous growth stage","","","","36","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"Asymmetric Training in RealnessGAN","X. Wang; K. Jin; K. Yu; Y. Cheng","Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, China University of Mining and Technology, Xuzhou, China; Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, China University of Mining and Technology, Xuzhou, China","IEEE Transactions on Multimedia","","2022","PP","99","1","13","Generative adversarial networks (GANs) have demonstrated superior performances in image generation. In recent years, various improvements of network structure and learning theory related to GANs have undergone numerous advancement. Among these improvement techniques, the asymmetric training on the generator and discriminator networks has been widely adopted. For example, the batch normalization is used in generator while the spectral normalization is used in discriminator, or using different learning rates for the generator and discriminator. However, the asymmetric training on the real and generated samples has not been taken into consideration till now. In this paper, we proposed a novel asymmetric training-based RealnessGAN (ATRGAN) which applies the idea of asymmetric training on both samples and networks. Specifically, the asymmetric training on samples refers to performing the differential learning on the real and generated samples by controlling the information entropies of real and fake anchor distributions. The asymmetric training on networks is realized via the sampling transmission $G2D$, which abandons the commonly used independent random sampling. With the help of $G2D$, the discriminator can obtain a dominant training position than the generator, so as to ensure that the discriminator can guide the generator more effectively during training. In addition, we proposed the floating anchor distribution technique and constructed the objective function of generator for ATRGAN. Through comparative experiments, we demonstrated ATRGAN's ability of achieving better generation performance than various SOTA GANs on CIFAR-10, CAT, and CelebA-HQ datasets.","1941-0077","","10.1109/TMM.2022.3233307","National Natural Science Foundation of China(grant numbers:62176259,61976215); Key Research and Development Program of Jiangsu Province(grant numbers:BE2022095); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10003660","RealnessGAN;asymmetric training;information entropy;sampling transmission;floating anchor distribution","Training;Generators;Linear programming;Generative adversarial networks;Painting;Image synthesis;Task analysis","","","","","","","IEEE","30 Dec 2022","","","IEEE","IEEE Early Access Articles"
"Is Deep Diffusion Probabilistic Model Applicable for Fingerprint-based Indoor Localization?","D. J. Suroso; P. Sooraksa; P. Cherntanomwong","School of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, Thailand; School of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, Thailand; School of Engineering, King Mongkut’s Institute of Technology Ladkrabang, Bangkok, Thailand","2022 26th International Computer Science and Engineering Conference (ICSEC)","24 Feb 2023","2022","","","202","207","The latest deep learning (DL) phenomenon is the Denoising Diffusion Model (DDM). DDM is in a class of latent variable models of the deep generative model (DGM) along with the big name of Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). Moreover, in a recent finding, DDM beats GANs in image synthesis. This paper presents the prospective applicability discussion of DDM for indoor localization research as previous models, e.g., GANs and VAEs, which are successfully implemented. Here, we focus more on how DDM can synthesize localization parameters with the help of fingerprinting technique's database enhancement. The fingerprint technique needs a preconstructed database which has the main drawbacks of its cost, time inefficient, and high complexity. We found valuable works of literature on this specific topic for GANs and VAEs. However, there are few DDM applications for discrete data types, and as the authors' concern, there is no attempt to apply them to indoor localization yet. DDM implementation is to generate continuous data domains, e.g., image, text, and audio data. A radio map or fingerprint database is essentially needed for fingerprint-based indoor localization. Learning this database pattern helps increase the system's performance. Obtaining a high-density and quality database is expensive and challenging to implement. Then, it raises a question, is DDM applicable for synthesizing this database and alleviating this problem?","","978-1-6654-9198-3","10.1109/ICSEC56337.2022.10049366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049366","deep generative model;indoor localization;denoising diffusion;variational autoencoders;generative adversarial networks","Location awareness;Deep learning;Databases;Image synthesis;Computational modeling;System performance;Noise reduction","","","","","","28","IEEE","24 Feb 2023","","","IEEE","IEEE Conferences"
"Multi-Channel Attention Selection GANs for Guided Image-to-Image Translation","H. Tang; P. H. S. Torr; N. Sebe","Department of Information Technology and Electrical Engineering, ETH Zurich, Zurich, Switzerland; Department of Engineering Science, University of Oxford, Oxford, United Kingdom; Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, Italy","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2022","PP","99","1","16","We propose a novel model named Multi-Channel Attention Selection Generative Adversarial Network (SelectionGAN) for guided image-to-image translation, where we translate an input image into another while respecting an external semantic guidance. The proposed SelectionGAN explicitly utilizes the semantic guidance information and consists of two stages. In the first stage, the input image and the conditional semantic guidance are fed into a cycled semantic-guided generation network to produce initial coarse results. In the second stage, we refine the initial results by using the proposed multi-scale spatial pooling & channel selection module and the multi-channel attention selection module. Moreover, uncertainty maps automatically learned from attention maps are used to guide the pixel loss for better network optimization. Exhaustive experiments on four challenging guided image-to-image translation tasks (face, hand, body, and street view) demonstrate that our SelectionGAN is able to generate significantly better results than the state-of-the-art methods. Meanwhile, the proposed framework and modules are unified solutions and can be applied to solve other generation tasks such as semantic image synthesis. The code is available at https://github.com/Ha0Tang/SelectionGAN.","1939-3539","","10.1109/TPAMI.2022.3212915","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913676","Cascade generation;deep attention selection;GANs;guided image-to-image translation","Semantics;Task analysis;Image synthesis;Skeleton;Generative adversarial networks;Image segmentation;Uncertainty","","","","","","","IEEE","10 Oct 2022","","","IEEE","IEEE Early Access Articles"
"Learning to synthesize faces using voice clips for Cross-Modal biometric matching","P. Agarwal; S. Poddar; A. Hazarika; H. Rahaman","Electronics and Communication IIIT Guwahati, Guwahati, India; Electronics and Communication IIIT Guwahati, Guwahati, India; Electronics and Communication IIIT Guwahati, Guwahati, India; Information Technology IIEST Shibpur, Howrah, India","2019 IEEE Region 10 Symposium (TENSYMP)","30 Jan 2020","2019","","","397","402","Cross-Modal biometric matching has been a scarcely explored field but carries several important applications and aims to further secure the currently existing security systems. In this paper, a framework for cross-modal biometric matching is presented, where faces of an individual are generated using his/her voice clips and further the synthesized faces are tested using a face classification network. Generative Adversarial Network (GAN) has become a recent trend in deep learning and has been widely used for image synthesis. We explore the advancements of Convolutional Neural Network (CNN) for feature extraction and generative networks for image synthesis. In the experiment, we compare the performance of Variational Autoencoders(VAE), Conditional Generative Adversarial Networks(C-GAN) and Regularized Conditional Generative Adversarial Networks(RC-GAN) and show that RC-GAN that is C-GAN with a regularization factor added to its loss is able to generate faces corresponding to the true identity of the voice clips with the best accuracy of 84.52% while VAE generates a less noise prone image with the highest PSNR of 28.276 decibels but with an accuracy of 72.61%.","2642-6102","978-1-7281-0297-9","10.1109/TENSYMP46218.2019.8971330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8971330","Biometric matching;Deep Learning;Convolutional Neural Network;Generative Adversarial Network","","biometrics (access control);convolutional neural nets;face recognition;feature extraction;image classification;image matching;learning (artificial intelligence);speech recognition","voice clips;cross-modal biometric matching;face classification network;deep learning;image synthesis;convolutional neural network;regularized conditional generative adversarial networks;RC-GAN;VAE;variational autoencoders;CNN;feature extraction","","1","","28","IEEE","30 Jan 2020","","","IEEE","IEEE Conferences"
"SAR image synthesis with GAN and continuous aspect angle and class constraints","Y. Giry-Fouquet; A. Baussard; C. Enderli; T. Porges",NA; NA; NA; NA,"EUSAR 2022; 14th European Conference on Synthetic Aperture Radar","10 Nov 2022","2022","","","1","6","Target classification generally requires large databases, especially for deep learning methods. However, it is not always possible to have access to a database of sufficient size for certain imaging modalities. For example, in synthetic aperture radar (SAR) imaging only limited incidence angles and aspect angles can be available. Unfortunately, to overcome this problem, most of the classical data augmentation methods are inappropriate for SAR data. Thus, in a previous work, we evaluated conditional Generative Adversarial Networks to generate synthetic SAR images at given aspect angles and for specific target classes. Among the various models evaluated the so-called StyleGAN2-ada, slightly modified to take into account the specificity of SAR images, appear to be the most efficient model. However, we observed that some of the generated images had wrong aspect angles. In this contribution we propose to correct this problem by adding a regularization term recently proposed in a model called Generator Regularized-cGAN. Our experiments show that this modification strongly reduce the problem.","","978-3-8007-5823-4","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944365","","","","","","","","","","10 Nov 2022","","","VDE","VDE Conferences"
"R²GAN: Cross-Modal Recipe Retrieval With Generative Adversarial Network","B. Zhu; C. -W. Ngo; J. Chen; Y. Hao",City University of Hong Kong; City University of Hong Kong; National University of Singapore; City University of Hong Kong,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","11469","11478","Representing procedure text such as recipe for crossmodal retrieval is inherently a difficult problem, not mentioning to generate image from recipe for visualization. This paper studies a new version of GAN, named Recipe Retrieval Generative Adversarial Network (R2GAN), to explore the feasibility of generating image from procedure text for retrieval problem. The motivation of using GAN is twofold: learning compatible cross-modal features in an adversarial way, and explanation of search results by showing the images generated from recipes. The novelty of R2GAN comes from architecture design, specifically a GAN with one generator and dual discriminators is used, which makes the generation of image from recipe a feasible idea. Furthermore, empowered by the generated images, a two-level ranking loss in both embedding and image spaces are considered. These add-ons not only result in excellent retrieval performance, but also generate close-to-realistic food images useful for explaining ranking of recipes. On recipe1M dataset, R2GAN demonstrates high scalability to data size, outperforms all the existing approaches, and generates images intuitive for human to interpret the search results.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953293","Recognition: Detection;Categorization;Retrieval;Image and Video Synthesis;Representation Learning;Vision + Language","Visualization;Image resolution;Image synthesis;Scalability;Network architecture;Generative adversarial networks;Robustness","feature extraction;information retrieval;learning (artificial intelligence);neural nets;text analysis","recipe1M dataset;R2GAN;2 GAN;procedure text;crossmodal retrieval;image generation;retrieval problem;cross-modal features;close-to-realistic food images;recipe retrieval generative adversarial network","","59","","41","IEEE","9 Jan 2020","","","IEEE","IEEE Conferences"
"GAN Prior Embedded Network for Blind Face Restoration in the Wild","T. Yang; P. Ren; X. Xie; L. Zhang","DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; DAMO Academy, Alibaba Group; Department of Computing, The Hong Kong Polytechnic University","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","672","681","Blind face restoration (BFR) from severely degraded face images in the wild is a very challenging problem. Due to the high illness of the problem and the complex unknown degradation, directly training a deep neural network (DNN) usually cannot lead to acceptable results. Existing generative adversarial network (GAN) based methods can produce better results but tend to generate over-smoothed restorations. In this work, we propose a new method by first learning a GAN for high-quality face image generation and embedding it into a U-shaped DNN as a prior decoder, then fine-tuning the GAN prior embedded DNN with a set of synthesized low-quality face images. The GAN blocks are designed to ensure that the latent code and noise input to the GAN can be respectively generated from the deep and shallow features of the DNN, controlling the global face structure, local face details and background of the reconstructed image. The proposed GAN prior embedded network (GPEN) is easy-to-implement, and it can generate visually photo-realistic results. Our experiments demonstrated that the proposed GPEN achieves significantly superior results to state-of-the-art BFR methods both quantitatively and qualitatively, especially for the restoration of severely degraded face images in the wild. The source code and models can be found at https://github.com/yangxy/GPEN.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00073","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578247","","Training;Degradation;Deep learning;Codes;Image synthesis;Face recognition;Generative adversarial networks","deep learning (artificial intelligence);face recognition;image restoration","embedded network;blind face restoration;severely degraded face images;complex unknown degradation;deep neural network;generative adversarial network based methods;high-quality face image generation;DNN;prior decoder;low-quality face images;GAN blocks;local face details;reconstructed image;BFR methods","","40","","53","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Joint Generative and Contrastive Learning for Unsupervised Person Re-identification","H. Chen; Y. Wang; B. Lagadec; A. Dantcheva; F. Bremond",European Systems Integration; Université Côte d’Azur; European Systems Integration; Université Côte d’Azur; Université Côte d’Azur,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","2004","2013","Recent self-supervised contrastive learning provides an effective approach for unsupervised person re-identification (ReID) by learning invariance from different views (transformed versions) of an input. In this paper, we incorporate a Generative Adversarial Network (GAN) and a contrastive learning module into one joint training framework. While the GAN provides online data augmentation for contrastive learning, the contrastive module learns view-invariant features for generation. In this context, we propose a mesh-based view generator. Specifically, mesh projections serve as references towards generating novel views of a person. In addition, we propose a view-invariant loss to facilitate contrastive learning between original and generated views. Deviating from previous GAN-based unsupervised ReID methods involving domain adaptation, we do not rely on a labeled source dataset, which makes our method more flexible. Extensive experimental results show that our method significantly outperforms state-of-the-art methods under both, fully unsupervised and unsupervised domain adaptive settings on several large scale ReID dat-sets. Source code and models are available under https://github.com/chenhao2345/GCL.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577603","","Training;Computer vision;Adaptation models;Three-dimensional displays;Codes;Image synthesis;Generative adversarial networks","object detection;unsupervised learning","unsupervised person re-identification;self-supervised contrastive learning;generative adversarial network;contrastive learning module;joint training framework;contrastive module;view-invariant features;mesh-based view generator;view-invariant loss;fully unsupervised domain adaptive settings;joint generative and contrastive learning;GAN-based unsupervised ReID methods","","33","","54","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Grayscale Enhancement Colorization Network for Visible-Infrared Person Re-Identification","X. Zhong; T. Lu; W. Huang; M. Ye; X. Jia; C. -W. Lin","Hubei Key Laboratory of Transportation Internet of Things, Wuhan University of Technology, Wuhan, China; School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; School of Computer Science and Information Engineering, Hubei University, Wuhan, China; School of Computer Science, Wuhan University, Wuhan, China; School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China; Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Circuits and Systems for Video Technology","8 Mar 2022","2022","32","3","1418","1430","Visible-infrared person re-identification (VI-ReID) is an emerging and challenging cross-modality image matching problem because of the explosive surveillance data in night-time surveillance applications. To handle the large modality gap, various generative adversarial network models have been developed to eliminate the cross-modality variations based on a cross-modal image generation framework. However, the lack of point-wise cross-modality ground-truths makes it extremely challenging to learn such a cross-modal image generator. To address these problems, we learn the correspondence between single-channel infrared images and three-channel visible images by generating intermediate grayscale images as auxiliary information to colorize the single-modality infrared images. We propose a grayscale enhancement colorization network (GECNet) to bridge the modality gap by retaining the structure of the colored image which contains rich information. To simulate the infrared-to-visible transformation, the point-wise transformed grayscale images greatly enhance the colorization process. Our experiments conducted on two visible-infrared cross-modality person re-identification datasets demonstrate the superiority of the proposed method over the state-of-the-arts.","1558-2205","","10.1109/TCSVT.2021.3072171","Department of Science and Technology, Hubei Provincial People’s Government(grant numbers:2017CFA012); Fundamental Research Funds for the Central Universities of China(grant numbers:191010001); Hubei Key Laboratory of Transportation Internet of Things(grant numbers:2018IOT003,2020III026GX); National Natural Science Foundation of China(grant numbers:62066021); Ministry of Science and Technology, Taiwan(grant numbers:MOST 109-2634-F-007-013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399508","Person re-identification;visible-infrared;colorization;cross-modality;grayscale enhancement","Gray-scale;Image color analysis;Image synthesis;Generative adversarial networks;Training;Gallium nitride;Feature extraction","feature extraction;gradient methods;image classification;image colour analysis;image enhancement;image matching;image segmentation;object detection;surveillance","VI-ReID;night-time surveillance applications;modality gap;generative adversarial network models;cross-modality variations;cross-modal image generation framework;point-wise cross-modality ground-truths;cross-modal image generator;single-channel infrared images;three-channel visible images;intermediate grayscale images;single-modality infrared images;grayscale enhancement colorization network;colored image;infrared-to-visible transformation;point-wise transformed grayscale images;visible-infrared cross-modality person re-identification datasets;visible-infrared person re-identification;cross-modality image matching problem;GECNet","","25","","67","IEEE","9 Apr 2021","","","IEEE","IEEE Journals"
"E2I: Generative Inpainting From Edge to Image","S. Xu; D. Liu; Z. Xiong","CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China","IEEE Transactions on Circuits and Systems for Video Technology","2 Apr 2021","2021","31","4","1308","1322","Deep learning-based methods especially using convolutional neural network (CNN) and generative adversarial network (GAN) have achieved certain success for the task of image inpainting. The previous methods usually try to generate the content in the missing areas from scratch. However, these methods have difficulty in producing salient image structures that appear natural and consistent with the neighborhood, especially when the missing area is large. In this paper, we address the challenge by introducing edges into the convolutional GAN-based inpainting. We split the inpainting task into two steps: first edge generation, then edge-based image generation. We adopt CNN to accomplish the two steps and use GAN-based training, thus our method is named E2I: generative inpainting from edge to image. Specifically, we adopt a deep network-based edge detector to achieve an edgeness map of an incomplete image, then we fill-in the missing areas in the edgeness map, and finally generate the missing pixels with the assistance of the complete edgeness map. We verify the proposed method on three challenging image datasets: Places2, ImageNet, and CelebA. We also compare our method with the state-of-the-arts on the Places2 test set. Our experimental results demonstrate the superior performance of our method in producing more plausible inpainting results.","1558-2205","","10.1109/TCSVT.2020.3001267","National Key Research and Development Program of China(grant numbers:2018YFA0701603); Natural Science Foundation of China(grant numbers:61772483,61931014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9113276","Convolutional neural network (CNN);edge inpainting;generative adversarial network (GAN);image inpainting","Image edge detection;Gallium nitride;Generative adversarial networks;Task analysis;Feature extraction;Image synthesis;Training","edge detection;image classification;image restoration;learning (artificial intelligence);neural nets","image generation;edge generation;inpainting task;convolutional GAN-based inpainting;salient image structures;image inpainting;generative adversarial network;deep learning-based methods;plausible inpainting results;Places2 test set;challenging image datasets;complete edgeness map;missing pixels;missing area;incomplete image;deep network-based edge detector;generative inpainting;E2I;GAN-based training;CNN","","19","","52","IEEE","10 Jun 2020","","","IEEE","IEEE Journals"
"Repurposing GANs for One-shot Semantic Part Segmentation","N. Tritrong; P. Rewatbowornwong; S. Suwajanakorn","VISTEC, Thailand; VISTEC, Thailand; VISTEC, Thailand","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","4473","4483","While GANs have shown success in realistic image generation, the idea of using GANs for other tasks unrelated to synthesis is underexplored. Do GANs learn meaningful structural parts of objects during their attempt to reproduce those objects? In this work, we test this hypothesis and propose a simple and effective approach based on GANs for semantic part segmentation that requires as few as one label example along with an unlabeled dataset. Our key idea is to leverage a trained GAN to extract a pixel-wise representation from the input image and use it as feature vectors for a segmentation network. Our experiments demonstrate that this GAN-derived representation is ""readily discriminative"" and produces surprisingly good results that are comparable to those from supervised baselines trained with significantly more labels. We believe this novel repurposing of GANs underlies a new class of unsupervised representation learning, which can generalize to many other tasks. More results are available at https://RepurposeGANs.github.io/.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00445","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578165","","Image segmentation;Image synthesis;Semantics;Transfer learning;Pipelines;Feature extraction;Generative adversarial networks","image representation;image segmentation;neural nets;unsupervised learning;vectors","GAN;one-shot semantic part segmentation;realistic image generation;GAN-derived representation;feature vectors;unsupervised representation learning;pixel-wise representation","","17","","58","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Remote Sensing Data Augmentation Through Adversarial Training","N. Lv; H. Ma; C. Chen; Q. Pei; Y. Zhou; F. Xiao; J. Li","State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Ministry of Water Resources of China, Beijing, China; Ministry of Water Resources of China, Beijing, China; Ministry of Water Resources of China, Beijing, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","24 Sep 2021","2021","14","","9318","9333","The lack of remote sensing images and poor quality limit the performance improvement of follow-up research such as remote sensing interpretation. In this article, a generative adversarial network (GAN) is proposed for data augmentation of remote sensing images abstracted from Jiangxi and Anhui Provinces in China, i.e., deeply supervised GAN (D-sGAN). D-sGAN can generate high-quality images that are rich in changes, greatly shorten the generation time, and provide data support for applications such as semantic interpretation of remote sensing images. First, to modulate the layer activations, a downsampling scheme is designed based on the segmentation map. Then, the architecture of the generator is Unet++ with the proposed downsampling module. Next, the generator of this net is deeply supervised by the discriminator using deep convolutional neural network. This article further proved that the proposed downsampling module and the dense connection characteristics of UNet++ are significantly beneficial to the retention of semantic information of remote sensing images. Numerical results demonstrated that the images generated by D-sGAN could be used to improve accuracy of the segmentation network, with the faster generation speed compared to the CoGAN, SimGAN, and CycleGAN models. Furthermore, the remote sensing data generated by the model helped the interpretation network to increase the accuracy by 9%, meeting actual generation requirements.","2151-1535","","10.1109/JSTARS.2021.3110842","National Key R&D Program of China(grant numbers:2020YFB1807500); National Natural Science Foundation of China(grant numbers:62072360,62001357,61672131,61901367); Key Research and Development Plan of Shaanxi province(grant numbers:2021ZDLGY02-09,2020JQ-844); Key Laboratory of Embedded System and Service Computing(grant numbers:Tongji University,ESSCKF2019-05); Ministry of Education, Xi'an Science and Technology Plan(grant numbers:20RGZN0005); Xi'an Key Laboratory of Mobile Edge Computing and Security(grant numbers:201805052-ZD3CG36); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9531488","Data augmentation;deep supervision;downsampling;GAN","Remote sensing;Generative adversarial networks;Generators;Semantics;Training;Task analysis;Image synthesis","convolutional neural nets;deep learning (artificial intelligence);geophysical image processing;image segmentation;learning (artificial intelligence);remote sensing","remote sensing images;remote sensing data;sensing data augmentation;remote sensing interpretation;generative adversarial network;high-quality images;downsampling module;adversarial training;Jiangxi provinces;Anhui provinces;China;deeply supervised GAN;D-sGAN;Unet++;deep convolutional neural network;CoGAN;SimGAN;CycleGAN models;interpretation network;generation requirements","","15","","47","CCBY","8 Sep 2021","","","IEEE","IEEE Journals"
"Looking back at Labels: A Class based Domain Adaptation Technique","V. K. Kurmi; V. P. Namboodiri",Indian Institute of Technology Kanpur; Indian Institute of Technology Kanpur,"2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","8","In this paper, we solve the problem of adapting classifiers across domains. We consider the problem of domain adaptation for multi-class classification where we are provided a labeled set of examples in a source dataset and we are provided a target dataset with no supervision. In this setting, we propose an adversarial discriminator based approach. While the approach based on adversarial discriminator has been previously proposed; in this paper, we present an informed adversarial discriminator. Our observation relies on the analysis that shows that if the discriminator has access to all the information available including the class structure present in the source dataset, then it can guide the transformation of features of the target set of classes to a more structure adapted space. Using this formulation, we obtain state-of-the-art results for the standard evaluation on benchmark datasets. We further provide detailed analysis which shows that using all the labeled information results in an improved domain adaptation.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852199","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852199","","Feature extraction;Adaptation models;Task analysis;Neural networks;Generative adversarial networks;Deep learning;Image synthesis","pattern classification","class based domain adaptation technique;multiclass classification;source dataset;target dataset;adversarial discriminator based approach;informed adversarial discriminator;benchmark datasets;improved domain adaptation;labeled information;class structure","","14","","61","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"LoFGAN: Fusing Local Representations for Few-shot Image Generation","Z. Gu; W. Li; J. Huo; L. Wang; Y. Gao","State Key Laboratory for Novel Software Technology, Nanjing University; State Key Laboratory for Novel Software Technology, Nanjing University; State Key Laboratory for Novel Software Technology, Nanjing University; School of Computing and Information Technology, University of Wollongong; State Key Laboratory for Novel Software Technology, Nanjing University","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","8443","8451","Given only a few available images for a novel unseen category, few-shot image generation aims to generate more data for this category. Previous works attempt to globally fuse these images by using adjustable weighted coefficients. However, there is a serious semantic misalignment between different images from a global perspective, making these works suffer from poor generation quality and diversity. To tackle this problem, we propose a novel Local-Fusion Generative Adversarial Network (LoFGAN) for fewshot image generation. Instead of using these available images as a whole, we first randomly divide them into a base image and several reference images. Next, LoFGAN matches local representations between the base and reference images based on semantic similarities, and replaces the local features with the closest related local features. In this way, LoFGAN can produce more realistic and diverse images at a more fine-grained level, and simultaneously enjoy the characteristic of semantic alignment. Furthermore, a local reconstruction loss is also proposed, which can provide better training stability and generation quality. We conduct extensive experiments on three datasets, which successfully demonstrates the effectiveness of our proposed method for few-shot image generation and downstream visual applications with limited data. Code is available at https://github.com/edward3862/LoFGAN-pytorch.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00835","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710556","Transfer/Low-shot/Semi/Unsupervised Learning","Training;Visualization;Computer vision;Image synthesis;Fuses;Semantics;Generative adversarial networks","","","","9","","26","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Naturalistic Physical Adversarial Patch for Object Detectors","Y. -C. -T. Hu; J. -C. Chen; B. -H. Kung; K. -L. Hua; D. S. Tan","National Taiwan University of Science and Technology; Research Center for Information Technology Innovation, Academia Sinica; Research Center for Information Technology Innovation, Academia Sinica; National Taiwan University of Science and Technology; Research Center for Information Technology Innovation, Academia Sinica","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","7828","7837","Most prior works on physical adversarial attacks mainly focus on the attack performance but seldom enforce any restrictions over the appearance of the generated adversarial patches. This leads to conspicuous and attention-grabbing patterns for the generated patches which can be easily identified by humans. To address this issue, we pro-pose a method to craft physical adversarial patches for object detectors by leveraging the learned image manifold of a pretrained generative adversarial network (GAN) (e.g., BigGAN and StyleGAN) upon real-world images. Through sampling the optimal image from the GAN, our method can generate natural looking adversarial patches while maintaining high attack performance. With extensive experiments on both digital and physical domains and several independent subjective surveys, the results show that our proposed method produces significantly more realistic and natural looking patches than several state-of-the-art base-lines while achieving competitive attack performance. 1","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00775","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710707","Adversarial learning;Detection and localization in 2D and 3D","Manifolds;Computer vision;Image synthesis;Computational modeling;Detectors;Generative adversarial networks;Quality assessment","","","","7","","50","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Inpainting-Based Virtual Try-on Network for Selective Garment Transfer","L. Yu; Y. Zhong; X. Wang","College of Textiles, Donghua University, Shanghai, China; Key Laboratory of Textile Science and Technology, Ministry of Education, College of Textiles, Donghua University, Shanghai, China; College of Textiles, Donghua University, Shanghai, China","IEEE Access","25 Sep 2019","2019","7","","134125","134136","Image-based garment transfer systems aim to swap the desired clothes from a model to arbitrary users. However, existing works cannot provide the capacity for users to try on various fashion articles according to their wishes, i.e., users can decide which article (e.g., tops, pants or both) to be swapped. In this paper, we propose an Inpainting-based Virtual Try-On Network (I-VTON) which allows the user to try on arbitrary clothes from the model image in a selective manner. To realize the selectivity, we reshape the virtual try-on as a task of image inpainting. Firstly, the texture from the garment and the user are extracted respectively to form a coarse result. In this phase, users can decide which clothes they hope to try on via an interactive texture control mechanism. Secondly, the missing regions in the coarse result are recovered via a Texture Inpainting Network (TIN). We introduce a triplet training strategy to ensure the naturalness of the final result. Qualitative and quantitative experimental results demonstrate that I-VTON outperforms the state-of-the-art methods on both the garment details and the user identity. It is also confirmed our approach can flexibly transfer the clothes in a selective manner.","2169-3536","","10.1109/ACCESS.2019.2941378","National Natural Science Foundation of China(grant numbers:61572124); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8836494","Generative adversarial network;self-supervised learning;texture inpainting;virtual try-on","Clothing;Task analysis;Training;Gallium nitride;Skin;Generative adversarial networks;Image synthesis","clothing;image restoration;image texture;production engineering computing","quantitative experimental results;I-VTON;garment details;user identity;selective garment transfer;garment transfer systems;desired clothes;arbitrary users;fashion articles;wishes;tops;pants;Inpainting-based Virtual;arbitrary clothes;image inpainting;interactive texture control mechanism;Texture Inpainting Network;triplet training strategy","","7","","52","CCBY","13 Sep 2019","","","IEEE","IEEE Journals"
"TH-GAN: Generative Adversarial Network Based Transfer Learning for Historical Chinese Character Recognition","J. Cai; L. Peng; Y. Tang; C. Liu; P. Li","Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Department of Electronic Engineering, Tsinghua University, Beijing, China; Beijing Institute of Electronics Technology and Application, Beijing, China","2019 International Conference on Document Analysis and Recognition (ICDAR)","3 Feb 2020","2019","","","178","183","Historical Chinese character recognition faces problems including low image quality and lack of labeled training samples. We propose a generative adversarial network (GAN) based transfer learning method to ease these problems. The proposed TH-GAN architecture includes a discriminator and a generator. The network structure of the discriminator is based on a convolutional neural network (CNN). Inspired by Wasserstein GAN, the loss function of the discriminator aims to measure the probabilistic distribution distance of the generated images and the target images. The network structure of the generator is a CNN based encoder-decoder. The loss function of the generator aims to minimize the distribution distance between the real samples and the generated samples. In order to preserve the complex glyph structure of a historical Chinese character, a weighted mean squared error (MSE) criterion by incorporating both the edge and the skeleton information in the ground truth image is proposed as the weighted pixel loss in the generator. These loss functions are used for joint training of the discriminator and the generator. Experiments are conducted on two tasks to evaluate the performance of the proposed TH-GAN. The first task is carried out on style transfer mapping for multi-font printed traditional Chinese character samples. The second task is carried out on transfer learning for historical Chinese character samples by adding samples generated by TH-GAN. Experimental results show that the proposed TH-GAN is effective.","2379-2140","978-1-7281-3014-9","10.1109/ICDAR.2019.00037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8978106","Generative Adversarial Network;Transfer Learning;Historical Chinese Character Recognition","Generative adversarial networks;Task analysis;Generators;Gallium nitride;Training;Character recognition;Image synthesis","convolutional neural nets;document image processing;feature extraction;Gaussian processes;image classification;learning (artificial intelligence);mean square error methods;natural language processing;optical character recognition;statistical distributions","Historical Chinese character recognition faces problems;low image quality;labeled training samples;generative adversarial network based transfer learning method;TH-GAN architecture;network structure;convolutional neural network;Wasserstein GAN;loss function;probabilistic distribution distance;target images;CNN based encoder-decoder;complex glyph structure;weighted mean squared error criterion;ground truth image;weighted pixel loss;style transfer mapping;traditional Chinese character samples;historical Chinese character samples","","6","1","19","IEEE","3 Feb 2020","","","IEEE","IEEE Conferences"
"Self-Supervised Animation Synthesis Through Adversarial Training","C. Yu; W. Wang; J. Yan","Chongqing College of Electronic Engineering, Chongqing, China; International Institute of Next Generation Internet, Macau University of Science and Technology, Taipa, China; Faculty of Information Technology, Macau University of Science and Technology, Taipa, China","IEEE Access","21 Jul 2020","2020","8","","128140","128151","In this paper, we propose a novel deep generative model for image animation synthesis. Based on self-supervised learning and adversarial training, the model can find labeling rules and mark them without origin sample labels. In addition, our model can generate continuous changing images based on the automatically labels learning. The labels learning model can be implemented on a large number of out-of-order samples to generate two types of pseudo-labels, discrete labels and continuous labels. The discrete labels can generate different animation clips, and the continuous labels can generate different frames in the same clip. Embedding pseudo-labels with latent variables into latent space, our model discovers regularities and features from latent space. Animation features are fully characterized by the pseudo-labels learned from the self-supervised module. Using upgraded adversarial training steps, the model learns to map animation features to pseudo-labels from the latent space and then organizes pseudo-labels embedding into latent variables to generate animation features. By adapting dimensions of pseudo-labels, we match fine features with latent variables. Such as using the two types of pseudo-labels, our model can also generate different styles of videos from the same dataset. The specific implementation tricks depend on the different pseudo-label dimensions and the number of pseudo-label dimensions. Comparing the results of our model with other state-of-the-art approaches, the model does not use complicated components, such as 3D convolution layers and recurrent neural networks. Our experimental results show that an appropriate number of the pseudo-label dimensions can better characterize animation features. In this case, an animation which reached human-level perception can be synthesized. The performance of animation synthesis has reached relatively superior results on several challenging datasets.","2169-3536","","10.1109/ACCESS.2020.3008523","Science and Technology Development Fund (FDCT) of Macau(grant numbers:0016/2019/A1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139251","Self-supervised learning;animation synthesis;pseudo-label;adversarial training","Animation;Training;Videos;Generative adversarial networks;Task analysis;Image synthesis;Gallium nitride","computer animation;learning (artificial intelligence);recurrent neural nets","labeling rules;origin sample labels;discrete labels;continuous labels;animation clips;embedding pseudolabels;latent variables;latent space;map animation features;organizes pseudolabels;self-supervised animation synthesis;adversarial training;deep generative model;image animation synthesis;pseudolabel dimensions","","4","","50","CCBY","13 Jul 2020","","","IEEE","IEEE Journals"
"Four Discriminator Cycle-Consistent Adversarial Network for Improving Railway Defective Fastener Inspection","J. Liu; Z. Ma; Y. Qiu; X. Ni; B. Shi; H. Liu","College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China; College of Electrical and Information Engineering, Hunan University, Changsha, China","IEEE Transactions on Intelligent Transportation Systems","10 Aug 2022","2022","23","8","10636","10645","This article aims to improve the performance of deep learning-based defective fastener inspection method. Due to the defective fasteners are insufficient and far less than the defect-free ones in real railway, it is difficult to train a robust fastener inspection model on such imbalanced dataset. In view of this problem, a novel image generation method called four-discriminator cycle-consistent adversarial network (FD-Cycle-GAN) is proposed to generate the defect fastener images using a large number of defect-free ones. Extensive experiments are conducted on the real fastener images and generated images. Experimental results demonstrate that the defect fastener images generated by our proposed method have better quality and richer diversity than those generated by other state-of-the-art methods. In addition, compared with the CNN-only baseline, the performance of the fastener inspection model trained on the expanded dataset containing the defect fastener images generated by FD-Cycle-GAN is improved significantly. The detection accuracy and relative IMP reach 93.25% and 21.59% respectively.","1558-0016","","10.1109/TITS.2021.3095167","National Natural Science Foundation of China(grant numbers:61771191,61971182); Hunan Provincial Natural Science Foundation of China(grant numbers:2020JJ4213); Changsha City Science and Technology Department Funds(grant numbers:KQ2004007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9484674","Fastener inspection;image generation;GAN;classification model;deep learning","Fasteners;Inspection;Generative adversarial networks;Image synthesis;Feature extraction;Solid modeling;Rail transportation","computer vision;fasteners;feature extraction;image recognition;inspection;learning (artificial intelligence);pattern classification;railway engineering;railways;statistical analysis","improving railway defective fastener inspection;deep learning-based defective fastener inspection method;defective fasteners;defect-free ones;robust fastener inspection model;image generation method;four-discriminator cycle-consistent adversarial network;FD-Cycle-GAN;defect fastener images","","4","","30","IEEE","14 Jul 2021","","","IEEE","IEEE Journals"
"Local and Global GANs With Semantic-Aware Upsampling for Image Generation","H. Tang; L. Shao; P. H. S. Torr; N. Sebe","Department of Information Technology and Electrical Engineering, ETH Zurich, Zurich, Switzerland; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; Department of Engineering Science, University of Oxford, Oxford, UK; Department of Information Engineering and Computer Science (DISI), University of Trento, Trento, TN, Italy","IEEE Transactions on Pattern Analysis and Machine Intelligence","5 Dec 2022","2023","45","1","768","784","In this paper, we address the task of semantic-guided image generation. One challenge common to most existing image-level generation methods is the difficulty in generating small objects and detailed local textures. To address this, in this work we consider generating images using local context. As such, we design a local class-specific generative network using semantic maps as guidance, which separately constructs and learns subgenerators for different classes, enabling it to capture finer details. To learn more discriminative class-specific feature representations for the local generation, we also propose a novel classification module. To combine the advantages of both global image-level and local class-specific generation, a joint generation network is designed with an attention fusion module and a dual-discriminator structure embedded. Lastly, we propose a novel semantic-aware upsampling method, which has a larger receptive field and can take far-away pixels that are semantically related for feature upsampling, enabling it to better preserve semantic consistency for instances with the same semantic labels. Extensive experiments on two image generation tasks show the superior performance of the proposed method. State-of-the-art results are established by large margins on both tasks and on nine challenging public benchmarks. The source code and trained models are available at https://github.com/Ha0Tang/LGGAN.","1939-3539","","10.1109/TPAMI.2022.3155989","EU H2020 AI4Media(grant numbers:951911); PRIN(grant numbers:2020ZSL9F9); National Natural Science Foundation of China(grant numbers:61929104); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732069","GANs;local and global;feature upsampling;semantic-guided;image generation","Semantics;Image synthesis;Task analysis;Generators;Kernel;Generative adversarial networks;Interpolation","image classification;image representation;image texture;learning (artificial intelligence);visual databases","attention fusion module;class-specific generation;detailed local textures;discriminative class-specific feature representations;dual-discriminator structure;feature upsampling;finer details;global GANs;global image-level;image generation tasks;image-level generation methods;joint generation network;local class-specific generative network;local GANs;local generation;preserve semantic consistency;semantic labels;semantic maps;semantic-aware upsampling method;semantic-guided image generation","","3","","65","IEEE","9 Mar 2022","","","IEEE","IEEE Journals"
"A Unified Framework for Bidirectional Prototype Learning From Contaminated Faces Across Heterogeneous Domains","M. Pang; B. Wang; S. Huang; Y. -M. Cheung; B. Wen","School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; John A. Paulson School of Engineering and Applied Sciences, Harvard University, Boston, MA, USA; Department of Computer Science, Hong Kong Baptist University, Hong Kong, China; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore, Singapore","IEEE Transactions on Information Forensics and Security","28 Apr 2022","2022","17","","1544","1557","Existing heterogeneous face synthesis (HFS) methods focus on performing accurate image-to-image translation across domains, while they cannot effectively remove the nuisance facial variations such as poses, expressions or occlusions. To address such challenges, this paper studies a new practical heterogeneous prototype learning (HPL) problem. To be specific, given a face image contaminated by facial variations from a source domain, HPL aims to reconstruct the variation-free prototype in a specified target domain. To tackle HPL, we propose a unified and end-to-end framework named bidirectional heterogeneous prototype learning (BHPL). As a bidirectional learning framework, BHPL is able to simultaneously reconstruct the heterogeneous prototypes across source-to-target as well as target-to-source domains. Furthermore, BHPL is capable of learning the identity prototype features for the contaminated face images from both source and target domains in order to perform robust heterogeneous face recognition. BHPL consists of an encoder-decoder structural generator and two dual-task discriminators, which play an adversarial game such that the generator learns the identity prototype feature and generates the cross-domain identity-preserved prototype for each input face image from both domains, and the discriminators accurately predict face identity and distinguish real versus fake prototypes. Empirically studies on multiple heterogeneous face datasets containing facial variations demonstrate the effectiveness of BHPL.","1556-6021","","10.1109/TIFS.2022.3164215","Ministry of Education, Republic of Singapore, through the Start-Up Grant; National Research Foundation (NRF) Singapore; Singapore Cybersecurity Consortium (SGCSC) Grant Office(grant numbers:SGCSC_Grant_2019-S01); NSFC/Research Grants Council (RGC) Joint Research Scheme(grant numbers:N_HKBU214/21); RGC General Research Fund(grant numbers:12201321); NSFC(grant numbers:61672444); Hong Kong Baptist University(grant numbers:RC-FNRA-IG/18-19/SCI/03,RC-IRCMs/18-19/SCI/01); Innovation and Technology Fund of Innovation and Technology Commission, Government of the Hong Kong Special Administrative Region(grant numbers:ITS/339/18); Startup Funding; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745912","Face synthesis;heterogeneous prototype learning;heterogeneous face recognition;adversarial learning","Prototypes;Faces;Hafnium;Generative adversarial networks;Image reconstruction;Image synthesis;Generators","face recognition;image reconstruction;learning (artificial intelligence)","bidirectional prototype learning;contaminated faces;heterogeneous domains;accurate image-to-image translation;nuisance facial variations;HPL;face image;source domain;variation-free prototype;specified target domain;unified end-to-end framework;bidirectional heterogeneous prototype learning;BHPL;bidirectional learning framework;heterogeneous prototypes;source-to-target;target-to-source domains;identity prototype feature;contaminated face images;target domains;robust heterogeneous face recognition;encoder-decoder structural generator;cross-domain identity-preserved prototype;face identity;multiple heterogeneous face datasets;heterogeneous prototype learning problem;heterogeneous face synthesis methods","","3","","69","IEEE","1 Apr 2022","","","IEEE","IEEE Journals"
"Estimating Regions of Deterioration in Electron Microscope Images of Rubber Materials via a Transfer Learning-Based Anomaly Detection Model","R. Togo; N. Saito; T. Ogawa; M. Haseyama","Faculty of Information Science and Technology, Hokkaido University, Sapporo, Japan; Department of Creative Engineering, National Institute of Technology, Kushiro College, Kushiro, Japan; Faculty of Information Science and Technology, Hokkaido University, Sapporo, Japan; Faculty of Information Science and Technology, Hokkaido University, Sapporo, Japan","IEEE Access","15 Nov 2019","2019","7","","162395","162404","A method for estimating regions of deterioration in electron microscope images of rubber materials is presented in this paper. Deterioration of rubber materials is caused by molecular cleavage, external force, and heat. An understanding of these characteristics is essential in the field of material science for the development of durable rubber materials. Rubber material deterioration can be observed by using on electron microscope but it requires much effort and specialized knowledge to find regions of deterioration. In this paper, we propose an automated deterioration region estimation method based on deep learning and anomaly detection techniques to support such material development. Our anomaly detection model, called Transfer Learning-based Deep Autoencoding Gaussian Mixture Model (TL-DAGMM), uses only normal regions for training since obtaining training data for regions of deterioration is difficult. TL-DAGMM makes use of extracted high representation features from a pre-trained deep learning model and can automatically learn the characteristics of normal rubber material regions. Regions of deterioration are estimated at the pixel level by calculated anomaly scores. Experiments on real rubber material electron microscope images demonstrated the effectiveness of our model.","2169-3536","","10.1109/ACCESS.2019.2950972","MIC/SCOPE(grant numbers:#181601001); Japan Society for the Promotion of Science(grant numbers:JP17H01744,JP19J10821); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8889738","Materials informatics;anomaly detection;deep learning;transfer learning","Image synthesis;Training;X-ray imaging;Gallium nitride;Inspection;Biomedical imaging;Generative adversarial networks","electron microscopes;feature extraction;Gaussian processes;image representation;learning (artificial intelligence);materials science computing;mixture models;neural nets;rubber","material science;durable rubber materials;rubber material deterioration;automated deterioration region estimation method;pre-trained deep learning model;normal rubber material regions;rubber material electron microscope images;transfer learning-based anomaly detection model;transfer learning-based deep autoencoding Gaussian M ixture Model","","2","","34","CCBY","1 Nov 2019","","","IEEE","IEEE Journals"
"Instance-Level Image Translation With a Local Discriminator","M. Xu; J. Lee; A. Fuentes; D. S. Park; J. Yang; S. Yoon","Department of Electronics Engineering, JeonBuk National University, Jeonbuk, South Korea; Department of Electronics Engineering, JeonBuk National University, Jeonbuk, South Korea; Department of Electronics Engineering, JeonBuk National University, Jeonbuk, South Korea; Core Research Institute of Intelligent Robots, JeonBuk National University, Jeonbuk, South Korea; College of Artificial Intelligence, Tianjin University of Science and Technology, Tianjin, China; Department of Computer Engineering, Mokpo National University, Jeonnam, South Korea","IEEE Access","13 Aug 2021","2021","9","","111802","111813","Instance-level image translation aims to only translate instance of interest and can be operated more finely and flexibly than object-level and holistic-level image translation. However, current algorithms are not suitable to do it since they employ a holistic or object level’s discriminator that tends to change the whole image or all instances. To address the issue, we propose a simple yet effective local discriminator, in which the input image is split into two parts, region of interest (ROI) and background. Instance mask is employed to align the ROI and the background is design to be random in a prior distribution to mitigate a divergence between the ROI and the background. In this way, we obtain translated instance with decent margins without artifacts as current algorithms get. Moreover we propose a new architecture to simultaneously realize versatile instance-level image translation. Experimental results prove that our proposed algorithm outperforms the state-of-the-art in position accuracy and background retainment by a clear margin.","2169-3536","","10.1109/ACCESS.2021.3102263","Basic Science Research Program through the National Research Foundation of Korea (NRF) by the Ministry of Education(grant numbers:2019R1A6A1A09031717); NRF through the Ministry of Science and ICT (MSIT)(grant numbers:2020R1A2C2013060); Korea Institute of Planning and Evaluation for Technology in Food, Agriculture and Forestry (IPET) through the Smart Farm Innovation Technology Development Program by the Rural Development Administration (RDA) and MSIT and the Ministry of Agriculture, Food and Rural Affairs (MAFRA)(grant numbers:421027-04); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9505697","Image translation;local discriminator;generative adversarial network","Generators;Generative adversarial networks;Feature extraction;Image synthesis;Training data;Semantics","image classification;image matching;image reconstruction;image representation;learning (artificial intelligence);pattern matching","object-level;holistic-level image translation;effective local discriminator;input image;ROI;instance mask;versatile instance-level image translation","","2","","39","CCBY","3 Aug 2021","","","IEEE","IEEE Journals"
"The Conditional Boundary Equilibrium Generative Adversarial Network and its Application to Facial Attributes","A. Marzouk; P. Barros; M. Eppe; S. Wermter","Knowledge Technology, Department of Informatics, University of Hamburg, Germany, Hamburg, Germany; Knowledge Technology, Department of Informatics, University of Hamburg, Germany, Hamburg, Germany; Knowledge Technology, Department of Informatics, University of Hamburg, Germany, Hamburg, Germany; Knowledge Technology, Department of Informatics, University of Hamburg, Germany, Hamburg, Germany","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","7","We propose an extension of the Boundary Equilibrium GAN (BEGAN) neural network, named Conditional BEGAN (CBEGAN), as a general generative and transformational approach for data processing. As a novelty, the system is able of both data generation and transformation under conditional input. We evaluate our approach for conditional image generation and editing using five controllable attributes for images of faces from the CelebA dataset: age, smiling, cheekbones, eyeglasses and gender. We perform a set of objective quantitative experiments to evaluate the model's performance and a qualitative user study to evaluate how humans assess the generated and edited images. Both evaluations yield coinciding results which show that the generated facial attributes are recognizable in more than 80% of all new testing samples.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852164","Conditional GAN;image generation;image translation","Image reconstruction;Generators;Gallium nitride;Generative adversarial networks;Image synthesis;Training;Decoding","face recognition;neural nets","general generative approach;transformational approach;data processing;data generation;image generation;conditional boundary equilibrium generative adversarial network;conditional BEGAN;neural network;facial attributes;CelebA dataset","","2","","21","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"PerceptionGAN: Real-world Image Construction from Provided Text through Perceptual Understanding","K. Garg; A. K. Singh; D. Herremans; B. Lall","Indian Institute of Technology Delhi, India; Indian Institute of Technology Delhi, India; Singapore University of Technology and Design, Singapore; Indian Institute of Technology Delhi, India","2020 Joint 9th International Conference on Informatics, Electronics & Vision (ICIEV) and 2020 4th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","7 Jan 2021","2020","","","1","7","Generating an image from a provided descriptive text is quite a challenging task because of the difficulty in incorporating perceptual information (object shapes, colors, and their interactions) along with providing high relevancy related to the provided text. Current methods first generate an initial low-resolution image, which typically has irregular object shapes, colors, and interaction between objects. This initial image is then improved by conditioning on the text. However, these methods mainly address the problem of using text representation efficiently in the refinement of the initially generated image, while the success of this refinement process depends heavily on the quality of the initially generated image, as pointed out in the Dynamic Memory Generative Adversarial Network (DM-GAN) paper. Hence, we propose a method to provide good initialized images by incorporating perceptual understanding in the discriminator module. We improve the perceptual information at the first stage itself, which results in significant improvement in the final generated image. In this paper, we have applied our approach to the novel StackGAN architecture. We then show that the perceptual information included in the initial image is improved while modeling image distribution at multiple stages. Finally, we generated realistic multi-colored images conditioned by text. These images have good quality along with containing improved basic perceptual information. More importantly, the proposed method can be integrated into the pipeline of other state-of-the-art text-based-image-generation models such as DM-GAN and AttnGAN to generate initial low-resolution images. We also worked on improving the refinement process in StackGAN by augmenting the third stage of the generator-discriminator pair in the StackGAN architecture. Our experimental analysis and comparison with the state-of-the-art on a large but sparse dataset MS COCO further validate the usefulness of our proposed approach. Contribution-This paper improves the pipeline for text to image generation by incorporating perceptual understanding in the initial stage of image generation.","","978-1-7281-9331-1","10.1109/ICIEVicIVPR48672.2020.9306618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306618","Deep Learning;GAN;MS COCO;Text to Image Generation;PerceptionGAN;Captioner Loss","Gallium nitride;Shape;Image synthesis;Training;Image color analysis;Generative adversarial networks;Generators","computer vision;feature extraction;image colour analysis;image representation;image resolution;image retrieval;text analysis","StackGAN architecture;text-based-image-generation models;dynamic memory generative adversarial network;generator-discriminator pair;improved basic perceptual information;realistic multicolored images;image distribution;final generated image;perceptual understanding;refinement process;initially generated image;text representation;irregular object shapes;initial low-resolution image;descriptive text;real-world image construction","","1","","27","IEEE","7 Jan 2021","","","IEEE","IEEE Conferences"
"TransferI2I: Transfer Learning for Image-to-Image Translation from Small Datasets","Y. Wang; H. Laria; J. van de Weijer; L. Lopez-Fuentes; B. Raducanu","Computer Vision Center, Universitat Autònoma de Barcelona, Spain; Computer Vision Center, Universitat Autònoma de Barcelona, Spain; Computer Vision Center, Universitat Autònoma de Barcelona, Spain; Universitat de les Illes Balears, Spain; Computer Vision Center, Universitat Autònoma de Barcelona, Spain","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","13990","13999","Image-to-image (I2I) translation has matured in recent years and is able to generate high-quality realistic images. However, despite current success, it still faces important challenges when applied to small domains. Existing methods use transfer learning for I2I translation, but they still require the learning of millions of parameters from scratch. This drawback severely limits its application on small domains. In this paper, we propose a new transfer learning for I2I translation (TransferI2I). We decouple our learning process into the image generation step and the I2I translation step. In the first step we propose two novel techniques: source-target initialization and self-initialization of the adaptor layer. The former finetunes the pretrained generative model (e.g., StyleGAN) on source and target data. The latter allows to initialize all non-pretrained network parameters without the need of any data. These techniques provide a better initialization for the I2I translation step. In addition, we introduce an auxiliary GAN that further facilitates the training of deep I2I systems even from small datasets. In extensive experiments on three datasets, (Animal faces, Birds, and Foods), we show that we outperform existing methods and that mFID improves on several datasets with over 25 points. Our code is available at: https://github.com/yaxingwang/TransferI2I.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01375","Generalitat de Catalunya; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9711185","Image and video synthesis","Training;Computer vision;Codes;Image synthesis;Transfer learning;Generative adversarial networks;Birds","","","","1","","61","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Spatial Consistency Constrained GAN for Human Motion Transfer","F. Ma; G. Xia; Q. Liu","Jiangsu Key Laboratory of Big Data Analysis Technology, School of Electronic and Information Engineering, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Jiangsu Key Laboratory of Big Data Analysis Technology, School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China","IEEE Transactions on Circuits and Systems for Video Technology","3 Feb 2022","2022","32","2","730","742","In this paper, we propose a new GAN-based framework to implement video-based human motion transfer, i.e., transferring the motions from the source person to the target one with the help of pose information. Human motion transfer involves large scaled spatial deformations from pose to body image and emphasizes spatial consistency of details. However, GAN is not suitable for the region-unaligned task due to the global adversarial loss does not focus on the spatial details. Therefore, we design a two-stage Spatial Consistency Constrained GAN architecture to generate realistic target person images. Within the model, we first generate a segment map to align the regions of different body parts with a given pose in stage-I and then concatenate the pose and the segment map as condition to generate a target person image in stage-II, so that the deformation problem is avoided. Furthermore, to improve the spatial detail consistency, we propose the shape consistency loss for the segment map generation to make the model pay more attention to the shape of each body part. We also propose a pose consistency loss for the target person image generation to enforce the generated images to contain similar enough poses to the input ones. The synthesized images with clear shape and sharp details demonstrate the effectiveness of the proposed method.","1558-2205","","10.1109/TCSVT.2021.3064035","National Natural Science Foundation of China(grant numbers:61825601,61802198); Natural Science Foundation of Jiangsu Province(grant numbers:BK20192004B,BK20180788); Natural Science Foundation of the Jiangsu Higher Education Institutions(grant numbers:18KJB520031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9370116","Motion transfer;conditional GAN;pose;segment map;person image generation;spatial consistency","Shape;Task analysis;Gallium nitride;Image synthesis;Image segmentation;Generative adversarial networks;Strain","image motion analysis;image segmentation;neural nets;pose estimation;video signal processing","pose information;spatial deformations;body image;region-unaligned task;global adversarial loss;GAN architecture;spatial detail consistency;shape consistency loss;pose consistency loss;target person image generation;video-based human motion transfer;source person;spatial consistency constrained GAN","","1","","48","IEEE","4 Mar 2021","","","IEEE","IEEE Journals"
"Omni-GAN: On the Secrets of cGANs and Beyond","P. Zhou; L. Xie; B. Ni; C. Geng; Q. Tian","Shanghai Jiao Tong University, Shanghai, China; Huawei Inc.; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Huawei Inc.","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","14041","14051","The conditional generative adversarial network (cGAN) is a powerful tool of generating high-quality images, but existing approaches mostly suffer unsatisfying performance or the risk of mode collapse. This paper presents Omni-GAN, a variant of cGAN that reveals the devil in designing a proper discriminator for training the model. The key is to ensure that the discriminator receives strong supervision to perceive the concepts and moderate regularization to avoid collapse. Omni-GAN is easily implemented and freely integrated with off-the-shelf encoding methods (e.g., implicit neural representation, INR). Experiments validate the superior performance of Omni-GAN and Omni-INR-GAN in a wide range of image generation and restoration tasks. In particular, Omni-INR-GAN sets new records on the ImageNet dataset with impressive Inception scores of 262.85 and 343.22 for the image sizes of 128 and 256, respectively, surpassing the previous records by 100+ points. Moreover, leveraging the generator prior, Omni-INR-GAN can extrapolate low-resolution images to arbitrary resolution, even up to ×60+ higher resolution. Code is available 1.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01380","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710697","Image and video synthesis","Training;Image resolution;Image synthesis;Computational modeling;Performance gain;Generative adversarial networks;Generators","","","","1","","74","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation","S. Matsumori; Y. Abe; K. Shingyouchi; K. Sugiura; M. Imai","Faculty of Science and Technology, Keio University, Kohoku-ku, Yokohama, Kanagawa, Japan; Faculty of Science and Technology, Keio University, Kohoku-ku, Yokohama, Kanagawa, Japan; Faculty of Science and Technology, Keio University, Kohoku-ku, Yokohama, Kanagawa, Japan; Faculty of Science and Technology, Keio University, Kohoku-ku, Yokohama, Kanagawa, Japan; Faculty of Science and Technology, Keio University, Kohoku-ku, Yokohama, Kanagawa, Japan","IEEE Access","10 Dec 2021","2021","9","","160521","160532","Text-guided image manipulation tasks have recently gained attention in the vision-and-language community. While most of the prior studies focused on single-turn manipulation, our goal in this paper is to address the more challenging multi-turn image manipulation (MTIM) task. Previous models for this task successfully generate images iteratively, given a sequence of instructions and a previously generated image. However, this approach suffers from under-generation and a lack of generated quality of the objects that are described in the instructions, which consequently degrades the overall performance. To overcome these problems, we present a novel architecture called a Visually Guided Language Attention GAN (LatteGAN). Here, we address the limitations of the previous approaches by introducing a Visually Guided Language Attention (Latte) module, which extracts fine-grained text representations for the generator, and a Text-Conditioned U-Net discriminator architecture, which discriminates both the global and local representations of fake or real images. Extensive experiments on two distinct MTIM datasets, CoDraw and i-CLEVR, demonstrate the state-of-the-art performance of the proposed model. The code is available online (https://github.com/smatsumori/LatteGAN).","2169-3536","","10.1109/ACCESS.2021.3129215","Japan Science and Technology Agency (JST) CREST, Japan(grant numbers:JPMJCR19A1); Japan Society for the Promotion of Science (JSPS) KAKENHI, Japan(grant numbers:JP21J13789); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9620071","Generative adversarial network (GAN);multi-turn text-conditioned image manipulation","Task analysis;Feature extraction;Visualization;Image synthesis;Generators;Semantics;Generative adversarial networks","image processing;neural nets;text analysis","LatteGAN;text representations;visually guided language attention GAN;visually guided language attention module;multiturn text-conditioned image manipulation;text-guided image manipulation tasks;vision-and-language community","","1","","35","CCBYNCND","18 Nov 2021","","","IEEE","IEEE Journals"
"Synthetic Images Generation Using Conditional Generative Adversarial Network for Skin Cancer Classification","R. Kaur; H. GholamHosseini; R. Sinha","School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand; School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand; School of Engineering, Computer and Mathematical Sciences, Auckland University of Technology, Auckland, New Zealand","TENCON 2021 - 2021 IEEE Region 10 Conference (TENCON)","16 Feb 2022","2021","","","381","386","Deep learning and computer vision have achieved remarkable success in many areas of machine learning and medical diagnostics. However, there is still a remarkable gap between dermatologists' skin cancer diagnosis and reliable computer-aided melanoma detection. There are several reasons behind this gap, and the availability of insufficient data for training deep learning networks is one of them. Data augmen-tation is a popular technique to increase training data manifolds to mitigate the lack of data. In this paper, a conditional generative adversarial network (CGAN) is proposed to produce high-resolution synthetic images to augment the training data and gain higher performance of skin cancer detection systems. The artificial generation of images resembling real images is a difficult task owing to unstable information present in the skin lesions such as irregular borders, diameter, shape, color, and texture. The generator module of CGAN is designed to aggregate the information from all feature layers and produce synthetic images. Additionally, the generator incorporates the auxiliary information along with image inputs to map latent feature components successfully. The network is trained on 10,015 skin cancer images taken from the International Skin Imaging Collaboration (ISIC 2018). It was concluded from the experiments that the proposed model obtained better classi-fication performance as compared to the imbalanced original dataset and other state-of-the-art methods.","2159-3450","978-1-6654-9532-5","10.1109/TENCON54134.2021.9707291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9707291","Skin cancer;Data augmentation;Generative adversarial convolutional network;Deep learning","Training;Deep learning;Image synthesis;Computational modeling;Training data;Generative adversarial networks;Generators","cancer;computer vision;feature extraction;image classification;learning (artificial intelligence);medical image processing;skin","synthetic images generation;conditional generative adversarial network;Skin cancer classification;computer vision;machine learning;remarkable gap;dermatologists;reliable computer-aided melanoma detection;insufficient data;training deep learning networks;data augmen-tation;training data manifolds;high-resolution synthetic images;skin cancer detection systems;artificial generation;skin lesions;generator module;image inputs;10 skin cancer images;015 skin cancer images;International Skin Imaging Collaboration","","1","","27","IEEE","16 Feb 2022","","","IEEE","IEEE Conferences"
"Water Gauge Image Augmentation based on Generative Adversarial Network","Z. Han; N. Lv; X. Ai; Y. Zhou; J. Jiang; C. Chen","School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China; Ministry of water resources, Goldwater Information Technology Development, Beijing, China; School of Electronic Engineering, Xidian University, Xi'an, China; School of Electronic Engineering, Xidian University, Xi'an, China","2022 IEEE International Conference on Smart Internet of Things (SmartIoT)","8 Sep 2022","2022","","","154","160","Water level monitoring based on water gauge is a very widely used way because of its cheapness and portability. However, the insufficiency and low quality of water gauge images restrict the performance of water level measuring task based on deep learning methods such as object detection and semantic segmentation. In this article, we proposed a generative adversarial network (GAN) named Contextual adjustment GAN (CA-GAN) for data augmentation of water gauge images obtained from Wuyuan, Jiangxi Province in China, i.e. CA-GAN can generate high-quality images containing various scales and types water gauge, which provide image data for application such as deep-learning based water level measuring method. First, a improved downsampling module is designed with the help of segmentation map for the semantic activation modulation. Then, the Unet++ structure with the improved downsampling module is applied in the generator. Finally, to modulate the semantic relationship, a contextual adjustment scheme is de-signed between adjacent layers. This article conducts detailed experiments, proving that the improved downsampling module contributes to the maintenance of semantic information of water gauge images. It is illustrated that the water gauge images generated by CA-GAN have higher quality comparing with other three GAN models. And our method is expected to promote the water level measurement and hydrological monitoring application development.","2770-2677","978-1-6654-7952-3","10.1109/SmartIoT55134.2022.00033","National Key Research and Development Program of China(grant numbers:2018YFE0126000); National Natural Science Foundation of China(grant numbers:62072360,61902292,62001357,62072359,62072355); Tongji University(grant numbers:ESSCKF2019-05); Ministry of Education; Xi' an Science and Technology Plan(grant numbers:20RGZN0005); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9874494","data augmentation;hydrological monitoring;deep learning;generative adversarial network","Image segmentation;Image synthesis;Smart cities;Semantics;Object detection;Generative adversarial networks;Feature extraction","computerised monitoring;gauges;geophysical image processing;image resolution;image sampling;image segmentation;learning (artificial intelligence);level measurement;object detection","water gauge image augmentation;generative adversarial network;water level monitoring;water gauge images;CA-GAN;high-quality images;types water gauge;deep-learning based water level;improved downsampling module;water level measurement","","","","27","IEEE","8 Sep 2022","","","IEEE","IEEE Conferences"
"S3GRN: Structural Similar Stepwise Generative Recognizable Network for Human Action Recognition With Limited Training Data","C. Li; X. Shen; H. Li","School of Information Science and Technology, Nantong University, Nantong, China; School of Information Science and Technology, Nantong University, Nantong, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","IEEE Access","8 Dec 2020","2020","8","","216219","216230","Human action recognition is a hot topic and it has been applied to various fields. Deep learning is one of the techniques in human action recognition which has achieved good results. However, the task is still challenging due to the less collected samples. In order to address this challenge and improve the recognition accuracy, the stepwise generative recognizable network is proposed based on the generative adversarial network, which can be used to expand limited training samples and then recognize. Firstly, the stepwise generative recognizable network is designed to combine the function of images generation and recognition for human action. Secondly, the structural similar constraint is introduced to stepwise generative recognizable network, called structural similar stepwise generative recognizable network, which can compare the similarity of generated images with real data to improve quality and diversity of generated images. Finally, the performance of proposed networks is verified by common databases and the self-build database which is collected in daily life. We achieved 97.14%, 94.88% and 99.69% recognition accuracy on MNIST, Weizmann and self-build dataset, respectively. The experimental results show that the combination of generation and recognition can improve the recognition accuracy without abundant training data, and the structural similar constraint not only can improve the quality and diversity of generated images but also perform better in convergence. The structural similar stepwise generative recognizable network reduces the workload of manual collection and solves the problem of lower recognition accuracy for limited training samples, which achieves the characteristics of natural expanded samples.","2169-3536","","10.1109/ACCESS.2020.3040758","National Natural Science Foundation of China(grant numbers:61871241,61976120); Nanjing University State Key Laboratory for Novel Software Technology(grant numbers:KFKT2019B15); Postgraduate Research and Practice Innovation Program of Jiangsu Province(grant numbers:KYCX19_2056); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272277","Generative network;recognizable network;combination;structural similar constraint","Image recognition;Generative adversarial networks;Generators;Training;Convolution;Image synthesis;Convergence","image motion analysis;image recognition;learning (artificial intelligence);neural nets","human action recognition;generative adversarial network;image generation;structural similar constraint;recognition accuracy;structural similar stepwise generative recognizable network;MNIST;Weizmann dataset;self-build dataset;deep learning;S3GRN","","","","42","CCBY","26 Nov 2020","","","IEEE","IEEE Journals"
"Generation and Transformation Invariant Learning for Tomato Disease Classification","G. Yilma; K. Gedamu; M. Assefa; A. Oluwasanmi; Z. Qin","School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Computer Science and Technology, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, China","2021 IEEE 2nd International Conference on Pattern Recognition and Machine Learning (PRML)","26 Aug 2021","2021","","","121","128","Deep learning-based plant disease management became a cost-effective way to improved agro-productivity. Advanced train sample generation and augmentation methods enlarge train sample size and improve feature distribution but generation and augmentation introduced sample feature discrepancy due to the generation learning process and augmentation artificial bias. We proposed a generation and geometric transformation invariant feature learning method using Siamese networks with maximum mean discrepancy loss to minimize the feature distribution discrepancies coming from the generated and augmented samples. Through variational GAN and geometric transformation, we created four dataset settings to train the proposed approach. The abundant evaluation results on the PlantVillage tomato dataset demonstrated the effectiveness of the proposed approach for the ResNet50 Siamese networks in learning generation and transformation invariant features for plant disease classification.","","978-1-6654-4383-8","10.1109/PRML52754.2021.9520693","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9520693","generation invariant learning;tomato disease classification;maximum mean discrepancy;variational GAN generation","Training;Learning systems;Image synthesis;Learning (artificial intelligence);Generative adversarial networks;Pattern recognition;Task analysis","diseases;feature extraction;image classification;image representation;learning (artificial intelligence)","geometric transformation invariant feature;maximum mean discrepancy loss;feature distribution discrepancies;generated augmented samples;PlantVillage tomato dataset;ResNet50 Siamese networks;transformation invariant features;plant disease classification;transformation invariant learning;tomato disease classification;deep learning-based plant disease management;improved agro-productivity;advanced train sample generation;train sample size;augmentation introduced sample feature discrepancy;augmentation artificial bias","","","","34","IEEE","26 Aug 2021","","","IEEE","IEEE Conferences"
"From Objects to a Whole Painting","F. Wang; B. Liu; W. Liu","College of Oceanography and Space Informatics, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China; College of Control Science and Engineering, China University of Petroleum (East China), Qingdao, China","2021 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","6 Jan 2022","2021","","","2177","2182","Style image painting is the process of using some stylized strokes to redraw a reference image purposefully and meaningfully. It is a kind of style image generation. In recent years, the application of GAN has greatly improved the quality of generated images for style image generation. However, those methods which use GAN are usually non-serialized. To solve this problem, reinforcement learning and RNN methods are applied to the generation of style images based on strokes. To speed up training, stroke-based style image painting using CNN framework is also proposed. But none of the existing style image painting methods takes into account the content distribution in the reference image, which makes the painting process lack a clear order. We extract the feature of the object contained in the image through the content acquisition module and use the optimal transmission theory to construct a compound loss function to integrate the content information into the painting process. As more advanced macro content information is added to the painting process, our painting method can draw images in a more orderly way. We have conducted a lot of experiments to prove that our method is superior to the current state-of-the-art style image painting method.","2577-1655","978-1-6654-4207-7","10.1109/SMC52423.2021.9658845","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658845","","Training;Image synthesis;Reinforcement learning;Feature extraction;Rendering (computer graphics);Propagation losses;Generative adversarial networks","art;convolutional neural nets;feature extraction;image processing;reinforcement learning","stylized strokes;style image generation;GAN;reinforcement learning;stroke-based style image painting;CNN;feature extraction;content acquisition module;optimal transmission theory","","","","31","IEEE","6 Jan 2022","","","IEEE","IEEE Conferences"
"A method for generating images of abnormal combustion state in MSWI process based on DCGAN","H. Guo; J. Tang; H. Zhang; D. Wang","Beijing Laboratory of Smart Environmental Protection, Beijing, China; Beijing Laboratory of Smart Environmental Protection, Beijing, China; Beijing Laboratory of Smart Environmental Protection, Beijing, China; Beijing Laboratory of Smart Environmental Protection, Beijing, China","2021 3rd International Conference on Industrial Artificial Intelligence (IAI)","30 Nov 2021","2021","","","1","6","This article is to provide qualified images of abnormal combustion state for the research of machine vision in municipal solid waste incineration (MSWI) process. Owing to the scarcity of the images of abnormal combustion state and the high cost of labeling, it is difficult to obtain sufficient images of abnormal combustion state. Aim at the problem, this paper proposes a method for generating images of abnormal combustion state based on a deep convolutional generative adversarial network (DCGAN). First, the real image data of abnormal combustion state is preprocessed. Second, the abnormal combustion state image generation generates false combustion images. Third, the real images and the generated images are fed into the discrimination network. The loss values are used to train the discrimination and generation. Finally, whether to update the parameters of the generation and discrimination network is determined by the error and epoch. The qualified generated abnormal combustion state images are obtained after the epoch setting met. The evaluation result of the generated image quality based on the Fréchet Inception Distance (FID) shows that DCGAN can realize the generation of abnormal combustion state images.","","978-1-6654-3517-8","10.1109/IAI53119.2021.9619455","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619455","Municipal solid waste incineration (MSWI);Abnormal combustion state;Image generation;deep convolutional generative adversarial network (DCGAN)","Waste management;Image quality;Waste materials;Image synthesis;Incineration;Machine vision;Generative adversarial networks","combustion;computer vision;incineration;neural nets","abnormal combustion state image generation;false combustion images;abnormal combustion state images;generated image quality","","","","15","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Scene Aware Person Image Generation through Global Contextual Conditioning","P. Roy; S. Ghosh; S. Bhattacharya; U. Pal; M. Blumenstein","University of Technology, Sydney, Australia; University of Technology, Sydney, Australia; Indian Institute of Technology, Kharagpur, India; Indian Statistical Institute, Kolkata, India; University of Technology, Sydney, Australia","2022 26th International Conference on Pattern Recognition (ICPR)","29 Nov 2022","2022","","","2764","2770","Person image generation is an intriguing yet challenging problem. However, this task becomes even more difficult under constrained situations. In this work, we propose a novel pipeline to generate and insert contextually relevant person images into an existing scene while preserving the global semantics. More specifically, we aim to insert a person such that the location, pose, and scale of the person being inserted blends in with the existing persons in the scene. Our method uses three individual networks in a sequential pipeline. At first, we predict the potential location and the skeletal structure of the new person by conditioning a Wasserstein Generative Adversarial Network (WGAN) on the existing human skeletons present in the scene. Next, the predicted skeleton is refined through a shallow linear network to achieve higher structural accuracy in the generated image. Finally, the target image is generated from the refined skeleton using another generative network conditioned on a given image of the target person. In our experiments, we achieve high-resolution photo-realistic generation results while preserving the general context of the scene. We conclude our paper with multiple qualitative and quantitative benchmarks on the results.","2831-7475","978-1-6654-9062-7","10.1109/ICPR56361.2022.9956682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956682","","Visualization;Image synthesis;Pipelines;Semantics;Benchmark testing;Generative adversarial networks;Skeleton","edge detection;image denoising;image motion analysis;image segmentation;image sensors;image texture;object detection;realistic images;solid modelling","constrained situations;contextually relevant person images;existing human skeletons;existing persons;existing scene;generative network;global contextual conditioning;global semantics;high-resolution photo-realistic generation results;individual networks;intriguing yet challenging problem;potential location;predicted skeleton;scene aware person image generation;sequential pipeline;shallow linear network;target image;target person;Wasserstein Generative Adversarial Network","","","","44","IEEE","29 Nov 2022","","","IEEE","IEEE Conferences"
"Deep Learning Approach For Facial Age Recognition","A. Muneer; R. F. Ali; A. A. Al-Sharai","Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Perak, Malaysia; Department of Computer and Information Sciences, Universiti Teknologi PETRONAS, Perak, Malaysia; Faculty of Electrical & Electronic Engineering, Universiti Tun Hussein Onn Malaysia, Johor, Malaysia","2021 International Conference on Innovative Computing (ICIC)","31 Jan 2022","2021","","","1","6","Age estimate using facial images is a fascinating and challenging issue. The characteristics from the face images are utilized to assess people’s age, gender, ethnic origin, and emotion. Among this group of characteristics, age estimates can be beneficial in numerous possible real-time applications. Deep learning has recently achieved great success. Hence, we are using the Generative Adversarial Network (GAN) based method for automatic aging of faces. GAN produces images by altering facial attributes, and we create them to preserve the original person’s identity in any age version. The deep generative networks have exhibited a remarkable capability in image generation. To the end, we introduced an approach for Identity-Preserving and GAN’s Latent vector optimization. The evaluation of the objective of the proposed method demonstrates the following results proposed framework produced more realistic by comparing the state-of-art and ground truth. It can also be used for cross-age verification. We will be using the Dataset of MORPH and CACD to train our GAN model as it requires much data to learn. Moreover, an adversarial learning technique is presented to train a generator and parallel discriminators simultaneously, resulting in smooth continuous face aging sequences.","","978-1-6654-0091-6","10.1109/ICIC53490.2021.9692943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9692943","Generative Adversarial Network;age progression;CACD;face verification;and age estimation","Deep learning;Image synthesis;Face recognition;Estimation;Aging;Pareto optimization;Generative adversarial networks","face recognition;feature extraction;learning (artificial intelligence)","facial attributes;original person;age version;deep generative networks;image generation;Identity-Preserving;GAN's Latent vector optimization;cross-age verification;GAN model;adversarial learning technique;smooth continuous face aging sequences;deep learning approach;facial age recognition;age estimate;facial images;fascinating issue;face images;people;ethnic origin;real-time applications;Generative Adversarial Network based method","","","","24","IEEE","31 Jan 2022","","","IEEE","IEEE Conferences"
"Integrating Feedforward Design into a Generative Network to Synthesize Supplementary Training Data for Object Classification","I. Kuhlmann; V. Seib; D. Paulus","University of Koblenz-Landau, Koblenz, Germany; University of Koblenz-Landau, Koblenz, Germany; University of Koblenz-Landau, Koblenz, Germany","2021 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)","18 May 2021","2021","","","215","220","Training a neural network typically requires large amounts of data. Yet, gathering such amounts of data may pose a problem. This can be addressed by augmenting a data set, i. e., by adding artificial data to the original data set. One method of augmentation is generating additional images, for example with a generative adversarial network. However, this technique still requires a lot of data. This work tackles this problem by implementing the so-called feedforward design as an alternative neural network training method which supposedly requires fewer training data than regular backpropagation. We apply feedforward design to the task of image generation. The results are evaluated by augmenting original data sets to form a new training set for a neural network classifier.","","978-1-6654-3198-9","10.1109/ICARSC52212.2021.9429808","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9429808","feedforward design;image generation;augmentation","Training;Backpropagation;Image synthesis;Conferences;Training data;Generative adversarial networks;Feedforward neural networks","image classification;learning (artificial intelligence);neural nets","object classification;generative adversarial network;feedforward design;neural network training;image generation;neural network classifier","","","","21","IEEE","18 May 2021","","","IEEE","IEEE Conferences"
"Dose-Conditioned Synthesis of Radiotherapy Dose With Auxiliary Classifier Generative Adversarial Network","W. Liao; Y. Pu","University of Chinese Academy of Sciences, Beijing, China; Shanghai APACTRON Particle Equipment Company, Ltd., Shanghai, China","IEEE Access","22 Jun 2021","2021","9","","87972","87981","In recent years, there are more and more researches on automatic radiotherapy planning based on artificial intelligence technology. Most of the work focuses on the dose prediction of radiotherapy planning, that is, the generation of radiation dose distribution image. Because of the small sample nature of radiotherapy planning data, it is difficult to obtain large-scale training data sets. In this paper, we propose a model of Dose-Conditioned Synthesis of Radiotherapy dose by using Auxiliary Classifier Generative Adversarial Network(ACGAN), and a method of customize and synthesis dose distribution images of specific tumor types and beam types is considered. This method can customize and generate dose distribution images of tumor types and beam types. The dose distribution images generated by our model are evaluated by MS-SSIM and PSNR, the results show that the image quality of dose distribution generated by ACGAN model was excellent, which was very close to the real data and shows high diversity, it can be used for data enhancement work of training data sets of dose prediction methods.","2169-3536","","10.1109/ACCESS.2021.3089369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9454486","Radiotherapy IMRT dose distribution autoplanning ACGAN data enhancement","Tumors;Generative adversarial networks;Image synthesis;Data mining;Visualization;Task analysis","artificial intelligence;cancer;dosimetry;radiation therapy;tumours","image quality;data enhancement work;dose prediction methods;Dose-conditioned synthesis;radiotherapy dose;automatic radiotherapy planning;artificial intelligence technology;radiation dose distribution image;radiotherapy planning data;large-scale training data sets;Dose-Conditioned Synthesis;specific tumor types;beam types;dose distribution images","","","","28","CCBYNCND","14 Jun 2021","","","IEEE","IEEE Journals"
"SAR Target Image Generation Method Using Azimuth-Controllable Generative Adversarial Network","C. Wang; J. Pei; X. Liu; Y. Huang; D. Mao; Y. Zhang; J. Yang","Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China; Department of Electrical Engineering, University of Electronic Science and Technology of China, Chengdu, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","11 Nov 2022","2022","15","","9381","9397","Sufficient synthetic aperture radar (SAR) target images are very important for the development of research works. However, available SAR target images are often limited in practice, which hinders the progress of SAR application. In this article, we propose an azimuth-controllable generative adversarial network to generate precise SAR target images with an intermediate azimuth between two given SAR images' azimuths. This network mainly contains three parts: 1) generator, 2) discriminator, and 3) predictor. Through the proposed specific network structure, the generator can extract and fuse the optimal target features from two input SAR target images to generate an SAR target image. Then, a similarity discriminator and an azimuth predictor are designed. The similarity discriminator can differentiate the generated SAR target images from the real SAR images to ensure the accuracy of the generated while the azimuth predictor measures the difference of azimuth between the generated and the desired to ensure the azimuth controllability of the generated. Therefore, the proposed network can generate precise SAR images, and their azimuths can be controlled well by the inputs of the deep network, which can generate the target images in different azimuths to solve the small sample problem to some degree and benefit the research works of SAR images. Extensive experimental results show the superiority of the proposed method in azimuth controllability and accuracy of SAR target image generation.","2151-1535","","10.1109/JSTARS.2022.3218369","National Natural Science Foundation of China(grant numbers:61901091,61901090); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933645","Automatic target recognition (ATR);azimuth-controllable;deep learning;generative adversarial network (GAN);synthetic aperture radar (SAR);target image generation","Synthetic aperture radar;Azimuth;Radar polarimetry;Generative adversarial networks;Generators;Image synthesis;Target recognition","control engineering computing;neural nets;radar computing;radar imaging;radar tracking;synthetic aperture radar;target tracking;telecommunication control","synthetic aperture radar target images;azimuth controllable generative adversarial network;different azimuths;azimuth controllability;generated SAR target images;azimuth predictor;similarity discriminator;input SAR target images;optimal target features;intermediate azimuth;SAR application;available SAR target images;SAR target image generation method","","","","52","CCBY","31 Oct 2022","","","IEEE","IEEE Journals"
"Chip Surface Defect Recognition based on Improved Faster R-CNN","C. Ma; Y. Chao; J. Zhu; Y. Wang; W. Liu; Z. Han","School of Mechanical Engineering, Jiangsu University of Technology, Changzhou, China; School of Mechanical Engineering, Jiangsu University of Technology, Changzhou, China; School of Mechanical Engineering, Jiangsu University of Technology, Changzhou, China; School of Mechanical Engineering, Jiangsu University of Technology, Changzhou, China; School of Mechanical Engineering, Jiangsu University of Technology, Changzhou, China; School of Mechanical Engineering, Jiangsu University of Technology, Changzhou, China","2022 28th International Conference on Mechatronics and Machine Vision in Practice (M2VIP)","16 Feb 2023","2022","","","1","6","The surface defects on integrated circuit chips are small and slender, and there exist low accuracy and difficulty in feature extraction and classification using networks models for defect recognition. Aiming at the recognition of five categories of defects on chip surfaces, an improved Faster R-CNN (Faster Region-based Convolutional Neural Network) deep learning network is proposed in this paper. Firstly, the sample data is enhanced by a combination of traditional image data augmentation and improved DCGAN (Deep Convolutional Generative Adversarial Network) to ensure the effectiveness of the model training process. Meanwhile, to improve the recognition accuracy of the model, the ResNet50 network is introduced into the backbone network to replace the original VGG16 (GG-Very-Deep-16 CNN) network in Faster R-CNN. The experimental results show that the average accuracy and speed of the chip surface defect recognition using the proposed method are 94.92% and 27 FPS, respectively. Compared with the YOLO-v4 (You Only Look Once v4) algorithm and the original Faster R-CNN algorithm, it can better meet the accuracy and speed requirements of actual chip surface defect recognition.","","979-8-3503-3433-3","10.1109/M2VIP55626.2022.10041049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041049","integrated circuit chips;defect recognition;DCGAN;Faster R-CNN","Training;Mechatronics;Image synthesis;Machine vision;Feature extraction;Generative adversarial networks;Convolutional neural networks","","","","","","17","IEEE","16 Feb 2023","","","IEEE","IEEE Conferences"
"GAN-based Gaussian Mixture Model Responsibility Learning","W. Huang; R. Y. Da Xu; S. Jiang; X. Liang; I. Oppermann","Faculty of Engineering and IT, University of Technology Sydney, Sydney, Australia; Faculty of Engineering and IT, University of Technology Sydney, Sydney, Australia; Faculty of Engineering and IT, University of Technology Sydney, Sydney, Australia; Faculty of Engineering and IT, University of Technology Sydney, Sydney, Australia; NSW Data Analytics Centre, Sydney, Australia","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","3467","3474","Mixture model (MM) is a probabilistic framework allows us to define dataset containing $K$ different modes. When each of the modes is associated with a Gaussian distribution, we refer to it as Gaussian MM or GMM. Given a data point x, a GMM may assume the existence of a random index $k$ ∊ {1, …, K} identifying which Gaussian the particular data is associated with. In a traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional likelihood p(x|k, θ) as well as the responsibility probability p(k| x, θ) describing the distribution weights for each data. Computing the responsibility allows us to retrieve many important statistics of the overall dataset, including the weights of each of the modes/clusters. Modern large datasets are often containing multiple unlabelled modes, such as paintings dataset may contain several styles; fashion images containing several unlabelled categories. In its raw representation, the Euclidean distances between the data (e.g., images) do not allow them to form mixtures naturally, nor it's feasible to compute responsibility distribution analytically, making GMM unable to apply. In this paper, we utilize the generative adversarial network (GAN) framework to achieve a plausible alternative method to compute these probabilities. The key insight is that we compute them at the data's latent space $z$ instead of x. However, this process of $z$ → $x$ is irreversible under GAN which renders the computation of responsibility p(k|x, θ) infeasible. Our paper proposed a novel method to solve it by using a socalled posterior consistency module (PCM). PCM acts like a GAN, except its generator $C$PCM does not output the data, but instead it outputs a distribution to approximate p(k|x, θ). The entire network is trained in an “end-to-end” fashion. Trough these techniques, it allows us to model the dataset of very complex structure using GMM and subsequently to discover interesting properties of an unsupervised dataset, including its segments, as well as generating new “out-distribution” data by smooth linear interpolation across any combinations of the modes in a completely unsupervised manner.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412309","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412309","","Phase change materials;Technological innovation;Image synthesis;Probability;Generative adversarial networks;Probabilistic logic;Generators","data analysis;expectation-maximisation algorithm;Gaussian distribution;Gaussian processes;interpolation;mixture models;neural nets;pattern clustering;probability;unsupervised learning","PCM;unsupervised dataset;out-distribution data;GAN-based Gaussian mixture model responsibility learning;probabilistic framework;Gaussian distribution;Gaussian MM;data point x;random index;distribution weights;modern large datasets;multiple unlabelled modes;paintings dataset;fashion images;unlabelled categories;responsibility distribution;GMM;generative adversarial network;responsibility probability","","","","15","IEEE","5 May 2021","","","IEEE","IEEE Conferences"
"Class-aware data augmentation by GAN specialisation to improve endoscopic images classification","C. Plateau-Holleville; Y. Benezeth","Univ. Bourgogne Franche-Comte, ImViA EA7535, France; Univ. Bourgogne Franche-Comte, ImViA EA7535, France","2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)","4 Nov 2022","2022","","","1","7","An expert eye is often needed to correctly identify mucosal lesions within endoscopic images. Hence, computer-aided diagnosis systems could decrease the need for highly specialized senior endoscopists and the effect of medical desertification. Moreover, they can significantly impact the latest endoscopic classification challenges such as the Inflammatory Bowel Disease (IBD) gradation. Most of the existing methods are based on deep learning algorithms. However, it is well known that these techniques suffer from the lack of data and/or class imbalance which can be lowered by using augmentation strategies thanks to synthetic generations. Late GAN framework progress made available accurate and production-ready artificial image generation that can be harnessed to extend training sets. It requires, however, to deal with the unsupervised nature of those networks to produce class-aware artificial images. In this article, we present our work to extend two datasets through a class-aware GAN-based augmentation strategy with the help of the state-of-the-art framework StyleGAN2-ADA and fine-tuning. We especially focused our efforts on endoscopic and IBD datasets to improve the classification and gradation of these images.","2641-3604","978-1-6654-8791-7","10.1109/BHI56158.2022.9926846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926846","","Training;Deep learning;Image synthesis;Generative adversarial networks;Computer aided diagnosis;Classification algorithms;Lesions","biomedical equipment;biomedical optical imaging;diseases;endoscopes;image classification;learning (artificial intelligence);medical image processing;patient diagnosis;pattern classification","Inflammatory Bowel Disease gradation;deep learning algorithms;augmentation strategies thanks;synthetic generations;GAN framework progress;available accurate production-ready;class-aware artificial images;class-aware GAN-based augmentation strategy;state-of-the-art framework StyleGAN2-ADA;endoscopic IBD datasets;class-aware data augmentation;GAN specialisation;endoscopic images classification;expert eye;mucosal lesions;computer-aided diagnosis systems;highly specialized senior endoscopists;medical desertification;latest endoscopic classification challenges","","","","37","IEEE","4 Nov 2022","","","IEEE","IEEE Conferences"
"Correlation-Based Data Augmentation for Machine Learning and Its Application to Road Environment Recognition","S. Omachi; M. Omachi","Graduate School of Engineering, Tohoku University, Sendai, Japan; National Institute of Technology Sendai College, Natori, Japan","IEEE Transactions on Vehicular Technology","15 Jul 2022","2022","71","7","7113","7121","The accuracy of machine learning depends largely on the quantity and quality of the training data. However, it is generally difficult to prepare a large number of high-quality data. To generate diverse image data, image generation techniques using deep learning, such as a generative adversarial network, can be used. However, because these methods require a large number of training data and a significant calculation time, they are unsuitable for generating training data for machine learning. In this article, we propose an image data augmentation model based on the statistical properties of the training data. With the proposed method, each image is divided into sub-regions based on the correlation calculated using a set of images. The image of each sub-region is modeled through a Gaussian mixture, and data augmentation is conducted by generating images based on this model. The proposed method does not require a large number of training data and can generate data within a relatively short calculation time. The proposed method is applied to the task of road environment recognition. The experiment results showed that the accuracy was improved through image augmentation using the proposed model.","1939-9359","","10.1109/TVT.2022.3167048","JSPS KAKENHI(grant numbers:JP19K12033,JP20H04201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756922","Data augmentation;statistical model;correlation;road environment recognition","Data models;Training data;Image synthesis;Machine learning;Training;Roads;Generative adversarial networks","Gaussian processes;image processing;learning (artificial intelligence);mixture models;statistical analysis;traffic engineering computing","correlation-based data augmentation;machine learning;road environment recognition;high-quality data;image generation techniques;generative adversarial network;image data augmentation model;image augmentation;Gaussian mixture","","","","43","IEEE","13 Apr 2022","","","IEEE","IEEE Journals"
"Generative Adversarial Style Transfer Networks for Face Aging","S. Palsson; E. Agustsson; R. Timofte; L. Van Gool","D-ITET, ETH Zurich; D-ITET, ETH Zurich; D-ITET, ETH zurich Merantix; D-ITET, ESAT, KU Leuven","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","2165","21658","How somebody looked like when younger? What could a person look like when 10 years older? In this paper we look at the problem of face aging, which relates to processing an image of a face to change its apparent age. This task involves synthesizing images and modeling the aging process, which both are problems that have recently enjoyed much research interest in the field of face and gesture recognition. We propose to look at the problem from the perspective of image style transfer, where we consider the age of the person as the underlying style of the image. We show that for large age differences, convincing face aging can be achieved by formulating the problem with a pairwise training of Cycle-consistent Generative Adversarial Networks (CycleGAN) over age groups. Furthermore, we propose a variant of CycleGAN which directly incorporates a pre-trained age prediction model, which performs better when the desired age difference is smaller. The proposed approaches are complementary in strengths and their fusion performs well for any desired level of aging effect. We quantitatively evaluate our proposed method through a user study and show that it outperforms prior state-of-the-art techniques for face aging.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575452","","Face;Aging;Gallium nitride;Training;Generators;Task analysis;Generative adversarial networks","age issues;face recognition;feature extraction;image processing;learning (artificial intelligence);neural nets","image style;age differences;pre-trained age prediction model;personalized features;face recognition;gesture recognition;cycle-consistent generative adversarial network training;image synthesis;image processing;generative adversarial style transfer networks;face aging","","37","","33","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Synthesize High-Quality Multi-Contrast Magnetic Resonance Imaging From Multi-Echo Acquisition Using Multi-Task Deep Generative Model","G. Wang; E. Gong; S. Banerjee; D. Martin; E. Tong; J. Choi; H. Chen; M. Wintermark; J. M. Pauly; G. Zaharchuk","Department of Biomedical Engineering, Tsinghua University, Beijing, China; Department of Electrical Engineering, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Biomedical Engineering, Tsinghua University, Beijing, China; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Biomedical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Radiology, Stanford University, Stanford, CA, USA; Department of Electrical Engineering, Stanford University, Stanford, CA, USA; Department of Radiology, Stanford University, Stanford, CA, USA","IEEE Transactions on Medical Imaging","30 Sep 2020","2020","39","10","3089","3099","Multi-echo saturation recovery sequence can provide redundant information to synthesize multi-contrast magnetic resonance imaging. Traditional synthesis methods, such as GE's MAGiC platform, employ a model-fitting approach to generate parameter-weighted contrasts. However, models' over-simplification, as well as imperfections in the acquisition, can lead to undesirable reconstruction artifacts, especially in T2-FLAIR contrast. To improve the image quality, in this study, a multi-task deep learning model is developed to synthesize multi-contrast neuroimaging jointly using both signal relaxation relationships and spatial information. Compared with previous deep learning-based synthesis, the correlation between different destination contrast is utilized to enhance reconstruction quality. To improve model generalizability and evaluate clinical significance, the proposed model was trained and tested on a large multi-center dataset, including healthy subjects and patients with pathology. Results from both quantitative comparison and clinical reader study demonstrate that the multi-task formulation leads to more efficient and accurate contrast synthesis than previous methods.","1558-254X","","10.1109/TMI.2020.2987026","GE Healthcare; Stanford University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9063444","Magnetic resonance imaging (MRI);image synthesis;generative adversarial network (GAN);deep learning (DL);image fusion","Magnetic resonance imaging;Generators;Machine learning;Generative adversarial networks;Training;Image reconstruction;Pathology","biomedical MRI;data acquisition;image reconstruction;image sequences;learning (artificial intelligence);medical image processing","multiecho acquisition;multitask deep generative model;multiecho saturation recovery sequence;parameter-weighted contrasts;T2-FLAIR contrast;image quality;multitask deep learning;multicontrast neuroimaging;reconstruction quality;high-quality multicontrast magnetic resonance imaging;pathology","Artifacts;Brain;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Neuroimaging","17","","61","IEEE","10 Apr 2020","","","IEEE","IEEE Journals"
"Apparel-Invariant Feature Learning for Person Re-Identification","Z. Yu; Y. Zhao; B. Hong; Z. Jin; J. Huang; D. Cai; X. -S. Hua","DAMO Academy, Alibaba Group, Hangzhou, China; Zhejiang University, Hangzhou, China; DAMO Academy, Alibaba Group, Hangzhou, China; DAMO Academy, Alibaba Group, Hangzhou, China; DAMO Academy, Alibaba Group, Hangzhou, China; State Key Laboratory of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, China; DAMO Academy, Alibaba Group, Hangzhou, China","IEEE Transactions on Multimedia","12 Dec 2022","2022","24","","4482","4492","With the rise of deep learning methods, person Re-Identification (ReID) performance has been improved tremendously in many public datasets. However, most public ReID datasets are collected in a short time window in which persons’ appearance rarely changes. In real-world applications such as in a shopping mall, the same person may change their wearings, and different persons may wear similar apparel. It reveals a critical problem that current ReID models heavily rely on a person’s apparel, resulting in an inconsistent ReID performance. Therefore, it is crucial to learn an apparel-invariant person representation under clothes changing or several persons wearing similar clothes cases. In this work, we tackle this problem from the viewpoint of invariant feature representation learning. The main contributions of this work are as follows. (1) We propose the semi-supervised Apparel-invariant Feature Learning (AIFL) framework to learn an apparel-invariant pedestrian representation using images of the same person wearing different clothes. (2) To obtain images of the same person wearing different clothes, we propose an unsupervised apparel-simulation GAN (AS-GAN) to synthesize cloth-changing images according to the target cloth embedding. It is worth noting that the images used in ReID tasks were cropped from real-world low-quality CCTV videos, making it more challenging to synthesize cloth-changing images. Extensive experiments demonstrate that our proposal can improve the ReID performance of the baseline models.","1941-0077","","10.1109/TMM.2021.3119133","The National Key Research and Development Program of China(grant numbers:2018AAA0101400); National Natural Science Foundation of China(grant numbers:62036009,U1909203,61936006); Innovation Capability Support Program of Shaanxi(grant numbers:2021TD-05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566823","Person re-identification;image synthesis;GAN;transfer learning","Proposals;Generators;Image color analysis;Generative adversarial networks;Cameras;Visualization;Training data","closed circuit television;clothing industry;feature extraction;image representation;learning (artificial intelligence);production engineering computing","apparel invariant feature learning;apparel invariant pedestrian representation;apparel invariant person representation;cloth changing images;current ReID models;deep learning methods;different clothes;inconsistent ReID performance;invariant feature representation learning;public ReID datasets;reidentification performance;semisupervised Apparel-invariant Feature;unsupervised apparel simulation GAN","","7","","56","IEEE","11 Oct 2021","","","IEEE","IEEE Journals"
"Adaptive Cycle-consistent Adversarial Network for Malaria Blood Cell Image Synthetization","Z. Liang; J. X. Huang","Dept. of Electrical Engineering & Computer Science, York University, Toronto, Canada; School of Information Technology, York University, Toronto, Canada","2021 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","26 Apr 2022","2021","","","1","7","Malaria is a tropical infectious disease that causes massive global deaths. The convolution neural network (CNN) models can theoretically classify the malaria infected blood cells from normal cells, but they are vulnerable to network attacks even with simple uniform noise. A typical drawback of CNN is that the algorithm cannot properly capture the meaningful patterns with clinical significance. We propose a novel adaptive cycle-consistent adversarial network (Ad Cycle GAN) to synthesize malaria significant patterns based on a homogeneous image template with randomness. The Ad Cycle GAN model consists of a pretrained convolutional variational autoencoder (CVAE) and conventional cycle-consistent adversarial network (Cycle GAN). The CVAE model is trained by a large, segmented blood cell dataset with 27,578 images. The model is optimized for 120 epochs. The CVAE is pipelined to a conventional Cycle GAN model with two generator-discriminator combinations. The real malaria positive images are at first sent to the pretrained CVAE to generate template images for the adversarial optimization with the real images. Therefore, the optimization process is to use generator G to convert the CVAE generated images from the synthetic domain (X) to the real malaria positive image domain (Y), then use generator F to convert the real malaria positive images from the real positive image domain (Y) to the CVAE synthetic image domain (X). The total generator loss is composed of adversarial loss, cycle loss, and identity loss, all loss terms are computed by least squared loss. The Ad Cycle GAN architecture is optimized by 150 epochs. When using a pretrained classifier to differentiate the real and synthetic malaria positive image, 99.61% of the real images from the real image set are accurately recognized, compared to 86.6% of the synthetic images are accurately classified. The average score of Frechet Inception Distance (FID) of the generated images by the Ad Cycle GAN is 0.0053 (Std=0.0004). By human eye observation, the Ad Cycle GAN generated images have reasonable fidelity as real blood cells with meaningful pathological patterns that properly mimics real malaria infected blood cells. The proposed Ad Cycle model can generate synthetic malaria infected blood cell images to successfully optimize the deep neural network model for high classification accuracy. We conclude that the new Ad Cycle GAN model can generate high quality malaria infected blood cell images with good diversity.","2332-5615","978-1-6654-2471-4","10.1109/AIPR52630.2021.9762068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762068","image augmentation;Ad Cycle GAN;image synthesis;deep neural network;malaria","Adaptation models;Adaptive systems;Neural networks;Cells (biology);Generative adversarial networks;Generators;Convolutional neural networks","blood;cellular biophysics;deep learning (artificial intelligence);diseases;image classification;image segmentation;medical image processing;object detection;optimisation","malaria positive image domain;CVAE synthetic image domain;Ad Cycle GAN architecture;real malaria positive image;synthetic malaria positive image;synthetic malaria infected blood cell images;deep neural network model;Ad Cycle GAN model;high quality malaria infected blood cell images;adaptive cycle-consistent adversarial network;malaria blood cell image synthetization;convolution neural network models;network attacks;malaria significant patterns;homogeneous image template;convolutional variational autoencoder;conventional cycle-consistent adversarial network;CVAE model;segmented blood cell;conventional Cycle GAN model;generator-discriminator combinations;template images;adversarial optimization;Frechet inception distance;FID;human eye observation;pathological patterns","","2","","19","IEEE","26 Apr 2022","","","IEEE","IEEE Conferences"
"Modified GAN-CAED to Minimize Risk of Unintentional Liver Major Vessels Cutting by Controlled Segmentation Using CTA/SPET-CT","M. N. Cheema; A. Nazir; P. Yang; B. Sheng; P. Li; H. Li; X. Wei; J. Qin; J. Kim; D. D. Feng","Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, COMSATS University Islamabad, Islamabad, Pakistan; Department of Computer Science, The University of Sheffield, Sheffield, U.K.; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Shanghai Jiao Tong University Affiliated Sixth People's Hospital, Shanghai, China; Shanghai Jiao Tong University Affiliated Sixth People's Hospital, Shanghai, China; Centre for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia; Biomedical and Multimedia Information Technology Research Group, School of Information Technologies, The University of Sydney, Sydney, NSW, Australia","IEEE Transactions on Industrial Informatics","26 Aug 2021","2021","17","12","7991","8002","This article substantially advances upon state-of-the-art to enhance liver vessels segmentation accuracy by leveraging advantages of synthetic PET-CT (SPET-CT) images in addition to computed tomography angiography (CTA) volumes. Our setup makes a hybrid solution of modified generative adversarial network-convolutional autoencoder (GAN-cAED) combining synthetic ability of GAN to deliver SPET-CT images with generative ability of cAED network in terms of latent learning to more refined segmentation of major liver vessels. We improve time complexity through a novel concept of controlled segmentation by introducing a threshold metric to stop segmentation up to a desired level. The innovative concept of controlled vessel segmentation with a stopping criterion via variant threshold levels will help surgeons to avoid unintentional major blood vessels cutting, reducing the risk of excessive blood loss. Clinically, such solutions offer computer-aided liver surgeries and drug treatment evaluation in a CTA-only environment, shorten the requirement of radioactive and expensive fused PET-CT images.","1941-0050","","10.1109/TII.2021.3064369","National Natural Science Foundation of China(grant numbers:61872241,61572316); Hong Kong Polytechnic University(grant numbers:P0030419,P0030929); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372834","Fused positron emission tomography-computed tomography (PET-CT);image synthesis;liver resection;liver vessel segmentation;synthesized PET-CT (SPET-CT)","Liver;Image segmentation;Computed tomography;Surgery;Generative adversarial networks;Measurement;Informatics","angiocardiography;blood;blood vessels;computerised tomography;image segmentation;liver;medical image processing;positron emission tomography;surgery","leveraging advantages;synthetic PET-CT images;computed tomography angiography volumes;hybrid solution;modified generative adversarial network-convolutional autoencoder;SPET-CT images;generative ability;cAED network;refined segmentation;controlled segmentation;controlled vessel segmentation;variant threshold levels;unintentional major blood vessels;computer-aided liver surgeries;CTA-only environment;radioactive fused PET-CT images;expensive fused PET-CT images;modified GAN-CAED;minimize risk;unintentional liver major vessels cutting;article substantially advances;liver vessels segmentation accuracy","","2","","43","IEEE","8 Mar 2021","","","IEEE","IEEE Journals"
"Semi-Supervised Learning of MRI Synthesis Without Fully-Sampled Ground Truths","M. Yurt; O. Dalmaz; S. Dar; M. Ozbey; B. Tınaz; K. Oguz; T. Çukur","Department of Electrical Engineering, Stanford University, Palo Alto, CA, USA; Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA; Department of Radiology, Hacettepe University, Ankara, Turkey; Department of Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey","IEEE Transactions on Medical Imaging","2 Dec 2022","2022","41","12","3895","3906","Learning-based translation between MRI contrasts involves supervised deep models trained using high-quality source- and target-contrast images derived from fully-sampled acquisitions, which might be difficult to collect under limitations on scan costs or time. To facilitate curation of training sets, here we introduce the first semi-supervised model for MRI contrast translation (ssGAN) that can be trained directly using undersampled k-space data. To enable semi-supervised learning on undersampled data, ssGAN introduces novel multi-coil losses in image, k-space, and adversarial domains. The multi-coil losses are selectively enforced on acquired k-space samples unlike traditional losses in single-coil synthesis models. Comprehensive experiments on retrospectively undersampled multi-contrast brain MRI datasets are provided. Our results demonstrate that ssGAN yields on par performance to a supervised model, while outperforming single-coil models trained on coil-combined magnitude images. It also outperforms cascaded reconstruction-synthesis models where a supervised synthesis model is trained following self-supervised reconstruction of undersampled data. Thus, ssGAN holds great promise to improve the feasibility of learning-based multi-contrast MRI synthesis.","1558-254X","","10.1109/TMI.2022.3199155","TUBITAK 1001(grant numbers:118E256); Turkish Academy of Sciences GEBIP 2015 fellowship; Science Academy BAGEP 2017 fellowship; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857899","Magnetic resonance imaging;image synthesis;semi-supervised;adversarial;undersampled","Magnetic resonance imaging;Training;Computational modeling;Image reconstruction;Generative adversarial networks;Data models;Biological system modeling","biomedical MRI;brain;coils;image reconstruction;image sampling;learning (artificial intelligence);medical image processing","acquired k-space samples;cascaded reconstruction-synthesis models;coil-combined magnitude images;deep models;fully-sampled acquisitions;fully-sampled ground truths;high-quality source;learning-based multicontrast MRI synthesis;MRI contrast translation;multicoil losses;retrospectively undersampled multicontrast brain MRI datasets;scan costs;self-supervised reconstruction;semisupervised learning;semisupervised model;single-coil models;single-coil synthesis models;ssGAN yields;supervised model;supervised synthesis model;target-contrast images;traditional losses;training sets;undersampled data;undersampled k-space data","Image Processing, Computer-Assisted;Algorithms;Retrospective Studies;Magnetic Resonance Imaging;Supervised Machine Learning","1","","68","IEEE","16 Aug 2022","","","IEEE","IEEE Journals"
"PMAN: Progressive Multi-Attention Network for Human Pose Transfer","B. Chen; Y. Zhang; H. Tan; B. Yin; X. Liu","School of Mathematical Science, Dalian University of Technology, Dalian, China; School of Mathematical Science, Dalian University of Technology, Dalian, China; School of Mathematical Science, Dalian University of Technology, Dalian, China; Department of Electronic Information and Electrical Engineering, Dalian University of Technology, Dalian, China; School of Mathematical Science, Dalian University of Technology, Dalian, China","IEEE Transactions on Circuits and Systems for Video Technology","11 Jan 2022","2022","32","1","302","314","This paper presents a novel approach for human pose transfer, progressive multi-attention network (PMAN), which generates a new human image by transferring the pose of a given person to a target pose. The network gradually updates the pose feature and the image feature through a series of multi-attention transfer blocks (MATBs). Each MATB consists of two attention mechanisms: pose-conditioned batch normalization (PCBN) and cooperative attention mechanism (CAM). Specifically, in low-level feature space, the PCBN layer with pose information is used to replace the BN layer of the image channel to realize the preliminary guidance of pose to image. The CAM is implemented as two gated mechanisms in high-level feature space, which reveals the essence of human pose transfer, that is, mutual guidance and dynamic control between pose and image. Gated memory writing is used to calculate the pixel-wise weight of the pose by using global image information to guide the update of the pose. Gated response utilizes an adaptive gating mechanism to dynamically control the pose information flow so as to update the image. A large number of subjective and objective experiments on DeepFashion and Market-1501 demonstrate the superiority of our method. The proposed multi-attention mechanism is well adapted to the human pose transfer task and provides a possible new idea for other cross-domain generation tasks.","1558-2205","","10.1109/TCSVT.2021.3059706","National Natural Science Foundation of China(grant numbers:61976040,61632006); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354827","Human pose transfer;person image synthesis;pose-conditioned batch normalization;cooperative attention mechanism","Task analysis;Logic gates;Training;Generators;Clothing;Generative adversarial networks;Decoding","feature extraction;object recognition;pose estimation","PMAN;progressive multiattention network;human pose transfer;human image;image feature;multiattention transfer blocks;attention mechanisms;cooperative attention mechanism;low-level feature space;pose information;image channel;gated mechanisms;high-level feature space;global image information;adaptive gating mechanism;multiattention mechanism;MATB;pose-conditioned batch normalization;mutual guidance;dynamic control;gated memory writing;pixel-wise weight;DeepFashion;Market-1501;cross-domain generation tasks","","1","","49","IEEE","16 Feb 2021","","","IEEE","IEEE Journals"
"Synthesizing Cell Protein data for Human Protein Cell Profiling Using Dual Deep Generative Modeling","R. Ranjan; S. Inoue; T. Shibata","Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology, Kitakyushu City, Fukuoka, Japan; Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology, Kitakyushu City, Fukuoka, Japan; Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology, Kitakyushu City, Fukuoka, Japan","2020 Joint 9th International Conference on Informatics, Electronics & Vision (ICIEV) and 2020 4th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)","7 Jan 2021","2020","","","1","6","To understand the biology of health, and how molecular dysfunction leads to disease, knowledge of the human cell is essential. The protein is the core unit of the human body made from trillions of cells, forming the body's various tissues. These tissues come together to create human organs. It is essential to understand the Spatio-temporal distribution of proteins in cells and to investigate human RNA-sequencing for human genes characterization. For this, it requires a massive amount of annotated data. However, due to many considerations like the high cost of data sample collection, lack of data sample availability, and lawful clauses for patient privacy, the majority of medical data is out of reach for general public research. In this study, we propose a new dual deep generative method for synthesizing human cell protein images by using the Generative Adversarial Network technique. Specifically, for that, we pair original cell protein images with their respective Cell-protein-tree. These pairs are then used to learn the mapping from a binary cell protein to a new cell protein image. For this purpose, we use an image-to-image translation technique based on adversarial learning. The generated cell protein images are expected to preserve the structural and visual quality of the training images. Visual and quantitative analysis of the experimental results demonstrates that the synthesized data are preserving the desired quality while maintaining the different forms of original data. Contribution-We have proposed a new dual deep generative model for synthesizing cell protein data.","","978-1-7281-9331-1","10.1109/ICIEVicIVPR48672.2020.9306574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9306574","Dual deep generative modeling;Cell protein image synthesis;Image-to-image translation;Generative adversarial network","Generative adversarial networks;Gallium nitride;Proteins;Biomedical imaging;Training;Computer architecture;Microprocessors","biology computing;cellular biophysics;diseases;genetics;learning (artificial intelligence);molecular biophysics;patient diagnosis;physiological models;proteins;RNA","generative adversarial network technique;respective Cell-protein-tree;binary cell protein;cell protein image;image-to-image translation technique;synthesized data;dual deep generative model;cell protein data;human body;tissues;human organs;human RNA-sequencing;human genes characterization;annotated data;data sample collection;data sample availability;medical data;dual deep generative method;human cell protein images;dual deep generative modeling;human protein cell profiling;original cell protein images","","1","","30","IEEE","7 Jan 2021","","","IEEE","IEEE Conferences"
"Delving into the Methods of Coverless Image Steganography","K. Y. Ng; S. Ong; K. Wong","Faculty of Computer Science and Information Technology, University of Malaya, Malaysia; Faculty of Computer Science and Information Technology, University of Malaya, Malaysia; School of Information Technology, Monash University, Malaysia","2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","5 Mar 2020","2019","","","1763","1772","Conventional cover-based image steganography methods embed secret information by modifying the original state of a cover image. This type of algorithm leaves a trace of changes on output stego image and eventually leads to successful detection by common steganalysis tools. As a solution, a coverless image steganographic method is proposed, where no cover image is required for embedding secret information. In this paper, the conventional coverless image steganography methods are first reviewed and categorized into constructive and nonconstructive-based methods. Next, these methods are summarized and analyzed, followed by a discussion about their advantages and drawbacks. Finally, the performances of the proposed methods are discussed using the common steganography evaluation metrics, including resistance to attack, embedding capacity, and perceptual image quality.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023053","─Carrier-less;stego image;constructive;image synthesis","Image quality;Receivers;Resistance;Histograms;Image color analysis;Generative adversarial networks;Image coding","image processing;steganography","nonconstructive-based method;constructive-based method;perceptual image quality;common steganography evaluation metrics;coverless image steganographic method;common steganalysis tools;output stego image;secret information","","","","40","IEEE","5 Mar 2020","","","IEEE","IEEE Conferences"
"Tumor-Attentive Segmentation-Guided GAN for Synthesizing Breast Contrast-Enhanced MRI Without Contrast Agents","E. Kim; H. -H. Cho; J. Kwon; Y. -T. Oh; E. S. Ko; H. Park","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Medical Aritifical Intelligence, Konyang University, Daejon, South Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Radiology, School of Medicine, Samsung Medical Center, Sungkyunkwan University, Seoul, South Korea; Center for Neuroscience Imaging Research, Institute for Basic Science, Suwon, South Korea","IEEE Journal of Translational Engineering in Health and Medicine","23 Nov 2022","2023","11","","32","43","Objective: Breast dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) is a sensitive imaging technique critical for breast cancer diagnosis. However, the administration of contrast agents poses a potential risk. This can be avoided if contrast-enhanced MRI can be obtained without using contrast agents. Thus, we aimed to generate T1-weighted contrast-enhanced MRI (ceT1) images from pre-contrast T1 weighted MRI (preT1) images in the breast. Methods: We proposed a generative adversarial network to synthesize ceT1 from preT1 breast images that adopted a local discriminator and segmentation task network to focus specifically on the tumor region in addition to the whole breast. The segmentation network performed a related task of segmentation of the tumor region, which allowed important tumor-related information to be enhanced. In addition, edge maps were included to provide explicit shape and structural information. Our approach was evaluated and compared with other methods in the local (n = 306) and external validation (n = 140) cohorts. Four evaluation metrics of normalized mean squared error (NRMSE), Pearson cross-correlation coefficients (CC), peak signal-to-noise ratio (PSNR), and structural similarity index map (SSIM) for the whole breast and tumor region were measured. An ablation study was performed to evaluate the incremental benefits of various components in our approach. Results: Our approach performed the best with an NRMSE 25.65, PSNR 54.80 dB, SSIM 0.91, and CC 0.88 on average, in the local test set. Conclusion: Performance gains were replicated in the validation cohort. Significance: We hope that our method will help patients avoid potentially harmful contrast agents. Clinical and Translational Impact Statement—Contrast agents are necessary to obtain DCE-MRI which is essential in breast cancer diagnosis. However, administration of contrast agents may cause side effects such as nephrogenic systemic fibrosis and risk of toxic residue deposits. Our approach can generate DCE-MRI without contrast agents using a generative deep neural network. Thus, our approach could help patients avoid potentially harmful contrast agents resulting in an improved diagnosis and treatment workflow for breast cancer.","2168-2372","","10.1109/JTEHM.2022.3221918","National Research Foundation(grant numbers:NRF-2020M3E5D2A01084892); Institute for Basic Science(grant numbers:IBS-R015-D1); Ministry of Science and Information Communication Technology (ICT)(grant numbers:IITP-2020-2018-0-01798); Institute for Information & communication Technology Planning & evaluation (IITP) Grant through the Artificial Intelligence (AI) Graduate School Support Program(grant numbers:2019-0-00421); ICT Creative Consilience program(grant numbers:IITP-2020-0-01821); Artificial Intelligence Innovation Hub program(grant numbers:2021-0-02068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9947047","Breast magnetic resonance imaging;image synthesis;tumor-attentive;segmentationguided;adversarial learning","Tumors;Magnetic resonance imaging;Image segmentation;Generators;Breast;Generative adversarial networks;Image edge detection","biological organs;biomedical MRI;cancer;image enhancement;image segmentation;medical image processing;neural nets;tumours","ablation study;breast cancer diagnosis;breast contrast-enhanced MRI;breast dynamic contrast-enhanced magnetic resonance imaging;ceT1;DCE-MRI;edge maps;evaluation metrics;generative adversarial network;local discriminator;normalized mean squared error;peak signal-to-noise ratio;Pearson cross-correlation coefficients;pre-contrast T1 weighted MRI images;preT1 breast images;segmentation task network;sensitive imaging technique;structural information;structural similarity index map;T1-weighted contrast-enhanced MRI images;tumor region;tumor-attentive segmentation-guided GAN;tumor-related information","Humans;Female;Contrast Media;Magnetic Resonance Imaging;Breast Neoplasms","","","49","CCBY","14 Nov 2022","","","IEEE","IEEE Journals"
"CLAST: Contrastive Learning for Arbitrary Style Transfer","X. Wang; W. Wang; S. Yang; J. Liu","Wangxuan Institute of Computer Technology, Peking University, Beijing, China; Wangxuan Institute of Computer Technology, Peking University, Beijing, China; S-Lab for Advanced Intelligence, Nanyang Technological University, Jurong West, Singapore; Wangxuan Institute of Computer Technology, Peking University, Beijing, China","IEEE Transactions on Image Processing","28 Oct 2022","2022","31","","6761","6772","Arbitrary style transfer aims at migrating the style of a reference style painting to a target content image. Existing methods find it challenging to achieve good content fidelity and style migration at the same time. Moreover, they all rely on manually defined content and style, which is of limited universality and robustness. In this paper, we propose to introduce contrastive learning into style transfer, instructing the network to automatically learn to model the structural content and artistic style based on natural contrastive relationships in style transfer. Compared with existing methods, our learned modeling of content and style is more robust and universal. In addition, we further propose instance-wise contrastive style losses and a patch-wise contrastive content loss to guide style transfer. Combining the proposed contrastive losses and two self-reconstruction strategies, we develop a new style transfer framework, which is pluggable and can be flexibly applied to various style transfer modules. Experimental results demonstrate that our method has strong flexibility and synthesizes stylized images with higher quality.","1941-0042","","10.1109/TIP.2022.3215899","National Natural Science Foundation of China(grant numbers:62172020); Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9929327","Style transfer;image synthesis;contrastive learning;image processing;self-supervised learning","Feature extraction;Painting;Training;Neural networks;Loss measurement;Modulation;Generative adversarial networks","computer graphics;image enhancement;image reconstruction;learning (artificial intelligence);rendering (computer graphics)","contrastive learning;arbitrary style transfer;reference style painting;target content image;good content fidelity;manually defined content;structural content;artistic style;natural contrastive relationships;learned modeling;instance-wise contrastive style losses;patch-wise contrastive content loss;contrastive losses;style transfer framework;style transfer modules","","","","57","IEEE","25 Oct 2022","","","IEEE","IEEE Journals"
"Learning Invariance from Generated Variance for Unsupervised Person Re-identification","H. Chen; Y. Wang; B. Lagadec; A. Dantcheva; F. Bremond","Inria and Universitë Côte d'Azur, Valbonne, France; Inria and Universitë Côte d'Azur, Valbonne, France; European Systems Integration, Le Cannet, France; Inria and Universitë Côte d'Azur, Valbonne, France; Inria and Universitë Côte d'Azur, Valbonne, France","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2022","PP","99","1","15","This work focuses on unsupervised representation learning in person re-identification (ReID). Recent self-supervised contrastive learning methods learn invariance by maximizing the representation similarity between two augmented views of a same image. However, traditional data augmentation may bring to the fore undesirable distortions on identity features, which is not always favorable in id-sensitive ReID tasks. In this paper, we propose to replace traditional data augmentation with a generative adversarial network (GAN) that is targeted to generate augmented views for contrastive learning. A 3D mesh guided person image generator is proposed to disentangle a person image into id-related and id-unrelated features. Deviating from previous GAN-based ReID methods that only work in id-unrelated space (pose and camera style), we conduct GAN-based augmentation on both id-unrelated and id-related features. We further propose specific contrastive losses to help our network learn invariance from id-unrelated and id-related augmentations. By jointly training the generative and the contrastive modules, our method achieves new state-of-the-art unsupervised person ReID performance on mainstream large-scale benchmarks.","1939-3539","","10.1109/TPAMI.2022.3226866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9970293","Person re-identification;image synthesis;representation disentanglement;data augmentation;contrastive learning","Three-dimensional displays;Image color analysis;Cameras;Shape;Lighting;Generators;Generative adversarial networks","","","","","","","IEEE","5 Dec 2022","","","IEEE","IEEE Early Access Articles"
"Undersampled Multi-Contrast MRI Reconstruction Based on Double-Domain Generative Adversarial Network","H. Wei; Z. Li; S. Wang; R. Li","Center for Biomedical Imaging Research, Department of Biomedical Engineering, Medical School, Tsinghua University, Beijing, China; Center for Biomedical Imaging Research, Department of Biomedical Engineering, Medical School, Tsinghua University, Beijing, China; Center for Biomedical Imaging Research, Department of Biomedical Engineering, Medical School, Tsinghua University, Beijing, China; Center for Biomedical Imaging Research, Department of Biomedical Engineering, Medical School, Tsinghua University, Beijing, China","IEEE Journal of Biomedical and Health Informatics","9 Sep 2022","2022","26","9","4371","4377","Multi-contrast magnetic resonance imaging can provide comprehensive information for clinical diagnosis. However, multi-contrast imaging suffers from long acquisition time, which makes it inhibitive for daily clinical practice. Subsampling k-space is one of the main methods to speed up scan time. Missing k-space samples will lead to inevitable serious artifacts and noise. Considering the assumption that different contrast modalities share some mutual information, it may be possible to exploit this redundancy to accelerate multi-contrast imaging acquisition. Recently, generative adversarial network shows superior performance in image reconstruction and synthesis. Some studies based on k-space reconstruction also exhibit superior performance over conventional state-of-art method. In this study, we propose a cross-domain two-stage generative adversarial network for multi-contrast images reconstruction based on prior full-sampled contrast and undersampled information. The new approach integrates reconstruction and synthesis, which estimates and completes the missing k-space and then refines in image space. It takes one fully-sampled contrast modality data and highly undersampled data from several other modalities as input, and outputs high quality images for each contrast simultaneously. The network is trained and tested on a public brain dataset from healthy subjects. Quantitative comparisons against baseline clearly indicate that the proposed method can effectively reconstruct undersampled images. Even under high acceleration, the network still can recover texture details and reduce artifacts.","2168-2208","","10.1109/JBHI.2022.3143104","National Natural Science Foundation of China(grant numbers:81971604); Natural Science Foundation of Beijing Municipality(grant numbers:L192013); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9682539","Generative adversarial network;cross-domain deep learning;multi-contrast MRI;image reconstruction;image synthesis","Image reconstruction;Magnetic resonance imaging;Generative adversarial networks;Training;Generators;Biomedical imaging;Deep learning","biomedical MRI;brain;image reconstruction;image resolution;image sampling;image texture;medical image processing","undersampled multicontrast MRI reconstruction;double-domain generative adversarial network;multicontrast magnetic resonance imaging;clinical diagnosis;multicontrast imaging suffers;long acquisition time;daily clinical practice;subsampling k-space;scan time;missing k-space samples;inevitable serious artifacts;different contrast modalities;mutual information;multicontrast imaging acquisition;image reconstruction;k-space reconstruction;state-of-art method;cross-domain;multicontrast images reconstruction;undersampled information;image space;contrast modality data;highly undersampled data;high quality images;undersampled images","Artifacts;Brain;Head;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging","","","23","IEEE","14 Jan 2022","","","IEEE","IEEE Journals"
"Transfer Learning based Noise Classification in Chest X-Ray Images","A. Tripathy; A. Das; M. Patel; E. Singhai; S. Tripathy","Deep Learning Research, Braynix AI, Bhubaneswar, Odisha; Deep Learning Research, Braynix AI, Bhubaneswar, Odisha; Deep Learning Research, Braynix AI, Ahmedabad, Gujarat; Department of Earth Sciences, IIT Roorkee, Indore, Madhya Pradesh; Department of Medical Engineering, Friedrich-Alexander, Universität Erlangen-Nürnberg, Nurnberg, Germany","2021 Smart Technologies, Communication and Robotics (STCR)","10 Nov 2021","2021","","","1","7","The chest radiograph (Chest X-ray) is the most commonly requested radiological examination owing to its effectiveness in the characterization and detection of cardiothoracic and pulmonary abnormalities. Deep Learning is emerging as the leading tool for medical image analysis. In particular, deep Convolutional Neural Networks (CNNs) have proven to be powerful tools for a wide range of computer vision tasks. The presence of noise in medical images reduces the efficacy of disease diagnosis and detection using manual inspection. This paper discusses different types of noise typically detected in Chest radiographs. The dataset is created using synthesized noise added mathematically onto the images. Transfer Learning methods are implemented in CNN based architectures to determine the best suitable model and the accuracy for real-time implementation. A comparison is also performed in training the model using data with augmentation and training without data augmentation. The geometrical variations induced in data augmentation and the three levels of noise added for each category suffice the requirement for generalizing the model to obtain validation accuracy of 99.41% for VGGnet and DenseNet. MobileNet V2 having inverted residual structure performs well on mobile devices that achieved an accuracy of 96.17%.","","978-1-6654-1806-5","10.1109/STCR51658.2021.9588878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9588878","Transfer learning;Image synthesis;Medical imaging;Noise detection;Noise classification","Training;Deep learning;Filtering;Transfer learning;Noise reduction;Tools;Generative adversarial networks","computer vision;convolutional neural nets;deep learning (artificial intelligence);diagnostic radiography;diseases;image classification;medical image processing;neural net architecture","data augmentation;noise classification;chest X-ray images;chest radiograph;cardiothoracic abnormalities;pulmonary abnormalities;deep learning;medical image analysis;deep convolutional neural networks;computer vision tasks;disease diagnosis;manual inspection;chest radiographs;synthesized noise;transfer learning methods;CNN based architectures;VGGnet;DenseNet;MobileNet V2","","","","17","IEEE","10 Nov 2021","","","IEEE","IEEE Conferences"
"Joint Pose and Expression Modeling for Facial Expression Recognition","F. Zhang; T. Zhang; Q. Mao; C. Xu","CAS, Institute of Automation; University of Chinese Academy of Sciences; School of Computer Science and Communication Engineering, Jiangsu University, China; University of Chinese Academy of Sciences","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","3359","3368","Facial expression recognition (FER) is a challenging task due to different expressions under arbitrary poses. Most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifiers for each pose. Different from existing methods, in this paper, we propose an end-to-end deep learning model by exploiting different poses and expressions jointly for simultaneous facial image synthesis and pose-invariant facial expression recognition. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, the encoder-decoder structure of the generator can learn a generative and discriminative identity representation for face images. Second, the identity representation is explicitly disentangled from both expression and pose variations through the expression and pose codes. Third, our model can automatically generate face images with different expressions under arbitrary poses to enlarge and enrich the training set for FER. Quantitative and qualitative evaluations on both controlled and in-the-wild datasets demonstrate that the proposed algorithm performs favorably against state-of-the-art methods.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578452","","Face recognition;Generative adversarial networks;Feature extraction;Gallium nitride;Training;Image recognition;Face","emotion recognition;face recognition;learning (artificial intelligence);pose estimation","expression modeling;arbitrary poses;face frontalization;nonfrontal facial image;end-to-end deep learning model;simultaneous facial image synthesis;pose-invariant facial expression recognition;generative adversarial network;generative identity representation;discriminative identity representation;face images;joint pose and expression modeling","","130","","66","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"Representation Learning by Rotating Your Faces","L. Tran; X. Yin; X. Liu","Department of Computer Science and Engineering, Michigan State University, East Lansing, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","4 Nov 2019","2019","41","12","3007","3021","The large pose discrepancy between two face images is one of the fundamental challenges in automatic face recognition. Conventional approaches to pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes a Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator enables DR-GAN to learn a representation that is both generative and discriminative, which can be used for face image synthesis and pose-invariant face recognition. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified identity representation along with an arbitrary number of synthetic face images. Extensive quantitative and qualitative evaluation on a number of controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art in both learning representations and rotating large-pose face images.","1939-3539","","10.1109/TPAMI.2018.2868350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453880","Representation learning;generative adversarial network;pose-invariant face recognition;face rotation and frontalization","Face recognition;Generators;Generative adversarial networks;Image generation;Image quality;Task analysis","face recognition;image representation;learning (artificial intelligence);neural nets;pose estimation","DR-GAN;pose-invariant face recognition;face frontalization;pose-invariant representation;nonfrontal face image;face image synthesis;face variations;pose code;disentangled representation learning-generative adversarial network","Algorithms;Biometric Identification;Face;Female;Humans;Image Processing, Computer-Assisted;Machine Learning;Male;Pattern Recognition, Automated;Rotation","70","","74","IEEE","2 Sep 2018","","","IEEE","IEEE Journals"
"VGAN-Based Image Representation Learning for Privacy-Preserving Facial Expression Recognition","J. Chen; J. Konrad; P. Ishwar",Boston University; Boston University; Boston University,"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","1651","165109","Reliable facial expression recognition plays a critical role in human-machine interactions. However, most of the facial expression analysis methodologies proposed to date pay little or no attention to the protection of a user's privacy. In this paper, we propose a Privacy-Preserving Representation-Learning Variational Generative Adversarial Network (PPRL-VGAN) to learn an image representation that is explicitly disentangled from the identity information. At the same time, this representation is discriminative from the standpoint of facial expression recognition and generative as it allows expression-equivalent face image synthesis. We evaluate the proposed model on two public datasets under various threat scenarios. Quantitative and qualitative results demonstrate that our approach strikes a balance between the preservation of privacy and data utility. We further demonstrate that our model can be effectively applied to other tasks such as expression morphing and image completion.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575368","","Face;Face recognition;Gallium nitride;Privacy;Visualization;Image recognition;Generative adversarial networks","data privacy;emotion recognition;face recognition;feature extraction;image representation;learning (artificial intelligence)","image completion;VGAN-based image representation learning;expression-equivalent face image synthesis;PPRL-VGAN;Privacy-Preserving Representation-Learning Variational Generative Adversarial Network;privacy-preserving facial expression recognition","","57","","36","IEEE","16 Dec 2018","","","IEEE","IEEE Conferences"
"AnonymousNet: Natural Face De-Identification With Measurable Privacy","T. Li; L. Lin","Department of Computer Science, Purdue University; Goergen Institute for Data Science, University of Rochester","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","9 Apr 2020","2019","","","56","65","With billions of personal images being generated from social media and cameras of all sorts on a daily basis, security and privacy are unprecedentedly challenged. Although extensive attempts have been made, existing face image de-identification techniques are either insufficient in photo-reality or incapable of balancing privacy and usability qualitatively and quantitatively, i.e., they fail to answer counterfactual questions such as ""is it private now?"", ""how private is it?"", and ""can it be more private?"" In this paper, we propose a novel framework called AnonymousNet, with an effort to address these issues systematically, balance usability, and enhance privacy in a natural and measurable manner. The framework encompasses four stages: facial attribute estimation, privacy-metric-oriented face obfuscation, directed natural image synthesis, and adversarial perturbation. Not only do we achieve the state-of-the-arts in terms of image quality and attribute prediction accuracy, we are also the first to show that facial privacy is measurable, can be factorized, and accordingly be manipulated in a photo-realistic fashion to fulfill different requirements and application scenarios. Experiments further demonstrate the effectiveness of the proposed framework.","2160-7516","978-1-7281-2506-0","10.1109/CVPRW.2019.00013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025628","","Face;Privacy;Generative adversarial networks;Measurement;Gallium nitride;Image quality","data privacy;face recognition;security of data","AnonymousNet;natural face de-identification;measurable privacy;personal images;social media;cameras;security;face image de-identification techniques;balance usability;facial attribute estimation;privacy-metric-oriented face obfuscation;directed natural image synthesis;image quality;attribute prediction accuracy;facial privacy;photo-realistic fashion","","39","","64","IEEE","9 Apr 2020","","","IEEE","IEEE Conferences"
"Image-to-Image Translation: Methods and Applications","Y. Pang; J. Lin; T. Qin; Z. Chen","Department of Electronic Engineer and Information Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Electronic Engineering, Hunan University, Changsha, China; Microsoft Research Asia, Beijing, China; Department of Electronic Engineer and Information Science, University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Multimedia","9 Aug 2022","2022","24","","3859","3881","Image-to-image translation (I2I) aims to transfer images from a source domain to a target domain while preserving the content representations. I2I has drawn increasing attention and made tremendous progress in recent years because of its wide range of applications in many computer vision and image processing problems, such as image synthesis, segmentation, style transfer, restoration, and pose estimation. In this paper, we provide an overview of the I2I works developed in recent years. We will analyze the key techniques of the existing I2I works and clarify the main progress the community has made. Additionally, we will elaborate on the effect of I2I on the research and industry community and point out remaining challenges in related fields.","1941-0077","","10.1109/TMM.2021.3109419","National Natural Science Foundation of China(grant numbers:U1908209,61632001,62021001); National Key Research and Development Program of China(grant numbers:2018AAA0101400); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9528943","Image-to-image translation;two-domain I2I;multi-domain I2I;supervised methods;unsupersived methods;semi-supervised methods;few-shot methods","Task analysis;Data models;Generative adversarial networks;Measurement;Generators;PSNR;Analytical models","computer vision;image processing;image representation;image segmentation;pose estimation","image-to-image translation;image processing problems;image synthesis","","27","","258","IEEE","3 Sep 2021","","","IEEE","IEEE Journals"
"Progressive Text-to-Face Synthesis with Generative Adversarial Network","X. Qiao; Y. Han; Y. Wu; Z. Zhang","College of Computer and Information Science, Southwest University, ChongQing, China; College of Computer and Information Science, Southwest University, ChongQing, China; College of Computer and Information Science, Southwest University, ChongQing, China; College of Computer and Information Science, Southwest University, ChongQing, China","2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)","12 Jan 2022","2021","","","1","8","Text-to-Face synthesis has considerable challenges and potentials in the field of public safety. Compared with the Text-to-Image synthesis models, the text descriptions of facial features are more complex and diverse. For the text embedding, most of the previous Text-to-Face synthesis models only deal with a single sentence containing several features of face images, and the generated images are vague and lack of details. In this paper, a novel Progressive Text-to-Face synthesis with Generative Adversarial Network (PFGAN) is proposed to generate natural face images from text descriptions. Firstly, a new text encoding method Convolution-Deconvolution Word Embedding LSTM (CDWE-BLSTM) is leveraged as the text encoder, which tackles more complex sentences and improves the accuracy of text encoding. Secondly, the PFGAN is composed of multiple generators and discriminators arranged in a tree-like structure. Furthermore, face images at multiple scales are progressively generated from different branches of the tree, corresponding to the same descriptions. images at multiple scales corresponding to the same scene are generated from different branches of the tree. By comparing with three existing Text-to-Face synthesis methods, extensive experiments demonstrate that the proposed PFGAN is very competitive in the IS (Inception Scores), FID (Frechet Inception Distance) and resolution of the generated face images.","","978-1-6654-3176-7","10.1109/FG52635.2021.9667004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9667004","","Training;Image resolution;Uncertainty;Face recognition;Generative adversarial networks;Encoding;Safety","convolutional neural nets;face recognition;image coding;text analysis;trees (mathematics)","generative adversarial network;text-to-image synthesis models;natural face images;multiple generators;generated face images;text-to-face synthesis methods;text encoding method;convolution-deconvolution word embedding LSTM;progressive text-to-face synthesis;CDWE-BLSTM;Frechet inception distance;PFGAN","","","","29","IEEE","12 Jan 2022","","","IEEE","IEEE Conferences"
"Multimodal Face Synthesis From Visual Attributes","X. Di; V. M. Patel","Whiting School of Engineering, Johns Hopkins University, Baltimore, MD, USA; Whiting School of Engineering, Johns Hopkins University, Baltimore, MD, USA","IEEE Transactions on Biometrics, Behavior, and Identity Science","29 Jun 2021","2021","3","3","427","439","Synthesis of face images from visual attributes is an important problem in computer vision and biometrics due to its applications in law enforcement and entertainment. Recent advances in deep generative networks have made it possible to synthesize high-quality face images from visual attributes. However, existing methods are specifically designed for generating unimodal images (i.e., visible faces) from attributes. In this paper, we propose a novel generative adversarial network which simultaneously synthesizes identity preserving multimodal face images (i.e., visible, sketch, thermal, etc.) from visual attributes without requiring paired data in different domains for training the network. We introduce a novel generator with multimodal stretch-out modules to simultaneously synthesize multimodal face images. Additionally, multimodal stretch-in modules are introduced in the discriminator which discriminate between real and fake images. Extensive experiments and comparison with several state-of-the-art methods are performed to verify the effectiveness of the proposed attribute-based multimodal synthesis method.","2637-6407","","10.1109/TBIOM.2021.3082038","ARO(grant numbers:W911NF-21-1-0135); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435620","Multimodal face synthesis;visual attributes;generative adversarial networks","Generators;Visualization;Faces;Training;Face recognition;Image resolution;Image synthesis","computer vision;face recognition","visual attributes;deep generative networks;high-quality face images;unimodal images;visible faces;novel generative adversarial network;multimodal face images;multimodal stretch-out modules;multimodal stretch-in modules;fake images;attribute-based multimodal synthesis method;multimodal face synthesis","","3","","60","IEEE","19 May 2021","","","IEEE","IEEE Journals"
"I2S2: Image-to-Scene Sketch Translation Using Conditional Input and Adversarial Networks","D. McGonigle; T. Wang; J. Yuan; K. He; B. Li","University of Southern Mississippi, Long Beach, USA; Austin Peay State University, Clarksville, USA; University of Southern Mississippi, Long Beach, USA; University of Southern Mississippi, Long Beach, USA; University of Southern Mississippi, Long Beach, USA","2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)","24 Dec 2020","2020","","","773","778","Image generation from sketch is a popular and well-studied computer vision problem. However, the inverse problem image-to-sketch (I2S) synthesis still remains open and challenging, let alone image-to-scene sketch (I2S2) synthesis, especially when full-scene sketch generations are highly desired. In this paper, we propose a framework for generating full-scene sketch representations from natural scene images, aiming to generate outputs that approximate hand-drawn scene sketches. Specifically, we exploit generative adversarial models to produce full-scene sketches given arbitrary input images that are actually conditions which are incorporated to guide the distribution mapping in the context of adversarial learning. To advance the use of such conditions, we further investigate edge detection solutions and propose to utilize Holistically-nested Edge Detection (HED) maps to condition the generative model. We conduct extensive experiments to validate the proposed framework and provide detailed quantitative and qualitative evaluations to demonstrate its effectiveness. In addition, we also demonstrate the flexibility of the proposed framework by using different conditional inputs, such as the Canny edge detector.","2375-0197","978-1-7281-9228-4","10.1109/ICTAI50040.2020.00123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288276","image generation;scene sketch;image-to-scene sketch translation;conditional input;generative adversarial networks;edge map","Inverse problems;Image synthesis;Image edge detection;Conferences;Detectors;Tools;Context modeling","computer vision;edge detection;feature extraction;image segmentation;image texture;natural scenes","full-scene sketches;arbitrary input images;adversarial learning;Edge Detection maps;generative model;different conditional inputs;2S2;image-to-scene sketch translation;conditional input;adversarial networks;image generation;computer vision problem;problem image-to-sketch;full-scene sketch generations;full-scene sketch representations;natural scene images;approximate hand-drawn scene sketches;generative adversarial models","","2","","22","IEEE","24 Dec 2020","","","IEEE","IEEE Conferences"
"One-Shot Image-to-Image Translation via Part-Global Learning With a Multi-Adversarial Framework","Z. Zheng; Z. Yu; H. Zheng; Y. Yang; H. T. Shen","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Electronic Information engineering, Ocean University of China, Qingdao, Shandong, China; School of Electronic Information engineering, Ocean University of China, Qingdao, Shandong, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China","IEEE Transactions on Multimedia","21 Jan 2022","2022","24","","480","491","It is well known that humans can learn and recognize objects effectively from several limited image samples. However, learning from just a few images is still a tremendous challenge for existing main-stream deep neural networks. Inspired by analogical reasoning in the human mind, a feasible strategy is to “translate” the abundant images of a rich source domain to enrich the relevant yet different target domain with insufficient image data. To achieve this goal, we propose a novel, effective multi-adversarial framework (<bold>MA</bold>) based on part-global learning, which accomplishes the one-shot cross-domain image-to-image translation. In specific, we first devise a part-global adversarial training scheme to provide an efficient way for feature extraction and prevent discriminators from being overfitted. Then, a multi-adversarial mechanism is employed to enhance the image-to-image translation ability to unearth the high-level semantic representation. Moreover, a balanced adversarial loss function is presented, which aims to balance the training data and stabilize the training process. Extensive experiments demonstrate that the proposed approach can obtain impressive results on various datasets between two extremely imbalanced image domains and outperform state-of-the-art methods on one-shot image-to-image translation. Our code will be released with this paper at <uri>https://github.com/zhengziqiang/OST</uri>.","1941-0077","","10.1109/TMM.2021.3053775","National Natural Science Foundation of China(grant numbers:U20B2063); Sichuan Science and Technology Program, China(grant numbers:2020YFS0057); National Key Research and Development Program of China(grant numbers:2018AAA0102200); Fundamental Research Funds for the Central Universities(grant numbers:ZYGX2019Z015); Leading Innovative and Entrepreneurial Talents; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9334433","One-shot;generative adversarial networks;image-to-image translation;unpaired cross-domain translation","Training;Semantics;Gallium nitride;Task analysis;Image synthesis;Image reconstruction;Visualization","deep learning (artificial intelligence);image processing","part-global learning;one-shot cross-domain image-to-image translation;part-global adversarial training scheme;image-to-image translation ability;balanced adversarial loss function;image domains;one-shot image-to-image translation;multiadversarial framework;image samples;main-stream deep neural networks;abundant images;target domain;MA","","1","","71","IEEE","22 Jan 2021","","","IEEE","IEEE Journals"
"Omni-Directional Image Generation from Single Snapshot Image","K. Okubo; T. Yamanaka","Department of Information and Communication Sciences, Sophia University, Tokyo, Japan; Department of Information and Communication Sciences, Sophia University, Tokyo, Japan","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","3021","3028","An omni-directional image (ODI) is the image that has a field of view covering the entire sphere around the camera. The ODIs have begun to be used in a wide range of fields such as virtual reality (VR), robotics, and social network services. Although the contents using ODI have increased, the available images and videos are still limited, compared with widespread snapshot images. A large number of ODIs are desired not only for the VR contents, but also for training deep learning models for ODI. For these purposes, a novel computer vision task to generate ODI from a single snapshot image is proposed in this paper. To tackle this problem, the conditional generative adversarial network was applied in combination with class-conditioned convolution layers. With this novel task, VR images and videos will be easily created even with a smartphone camera.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283047","omni-directional image;generative adversarial networks;deep learning","Convolution;Image synthesis;Virtual reality;Cameras;Generators;Task analysis;Videos","cameras;computer vision;learning (artificial intelligence);neural nets;smart phones;social networking (online);virtual reality","omni-directional image generation;single snapshot image;ODI;social network services;snapshot images;VR contents;conditional generative adversarial network;class-conditioned convolution layer","","","","16","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"Research of Various Approaches to Forming Book Covers Based on Semantic Text Analysis","A. K. Derevianko; T. K. Ilyasov","Department of Cybernetics National Research Nuclear, University MEPhI (Moscow Engineering Physics Institute), Moscow, Russian Federation; Department of Computer Systems and Technologies, National Research Nuclear University MEPhI (Moscow Engineering Physics Institute), Moscow, Russian Federation","2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus)","9 Apr 2021","2021","","","278","283","The article represents the use of various methods of text analysis in natural language processing tasks and the extraction of key information from it. In addition, various approaches to image generation using machine learning technologies are discussed, as well as the use of various methods for subsequent image styling. The research aims are to develop an algorithm and technology for generating the cover of a printed book based on the analysis of the text of the work. The research is divided into several main stages. The first stage is searching for the necessary information. The next step was to use text analysis methods to extract the main information from it. Next, generating images using machine learning technologies or parsing images for further placement on the cover. In the end, images are edited and stylized. Different approaches were applied for each of the stages, and the results were used to select the most appropriate one.","2376-6565","978-1-6654-0476-1","10.1109/ElConRus51938.2021.9396326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9396326","text analysis;extracting information from text;natural language processing;text templates;image generation;generative adversarial networks;neural style transfer;web scraping;web parsing","Text analysis;Machine learning algorithms;Image synthesis;Semantics;Machine learning;Natural language processing;Task analysis","learning (artificial intelligence);natural language processing;text analysis","forming book covers;semantic text analysis;natural language processing tasks;image generation;machine learning technologies;subsequent image styling;research aims;printed book;text analysis methods;main information;parsing images","","","","9","IEEE","9 Apr 2021","","","IEEE","IEEE Conferences"
"Instance Map Based Image Synthesis With a Denoising Generative Adversarial Network","Z. Zheng; C. Wang; Z. Yu; H. Zheng; B. Zheng","Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China; Department of Electronic Engineering, Ocean University of China, Qingdao, China","IEEE Access","4 Jul 2018","2018","6","","33654","33665","Semantic layout-based image synthesizing, which has benefited from the success of generative adversarial networks (GANs), has received a substantial amount of attention recently. How to enhance the synthesis image equality while maintaining the stochasticity of the GAN remains a challenge. We propose a novel denoising framework to handle this problem. The generation of overlapping objects is another challenging task when synthesizing images from a semantic layout to a realistic RGB photograph. To overcome this deficiency, we include a one-hot semantic label map to force the generator to pay more attention to the generation of overlapping objects. Furthermore, we improve the loss function of the discriminator by considering the perturbed loss and cascade layer loss to guide the generation process. We applied our methods to the Cityscapes, photo-sketch, day-night, facades, and NYU datasets to demonstrate the image generation ability of our model.","2169-3536","","10.1109/ACCESS.2018.2849108","National Natural Science Foundation of China(grant numbers:61701463); China Postdoctoral Science Foundation(grant numbers:2017M622277); Natural Science Foundation of Shandong Province(grant numbers:ZR2017BF011); Fundamental Research Funds for the Central Universities(grant numbers:201713019); Qingdao Postdoctoral Science Foundation of China; Nvidia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8389206","Underwater technology;artificial neural networks;image processing","Gallium nitride;Semantics;Generators;Task analysis;Convolution;Layout;Neural networks","image colour analysis;image denoising;image segmentation","instance map;image synthesis;semantic layout-based image synthesizing;GAN;synthesis image equality;overlapping objects;one-hot semantic label map;perturbed loss;cascade layer loss;generation process;image generation ability;generative adversarial network denoising;RGB photograph;Cityscapes;photo-sketch;NYU datasets","","7","1","52","OAPA","20 Jun 2018","","","IEEE","IEEE Journals"
"Context-Aware Layout to Image Generation with Enhanced Object Appearance","S. He; W. Liao; M. Y. Yang; Y. Yang; Y. -Z. Song; B. Rosenhahn; T. Xiang","iFlyTek-Surrey Joint Research Centre on Artificial Intelligence; TNT, Leibniz University Hannover; SUG, University of Twente; iFlyTek-Surrey Joint Research Centre on Artificial Intelligence; iFlyTek-Surrey Joint Research Centre on Artificial Intelligence; TNT, Leibniz University Hannover; iFlyTek-Surrey Joint Research Centre on Artificial Intelligence","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","15044","15053","A layout to image (L2I) generation model aims to generate a complicated image containing multiple objects (things) against natural background (stuff), conditioned on a given layout. Built upon the recent advances in generative adversarial networks (GANs), existing L2I models have made great progress. However, a close inspection of their generated images reveals two major limitations: (1) the object-to-object as well as object-to-stuff relations are often broken and (2) each object’s appearance is typically distorted lacking the key defining characteristics associated with the object class. We argue that these are caused by the lack of context-aware object and stuff feature encoding in their generators, and location-sensitive appearance representation in their discriminators. To address these limitations, two new modules are proposed in this work. First, a context-aware feature transformation module is introduced in the generator to ensure that the generated feature encoding of either object or stuff is aware of other coexisting objects/stuff in the scene. Second, instead of feeding location-insensitive image features to the discriminator, we use the Gram matrix computed from the feature maps of the generated object images to preserve location-sensitive information, resulting in much enhanced object appearance. Extensive experiments show that the proposed method achieves state-of-the-art performance on the COCO-Thing-Stuff and Visual Genome benchmarks. Code available at: https://github.com/wtliao/layout2img.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01480","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578900","","Visualization;Image synthesis;Computational modeling;Layout;Benchmark testing;Inspection;Generators","feature extraction;image representation;image segmentation;neural nets;object detection;ubiquitous computing","enhanced object appearance;generation model;complicated image;natural background;generative adversarial networks;object-to-object;object-to-stuff relations;object class;context-aware object;location-sensitive appearance representation;context-aware feature transformation module;generated feature encoding;location-insensitive image features;generated object images;COCO-Thing-Stuff;context-aware layout to image generation","","9","","51","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Hijack-GAN: Unintended-Use of Pretrained, Black-Box GANs","H. -P. Wang; N. Yu; M. Fritz","CISPA Helmholtz Center for Information Security, Germany; University of Maryland, College Park; CISPA Helmholtz Center for Information Security, Germany","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","7868","7877","While Generative Adversarial Networks (GANs) show increasing performance and the level of realism is becoming indistinguishable from natural images, this also comes with high demands on data and computation. We show that state-of-the-art GAN models – such as they are being publicly released by researchers and industry – can be used for a range of applications beyond unconditional image generation. We achieve this by an iterative scheme that also allows gaining control over the image generation process despite the highly non-linear latent spaces of the latest GAN models. We demonstrate that this opens up the possibility to re-use state-of-the-art, difficult to train, pre-trained GANs with a high level of control even if only black-box access is granted. Our work also raises concerns and awareness that the use cases of a published GAN model may well reach beyond the creators’ intention, which needs to be taken into account before a full public release. Code is available at https://github.com/a514514772/hijackgan.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00778","Helmholtz Association; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578512","","Industries;Computer vision;Codes;Image synthesis;Computational modeling;Process control;Aerospace electronics","image processing;iterative methods;learning (artificial intelligence);neural nets","hijack-GAN;black-box GAN;generative adversarial networks;natural images;state-of-the-art GAN models;unconditional image generation;iterative scheme;image generation process;nonlinear latent spaces;pretrained GAN;black-box access;published GAN model;public release","","8","","35","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Nested Variance Estimating VAE/GAN for Face Generation","H. -Y. Chen; C. -J. Lu","Institute of Information Science, Academia Sinica, Taipei, Taiwan; Institute of Information Science, Academia Sinica, Taipei, Taiwan","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","8","We study the task of conditional image generation and provide a general framework for combining autoencoders (AEs) with generative adversarial networks (GANs). Our framework provides a principled way to avoid well-known problems such as mode collapse and training instability. We use two AEs, a big parent-AE and a small child-AE, to play different roles. Our parent-AE is trained to minimize only one single objective: the reconstruction loss, which makes its training process stable and efficient. It is then fixed and used for several purposes, including initializing the generator and providing powerful features for a simple discriminator of GAN. It also plays the role of reducing the initial harder task of image generation to a simpler one: sampling from its latent distribution, which is given to child-AE to accomplish. Child-AE can be trained very efficiently due to its small size, and we only need to modify it if necessary for different applications, which makes our framework very flexible. Our experiments show that our model is capable of generating high quality novel images with controllable attributes.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852154","","Training;Generators;Image reconstruction;Task analysis;Image synthesis;Gallium nitride;Decoding","face recognition;feature extraction;image representation;neural nets","reconstruction loss;initial harder task;high quality novel images;face generation;conditional image generation;autoencoders;generative adversarial networks;GANs;mode collapse;training instability;nested variance estimating VAE/GAN","","1","","24","IEEE","30 Sep 2019","","","IEEE","IEEE Conferences"
"Improving GAN Equilibrium by Raising Spatial Awareness","J. Wang; C. Yang; Y. Xu; Y. Shen; H. Li; B. Zhou","The Australian National University; NA; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Australian National University; University of California, Los Angeles","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","11275","11283","The success of Generative Adversarial Networks (GANs) is largely built upon the adversarial training between a generator (G) and a discriminator (D). They are expected to reach a certain equilibrium where D cannot distinguish the generated images from the real ones. However, such an equilibrium is rarely achieved in practical GAN training, instead, D almost always surpasses G. We attribute one of its sources to the information asymmetry between D and G. We observe that D learns its own visual attention when determining whether an image is real or fake, but G has no explicit clue on which regions to focus on for a particular synthesis. To alleviate the issue of D dominating the competition in GANs, we aim to raise the spatial awareness of G. Randomly sampled multi-level heatmaps are encoded into the intermediate layers of G as an inductive bias. Thus G can purposefully improve the synthesis of certain image regions. We further propose to align the spatial awareness of G with the attention map induced from D. Through this way we effectively lessen the information gap between D and G. Extensive results show that our method pushes the two-player game in GANs closer to the equilibrium, leading to a better synthesis performance. As a byproduct, the intro-duced spatial awareness facilitates interactive editing over the output synthesis. Demo video and code are available at https://genforce.github.io/eqgan-sa/","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.01100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878655","Image and video synthesis and generation","Training;Heating systems;Visualization;Computer vision;Image synthesis;Computational modeling;Games","computer games;data visualisation;gallium compounds;game theory;Internet;knowledge acquisition;learning (artificial intelligence);public domain software;text analysis","practical GAN training;information asymmetry;visual attention;explicit clue;particular synthesis;multilevel heatmaps;intermediate layers;inductive bias;image regions;attention map;information gap;GANs closer;synthesis performance;intro-duced spatial awareness;output synthesis;GAN equilibrium;raising spatial;Generative Adversarial Networks;adversarial training;generator","","1","","28","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"SMILE: Semantically-guided Multi-attribute Image and Layout Editing","A. Romero; L. Van Gool; R. Timofte","Computer Vision Lab, ETH Zürich; KU Leuven; Computer Vision Lab, ETH Zürich","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","1924","1933","Attribute image manipulation has been a very active topic since the introduction of Generative Adversarial Networks (GANs). Exploring the disentangled attribute space within a transformation is a very challenging task due to the multiple and mutually-inclusive nature of the facial images, where different labels (eyeglasses, hats, hair, identity, etc.) can co-exist at the same time. Several works address this issue either by exploiting the modality of each domain/attribute using a conditional random vector noise, or extracting the modality from an exemplary image. However, existing methods cannot handle both random and reference transformations for multiple attributes, which limits the generality of the solutions. In this paper, we successfully exploit a multimodal representation that handles all attributes, be it guided by random noise or exemplar images, while only using the underlying domain information of the target domain. We present extensive qualitative and quantitative results for facial datasets and several different attributes that show the superiority of our method. Additionally, our method is capable of adding, removing or changing either fine-grained or coarse attributes by using an image as a reference or by exploring the style distribution space, and it can be easily extended to head-swapping and face-reenactment applications without being trained on videos.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607569","","Hair;Image segmentation;Computer vision;Image synthesis;Conferences;Semantics;Layout","face recognition;image representation;learning (artificial intelligence);neural nets;vectors;visual databases","facial datasets;fine-grained attributes;coarse attributes;semantically-guided multiattribute image;attribute image manipulation;active topic;generative adversarial networks;disentangled attribute space;mutually-inclusive nature;facial images;conditional random vector noise;exemplary image;multiple attributes;random noise;exemplar images;domain information;target domain;multimodal representation","","1","","47","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Verifying the Applicability of Synthetic Image Generation for Object Detection in Industrial Quality Inspection","M. Shirazi; M. Schmitz; S. Janssen; A. Thies; G. Safronov; A. Rizk; P. Mayr; P. Engelhardt","BMW Group, Munich, Germany; BMW Group, Munich, Germany; BMW Group, Munich, Germany; Technical University of Munich, Munich, Germany; Technical University of Munich, Munich, Germany; University of Duisburg-Essen, Essen, Germany; Technical University of Munich, Munich, Germany; BMW Group, Munich, Germany","2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)","25 Jan 2022","2021","","","1365","1372","Sparse and imbalanced data is a common challenge that practitioners must overcome when implementing industrial ML applications. This challenge concerns deep learning-based quality inspection systems in particular, as they often are obligated to adhere to high constraints in terms of reliability and performance. As deep learning quality inspection systems are usually implemented in a supervised manner, they additionally require balanced datasets that may be difficult or costly to obtain in production environments. However, new approaches using Generative Adversarial Networks for synthetic image generation promise a remedy by increasing the data amount of sparse classes, such as faults or defects. This paper presents an experimental use case where we employ a state-of-the-art image generator model of StyleGAN2 to a quality inspection application in laser beam welding to increase the number of defect images for training an object detector. We evaluate the generated images and their influence on the object detector’s performance using several training configurations. Our results reveal that with the limited amount of data, we are able to generate synthetic images that look promising at first glance. However, in the evaluation based on the object detector, we find that introducing synthetic images had an adverse effect on detection performance and robustness of the system. Further research is required to generate defect images from sparse datasets that can improve the performance of object detection systems in quality inspection.","","978-1-6654-4337-1","10.1109/ICMLA52953.2021.00221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680138","deep learning;image generation;quality inspection;object detection;laser beam welding","Training;Image synthesis;Welding;Object detection;Detectors;Production;Inspection","automatic optical inspection;deep learning (artificial intelligence);laser beam welding;object detection;production engineering computing;quality control;reliability","synthetic image generation;industrial quality inspection;deep learning quality inspection systems;generative adversarial networks;image generator model;defect images;object detector;object detection systems;industrial ML applications;reliability;production environments;faults;StyleGAN2;laser beam welding","","","","55","IEEE","25 Jan 2022","","","IEEE","IEEE Conferences"
"Viewpoint-agnostic Image Rendering","H. Aizawa; H. Kataoka; Y. Satoh; K. Kato",Gifu University; National Institute of Advanced Industrial Science and Technology (AIST); National Institute of Advanced Industrial Science and Technology (AIST); Gifu University,"2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 Jun 2021","2021","","","3802","3811","Rendering an any-viewpoint image is extremely difficult for Generative Adversarial Networks. This is because conventional GANs do not understand 3D information under-lying a given viewpoint image such as an object shape and relationship between viewpoint and objects in 3D space. In this paper, we present how to perform a Viewpoint-Agnostic Image Rendering (VAIR), equipping a conditional GAN with a mechanism to reconstruct 3D information of the input view. VAIR realizes any-viewpoint image generation by manipulating a viewpoint in 3D space where the reconstructed instance shape is arranged. In addition, we convert the reconstructed 3D shape into a 2D representation for image-based conditional GAN, while preserving detail 3D information. The representation consists of a depth image and 2D semantic keypoint images, which are obtained by rendering the shape from a viewpoint. In the experiment, we evaluate using a CUB-200-2011 dataset, which contains few-samples biased a viewpoint such that covers only part of the target appearance. As a result, our VAIR clearly renders an any-viewpoint image.","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00385","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423046","","Visualization;Computer vision;Three-dimensional displays;Shape;Image synthesis;Conferences;Semantics","computer vision;gallium compounds;image classification;image matching;image reconstruction;image representation;image segmentation;object detection;object recognition;rendering (computer graphics);solid modelling","Viewpoint-agnostic Image Rendering;Generative Adversarial Networks;given viewpoint image;object shape;Viewpoint-Agnostic Image Rendering;VAIR;any-viewpoint image generation;reconstructed instance shape;image-based conditional GAN;depth image;keypoint images","","","","64","IEEE","14 Jun 2021","","","IEEE","IEEE Conferences"
"Automatic Correction of Internal Units in Generative Neural Networks","A. Tousi; H. Jeong; J. Han; H. Choi; J. Choi","Korea Advanced Institute of Science and Technology (KAIST), South Korea; Ulsan National Institute of Science and Technology (UNIST), South Korea; Korea Advanced Institute of Science and Technology (KAIST), South Korea; Korea Advanced Institute of Science and Technology (KAIST), South Korea; INEEJI, South Korea","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","7928","7936","Generative Adversarial Networks (GANs) have shown satisfactory performance in synthetic image generation by devising complex network structure and adversarial training scheme. Even though GANs are able to synthesize realistic images, there exists a number of generated images with defective visual patterns which are known as artifacts. While most of the recent work tries to fix artifact generations by perturbing latent code, few investigate internal units of a generator to fix them. In this work, we devise a method that automatically identifies the internal units generating various types of artifact images. We further propose the sequential correction algorithm which adjusts the generation flow by modifying the detected artifact units to improve the quality of generation while preserving the original outline. Our method outperforms the baseline method in terms of FID-score and shows satisfactory results with human evaluation.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577278","","Training;Visualization;Computer vision;Codes;Image synthesis;Neural networks;Complex networks","complex networks;learning (artificial intelligence);neural nets;realistic images","automatic correction;internal units;generative neural Networks;Generative Adversarial Networks;GANs;synthetic image generation;complex network structure;adversarial training scheme;realistic images;defective visual patterns;artifact generations;artifact images;sequential correction algorithm;generation flow;detected artifact units","","","","24","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Geometric Constraint and Image Inpainting-Based Railway Track Fastener Sample Generation for Improving Defect Inspection","S. Su; S. Du; X. Lu","Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, Nanjing, China","IEEE Transactions on Intelligent Transportation Systems","6 Dec 2022","2022","23","12","23883","23895","Defective fastener images detection is an essential task in the vision-based railway track safety inspection. Although existing methods have achieved some level of success, the detection accuracy in this field suffers from the defective fasteners being far less common than normal fasteners. One way to tackle this problem is to expand the defect sample. However, current state-of-the-art defective fastener generation methods mainly rely on generative adversarial networks or simply augment the defect data through traditional image processing. These methods may not be ideal as it is difficult to produce images with high quality and rich diversity at the same time. This paper proposes a new method for fastener sample generation that actively divides the sample generation into two independent parts: defective foregrounds generation and complete backgrounds generation. The key to this method is to generate foregrounds and backgrounds based on geometric constraint and image inpainting, respectively. Specifically, we adopt a skeleton mapping algorithm to directionally control the generated types of defective foregrounds. Meanwhile, an image inpainting network is employed to expand the background. The experiments show that this enables us to generate better-quality and richer-diversity images by combining deep learning and image processing advantages. To the best of our knowledge, our method is the first to achieve state-of-the-art performance, i.e., the classification accuracy reaches 97.97%, without using real defective fastener images during the defect classification network training process.","1558-0016","","10.1109/TITS.2022.3207490","National Natural Science Foundation of China(grant numbers:62271143,62001110); Major Scientific Research Projects of China Railway Group(grant numbers:K2019G046); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200353); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903278","Image generation;image inpainting;geometric constraint;railway fastener inspection","Fasteners;Rail transportation;Inspection;Task analysis;Skeleton;Image synthesis;Training","computer vision;fasteners;feature extraction;image classification;image processing;image recognition;image restoration;image segmentation;inspection;learning (artificial intelligence);neural nets;pattern classification;railway safety;railways","complete backgrounds generation;current state-of-the-art defective fastener generation methods;defect classification network training process;defect data;defect sample;defective fastener images detection;defective fasteners;defective foregrounds generation;detection accuracy;fastener sample generation;generated types;generative adversarial networks;geometric constraint;image inpainting network;image processing advantages;improving defect inspection;normal fasteners;richer-diversity images;traditional image processing;vision-based railway track safety inspection","","","","49","IEEE","26 Sep 2022","","","IEEE","IEEE Journals"
"Generating and Controlling Diversity in Image Search","M. Mehrab Tanjim; R. Sinha; K. K. Singh; S. Mahadevan; D. Arbour; M. Sinha; G. W. Cottrell",UC San Diego; Adobe Research; Adobe Research; Adobe Research; Adobe Research; Adobe Applied ML; UC San Diego,"2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","15 Feb 2022","2022","","","3908","3916","In our society, generations of systemic biases have led to some professions being more common among certain genders and races. This bias is also reflected in image search on stock image repositories and search engines, e.g., a query like “male Asian administrative assistant” may produce limited results. The pursuit of a utopian world demands providing content users with an opportunity to present any profession with diverse racial and gender characteristics. The limited choice of existing content for certain combinations of profession, race, and gender presents a challenge to content providers. Current research dealing with bias in search mostly focuses on re-ranking algorithms. However, these methods cannot create new content or change the overall distribution of protected attributes in photos. To remedy these problems, we propose a new task of high-fidelity image generation conditioning on multiple attributes from imbalanced datasets. Our proposed task poses new sets of challenges for the state-of-the-art Generative Adversarial Networks (GANs). In this paper, we also propose a new training framework to better address the challenges. We evaluate our framework rigorously on a real-world dataset and perform user studies that show our model is preferable to the alternatives.","2642-9381","978-1-6654-0915-5","10.1109/WACV51458.2022.00396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706969","Explainable AI; Fairness; Accountability; Privacy and Ethics in Vision Datasets; Evaluation and Comparison of Vision Algorithms; Deep Learning -> Neural Generative Models; Autoencoders; GANs; Large-scale Vision Applications","Training;Measurement;Computer vision;Image resolution;Image synthesis;Computational modeling;Process control","data visualisation;inference mechanisms;Internet;learning (artificial intelligence);search engines","image search;systemic biases;profession;genders;races;stock image repositories;male Asian administrative assistant;utopian world demands;content users;diverse racial gender characteristics;existing content;high-fidelity image generation conditioning;state-of-the-art Generative Adversarial Networks","","","","23","IEEE","15 Feb 2022","","","IEEE","IEEE Conferences"
"A 3D GAN Architecture for Volumetric Synthetic Aperture Sonar","G. D. Vetaw; A. Reed; D. C. Brown; S. Jayasuriya","School of Electrical, Computer and Energy Engineering, Arizona State University; School of Electrical, Computer and Energy Engineering, Arizona State University; Graduate Program in Acoustics, Pennsylvania State University; School of Electrical, Computer and Energy Engineering, Arizona State University","OCEANS 2021: San Diego – Porto","15 Feb 2022","2021","","","1","9","Synthetic aperture sonar (SAS) is used extensively in underwater imaging for visualizing the seafloor and objects present on it. However, processing SAS images can be time-consuming and tedious, with machine learning techniques being ineffective due to the lack of available data. In particular, automated target recognition (ATR) with 3D SAS data for machine learning is challenging in many ways due to the complexity with working with 3D volumetric data. Recently, researchers have introduced generative adversarial networks (GANs) to help perform 2D SAS image generation for data augmentation. Following this line of work in this paper, we introduce a 3D-GAN architecture to generate photorealistic 3D SAS data which matches the fidelity of real data. In particular, we discuss novel latent space sampling and normalization to help 3D GANs overcome mode collapse for generating volumetric SAS information. Experimental results are shown on real 3D SAS data, showing the potential of using 3D GANs for dataset augmentation in the future.","0197-7385","978-0-692-93559-0","10.23919/OCEANS44145.2021.9706033","Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706033","","Three-dimensional displays;Target recognition;Image synthesis;Sea floor;Imaging;Data visualization;Machine learning","image reconstruction;learning (artificial intelligence);sonar imaging;synthetic aperture sonar;ultrasonic transducers","3D GAN architecture;volumetric synthetic aperture sonar;underwater imaging;seafloor;SAS images;particular target recognition;automated target recognition;machine learning;3D volumetric data;generative adversarial networks;GANs;2D SAS image generation;data augmentation;3D-GAN architecture;volumetric SAS information","","","","35","","15 Feb 2022","","","IEEE","IEEE Conferences"
"Deep generative image model using a hybrid system of generative adversarial nets (GANs)","A. M. Yousef; Y. M. K. Omar; E. Fakharany","College of Computing and Information Technology, Arab Academy for Science Technology and Maritime Transport, Cairo, Egypt; College of Computing and Information Technology, Arab Academy for Science Technology and Maritime Transport, Cairo, Egypt; College of Computing and Information Technology, Arab Academy for Science Technology and Maritime Transport, Cairo, Egypt","2017 Intl Conf on Advanced Control Circuits Systems (ACCS) Systems & 2017 Intl Conf on New Paradigms in Electronics & Information Technology (PEIT)","1 Mar 2018","2017","","","278","285","Synthesizing realistic images has been a challenge in machine learning, due to images being complex and high dimensional, thus making them hard to model well. Building on the recent progress made in both, generative multi adversarial nets (GMAN) and conditional generative adversarial nets (CGAN), this research aims at introducing a new method to improve image synthesis in generative adversarial networks (GAN). The research benefits from combining the best of both techniques to build a model (Hybrid-GAN) that produces higher images quality, which is hardly distinguished from real images. Furthermore, this model significantly enhances log-likelihood of test data under the conditional distributions. To validate the results, we have conducted a detailed comparison between images generated by our new model, Hybrid-GAN and those images produced by standard GANs. We execute the new model using MNIST dataset and demonstrated the results obtained from the generating task.","","978-1-5386-6407-0","10.1109/ACCS-PEIT.2017.8303052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8303052","","Gallium nitride;Generators;Training;Task analysis;Image generation;Linear programming;Mathematical model","learning (artificial intelligence);realistic images","Hybrid-GAN;generative image model;hybrid system;realistic images;machine learning;generative multiadversarial nets;image synthesis;generative adversarial networks;image quality","","","","25","IEEE","1 Mar 2018","","","IEEE","IEEE Conferences"
"A New Chapter for Medical Image Generation: The Stable Diffusion Method","L. X. Nguyen; P. Sone Aung; H. Q. Le; S. -B. Park; C. S. Hong","Department of Computer Science and Engineering, Kyung Hee University, Republic of Korea; Department of Computer Science and Engineering, Kyung Hee University, Republic of Korea; Department of Computer Science and Engineering, Kyung Hee University, Republic of Korea; Department of Computer Science and Engineering, Kyung Hee University, Republic of Korea; Department of Computer Science and Engineering, Kyung Hee University, Republic of Korea","2023 International Conference on Information Networking (ICOIN)","22 Feb 2023","2023","","","483","486","Data collecting and sharing have been widely accepted and adopted to improve the performance of deep learning models in almost every field. Nevertheless, in the medical field, sharing the data of patients can raise several critical issues, such as privacy and security or even legal issues. Synthetic medical images have been proposed to overcome such challenges; these synthetic images are generated by learning the distribution of realistic medical images but completely different from them so that they can be shared and used across different medical institutions. Currently, the diffusion model (DM) has gained lots of attention due to its potential to generate realistic and high-resolution images, particularly outperforming generative adversarial networks (GANs) in many applications. The DM defines state of the art for various computer vision tasks such as image inpainting, class-conditional image synthesis, and others. However, the diffusion model is time and power consumption due to its large size. Therefore, this paper proposes a lightweight DM to synthesize the medical image; we use computer tomography (CT) scans for SARS-CoV-2 (Covid-19) as the training dataset. Then we do extensive simulations to show the performance of the proposed diffusion model in medical image generation, and then we explain the key component of the model.","1976-7684","978-1-6654-6268-6","10.1109/ICOIN56518.2023.10049010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049010","Medical Image Generation;Diffusion Model;UNet architecture;CT scan of Covid-19","Training;COVID-19;Performance evaluation;Power demand;Image synthesis;Computational modeling;Data models","","","","","","14","IEEE","22 Feb 2023","","","IEEE","IEEE Conferences"
"Self-Attentive Spatial Adaptive Normalization for Cross-Modality Domain Adaptation","D. Tomar; M. Lortkipanidze; G. Vray; B. Bozorgtabar; J. -P. Thiran","Signal Processing Laboratory (LT55), École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; Signal Processing Laboratory (LT55), École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; Signal Processing Laboratory (LT55), École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland; CIBM Center for Biomedical Imaging, Lausanne, Switzerland; CIBM Center for Biomedical Imaging, Lausanne, Switzerland","IEEE Transactions on Medical Imaging","30 Sep 2021","2021","40","10","2926","2938","Despite the successes of deep neural networks on many challenging vision tasks, they often fail to generalize to new test domains that are not distributed identically to the training data. The domain adaptation becomes more challenging for cross-modality medical data with a notable domain shift. Given that specific annotated imaging modalities may not be accessible nor complete. Our proposed solution is based on the cross-modality synthesis of medical images to reduce the costly annotation burden by radiologists and bridge the domain gap in radiological images. We present a novel approach for image-to-image translation in medical images, capable of supervised or unsupervised (unpaired image data) setups. Built upon adversarial training, we propose a learnable self-attentive spatial normalization of the deep convolutional generator network’s intermediate activations. Unlike previous attention-based image-to-image translation approaches, which are either domain-specific or require distortion of the source domain’s structures, we unearth the importance of the auxiliary semantic information to handle the geometric changes and preserve anatomical structures during image translation. We achieve superior results for cross-modality segmentation between unpaired MRI and CT data for multi-modality whole heart and multi-modal brain tumor MRI (T1/T2) datasets compared to the state-of-the-art methods. We also observe encouraging results in cross-modality conversion for paired MRI and CT images on a brain dataset. Furthermore, a detailed analysis of the cross-modality image translation, thorough ablation studies confirm our proposed method’s efficacy.","1558-254X","","10.1109/TMI.2021.3059265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354186","Domain adaptation;image synthesis;generative adversarial networks;unpaired domains;self-attention","Image segmentation;Biomedical imaging;Computed tomography;Magnetic resonance imaging;Semantics;Anatomical structure;Task analysis","biomedical MRI;brain;computerised tomography;image classification;image segmentation;learning (artificial intelligence);medical image processing;neural nets;tumours","spatial adaptive normalization;cross-modality domain adaptation;deep neural networks;challenging vision tasks;test domains;training data;cross-modality medical data;notable domain shift;specific annotated imaging modalities;cross-modality synthesis;medical images;costly annotation burden;domain gap;radiological images;unpaired image data;adversarial training;self-attentive spatial normalization;deep convolutional generator network;previous attention-based image-to-image translation approaches;domain-specific;source domain;cross-modality segmentation;multimodality whole heart;multimodal brain tumor MRI;cross-modality conversion;cross-modality image translation","Brain Neoplasms;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Neural Networks, Computer;Neuroimaging","15","","53","IEEE","12 Feb 2021","","","IEEE","IEEE Journals"
"GMLight: Lighting Estimation via Geometric Distribution Approximation","F. Zhan; Y. Yu; C. Zhang; R. Wu; W. Hu; S. Lu; F. Ma; X. Xie; L. Shao","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Alibaba DAMO Academy, Hangzhou, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; The Chinese University of Hong Kong, Hong Kong; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Alibaba DAMO Academy, Hangzhou, China; Alibaba DAMO Academy, Hangzhou, China; Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates","IEEE Transactions on Image Processing","11 Mar 2022","2022","31","","2268","2278","Inferring the scene illumination from a single image is an essential yet challenging task in computer vision and computer graphics. Existing works estimate lighting by regressing representative illumination parameters or generating illumination maps directly. However, these methods often suffer from poor accuracy and generalization. This paper presents Geometric Mover’s Light (GMLight), a lighting estimation framework that employs a regression network and a generative projector for effective illumination estimation. We parameterize illumination scenes in terms of the geometric light distribution, light intensity, ambient term, and auxiliary depth, which can be estimated by a regression network. Inspired by the earth mover’s distance, we design a novel geometric mover’s loss to guide the accurate regression of light distribution parameters. With the estimated light parameters, the generative projector synthesizes panoramic illumination maps with realistic appearance and high-frequency details. Extensive experiments show that GMLight achieves accurate illumination estimation and superior fidelity in relighting for 3D object insertion. The codes are available at https://github.com/fnzhan/Illumination-Estimation","1941-0042","","10.1109/TIP.2022.3151997","RIE2020 Industry Alignment Fund—Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9725240","Lighting estimation;image synthesis;generative adversarial networks","Lighting;Geometry;Estimation;Light sources;Three-dimensional displays;Costs;Harmonic analysis","","","","3","","40","IEEE","2 Mar 2022","","","IEEE","IEEE Journals"
"A Semi-Supervised Image-to-Image Translation Framework for SAR–Optical Image Matching","W. -L. Du; Y. Zhou; H. Zhu; J. Zhao; Z. Shao; X. Tian","Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; Engineering Research Center of Mine Digitization, Ministry of Education of Peoples Republic of China, Xuzhou, China; State Key Laboratory of Lunar and Planetary Sciences, Macau University of Science and Technology, Taipa, Macau","IEEE Geoscience and Remote Sensing Letters","1 Dec 2022","2022","19","","1","5","Synthetic aperture radar (SAR) and optical image matching aims to acquire correspondences from a certain pair of SAR and optical images. Recent advances in the image-to-image translation provided a way to simplify the SAR–optical image matching into the SAR–SAR or optical–optical image matchings. The existing image-to-image translations mainly focus on supervised or unsupervised learning. However, gathering sufficient amounts of aligned training data for supervised learning is challenging, while unsupervised learning cannot guarantee enough correct correspondences. In this work, we investigate the applicability of semi-supervised image-to-image translation for SAR–optical image matching such that both aligned and unaligned SAR–optical images could be used. To this end, we combine the benefits of both supervised and unsupervised well-known image-to-image translation methods, i.e., Pix2pix and CycleGAN, and propose a simple yet effective semi-supervised image-to-image translation framework. Through extensive experimental comparisons to the baseline methods, we verify the effectiveness of the proposed framework in both semi-supervised and fully supervised settings. Our codes are available at https://github.com/WenliangDu/Semi-I2I.","1558-0571","","10.1109/LGRS.2022.3223353","National Natural Science Foundation of China(grant numbers:62002360,62272461,62101555,61806206,62106268); Science and Technology Development Fund of Macau (Macau FDCT)(grant numbers:0038/2020/A1); Opening Fund of State Key Laboratory of Lunar and Planetary Sciences (Macau University of Science and Technology) (Macau FDCT)(grant numbers:119/2017/A3); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201346,BK20210488); “Double First-Class” Project of China University of Mining and Technology for Independent Innovation and Social Service(grant numbers:2022ZZCX06); China Postdoctoral Science Foundation(grant numbers:2022M713379); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9955555","Generative adversarial networks (GANs);image matching;semi-supervised image synthesis;synthetic aperture radar (SAR)","Optical imaging;Optical sensors;Adaptive optics;Image matching;Training;Synthetic aperture radar;Optical distortion","image matching;learning (artificial intelligence);optical images;radar imaging;synthetic aperture radar;unsupervised learning","aligned SAR-optical images;fully supervised settings;image-to-image translation methods;optical image matching;optical-optical image matchings;SAR-optical image;SAR-SAR;semisupervised image-to-image translation framework;unaligned SAR-optical images","","","","14","IEEE","18 Nov 2022","","","IEEE","IEEE Journals"
"GH-Feat: Learning Versatile Generative Hierarchical Features From GANs","Y. Xu; Y. Shen; J. Zhu; C. Yang; B. Zhou","Department of Information Engineering, the Chinese University of Hong Kong, Hong Kong SAR; Ant Research, China; School of Computer Science and Engineering, the Hong Kong University of Science and Technology, Hong Kong SAR; Department of Information Engineering, the Chinese University of Hong Kong, Hong Kong SAR; Computer Science Department, University of California, Los Angeles, USA","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2022","PP","99","1","16","Recent years witness the tremendous success of generative adversarial networks (GANs) in synthesizing photo-realistic images. GAN generator learns to compose realistic images and reproduce the real data distribution. Through that, a hierarchical visual feature with multi-level semantics spontaneously emerges. In this work we investigate that such a generative feature learned from image synthesis exhibits great potentials in solving a wide range of computer vision tasks, including both generative ones and more importantly discriminative ones. We first train an encoder by considering the pre-trained StyleGAN generator as a learned loss function. The visual features produced by our encoder, termed as Generative Hierarchical Features (GH-Feat), highly align with the layer-wise GAN representations, and hence describe the input image adequately from the reconstruction perspective. Extensive experiments support the versatile transferability of GH-Feat across a range of applications, such as image editing, image processing, image harmonization, face verification, landmark detection, layout prediction, image retrieval, etc. We further show that, through a proper spatial expansion, our developed GH-Feat can also facilitate fine-grained semantic segmentation using only a few annotations. Both qualitative and quantitative results demonstrate the appealing performance of GH-Feat. Code and models are available at https://genforce.github.io/ghfeat/.","1939-3539","","10.1109/TPAMI.2022.3225788","Early Career Scheme (ECS) through the Research Grants Council (RGC) of Hong Kong(grant numbers:24206219,14204521); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9968154","Generative adversarial network;generative representation;feature learning;image editing","Task analysis;Generators;Visualization;Generative adversarial networks;Feature extraction;Training;Representation learning","","","","","","","IEEE","1 Dec 2022","","","IEEE","IEEE Early Access Articles"
"Atrous cGAN for SAR to Optical Image Translation","J. Noa Turnes; J. D. B. Castro; D. L. Torres; P. J. S. Vega; R. Q. Feitosa; P. N. Happ","Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil; Department of Electrical Engineering, Pontifical Catholic University of Rio de Janeiro, Rio de Janeiro, Brazil","IEEE Geoscience and Remote Sensing Letters","17 Dec 2021","2022","19","","1","5","Conditional (cGAN)-based methods proposed so far for synthetic aperture radar (SAR)-to-optical image synthesis tend to produce noisy and unsharp optical outcomes. In this work, we propose the atrous-cGAN, a novel cGAN architecture that improves the SAR-to-optical image translation. The proposed generator and discriminator networks rely on atrous convolutions and incorporate an atrous spatial pyramid pooling (ASPP) module to enhance fine details in the generated optical image by exploiting spatial context at multiple scales. This letter reports experiments carried out to assess the performance of atrous-cGAN for the synthesis of Landsat-8 images from Sentinel-1A data based on three public data sets. The experimental analysis indicated that the atrous-cGAN consistently outperformed the classical pix2pix counterpart in terms of visual quality, similar to the true optical image, and as a feature learning tool for semantic segmentation.","1558-0571","","10.1109/LGRS.2020.3031199","Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES); Conselho Nacional de Desenvolvimento Científico e Tecnológico (CNPq); NVIDIA Corporation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241239","Atrous spatial pyramid pooling (ASPP);generative adversarial networks;synthetic aperture radar (SAR)-optical synthesis","Optical imaging;Optical sensors;Generators;Synthetic aperture radar;Convolutional codes;Optical interferometry;Artificial satellites","geophysical image processing;image classification;image enhancement;image segmentation;learning (artificial intelligence);object detection;optical images;radar imaging;synthetic aperture radar","incorporate;atrous spatial pyramid pooling module;generated optical image;Landsat-8 images;atrous-cGAN;atrous cGAN;conditional-based methods;radar-to-optical image synthesis;noisy outcomes;unsharp optical outcomes;SAR-to-optical image translation;generator;discriminator networks;atrous convolutions;current 1.0 A","","10","","18","IEEE","27 Oct 2020","","","IEEE","IEEE Journals"
"Image Processing Using Multi-Code GAN Prior","J. Gu; Y. Shen; B. Zhou",The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","3009","3018","Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157000","","Gallium nitride;Image reconstruction;Task analysis;Generators;Semantics;Image resolution","image colour analysis;image reconstruction;image resolution;learning (artificial intelligence);neural nets","multicode GAN prior;generative adversarial networks;image synthesis;latent space;image processing tasks;multiple latent codes;multiple feature maps;image reconstruction quality;high-fidelity image reconstruction;image colorization;image inpainting;back-propagation;mGANprior;intermediate layer;super-resolution;semantic manipulation;layer-wise representation","","96","","53","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Regularizing Discriminative Capability of CGANs for Semi-Supervised Generative Learning","Y. Liu; G. Deng; X. Zeng; S. Wu; Z. Yu; H. -S. Wong","School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; School of Computer Science and Engineering, South China University of Technology; Department of Computer Science, City University of Hong Kong; School of Computer Science and Engineering, South China University of Technology; Department of Computer Science, City University of Hong Kong","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","5719","5728","Semi-supervised generative learning aims to learn the underlying class-conditional distribution of partially labeled data. Generative Adversarial Networks (GANs) have led to promising progress in this task. However, it still needs to further explore the issue of imbalance between real labeled data and fake data in the adversarial learning process. To address this issue, we propose a regularization technique based on Random Regional Replacement (R3-regularization) to facilitate the generative learning process. Specifically, we construct two types of between-class instances: cross-category ones and real-fake ones. These instances could be closer to the decision boundaries and are important for regularizing the classification and discriminative networks in our class-conditional GANs, which we refer to as R3-CGAN. Better guidance from these two networks makes the generative network produce instances with class-specific information and high fidelity. We experiment with multiple standard benchmarks, and demonstrate that the R3-regularization can lead to significant improvement in both classification and class-conditional image synthesis.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156751","","Training;Generators;Predictive models;Gallium nitride;Image generation;Data models;Games","image classification;neural nets;supervised learning","adversarial learning process;regularization technique;generative learning process;between-class instances;discriminative networks;class-conditional GANs;class-specific information;class-conditional image synthesis;semisupervised generative learning;class-conditional distribution;partially labeled data;generative adversarial networks;fake data;CGAN;random regional replacement;real labeled data","","9","","46","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Learning Generative Models of Tissue Organization with Supervised GANs","L. Han; R. F. Murphy; D. Ramanan","Robotics Institute, Carnegie Mellon University; Computational Biology Department and Department of Biological Sciences, Carnegie Mellon University; NA","2018 IEEE Winter Conference on Applications of Computer Vision (WACV)","7 May 2018","2018","","","682","690","A key step in understanding the spatial organization of cells and tissues is the ability to construct generative models that accurately reflect that organization. In this paper, we focus on building generative models of electron microscope (EM) images in which the positions of cell membranes and mitochondria have been densely annotated, and propose a two-stage procedure that produces realistic images using Generative Adversarial Networks (or GANs) in a supervised way. In the first stage, we synthesize a label ""image"" given a noise ""image"" as input, which then provides supervision for EM image synthesis in the second stage. The full model naturally generates label-image pairs. We show that accurate synthetic EM images are produced using assessment via (1) shape features and global statistics, (2) segmentation accuracies, and (3) user studies. We also demonstrate further improvements by enforcing a reconstruction loss on intermediate synthetic labels and thus unifying the two stages into one single end-to-end framework.","","978-1-5386-4886-5","10.1109/WACV.2018.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8354184","","Gallium nitride;Biological system modeling;Cells (biology);Organizations;Microscopy;Frequency modulation","biology computing;cellular biophysics;image classification;image segmentation;learning (artificial intelligence);realistic images","EM images;supervised GAN;generative adversarial networks;cell membranes;electron microscope images;building generative models;spatial organization;tissue organization;intermediate synthetic labels;accurate synthetic EM images;label-image pairs;EM image synthesis;noise image;label image;realistic images;two-stage procedure;mitochondria","","8","","32","IEEE","7 May 2018","","","IEEE","IEEE Conferences"
"BodyGAN: General-purpose Controllable Neural Human Body Generation","C. Yang; H. Li; S. Wu; S. Zhang; H. Yan; N. Jiao; J. Tang; R. Zhou; X. Liang; T. Zheng","Beijing Momo Technology Co., Ltd.; Shenzhen Campus of Sun Yat-sen University; Beijing Momo Technology Co., Ltd.; Beijing Momo Technology Co., Ltd.; Beijing Momo Technology Co., Ltd.; Beijing Momo Technology Co., Ltd.; Beijing Momo Technology Co., Ltd.; Beijing Momo Technology Co., Ltd.; Shenzhen Campus of Sun Yat-sen University; Beijing Momo Technology Co., Ltd.","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","27 Sep 2022","2022","","","7723","7732","Recent advances in generative adversarial networks (GANs) have provided potential solutions for photo-realistic human image synthesis. However, the explicit and individual control of synthesis over multiple factors, such as poses, body shapes, and skin colors, remains difficult for existing methods. This is because current methods mainly rely on a single pose/appearance model, which is limited in dis-entangling various poses and appearance in human images. In addition, such a unimodal strategy is prone to causing severe artifacts in the generated images like color distortions and unrealistic textures. To tackle these issues, this paper proposes a multi-factor conditioned method dubbed BodyGAN. Specifically, given a source image, our Body-GAN aims at capturing the characteristics of the human body from multiple aspects: (i) A pose encoding branch consisting of three hybrid subnetworks is adopted, to generate the semantic segmentation based representation, the 3D surface based representation, and the key point based rep-resentation of the human body, respectively. (ii) Based on the segmentation results, an appearance encoding branch is used to obtain the appearance information of the human body parts. (iii) The outputs of these two branches are represented by user-editable condition maps, which are then processed by a generator to predict the synthesized image. In this way, our BodyGAN can achieve the fine-grained dis-entanglement of pose, body shape, and appearance, and consequently enable the explicit and effective control of syn-thesis with diverse conditions. Extensive experiments on multiple datasets and a comprehensive user study show that our BodyGAN achieves the state-of-the-art performance.","2575-7075","978-1-6654-6946-3","10.1109/CVPR52688.2022.00758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878656","Image and video synthesis and generation","Training;Image coding;Three-dimensional displays;Shape;Image color analysis;Metaverse;Computational modeling","image colour analysis;image representation;image segmentation;object detection;user interfaces","BodyGAN;general-purpose controllable neural human Body generation;generative adversarial networks;photo-realistic human image synthesis;body shape;skin colors;dis-entangling;human images;color distortions;multifactor conditioned method;source image;Body-GAN;branch consisting;semantic segmentation based representation;3D surface based representation;key point based rep-resentation;appearance encoding branch;appearance information;human body parts;user-editable condition maps;synthesized image;fine-grained dis-entanglement","","","","47","IEEE","27 Sep 2022","","","IEEE","IEEE Conferences"
"A Heavy-tailed Distribution Data Generation Method Based On Generative Adversarial Network","X. Zhang; J. Zhou","College of Information Science & Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science & Technology, Beijing University of Chemical Technology, Beijing, China","2021 IEEE 10th Data Driven Control and Learning Systems Conference (DDCLS)","25 Jun 2021","2021","","","535","540","Heavy-tailed distribution widely exists in economic, financial, industrial and other data. The tail of heavy-tailed distribution is thicker than that of Gaussian distribution. Generative adversarial networks (GAN) is a data generation model, which is mainly used for image generation. The traditional GANs method usually assumes that the samples follow Gaussian distribution, so it is often not used directly in the heavy-tailed distribution data. In this paper, a generative adversarial network HTGAN for industrial heavy-tailed data is proposed. By improving the method of data preprocessing, the distribution of hidden variable Z is changed to T distribution. This method is applied to the fault data of TE process data. Compared with the traditional GAN method, the superiority of this method in industrial heavy tail data generation is proved.","2767-9861","978-1-6654-2423-3","10.1109/DDCLS52934.2021.9455631","NSFC(grant numbers:62073023); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9455631","Generative adversarial network;Heavy-tailed distribution;Virtual samples generation","Learning systems;Economics;Correlation;Image synthesis;Design methodology;Data preprocessing;Gaussian distribution","chemical engineering computing;data handling;Gaussian distribution;manufacturing data processing;neural nets;statistical distributions","TE process data;industrial heavy tail data generation;heavy-tailed distribution data generation method;Gaussian distribution;data generation model;image generation;generative adversarial network HTGAN;data preprocessing;T distribution;fault data","","","","13","IEEE","25 Jun 2021","","","IEEE","IEEE Conferences"
"Positional Encoding as Spatial Inductive Bias in GANs","R. Xu; X. Wang; K. Chen; B. Zhou; C. C. Loy","CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong; Applied Research Center, Tencent PCG; Shanghai AI Laboratory; CUHK-SenseTime Joint Lab, The Chinese University of Hong Kong; S-Lab, Nanyang Technological University","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","13564","13573","SinGAN shows impressive capability in learning internal patch distribution despite its limited effective receptive field. We are interested in knowing how such a translationinvariant convolutional generator could capture the global structure with just a spatially i.i.d. input. In this work, taking SinGAN and StyleGAN2 as examples, we show that such capability, to a large extent, is brought by the implicit positional encoding when using zero padding in the generators. Such positional encoding is indispensable for generating images with high fidelity. The same phenomenon is observed in other generative architectures such as DCGAN and PGGAN. We further show that zero padding leads to an unbalanced spatial bias with a vague relation between locations. To offer a better spatial inductive bias, we investigate alternative positional encodings and analyze their effects. Based on a more flexible positional encoding explicitly, we propose a new multi-scale training strategy and demonstrate its effectiveness in the state-of-the-art unconditional generator StyleGAN2. Besides, the explicit spatial inductive bias substantially improves SinGAN for more versatile image manipulation. 1","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578524","","Training;Computer vision;Image coding;Image synthesis;Computer architecture;Generators;Encoding","convolutional neural nets;image coding;image resolution;learning (artificial intelligence);neural net architecture","SinGAN;positional encoding;generative architectures;unconditional generator;spatial inductive bias;translation invariant convolutional generator;StyleGAN2;multiscale training strategy;image manipulation;generative adversarial networks;image resolution","","13","","53","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"A Generative Model for Volume Rendering","M. Berger; J. Li; J. A. Levine","Department of Computer Science, University of Arizona, Tucson, AZ; Department of Computer Science, University of Arizona, Tucson, AZ; Department of Computer Science, University of Arizona, Tucson, AZ","IEEE Transactions on Visualization and Computer Graphics","28 Feb 2019","2019","25","4","1636","1650","We present a technique to synthesize and analyze volume-rendered images using generative models. We use the Generative Adversarial Network (GAN) framework to compute a model from a large collection of volume renderings, conditioned on (1) viewpoint and (2) transfer functions for opacity and color. Our approach facilitates tasks for volume analysis that are challenging to achieve using existing rendering techniques such as ray casting or texture-based methods. We show how to guide the user in transfer function editing by quantifying expected change in the output image. Additionally, the generative model transforms transfer functions into a view-invariant latent space specifically designed to synthesize volume-rendered images. We use this space directly for rendering, enabling the user to explore the space of volume-rendered images. As our model is independent of the choice of volume rendering process, we show how to analyze volume-rendered images produced by direct and global illumination lighting, for a variety of volume datasets.","1941-0506","","10.1109/TVCG.2018.2816059","US National Science Foundation(grant numbers:IIS-1654221); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8316963","Volume rendering;generative models;deep learning;generative adversarial networks","Rendering (computer graphics);Transfer functions;Solid modeling;Gallium nitride;Image color analysis;Computational modeling;Sensitivity","image colour analysis;rendering (computer graphics);transfer functions","volume rendering process;generative adversarial network framework;GAN framework;volume-rendered image analysis;ray casting;texture-based methods;transfer function;view-invariant latent space;volume-rendered image synthesis;global illumination lighting;direct illumination lighting;generative model transforms transfer functions","","41","","53","IEEE","15 Mar 2018","","","IEEE","IEEE Journals"
"Closed-Form Factorization of Latent Semantics in GANs","Y. Shen; B. Zhou",The Chinese University of Hong Kong; The Chinese University of Hong Kong,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","1532","1540","A rich set of interpretable dimensions has been shown to emerge in the latent space of the Generative Adversarial Networks (GANs) trained for synthesizing images. In order to identify such latent dimensions for image editing, previous methods typically annotate a collection of synthesized samples and train linear classifiers in the latent space. However, they require a clear definition of the target attribute as well as the corresponding manual annotations, limiting their applications in practice. In this work, we examine the internal representation learned by GANs to reveal the underlying variation factors in an unsupervised manner. In particular, we take a closer look into the generation mechanism of GANs and further propose a closedform factorization algorithm for latent semantic discovery by directly decomposing the pre-trained weights. With a lightning-fast implementation, our approach is capable of not only finding semantically meaningful dimensions comparably to the state-of-the-art supervised methods, but also resulting in far more versatile concepts across multiple GAN models trained on a wide range of datasets.1","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.00158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9578007","","Computer vision;Limiting;Closed-form solutions;Annotations;Computational modeling;Semantics;Manuals","image representation;neural nets;unsupervised learning","latent semantics;GANs;generative adversarial networks;image synthesis;latent dimensions;image editing;linear classifier training;closed-form factorization;latent semantic discovery;semantically meaningful dimensions;supervised methods;multiple GAN models;unsupervised manner","","97","","29","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Repurposing GANs for One-shot Semantic Part Segmentation","P. Rewatbowornwong; N. Tritrong; S. Suwajanakorn","VISTEC, Thailand; VISTEC, Thailand; VISTEC, Thailand","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2022","PP","99","1","12","While GANs have shown success in realistic image generation, the idea of using GANs for other tasks unrelated to synthesis is underexplored. Do GANs learn meaningful structural parts of objects during their attempt to reproduce those objects? And can image synthesis serve as an “upstream” representation learning task? In this work, we test these hypotheses and propose a simple and effective approach based on GANs for fundamental vision tasks: semantic part segmentation and landmark detection. With our approach, these tasks only require as few as one labeled example along with an unlabeled dataset, rather than thousands of examples. Our key idea is to leverage a trained GAN to extract a pixel-wise representation from the input image and use it as feature vectors for a segmentation network. Our experiments demonstrate that this GAN-derived representation is “readily discriminative” and produces surprisingly good results that are comparable to those from supervised baselines trained with significantly more labels. We believe this novel repurposing of GANs underlies a new class of unsupervised representation learning, which can generalize to many other tasks. More results are available at https://RepurposeGANs.github.io/.","1939-3539","","10.1109/TPAMI.2022.3201285","PMU-B(grant numbers:B04G640230); ThaiSC Pioneer Program(grant numbers:pre5015); PTT public company limited; SCB public company limited; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9866571","representation learning;few-shot Learning;image part segmentation;generative model","Image segmentation;Task analysis;Semantics;Annotations;Training;Representation learning;Generative adversarial networks","","","","","","","IEEE","24 Aug 2022","","","IEEE","IEEE Early Access Articles"
"P$^{2}$-GAN: Efficient Stroke Style Transfer using Single Style Image","Z. Zheng; J. Liu; N. Zheng","Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Shaanxi, China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Shaanxi, China; Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University, Shaanxi, China","IEEE Transactions on Multimedia","","2022","PP","99","1","13","Style transfer is a useful image synthesis technique that can re-render given image into another artistic style while preserving its content information. Generative Adversarial Network (GAN) is a widely adopted framework toward this task for its better representation ability on local style patterns than the traditional Gram-matrix based methods. However, most previous methods rely on sufficient amount of pre-collected style images to train the model. In this paper, a novel Patch Permutation GAN (P$^{2}$-GAN) network that can efficiently learn the stroke style from a single style image is proposed. We use patch permutation to generate multiple training samples from the given style image. A patch discriminator that can simultaneously process patch-wise images and natural images seamlessly is designed. We also propose a local texture descriptor based criterion to quantitatively evaluate the style transfer quality. Experimental results showed that our method can produce finer quality re-renderings from single style image with improved computational efficiency compared with many state-of-the-arts methods.","1941-0077","","10.1109/TMM.2022.3203220","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9872037","Style Transfer;Generative Adversarial Network;Stroke Style;Patch Permutation","Training;Generative adversarial networks;Real-time systems;Generators;Feature extraction;Task analysis;Learning systems","","","","","","","IEEE","31 Aug 2022","","","IEEE","IEEE Early Access Articles"
"Multiple Facial Expressions Synthesis Driven by Editable Line Maps","D. Liu; Y. Yang; X. Jing","School of Software Engineering, Xi’an Jiaotong University, Xi’an, China; School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China; School of Automation Science and Engineering, Xi’an Jiaotong University, Xi’an, China","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","1645","1650","Facial expression is an important facial semantics on visual aspect. The facial expressions synthesis has a wide range of applications in human-computer interaction and virtual reality. In recent years, image synthesis base on generative adversarial networks(GANs) is developing rapidly. In the image-to-image translation work, we propose a new facial expression generation method base on the idea of conditional GANs and realize the optimization of the generated results. The main work of this paper includes: Editable facial lines map is utilized as a constraint, combining with neutral face images as inputs of generator, so that a variety of facial expression images can be generated by editing the constraints. Correntropy loss of feature matching is added, which is used to measure the intermediate representation between the real images and the generated images by improving the adversarial loss. Consequently, the generated facial expressions can be more realistic. Base on the ideas above, the proposed method needs only one generator to generate different realistic facial images with various expressions.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283195","Research and Development; Research and Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283195","Generative Adversarial Network;Facial Expression Synthesis;Correntropy;Image-to-Image","Training;Visualization;Semantics;Virtual reality;Loss measurement;Generators;Task analysis","face recognition;feature extraction;neural nets;virtual reality","important facial semantics;visual aspect;human-computer interaction;virtual reality;image synthesis base;image-to-image translation work;facial expression generation method base;conditional GAN;generated results;neutral face images;facial expression images;generated facial expressions;realistic facial images;multiple facial expressions synthesis;editable line maps;editable facial lines map","","","","25","IEEE","14 Dec 2020","","","IEEE","IEEE Conferences"
"LatentCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions","O. K. Yüksel; E. Simsar; E. G. Er; P. Yanardag",EPFL; Boğaziçi University; Boğaziçi University; Boğaziçi University,"2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","14243","14252","Recent research has shown that it is possible to find interpretable directions in the latent spaces of pre-trained Generative Adversarial Networks (GANs). These directions enable controllable image generation and support a wide range of semantic editing operations, such as zoom or rotation. The discovery of such directions is often done in a supervised or semi-supervised manner and requires manual annotations which limits their use in practice. In comparison, unsupervised discovery allows finding subtle directions that are difficult to detect a priori. In this work, we propose a contrastive learning-based approach to discover semantic directions in the latent space of pre-trained GANs in a self-supervised manner. Our approach finds semantically meaningful dimensions compatible with state-of-the-art methods.","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.01400","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710471","Image and video synthesis;Neural generative models","Computer vision;Image synthesis;Annotations;Computational modeling;Semantics;Manuals;Aerospace electronics","","","","12","","40","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Geometry-Guided Street-View Panorama Synthesis From Satellite Imagery","Y. Shi; D. Campbell; X. Yu; H. Li","Australian National University, Canberra, ACT, Australia; University of Oxford, Oxford, U.K.; University of Technology Sydney, Ultimo, NSW, Australia; Australian National University, Canberra, ACT, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","7 Nov 2022","2022","44","12","10009","10022","This paper presents a new approach for synthesizing a novel street-view panorama given a satellite image, as if captured from the geographical location at the center of the satellite image. Existing works approach this as an image generation problem, adopting generative adversarial networks to implicitly learn the cross-view transformations, but ignore the geometric constraints. In this paper, we make the geometric correspondences between the satellite and street-view images explicit so as to facilitate the transfer of information between domains. Specifically, we observe that when a 3D point is visible in both views, and the height of the point relative to the camera is known, there is a deterministic mapping between the projected points in the images. Motivated by this, we develop a novel satellite to street-view projection (S2SP) module which learns the height map and projects the satellite image to the ground-level viewpoint, explicitly connecting corresponding pixels. With these projected satellite images as input, we next employ a generator to synthesize realistic street-view panoramas that are geometrically consistent with the satellite images. Our S2SP module is differentiable and the whole framework is trained in an end-to-end manner. Extensive experimental results on two cross-view benchmark datasets demonstrate that our method generates more accurate and consistent images than existing approaches.","1939-3539","","10.1109/TPAMI.2022.3140750","ARC-Discovery(grant numbers:190102261); China Scholarship Council; Continental AG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9674229","Novel view synthesis;satellite imagery;street-view imagery","Satellites;Cameras;Image synthesis;Task analysis;Semantics;Generators;Visualization","cameras;feature extraction;geometry;image classification;image matching;image representation;image segmentation;image sensors;learning (artificial intelligence);object detection;shape recognition","accurate images;consistent images;cross-view benchmark datasets;cross-view transformations;geometry-guided street-view panorama synthesis;image generation problem;projected satellite images;realistic street-view panoramas;satellite image;satellite imagery;street-view images;street-view projection module","","3","","31","IEEE","7 Jan 2022","","","IEEE","IEEE Journals"
"Homomorphic Interpolation Network for Unpaired Image-to-Image Translation","Y. -C. Chen; J. Jia","Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Computer Science, School of Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Transactions on Pattern Analysis and Machine Intelligence","1 Apr 2022","2022","44","5","2534","2547","Generative adversarial networks have achieved great success in unpaired image-to-image translation. Cycle consistency, a key component for this task, allows modeling the relationship between two distinct domains without paired data. In this paper, we propose an alternative framework, as an extension of latent space interpolation, to consider the intermediate region between two domains during translation. It is based on the assumption that in a flat and smooth latent space, there exist many paths that connect two sample points. Properly selecting paths makes it possible to change only certain image attributes, which is useful for generating intermediate images between the two domains. With this idea, our framework includes an encoder, an interpolator and a decoder. The encoder maps natural images to a convex and smooth latent space where interpolation is applicable. The interpolator controls the interpolation path so that desired intermediate samples can be obtained. Finally, the decoder inverts interpolated features back to pixel space. We also show that by choosing different reference images and interpolation paths, this framework can be applied to multi-domain and multi-modal translation. Extensive experiments manifest that our framework achieves superior results and is flexible for various tasks.","1939-3539","","10.1109/TPAMI.2020.3036543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9250648","","Interpolation;Task analysis;Decoding;Aerospace electronics;Faces;Image synthesis;Generators","","","Algorithms","","","51","IEEE","6 Nov 2020","","","IEEE","IEEE Journals"
"Panoptic-aware Image-to-Image Translation","L. Zhang; P. Ratsamee; B. Wang; Z. Luo; Y. Uranishi; M. Higashida; H. Takemura","Osaka University, Japan; Osaka Institute of Technology, Japan; Osaka University, Japan; Osaka University, Japan; Osaka University, Japan; Osaka University, Japan; Osaka University, Japan","2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)","6 Feb 2023","2023","","","259","268","Despite remarkable progress in image translation, the complex scene with multiple discrepant objects remains a challenging problem. The translated images have low fidelity and tiny objects in fewer details causing unsatisfactory performance in object recognition. Without thorough object perception (i.e., bounding boxes, categories, and masks) of images as prior knowledge, the style transformation of each object will be difficult to track in translation. We propose panoptic-aware generative adversarial networks (PanopticGAN) for image-to-image translation together with a compact panoptic segmentation dataset. The panoptic perception (i.e., foreground instances and background semantics of the image scene) is extracted to achieve alignment between object content codes of the input domain and panoptic-level style codes sampled from the target style space, then refined by a proposed feature masking module for sharping object boundaries. The image-level combination between content and sampled style codes is also merged for higher fidelity image generation. Our proposed method was systematically compared with different competing methods and obtained significant improvement in both image quality and object recognition performance.","2642-9381","978-1-6654-9346-8","10.1109/WACV56688.2023.00034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10030413","Algorithms: Computational photography;image and video synthesis;Image recognition and understanding (object detection;categorization;segmentation;scene modeling;visual reasoning);Machine learning architectures;formulations;and algorithms (including transfer;low-shot;semi-;self-;and un-supervised learning)","Image quality;Training;Image segmentation;Computer vision;Codes;Image synthesis;Semantics","","","","","","43","IEEE","6 Feb 2023","","","IEEE","IEEE Conferences"
"High-Resolution UAV Image Generation for Sorghum Panicle Detection","E. Cai; Z. Luo; S. Baireddy; J. Guo; C. Yang; E. J. Delp","School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana; School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana; School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana; School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana; School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana; School of Electrical and Computer Engineering, Purdue University, West Lafayette, Indiana","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","23 Aug 2022","2022","","","1675","1684","The number of panicles (or heads) of Sorghum plants is an important phenotypic trait for plant development and grain yield estimation. The use of Unmanned Aerial Vehicles (UAVs) enables the capability of collecting and analyzing Sorghum images on a large scale. Deep learning can provide methods for estimating phenotypic traits from UAV images but requires a large amount of labeled data. The lack of training data due to the labor-intensive ground truthing of UAV images causes a major bottleneck in developing methods for Sorghum panicle detection and counting. In this paper, we present an approach that uses synthetic training images from generative adversarial networks (GANs) for data augmentation to enhance the performance of Sorghum panicle detection and counting. Our method can generate synthetic high-resolution UAV RGB images with panicle labels by using image-to-image translation GANs with a limited ground truth dataset of real UAV RGB images. The results show the improvements in panicle detection and counting using our data augmentation approach.","2160-7516","978-1-6654-8739-9","10.1109/CVPRW56347.2022.00174","Advanced Research Projects Agency; U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857394","","Training;Deep learning;Head;Image synthesis;Conferences;Training data;Autonomous aerial vehicles","autonomous aerial vehicles;crops;geophysical image processing;image classification;image colour analysis;image segmentation;learning (artificial intelligence);remotely operated vehicles","phenotypic traits;UAV images;training data;labor-intensive ground truthing;Sorghum panicle detection;synthetic training images;panicle labels;image-to-image translation GANs;UAV RGB images;high-resolution UAV image generation;Sorghum plants;important phenotypic trait;plant development;grain yield estimation;Unmanned Aerial Vehicles;UAVs;collecting analyzing Sorghum images","","","","48","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"Distinguishing between natural and GAN-generated face images by combining global and local features","B. Chen; W. Tan; Y. Wang; G. Zhao","Nanjing University of Information Science and Technology, Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET), Nanjing, China; Nanjing University of Information Science and Technology, School of Computer, Nanjing, China; University of Warwick, Warwick Manufacturing Group, Coventry, UK; University of Oulu, Center for Machine Vision and Signal Analysis, Oulu, Finland","Chinese Journal of Electronics","10 Jan 2023","2022","31","1","59","67","With the development of face image synthesis and generation technology based on generative adversarial networks (GANs), it has become a research hotspot to determine whether a given face image is natural or generated. However, the generalization capability of the existing algorithms is still to be improved. Therefore, this paper proposes a general algorithm. To do so, firstly, the learning on important local areas, containing many face key-points, is strengthened by combining the global and local features. Secondly, metric learning based on the ArcFace loss is applied to extract common and discriminative features. Finally, the extracted features are fed into the classification module to detect GAN-generated faces. The experiments are conducted on two publicly available natural datasets (CelebA and FFHQ) and seven GAN-generated datasets. Experimental results demonstrate that the proposed algorithm achieves a better generalization performance with an average detection accuracy over 0.99 than the state-of-the-art algorithms. Moreover, the proposed algorithm is robust against additional attacks, such as Gaussian blur, and Gaussian noise addition.","2075-5597","","10.1049/cje.2020.00.372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9994806","Generated image;Global feature;Local features;Generative adversarial network;Metric learning","Feature extraction;Faces;Face recognition;Measurement;Convolutional codes;Classification algorithms;Training","","","","","","","","10 Jan 2023","","","CIE","CIE Journals"
"Guest Editorial Generative Adversarial Networks in Biomedical Image Computing","H. Fu; T. Zhou; S. Li; A. F. Frangi","Agency for Science, Technology and Research, Singapore; Nanjing University of Science and Technology, China; University of Western Ontario, Canada; KU Leuven, Belgium","IEEE Journal of Biomedical and Health Informatics","17 Jan 2022","2022","26","1","4","6","The papers in this special section focus on generative adversarial networks in biomedical image computing. The field of biomedical imaging has obtained great progress from Roentgen’s original discovery of the X-ray to the current imaging tools, including Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Computed Tomography (CT), and Ultrasound (US). The benefits of using these non-invasive imaging technologies are to assess the current condition of an organ or tissue, which can be used to monitor a patient over time over time for accurate and timely diagnosis and treatment.With the development of imaging technologies, developing advanced artificial intelligence algorithms for automated image analysis has shown the potential to change many aspects of clinical applications within the next decade. Meanwhile, these advanced technologies have also brought new issues and challenges. Thus, there has been a growing demand for biomedical imaging computing to be a component of clinical trials and device improvement. Currently, Generative adversarial networks (GANs) have been attached growing interests in the computer vision community due to their capability of data generation or translation. GAN-based models are able to learn from a set of training data and generate new data with the same characteristics as the training ones, which have also proven to be the state of the art for generating sharp and realistic images. More importantly, GAN has been rapidly applied to many traditional and novel applications in the medical domain, such as image reconstruction, segmentation, diagnosis, synthesis, and so on. Despite GAN substantial progress in these areas, their application to medical image computing still faces challenges and unsolved problems remain.","2168-2208","","10.1109/JBHI.2021.3134004","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684161","","Special issues and sections;Biomedical imaging;Generative adversarial networks;Image segmentation;Image synthesis;Image reconstruction;Magnetic resonance imaging;Feature extraction;Adversarial machine learning","","","","","","7","IEEE","17 Jan 2022","","","IEEE","IEEE Journals"
