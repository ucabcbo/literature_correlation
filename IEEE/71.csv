"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Cloud Removal of Satellite Images Using Convolutional Neural Network With Reliable Cloudy Image Synthesis Model","K. -Y. Lee; J. -Y. Sim","School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea; School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology, Ulsan, South Korea","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","3581","3585","Cloudy pixels in satellite images degrade the visibility of captured surface structure. We propose a novel cloudy image synthesis model and develop a cloud removal algorithm using convolutional neural network. We extract the cloud masks from real cloudy satellite images and from real sky images with clouds. Then we investigate the characteristics of real cloudy images and devise a reliable cloudy image synthesis model which considers the background surface color, misalignement of channel images, and blur in clouds. We train a hierarchical cloud removal network using the synthetic cloudy images. Experimental results demonstrate that the proposed algorithm removes the clouds from cloudy satellite images faithfully and outperforms the existing methods.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803666","Cloud removal;satellite image;convolutional neural network","Clouds;Cloud computing;Satellites;Image color analysis;Remote sensing;Image synthesis;Surface structures","clouds;geophysical image processing;image classification;image colour analysis;image denoising;image resolution;infrared imaging;neural nets;remote sensing","convolutional neural network;reliable cloudy image synthesis model;cloudy pixels;novel cloudy image synthesis model;cloud removal algorithm;cloud masks;cloudy satellite images;sky images;channel images;hierarchical cloud removal network;synthetic cloudy images","","8","","14","IEEE","26 Aug 2019","","","IEEE","IEEE Conferences"
"Satellite image formation and preprocessing methods","T. Kuchkorov; S. Urmanov; M. Kuvvatova; I. Anvarov","Computer Engineering Tashkent University of Information Technologies named after Muhammad al-Khwarizmi, Tashkent, Uzbekistan; Computer Engineering Tashkent University of Information Technologies named after Muhammad al-Khwarizmi, Tashkent, Uzbekistan; Computer Engineering Nukus branch of Tashkent University of Information Technologies named after Muhammad al-Khwarizmi, Nukus, Uzbekistan; Remote sensing Electronics Mikroelektronikaplus, Tashkent, Uzbekistan","2020 International Conference on Information Science and Communications Technologies (ICISCT)","19 Feb 2021","2020","","","1","4","This paper gives a conceptual and practical introduction to the field of satellite image formation and preprocessing. In particular, the process of satellite image generation is discussed going through each step of its lifecycle. A special explanatory emphasis is given to the process of remote sensing and further mapping and fusion of satellite imagery within acquired segments of electromagnetic spectrum. Moreover, methods of satellite imagery retrieval have been practically demonstrated. The satellite imagery data has been extracted from Sentinel 2 dataset and analyzed with Python language image processing libraries.","","978-1-7281-9969-6","10.1109/ICISCT50599.2020.9351456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351456","Computer vision;satellite image;image formation;image preprocessing;NDVI;Sentinel 2;satellite;imagery;python;sentinel;remote sensing","Information science;Satellites;Image synthesis;Wavelength measurement;Libraries;Sensors;Remote sensing","artificial satellites;geophysical image processing;image processing;Python;remote sensing","conceptual introduction;practical introduction;satellite image formation;satellite image generation;special explanatory emphasis;satellite imagery retrieval;satellite imagery data;preprocessing methods;Sentinel 2 dataset;Python language image processing libraries","","3","","5","IEEE","19 Feb 2021","","","IEEE","IEEE Conferences"
"SatGAN: Satellite Image Generation using Conditional Adversarial Networks","M. Shah; M. Gupta; P. Thakkar","Computer Science and Engineering, Nirma University, Ahmedabad, India; Computer Science and Engineering, Nirma University, Ahmedabad, India; Computer Science and Engineering, Nirma University, Ahmedabad, India","2021 International Conference on Communication information and Computing Technology (ICCICT)","12 Aug 2021","2021","","","1","6","Mapping accurate satellite images from street view is a challenging image-to-image translation task. Recent advances in Generative Adversarial Networks (GANs) have shown promising results in image-to-image translation. Pix2Pix is a generalized framework for image-to-image translation which uses Conditional Adversarial Networks. It has performed well on a large number of datasets. However, it uses a simple pixel-level reconstruction loss due to which the output suffers distortion in some cases. In this paper, we propose SatGAN, which is based on Pix2Pix but adds the perceptual reconstruction loss with the pixel-level reconstruction loss to produce colourful and blur free images. The perceptual reconstruction loss forces the Generator to generate samples having a similar feature representation with that of the ground truth. We experimentally prove that SatGAN produces both qualitatively and quantitatively better results than Pix2Pix for street-view to satellite dataset.","","978-1-6654-0430-3","10.1109/ICCICT50803.2021.9510104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9510104","Conditional Adversarial Network;Image-to-Image translation;Perceptual reconstruction loss;Pix2Pix","Computers;Satellites;Image synthesis;Generative adversarial networks;Distortion;Generators;Task analysis","cartography;image colour analysis;image reconstruction;image restoration;neural nets;satellite communication","Pix2Pix;simple pixel-level reconstruction loss;SatGAN;perceptual reconstruction loss;satellite image generation;street view;conditional adversarial networks;accurate satellite images mapping;image-to-image translation task;generative adversarial networks;pixel-level reconstruction loss;blur free images;colourful images;feature representation;ground truth","","","","37","IEEE","12 Aug 2021","","","IEEE","IEEE Conferences"
"High-reflectivity objects distributed optical satellite image fusion based on NDVI classification","Z. Zexing; X. Qizhi; W. Haibo; Y. Wenyong","Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; Beijing Key Laboratory of Digital Media, Beihang University, Beijing, China; China Centre for Resources satellite Data and Application, Beijing, China; China Centre for Resources satellite Data and Application, Beijing, China","2017 2nd International Conference on Frontiers of Sensors Technologies (ICFST)","18 Dec 2017","2017","","","231","235","Ratioing method is one type of the most famous fusion methods in remote sensing image fusion domain. Generally, the ratioing method synthesizes a low-resolution panchromatic (Pan) image by adaptive weighted summation of a multispectral (MS) image. Consequently, the accuracy of the weights for low-resolution Pan image synthesis is of great importance. However, in most cases, the optical satellite images contain lots of high-reflectivity objects, such as clouds covered regions, and high-reflectivity buildings. These objects are saturate due to their strong reflectance. The distortion of saturated objects results in the failure of weights calculation, so that causes the color distortion of fused images. To solve the problem, this paper proposes a high-reflectivity objects distributed optical satellite image fusion method based on NDVI classification. First, the NDVI index is employed to classify the pixels of a MS image into high-reflectivity group and normal group, then the pixels in normal group is used to calculate the weighted coefficients, finally the fused image is obtained by ratioing transform. Experimental results on a large number of test images show that the proposed method has good performance on reducing color distortion.","","978-1-5090-4860-1","10.1109/ICFST.2017.8210509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8210509","Image fusion;Pan-sharpening;Remote sensing image;NDVI index","Remote sensing;Optical distortion;Optical sensors;Image color analysis;Image resolution;Optical imaging;Distortion","geophysical image processing;image classification;image colour analysis;image fusion;image resolution;sensor fusion;vegetation;vegetation mapping","high-reflectivity objects;NDVI classification;ratioing method;famous fusion methods;remote sensing image fusion domain;low-resolution panchromatic image;adaptive weighted summation;multispectral image;low-resolution Pan image synthesis;optical satellite images;high-reflectivity buildings;strong reflectance;saturated objects results;weights calculation;color distortion;fused image;optical satellite image fusion method;MS image;high-reflectivity group;test images;ratioing transform;NDVI index","","","","20","IEEE","18 Dec 2017","","","IEEE","IEEE Conferences"
"Geometry-Aware Satellite-to-Ground Image Synthesis for Urban Areas","X. Lu; Z. Li; Z. Cui; M. R. Oswald; M. Pollefeys; R. Qin",The Ohio State University; ETH Zürich; ETH Zürich; ETH Zürich; Microsoft; The Ohio State University,"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","856","864","We present a novel method for generating panoramic street-view images which are geometrically consistent with a given satellite image. Different from existing approaches that completely rely on a deep learning architecture to generalize cross-view image distributions, our approach explicitly loops in the geometric configuration of the ground objects based on the satellite views, such that the produced ground view synthesis preserves the geometric shape and the semantics of the scene. In particular, we propose a neural network with a geo-transformation layer that turns predicted ground-height values from the satellite view to a ground view while retaining the physical satellite-to-ground relation. Our results show that the synthesized image retains well-articulated and authentic geometric shapes, as well as texture richness of the street-view in various scenarios. Both qualitative and quantitative results demonstrate that our method compares favorably to other state-of-the-art approaches that lack geometric consistency.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9156602","","Satellites;Semantics;Transforms;Three-dimensional displays;Layout;Gallium nitride;Roads","feature extraction;image representation;image segmentation;image sensors;image texture;learning (artificial intelligence)","satellite-to-ground relation;synthesized image;authentic geometric shapes;geometric consistency;geometry-aware satellite-to-ground image synthesis;urban areas;panoramic street-view images;given satellite image;deep learning architecture;cross-view image distributions;geometric configuration;satellite view;produced ground view synthesis;geometric shape;predicted ground-height values","","18","","28","IEEE","5 Aug 2020","","","IEEE","IEEE Conferences"
"Detection of Synthesized Satellite Images Using Deep Neural Networks","W. -H. Liao; Y. -S. Chang; Y. -C. Wu","Dept. of Computer Science, National Chengchi University, Taipei, Taiwan; Dept. of Computer Science, National Chengchi University, Taipei, Taiwan; Artificial Intelligence and E-learning Center, National Chengchi University, Taipei, Taiwan","2023 17th International Conference on Ubiquitous Information Management and Communication (IMCOM)","8 Feb 2023","2023","","","1","5","The technology of generative adversarial networks (GAN) is constantly evolving, and synthesized images can no longer be accurately distinguished by the human eyes alone. GAN has been applied to the analysis of satellite images, mostly for the purpose of data augmentation. Recently, however, we have seen a twist in its usage. In information warfare, GAN has been used to create fake satellite images or modify the image content by putting fake bridges, buildings and clouds to mislead or conceal important intelligence. To address the increasing counterfeit cases in satellite images, the goal of this research is to develop algorithms that can classify fake remote sensing images robustly and efficiently. There exist many techniques to synthesize or manipulate the content of satellite images. In this paper, we focus on the case when the entire image is forged. Three satellite image synthesis methods, including ProGAN, cGAN and CycleGAN will be investigated. The effect of image pre-processing such as histogram equalization and bilateral filter will also be evaluated. Experiments show that satellite images generated by different GANs can be easily identified by individually trained models. The performance degraded when model trained with one type of GAN samples is employed to determine the originality of images synthesized with other types of GANs. Additionally, when histogram equalization is applied to the images, the detection model fails to distinguish its authenticity. A four-class universal classification model is proposed to address this issue. An overall accuracy of over 99% has been achieved even when pre-processing has been applied.","","978-1-6654-5348-6","10.1109/IMCOM56909.2023.10035570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035570","Satellite Imagery;Generative Adversarial Network;Image Forgery Detection","Histograms;Satellites;Image synthesis;Neural networks;Interference;Generative adversarial networks;Forgery","","","","","","12","IEEE","8 Feb 2023","","","IEEE","IEEE Conferences"
"Geometry-Guided Street-View Panorama Synthesis From Satellite Imagery","Y. Shi; D. Campbell; X. Yu; H. Li","Australian National University, Canberra, ACT, Australia; University of Oxford, Oxford, U.K.; University of Technology Sydney, Ultimo, NSW, Australia; Australian National University, Canberra, ACT, Australia","IEEE Transactions on Pattern Analysis and Machine Intelligence","7 Nov 2022","2022","44","12","10009","10022","This paper presents a new approach for synthesizing a novel street-view panorama given a satellite image, as if captured from the geographical location at the center of the satellite image. Existing works approach this as an image generation problem, adopting generative adversarial networks to implicitly learn the cross-view transformations, but ignore the geometric constraints. In this paper, we make the geometric correspondences between the satellite and street-view images explicit so as to facilitate the transfer of information between domains. Specifically, we observe that when a 3D point is visible in both views, and the height of the point relative to the camera is known, there is a deterministic mapping between the projected points in the images. Motivated by this, we develop a novel satellite to street-view projection (S2SP) module which learns the height map and projects the satellite image to the ground-level viewpoint, explicitly connecting corresponding pixels. With these projected satellite images as input, we next employ a generator to synthesize realistic street-view panoramas that are geometrically consistent with the satellite images. Our S2SP module is differentiable and the whole framework is trained in an end-to-end manner. Extensive experimental results on two cross-view benchmark datasets demonstrate that our method generates more accurate and consistent images than existing approaches.","1939-3539","","10.1109/TPAMI.2022.3140750","ARC-Discovery(grant numbers:190102261); China Scholarship Council; Continental AG; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9674229","Novel view synthesis;satellite imagery;street-view imagery","Satellites;Cameras;Image synthesis;Task analysis;Semantics;Generators;Visualization","cameras;feature extraction;geometry;image classification;image matching;image representation;image segmentation;image sensors;learning (artificial intelligence);object detection;shape recognition","accurate images;consistent images;cross-view benchmark datasets;cross-view transformations;geometry-guided street-view panorama synthesis;image generation problem;projected satellite images;realistic street-view panoramas;satellite image;satellite imagery;street-view images;street-view projection module","","3","","31","IEEE","7 Jan 2022","","","IEEE","IEEE Journals"
"UAV-Satellite View Synthesis for Cross-View Geo-Localization","X. Tian; J. Shao; D. Ouyang; H. T. Shen","Center for Future Media, School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; Sichuan Artificial Intelligence Research Institute, Yibin, China; College of Computer Science, Chongqing University, Chongqing, China; Sichuan Artificial Intelligence Research Institute, Yibin, China","IEEE Transactions on Circuits and Systems for Video Technology","1 Jul 2022","2022","32","7","4804","4815","The goal of cross-view image matching based on geo-localization is to determine the location of a given ground-view image (front view) by matching it with a group of satellite-view images (vertical view) with geographic tags. Due to the rapid development of unmanned aerial vehicle (UAV) technology in recent years, it has provided a real viewpoint close to 45 degrees (oblique view) to bridge the visual gap between views. However, existing methods ignore the direct geometric space correspondence of UAV-satellite views, and only use brute force for feature matching, leading to inferior performance. In this context, we propose an end-to-end cross-view matching method that integrates cross-view synthesis module and geo-localization module, which fully considers the spatial correspondence of UAV-satellite views and the surrounding area information. To be specific, the cross-view synthesis module includes two parts: the oblique view of UAV is first converted to the vertical view by perspective projection transformation (PPT), which makes the UAV image closer to the satellite image; then we use conditional generative adversarial nets (CGAN) to synthesize the UAV image with vertical view style, which is close to the real satellite image by learning the converted UAV as the input image and the real satellite image as the label. Geo-localization module refers to existing local pattern network (LPN), which explicitly considers the surrounding environment of the target building. These modules are integrated in a single architecture called PCL, which mutually reinforce each other. Our method is superior to the existing UAV-satellite cross-view methods, which improves by about 5%.","1558-2205","","10.1109/TCSVT.2021.3121987","National Key Research and Development Program of China(grant numbers:2018AAA0102200); National Natural Science Foundation of China(grant numbers:61832001,61632007); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9583266","Cross-view image matching;geo-localization;image synthesis","Satellites;Unmanned aerial vehicles;Feature extraction;Task analysis;Location awareness;Image matching;Visualization","autonomous aerial vehicles;image matching;remotely operated vehicles;transforms","UAV image;vertical view style;satellite image;converted UAV;input image;geo-localization module;existing UAV-satellite cross-view methods;UAV-satellite view synthesis;cross-view geo-localization;cross-view image;given ground-view image;front view;satellite-view images;unmanned aerial vehicle technology;45 degrees;UAV-satellite views;end-to-end cross-view matching method;integrates cross-view synthesis module","","10","","51","IEEE","21 Oct 2021","","","IEEE","IEEE Journals"
